<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Fix for MatchQueryBuilderTests.testToQuery test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16056</link><project id="" key="" /><description>See http://build-us-00.elastic.co/job/es_core_master_window-2012/2308/testReport/junit/org.elasticsearch.index.query/MatchQueryBuilderTests/testToQuery/ for test failure details.

The change fixes the above test failure.

@nik9000 can you take a quick look and check this makes sense wrt. #15860 
</description><key id="127237726">16056</key><summary>Fix for MatchQueryBuilderTests.testToQuery test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Query DSL</label><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T14:33:07Z</created><updated>2016-01-19T08:57:46Z</updated><resolved>2016-01-19T08:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-18T14:59:19Z" id="172552529">LGTM. Thanks! Its much nicer.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow RandomQueryBuilder to work independent from AbstractQueryTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16055</link><project id="" key="" /><description>Currently random test query generation via RandomQueryBuilder.createQuery() will initialize AbstractQueryTestCase, and for that reason is unusable from other tests (like SearchSourceBuilderTests). This change factors out common test query generation code into static helper methods in the respective test classes that can be used without having to instantiate AbstractQueryTestCase.
</description><key id="127218240">16055</key><summary>Allow RandomQueryBuilder to work independent from AbstractQueryTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>test</label></labels><created>2016-01-18T12:34:04Z</created><updated>2016-09-14T17:27:48Z</updated><resolved>2016-09-13T09:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T15:03:32Z" id="208389310">I wonder if it'd be a good idea to make a test superclass that had similar bits of mocking to AbstractQueryTestCase and let all users inherit from that. That sounds less invasive but pretty twisted. Also, in that case you'd probably want to more RandomQueryBuilder to a method on that class.
</comment><comment author="dakrone" created="2016-09-12T21:23:25Z" id="246498493">@cbuescher ping, what's the word on this?
</comment><comment author="cbuescher" created="2016-09-13T09:40:40Z" id="246629284">@dakrone will close this, the PR is pretty stale. We are still tracking our wish to have more generallized random query generation in #14414 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut over all index scope settings to the new setting infrastrucuture</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16054</link><project id="" key="" /><description>This change moves all `index.*` settings over to the new infrastructure. This means in short that:
- every setting that has an index scope must be registered up-front
- index settings are validated on index creation,  template creation, index settings update
- node level settings starting with `index.*` are validated on node startup
- settings that are private to ES like `index.version.created` can only be set by tests when they install a specific test plugin.
- all index settings can be reset by passing `null` as their value on update
- all index settings defaults can be listed via the settings APIs

Closes #12854
Closes #6732
Closes #16032
Closes #12790

Thanks @brwe who helped me a lot by going through the mechanical part of converting settings. Much appreciated!
</description><key id="127216050">16054</key><summary>Cut over all index scope settings to the new setting infrastrucuture</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T12:22:11Z</created><updated>2016-01-20T13:40:09Z</updated><resolved>2016-01-19T11:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-18T16:29:14Z" id="172577528">Its a huge improvement. I left some comments that amount to "I don't like this name" and "you just touched some undocumented code, can you document it now?" but I didn't see anything that should block merging. LGTM. If you merge I'll open another PR for some of the things I suggested.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor `GET {index}` vs `GET {index}/_settings` etc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16053</link><project id="" key="" /><description>Today the RestGetIndicesAction registers:

```
    controller.registerHandler(GET, "/{index}", this);
    controller.registerHandler(GET, "/{index}/{type}", this);
```

... where `{type}` is zero or more of `_aliases`, `_mappings`,  `_settings`, `_warmers` (which should be removed)

But then we have RestGetFieldMappingAction which registers:

```
    controller.registerHandler(GET, "/_mapping/field/{fields}", this);
    controller.registerHandler(GET, "/_mapping/{type}/field/{fields}", this);
    controller.registerHandler(GET, "/{index}/_mapping/field/{fields}", this);
    controller.registerHandler(GET, "/{index}/{type}/_mapping/field/{fields}", this);
    controller.registerHandler(GET, "/{index}/_mapping/{type}/field/{fields}", this);
```

and RestGetSettingsAction which registers:

```
    controller.registerHandler(GET, "/{index}/_settings/{name}", this);
    controller.registerHandler(GET, "/_settings/{name}", this);
    controller.registerHandler(GET, "/{index}/_setting/{name}", this);
```

and the deprecated RestGetIndicesAliasesAction which registers:

```
    controller.registerHandler(GET, "/{index}/_aliases/{name}", this);
    controller.registerHandler(GET, "/_aliases/{name}", this);
```

This makes for some confusion eg you'd expect `GET index/_settings` to be handled by RestGetSettingsAction but actually you have to specify eg `GET index/_settings/*` in order to force that code path.

Can we refactor this to simplify the code?
- RestGetIndicesAliasesAction is already deprecated.  
- I'm not sure that `GET {index}/_settings/{name}` really needs to be supported. I think we can get rid of it in master.
- The `GET index/type/_mapping` order has been deprecated since 1.0 in favour of `GET index/_mapping/type`, which just leaves handling the GET _mapping/field structure
</description><key id="127214414">16053</key><summary>Refactor `GET {index}` vs `GET {index}/_settings` etc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-18T12:10:50Z</created><updated>2016-01-18T12:25:07Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-18T12:25:07Z" id="172514604">See also #13906 which is about removing `GET aliases`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 1.7 explain API seems doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16052</link><project id="" key="" /><description>In according with documentation (https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-explain.html) i'm trying to use explain API.

curl -XGET 'localhost:9200/index/type/1/_explain?q=surname:mattia'

The result is : 
`{"_index":"neen","_type":"cliente","_id":"_explain","_version":1,"found":true,"_source":`

and not sometihng like this : 
`{
  "matches" : true,
  "explanation" : {
    "value" : 0.15342641,
    "description" : "fieldWeight(message:search in 0), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "tf(termFreq(message:search)=1)"
    }, {
      "value" : 0.30685282,
      "description" : "idf(docFreq=1, maxDocs=1)"
    }, {
      "value" : 0.5,
      "description" : "fieldNorm(field=message, doc=0)"
    } ]
  }
}`

like documentation say.
Why it doesn't work?
</description><key id="127207877">16052</key><summary>ElasticSearch 1.7 explain API seems doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iskyd</reporter><labels /><created>2016-01-18T11:30:54Z</created><updated>2016-01-18T13:46:25Z</updated><resolved>2016-01-18T13:46:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T13:46:24Z" id="172530573">This works for me:

```
PUT index/type/1
{
  "surname": "mattia"
}

GET /index/type/1/_explain?q=surname:mattia
```

or:

```
curl -XPUT "http://localhost:9200/index/type/1" -d'
{
  "surname": "mattia"
}'

curl -XGET "http://localhost:9200/index/type/1/_explain?q=surname:mattia"
```

Not sure what you're doing wrong. Maybe something with shell escaping?  Best place to ask is in the forums: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add link to ruby client doc in bulk API documentation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16051</link><project id="" key="" /><description /><key id="127203092">16051</key><summary>add link to ruby client doc in bulk API documentation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cavvia</reporter><labels /><created>2016-01-18T11:02:24Z</created><updated>2016-01-18T13:29:43Z</updated><resolved>2016-01-18T13:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T13:29:43Z" id="172527364">Hi @cavvia 

The Perl and Python modules provide higher level "bulk helpers", rather than just exposing the raw bulk API.  That's why they're linked to from these docs.  When the Ruby client does the same, then we should add the link

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent scoring on unscored queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16050</link><project id="" key="" /><description>Checked against **Elasticsearch 2.1.1**.

An unscored query placed directly in the `query` element assign a score of 1, while the same query placed in the `filter` section of a `bool` query assigns 0s.

``` bash
curl -XPOST localhost:9200/test/test/1 -d '
{
   "rank": 3
}'

# this assigns score 0 to each document
curl -XPOST /test/_search -d '
{
   "query": {
      "bool": {
         "filter": {
            "range": {
               "rank": {
                  "gte": 2
               }
            }
         }
      }
   }
}'

# this assigns score 1 to all documents
curl -XPOST localhost/test/_search -d '
{
   "query": {
      "range": {
         "rank": {
            "gte": 2
         }
      }
   }
}'
```

While, as discussed in https://github.com/elastic/elasticsearch/issues/15921, I understand that using a `constant_score` query solves the 0-score issue, I believe this behavioral difference is more a bug than a feature.
</description><key id="127201220">16050</key><summary>Inconsistent scoring on unscored queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2016-01-18T10:51:34Z</created><updated>2016-02-25T10:26:48Z</updated><resolved>2016-02-25T10:26:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T13:26:01Z" id="172526735">If you use a query, you get scores. If you only use filters, you don't get scores. That seems consistent to me.
</comment><comment author="clintongormley" created="2016-01-18T13:27:03Z" id="172526904">Note: this is not a breaking change as it wasn't possible to specify only filters before 2.0.
</comment><comment author="clintongormley" created="2016-01-18T13:27:53Z" id="172527039">@micpalmia Why do you think we should change this behaviour? How does it affect you?
</comment><comment author="micpalmia" created="2016-01-18T18:21:38Z" id="172611882">You wrote:

&gt; If you use a query, you get scores. If you only use filters, you don't get scores.

The documentation says

&gt; Queries and filters have been merged&#8201;&#8212;&#8201;**all filter clauses are now query clauses**

Also, you do get scores anyways - `_score` is not an optional part of the json output.

To me, those are both unscored queries - they are complete equivalents - why would they produce different scores? I understand one `range` query is in _filter context_ and one is in _query context_, but what's the rationale behind query context defaulting to score 1 and filter context defaulting to score 0?

This does not affect me when writing software on top of ES, but (as already happened in the issue linked above) can be confusing. 

Obviously feel free to close if deemed irrelevant :)
</comment><comment author="micpalmia" created="2016-01-18T18:27:15Z" id="172613407">Also, it seems to me that having the default score for _filter context_ set to 1 could only be beneficial (in terms of consistency, least surprise, etc.).
</comment><comment author="rmuir" created="2016-01-18T18:31:41Z" id="172616292">It must be the case that `filter`'d queries do not modify the score at all, that is why it is 0.

If you dont want queries to change the score, use `filter` to indicate that. Otherwise, they will.
</comment><comment author="micpalmia" created="2016-01-19T09:53:48Z" id="172796947">&gt; It must be the case that filter'd queries do not modify the score at all, that is why it is 0.

My observation was very far from suggesting that `filter`ed queries should modify the score.
The fact that filtered queries cannot modify the score, **thus** they score documents 0 does not really makes much sense to me.
</comment><comment author="micpalmia" created="2016-02-25T10:26:48Z" id="188710542">I still find this mildly annoying but it seems like it's not something that people in general find problematic. Thank you for taking the time to discuss this :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge feature/ingest branch into master branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16049</link><project id="" key="" /><description>This PR merges the Ingest Node feature as is describes in #14049 that was developed in the last months into the master branch.

This PR includes changes from all closed PRs with the [:ingest](https://github.com/elastic/elasticsearch/pulls?q=is%3Aclosed+is%3Apr+label%3A%22%3AIngest%22) label.
</description><key id="127192350">16049</key><summary>Merge feature/ingest branch into master branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T10:04:02Z</created><updated>2016-04-11T22:42:57Z</updated><resolved>2016-01-26T12:41:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-20T10:22:43Z" id="173161721">@martijnvg I looked through all the core changes - I think we are in good shape. The testing looks very very good thanks for all the work along those lines!
</comment><comment author="martijnvg" created="2016-01-20T15:53:09Z" id="173246581">@s1monw I've updated the PR and addressed or answered the comments.
</comment><comment author="s1monw" created="2016-01-26T12:28:18Z" id="174980758">I looked at this so often I think we need to get it in! It's in great shape and we can iterate on this once in master. @martijnvg  @javanna @talevy great job! 

here is the formal LGTM
</comment><comment author="sandstrom" created="2016-04-11T22:42:57Z" id="208596136">Just wanted to drop a note saying this is an awesome addition! :boat: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove lots of raw from aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16048</link><project id="" key="" /><description>These are the simple to remove raw type warnings from aggregations and a few
other nearby bits of code.
</description><key id="127147309">16048</key><summary>Remove lots of raw from aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label></labels><created>2016-01-18T03:30:36Z</created><updated>2016-01-21T19:14:44Z</updated><resolved>2016-01-18T14:57:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-18T03:43:16Z" id="172417543">@polyfractal, can you have a look at these?

The raw type warnings that remain in ValueSourceParser are a bit worrying - it looks like it'll spit back ValueSourceConfigs for ValueSource or ValuesSource.Bytes or ValuesSource.Numeric no matter what the intended VS type parameter was. I'm not at all familiar with the code but it just looks like a ClassCastException waiting to happen.
</comment><comment author="jpountz" created="2016-01-18T11:13:30Z" id="172500699">LGTM
</comment><comment author="polyfractal" created="2016-01-18T14:48:06Z" id="172548794">LGTM, but admittedly I only half understand how the whole ValueSource business works, so don't rely on my opinion much :)
</comment><comment author="nik9000" created="2016-01-18T14:57:17Z" id="172552037">I did what @jpountz asked - it was a very good idea.
</comment><comment author="colings86" created="2016-01-21T14:36:22Z" id="173586710">After speaking with @jpountz I am going to revert this commit on master since the aggregation refactoring changes most of these classes so much that these changes are no longer relevant (much less problems with raw type warnings) and merging the agg factoring with these changes is proving impossible. I will re-apply the changes which are still relevant here once the agg refactoring has been merged into master
</comment><comment author="jpountz" created="2016-01-21T14:37:06Z" id="173586900">+1
</comment><comment author="colings86" created="2016-01-21T15:31:44Z" id="173607085">Reverting a merge commit is apparently impossible without a world of pain so this commit stands and I am having to work out the horrendous merge conflicts on the agg factoring feature branch
</comment><comment author="clintongormley" created="2016-01-21T19:14:36Z" id="173678145">Reverted in 24ac9506bd719a17e5b37f74c980ba7090bd847a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove some raw warnings from settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16047</link><project id="" key="" /><description>Also sneaks in a couple of javadocs and one early return that I think makes
the method easier to read.
</description><key id="127143730">16047</key><summary>Remove some raw warnings from settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T02:44:28Z</created><updated>2016-01-18T15:19:02Z</updated><resolved>2016-01-18T15:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-18T11:19:21Z" id="172501589">LGTM
</comment><comment author="nik9000" created="2016-01-18T15:01:38Z" id="172553069">Thanks for reviewing these! I know they can be a lot.
</comment><comment author="jpountz" created="2016-01-18T15:19:02Z" id="172557278">No no, thank _you_. These warnings are annoying, not that much because of all this yellow in wy IDE, but rather because they mean we could accidentally do things that are not type safe. So I'm very happy that someone is taking these warnings seriously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Squash more raw type warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16046</link><project id="" key="" /><description /><key id="127142686">16046</key><summary>Squash more raw type warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-18T02:30:25Z</created><updated>2016-01-18T14:15:21Z</updated><resolved>2016-01-18T14:15:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-18T08:05:42Z" id="172457188">LGTM. Thanks @nik9000 !
</comment><comment author="nik9000" created="2016-01-18T14:15:17Z" id="172537525">Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup ParentFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16045</link><project id="" key="" /><description>The only purpose of the `ParentFieldMapper` is to make sure that the `has_child` query, `has_parent` query and `children` aggregation work. With this in mind the `ParentFieldMapper` should be cleaned up:
- Stop storing stored fields and indexed fields. The _parent field's only purpose is to support joins between parent and child type and only storing doc value fields is sufficient.
- In the mapping the parent field mapper is now known under '{parent}#{child}' key, because this is the field the parent/child join uses too.
- Added new sub fetch phase to lookup that _parent field from doc values field if that is requested. (before this was fetched from stored _parent field)
- Removed the ability to query directly on `_parent` in the query dsl. Instead the `{parent}#{child}` field should be used. Under the hood a doc values query is used instead of a term query, because only doc values fields are stored now.
- Also in aggregations `_parent` field can't be used any more and `{parent}#{child}` field name should be used instead to aggregate directly on the `_parent` join field.
- Remove remaining 1.x bwc logic.
</description><key id="127128562">16045</key><summary>Cleanup ParentFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2016-01-17T22:18:16Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-01-20T16:34:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-17T22:45:41Z" id="172392015">One problem I see that is pre-existing, but that we should fix, is that this `{parent}#{child}` field is essentially a subfield. It should really be returned as a subfield from the mapper, so that doing a PUT mapping will reject creating a mapper with the same name.
</comment><comment author="rjernst" created="2016-01-17T22:48:36Z" id="172392207">Just to be clear, by return that as a subfield I mean that it should have its own mapper, and that should be returned by overriding `iterator()` from `Mapper` in `ParentFieldMapper`.
</comment><comment author="martijnvg" created="2016-01-18T15:01:53Z" id="172553124">Thanks for checking this out @rjernst. I've updated this PR.
</comment><comment author="rjernst" created="2016-01-18T19:51:53Z" id="172633919">The change looks good, but the one thing that still bugs me is a user having to know the special syntax of a field we auto create, `{parent}#{child}`. Should this be a query that takes both parent and child so the user does not need to know how we build this field internally?
</comment><comment author="martijnvg" created="2016-01-18T22:39:11Z" id="172672943">@rjernst agreed would be great if just `_parent:{value}` can be used. The problem is that in the query dsl we can't know for sure what child type a user is referring to. There can several parent/child relations defined in an index. Currently the non lenient syntax in master/2.x is: `_parent:{parent_type}#{id}`. I can bring back the lenient syntax that just tries to find any child doc matching with a parent id value?
</comment><comment author="rjernst" created="2016-01-18T22:51:33Z" id="172674934">I did not mean to suggest adding back the previous behavior. I meant having a query in the dsl that does the joining of the values, so that a user doesn't need to know how the values are encoded. Eg something like:

```
{"query": {"parentquery":{"type": "footype", "id": "myparentid"}}}
```

Fill in "parentquery" with a name that actually makes sense, but I think that demonstrates the idea.
</comment><comment author="martijnvg" created="2016-01-19T08:14:32Z" id="172771638">+1 Adding such a query makes it easer to query child docs by parent id. 
</comment><comment author="martijnvg" created="2016-01-19T20:36:18Z" id="172978213">I've updated the PR and added a new `parent_id` query that selects the right field to be used.
</comment><comment author="rjernst" created="2016-01-19T21:35:52Z" id="172994668">Thanks @martijnvg! LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_upgrade doesn't upgrade index directories with no shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16044</link><project id="" key="" /><description>I have a 40 node cluster that I've been upgrading from 0.18 over the years currently running 1.7.3.  I've done the _upgrade a few times, and I'm trying to go to 2.1.1.  After restarting the full cluster certain nodes have the dreaded (although the index with the issue is different on each)

`java.lang.IllegalStateException: The index [files_v2] was created before v0.90.0 and wasn't upgraded. This index should be open using a version before 2.0.0 and upgraded using the upgrade API.`

However if I go look at the files_v2 directory on that host there are no shard directories only a _state directory which just has one file state-0.st, which was last changed over a month ago.  On the nodes that actually have files_v2 shards, there is no _state directory at the top level they are under the shard directories.

Are there any easy commands to go clean up these no shard indexes?
Maybe the start code should delete them or something?
</description><key id="127125771">16044</key><summary>_upgrade doesn't upgrade index directories with no shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2016-01-17T21:36:19Z</created><updated>2016-03-10T20:36:59Z</updated><resolved>2016-03-01T12:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-18T08:33:24Z" id="172461940">Having a 40 nodes cluster, I presume you use dedicated master nodes? if so, I suspect that the state-0.st files are on the master nodes while the data is stored on data. This is expected behaviour for 1.x (changing in 2.0). 

I also understand that you still have the cluster running under 1.7.3 ?  can you post the output of `GET _cluster/state/metadata/index/` ? I'm interested in the settings and state sections.

Also, do see a message in the data node logs starting with "Not updating settings for the index... because upgraded of some primary shards failed..." ?
</comment><comment author="awick" created="2016-01-18T16:44:59Z" id="172582908">I have a single master node, although it also has data on it (so it isn't dedicated.)

The master node was NOT the node with the issue in this case, although it had its own issue for other indexes.  However this cluster has been running "forever", and at one point (years ago) I didn't have a single master, was just using the default of all hosts can be a master, so it is very possible that it could have been a master in a previous time.

Reading between the lines can I just go delete _state from top level indices that are NOT the master node and that will fix everything?  (I did try renaming _state on a node, and that node could start up, so I'm guessing yes.)

```
  "files_v2" : {
    "state" : "open",
    "settings" : {
      "index" : {
        "number_of_shards" : "2",
        "version" : {
          "created" : "191299",
          "upgraded" : "1070399",
          "minimum_compatible" : "4.10.3"
        },
        "number_of_replicas" : "2",
        "auto_expand_replicas" : "0-2"
      }
    },
```
</comment><comment author="bleskes" created="2016-01-19T11:26:35Z" id="172822607">OK, I _think_ what you are saying is that this state files are from the days that a node used to be a master node but now it's a data node only (i.e., `node.master: false` is set in elasticsearch.yml). If that's the case then yes, that state file might be confusing the cluster and you can delete it. If you don't have `node.master: false` on those nodes, deleting the file is dangerous and we should dig further to see what's wrong.
</comment><comment author="awick" created="2016-01-19T12:40:19Z" id="172840202">I have no clue if that is where the state files are from, that is my guess, I was hoping you could confirm :)

I do have node.master: false on those nodes NOW, my point was in the past it was true.

I do think this is a bug though, either:
- _upgrade should have upgraded them everywhere, even if no shards
-  they should just be ignored since node.master: false is set
</comment><comment author="bleskes" created="2016-01-19T20:39:22Z" id="172978935">&gt; I do have node.master: false on those nodes NOW

double checking - did you have these when you run the `_upgrade` API? if so, you can just delete those index _state file on all data nodes.

&gt; _upgrade should have upgraded them everywhere, even if no shards

Agreed. The problem is that 1.x didn't write them on data nodes  and thus the _upgrade API didn't take it into account. With 2.x this has changed and we write them (and upgrade) on all nodes.
</comment><comment author="awick" created="2016-01-19T20:55:50Z" id="172984104">They must have been there before I did _upgrade, since the last time stamp is over a month ago.

Ok I'll just delete them manually.  Can I delete index level _state directories on non master nodes always, or only if that data node has no shards?
</comment><comment author="bleskes" created="2016-01-19T20:57:43Z" id="172984670">you can delete them on all nodes.  If it makes things simpler, just do so on nodes that fail to start. They will have no effect o.w.

Note that 2.x will write those files right back (and keep maintaining it correctly).

&gt; On 19 Jan 2016, at 21:56, Andy Wick notifications@github.com wrote:
&gt; 
&gt; They must have been there before I did _upgrade, since the last time stamp is over a month ago.
&gt; 
&gt; Ok I'll just delete them manually. Can I delete index level _state directories on non master nodes always, or only if that data node has no shards?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="bleskes" created="2016-03-01T12:55:59Z" id="190713262">Closing this as I presume we diagnosed it correctly and it's a non-issue. Please reopen if this turns out to be wrong.
</comment><comment author="awick" created="2016-03-01T13:34:37Z" id="190726149">Yes I was able to delete them everywhere and it worked.  I still think elasticsearch should have just ignored them or deleted them for me. 
</comment><comment author="awick" created="2016-03-09T14:48:45Z" id="194327539">ok, i'm having the similar issue with replicated shards.  This seems like a bug with the _upgrade, where maybe it doesn't check the version of all the replicates but only the master?
</comment><comment author="awick" created="2016-03-09T15:16:28Z" id="194339372">So I went thru all the nodes and delete the files that matched

find . -name "state-*" -ls | grep 201[234]

So unlike the original bug report, these shards had index data but were replicates and didn't get upgraded.
</comment><comment author="bleskes" created="2016-03-09T16:37:02Z" id="194385288">It's the same issue - we check the index level state files on node startup, regardless of whether the data folder has shards or not. Stale files from the node being a master node will cause this issue. 

**Do** note  (and everyone reading this in the future) that the command you posted can be very dangerous - if you're not careful it will also delete the shard level state files, rendering the data useless and causing data loss. 
</comment><comment author="awick" created="2016-03-09T17:14:02Z" id="194405300">I guess I'm confused why this isn't considered a bug in 1.7.3?  I can understand if there is no shard data and it was left over, but in this case there was shard data (it was just a replicate)

Yes definitely look at the output before removing, although the only thing I found that was 2 years old or later was these files, since in use state files are within the last 2 years. :)
</comment><comment author="bleskes" created="2016-03-10T20:26:50Z" id="195033094">I agree that this is not the best behevior. It was fixed in 2.0 (we update these files) but the fix was too involved to back port to 1.7.
</comment><comment author="awick" created="2016-03-10T20:34:04Z" id="195035254">Maybe just clear up the message about what exact file is causing the issue?  Fixing it in 2.0 doesn't help because I still would have gotten the error if I had got from 1.7.3 to 2.0 right?  I guess maybe there aren't many of us who have been upgrading for 4+ years now. :)
</comment><comment author="bleskes" created="2016-03-10T20:36:59Z" id="195036224">I agree it's rare - but remember the message can also be genuine. 

&gt; I still would have gotten the error if I had got from 1.7.3 to 2.0 right?

correct

&gt; I guess maybe there aren't many of us who have been upgrading for 4+ years now. :)

That does make you special.  In a very good way :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't replace found fields if map unmapped fields as string is enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16043</link><project id="" key="" /><description>PR for #10500
</description><key id="127124387">16043</key><summary>Don't replace found fields if map unmapped fields as string is enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-17T21:10:21Z</created><updated>2016-01-29T10:57:19Z</updated><resolved>2016-01-29T10:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-28T09:49:43Z" id="176091755">Good catch. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Move Jarhell check inside the test.security.manager guard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16042</link><project id="" key="" /><description>This change moves the JarHell check under the test.security.manager guard
such that users that have to opt out of JarHell for testing can do so by disabling
security. It will all users to use our test framework but when doing so the parameter
`-Dtests.security.manager=false` will signal how serious it is to opt out of this check

Relates to #11932
</description><key id="127115395">16042</key><summary>[TEST] Move Jarhell check inside the test.security.manager guard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>blocker</label><label>PITA</label><label>review</label><label>test</label><label>v2.3.0</label></labels><created>2016-01-17T18:52:25Z</created><updated>2016-03-13T13:42:54Z</updated><resolved>2016-03-13T13:42:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2016-01-17T18:53:45Z" id="172365144">:+1: 
</comment><comment author="rmuir" created="2016-01-17T19:14:27Z" id="172368432">I'm -1 because here is what will happen. you commit this change, then lusers with jar hell will realize that they want to load plugins in their tests too, and will want more leniency.

the excuse will be used, that this change is "precedence" and then everything goes in the wrong direction. better to just stop it right here and now.
</comment><comment author="s1monw" created="2016-01-17T19:54:12Z" id="172371645">&gt; I'm -1 because here is what will happen. you commit this change, then lusers with jar hell will realize that they want to load plugins in their tests too, and will want more leniency.

what does the user prevent from adding plugins here? I mean they can just do that without further checks and leniency nothing prevents them?
</comment><comment author="s1monw" created="2016-01-18T19:24:48Z" id="172628288">@kimchy @clintongormley I am going to close this again since there seems no room for compromises here. 
</comment><comment author="synhershko" created="2016-01-18T20:35:17Z" id="172647026">I'm confused - didn't you just state the complete opposite here and in #11932 ?
</comment><comment author="s1monw" created="2016-01-19T08:00:37Z" id="172769505">&gt; I'm confused - didn't you just state the complete opposite here and in #11932 ?

rob vetoed that change so we have to go back and re-iterate to see how we can improve the situation or find other compromises. Stay tuned @synhershko 
</comment><comment author="s1monw" created="2016-01-19T15:50:59Z" id="172894760">I had a longer conversation about this with @rmuir today and we have a couple action items for this.
- for 2.2 we can commit the patch as it is
- for 2.3 we will remove JAR hell entirely from the testing bootstrap and add an explicit check to the plugin pom.xml file that runs jar hell as part of the build process with the plugin classpath.
- for 3.0 we remove that check from the testing bootstrap since we already have the check we have to add for 2.3

@rmuir did I miss something?
</comment><comment author="rmuir" created="2016-01-19T16:44:19Z" id="172912787">@s1monw yes that is fine, the only thing i see missing is that long term we address the issue of "client". In general our test-framework is not setup for that, and it is crucial to separate from the server code for many other reasons. Until that happens, the road will be rocky for someone trying to use this test-framework to test client code. This is just one of the first checks they will hit.
</comment><comment author="s1monw" created="2016-01-19T16:47:10Z" id="172913735">&gt; @s1monw yes that is fine, the only thing i see missing is that long term we address the issue of "client". In general our test-framework is not setup for that, and it is crucial to separate from the server code for many other reasons. Until that happens, the road will be rocky for someone trying to use this test-framework to test client code. This is just one of the first checks they will hit.

agreed @rmuir I think ultimately we have to fix that with test-fixtures and a separate client
</comment><comment author="s1monw" created="2016-03-13T13:42:51Z" id="195959256">I ported #16174 to 2.x such that folks can disable in 2.3 as well - We have a good build system fix in master so we don't need to waste time calling ant from maven
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add handling of channel failures when starting a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16041</link><project id="" key="" /><description>This commit adds handling of channel failures when starting a shard to
o.e.c.a.s.ShardStateAction. This means that shard started requests
that timeout or occur when there is no master or the master leaves
after the request is sent will now be retried from here. The listener
for a shard state request will now only be notified upon successful
completion of the shard state request, or when a catastrophic
non-channel failure occurs.

This commit also refactors the handling of shard failure requests so
that the two shard state actions of shard failure and shard started
now share the same channel-retry and notification logic.

Closes #15895
</description><key id="127104208">16041</key><summary>Add handling of channel failures when starting a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-17T15:57:24Z</created><updated>2016-01-19T12:05:47Z</updated><resolved>2016-01-19T12:05:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-19T10:33:04Z" id="172811879">@bleskes This is rebased on the latest changes from #16057.
</comment><comment author="bleskes" created="2016-01-19T11:40:25Z" id="172825446">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>calculate ratio between aggregation buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16040</link><project id="" key="" /><description>Hi,
I'm using the following terms aggregations to get views and clicks of each campaign ( by campaign_id ) :

```
{
    "aggregations": {
        "campaigns": {
            "terms": {
                "field": "campaign_id",
                "size": 10,
                "order": {
                    "_term": "asc"
                }
            },
            "aggregations": {
                "actions": {
                    "terms": {
                        "field": "action",
                        "size": 10
                    }
                }
            }
        }
    }
}
```

This is the response I get:

```
{
    "aggregations": {
        "campaigns": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": "someId",
                    "doc_count": 12,
                    "actions": {
                        "doc_count_error_upper_bound": 0,
                        "sum_other_doc_count": 0,
                        "buckets": [
                            {
                                "key": "click",
                                "doc_count": 3
                            },
                            {
                                "key": "view",
                                "doc_count": 9
                            }
                        ]
                    }
                }
            ]
        }
    }
}
```

Example of a document:

```
{
    "_index": "action",
    "_type": "click",
    "_id": "AVI2XOTl8otXlszOjypT",
    "_score": 1,
    "_source": {
        "ip": "127.0.0.1",
        "timestamp": "2016-01-12T15:03:23.622743524Z",
        "action": "click",
        "campaign_id": "IypmiroC"
    }
}
```

I need to be able to retrieve the conversion rate of each campaign ( clicks / views ) , and I can't do it on the client side since I need to be able to sort by conversion rate. Is it possible on elasticsearch?

Any help would be much appreciated.
</description><key id="127100611">16040</key><summary>calculate ratio between aggregation buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">orrchen</reporter><labels /><created>2016-01-17T15:09:19Z</created><updated>2016-11-07T13:40:44Z</updated><resolved>2016-01-18T12:43:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T12:43:20Z" id="172518421">No, it's not possible to do this at the moment, with the current structure of your documents.  You can calculate the ratio using a pipeline aggregation as follows:

```
GET _search?size=0
{
  "aggregations": {
    "campaigns": {
      "terms": {
        "field": "campaign_id"
      },
      "aggregations": {
        "clicks": {
          "filter": {
            "term": {
              "action": "click"
            }
          }
        },
        "views": {
          "filter": {
            "term": {
              "action": "view"
            }
          }
        },
        "ratio": {
          "bucket_script": {
            "buckets_path": {
              "clicks": "clicks._count",
              "views": "views._count"
            },
            "script": "clicks/views"
          }
        }
      }
    }
  }
}
```

But you can't sort the terms agg on the result of `ratio`.  Possibly you'll be able to sort later on once https://github.com/elastic/elasticsearch/issues/14928 is implemented.

What you can do though is to create a summarised view on your indexed documents to group them together and index click and view rates within a single document for each campaign. Then what you're after will be easy to do.  

Have a look at this talk about entity centric indexing for more guidance: https://www.youtube.com/watch?v=yBf7oeJKH2Y
</comment><comment author="DeeeFOX" created="2016-11-07T13:40:44Z" id="258837839">This is not suitable to data which is ambiguous of how many types it does have util aggregation result show up to us. Just like in this example, if we don't know the exactly action name ("clicks" or "views"), we can't get the aggregation result of the &lt;each_type_count&gt;/&lt;total_count&gt;!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>:9200/_cat/nodes does not return</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16039</link><project id="" key="" /><description>I'm using ES 1.5.2. Time to time xx.xx.xx.xx:9200/_cat/nodes does not return anything. Hence Kibana or KOPF also not loading. But xx.xx.xx.xx:9200  always returns data. Restarting a node will fix the issue.  Please Note - I have two client nodes in different subnet and port 9200-9400 open both ways.
I couldn't find any errors in the ES log files.
</description><key id="127078320">16039</key><summary>:9200/_cat/nodes does not return</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lakal-malimage</reporter><labels /><created>2016-01-17T06:43:41Z</created><updated>2016-01-18T08:15:17Z</updated><resolved>2016-01-18T08:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lakal-malimage" created="2016-01-18T08:15:17Z" id="172458734">This seems OK after reducing TCP keepalive_time.
net.ipv4.tcp_keepalive_time
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Remove site plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16038</link><project id="" key="" /><description>Site plugins used to be used for things like kibana and marvel, but
there is no longer a need since kibana (and marvel as a kibana plugin)
uses node.js. This change removes site plugins, as well as the flag for
jvm plugins. Now all plugins are jvm plugins.
</description><key id="127076776">16038</key><summary>Plugins: Remove site plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-01-17T06:07:33Z</created><updated>2016-03-15T22:05:15Z</updated><resolved>2016-01-21T20:32:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-17T18:46:38Z" id="172363985">@rjernst this LGTM I guess plugins that used this behavior will be able to just register handlers and serve static content from there. I wonder if we should add an example somewhere how to do that? this must not block this issue in any case I would just want to show folks that used that how to proceed from here. WDYT? Don't get me wrong it's not on you to do that I just wanna get your opinion
</comment><comment author="rjernst" created="2016-01-17T19:09:12Z" id="172367209">@s1monw Is that what we really recommend? Site plugins were for using ES as a generic webserver...I'm not sure we should continue that? It seems like suggesting to build kibana plugins or serve the files with a real webserver is better?  Site plugins just served static files right?
</comment><comment author="s1monw" created="2016-01-17T20:21:44Z" id="172377036">&gt; @s1monw Is that what we really recommend? Site plugins were for using ES as a generic webserver...I'm not sure we should continue that? It seems like suggesting to build kibana plugins or serve the files with a real webserver is better? Site plugins just served static files right?

I guess that's a better alternative... lets see how we can communicate that best
</comment><comment author="clintongormley" created="2016-01-21T11:30:41Z" id="173543620">I've chatted to the Kibana guys about how easy it is to migrate a site plugin to Kibana and the answer is: pretty easy.  There is a Yeoman generator for Kibana plugins (https://github.com/elastic/generator-kibana-plugin) which generates all the boiler plate you need, and the rest is pretty much copy and paste.  There are one or two things to watch out for (eg using the same version of jQuery for instance), but the rest should be pretty easy.  We will do a blog post demonstrating how to migrate a site plugin to Kibana as a conversion guide for our community authors.

Once migrated, these kibana plugins can start taking advantage of some of the functionality available in Kibana, although I think most of that will probably come later - the public API is still in flux.

Site plugins have always been a bit of a hack, and have been the source of a few CVEs in Elasticsearch's history, so it'll be good to reduce the attack surface area here.  Also, the fact that Kibana is backed by a node.js server opens up more possibilities to plugin authors.
</comment><comment author="clintongormley" created="2016-01-22T10:54:33Z" id="173878003">@rjernst could we have a note in the breaking changes docs for 3.0 please
</comment><comment author="rjernst" created="2016-01-27T16:13:54Z" id="175717719">Added a note: 60180fe
</comment><comment author="jettro" created="2016-01-29T14:08:58Z" id="176772245">When migrating site plugins that do more than just a few queries the migration is more complicated. A best practise of interacting with the connected elasticsearch cluster would be cool. I see Kibana uses a proxy with very limited functionality. Sense does something else to connect to elastic. What is the path to chose for you migrated site plugin?
</comment><comment author="clintongormley" created="2016-02-02T12:23:37Z" id="178548878">@jettro Sense sends its requests via the Kibana backend.  One advantage to this is that you can have multiple Elasticsearch clusters setup in Kibana and just have the one Kibana instance to talk to them.  You don't then need to worry about configuring CORS settings on each cluster.  We will be following up this PR with a blog post demonstrating how to migrate, which will include using the Kibana backend as a proxy.
</comment><comment author="javanna" created="2016-02-02T13:24:16Z" id="178573301">++ on the blogpost and thanks @jettro for bringing this up ;)
</comment><comment author="rashidkpc" created="2016-03-15T21:45:42Z" id="197037983">I still think we should talk about deprecating this instead of removing it. 

The Kibana plugin API, like the Elasticsearch plugin API, is not stable. If we expect site plugin authors to regularly update we're going to be in trouble, kopf hasn't had a commit in 2 months, head hasn't had one in a month. I don't think this is a good idea. 
</comment><comment author="rashidkpc" created="2016-03-15T22:05:15Z" id="197046111">Also, we should probably pull the list of site plugins out of the docs: https://github.com/elastic/elasticsearch/search?utf8=%E2%9C%93&amp;q=kopf
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex should copy status from any search failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16037</link><project id="" key="" /><description>Currently it only copies from search failures. Oops.
</description><key id="127069200">16037</key><summary>Reindex should copy status from any search failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label></labels><created>2016-01-17T02:08:51Z</created><updated>2016-03-10T18:48:19Z</updated><resolved>2016-03-10T18:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Limit the accepted length of the _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16036</link><project id="" key="" /><description>Elasticsearch should reject ids that are this long, to ensure a document
always remains retrievable for clients that impose a maximum URI length

Closes #16034
</description><key id="127045047">16036</key><summary>Limit the accepted length of the _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-16T18:41:49Z</created><updated>2016-02-29T16:37:32Z</updated><resolved>2016-02-22T20:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-16T19:52:01Z" id="172249673">512 seems insanely long to me, but some limit is better than none...
</comment><comment author="jasontedor" created="2016-01-16T23:38:02Z" id="172273144">&gt; 512 seems insanely long to me, but some limit is better than none...

I agree. There are no _reasonable_ ID sources that come anywhere near 512 bytes in length (using SHA-512 hashes would only be 64 bytes, or 128 bytes if formatted as a hexadecimal string).
</comment><comment author="clintongormley" created="2016-01-18T10:58:45Z" id="172497467">For sure there are users who are indexing URLs (including lots of query string params) which could well be over 512 bytes.  What are we doing for bwc here, so that docs in old indices with too-long IDs are still accessible?
</comment><comment author="dakrone" created="2016-01-18T16:27:21Z" id="172577095">Pushed a new commit that changes the limit to bytes instead of unicode code units.

&gt; What are we doing for bwc here, so that docs in old indices with too-long IDs are still accessible?

Absolutely nothing, those documents will continue to remain accessible, this _only_ affects indexing requests
</comment><comment author="rjernst" created="2016-01-18T19:22:34Z" id="172627825">New tests look good to me. I'm not sure what the rest tests buys us (what integration is it testing?), but that's just a side opinion that we should limit integration tests to testing integration, rather than re-testing every part of the api.
</comment><comment author="dakrone" created="2016-01-18T20:54:46Z" id="172650881">&gt; I'm not sure what the rest tests buys us (what integration is it testing?)

It buys us a check that this doesn't break in the future for the HTTP API exclusively while only being tested from the Java API.

Pushed a commit for adjusting the wording of the error message.
</comment><comment author="davidvgalbraith" created="2016-01-18T23:41:17Z" id="172683050">I don't understand why this is necessary or helpful. Won't the `_search` endpoint retrieve documents irrespective of their `_id` length? I for one often index documents with long `_id` fields. At the very least it should be a configurable setting instead of an arbitrary hardcoded 512.
</comment><comment author="rjernst" created="2016-01-18T23:50:43Z" id="172685617">If we make it configurable, then the default should be more reasonable like 64 bytes as @jasontedor mentioned.
</comment><comment author="dakrone" created="2016-01-18T23:59:41Z" id="172688145">@rjernst 64 bytes is only 16 4-byte characters. I think that limit is too low. If we really think this should be configurable I will make it configurable.

@davidvgalbraith yes, the documents will still be accessible for search, they would only not be able to use the realtime GET API.
</comment><comment author="davidvgalbraith" created="2016-01-19T00:06:07Z" id="172689092">Right! So I think it's overblown for https://github.com/elastic/elasticsearch/issues/16034 to claim "they will not be able to retrieve the document via the HTTP API without resorting to something like the ids query", since the search API can retrieve long-ID documents just fine. So it doesn't seem like long-ID documents should be rejected, even though they may not be queryable by the GET API.
</comment><comment author="dakrone" created="2016-01-19T00:07:33Z" id="172689253">@davidvgalbraith I updated the issue to change the wording to "...will not be able to retrieve the document via id (the get-document API) using the HTTP API"
</comment><comment author="davidvgalbraith" created="2016-01-19T00:09:01Z" id="172689438">Thanks! Do you still feel it's worth rejecting documents over?
</comment><comment author="dakrone" created="2016-01-19T01:10:00Z" id="172696642">&gt; Do you still feel it's worth rejecting documents over?

Absolutely, this was prompted by a customer that was indexing documents using the Java API (with no such length limit) and then were unable to retrieve them with their client application, causing a production outage.
</comment><comment author="davidvgalbraith" created="2016-01-19T01:14:40Z" id="172697166">How does this help a situation like that? Now they can't even store the documents they wanted to retrieve. They should use a client without a URI length limit to retrieve long-ID documents by ID.
</comment><comment author="dakrone" created="2016-01-19T01:21:48Z" id="172697977">&gt; How does this help a situation like that? Now they can't even store the documents they wanted to retrieve. They should use a client without a URI length limit to retrieve long-ID documents by ID.

This helps by preventing indexing a document like that in the first case. It should be clear that indexing a document with an `_id` that is outrageously long is a Bad Idea. Not only is it easily worked around (I mean, 512 bytes is still a loooong id! someone can always SHA256 their `_id` and use that for the id if it's not long enough), but it causes other problems.

For instance, if someone indexes a 100mb _id, it means a 100mb ordinal in the parent/child id cache. If they index a term over 32k bytes which is _Lucene's_ maximum term size then they run into problems just the same. A reasonable default limit is the best solution for this.
</comment><comment author="davidvgalbraith" created="2016-01-19T01:25:05Z" id="172698379">Well I disagree but I have no power here. Just pretty please make it configurable, my application is a timeseries database where the `_id`s are row keys, which can run into several kilobytes and I can't just hash them as I use the row key / `_id` during queries.
</comment><comment author="bleskes" created="2016-01-19T09:50:53Z" id="172795543">+1 to limit on 512b for starts and making it configurable. With these kind of things it's always good to start high, get feedback and harden based on it. We followed the same path in many of these limits.
</comment><comment author="clintongormley" created="2016-01-20T12:24:04Z" id="173188465">&gt; Absolutely, this was prompted by a customer that was indexing documents using the Java API (with no such length limit) and then were unable to retrieve them with their client application, causing a production outage.

The could always use the  `mget` API instead, which doesn't have the same restrictions.

As long as it is a dynamic setting, then I'm OK with this - I think it falls into the "safeguards" category https://github.com/elastic/elasticsearch/issues/11511
</comment><comment author="dakrone" created="2016-02-21T19:55:13Z" id="186897826">@clintongormley and @bleskes: I looked at making this configurable, and
dynamically configurable at that. The problem with this is that the `Settings`
class is not accessible within the `IndexRequest` action, which is where the
validation _should_ take place.

I looked at adding this into the `TransportIndexAction`, which is the next level
where it could have access to the `Settings` object, the issue there is that it
does not fall into the codepath that the `_bulk` API goes through, so it will
have to be added in multiple places (which is against the whole notion of the
validation framework for requests). I also spoke with Simon about this, and he
suggested a system property, however, this also seems like a messy solution to
making it configurable (see the whole debacle with system properties and ES
daemonization that Jason is working on).

So with that, I would really like to keep this as a static, breaking,
non-dynamic change limiting the size. This lets the code remain very small, and
(rightly!) limits dangerous behavior (since these _ids can potentially be
ordinals in an in-memory map, they should have a reasonable limit). If making it
non-dynamic means we need to increase it to `1024b` then that's fine, but I
think it would be a step toward more messiness to try to add it as a dynamic or
non-dynamic setting.
</comment><comment author="davidvgalbraith" created="2016-02-21T20:12:18Z" id="186904037">Why aren't the settings accessible within the index request action? Let's make them accessible. 
</comment><comment author="dakrone" created="2016-02-21T20:24:38Z" id="186906766">&gt; Why aren't the settings accessible within the index request action?
&gt; Let's make them accessible.

The point of the &lt;NNN&gt;Request.java classes are that they are holders for
the requests themselves, they don't and shouldn't know about any ES
internals such as the Settings object.
</comment><comment author="davidvgalbraith" created="2016-02-21T20:28:47Z" id="186908464">Ok, then let's just make this setting accessible to the index request object. There's something wrong with the engineering if it's completely impossible to make this configurable. 
</comment><comment author="bleskes" created="2016-02-21T21:03:46Z" id="186916101">&gt; @clintongormley and @bleskes: I looked at making this configurable, and
&gt; dynamically configurable at that. The problem with this is that the Settings
&gt; class is not accessible within the IndexRequest action, which is where the
&gt; validation should take place.

Thanks Lee, for looking at our options. While I would have gone with request validation myself as a first place to consider these things, I think there are other ways to think about this as well. There is nothing inherit to the indexing request that puts a limit on our ids (compared to a negative ttl which is an invalid value). How would you feel if we implement this in a similar way we did for SearchHits - #13188  and let the IndexShard reject it?
</comment><comment author="s1monw" created="2016-02-22T00:13:52Z" id="186949115">&gt; Why aren't the settings accessible within the index request action? Let's make them accessible.

that is not how this abstraction should work. You are having an exceptional use-case and we have to figure out if we can support it by adding a simple way to opt out of this check or if we move to a sys property solution since your use-case is exceptional?!

There is a challenge that is common in distributed systems that not all data required to make the right decision is available on the node processing the request. We have to make decisions that are sustainable and adding such a dependency is not something we should do without thinking about the implications form a maintainability perspective. 

That said, I wanna understand your usecase more so please bare with me:

&gt; Well I disagree but I have no power here. Just pretty please make it configurable, my application is a timeseries database where the _ids are row keys, which can run into several kilobytes and I can't just hash them as I use the row key / _id during queries.

why can't you have a hash of that row as they IDs and use another field to store the full row key to use for search? You can also use custom routing based on that field to make sure you can identify they and control where they go shard wise?
</comment><comment author="davidvgalbraith" created="2016-02-22T01:49:13Z" id="186967482">What collision-free hash function can I use? If there's a hash collision I'll be unable to store the colliding document. So the "you can just hash it" argument doesn't make any sense in general, not just in my case. 
</comment><comment author="davidvgalbraith" created="2016-02-22T02:21:12Z" id="186972117">And how do you know my use case is exceptional? Did you survey every user of custom _id fields and ascertain their length distributions? I highly doubt I'm the only one in the world who'll be stymied by this change!
</comment><comment author="jasontedor" created="2016-02-22T02:52:45Z" id="186977332">&gt; What collision-free hash function can I use? If there's a hash collision I'll be unable to store the colliding document. So the "you can just hash it" argument doesn't make any sense in general, not just in my case.

@davidvgalbraith A collision-free hash function is impossible for your case; it's a simple pigeonhole argument as you have more input keys than output hashes. But with any of the cryptographic hash functions with a large number of bits you will have _practically_ no chance of collision. For example, with a 128-bit cryptographic hash function and 2^32 documents, you have a probability of collision on the order of 10^-20 (it's a variant of the birthday problem). And it's even more incomprehensibly small with a 160-bit hash (like SHA-1 that git uses) and beyond. And you can reduce the probability even more by appending a few bytes from the key to the hash. Collisions are just not something to be worrying about here.
</comment><comment author="mikemccand" created="2016-02-22T10:26:31Z" id="187112974">+1 for a hard massive (512!) limit on ID length.

Ridiculously long IDs can result in very poor indexing performance as well, as Lucene must try to locate the prior ID and do long comparisons on each possible candidate it tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>docs: Bulk API Reference lacking response format and other details</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16035</link><project id="" key="" /><description>Correctly processing the _response_ to a Bulk API call is incredibly important to ensuring data integrity, however the Bulk API reference docs [2.x](https://www.elastic.co/guide/en/elasticsearch/reference/2.x/docs-bulk.html) only state:

&gt; The response to a bulk action is a large JSON structure with the individual results of each action that was performed.

The only description of that "large JSON structure" is in an example curl command at the top of the page which only shows what a single action's response might look like:

``` json
{"took":7,"items":[{"create":{"_index":"test","_type":"type1","_id":"1","_version":1}}]}
```

There is a ton to infer from that snippet:
- `items` is an array of the same length and order as the sent actions+documents?
- The response to `index` actions will indicate how they were applied? So for a new document the response type will be `create` but for an existing doc it would be...?
- What indicates an error?!
## Other oddity: non-HTTP API?

Same page and small, so I don't think it's worth a separate issue but:

&gt; If using the HTTP API...

This is the only hint on the entire page that there's a non-HTTP bulk API. Pretty sure this phrase should just be removed?

_Update:_ Aha! https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking_20_removed_features.html#_bulk_udp
## Final nit pick: Write consistency means...?

The write consistency docs only say enough shards must be _active_ to satisfy write consistency concerns.

In other distributed systems I associate write consistency with how many nodes must _commit_ a write before the write is considered successfully applied.

Does write consistency depend solely on cluster state (the mere availability of active shards), or does it actually block on the writes to remote hosts?

(Also with replicas=1 quorum should really mean the primary+replica both must be available, not just one of them... `one` and `quroum` should not be the same, but thanks for at least documenting that they are!)
</description><key id="127042600">16035</key><summary>docs: Bulk API Reference lacking response format and other details</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">schmichael</reporter><labels><label>:Bulk</label><label>docs</label></labels><created>2016-01-16T18:07:59Z</created><updated>2017-01-27T01:02:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="schmichael" created="2016-01-16T18:13:46Z" id="172240351">Just stumbled across [this getting started doc](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/_batch_processing.html) which is a less detailed version of the reference doc except one nice additional detail:

&gt; When the bulk API returns, it will provide a status for each action (in the same order it was sent in)

The reference doc lacks the ordering part.
</comment><comment author="schmichael" created="2016-01-16T18:19:08Z" id="172241374">Aha! Found the [bulk API section of the Guide](https://www.elastic.co/guide/en/elasticsearch/guide/master/bulk.html) which includes more details for the request and response formats, but lacks a lot of other details. It also has a big Out of Date warning at the top.

If nothing else the guide should link to the reference and vice versa. This would at least make it easy for readers to find all of the details spread across these two pages.
</comment><comment author="ahuff44" created="2017-01-27T01:02:03Z" id="275562129">+1; I just spent an hour debugging before I realized that `client.bulk` was never calling my error callback and was instead returning error info in the response</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch should reject _id longer than the maximum URI length</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16034</link><project id="" key="" /><description>If a user indexes a document with an `_id` value longer than the maximum allowed length of an HTTP URI (for instance, with the java api), they will not be able to retrieve the document via id (the get-document API) using the HTTP API without resorting to something like the `ids` query.

Elasticsearch should reject ids that are this long, to ensure a document always remains retrievable
</description><key id="127042177">16034</key><summary>Elasticsearch should reject _id longer than the maximum URI length</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-16T18:00:58Z</created><updated>2017-02-26T13:57:14Z</updated><resolved>2016-02-22T20:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-16T18:03:05Z" id="172238412">The RFC does not set a limit on the URL length, however, many clients and browsers _do_ set a limit, so we should limit ourselves as well.
</comment><comment author="doried-a-a" created="2017-02-24T20:09:09Z" id="282391835">Please cancel this annoying restriction! </comment><comment author="jasontedor" created="2017-02-24T20:25:04Z" id="282395439">&gt; Please cancel this annoying restriction!

It's far more productive to upfront say why this is change is restricting you. Perhaps there is a use case that we have not consisered. Perhaps you're doing something that would be done more effectively without using excessively long IDs. We want to help you but we can not make that assessment from what you've posted.</comment><comment author="doried-a-a" created="2017-02-25T10:48:42Z" id="282476200">Thanks for reply!

We were using elasticsearch 1.7 to index crawled web pages, where long IDs where allowed. But now I'm migrating to 5.2
Crawlers use the page url as the id. You know, some web pages put alot of unnecessary details in the url, like the title of the page, or even some of the content !
I know it's not effective to use url as an ID, and maybe better to use a hash of it for example.
But it needs a lot of modifications

Thanks for your kindness!</comment><comment author="clintongormley" created="2017-02-25T12:41:54Z" id="282481606">@doried-a-a As you can see from the description above, we didn't make this change just because we felt like it.  There is a genuine problem that is being solved.  

yes, it means you will need to make changes when moving to 5.2, but then you will have to reindex your data in order to move to 5.2 from 1.7 anyway.  This seems like the ideal time to make the change.</comment><comment author="jasontedor" created="2017-02-25T14:45:26Z" id="282488309">&gt; I know it's not effective to use url as an ID, and maybe better to use a hash of it for example.

Yes. If needed, you can store the URI as a field in the document.

</comment><comment author="doried-a-a" created="2017-02-26T13:57:14Z" id="282557618">I'll reindex anyway, right. Now I'm going to make changes in the crawlers too.
No problem.
Thanks for support!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the Task object available to the action caller</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16033</link><project id="" key="" /><description>Sometimes action callers might be interested in having access to the task that they have just initiated. This changes allows a caller to get access to the Task object of an action that it just started if the action runs on the same node and supports the task management.

@nik9000 do you want to review it?
</description><key id="126996378">16033</key><summary>Make the Task object available to the action caller</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>feature</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-16T03:05:15Z</created><updated>2016-01-19T14:42:36Z</updated><resolved>2016-01-19T03:13:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-16T14:36:20Z" id="172212644">Makes sense to me. I hadn't thought about it not being available to the transport client but it makes sense and should work fine for my purpose. I'd copy that javadoc about it returning null to ActionRequestBuilder as well.
</comment><comment author="imotov" created="2016-01-16T16:21:05Z" id="172220574">@nik9000 I thought more about it. Do we actually need to expose it all the way up to the client level for your use case? In the scenario where the task is actually available in this PR (the caller is calling execute on the node client) the caller typically has direct access to `Transport*Action`. So, it can directly call `TransportAction#execute` instead of going through the client. I feel like that would be a cleaner approach anyway.

If there is a real need to expose the task information all the way to the client level then there should be no difference between a node client and a transport client and we need to devise some method to ship the started task id back to the caller, which is much more complicated. Since the task might be executed on a remote node, we cannot return the `Task` object. It has to be some sort of `TaskInfo` object. It also has to be returned in an asynchronous way, because network communication might be involved.

So, if `TransportAction` level access would work for you, I am going to revise this PR to remove exposing task on the client level. If you need client level access, we should have a quick discussion about it earlier next week, since there are several ways of achieving it. 

What do you think?
</comment><comment author="nik9000" created="2016-01-17T01:53:52Z" id="172284138">Right. I see your way of thinking and I think I agree. If it can't work for TransportClient I don't think it should be part of the interface.

While I'd _love_ for clients to have access to task information - TaskInfo would be wonderful, but I had a quick look and I don't see any kind of "I've started your request" message that we could piggy back on so I don't think its worth it doing.

TransportAction level access will work I think. It'll make something like https://github.com/elastic/elasticsearch/pull/15687 more complicated but that is life.
</comment><comment author="nik9000" created="2016-01-17T01:54:45Z" id="172284173">So if you amend the PR to drop the Client level stuff and just do TransportAction level stuff you still have my LGTM.
</comment><comment author="nik9000" created="2016-01-17T01:56:19Z" id="172284214">Even better, it looks like without Client level changes this PR is about 2 lines.
</comment><comment author="clintongormley" created="2016-01-18T11:02:10Z" id="172498088">Not sure I follow the conversation above, but I'd imagine doing a reindex request with `?wait_for_completion=false` and getting back a task ID which I can then monitor.
</comment><comment author="nik9000" created="2016-01-18T14:13:40Z" id="172536807">&gt; Not sure I follow the conversation above, but I'd imagine doing a reindex request with ?wait_for_completion=false and getting back a task ID which I can then monitor.

Yup. That is what it'll do regardless of what we do above. This is more a question of "where do tasks show up in the API" and "what do rest tasks have to do to return the task ID rather than wait for completion" and "does the java client get to use them easily." The answers are "not in the client API. not yet anyway" and "get one or two more dependencies injected than normal and use them in obvious ways" and "no".
</comment><comment author="imotov" created="2016-01-18T20:39:58Z" id="172647852">@nik9000 I pushed the new version. I think I like it much more :)
</comment><comment author="nik9000" created="2016-01-18T21:38:52Z" id="172661103">LGTM. Its much simpler. If we want to give the clients some kind of task object on every request we'll do it as a separate PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting invalid codec causes shard to keep failing and stuck in initializing state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16032</link><project id="" key="" /><description>On ES 2.1.1.  Ran into this when trying to verify if a setting has persisted.  In this case, I am purposely setting the codec to an invalid codec name `best_compression1` instead of `best_compression`:

```
POST /largeevent/_close
PUT /largeevent/_settings
{
  "index.codec":"best_compression1"
}
POST /largeevent/_open
GET /largeevent/_settings
```

As soon as, I reopen the index after making the setting change, the shard gets into a failure loop trying to initialize the shard and will not start up.

```
[2016-01-15 18:12:18,597][WARN ][cluster.action.shard     ] [node-1] [largeevent][1] received shard failed for [largeevent][1], node[O373snrkTTGclRFL0t7H5g], [P], v[1553], s[INITIALIZING], a[id=w3VoZACfQKq_QcsDGLaZ3Q], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-01-16T02:12:18.563Z], details[failed recovery, failure IndexShardRecoveryException[failed recovery]; nested: IllegalArgumentException[failed to find codec [best_compression1]]; ]], indexUUID [JEPoX4QkR_Om-W69VKV6iQ], message [failed recovery], failure [IndexShardRecoveryException[failed recovery]; nested: IllegalArgumentException[failed to find codec [best_compression1]]; ]
[largeevent][[largeevent][1]] IndexShardRecoveryException[failed recovery]; nested: IllegalArgumentException[failed to find codec [best_compression1]];
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:179)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: failed to find codec [best_compression1]
    at org.elasticsearch.index.codec.CodecService.codec(CodecService.java:94)
    at org.elasticsearch.index.engine.EngineConfig.getCodec(EngineConfig.java:255)
    at org.elasticsearch.index.engine.InternalEngine.createWriter(InternalEngine.java:1057)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:150)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    ... 3 more
```

Will be nice if we can handle this better.
</description><key id="126994366">16032</key><summary>Setting invalid codec causes shard to keep failing and stuck in initializing state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>bug</label></labels><created>2016-01-16T02:18:43Z</created><updated>2016-01-18T13:01:18Z</updated><resolved>2016-01-18T13:01:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T13:01:17Z" id="172521693">With the changes coming in #16054 you will be able to close the index, update the settings, and reopen the index.  The continuous failures can only be prevented once we remove Guice. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Usage-based caching of filters can be ineffective without tuning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16031</link><project id="" key="" /><description>For some use cases, the default settings of `UsageTrackingFilterCachingPolicy` are inappropriate.  It would be ideal if those settings were configurable.

For example, the default configuration includes a history that looks at the last 256 filters.  In the extreme case, if you had a stream of fairly complicated filters where the total number of distinct filters was significantly greater than 256, then you'd never get caching of any filters!

In a real world example, we just went through an upgrade from ES 1.7 to 2.1.1 and experienced pretty severe degradation of response times.  Analysis of the query_cache indicated that a) the cache was relatively sparsely populated; b) the miss ratio was very high.  Our particular use case was not well handled b/c we filter documents using arbitrary boolean combinations of ~1k terms.  As best as I could tell, the limited 256 history size meant that we were only caching the very most common terms.  Inefficient caching combined with a fairly large document count led to increased response times across the board and very painful response times under heavier load. 

Tellingly, what rescued performance was completely bypassing the usage-based cache via the undocumented `index.queries.cache.everything: true` setting and relying purely on the LRU-nature of the cache to keep the most useful filters around.

Tuning the usage policy would be great.  And, arguably, documented support of a policy that simply relies on using LRU to determine the useful filters would also be a good idea.  (It did not escape my attention that the `index.queries.cache.everything` setting appears to have only been added for testing purposes!)
</description><key id="126994326">16031</key><summary>Usage-based caching of filters can be ineffective without tuning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dbaggott</reporter><labels><label>:Cache</label><label>:Search</label></labels><created>2016-01-16T02:17:54Z</created><updated>2016-04-12T08:21:33Z</updated><resolved>2016-04-12T08:21:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jrots" created="2016-01-16T09:25:20Z" id="172176566">+1, been experiencing similar issues and been looking for this setting too for a while!
</comment><comment author="dbaggott" created="2016-01-16T15:51:42Z" id="172217878">Related question: I know the usage policy (including the underlying buffer that stores the history) is scoped to the shard but isn't the actual _usage accounting_ done at the segment level?  _IF that's true_, the mismatch adds a bit of complexity/variability as the runtime behavior will vary with the count of (large enough) segments -- in effect, segment count will shorten the _effective_ history size.  And that, in turn, alters the significance of the thresholds.  So, as segments come and go, the caching behavior will subtly (or not so subtly) change.
</comment><comment author="synhershko" created="2016-01-16T23:42:56Z" id="172274247">We are also experiencing a very similar issue. In our case it's not many different filters that overwhelm the LRU cache, but a single query with what could be considered a very heavy Terms Query in a filter context (thousands of terms on one field; think permissions filtering scenario), that simply refuse to get cached. Setting the aforementioned configuration option on the nodes cause it to be cached, dropping query latency from 15 seconds to less than 1 second.

@dbaggott to be fair, historically the query cache will represent the cache of the query results, not the filters being used. This seems to still be the case: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-cache.html. Pre 2.x there was a filter cache, which was either removed (since it's not managed by ES anymore apparently, but by Lucene) or hidden (since the user has no control over it anymore). Be that as it may, I'm pretty sure the current visibility on _filter_ caching behavior is zero.

When Elasticsearch moved to automatic decision making on filter caching following Lucene's path I suspected such errors will arise - no automatic decision process is perfect. An all-or-nothing approach like the OP and us are having is a bit disastrous. Best solution would be to bring back at least some user control over filter-caching, a la [cache key](https://github.com/elastic/elasticsearch/issues/1142) which is probably the most powerful feature ES's query DSL had (@bleskes, we discussed this shortly not long ago..).

I'm hoping ES / Lucene folks will realise some level of control over caches should be given to users - for error scenarios or for advanced usages.
</comment><comment author="dbaggott" created="2016-01-17T01:09:49Z" id="172281778">@synhershko, the documentation and the terminology are all a little confusing given that, as you alluded to, [queries and filters merged](https://www.elastic.co/blog/better-query-execution-coming-elasticsearch-2-0) but the "query cache" I'm referring to and the one in the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/query-cache.html) you mentioned is for *filters&#8226;.  Note the following (my emphasis):

&gt; The query cache _only_ caches queries which are being used in a _filter context_.

This cache is not to be confused with the [shard request cache](https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-request-cache.html).

As for visibility into the cache where filters are stored, there is good visibility into node-level stats via the node stats api: `curl -XGET 'http://localhost:9200/_nodes/stats?pretty'` (look for the "query_cache" entry).  It include count, hits, misses, evictions, etc.  You cannot interrogate the actual contents of the cache (only the hash of the filter and the doc id set are stored).

I'm guessing your problem is _exactly the problem of the history being too short_ for your use case.  A query in a filter context would be cached according to the same usage policy I'm referring to and each of your multiple terms is probably taking up a slot in the history buffer -- so the history never sees the same term more than once and caching is effectively disabled under the default policy.
</comment><comment author="jpountz" created="2016-01-18T09:55:27Z" id="172482361">I am all for improving how query caching works, but first I would like to make sure that we are not jumping too quickly to the conclusion. If you have identified requests that are much slower in 2.1 than in 1.7, would you be able to share them and capture hot threads while they are running (several times if possible) so that we can get an idea of what the bottleneck is?
</comment><comment author="jpountz" created="2016-01-18T09:58:57Z" id="172483133">&gt;  I know the usage policy (including the underlying buffer that stores the history) is scoped to the shard but isn't the actual usage accounting done at the segment level?

No, actual usage accounting is done at the shard level too.
</comment><comment author="dbaggott" created="2016-01-18T16:28:47Z" id="172577419">@jpountz, I'll gather documentation in support of the conclusion.

Is the conceptual problem clear from from my description though?  It's probably also easy to write a unit test against the default UsageTrackingFilterCachingPolicy that illustrates the problem case and then any query with a sufficiently complex filter context is going to trigger the problem.

Also, just to make sure I understand:

&gt;  No, actual usage accounting is done at the shard level too.

If there is a shard with 3 segments that are large enough to satisfy the segment policy, how many times will `UsageTrackingFilterCachingPolicy.shouldCache` be called?  You're saying only once, right?
</comment><comment author="jpountz" created="2016-01-18T17:47:23Z" id="172603692">&gt; Is the conceptual problem clear from from my description though? It's probably also easy to write a unit test against the default UsageTrackingFilterCachingPolicy that illustrates the problem case and then any query with a sufficiently complex filter context is going to trigger the problem.

Yes it is clear to me. If you have queries that involve hundreds of filters, then nothing will be cached. But there has to be a limit anyway so you will always be able to construct queries that always bypass the cache. 

I am not denying the fact that the query cache might be the problem but I would like to make sure it is in your case. There are so many things that happen at search time, I have been surprised many times how the actual cause of  a slowdown was very different from my initial expectations in spite of the fact that understanding slow queries is something that I do all the time.

The problem with this cache is that it does not behave like a regular cache, that can only make things faster. On the contrary, many queries leverage on-disk skip lists in order to only read the information that they are interested in. By caching them, we are forced to read every single matching document so that we can build a cached entry. So the worst thing that this cache could do would be to cache matching docs and then never reuse it. Unfortunately this happened a lot in 1.x, hence the pickyness of the new cache to get evidence of reuse before caching a filter.

&gt; If there is a shard with 3 segments that are large enough to satisfy the segment policy, how many times will UsageTrackingFilterCachingPolicy.shouldCache be called? You're saying only once, right?

`shouldCache` will be called 3 times, but `onCache` which increases the counters will be called only once.
</comment><comment author="dbaggott" created="2016-01-19T19:55:08Z" id="172967170">Thanks for the explanations (and, w/r/t the shard-level vs segment-level question, I'm assuming you meant `onUse` when you said `onCache`).

Here's some additional information surrounding what we're seeing.

In a cluster running 2.1.1, we configured it so all but one node had the `index.queries.cache.everything: true` setting.  We then ran a realistic query mixture against the cluster in the ballpark of 100 requests per second for about 10 minutes. 

The one node _without_ the "cache everything" behavior showed dramatically higher cpu for the entire duration as seen below (the drop off at the end is when load stopped)

![data node cpu](https://cloud.githubusercontent.com/assets/112677/12427920/b0044434-be96-11e5-96bd-93006ac470b1.png)

Here are some hot threads for the one node _without_ cache everything behavior:

```
::: {data-node-without-cache-everything}{xMs5UBKwSq6DdNsoVSzBKw}{ip}{ip:9300}{max_local_storage_nodes=1, aws_availability_zone=us-west-2c, master=false}
   Hot threads at 2016-01-18T20:21:52.375Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   81.1% (405.7ms out of 500ms) cpu usage by thread 'elasticsearch[data-node-without-cache-everything][search][T#6]'
     3/10 snapshots sharing following 40 elements
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:286)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:160)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$CustomBoostFactorWeight.scorer(FunctionScoreQuery.java:132)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     7/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   76.9% (384.6ms out of 500ms) cpu usage by thread 'elasticsearch[data-node-without-cache-everything][search][T#23]'
     4/10 snapshots sharing following 40 elements
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:286)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:160)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$CustomBoostFactorWeight.scorer(FunctionScoreQuery.java:132)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 36 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:297)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.getLeafCollector(FilterAggregator.java:61)
       org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:132)
       org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:131)
       org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:38)
       org.apache.lucene.search.MultiCollector.getLeafCollector(MultiCollector.java:117)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:763)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 36 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:286)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.getLeafCollector(FilterAggregator.java:61)
       org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:132)
       org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:131)
       org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:38)
       org.apache.lucene.search.MultiCollector.getLeafCollector(MultiCollector.java:117)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:763)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 45 elements
       sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
       sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)
       sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)
       sun.nio.ch.IOUtil.read(IOUtil.java:197)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:741)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:297)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:160)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$CustomBoostFactorWeight.scorer(FunctionScoreQuery.java:132)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   72.2% (360.7ms out of 500ms) cpu usage by thread 'elasticsearch[data-node-without-cache-everything][search][T#11]'
     2/10 snapshots sharing following 43 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:297)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:160)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$CustomBoostFactorWeight.scorer(FunctionScoreQuery.java:132)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     6/10 snapshots sharing following 43 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.lucene50.ForUtil.readBlock(ForUtil.java:197)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.refillDocs(Lucene50PostingsReader.java:368)
       org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.nextDoc(Lucene50PostingsReader.java:393)
       org.apache.lucene.util.BitSet.or(BitSet.java:94)
       org.apache.lucene.util.FixedBitSet.or(FixedBitSet.java:271)
       org.apache.lucene.util.DocIdSetBuilder.add(DocIdSetBuilder.java:87)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:286)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:160)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$CustomBoostFactorWeight.scorer(FunctionScoreQuery.java:132)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 28 elements
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.join.ToParentBlockJoinQuery$BlockJoinWeight.scorer(ToParentBlockJoinQuery.java:160)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$CustomBoostFactorWeight.scorer(FunctionScoreQuery.java:132)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
```

Here's the general style of the filter portion of the queries that struggle without the cache everything behavior.  There are further variations on this but I think this captures the core problem:

```
"filter": {
  "bool": {
    "must": [
      {
        "nested": {
          "query": {
            "terms": {
              "items.fieldA": [ 1, 2, 3 ]
            }
          },
          "path": "items"
        }
      },
      {
        "bool": {
          "must_not": {
            "multi_match": {
              "query": "multiple terms that stay constant across most queries",
              "fields": [ "field1", "field2" ]
            }
          }
        }
      }
    ]
  }
}
```

It's the "1, 2, 3" portion of the nested "items.fieldA" that varies.  The actual values are arbitrary and can range from 1 value to 100s of values.  In practice, there's probably a couple hundred different combinations of values (some with a few values some with 100s) that represent the bulk of the variants.  However, many more combinations are requested with less frequency.  This is the core of the problem for us, the 256 history is just too short to allow the relevant filters to be cached and it's only when, by luck, there's a series of searches with a small number of values that are repeated across searches that we get any caching!  And we probably never get much caching of the "non-leaf" filters.

Finally, I don't have the query_stats data handy but without the "cache everything" behavior, we see a few hundred filters cached and a _very high miss_ ratio.  With the "cache everything" behavior, we see many 1000s of filters cached and a _very high hit_ ratio.

Let me know if that's sufficient information or if anything is unclear.

Thanks!
</comment><comment author="jpountz" created="2016-01-19T22:49:07Z" id="173013414">&gt; I'm assuming you meant onUse when you said onCache

Woops indeed.

Thanks for the info. The hot threads suggest that the bottleneck is reading posting data from disk. Could you report how much memory your nodes have, how much of it is given to the JVM and how large your data directory is? I am thinking that maybe you did not give enough memory to the filesystem cache? Since the hit ratio of the filter cache is high when caching everything, then the same should be true for the filesystem cache.

Could you also check whether putting `index.store.type: mmapfs` in your elasticsearch.yml file (and restart) makes the performance any different?
</comment><comment author="dbaggott" created="2016-01-20T01:18:36Z" id="173045518">Interesting. 

Memory: 64 GB, with 30 GB given to the JVM (and nothing else of note running on the box).
Data: 33G /var/opt/elasticsearch

I'll need to get back to you re the mmapfs setting, I can't readily test that.

I should mention that we're under a constant stream of index updates right now as we're adding additional data to every document -- prior to enabling the "cache everything" behavior, we initially suspected the relatively heavier indexing and the higher percentage of deleted documents were to blame for performance.  However, we suspended the indexer for a couple of hours and it didn't help with performance so we dismissed that as a concern.  Similarly, we removed the deleted documents (via a force merge) but that didn't help either.
</comment><comment author="dbaggott" created="2016-01-20T01:27:27Z" id="173051474">Also, I said "box" but these are EC2 instances and the data is on EBS gp2 (300 / 3000) volumes -- that has obvious ramifications on disk access...
</comment><comment author="jpountz" created="2016-01-20T09:31:06Z" id="173143990">Your filesystem cache is almost the size of your data directory so I would expect the filesystem cache to do a very good job at caching posting data. The fact that caching everything helps and that you are seeing a high hit rate means that many filters are reused, which should also help with no query caching at all since the same data would be read from disk over and over again.

Another data point that could be interesting would be to know if you notice a difference if you force posting data to be loaded in the fs cache (for instance, run `cat $DATA_DIR/$CLUSTER_NAME/nodes/*/indices/*/*/index/*.doc &gt; /dev/null` just before running your experiment, no restart needed) and whether the hot threads still look the same.
</comment><comment author="jimczi" created="2016-01-20T09:44:43Z" id="173146933">The hot threads show that the term queries with more than 16 terms are the bottleneck here. When there is more than 16 terms the terms query populates a bit set with the matching docs and then returns a Scorer over this bit set. This is done in isolation (the other parts of the query are not taken into account when the bit set is populated). Your results with the cache all heuristic indicates that you have a lot of query that share the filter part (the same terms are used). Is it the expected behavior ? One thing to notice here is that the big terms query act exactly like if they were cached, the bit set contains the matching documents of the terms query alone, the only difference is that the bit set is not added to the cache at the end :(. This means that the cost of caching this query vs not caching it would be very small in terms of computation. 
@dbaggott can you compare the number of results between the big terms query alone (more than 16 terms) vs the non-filter part of your query. If the difference is big it might be good to test to split this big terms query into smaller ones (with 16 terms each) like below:

```
"bool": {
    "should": [
        {
            "terms": {
                "items.fieldA": [A_BLOCK_OF_16_TERMS ]
            }
        },
         {
            "terms": {
                "items.fieldA": [ANOTHER_BLOCK_OF_16_TERMS ]
            }
        }
     ]
}
```

... this could help only if the non-filter part of the query returns few results.

Another thing, in the cache there is a distinction between costly and cheap queries. Costly queries are cached when we see them twice in the 256 history whereas normal ones are cached after 5 appearances. It could be helpful to add the terms query with more than 16 terms in the list of costly queries (@jpountz ?), it seems that they are not.
</comment><comment author="jpountz" created="2016-01-20T13:41:15Z" id="173207467">Admittedly I did not check this possibility: my assumption was that if the terms query execution was the bottleneck then the hot threads would point either to the code that decodes postings lists or to the code that builds the DocIdSet. But here all stacks are pointing to disk reads, even though they should be almost free since the filters appear to be reused, which confuses me a bit. I suggested to test mith `mmapfs` to see if it makes things better to read directly from the fs cache instead of having a (quite large) intermediate buffer like `niofs` does.

That said, the suggestion to cache large terms filters more aggressively sounds good to me.
</comment><comment author="dbaggott" created="2016-01-20T15:22:48Z" id="173236384">Thank you so much for the feedback and suggestions.  I'm still working on setting up a proper testing environment so that I can perform ad hoc tests to my heart's content so I don't have feedback on your suggestions.  Yet.

&gt; Your results with the cache all heuristic indicates that you have a lot of query that share the filter part (the same terms are used). Is it the expected behavior ?

Yes, that is definitely expected.  The actual values of the terms are arbitrary and can range from 1 value to 100s of values. In practice, when you analyze the stream of incoming queries you see a couple hundred distinct combinations of terms (some combinations with just a few terms some with 100s) that represent the _vast bulk_ of the variants.  So, heavy reuse is expected and a very high hit ratio is exactly what I want/see with the "cache all" behavior.

&gt;  When there is more than 16 terms the terms query populates a bit set with the matching docs and then returns a Scorer over this bit set. This is done in isolation (the other parts of the query are not taken into account when the bit set is populated).

At what point are the other parts of the filter taken into account?

&gt; One thing to notice here is that the big terms query act exactly like if they were cached, the bit set contains the matching documents of the terms query alone, the only difference is that the bit set is not added to the cache at the end :(. This means that the cost of caching this query vs not caching it would be very small in terms of computation.

Can you clarify what you mean by this?  Particularly the very last statement.  Are you saying that the individual bit sets for each term alone is (potentially) cached and so the calculation of the overall bit set can be derived (relatively) cheaply from the individual (hopefully) cached bit sets?

&gt; ...can you compare the number of results between the big terms query alone (more than 16 terms) vs the non-filter part of your query. If the difference is big...

The delta will vary considerably depending on the exact nature of the query.  But, in general, the larger the "big terms" portion of the filter, the more likely it is that the delta will be large.  So, if this is a core problem, maybe it could still be a net win.  What are the downsides to breaking up the large terms filter when the query portion does NOT return few results?  I assume there must be some otherwise it would be implemented as an internal optimization?

Ok, so avenues of investigation to understand/improve the performance of the non-cached filters are:
1. mmapfs configuration
2. force posting data to be cached in the fs (@jpountz, this is most interestingly done prior to the mmapfs change, right?)
3. investigate delta between query-only portion of request and filter-only portion of request and split the "large terms" portion of the filter into groups no bigger than 16

&gt; That said, the suggestion to cache large terms filters more aggressively sounds good to me.

That sounds good to me too although, even if it's applicable to the performance of my query stream, it wouldn't help my particular use case much, if at all, given that the history size is insufficiently small to accurately sample my particular filter stream...

The performance suggestions are awesome (thank you!) and deserve to be worked through as the first-tier problem -- I'd much rather have performance improved to the point where caching wasn't necessary then to rely on caching.  Or, put another way, I don't want caching to cover up fundamental performance issues!

That being said, even if my use case no longer has a need for filter caching, there's still a fundamental issue with usage tracking of filters: ie, in order to detect repeated usage, the sampling methodology must accommodate the amount of variance in the data stream.  Otherwise, filter reuse won't be detected properly.

The challenge (as I'm sure you know) is to solve that problem in a generic fashion that doesn't expose a bunch of configuration complexity and doesn't require a priori understanding of your query complexity (which is prone to changing anyway).  I can imagine a time-based rolling window implementation would be more forgiving and therefore closer to "one size fits all".  Reservoir sampling might be helpful.  I can also imagine a variation of the current history-based one that simply dynamically resizes the history length based on the complexity of the filters.  The idea being that if you never see any repeated filters (or, more realistically, the percentage of repeated filters is below some threshold) then the history length is increased.  The various thresholds for caching would then be expressed as percentages instead of in absolute terms...
</comment><comment author="jimczi" created="2016-01-20T16:03:23Z" id="173249978">&gt; At what point are the other parts of the filter taken into account?

Just after, suppose you have a terms query with more than 16 terms inside a boolean query with two other clauses. First the bit set with the matching docs for the terms query is build and after that the bit set is used in conjunction with the two other clauses to build the final matching docs. This is done to avoid the explosion of boolean clauses when the number of terms is big. 

&gt; Can you clarify what you mean by this? Particularly the very last statement. Are you saying that the individual bit sets for each term alone is (potentially) cached and so the calculation of the overall bit set can be derived (relatively) cheaply from the individual (hopefully) cached bit sets?

No the overall bit set for the terms query is computed which is exactly how the cache would work if the terms query was added to the cache. The cache would build the bit set of matching docs for the terms query alone. What I am saying is that terms query with more than 16 terms build this bit set even if the query is not cached. If you cache a query it has the downside of building this individual query alone, so if you have another part of your query that reduce the number of matching docs it is not taken into account. This is why when a query enters the cache the response time can be much bigger. For terms query with more than 16 terms this extra work is done every time so the cost of adding this query to the cache would be the same in terms of computation (and only computation). If this is not clear please read the javadocs of org.apache.lucene.queries.TermsQuery which explain how the terms query are built.

&gt; What are the downsides to breaking up the large terms filter when the query portion does NOT return few results?

It's not exactly a downside but in that case the current heuristic on big terms query may be faster. The order of magnitude you're looking for is something like 1000 times smaller. Something like my terms query alone returns 1M results and the query part alone returns 1000 results.

&gt; That sounds good to me too although, even if it's applicable to the performance of my query stream, it wouldn't help my particular use case much, if at all, given that the history size is insufficiently small to accurately sample my particular filter stream...

Well you must consider that your big terms query (more than 16 terms) only counts for 1 entry in the cache and not one per individual terms. This is why a more aggressive caching of big term queries may solve your problem without changing the size of the window.
</comment><comment author="dbaggott" created="2016-01-20T20:05:28Z" id="173344613">@jimferenczi, thank you for the clarifications -- they are helpful!

And I hear you about the more aggressive caching of big term queries, it may help more than I think.  But I'm still suspicious my query stream is too diverse for the history size but, ultimately, that's an empirical question and I haven't done the analysis to answer it...

I'll get back to this thread w/r/t the suggestions...
</comment><comment author="lmenezes" created="2016-02-05T16:51:16Z" id="180436434">We have just upgraded a cluster from 1.4.1 to 2.1, and it seems this is also a problem for us. We did expect issues with the upgrade since we had a lot of queries being explicitly cached.

Anyway, having 2 clusters with the exact same configuration, data and handling exactly the same queries, we have the cluster running 2.1 with a cpu usage 30% higher when compared to 1.4.1.. Part of this could be attributed to the extra effort at indexing time(doc_values), but "pausing" indexing for awhile didn't really change things significantly. 

We will give it a try with _index.queries.cache.everything_, even though I'm afraid this is not a long term solution.

For now, query cache for one of my data nodes(running 2.1) looks like this:

```
query_cache: {
  memory_size_in_bytes: 14023808,
  total_count: 1079228986,
  hit_count: 12531589,
  miss_count: 1066697397,
  cache_size: 1892,
  cache_count: 5451,
  evictions: 3559
}
```

Will update this on monday after having _index.queries.cache.everything_ enabled for sometime.

@jpountz Could I gather any more relevant data for you? I do have both 1.4.1 and 2.1 running in parallel, so if you would like to compare something let me know :)
</comment><comment author="SeoJueun" created="2016-02-13T17:14:48Z" id="183704963">Here I am sharing my use case. I am running elasticsearch as advertisements targeting backend of adserver. There are ad campaigns with multiple targeting parameter like age, carrier, device name, gender, os, region and so on. Usually I have under 5000 campaigns and index size is less than 10MB. It is very read-heavy environment but none of my filter is cached because of small documents number. It would be nice if I can customize caching policy.
</comment><comment author="jpountz" created="2016-02-15T16:56:55Z" id="184299630">@lmenezes Thanks for offering help. Something that I am interested in would be to know what hot threads report on the 2.x cluster to try to get an idea of what the bottleneck is.

@SeoJueun The fact that none of your filters get cached suggests that you have less than 10k documents in your index. Queries should be very fast anyway on such a small index aren't they?

Notes to self about what to do/investigate next about this issue:
- we should probably cache terms queries more aggressively
- should we use mmap to read postings (.doc files) just like we do for the terms dictionary and doc values? this might perform better for the case that a terms query tries to match many low-cardinality values?
- should we raise the history size?
- for queries that need to build a doc id set anyway (like terms, prefix, etc.) could we cache this doc id set directly without waiting for reuse (since it is already available)?
</comment><comment author="moliware" created="2016-02-16T10:17:22Z" id="184610186">@jpountz I'm working with @lmenezes :)

First of all we set `index.queries.cache.everything` to true but the CPU usage went even higher and the performance didn't improve. The cache ratios looked better though.

This is a small list of traits that our expensive queries have:
- A query can contain a terms filter with a "big" number of terms (max 2000 terms)
- A query can contain lots of term filters with a single term.
- A query can be a function score query with a weight function that contains a big terms filter (max 2000 terms) 

As requested I collected some hot_threads from our data nodes:

```
"::: {-}{6vJGCdKfTyunL4pbFtfXNw}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.824Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   50.4% (252ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#26]'
     2/10 snapshots sharing following 35 elements
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.index.TermContext.build(TermContext.java:94)
       org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:192)
       org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.&lt;init&gt;(DisjunctionMaxQuery.java:126)
       org.apache.lucene.search.DisjunctionMaxQuery.createWeight(DisjunctionMaxQuery.java:212)
       org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:855)
       org.apache.lucene.search.ConstantScoreQuery.createWeight(ConstantScoreQuery.java:117)
       org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:855)
       org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:838)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery.createWeight(FiltersFunctionScoreQuery.java:136)
       org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:855)
       org.apache.lucene.search.BooleanWeight.&lt;init&gt;(BooleanWeight.java:56)
       org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:203)
       org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:855)
       org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:838)
       org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:76)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{c_fVYldtQGuCLio733YH7Q}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.825Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   43.0% (214.9ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#21]'
     4/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   39.7% (198.7ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#32]'
     2/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{hvI7cyYYS6yL-JE-qqTgEw}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.822Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   59.7% (298.6ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#6]'
     2/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   55.2% (275.9ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#25]'
     2/10 snapshots sharing following 34 elements
       sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
       sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)
       sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)
       sun.nio.ch.IOUtil.read(IOUtil.java:197)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:741)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 13 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     6/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{BtlDOZKYQVeNxWHjCWht6g}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.824Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   57.6% (288.1ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#6]'
     2/10 snapshots sharing following 20 elements
       org.apache.lucene.search.MultiTermQueryConstantScoreWrapper$1.scorer(MultiTermQueryConstantScoreWrapper.java:211)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:177)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 16 elements
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     6/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   52.7% (263.6ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#24]'
     2/10 snapshots sharing following 32 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 19 elements
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     5/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   52.7% (263.6ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#18]'
     4/10 snapshots sharing following 22 elements
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     6/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{k5Wscxb-Ts2XpcsMh_FVfg}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.824Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   50.1% (250.6ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#20]'
     3/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   45.3% (226.3ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#12]'
     2/10 snapshots sharing following 18 elements
       org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:855)
       org.apache.lucene.search.BooleanWeight.&lt;init&gt;(BooleanWeight.java:56)
       org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:203)
       org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:855)
       org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:838)
       org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:76)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 13 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     5/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{uZZ_3YFERjem1ALPjSns8A}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.821Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   45.3% (226.4ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#21]'
     2/10 snapshots sharing following 13 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   41.8% (208.7ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#22]'
     2/10 snapshots sharing following 23 elements
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     8/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{YtYknJhKTZS2KWWWzD-LCQ}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.824Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   46.6% (233.1ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#14]'
     3/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   41.5% (207.4ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#15]'
     3/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   38.0% (190ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#31]'
     7/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{6_pdQtBLTe2QdeMlFGjY8w}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.844Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   59.6% (298.2ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#22]'
     3/10 snapshots sharing following 20 elements
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     4/10 snapshots sharing following 20 elements
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   57.7% (288.5ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#17]'
     2/10 snapshots sharing following 29 elements
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 9 elements
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     6/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   40.9% (204.6ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#37]'
     2/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     6/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     unique snapshot
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader$BlockState.doReset(CompressingStoredFieldsReader.java:409)
       org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader$BlockState.reset(CompressingStoredFieldsReader.java:394)
       org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.document(CompressingStoredFieldsReader.java:573)
       org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:583)
       org.apache.lucene.index.CodecReader.document(CodecReader.java:81)
       org.apache.lucene.index.FilterLeafReader.document(FilterLeafReader.java:405)
       org.elasticsearch.search.fetch.FetchPhase.loadStoredFields(FetchPhase.java:406)
       org.elasticsearch.search.fetch.FetchPhase.createSearchHit(FetchPhase.java:203)
       org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:168)
       org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:589)
       org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)
       org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{pmv-1-KNQlK3t6YzyPjS-Q}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.846Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   60.0% (299.8ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#14]'
     3/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   56.6% (282.8ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#13]'
     5/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   47.7% (238.5ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#21]'
     7/10 snapshots sharing following 36 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues$CompressedBinaryTermsEnum.readHeader(Lucene50DocValuesProducer.java:1116)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues$CompressedBinaryTermsEnum.seekExact(Lucene50DocValuesProducer.java:1258)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues.get(Lucene50DocValuesProducer.java:1066)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$LongBinaryDocValues.get(Lucene50DocValuesProducer.java:1008)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$7.lookupOrd(Lucene50DocValuesProducer.java:629)
       org.apache.lucene.index.SingletonSortedSetDocValues.lookupOrd(SingletonSortedSetDocValues.java:59)
       org.elasticsearch.index.fielddata.FieldData$8.valueAt(FieldData.java:393)
       org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$1.collect(StringTermsAggregator.java:86)
       org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
       org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:145)
       org.apache.lucene.search.TimeLimitingCollector$1.collect(TimeLimitingCollector.java:158)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:218)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:169)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:772)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     3/10 snapshots sharing following 38 elements
       sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
       sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)
       sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)
       sun.nio.ch.IOUtil.read(IOUtil.java:197)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:741)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues$CompressedBinaryTermsEnum.readHeader(Lucene50DocValuesProducer.java:1116)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues$CompressedBinaryTermsEnum.seekExact(Lucene50DocValuesProducer.java:1258)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues.get(Lucene50DocValuesProducer.java:1066)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$LongBinaryDocValues.get(Lucene50DocValuesProducer.java:1008)
       org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$7.lookupOrd(Lucene50DocValuesProducer.java:629)
       org.apache.lucene.index.SingletonSortedSetDocValues.lookupOrd(SingletonSortedSetDocValues.java:59)
       org.elasticsearch.index.fielddata.FieldData$8.valueAt(FieldData.java:393)
       org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$1.collect(StringTermsAggregator.java:86)
       org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
       org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:145)
       org.apache.lucene.search.TimeLimitingCollector$1.collect(TimeLimitingCollector.java:158)
       org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:218)
       org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:169)
       org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:772)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

::: {-}{55aSbiJ4SnSYs3mr_2m6_g}{-}{-:9300}{dc=fra2, master=false}
   Hot threads at 2016-02-16T09:15:14.821Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:

   51.8% (258.9ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#30]'
     3/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   51.3% (256.5ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#25]'
     3/10 snapshots sharing following 12 elements
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 10 elements
       sun.misc.Unsafe.park(Native Method)
       java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
       java.util.concurrent.LinkedTransferQueue.awaitMatch(LinkedTransferQueue.java:737)
       java.util.concurrent.LinkedTransferQueue.xfer(LinkedTransferQueue.java:647)
       java.util.concurrent.LinkedTransferQueue.take(LinkedTransferQueue.java:1269)
       org.elasticsearch.common.util.concurrent.SizeBlockingQueue.take(SizeBlockingQueue.java:161)
       java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)

   47.6% (238ms out of 500ms) cpu usage by thread 'elasticsearch[-][search][T#27]'
     2/10 snapshots sharing following 34 elements
       sun.nio.ch.FileDispatcherImpl.pread0(Native Method)
       sun.nio.ch.FileDispatcherImpl.pread(FileDispatcherImpl.java:52)
       sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:220)
       sun.nio.ch.IOUtil.read(IOUtil.java:197)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:741)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 29 elements
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:54)
       org.apache.lucene.store.DataInput.readVInt(DataInput.java:125)
       org.apache.lucene.store.BufferedIndexInput.readVInt(BufferedIndexInput.java:221)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:157)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 22 elements
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 31 elements
       sun.nio.ch.NativeThread.current(Native Method)
       sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46)
       sun.nio.ch.FileChannelImpl.readInternal(FileChannelImpl.java:737)
       sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:727)
       org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:180)
       org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:342)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:140)
       org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:116)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnumFrame.loadBlock(SegmentTermsEnumFrame.java:176)
       org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:507)
       org.apache.lucene.queries.TermsQuery$1.rewrite(TermsQuery.java:283)
       org.apache.lucene.queries.TermsQuery$1.scorer(TermsQuery.java:347)
       org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:605)
       org.elasticsearch.indices.cache.query.IndicesQueryCache$CachingWeightWrapper.scorer(IndicesQueryCache.java:262)
       org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorWeight.scorer(FiltersFunctionScoreQuery.java:186)
       org.apache.lucene.search.BooleanWeight.scorer(BooleanWeight.java:274)
       org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
       org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:256)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
       org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
       org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
       org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
       org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
       org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
       org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
     2/10 snapshots sharing following 2 elements
       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
       java.lang.Thread.run(Thread.java:745)
"
```

Thanks!
</comment><comment author="SeoJueun" created="2016-02-16T16:09:52Z" id="184748582">@jpountz As you said, query is quite fast with small index. But I can get better performance with filter caching even if index is small. I did experiment to compare performance between index.queries.cache.everything true and false in our production environment. true version comes with about 30% less cpu consumption than false version. Since index is really small, our elasticsearch cluster is not memory bounded but cpu bounded. It would be really nice if I can utilize enough memory with filter caching.
I know my environment is one of very rare case but I think elasticsearch can be used as a perfect advertisements targeting platform as well. 
</comment><comment author="lmenezes" created="2016-02-16T16:25:34Z" id="184753940">@SeoJueun actually, we also use it for ads. We just haven't upgraded this particular cluster yet, but I share the concerns in this case too :)
</comment><comment author="jpountz" created="2016-02-29T13:17:40Z" id="190207911">For the record, here's one change that should help already: #16851.
</comment><comment author="jrots" created="2016-04-07T21:56:29Z" id="207107124">Are queries executing faster again in 2.3 with the fixes of @jpountz, has anyone upgraded and seen improvements? 
I'm still running 2.2 with

&gt;  index.queries.cache.everything: true 

in production, but a bit unsure to just make the switch without this settings if things would be slow again.
</comment><comment author="jrots" created="2016-04-12T08:01:50Z" id="208768455">Ok upgraded to 2.3.1 and removed index.queries.cache.everything: true.
Search latency is lower and I can achieve more searches per sec while load stays the same. 
So things are a lot better then before, thx for the fixes @jpountz
</comment><comment author="jpountz" created="2016-04-12T08:21:32Z" id="208777154">@jrots you are welcome! Thank you for the feedback, I will close this issue now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal: Remove varargs from log methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16030</link><project id="" key="" /><description>While varargs are convenient they allocate. That isn't usually a big deal but in the case of disabled log messages you'd prefer not to incur that penalty. My proposal is to remove the varargs from all log method, or at least the ones less than warning, and replace them with explicit 1, 2, and 3 argument versions. Those won't allocate at all except for boxing. Then create a version of the log methods that takes an array rather than varargs so that you have to intentionally allocate the array.

This should reduce the need for `if (logger.isDebugEnabled())` style checks while at the same time making it more clear when a log invocation allocates. It'll still be possible to hide allocations in log invocations, just a little bit harder.
</description><key id="126980446">16030</key><summary>Proposal: Remove varargs from log methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>enhancement</label><label>v5.0.0-beta1</label></labels><created>2016-01-15T23:36:41Z</created><updated>2016-09-14T14:45:49Z</updated><resolved>2016-09-01T03:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add reindex progress indicator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16029</link><project id="" key="" /><description>Adds a progress indicator for reindex and update_by_query requests that you
can fetch like so:

```
curl 'localhost:9200/_tasks/*/*update*?pretty&amp;detailed'
```

```
{
  "nodes" : {
    "4_g-ViDGQxCbcum5sruV7Q" : {
      "name" : "Advisor",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "attributes" : {
        "testattr" : "test"
      },
      "tasks" : [ {
        "node" : "4_g-ViDGQxCbcum5sruV7Q",
        "id" : 10446,
        "type" : "transport",
        "action" : "indices:data/write/update/byquery",
        "description" : "300/393[updated=300,created=0,deleted=0,batches=4,versionConflicts=0]"
      } ]
    }
  }
}
```

The actual progress part of that is in the "description" field, particularly
the `300/393` bit. The rest is just metadata about how the request is going.
</description><key id="126978846">16029</key><summary>Add reindex progress indicator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>review</label></labels><created>2016-01-15T23:21:26Z</created><updated>2016-02-13T13:30:27Z</updated><resolved>2016-02-04T18:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-15T23:21:44Z" id="172124451">@imotov have a look at the very first task management integration with reindex!
</comment><comment author="bleskes" created="2016-01-17T06:49:36Z" id="172296153">@nik9000 I glanced at this and things look OK to me, but I'm not the right judge for the integration with the Task framework. It does feels very weird to me that we report progress as part of a string (description) instead of structured json. It seems that the right approach is to have a dedicated TaskInfo  but it looks like that will cause serialization issues. Maybe this is a use case for `NamedWriteableRegistry` . I'm sure you and @imotov gave it some thought - can you elaborate? 
</comment><comment author="nik9000" created="2016-01-17T15:06:06Z" id="172334076">We did! We decided that we didn't really have any other examples of this
yet and designing a generic system with one example is too likely to be
missengineered. I'd prefer a some streamable, toxcontent status object
rather than a taskinfo subclass but its too early to make a good call now.
So string status for a release or two.

@nik9000 https://github.com/nik9000 I glanced at this and things look OK
to me, but I'm not the right judge for the integration with the Task
framework. It does feels very weird to me that we report progress as part
of a string (description) instead of structured json. It seems that the
right approach is to have a dedicated TaskInfo but it looks like that will
cause serialization issues. Maybe this is a use case for
NamedWriteableRegistry . I'm sure you and @imotov
https://github.com/imotov gave it some thought - can you elaborate?

&#8212;
Reply to this email directly or view it on GitHub
https://github.com/elastic/elasticsearch/pull/16029#issuecomment-172296153
.
</comment><comment author="clintongormley" created="2016-01-18T12:14:32Z" id="172512501">&gt; I'd prefer a some streamable, toxcontent status object rather than a taskinfo subclass but its too early to make a good call now. So string status for a release or two.

While I think a string is OK for now, I'd like to fix this before we release the reindex API.  I think a generic progress object basically needs something like this:

```
{ 
  "progress_percentage": 0.9,
  "other": {   key-values }
}
```

where `other` could be named something else.... "counters"? 
</comment><comment author="imotov" created="2016-01-18T16:44:27Z" id="172582781">After thinking more about it, I think we should have at least 2 fields - `description`, which is static info like field and `status`. I think `description` should remain a simple string, but I can make `status` more structured if needed. If nobody opposes, I will add this to my todo list. 
</comment><comment author="nik9000" created="2016-01-18T17:04:57Z" id="172592131">&gt; If nobody opposes, I will add this to my todo list.

It makes sense to me.
</comment><comment author="bleskes" created="2016-01-19T11:11:15Z" id="172819937">When I read the code I took the description to be a description of the task (action is opaque). I think we should keep it like this in the task info api as well and make status be arbitrary json/object containing whatever makes sense for the given task. If this is what you mean I'm ++ :)
</comment><comment author="imotov" created="2016-01-19T14:52:48Z" id="172876465">@bleskes indeed, that's what I meant
</comment><comment author="nik9000" created="2016-02-03T14:36:46Z" id="179268626">I'll have to rework this one #16356 is in.
</comment><comment author="nik9000" created="2016-02-04T14:28:35Z" id="179869985">OK! This is unblocked now. I'll merge #16356 into feature/reindex, rebase this on top of it, and then rework it to use the new status.
</comment><comment author="nik9000" created="2016-02-04T18:46:31Z" id="179995892">I'm going to nuke an recreate this....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refreshes to clear heap used by version map should not be visible to users</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16028</link><project id="" key="" /><description>This change makes it more predictable/controllable to users when recently indexed documents become visible for searches via refresh by making version map use a private searcher so that when it needs to free up heap, its refreshes won't be visible to users.

Note that this also means flush no longer also refreshes, so I had to fix some tests that were relying on this.

Note that as @bleskes explained on #157658 this won't be perfect, e.g. when shards relocate there is a user-visible refresh automatically done.  Progress not perfection...

Closes #15768 
</description><key id="126954709">16028</key><summary>Refreshes to clear heap used by version map should not be visible to users</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>discuss</label></labels><created>2016-01-15T20:42:57Z</created><updated>2016-06-24T15:46:39Z</updated><resolved>2016-06-24T15:46:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-17T07:12:36Z" id="172296948">Left some comments/questions here and there. While I may need similar functionality for the seq_no work (later on, not sure), I still have my doubts whether the change is worth the added complexity here. The engine is complex enough as it is and since this doesn't really give us 100% guarantees (as discussed) I feel like we are maybe sending people the wrong message here. Uwe wanted to control doc visibility so he can build on that and have some logic in his application. Now it will seem to work but sometimes wont (in subtle ways). I'm leaning towards saying we should be more upfornt about it which will also allow us to avoid some complexity.

Last, I'm fine with not refreshing after an internal flush, but I think we should if someone calls the explicit flush API. If a user calls flush (i.e., put everything into lucene and trim the translog) I would expect the docs (now part of lucene guaranteed) to be part of searchers. I understand that the underlying semantics can (and are) different but I think it will be confusing to users.  I wonder how other people feel.
</comment><comment author="mikemccand" created="2016-01-18T17:27:33Z" id="172598412">&gt; I still have my doubts whether the change is worth the added complexity here.

I don't think there's so much added complexity in this change.  Yes, we have two `SearcherManagers`, but they share a single reader pool inside `IndexWriter` so performance should be very nearly the same except when the two managers diverge substantially.

I really don't like that internal implementation details of ES's heap management randomly impact user visibility of recent changes.  I think it's important to minimize such leaky abstractions.  Solr already uses a private reader for its realtime lookups, though presumably is also not "perfect" when a shard moves.

I think if ES did segment (file) based replication instead of document based, such goals would be more natural since each node would have exactly the same point-in-time searcher versions visible (yeah, I know, that'd be a huge change, lots of tradeoffs).

I think for this change, we could maybe also use Lucene's `SearcherLifetimeManager` instead of two `SearcherManagers` ... it could be simpler since it'd mean we just have to hold the private and public searchers, pulled from a single `SearcherManager`.

Finally, if this really is a non-goal, i.e. the use case @uschindler asks about can't and won't be supported and is unimportant, then I agree any iota of added complexity is not worth it, and we should not pursue it.
</comment><comment author="bleskes" created="2016-01-18T19:47:22Z" id="172633088">&gt; I don't think there's so much added complexity in this change. ... I really don't like that internal implementation details of ES's heap management randomly impact user visibility of recent changes. I think it's important to minimize such leaky abstractions.

Fair enough. This is a judgment call. I don't feel too strongly about it. I wonder what other think.
</comment><comment author="uschindler" created="2016-01-18T23:33:59Z" id="172681620">Basically I think what we really need:

Lucene out of box provides the following functionality:
- you can do a commit
- you can delete tons of documents (delete by query which was problem here)
- then index new documents
- commit again

What Lucene provides is that the user sees this as one transaction! This is requested quite often by people when they want to replace some of their data, but cannot do that by deleting and updating single documents. They often can only remove all documents of one category and replace them by new ones. With delete-by-query or other background tasks refreshing automatically (like it did/does) this gets impossible!

Maybe we can add something else: How about adding an API to temporarily disable everything of Elasticsearch doing in background - also replication. Then do your updates (e.g. with 2nd searcher) and then call another API to reenable the whole system?

Basically, I and many other just want to have back the basic and trivial commit/snapshot functionality of Lucene, which works with Lucene and Solr, but not Elasticsearch.
</comment><comment author="uschindler" created="2016-01-19T10:04:57Z" id="172802471">Basically the API is there, it is called "disable refresh" and "enable refresh", but the stuff to be fixed here breaks it.
</comment><comment author="s1monw" created="2016-01-19T11:24:00Z" id="172822154">I think this change is fine and IMO the right thing to do. 

@uschindler the only reason why we could call refresh is a relocation of a shard and you can already disable this per index if you want. once this change is in
</comment><comment author="uschindler" created="2016-01-19T21:30:11Z" id="172993005">OK!
</comment><comment author="dakrone" created="2016-04-06T21:46:58Z" id="206585116">@mikemccand what's the word with this PR? Is it waiting on anything still?
</comment><comment author="mikemccand" created="2016-05-03T14:53:32Z" id="216554106">Sorry for the delays here ... this was somewhat controversial, and doesn't fully solve the @uschindler example (because relocations can still bring changes live) so I didn't push this change.  I'll remove 5.0.0 tag and put discuss on.
</comment><comment author="bleskes" created="2016-06-24T09:46:44Z" id="228304322">we discussed this at fix it friday and the general consensus was that the requested feature (full control of refresh cycles for full control of data visibility) is a high hanging fruit and we rather not start embarking on that right now. @mikemccand are you OK with closing this?
</comment><comment author="mikemccand" created="2016-06-24T15:46:39Z" id="228381763">Sure, I'll close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle closed readers in ShardCoreKeyMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16027</link><project id="" key="" /><description>Fixes an issue caused by trying to add a LeafReader for a closed index to
ShardCoreKeyMap. It add itself to the map half way before throwing
AlreadyClosedException, leaking some references and causesing Elasticsearch
to refuse to shut down cleanly when testing.
</description><key id="126937465">16027</key><summary>Handle closed readers in ShardCoreKeyMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Cache</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-15T19:11:34Z</created><updated>2016-01-18T18:09:35Z</updated><resolved>2016-01-18T14:16:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-15T19:25:32Z" id="172060160">@jpountz this is probably for you to review. The issue comes up when you delete an index at the same time that you try to query it for the first time, I believe. and #16026 made tracking it down take hours instead of minutes.
</comment><comment author="jpountz" created="2016-01-18T10:19:10Z" id="172488633">Tricky case. The fix looks good to me!
</comment><comment author="nik9000" created="2016-01-18T14:16:08Z" id="172537794">&gt; Tricky case. The fix looks good to me!

Thanks! I'm not sure how its not come up before but it was a huge pain to track down. Thanks for the review! I'll merge and backport to 2.x.
</comment><comment author="nik9000" created="2016-01-18T14:46:48Z" id="172548328">Cherry picked to 2.x with 2ef102f6b66b589d2d86a9c79706f5fe2ae45779.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal: ban IOUtils.closeWhileHandlingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16026</link><project id="" key="" /><description>IOUtils.closeWhileHandlingException eats all exceptions raised on close which makes it very difficult to track down failures. I've burned up 2 days this month alone just hunting down failures that were hidden by it that were detected very indirectly by things like the file and thread leak detectors.
</description><key id="126935743">16026</key><summary>Proposal: ban IOUtils.closeWhileHandlingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2016-01-15T19:01:12Z</created><updated>2017-05-05T15:09:38Z</updated><resolved>2017-05-05T15:09:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-18T10:24:20Z" id="172489658">I guess this method should take a compulsory Throwable parameter and call addSuppressed for on it for every Closeable that can't be closed successfully?
</comment><comment author="nik9000" created="2016-01-18T14:20:25Z" id="172538642">&gt; I guess this method should take a compulsory Throwable parameter and call addSuppressed for on it for every Closeable that can't be closed successfully?

I'd be quite happy with that.
</comment><comment author="mikemccand" created="2016-01-18T16:52:03Z" id="172585488">&gt; I guess this method should take a compulsory Throwable parameter 

+1
</comment><comment author="mikemccand" created="2016-02-01T15:42:04Z" id="178030729">&gt; I guess this method should take a compulsory Throwable parameter

Actually there's some interesting history here ... which I dug up while working on heroic exception handling in the translog (#16117).

Lucene's `IOUtils` APIs used to take a prior `IOException`, and use `addSuppressed` on that if any new exceptions occurred.

But this API was dangerous.  See https://issues.apache.org/jira/browse/LUCENE-5654 for the full story, but the summary is that Lucene had code all over the place like this:

```
     @Override
     public void close() throws IOException {
      IOException priorE = null;
       try {
         super.close();
      } catch (IOException ioe) {
         priorE = ioe;
       } finally {
         IOUtils.closeWhileHandlingException(priorE, writer);
       }
     }
```

And if you stare at the code for long enough you'll realize that if say an OOME is hit in that `super.close`, then `priorE` is passed as `null` to `IOUtils.closeWhileHandlingException` thus possibly ignoring (losing) the OOME and throwing some other odd `IOException` which could look a lot like index corruption.  So we removed all IOUtils APIs that took a prior exception!

That said, I think we could maybe safely add them back, if we took `Throwable` not `IOException` as the prior exception?  I.e. do this instead?

```
     @Override
     public void close() throws IOException {
      Throwable priorThrowable = null;
       try {
         super.close();
      } catch (Throwable t) {
         priorThrowable = t;
       } finally {
         IOUtils.closeWhileHandlingException(priorThrowable, writer);
       }
     }
```

Anyway, I'm not really sure what to do here, and heroic exception handling is hard and I think often not justified (adds non-trivial complexing for what are exceptional cases)... but wanted to at least call attention here to why none of Lucene's `IOUtils` methods take a prior exception anymore...
</comment><comment author="javanna" created="2017-05-05T14:25:07Z" id="299478863">@nik9000 given Mike's explanation it seems like we don't want to make this change? Should we close this issue then?</comment><comment author="nik9000" created="2017-05-05T15:09:37Z" id="299491079">As much as I hate it, sure. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify equality test in IndexShard#sameException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16025</link><project id="" key="" /><description>This commit simplifies an equality test in IndexShard#sameException
where the messages for two exceptions are being compared. The previous
condition first tested logical equality if the left exception is not
null, and otherwise tested reference equality. There is a convenience
method since JDK 7 for testing equality in this way: Objects#equals.
</description><key id="126924115">16025</key><summary>Simplify equality test in IndexShard#sameException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-15T17:57:31Z</created><updated>2016-01-15T19:22:26Z</updated><resolved>2016-01-15T19:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-15T19:15:32Z" id="172057621">LGTM
</comment><comment author="jasontedor" created="2016-01-15T19:22:26Z" id="172059304">Integrated via a7185a1d319488e602be3af8c2e9731e733a0886.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Website documentation recommending GET with body to do search is invalid according to http 1.1 spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16024</link><project id="" key="" /><description>I'm not sure where else to report this...

All throughout the website, you give examples of how to perform various operations related to search but the examples are GET requests with a data body.

This is wrong according to the Http 1.1 spec that dictates the body of a GET request should not change the response contents.  In other words, you should be able to ignore the request body and get the same result.

As an alternative, you should document the query string parameters better.  For example, it's not documented how to do highlighting with query string parameters, only a GET payload.

Ultimately, calling GET with a payload should be deprecated and removed from future releases of ES.  This is not recommended by anyone and I can find no support for doing things this way.
</description><key id="126920373">16024</key><summary>Website documentation recommending GET with body to do search is invalid according to http 1.1 spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">et304383</reporter><labels><label>:REST</label><label>discuss</label></labels><created>2016-01-15T17:39:10Z</created><updated>2016-08-11T17:41:48Z</updated><resolved>2016-01-18T10:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-18T10:28:21Z" id="172491501">This has been raised many times.

&gt; This is wrong according to the Http 1.1 spec that dictates the body of a GET request should not change the response contents. In other words, you should be able to ignore the request body and get the same result.

The RFC for HTTP 1.1 [RFC2616](https://www.ietf.org/rfc/rfc2616.txt) says this:

&gt; A server SHOULD
&gt;    read and forward a message-body on any request; if the request method
&gt;    does not include defined semantics for an entity-body, then the
&gt;    message-body SHOULD be ignored when handling the request.

So yes, the _recommendation_ (note: "SHOULD") is that a GET body shouldn't have any effect on server processing.

But GET is a better semantic fit for search than POST:

&gt;  In particular, the convention has been established that the GET and
&gt;    HEAD methods SHOULD NOT have the significance of taking an action
&gt;    other than retrieval. These methods ought to be considered "safe".
&gt;    This allows user agents to represent other methods, such as POST, PUT
&gt;    and DELETE, in a special way, so that the user is made aware of the
&gt;    fact that a possibly unsafe action is being requested.

Also mentioned in the spec:

&gt;  A cache or origin server receiving a conditional request, other than
&gt;    a _full-body GET request_, MUST use the strong comparison function to
&gt;    evaluate the condition.

Users often allow just GET requests to their servers in order to allow a range of non-destructive actions.  The problem is that a search request (with the query DSL, aggs, etc) can easily overflow the max URL length, so we allow sending it in the body instead.  Of course, some clients don't allow GET requests with a body, so we support two other ways of sending a search request:
- `GET _search?source=......`
- `POST _search`

&gt; As an alternative, you should document the query string parameters better. For example, it's not documented how to do highlighting with query string parameters, only a GET payload.

The Query DSL has so many options, restricting yourself to just the basic query string parameters would be a waste of time.  For instance, you wouldn't be able to use aggregations.

If you don't like using `GET _search` with a body, then use POST instead, but we're not planning on changing this.
</comment><comment author="et304383" created="2016-01-18T11:26:49Z" id="172502807">The "source" parameter needs to be documented.  We only discovered this on some google search on one forum mentioned by someone way down the line as a suggestion.  I can't even find that link right now it was so obscure and difficult to find.

Once we discovered the source parameter existed and could accept our json payload this was quite easier to resolve.

Regardless of limitations on query string parameters, how to send all options as as query string parameters should be documented, not just a select few.

Using a POST to do a search "feels" wrong and would be unacceptable regardless since we'd then have to open our Elasticsearch endpoint (Amazon ES) up to allow post to the public and would thus require additional logic to prevent destructive operations as you mentioned earlier such as additional authentication (not sure we can do this in Amazon ES) or a proxy server.

TLDR: more documentation is never a bad thing.

Thanks!
</comment><comment author="clintongormley" created="2016-01-18T13:42:39Z" id="172529855">&gt; The "source" parameter needs to be documented. 

It is documented under API conventions &gt; Common options, along with other request parameters that apply across all or most APIs: https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_request_body_in_query_string

&gt; Once we discovered the source parameter existed and could accept our json payload this was quite easier to resolve.

You are very likely to run into problems with having your search request truncated - it is too easy to run into server URL limits.

&gt; how to send all options as as query string parameters should be documented, not just a select few.

I'm not 100% certain, but I don't believe that highlighting is supported via query string parameters (other than encoding the body in `source`).

The query DSL (plus the rest of the search options) is just too rich to replicate purely with query string params.
</comment><comment author="et304383" created="2016-01-18T13:47:46Z" id="172530835">I understand where you're coming from.  Thanks for that link but that definitely needs to be placed somewhere in the SEARCH API docs.  Google isn't able to index properly when one searches for "elasticsearch uri search source" or similar searches because the content isn't falling on one page.

You may or may not care, but the majority of people don't have time to read all documentation for a piece of software - they have to rely on Google to index it properly so they can find exactly what they need quickly.

At the very least, the source parameter should be listed here (please):
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-uri-request.html

Edit: please understand that we've placed our Elasticsearch service behind API Gateway (may or may not be the best decision) and it does NOT support a GET request with a request body.  It is outright refused and the request is flagged as invalid.  That's why our hands are somewhat tied without redesigning / rebuilding our infrastructure.
</comment><comment author="asbjornu" created="2016-01-26T20:07:42Z" id="175204506">@clintongormley, I completely agree with @eric-tucker. [So does Roy Fielding](https://groups.yahoo.com/neo/groups/rest-discuss/conversations/messages/9962):

&gt; Server semantics for GET, however, are restricted such that a body, if any, has no semantic meaning to the request. The requirements on parsing are separate from the requirements on method semantics.
&gt; 
&gt; So, yes, you can send a body with GET, and no, it is never useful to do so.
&gt; 
&gt; This is part of the layered design of HTTP/1.1 that will become clear again once the spec is partitioned (work in progress).
&gt; 
&gt; ....Roy

I think it is a clear violation of the [Layered System constraint of REST](https://en.wikipedia.org/wiki/Representational_state_transfer#Layered_system), as well as [section 4.3 of RFC 2616](https://tools.ietf.org/html/rfc2616#section-4.3):

&gt; [...] if the request method does not include defined semantics for an entity-body, then the message-body SHOULD be ignored when handling the request.

On top of that, [section 4.3.1 of RFC 7231](https://tools.ietf.org/html/rfc7231#section-4.3.1) states:

&gt; A payload within a GET request message has no defined semantics; sending a payload body on a GET request might cause some existing implementations to reject the request.
&gt; 
&gt; The response to a GET request is cacheable; a cache MAY use it to satisfy subsequent GET and HEAD requests unless otherwise indicated by the Cache-Control header field ([Section 5.2 of [RFC7234]](https://tools.ietf.org/html/rfc7234#section-5.2)).

So I don't think there's anything to argue; this is not a recommended practice. If I put a Varnish server in front of Elasticsearch, I should expect it to cache the response of all `GET` requests sharing the same `Vary` semantics. With Elasticsearch's use of a message body, that will at best lead to different `GET` requests being fulfilled by the same cached response, and at worst, break badly.

I would actually rather use the [drafted `SEARCH` method](https://tools.ietf.org/html/draft-snell-search-method-00) than `GET` for these requests. [The method is already standardized in WebDAV (RFC 5323)](http://tools.ietf.org/search/rfc5323#section-2) and I don't think @reschke and @jasnell's draft requires too much work to be published. They could perhaps weigh in with their thoughts?
</comment><comment author="et304383" created="2016-01-26T20:15:44Z" id="175207070">This is a very good point regarding cache of GETs in Varnish.  You've now made a GET behave differently based on a payload but anything obeying RFC standards is completely valid in ignoring that payload and treating two (supposedly) different GETs as the same request.

Just because this is the way ES was setup and you don't want to change it doesn't mean it's correct and doesn't need to change long term.
</comment><comment author="asbjornu" created="2016-08-08T18:14:32Z" id="238327349">@clintongormley If @jasnell finishes his [Internet Draft of the `SEARCH` method](https://tools.ietf.org/html/draft-snell-search-method-00), would you consider adding support for it and in time phase out the current "`GET` with HTTP body" practice?
</comment><comment author="jasnell" created="2016-08-08T18:24:56Z" id="238330493">I will look at getting this moved forward. I had left it in the hands of a couple others but it looks like the interest / momentum faded out. If there's interest in implementing this, I can definitely work on getting it advanced.
</comment><comment author="glennblock" created="2016-08-09T08:22:31Z" id="238486181">+1 
</comment><comment author="clintongormley" created="2016-08-11T17:41:48Z" id="239235115">@asbjornu sure - makes sense
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse metadata mappers for dynamic updates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16023</link><project id="" key="" /><description>When a metadata mapper is not specified in a mapping update, it should default
to the current metadata mapper instead of the general default in order for the
update to not conflict with the current mapping.

Closes #15997
</description><key id="126904234">16023</key><summary>Reuse metadata mappers for dynamic updates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-15T16:16:14Z</created><updated>2016-01-19T08:28:06Z</updated><resolved>2016-01-19T08:28:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-15T16:21:53Z" id="172005459">Makes sense to me!
</comment><comment author="jpountz" created="2016-01-15T16:24:55Z" id="172006245">Thanks for looking @nik9000 ! I'll ping @rjernst as he initially suggested this idea.
</comment><comment author="rjernst" created="2016-01-18T17:56:09Z" id="172606579">This change looks good. With it, is there still a need for getDefault to take in the existing fieldtype?
</comment><comment author="jpountz" created="2016-01-18T18:07:48Z" id="172609304">Yes: we only reuse the current metadata mapper when updating a type that already exists. In the case that we are introducing a new type, we would only reuse the field type from another type (not the entire mapper) so that conflicts will not be introduced, but the mapper can still have different type-specific properties, such as the enabled flag.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unifies packaging for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16022</link><project id="" key="" /><description>Rename bin/plugins in bin/elasticsearch-plugin

Change the inner structure of the plugins zip:
- The zip must contain a top-level folder titled `elasticsearch` and only one sub-folder titled after the name of the plugin. If you use the gradle build, this structure is automatically generated.

```
&lt;plugin_name&gt;-&lt;version&gt;.zip
|____elasticsearch
| |____&lt;plugin_name&gt;
| | |____&lt;plugin_files, plugin-descriptor.properties&gt;
```
</description><key id="126903621">16022</key><summary>Unifies packaging for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugins</label><label>breaking</label></labels><created>2016-01-15T16:13:05Z</created><updated>2016-02-08T09:55:14Z</updated><resolved>2016-02-04T15:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-15T16:13:35Z" id="172003412">@rjernst this one is for you ;)
</comment><comment author="jimczi" created="2016-01-21T12:49:36Z" id="173560297">@rjernst I've updated this PR in order to reflect the design we all agreed on, can you take a look ?
</comment><comment author="jimczi" created="2016-01-27T18:26:50Z" id="175782551">This PR is stalled until @rjernst finishes the work on removing leniency from the plugin cli.
</comment><comment author="jimczi" created="2016-02-04T15:29:26Z" id="179899345">Split this PR in two (one for the renaming and one for the repackaging):
https://github.com/elastic/elasticsearch/pull/16453
https://github.com/elastic/elasticsearch/pull/16454
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The total number of results in the query response is the number of documents that matched the query minus the number of documents that were post-filtered. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16021</link><project id="" key="" /><description>Simple recreation:

```
PUT t/t/0
{
    "field": "foo"
}

PUT t/t/1
{
    "field": "bar"
}

GET t/_search?size=0
{
    "query": {
        "match_all": {}
    }
}
```

... match_all search returns the expected number of results: `total: 2`.
But with a post_filter the search returns the number of result for the query minus the ones that were were post-filtered:

```
GET t/_search?size=0
{
    "query": {
        "match_all": {}
    },
    "post_filter": {
        "term": {
            "field": "foo"
        }
    }
}
```

... so here total hits equals 1.
I don't know if it's expected, I guess it's good for the pagination but bad if you compare this number with the aggregations count (which are not exact anyway so maybe not relevant). 
I wonder if we should return the total results without the post_filter and the number of post filtered documents  like below:

```
"hits": {
      "total": 2,
      "post_filtered": 1
      "max_score": 0,
      "hits": []
}
```
</description><key id="126901496">16021</key><summary>The total number of results in the query response is the number of documents that matched the query minus the number of documents that were post-filtered. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-01-15T16:02:09Z</created><updated>2016-01-15T20:31:55Z</updated><resolved>2016-01-15T16:43:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T16:43:18Z" id="172011266">This is expected.  Aggregations are run on the results of the query, and post-filters are applied to the search results AFTER aggregations are calculated.  So eg if the user searches on "red" t-shirts, you could run a query for t-shirts, with a terms agg on popular "brands" across t-shirts of all colours, then post-filter the results down to just red t-shirts.

I don't think we need to change anything here, it's working as designed.
</comment><comment author="jimczi" created="2016-01-15T19:52:48Z" id="172066661">@clintongormley here https://github.com/elastic/elasticsearch/issues/15994 you're saying that the count should be the same and now you're saying that it's expected ? I understand that the post filter is only applied to the query part and not the aggregations but the count in the response are different.
''''
Without postfilter:
"hits":{"total":2

With postfilter:
"hits":{"total":1
''''
So my question is "should the total number of hits returned in the response be the same with and without postfilter". If the answer is yes then there is a bug and we should reopen this ticket.
</comment><comment author="jimczi" created="2016-01-15T20:31:55Z" id="172085288">@clintongormley I am totally wrong here, sorry. If the post_filter is replaced with a filter inside the query then the number of result must be the same like you said here: https://github.com/elastic/elasticsearch/issues/15994. 
So the answer to my question in the previous comment is that it's expected. 
Closing this and moving on ;).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0 Breaking Changes - Settings docs don't mention index.gateway.local.sync</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16020</link><project id="" key="" /><description>Seems like `index.gateway.local.sync` got renamed to `index.translog.sync_interval` in https://github.com/elastic/elasticsearch/commit/d596f5cc45284a60a88276b90f2bb1937415307b but it is not mentioned in the settings changes under [Setting changes](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_setting_changes.html)
</description><key id="126894431">16020</key><summary>2.0 Breaking Changes - Settings docs don't mention index.gateway.local.sync</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>docs</label></labels><created>2016-01-15T15:26:27Z</created><updated>2016-02-02T13:07:30Z</updated><resolved>2016-02-02T12:19:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T16:37:36Z" id="172009622">sure enough - @mfussenegger would you like to send a PR?
</comment><comment author="mfussenegger" created="2016-01-29T14:06:51Z" id="176769902">I've turned the issue into a PR with the change
</comment><comment author="clintongormley" created="2016-02-02T12:19:23Z" id="178545967">thanks @mfussenegger 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write a multinode qa test for ingest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16019</link><project id="" key="" /><description>Ingest has a proxy filter that reroutes ingest requests to ingest nodes when they are sent to nodes with node.ingest set to false. The filter is unit tested but we could definitely use some more testing. A qa test would be perfect but we would need to be able to set different node settings per node, which is something not yet possible in our qa test infra, which will be possible soon. Once we have that we should go ahead and write a qa multinode test for this.
</description><key id="126893926">16019</key><summary>Write a multinode qa test for ingest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>test</label></labels><created>2016-01-15T15:23:48Z</created><updated>2016-03-01T12:50:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-15T16:49:22Z" id="172012806">+1 we should have a multimode qa test. We can then also test the scenario where one node has the an ingest plugin installed and the other hasn't. I'm pretty sure sure that we don't fail nicely here. We use geoip as plugin to test with. 
</comment><comment author="martijnvg" created="2016-01-19T14:20:09Z" id="172867575">I quickly looked at this and starting a multi node qa test is easy. In order to test the scenario one node needs to be started with ingest enabled (using default node settings) and one node with ingest disabled (`node.ingest: false`), but setting specific settings per node doesn't seem possible.
</comment><comment author="javanna" created="2016-01-19T16:34:49Z" id="172908987">yea that is why I opened this issue, we have to wait for test infra improvements that are in the work to have different settings on each node. I discussed this with @rjernst .
</comment><comment author="talevy" created="2016-02-09T01:44:27Z" id="181664277">Aren't we achieving this by leveraging the `nodeOrdinal` in the test?

On this note,
I have begun exploring this while adding tests to the new pipeline validation. I am attempting to set up a cluster where one node has a certain plugin installed, while the other does not. From the way I see it, `nodePlugins` method can take in a `nodeOrdinal` just like the settings.
what do you think?
</comment><comment author="martijnvg" created="2016-02-09T07:40:43Z" id="181747753">&gt; Aren't we achieving this by leveraging the nodeOrdinal in the test?

Right not this is the only way how we can test it and I think for now this okay. However because we're extending from `ESIntegTestCase` we setup the nodes in a non realistic setup. (we use random settings that are unlikely to be used in production and the nodes aren't started from as one would normally do in production (using one of our assemblies and start it from the command line) ) So ideally this should be tested in a qa scenario, but can't yet customise the cluster to run with different plugins)
</comment><comment author="clintongormley" created="2016-03-01T12:44:54Z" id="190708920">Given that ingest is now a module, is this test still required?
</comment><comment author="martijnvg" created="2016-03-01T12:50:01Z" id="190710264">@clintongormley This issue is about adding a qa test that verifies that the redirecting works as expected when some nodes have ingest enabled and some don't. We have a Java integration test for this, but that isn't ideal as it doesn't test how users run ES. Also we have a qa test that test when all nodes have ingest disabled, but running some nodes with ingest enabled and some without isn't possible at the moment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamically map all numerics to floats by default?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16018</link><project id="" key="" /><description>Elasticsearch assumes that if a number contains a dot, then it should be mapped as a floating-point number (double in 2.x and float in master) and otherwise as an long. But this is quite trappy as it means that we expect that floating point numbers are consistently serialized with a dot (see eg. https://twitter.com/bitemyapp/status/687415657651154944 or #15961).

Instead, we could map all numerics to floats by default (but you could still use dynamic templates to override it if you want). This would have two drawbacks:
- floats can only represent integer values accurately up to 2^24 (~16M)
- it would increase storage requirements

I ran some simulations to see how worse it would be to store integers as floating point numbers, the good news being that since most bits will be zeros on the right side of the mantissa, gcd compression will help save some bits:
- less than 256 unique values: storing numbers as a float or as a long doesn't matter since we would use table compression in both cases
- more than 256 unique values between 1 and 1000: an int would require 10 bits per value, rounded to 12 for retrieval efficiency, while a float would use 17 bits rounded to 20 for retrieval efficiency (+67%)
- more than 256 unique values between 1 and 100,000: an int would require 17 bits per value, rounded to 20 for retrieval efficiency, while a float would use 24 bits rounded to 24 for retrieval efficiency (+20%)

I'm not sold yet about what we should do but thought we should have this discussion. Again, note that it would only apply to dynamically mapped fields, integers that are mapped as integers would remain as efficient as they are today.
</description><key id="126893666">16018</key><summary>Dynamically map all numerics to floats by default?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2016-01-15T15:22:29Z</created><updated>2017-03-01T23:23:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T16:28:30Z" id="172007226">I think you'd make half the people happy, and the other half unhappy. It's so easy to add a dynamic mapping rule that allows you to add all numeric fields as `float` should you choose to do so, I'm not sure it's worth the change.
</comment><comment author="lpic10" created="2017-03-01T23:23:36Z" id="283504752">I think this should be done because the defaults should try to favor usability over performance (or storage in this case) </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to exclude based on paths in XContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16017</link><project id="" key="" /><description>The current filtering capabilities in the XContent api only allow specifying paths to include and not exclude. This PR adds a boolean to relevant methods and constructors to make it possible to specify paths that should be excluded from the output generated by an XContent generator. This is particularly useful for cases where paths should be excluded either due to privacy/security or simply verbosity. Without this PR such cases would be required to specify every path to include and such an approach creates an unnecessary tight coupling with the data structure filtered. 
</description><key id="126888312">16017</key><summary>Add option to exclude based on paths in XContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">beiske</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-15T14:56:41Z</created><updated>2016-02-08T10:53:06Z</updated><resolved>2016-02-03T12:51:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-01-15T15:31:24Z" id="171992079">@beiske I left some comments. Have you seen `AbstractFilteringJsonGeneratorTestCase`? I think we need more tests to be complete.
</comment><comment author="beiske" created="2016-01-15T16:58:22Z" id="172015433">@tlrx I've implemented most of your suggestions, but in stead of delegating the createGenerator methods per implementation I moved it to a default method in the interface. With this change I think it is also possible to delete the backwards compatible constructors on the content generators.
</comment><comment author="beiske" created="2016-01-15T17:04:40Z" id="172017010">Regarding more testing with AbstractFilteringJsonGeneratorTestCase, that probably would not hurt, but I will not have time to address that until after the 26th. So if this PR is relevant for anyone else, it is totally up for grabs. If not I'll continue when I'm back.
</comment><comment author="beiske" created="2016-01-28T15:16:07Z" id="176230785">@tlrx I've added more tests. I believe I have addressed all the issues now. 
</comment><comment author="tlrx" created="2016-02-03T10:09:59Z" id="179143023">Left minor comments
</comment><comment author="tlrx" created="2016-02-03T12:50:47Z" id="179206849">LGTM
</comment><comment author="clintongormley" created="2016-02-08T09:33:16Z" id="181272801">@beiske this PR is missing documentation (and doesn't even have a description).  Please could you add.
</comment><comment author="beiske" created="2016-02-08T10:53:06Z" id="181304127">Added description to PR. This change only affects an internal API and is therefore only documented in javadoc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kibana: No Living connections</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16016</link><project id="" key="" /><description>I have ES 1.5.2 cluster with Kibana 4.0.1. This consists of 4 data/master nodes along with 4 client nodes. When I'm trying to load kibana sometimes it takes longer time to load data or sometimes I need to restart the ES client node which kibana is pointed to. 

I get below console error in Kibana page
Failed to load resource: net::ERR_EMPTY_RESPONSE - http://xx.xx.xx.xx/elasticsearch/_nodes

and below error in ES log
[2016-01-15 07:01:52,629][DEBUG][http.netty] [xxxxxx] Caught exception while handling client http traffic, closing connection [id: 0x8301fd2b, /xx.xx.xx.xx:54237 :&gt; /xx.xx.xx.xx:9200]

All the masters has below config

"index" : {
          "number_of_shards" : "3"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "threadpool" : {
          "search" : {
            "queue_size" : "2000",
            "size" : "20"
          }
"indices" : {
          "fielddata" : {
            "cache" : {
              "size" : "50%"
            }
          }
        },
        "discovery" : {
          "zen" : {
            "master_election" : {
              "filter_client" : "true"
            },
            "ping" : {
              "multicast" : {
                "enabled" : "false"
              },
              "timeout" : "6s",
              "unicast" : {
                "hosts" : [ "xxxx", "xxx", "xxxx" ]
              }
            }
          }
        },
        "name" : "xxxxx",
        "http" : {
          "enabled" : "true"
        },
        "action" : {
          "disable_delete_all_indices" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "config" : "/xxxxx/elasticsearch-1.5.2/config/elasticsearch.yml"
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 4,
        "cpu" : {
          "vendor" : "Intel",
          "model" : "Xeon",
          "mhz" : 2400,
          "total_cores" : 4,
          "total_sockets" : 2,
          "cores_per_socket" : 2,
          "cache_size_in_bytes" : 30720
        },
        "mem" : {
          "total_in_bytes" : 16820760576
        },
        "swap" : {
          "total_in_bytes" : 2145382400
        }
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 48090,
        "max_file_descriptors" : 65535,
        "mlockall" : true
      },
      "jvm" : {
        "pid" : 48090,
        "version" : "1.8.0_45",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "25.45-b02",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1452791955862,
        "mem" : {
          "heap_init_in_bytes" : 8589934592,
          "heap_max_in_bytes" : 8555069440,
          "non_heap_init_in_bytes" : 2555904,
          "non_heap_max_in_bytes" : 0,
          "direct_max_in_bytes" : 8555069440
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Metaspace", "Compressed Class Space", "Par Eden Space", "Par Survivor Space", "CMS Old Gen" ]
      },
      "thread_pool" : {
        "percolate" : {
          "type" : "fixed",
          "min" : 4,
          "max" : 4,
          "queue_size" : "1k"
        },
        "listener" : {
          "type" : "fixed",
          "min" : 2,
          "max" : 2,
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 4,
          "max" : 4,
          "queue_size" : "200"
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 2,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 4,
          "max" : 4,
          "queue_size" : "1k"
        },
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 2,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 20,
          "max" : 20,
          "queue_size" : "2k"
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 2,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "get" : {
          "type" : "fixed",
          "min" : 4,
          "max" : 4,
          "queue_size" : "1k"
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 2,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 4,
          "max" : 4,
          "queue_size" : "50"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 2,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "xxxxxx",
          "name" : "eth0",
          "mac_address" : "xxxxxxx"
        }
      },
      "transport" : {
        "bound_address" : "inet[/xxxxx:9300]",
        "publish_address" : "inet[/1xxxx:9300]",
        "profiles" : { }
      },
      "http" : {
        "bound_address" : "inet[/1xxxxx:9200]",
        "publish_address" : "inet[/xxxxx:9200]",
        "max_content_length_in_bytes" : 104857600
      },

All the clients has below config

"index" : {
          "number_of_shards" : "3"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "threadpool" : {
          "search" : {
            "queue_size" : "2000",
            "size" : "20"
          }
"indices" : {
          "fielddata" : {
            "cache" : {
              "size" : "50%"
            }
          }
        },
        "discovery" : {
          "zen" : {
            "master_election" : {
              "filter_client" : "true"
            },
            "ping" : {
              "multicast" : {
                "enabled" : "false"
              },
              "timeout" : "6s",
              "unicast" : {
                "hosts" : [ "xx.xx.xx.xx", "xx.xx.xx.xx", "xx.xx.xx.xx", "xx.xx.xx.xx" ]
              }
            }
          }
        },
        "name" : "xxxxxx",
        "action" : {
          "disable_delete_all_indices" : "true"
        },
        "client" : {
          "type" : "node"
        },
        "config" : "/xxxx/elasticsearch-1.5.2/config/elasticsearch.yml"
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 2,
        "cpu" : {
          "vendor" : "Intel",
          "model" : "Xeon",
          "mhz" : 2400,
          "total_cores" : 2,
          "total_sockets" : 1,
          "cores_per_socket" : 2,
          "cache_size_in_bytes" : 30720
        },
        "mem" : {
          "total_in_bytes" : 8365080576
        },
        "swap" : {
          "total_in_bytes" : 2145382400
        }
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 34778,
        "max_file_descriptors" : 65535,
        "mlockall" : true
      },
      "jvm" : {
        "pid" : 34778,
        "version" : "1.8.0_45",
        "vm_name" : "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version" : "25.45-b02",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1452792092500,
        "mem" : {
          "heap_init_in_bytes" : 4294967296,
          "heap_max_in_bytes" : 4277534720,
          "non_heap_init_in_bytes" : 2555904,
          "non_heap_max_in_bytes" : 0,
          "direct_max_in_bytes" : 4277534720
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Metaspace", "Compressed Class Space", "Par Eden Space", "Par Survivor Space", "CMS Old Gen" ]
      },
      "thread_pool" : {
        "percolate" : {
          "type" : "fixed",
          "min" : 2,
          "max" : 2,
          "queue_size" : "1k"
        },
        "listener" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 2,
          "max" : 2,
          "queue_size" : "200"
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 2,
          "max" : 2,
          "queue_size" : "1k"
        },
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 20,
          "max" : 20,
          "queue_size" : "2k"
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "get" : {
          "type" : "fixed",
          "min" : 2,
          "max" : 2,
          "queue_size" : "1k"
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 2,
          "max" : 2,
          "queue_size" : "50"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 1,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "xx.xx.xx.xx",
          "name" : "eth0",
          "mac_address" : "xx.xx.xx.xx"
        }
      },
      "transport" : {
        "bound_address" : "inet[/xx.xx.xx.xx:9300]",
        "publish_address" : "inet[/xx.xx.xx.xx:9300]",
        "profiles" : { }
      },
      "http" : {
        "bound_address" : "inet[/xx.xx.xx.xx:9200]",
        "publish_address" : "inet[/xx.xx.xx.xx:9200]",
        "max_content_length_in_bytes" : 104857600
      },
      "plugins" : [ ]
</description><key id="126886961">16016</key><summary>Kibana: No Living connections</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lakal-malimage</reporter><labels /><created>2016-01-15T14:50:37Z</created><updated>2016-01-15T15:16:28Z</updated><resolved>2016-01-15T15:16:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T15:16:28Z" id="171988423">Hi @lakal-malimage 

The place to ask questions like these is in the forums: http://discuss.elastic.co/.  The issues list is reserved for bug reports and feature requests

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move geoip db loading out of the processor factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16015</link><project id="" key="" /><description>GeoIpProcessor.Factory receives now the already loaded database readers for the geoip maxmind database. That required exposing the Environment of a Node though, so that we can get its config file location and look for the config files during IngestGeoIp#onModule execution rather than postponing this operation. This change also allowed to replace the BiFunction&lt;Environment, TemplateService, Processor.Factory&gt; with a simple Function, as environment is immediately available.
</description><key id="126882628">16015</key><summary>Move geoip db loading out of the processor factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-15T14:26:29Z</created><updated>2016-01-15T14:58:27Z</updated><resolved>2016-01-15T14:58:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-15T14:56:33Z" id="171982491">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RescoreBuilder: Add parsing and creating of RescoreSearchContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16014</link><project id="" key="" /><description>Adding the ability to parse from xContent to the rescore builder. Also making RescoreBuilder an abstract base class that encapsulates the `window_size` setting, with QueryRescoreBuilder as its only implementation at the moment.

Relates to #15559 
</description><key id="126872434">16014</key><summary>RescoreBuilder: Add parsing and creating of RescoreSearchContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-15T13:27:50Z</created><updated>2016-01-26T14:42:36Z</updated><resolved>2016-01-26T14:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-18T13:33:54Z" id="172528135">I added a second part to this PR which switches from using BytesReference to the refactored RescoreBuilder in SearchSourceBuilder.
</comment><comment author="colings86" created="2016-01-25T10:58:26Z" id="174470340">@cbuescher I left a few comments
</comment><comment author="cbuescher" created="2016-01-25T16:09:24Z" id="174556909">@colings86 thanks, I added a commit that changes the base builder holding the window_size to be an abstract superclass of QueryRescoreBuilder, also making RescoreBuilder the common interface. Although there is only one implementation at the moment this should enable possible future extensions.
</comment><comment author="colings86" created="2016-01-26T12:01:52Z" id="174969811">left one more comment but LGTM
</comment><comment author="cbuescher" created="2016-01-26T14:40:37Z" id="175050563">@colings86 thanks for the review, I renamed the abstract base class to RescoreBuilder as suggested and added a note about the merge of the former RescoreBuilder with the inner RescoreBuilder.Rescorer class to the migration doc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Mapping] Avoid NPE when merging default parent field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16013</link><project id="" key="" /><description>If type is unequal then report conflict and stop to avoid possible NPE down the line

Report on the forum: https://discuss.elastic.co/t/updating-child-mapping-with-new-field-fails-with-nullpointerexception/39256

This issue only occurs on 2.1.x and 2.0.x releases since from 2.2 mapping conflicts are no longer accumulated and instead if there is a conflict the an exception is thrown and the mapping merging aborts.

I do plan to add the test in this pr (slightly different) also to 2.2, 2.x and master branches.
</description><key id="126853546">16013</key><summary>[Mapping] Avoid NPE when merging default parent field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>:Parent/Child</label><label>bug</label><label>review</label></labels><created>2016-01-15T11:13:31Z</created><updated>2016-04-07T09:36:21Z</updated><resolved>2016-04-06T20:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T12:56:21Z" id="171953108">@martijnvg also see https://github.com/elastic/elasticsearch/issues/16002#issuecomment-171939611 and https://github.com/elastic/elasticsearch/issues/15997#issuecomment-171938807 
</comment><comment author="martijnvg" created="2016-01-15T15:08:28Z" id="171986448">@clintongormley I see, but I think it makes sense to fix these NPEs? Fixing the put mapping API to not make it required to include meta fields is a bigger change.
</comment><comment author="dakrone" created="2016-04-06T17:16:35Z" id="206471102">@martijnvg is this still applicable to 2.2 and 2.3 (and 2.x)?
</comment><comment author="martijnvg" created="2016-04-06T20:04:30Z" id="206534808">@dakrone This can be closed, is fixed in 2.2 and higher.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[feature request]put out of bounds bulk packages off heap or put them into disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16012</link><project id="" key="" /><description>as reference from 
https://github.com/elastic/elasticsearch/issues/15101
the replic bulk will be put into bulk queue no matter it is out of bounds or not. but it is really a great threaten to stability of the system.
so is it possible put out of bounds bulk packages off heap or put them into disk?
</description><key id="126846612">16012</key><summary>[feature request]put out of bounds bulk packages off heap or put them into disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-01-15T10:26:30Z</created><updated>2016-01-15T12:50:13Z</updated><resolved>2016-01-15T12:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T12:50:12Z" id="171952126">Bulk queues on replicas can grow bigger than their configured sizes because we don't want changes which have gone to the primary to not reach the replicas.  That said, the primary will wait for responses from all replicas, thus applying backpressure to the client by rejecting further requests.

We definitely are not going to start moving this data off heap or spooling to disk - that would just introduce a whole new slew of bugs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit request size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16011</link><project id="" key="" /><description>Overly large bulk request can threaten the stability of Elasticsearch. Hence we want to limit the size of a bulk request.

Although the need originally arose for bulk requests, the solution will apply to requests in general and not just bulk requests.

There will be two (configurable) limits:
- A limit on the size (in bytes) of an individual request
- A limit on the size (in bytes) of all requests that are currently in flight
</description><key id="126829693">16011</key><summary>Limit request size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Bulk</label><label>enhancement</label><label>resiliency</label></labels><created>2016-01-15T08:41:12Z</created><updated>2016-04-14T03:44:54Z</updated><resolved>2016-04-13T08:43:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-15T09:58:27Z" id="171917912">Just a thought: I wonder if the default limit should be something like the default `http.max_content_length` divided by two or something along those lines?
May be we should (also?) check that this max size is actually not bigger than `http.max_content_length` and reject the settings in such a case?
</comment><comment author="danielmitterdorfer" created="2016-01-15T12:28:41Z" id="171948369">Thanks for your input @dadoonet! I am not sure about your suggestion on the check of `http.max_content_length` though as this relies on a protocol level setting whereas I'd try to enforce bulk request size in a protocol-independent fashion. I mean this should also work if somebody uses only the transport protocol and then I wouldn't do a validation of this setting based on a http related one. But tbh I did not look into the code yet. wdyt?
</comment><comment author="dadoonet" created="2016-01-15T12:38:27Z" id="171950213">ha! That is correct. I thought `http.max_content_length` was applied to transport layer as well... 
Ignore me then! :D 
</comment><comment author="danielmitterdorfer" created="2016-01-15T12:39:54Z" id="171950410">Ok, thanks for the clarification.
</comment><comment author="danielmitterdorfer" created="2016-01-18T16:47:11Z" id="172583454">### Design Parameters

After looking at the source, I have identified a few options which I'd like to discuss before implementing this feature. Below are some design parameters which are worth considering from my point of view.

#### Type of endpoint

A (bulk) request can hit the cluster via the following endpoints:
1. HTTP: hint: the REST handler reissues the HTTP request as a transport request via the client interface.
2. Transport (or more precise: remote transport)
3. Local transport: An optimization to avoid the overhead of a remote call when just forwarding a request within the same process.

#### Applicability of the limit

This means whether we want to apply the limit for all types of requests or just for a limited number of explicitly defined request types (like bulk requests).

#### When to check the limit
- Apply during deserialization: This would allow us to abort request handling as soon as we know the limit is hit but at the expense of implementing this for all protocols separately. For the transport protocol, the right place looks like `MessageChannelHandler#handleRequest()`.
- Apply before a (transport) request is handled (i.e. in the appropriate subclass of `TransportAction`): This would simplify the validation as we would just check once for all endpoint types (as we know that HTTP requests are reissued as transport requests). However, at this point in time the request is already deserialized thus consuming memory and all we can do is preserve any additional system resources by aborting request processing.

#### Request size calculation

To know whether a request has (b)reached the limit, we need to calculate its size. Considering `BulkRequest` for now, we have two options on how to determine the request size:
- `BulkRequest#estimatedSize()`: This works but has shortcomings in corner cases (see e.g. #15589)
- If we enforce the limit already during deserialization we can calculate request size exactly, e.g. by writing a custom `StreamInput` implementation that counts the bytes deserialized.

### Proposal

Based on the options above, I want to sketch a simple solution proposal:

Considering that it is likely we want limit checks not only for bulk requests but also similar ones, like multi-gets, we should not tie this too specifically to bulk requests.

Hence, each for each request type that should be size-limited, the corresponding `TransportRequest` class implements a new interface `RequestSizeEstimator` (draft name):

``` java
public interface RequestSizeEstimator {
  int getRequestSizeInBytes();
}
```

We define one configurable request size limit (default e.g. 50MB) for all request types and implement limit breach detection in a high-level fashion as an `ActionFilter`. This `ActionFilter` checks whether the `TransportRequest` implements `RequestSizeEstimator` and checks against this limit. If the limit is breached, we throw an appropriate exception indicating a client error (4xx in HTTP speak).

In summary, the pros and cons of this solution are:

Pros:
- The limit is only checked in a single place in the code base
- Easily extensible: We can add checks for new request types easily by having the corresponding transport request class implement `RequestSizeEstimator`.

Cons:
- We deserialize the full request before we check the limit. This means we still hit a potential memory spike but we avoid using any other resources (CPU, I/O) after that point.
- The size calculation may not be entirely accurate (see my comments above).

Feedback is very much appreciated.
</comment><comment author="danielmitterdorfer" created="2016-01-19T12:51:56Z" id="172844033">Based on feedback I got by @bleskes, here's a revised approach:

We limit the size of requests on protocol level during deserialization. We consider two cases:
- Receiving a request: We will apply one global limit for all requests that are in flight
- Sending a request (internally in the cluster): We will apply a limit for each individual request

Limiting on protocol level has a couple of advantages:
- We cancel request processing as soon as we hit the configured memory limit
- It allows us to introduce a limit per action (not in the scope of this ticket though)

In the first step, we will implement an action-independent limit which applies to all actions not just bulk actions.

Transports that need to be considered are Netty transport and local transport (for testing purposes). HTTP is unaffected because we just stream data from the HTTP layer to the transport layer (no up-front allocation).

We need to check whether circuit breakers are a feasible option to detect limit breaches.
</comment><comment author="danielmitterdorfer" created="2016-01-21T09:01:53Z" id="173505136">@dakrone: Could you share your thoughts whether using `CircuitBreaker` is feasible for limiting memory usage in the following two scenarios?
1. Limiting the memory usage of all in-flight requests on a single node level.
2. Limiting the memory usage of each individual outgoing request of a node.

In both cases the memory usage will be determined by the size of the deserialized request payload.

(more details above; that's just the summary for you)

### Scenario 1

I have seen that there is already a "request" circuit breaker but its `BreakerSettings` are (a) cluster-wide and (b) relative to the amount of available heap memory. I'd tackle this by introducing new `BreakerSettings` for all requests in flight at node level (which would have to be a new value in `org.elasticsearch.common.settings.Settings.Scope`)) and with an absolute but user-configurable limit (e.g. 50mb).

### Scenario 2

We also want to limit memory usage on a single request basis (not across all requests that are in flight) and tbh I think that circuit breakers are not a good fit because we would need a new circuit breaker instance for each request as far as I understand it. However, it would make the implementation more uniform.

It would be great to hear your input on this.
</comment><comment author="dakrone" created="2016-02-01T17:50:43Z" id="178093416">@danielmitterdorfer 

&gt; Could you share your thoughts whether using CircuitBreaker is feasible for
&gt; limiting memory usage in the following two scenarios?
&gt; 
&gt;    Limiting the memory usage of all in-flight requests on a single node level.
&gt;    Limiting the memory usage of each individual outgoing request of a node.
&gt; 
&gt; I have seen that there is already a "request" circuit breaker but its
&gt; BreakerSettings are (a) cluster-wide and (b) relative to the amount of
&gt; available heap memory. I'd tackle this by introducing new BreakerSettings for
&gt; all requests in flight at node level (which would have to be a new value in
&gt; org.elasticsearch.common.settings.Settings.Scope)) and with an absolute but
&gt; user-configurable limit (e.g. 50mb).

This is exactly what the circuit breaker is for, you can easily define a new
breaker (and breaker type) in `HierarchyCircuitBreakerService` to do just that.
I would recommend this heartily :) (and I'm happy to help)

&gt; We also want to limit memory usage on a single request basis (not across all
&gt; requests that are in flight) and tbh I think that circuit breakers are not a
&gt; good fit because we would need a new circuit breaker instance for each request
&gt; as far as I understand it. However, it would make the implementation more
&gt; uniform.

To me, this doesn't sound like a good fit for the circuit breaker (since it is
designed to be per-node, not per-request). Instead, this sounds like a better
fit for the validation framework, since that works on the per-request level.
</comment><comment author="danielmitterdorfer" created="2016-02-02T09:59:01Z" id="178486776">Thanks for your feedback! I'll implement it based on your pointers. Would be great if you could take a look at it then.
</comment><comment author="danielmitterdorfer" created="2016-02-02T15:26:50Z" id="178633361">@dakrone,

I have started implementing request size limiting based on your pointers (see danielmitterdorfer/elasticsearch@802f6ed5f6eeb0ee8d46c294ddf53058aec3157b). I have added a new circuit breaker to `HierarchyCircuitBreakerService` but then fell over `CircuitBreakerServiceIT#testParentChecking()`. The test spuriously hangs and the reason is that the parent circuit breaker tripped and we cannot get any request through at the transport layer afterwards. I am not sure whether we want the dependency among circuit breakers at all in this case. wdyt?

Other things I have noted:
- I am setting an absolute limit for my new circuit breaker but it looks to me that the intention is that all leaf circuit breaker limits add up to 100%, so I think I should reduce the other values accordingly and create a relative setting instead of an absolute one, right?
- I have introduced an adapter interface `Limiter` in my implementation and either use one that delegates to a circuit breaker (for scenario 1; in-flight requests) and one were I manually check the bytes used (scenario 2, per request limiting). I throw a `CircuitBreakingException` in both implementations but for the second case I think it is a bit too implementation specific and does not fit well. I can now either introduce a new exception or rename / move `CircuitBreakingException` to something like `QuotaExceededException` which would fit in both cases.
</comment><comment author="dakrone" created="2016-02-04T18:18:55Z" id="179979142">&gt; The test spuriously hangs and the reason is that the parent circuit breaker
&gt; tripped and we cannot get any request through at the transport layer
&gt; afterwards. I am not sure whether we want the dependency among circuit
&gt; breakers at all in this case. wdyt?

This came up when I added the request breaker on `BigArrays`, because we always
want to be able to allocate a buffer to send an error response! The way that I
worked around it before was splitting it into a circuit-breaking and
non-circuit-breaking category, where the non-circuit-breaking did the
`addWithoutBreaking(numBytes)` so that the usage was still _tracked_, it just
didn't _prevent_ requests from being sent out.

So in your case, I think we may want to limit which requests fall into the
circuit breaking category (index requests, bulk requests, etc), and which don't
(errors, cluster health, cluster state, etc).

What do you think?

&gt; I am setting an absolute limit for my new circuit breaker but it looks to me
&gt; that the intention is that all leaf circuit breaker limits add up to 100%, so
&gt; I think I should reduce the other values accordingly and create a relative
&gt; setting instead of an absolute one, right?

No, they definitely don't have to add up to 100%, that's why the parent is
there. The idea is that you can have something like this:

fielddata: 40%
big_arrays: 30%
requests (your new one): 50mb

parent: 60%

Which allows us to limit individual parts to certain amounts (like absolute
amounts), while still limiting circuit breaking memory across the node on the
whole (the parent breaker).

I definitely think this new breaker should be absolute instead of relative.
You're on the right track with it.

&gt; I have introduced an adapter interface Limiter in my implementation and either
&gt; use one that delegates to a circuit breaker (for scenario 1; in-flight
&gt; requests) and one were I manually check the bytes used (scenario 2, per
&gt; request limiting). I throw a CircuitBreakingException in both implementations
&gt; but for the second case I think it is a bit too implementation specific and
&gt; does not fit well. I can now either introduce a new exception or rename / move
&gt; CircuitBreakingException to something like QuotaExceededException which would
&gt; fit in both cases.

What makes you think it is too implementation specific? You could always
sub-class `CircuitBreakingException` if you wanted it to be a separate type, but
I don't think it's necessarily bad to keep it as such. It is a circuit breaker,
just on the per-request side instead of the node side, so with a good message I
think that can be made clear to the user.
</comment><comment author="bleskes" created="2016-02-04T19:37:01Z" id="180017155">&gt;  because we always
&gt; want to be able to allocate a buffer to send an error response!

I think it makes sense to never circuit break a response (both when sending and receiving). 
</comment><comment author="danielmitterdorfer" created="2016-02-09T13:49:30Z" id="181871168">@dakrone: First of all, thanks for your thoughts.

&gt; So in your case, I think we may want to limit which requests fall into the circuit breaking category (index requests, bulk requests, etc), and which don't (errors, cluster health, cluster state, etc).

With the current implementation this is not as easy as I've added a `LimitingInputStream` and an `LimitingOutputStream` which wraps the actual streams. Therefore, size limiting is transparent to the user. We could make the client aware though and expose an additional method for disabling limit tracking.

Another approach I am thinking of is to decide at creation time of stream based on the action whether it needs to break or not. If yes, we wrap the original one in a limiting stream, otherwise we just leave the stream as is. For me, the most appropriate place to add this support seems to be `TransportRequestOptions`.

For now I have stabilized the test in question by increasing the limit so it still breaks as intended but does not hit the request size limit. I think this is ok given that we need some minimum amount to handle the request at all.

&gt; [The child circuit breaker limits] definitely don't have to add up to 100%
&gt; [...]
&gt; I definitely think this new breaker should be absolute instead of relative.
&gt; You're on the right track with it.

That's great, then I'll leave that part as is. Thanks for the clarification. :)

&gt; &gt; I think it is a bit too implementation specific [to throw a `CircuitBreakingException` in all cases]
&gt; &gt; What makes you think it is too implementation specific?

My thought was that we should throw `CircuitBreakingException` only when a "proper" circuit breaker is involved (as the name of the exception indicates) but I am fine with using `CircuitBreakingException` in the other case as well.

&gt; I think it makes sense to never circuit break a response (both when sending and receiving). 

@bleskes: Thanks for the hint. I have reduced the scope of size limiting to requests only (i.e. `TransportStatus.isRequest(status) == true`).
</comment><comment author="danielmitterdorfer" created="2016-02-11T12:28:33Z" id="182836460">I have pushed another commit on the [feature branch](https://github.com/danielmitterdorfer/elasticsearch/commits/bulk-size-limit). We exhibit the following behaviour when a (bulk) request hits the limit:
- Transport client: `CircuitBreakingException` is thrown
- Node client: We get errors on the individual items of a bulk request (i.e. `BulkItemResponse.isFailure() == true`). This is due to the fact that the node client does not hit the transport layer but starts executing `TransportBulkAction` directly (i.e. in the same process).
- REST API: This bypasses the transport layer so the recommended practice is to use `http.max_content_length`. While I think we could add support in `NettyHttpServerTransport` these two mechanisms are quite similar so I'd avoid reimplementing it there.

I am not too happy that we behave differently depending on the protocol but as this is implemented on (transport) protocol level, it is not much of a surprise. Wdyt @bleskes?
</comment><comment author="Dieken" created="2016-03-05T09:11:52Z" id="192610116">@danielmitterdorfer  I think the http.max_content_length is to limit content length for **any single** http request,  not the total size of in-flight http requests.

I made a patch https://github.com/Dieken/elasticsearch/commit/f2d487eaf1213daa6ad25f13fbccdd1d6b5930f4 against v2.2.0 to limit total size of in-flight HTTP bulk requests as a temporary solution,  because it seems your patch hasn't finished and it may be not merged to 2.x.
</comment><comment author="danielmitterdorfer" created="2016-03-07T07:20:51Z" id="193134632">@Dieken You're right: `http.max_content_length` limits only individual requests so we need to tackle this differently.

Regarding my changes: Support on transport level is finished but we need to iron out some details (property names, default values).

Your patch looks fine for the bulk use case but I just want to raise your awareness of one (minor) thing though: You use `LongAdder` which is a JDK 8 class but Elasticsearch 2.x should be built with JDK 7.
</comment><comment author="bleskes" created="2016-03-07T08:11:28Z" id="193147432">for people following - @danielmitterdorfer and I went through the code together and Daniel is working on another iteration.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add more descriptive pipeline factory errors in _simulate and PUT pipeline/id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16010</link><project id="" key="" /><description>When someone issues the following request to `_simulate`, The pipeline factory will error out while 
attempting to construct the supplied pipeline. The reason for this is that the `date` processor is not configured correctly, `locale` is invalid. The exception is thrown in the processor's `create` here: https://github.com/elastic/elasticsearch/blob/feature/ingest/core/src/main/java/org/elasticsearch/ingest/processor/DateProcessor.java#L130.

``` json
{
            "pipeline": {
              "description": "_description",
              "processors": [
                {
                  "date" : {
                  "processor_tag" : "date_processor",
                  "match_field" : "date_source_field",
                  "target_field" : "date_target_field",
                  "match_formats" : ["dd/MM/yyyy"],
                  "locale": "UNDEFINED"
                  }
                },
                {
                  "set" : {
                    "field" : "field3",
                    "value" : "third_val"
                  }
                }
              ]
            },
            "docs": [
              {
                "_index": "index",
                "_type": "type",
                "_id": "id",
                "_source": {
                  "foo": "bar"
                }
              }
            ]
          }
```

The response from this is an internal server error. This makes it difficult to know which processor was incorrectly configured within the pipeline definition. What would be nicer is a response object that is similar to this:

```
{
 "pipeline_configuration_error": [
   {
     "errors": [
       {
         "processor_tag": "date_processor",
         "error": {
           "root_cause": [
             {
               "type": "illegal_argument_exception",
               "reason": "Invalid language tag specified: UNDEFINED"
             }
           ],
           "type": "illegal_argument_exception",
           "reason": "Invalid language tag specified: UNDEFINED"
         }
       }
     ]
   }
 ]
}
```

It would be great if we can return this processor-level configuration error to all processors that are improperly configured, instead of just the first one. This would require extra logic in the pipline factory to record exceptions and collect them into one large exception instead of throwing the first one it receives.

@martijnvg @javanna what do you think?

@BigFunger, let me know if this is what you meant? The actual response object is still up in the air.
</description><key id="126824627">16010</key><summary>[Ingest] Add more descriptive pipeline factory errors in _simulate and PUT pipeline/id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-15T07:57:07Z</created><updated>2016-01-29T21:44:31Z</updated><resolved>2016-01-29T21:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-15T08:16:47Z" id="171898805">I'm on the fence about returning all errors for misconfigured processors. I prefer to just fail immediately on the first pipeline construction error. This keeps the logic simple. I think that if there are several misconfigurations across multiple processors that having several put pipeline / simulate requests to fix the the pipeline is okay.

I'm +1 on adding more structured information (like tag) to exception we throw so that it easy to figure out what processor has been misconfigured. 
</comment><comment author="javanna" created="2016-01-15T11:02:30Z" id="171933938">I tend to agree with Martijn, let's do this at least in two steps: first make each single error better and descriptive, then see about cumulating errors, which can come later.
</comment><comment author="martijnvg" created="2016-01-15T16:46:05Z" id="172011972">Talked about this during the weekly meeting. There is some confusion because the ingest api does two things: create a temporary pipeline and simulate how documents are processed by this pipeline.

We agreed that we should do the following:
- Errors that occur during pipeline construction should have more context. It should clear what the misconfigured processor is. This should be sufficient for anyone integrating with simulate api.
- Processors that fail badly or with a meaningless error message should be addressed on a case by case basis via separate issues.

On the ingest side we should introduce a PipelineBuildException that encapsulate the context of the failure, so that both the simulate and put pipeline api can render helpful error messages.
</comment><comment author="Bargs" created="2016-01-15T18:06:56Z" id="172038898">Is there currently any documentation for the structure of the error in the JSON response, and what type of errors each endpoint is capable of throwing? That might help us better understand the difference between pipeline build errors and document processing errors, and how we should handle each in Kibana.
</comment><comment author="talevy" created="2016-01-15T21:37:30Z" id="172101176">@Bargs we are still working on beefing up the docs. I'll start writing docs for this now. the error structure that we speak of here is only for the `_simulate`. all other errors will just return an internal http error with an arbitrary exception that is thrown (this is processor dependent and would be documented on a per processor basis). 
</comment><comment author="talevy" created="2016-01-15T21:40:03Z" id="172102098">@martijnvg cool! sounds great. I agree with the two-phase approach that @javanna mentioned. Firstly, we'll set up a `PipelineBuildException` that will provide a more structured representation of the error. I'd imagine this would include structured fields for which field of the config caused the exception, as well as a processor tag, etc.
</comment><comment author="talevy" created="2016-01-26T06:37:18Z" id="174857365">I have started hacking on this here: https://github.com/talevy/elasticsearch/commit/2e351e0d97277b9ae45939f280050ecb72e357fd

I have a very clear idea on how to finish up this commit now. just need to clean up some of the factory code for containing the error and pipeline object. then update `WritePipelineResponse` to reflect the new pipeline factory error response.

didn't get a chance to finish up my thought today, will continue and should finalize it tomorrow!

I was a bit conflicted with where in the pipeline creation we should introduce the concept of `Result`s vs. keeping traditional java exception handling. I played around with pushing it down into the `doCreate` of Processor, but felt it was too soon. so I just pass along the information using `Exception` classes. 
</comment><comment author="talevy" created="2016-01-27T02:41:56Z" id="175355254">I have updated this commit, it is super close I think.

https://github.com/talevy/elasticsearch/commit/07c5981fe4959a7ee393a462cb8fb692b4cb8fa5

a few todos/questions
- the objects holding the exception information are slightly flexible for my liking, and capture too much at times, I am open for changing them. It is just weird since sometimes pipeline errors are due to specific processor configuration fields, or they are general pipeline syntax issues. Mostly left that as a low-hanging-fruit todo item once all the transport/action layers are finalized.
- finishing up the `_simulate` support for this structured response is still incomplete: https://github.com/talevy/elasticsearch/commit/07c5981fe4959a7ee393a462cb8fb692b4cb8fa5#diff-d88dde11b6d6c66bb9acd6bbe7fadecfR61 The idea is to re-use the error object as much as possible without dirtying up the simulation response code we already have
- I have chosen to leave `WritePipelineResponse` as an `AcknowledgedResponse`, this makes the changes there pretty nice I think. All the previous code worked as is since this was the case. I simply override the `AcknowledgedRestListener` to appropriately add the additional structured error fields.

Still slightly rough around the edges. I would love to hear what aspects of ES I am not leveraging and/or suggestions on how this strategy should be changed @martijnvg @javanna.

thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Documentation needs updating</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16009</link><project id="" key="" /><description>After ingest was moved into core, its `docs/plugins/ingest.md` was not updated to reflect some of the changes.

This issue is here to track the progress of documentation for the Ingest Node.
</description><key id="126815592">16009</key><summary>[Ingest] Documentation needs updating</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-01-15T06:29:46Z</created><updated>2016-03-01T12:41:10Z</updated><resolved>2016-03-01T12:41:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-01-25T03:54:30Z" id="174387318">Here are some docs around `on_failure` handling that was missing from the asciidoc

https://github.com/elastic/elasticsearch/pull/16203
</comment><comment author="talevy" created="2016-01-25T20:10:38Z" id="174645492">a PR for some cleanup action items we discussed: https://github.com/elastic/elasticsearch/pull/16216
</comment><comment author="martijnvg" created="2016-01-27T21:17:57Z" id="175858346">@debadair @dedemorton We have updated the docs and added missing information:
- https://github.com/elastic/elasticsearch/blob/master/docs/reference/ingest/ingest.asciidoc
- https://github.com/elastic/elasticsearch/blob/master/docs/plugins/ingest-geoip.asciidoc

The docs are ready for a review.
</comment><comment author="inqueue" created="2016-02-04T04:18:26Z" id="179619819">Updates needed:

Grok 
- `match_fields` should be `fields`
- `match_pattern` should be `pattern`

All pipelines can be listed using `*`.

```
GET _ingest/pipeline/*
```
</comment><comment author="talevy" created="2016-02-04T05:41:28Z" id="179653029">thanks @inqueue, I'll update these tomorrow! I also need to update the failure responses that we introduced to both PUTpipeline and _simulate
</comment><comment author="martijnvg" created="2016-02-29T16:48:00Z" id="190284057">I think the docs are in a good shape now. @debadair I think we can close this issue?
</comment><comment author="dedemorton" created="2016-02-29T18:34:16Z" id="190323808">@martijnvg I still have some changes that I want to make to the documentation this week. I don't need this issue to track the changes, though, so you can close this now if you'd like.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/count docs clarification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16008</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/cat-count.html), we should really state that count does/does not include deleted documents (in ES).
</description><key id="126809587">16008</key><summary>_cat/count docs clarification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2016-01-15T05:20:47Z</created><updated>2016-03-01T12:41:19Z</updated><resolved>2016-03-01T12:41:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Querying by boolean nested value doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16007</link><project id="" key="" /><description>When querying a collection that has a nested type, the following exception gets thrown:

 Exception: {"error":{"root_cause":[{"type":"query_parsing_exception","reason":"[nested] nested object under path [catalogs] is not of nested type","index":"products","line":1,"col":259}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"products","node":"yQ55LabWQiiYmw2bUP041Q","reason":{"type":"query_parsing_exception","reason":"[nested] nested object under path [catalogs] is not of nested type","index":"products","line":1,"col":259}}]},"status":400}

Here is the JSON query:

{"from":0,"size":0,"sort":[{"name.raw":{"order":"asc","ignore_unmapped":true}}],"aggregations":{"manufacturers":{"terms":{"field":"manufacturer.id","size":40000}},"category":{"terms":{"field":"category.id","size":40000}}},"query":{"bool":{"must":[{"nested":{"path":"catalogs","query":{"bool":{"must":[{"match":{"catalogs.id":"55cdeece0a41216c008b4584"}},{"match":{"catalogs.status":"visible"}}]}}}}]}}}

Here is the index mapping:

```
            'catalogs'     =&gt; array(
                'type'       =&gt; 'nested',
                'include_in_parent' =&gt; true,
                'properties' =&gt; array(
                    'id'     =&gt; array('type' =&gt; 'string'),
                    'status' =&gt; array('type' =&gt; 'string')
                )
            )
```
</description><key id="126789329">16007</key><summary>Querying by boolean nested value doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">manioc2015</reporter><labels /><created>2016-01-15T01:45:15Z</created><updated>2016-01-15T12:35:22Z</updated><resolved>2016-01-15T12:35:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T12:35:22Z" id="171949730">Hi @manioc2015 

You have provided only snippets of the example, not a full recreation, so it is difficult to guess exactly where you are going wrong, but here's a recreation that shows that things do work correctly:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "catalogs": {
          "type": "nested",
          "include_in_parent": true,
          "properties": {
            "id": {
              "type": "string"
            },
            "status": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{
  "manufacturer": {
    "id": "xxx"
  },
  "category": {
    "id": "yyy"
  },
  "catalogs": {
    "id": "55cdeece0a41216c008b4584",
    "status": "visible"
  }
}

GET _search
{
  "from": 0,
  "size": 0,
  "sort": [
    {
      "name.raw": {
        "order": "asc",
        "ignore_unmapped": true
      }
    }
  ],
  "aggregations": {
    "manufacturers": {
      "terms": {
        "field": "manufacturer.id",
        "size": 40000
      }
    },
    "category": {
      "terms": {
        "field": "category.id",
        "size": 40000
      }
    }
  },
  "query": {
    "bool": {
      "must": [
        {
          "nested": {
            "path": "catalogs",
            "query": {
              "bool": {
                "must": [
                  {
                    "match": {
                      "catalogs.id": "55cdeece0a41216c008b4584"
                    }
                  },
                  {
                    "match": {
                      "catalogs.status": "visible"
                    }
                  }
                ]
              }
            }
          }
        }
      ]
    }
  }
}
```

It sounds like you have something incorrect, rather than there being a bug. Please ask these questions (with more details, and the version of ES that you're using) in the forums: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shadow Replicas require path.shared_data on every node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16006</link><project id="" key="" /><description>Instead of requiring the setting to exist on just the data nodes, it is required to be set on every node.

https://github.com/elastic/elasticsearch/blob/c50b22f95f48119213156bdb634925e1be91b794/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java#L465

``` java
String customPath = settings.get(IndexMetaData.SETTING_DATA_PATH, null);
List&lt;String&gt; validationErrors = new ArrayList&lt;&gt;();

if (customPath != null &amp;&amp; env.sharedDataFile() == null) {
    validationErrors.add("path.shared_data must be set in order to use custom data paths");
}
```

The code just checks for the lack of existence for the relevant settings, but it doesn't confirm that _it_ is a data node before causing a validation error.
</description><key id="126770296">16006</key><summary>Shadow Replicas require path.shared_data on every node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Shadow Replicas</label><label>bug</label></labels><created>2016-01-14T23:13:40Z</created><updated>2017-05-26T18:53:52Z</updated><resolved>2017-05-26T18:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2017-05-26T18:53:52Z" id="304361576">Shadow replicas have been removed and this is no longer applicable</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Clarify Shadow replica setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16005</link><project id="" key="" /><description>Clarifying that the path setting is required on every _data_ node, rather than _every_ node.

@dakrone Want to take a quick look?

Fixes #16004
</description><key id="126753071">16005</key><summary>[DOCS] Clarify Shadow replica setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Shadow Replicas</label><label>docs</label><label>enhancement</label></labels><created>2016-01-14T21:34:45Z</created><updated>2016-01-15T19:31:06Z</updated><resolved>2016-01-14T21:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-14T21:35:51Z" id="171787182">LGTM
</comment><comment author="pickypg" created="2016-01-15T19:31:06Z" id="172061619">This change was reverted because it was decided to require it for each node. If #16006 results in a change, then this should be redone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Clarify Shadow Replicas path required by data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16004</link><project id="" key="" /><description>Currently the docs describe the shared file system path as:

&gt; path is a shared filesystem that must be available on every node in the Elasticsearch cluster

However, in reality, it must only be visible to every data node.

PR incoming.
</description><key id="126751361">16004</key><summary>[DOCS] Clarify Shadow Replicas path required by data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>docs</label></labels><created>2016-01-14T21:25:21Z</created><updated>2016-01-15T19:30:41Z</updated><resolved>2016-01-14T21:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-01-15T19:30:41Z" id="172061522">This change was reversed because it was decided to require it for each node. If #16006 results in a change, then this should be redone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid JSON from mapped index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16003</link><project id="" key="" /><description>We were pushing geodata to an recently mapped experimental index with `uuid` as a `string`, and `location` as a `geo_point` and we received the following response. The response in invalid JSON. Any ideas what is going on?

```
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "experiment",
      "_type" : "heartbeat",
      "_id" : "AVIyEFGLc78HWEmcNWkB",
      "_score" : 1.0,
      "_source":{
    "uuid": "C5115D54-3EB0-409C-86C4-B36584F9D352",
    "location": [-111.6319810041340, 33.23192642539100]
  }
}

    }, {
      "_index" : "experiment",
      "_type" : "heartbeat",
      "_id" : "AVIyEDjZc78HWEmcNWkA",
      "_score" : 1.0,
      "_source":{
    "uuid": "C5115D54-3EB0-409C-86C4-B36584F9D352",
    "location": [-111.6319810041335, 33.23192642539100]
  }
}

    }, {
      "_index" : "experiment",
      "_type" : "heartbeat",
      "_id" : "AVIyEACJc78HWEmcNWj_",
      "_score" : 1.0,
      "_source":{
    "uuid": "C5115D54-3EB0-409C-86C4-B36584F9D352",
    "location": [-111.6319810041335, 33.23192642539109]
  }
}

    } ]
  }
} 

```
</description><key id="126745894">16003</key><summary>Invalid JSON from mapped index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">elbow-jason</reporter><labels /><created>2016-01-14T20:54:09Z</created><updated>2016-01-15T11:38:55Z</updated><resolved>2016-01-15T11:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-01-14T22:43:14Z" id="171805375">Can you provide a mapping &amp; set of documents that can reproduce this?
</comment><comment author="elbow-jason" created="2016-01-15T00:00:39Z" id="171821877">the mapping from `/experiment/heartbeat`:

```
{
  "experiment" : {
    "mappings" : {
      "heartbeat" : {
        "properties" : {
          "location" : {
            "type" : "geo_point"
          },
          "uuid" : {
            "type" : "string"
          }
        }
      }
    }
  }
}
```
</comment><comment author="elbow-jason" created="2016-01-15T00:02:13Z" id="171822155">and the index/root info:

```
{
  "status" : 200,
  "name" : "Wyatt Wingfoot",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.5.2",
    "build_hash" : "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    "build_timestamp" : "2015-04-27T09:21:06Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="eskibars" created="2016-01-15T00:28:17Z" id="171828421">Can you provide a full dump of an example query+response that fails using `curl`?
</comment><comment author="elbow-jason" created="2016-01-15T02:31:49Z" id="171850662">using `curl`

```
elbow-jason@Jasons-MacBook-Pro-1781 ~/g/s/g/a/dashboard&gt;
curl -XGET 'http://super.secret.host.com:9200/experiment/heartbeat/_search'
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":3,"max_score":1.0,"hits":[{"_index":"experiment","_type":"heartbeat","_id":"AVIyEFGLc78HWEmcNWkB","_score":1.0,"_source":{
    "uuid": "C5115D54-3EB0-409C-86C4-B36584F9D352",
    "location": [-111.6319810041340, 33.23192642539100]
  }
}
},{"_index":"experiment","_type":"heartbeat","_id":"AVIyEDjZc78HWEmcNWkA","_score":1.0,"_source":{
    "uuid": "C5115D54-3EB0-409C-86C4-B36584F9D352",
    "location": [-111.6319810041335, 33.23192642539100]
  }
}
},{"_index":"experiment","_type":"heartbeat","_id":"AVIyEACJc78HWEmcNWj_","_score":1.0,"_source":{
    "uuid": "C5115D54-3EB0-409C-86C4-B36584F9D352",
    "location": [-111.6319810041335, 33.23192642539109]
  }
}
}]}}
```
</comment><comment author="clintongormley" created="2016-01-15T11:38:55Z" id="171940245">Before 2.0, Elasticsearch would accept invalid JSON without complaining, and just store it in the `_source` field.  Then, when returning the `_source`, you'd get back the same invalid JSON that you had indexed.

This looks like what happened here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating Child Mapping with new field fails with NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16002</link><project id="" key="" /><description>I am running Elasticsearch 2.1.1. I have an Index which is made of a single parent type and multiple child types. I am trying to update a Child mapping by adding a new field. This results in a NullPointerException on the server:

[2016-01-14 10:48:32,354][DEBUG][action.admin.indices.mapping.put] [Philip Fetter] failed to put mappings on indices [[oha_patient_registry]], type [encounters]
java.lang.NullPointerException
at org.elasticsearch.index.mapper.MappedFieldType.checkCompatibility(MappedFieldType.java:246)
at org.elasticsearch.index.mapper.internal.ParentFieldMapper.merge(ParentFieldMapper.java:391)
at org.elasticsearch.index.mapper.Mapping.merge(Mapping.java:113)
at org.elasticsearch.index.mapper.DocumentMapper.merge(DocumentMapper.java:396)
at org.elasticsearch.cluster.metadata.MetaDataMappingService$2.execute(MetaDataMappingService.java:389)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

In Sense the error is displayed as:

{
"error": {
"root_cause": [
{
"type": "null_pointer_exception",
"reason": null
}
],
"type": "null_pointer_exception",
"reason": null
},
"status": 500
}

Now it is possible that updating Child mappings is not supported, although I didn't get that from the doc...  Or I am doing something stupid (which is always highly possible). That being said, a more descriptive error (not a NPE) would be expected. Thoughts?

Jeff
</description><key id="126745642">16002</key><summary>Updating Child Mapping with new field fails with NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">turp1twin</reporter><labels /><created>2016-01-14T20:52:53Z</created><updated>2016-01-15T11:35:57Z</updated><resolved>2016-01-15T11:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-15T11:35:57Z" id="171939611">This can be worked around by specifying the `_parent` mapping again in the update-mapping request.

In 2.2 this no longer throws an NPE, but it still doesn't work correctly (see https://github.com/elastic/elasticsearch/issues/15997#issuecomment-171938807)

Closing in favour of https://github.com/elastic/elasticsearch/issues/15997
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip capturing least/most FS info for an FS with no total</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16001</link><project id="" key="" /><description>If an operating system reports -1 for the total bytes of a filesystem
path, we should ignore it when capturing the least and most available
statistics.

Relates to #15919
</description><key id="126745066">16001</key><summary>Skip capturing least/most FS info for an FS with no total</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T20:50:19Z</created><updated>2016-01-15T20:31:20Z</updated><resolved>2016-01-15T20:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-14T21:08:25Z" id="171780210">the logic looks fine for this, but we may want to add a test for negative free (but not total). I know this can happen on freebsd.
</comment><comment author="dakrone" created="2016-01-14T21:15:09Z" id="171781891">Pushed another commit that tests when 'free' is negative but total is not.
</comment><comment author="rmuir" created="2016-01-14T21:26:14Z" id="171784760">thanks!
</comment><comment author="dakrone" created="2016-01-15T20:31:20Z" id="172085165">I merged this into master and 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor problems in the configuration doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16000</link><project id="" key="" /><description>`/_nodes/process` won't show file descriptors. should be `/_nodes/stats/process`
</description><key id="126728341">16000</key><summary>Fix minor problems in the configuration doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">FreeTymeKiyan</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T19:22:25Z</created><updated>2016-01-14T19:35:30Z</updated><resolved>2016-01-14T19:35:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-14T19:29:40Z" id="171752480">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update configuration.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15999</link><project id="" key="" /><description>Minor problems in the doc
`/_nodes/process` won't show file descriptors. should be `/_nodes/stats/process`
</description><key id="126725908">15999</key><summary>Update configuration.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FreeTymeKiyan</reporter><labels /><created>2016-01-14T19:13:36Z</created><updated>2016-01-14T19:19:25Z</updated><resolved>2016-01-14T19:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Filter(s) aggregation should create weights only once.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15998</link><project id="" key="" /><description>We have a performance bug that if a filter aggregation is below a terms
aggregation that has a cardinality of 1000, we will call Query.createWeight
1000 times as well. However, Query.createWeight can be a costly operation.
For instance in the case of a TermQuery it will seek the term in every
segment. Instead, we should create the Weight once, and then get as many
iterators as we need from this Weight.

I found this problem while trying to diagnose a performance regression while
upgrading from 1.7 to 2.1[1]. While the problem was not introduced in 2.x, the
fact that 1.7 cached very aggressively had hidden this problem, since you don't
need to seek the term anymore on a cached TermFilter.

Doing things once for every aggregator is not easy with the current API but
I discussed this with Colin and Aggregator factories will need to get an init
method for different reasons, where we will be able to put these steps that
need to be performed only once, no matter haw many aggregators need to be
created.

[1] https://discuss.elastic.co/t/aggregations-in-2-1-0-much-slower-than-1-6-0/38056/26
</description><key id="126717694">15998</key><summary>Filter(s) aggregation should create weights only once.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T18:29:49Z</created><updated>2016-01-17T12:13:21Z</updated><resolved>2016-01-15T09:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-14T18:57:49Z" id="171738653">LGTM
</comment><comment author="jpountz" created="2016-01-14T22:38:57Z" id="171804450">Any opinions about whether to backport to 2.2?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disabling _source prevents any further updates to mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15997</link><project id="" key="" /><description>It seems that if you disable the `_source` when first creating the index, and subsequent updates to the mapping will fail with `Merge failed with failures {[Cannot update enabled setting for [_source]]}`, even if the update doesn't change `_source` at all.

Tested on ES 2.1

``` json
{
   "name": "Domo",
   "cluster_name": "elasticsearch",
   "version": {
      "number": "2.1.0",
      "build_hash": "72cd1f1a3eee09505e036106146dc1949dc5dc87",
      "build_timestamp": "2015-11-18T22:40:03Z",
      "build_snapshot": false,
      "lucene_version": "5.3.1"
   },
   "tagline": "You Know, for Search"
}
```
### Example

``` js
DELETE /data

PUT /data
{
    "mappings": {
        "data": {
            "_source": {
                "enabled": false
            },
            "properties": {
                "float": {
                    "type": "float"
                },
                "double": {
                    "type": "double"
                }
            }
        }
    }
}

GET /data/_mapping
```

``` json
{
   "data": {
      "mappings": {
         "data": {
            "_source": {
               "enabled": false
            },
            "properties": {
               "double": {
                  "type": "double"
               },
               "float": {
                  "type": "float"
               }
            }
         }
      }
   }
}
```

All good, but if we try to update the mapping:

``` js
PUT /data/_mapping/data
{
   "properties": {
      "long": {
         "type": "long"
      }
   }
}
```

``` json
{
   "error": {
      "root_cause": [
         {
            "type": "merge_mapping_exception",
            "reason": "Merge failed with failures {[Cannot update enabled setting for [_source]]}"
         }
      ],
      "type": "merge_mapping_exception",
      "reason": "Merge failed with failures {[Cannot update enabled setting for [_source]]}"
   },
   "status": 400
}
```

If you repeat the process without touching `_source`, everything works as expected:

``` js
DELETE /data

PUT /data
{
    "mappings": {
        "data": {
            "properties": {
                "float": {
                    "type": "float"
                },
                "double": {
                    "type": "double"
                }
            }
        }
    }
}

GET /data/_mapping

PUT /data/_mapping/data
{
   "properties": {
      "long": {
         "type": "long"
      }
   }
}

GET /data/_mapping
```

``` json
{
   "data": {
      "mappings": {
         "data": {
            "properties": {
               "double": {
                  "type": "double"
               },
               "float": {
                  "type": "float"
               },
               "long": {
                  "type": "long"
               }
            }
         }
      }
   }
}
```

And it doesn't seem to matter if you enable or disable _source, both cases will cause the error.  It seems to be the presence of specifying _source that breaks future updates.
</description><key id="126715121">15997</key><summary>Disabling _source prevents any further updates to mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2016-01-14T18:18:22Z</created><updated>2016-01-19T08:28:04Z</updated><resolved>2016-01-19T08:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-01-14T18:20:03Z" id="171728494">I should note, if you explicitly set `_source` in your update it works as expected:

``` js
PUT /data/_mapping/data
{
   "_source": {
      "enabled": false
   },
   "properties": {
      "long": {
         "type": "long"
      }
   }
}
```
</comment><comment author="rjernst" created="2016-01-14T20:12:41Z" id="171766611">It looks to me like this would also be an issue with some other metadata mappers, eg `_timestamp`. The issue is currently when initializing the builder for a type (eg when parsing mappings), we use the current fieldtype for each current metadata mapper to get a metadata mapper for that specific type (so that by default it will match the metadata mapper used by other document types). However, we really need the entire metadata mapper, since there are settings like this that are not part of the field type. Also, SourceFieldMapper does not even use the existing field type (although that does not matter in this case).
</comment><comment author="clintongormley" created="2016-01-15T11:32:42Z" id="171938807">Same thing goes for `_parent`:

```
PUT t 
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT t/_mapping/child
{
  "properties": {}
}
```

Returns:

```
{
  "error": {
    "root_cause": [
      {
        "type": "illegal_argument_exception",
        "reason": "The _parent field's type option can't be changed: [parent]-&gt;[null]"
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "The _parent field's type option can't be changed: [parent]-&gt;[null]"
  },
  "status": 400
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>French phonetic filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15996</link><project id="" key="" /><description>French phonetic filter based on French pronunciation.
</description><key id="126711788">15996</key><summary>French phonetic filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">hcapitaine</reporter><labels><label>:Analysis</label><label>discuss</label><label>feature</label></labels><created>2016-01-14T17:59:25Z</created><updated>2016-06-24T09:57:27Z</updated><resolved>2016-06-24T09:57:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-14T19:35:43Z" id="171754368">Hi @hcapitaine! 

Could you sign the CLA?

I'm also wondering if this should be contributed to Lucene? @jpountz WDYT?
</comment><comment author="hcapitaine" created="2016-01-14T19:48:14Z" id="171758032">Hi David,

I already signed the cla and even received a receipt but it didn't
correctly worked after that.

On Thu, Jan 14, 2016 at 8:36 PM, David Pilato notifications@github.com
wrote:

&gt; Hi @hcapitaine https://github.com/hcapitaine!
&gt; 
&gt; Could you sign the CLA?
&gt; 
&gt; I'm also wondering if this should be contributed to Lucene? @jpountz
&gt; https://github.com/jpountz WDYT?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/15996#issuecomment-171754368
&gt; .
</comment><comment author="karmi" created="2016-01-15T09:04:33Z" id="171906569">@hcapitaine Seems like you have signed with a different e-mail then is used in your Git commit (and possibly on record in your Github profile). If you add your Gmail e-mail address to Github, it should work next time we check the records -- that's the easiest way. Ping me if you need more help!
</comment><comment author="hcapitaine" created="2016-01-15T09:21:00Z" id="171910474">Ok you're totally right. I added the email I commit with in my github
account. It should be ok now.

On Fri, Jan 15, 2016 at 10:05 AM, Karel Minarik notifications@github.com
wrote:

&gt; @hcapitaine https://github.com/hcapitaine Seems like you have signed
&gt; with a different e-mail then is used in your Git commit (and possibly on
&gt; record in your Github profile). If you add your Gmail e-mail address to
&gt; Github, it should work next time we check the records -- that's the easiest
&gt; way. Ping me if you need more help!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/15996#issuecomment-171906569
&gt; .
</comment><comment author="karmi" created="2016-01-15T09:44:00Z" id="171914504">@hcapitaine For some reason the check still fails, even when I re-trigger all the checks... We'll try to look into it! But we have received your signature properly, thanks!

@dadoonet Can you process the pull request? I think we can consider Harold as properly signed up for CLA.
</comment><comment author="ypereirareis" created="2016-03-03T09:39:01Z" id="191679691">Hi @hcapitaine !

How to use your work/plugin right now with elasticsearch ?

Thx :smile: 
</comment><comment author="karmi" created="2016-03-03T09:49:54Z" id="191682627">Hi @hcapitaine, I've tried to re-trigger a check for this particular PR to better debug this situation, but it still fails. Are you sure you've added both of your e-mails (gmail.com and galerieslafayette.com) to your Github profile? You've signed with your Gmail, and commited with your work mail. In theory, Github should match these e-mails with your Github profile, and it should work on our side. Can you please verify that both of these e-mails are attached to your Github profile?
</comment><comment author="hcapitaine" created="2016-03-03T12:32:47Z" id="191743568">Hi @karmi, 

my galeries address is now added to my account. It should work right now.
</comment><comment author="hcapitaine" created="2016-03-03T13:02:05Z" id="191752414">Hi @ypereirareis,

My work is not really tied to a specific version of elasticsearch except for the tests. Maybe I could do some others PR to include it in other versions of ES. @dadoonet what is your policy for backport?
</comment><comment author="hcapitaine" created="2016-03-03T13:31:13Z" id="191762097">@ypereirareis 
If you want to use it right now, you can checkout specific version of ES that you want. Apply diffs except integration test part at the same path in the code and it should work. Compile and use the generated plugin in your ES cluster.
</comment><comment author="karmi" created="2016-03-03T15:25:01Z" id="191813557">@hcapitaine Thanks!, I'm super-glad that it works as it should -- ie. since the e-mail is attached to Github, we can match you to the signature, and your comment triggers the check again.
</comment><comment author="clintongormley" created="2016-05-07T14:42:51Z" id="217641804">@rmuir would you mind taking a look at this?  Should this be submitted to Lucene instead?
</comment><comment author="jpountz" created="2016-06-24T09:57:25Z" id="228306327">We just discussed it in Fixit Friday. The analysis algorithms that we expose are usually backed by standards (eg. for normalization) or research papers that try to prove the quality of the algorithm. Since this one does not appear to have one, we would rather like it be remain a community plugin, which could then be listed at https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis.html#_community_contributed_analysis_plugins (just open a PR that edits `docs/plugins/analysis.asciidoc` when you publish it).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix calculation of age of pending tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15995</link><project id="" key="" /><description>This commit addresses a time unit conversion bug in calculating the age
of a PrioritizedRunnable. The issue was an incorrect conversion from
nanoseconds to milliseconds as instead the conversion was to
microseconds. This leads to the timeInQueue metric for pending tasks to
be off by three orders of magnitude.

Closes #15988
</description><key id="126699349">15995</key><summary>Fix calculation of age of pending tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T17:01:23Z</created><updated>2016-01-15T13:01:53Z</updated><resolved>2016-01-15T13:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-14T23:47:32Z" id="171819633">LGTM
</comment><comment author="bleskes" created="2016-01-15T06:52:03Z" id="171885277">LGTM2. Good catch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slowdown on date range queries in ES 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15994</link><project id="" key="" /><description>On a dataset of +/- 30M  (unoptimized / so lot's of segments)
The following query executes faster a lot faster :

``` json
curl -XGET "http://localhost:9200/users/person/_search" -d '
    {
        "from": 0,
        "size": 20,
        "_source": false,
        "query": {
            "constant_score": {
                "filter": {
                    "bool": {
                        "must": [
                            {
                                "term": {
                                    "gender": "female"
                                }
                            },
                            {
                                "terms": {
                                    "location.geohash": [
                                        "u143",
                                        "u149",
                                        "u14c",
                                        "u146",
                                        "u14d",
                                        "u14f",
                                        "u147",
                                        "u14e",
                                        "u14g"
                                    ]
                                }
                            },
                            {
                                "term": {
                                    "accountstatus": "active"
                                }
                            },
                            {
                                "terms": {
                                    "country": [
                                        "be"
                                    ]
                                }
                            }
                        ]
                    }
                }
            }
        },
        "post_filter": { 
            "range": {
                "birthdate": {
                    "gte": "1981-01-13",
                    "lte": "1997-01-13"
                }
            }
        }
    }   
    '
```

&lt; 110ms, consecutive runs, 
{"took":109,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":2454,"max_score":1.0

vs. this is one is always : 300ms to 400ms +

``` json
curl -XGET "http://localhost:9200/users/person/_search" -d '
  {
        "from": 0,
        "size": 20,
        "_source": false,
        "query": {
            "constant_score": {
                "filter": {
                    "bool": {
                        "must": [
                            {
                                "term": {
                                    "gender": "female"
                                }
                            },
                            {
                                "terms": {
                                    "location.geohash": [
                                        "u143",
                                        "u149",
                                        "u14c",
                                        "u146",
                                        "u14d",
                                        "u14f",
                                        "u147",
                                        "u14e",
                                        "u14g"
                                    ]
                                }
                            },
                            {
                                "term": {
                                    "accountstatus": "active"
                                }
                            },
                            {
                                "terms": {
                                    "country": [
                                        "be"
                                    ]
                                }
                            },
                            {
                                "range": {
                                    "birthdate": {
                                        "gte": "1981-01-13",
                                        "lte": "1997-01-13"
                                    }
                                }
                            }
                        ]
                    }
                }
            }
        }
    }
    '
```

{"took":304,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":2446,"max_score"

If this expected behaviour, please close the issue - 
But I think that ES should analyse the query and know when to do post filtering instead of generating a lot of lucene terms for date fields that slow down queries drastically. 
</description><key id="126697108">15994</key><summary>Slowdown on date range queries in ES 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">jrots</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-01-14T16:53:29Z</created><updated>2017-06-28T06:33:16Z</updated><resolved>2016-03-01T12:34:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T17:00:36Z" id="171704261">What version of ES?
How many times did you run it?
How many segments do you have?
What hardware are you running this on? (all your search times seem very slow for such a small index)
</comment><comment author="jrots" created="2016-01-14T17:15:44Z" id="171710099">- ES:  2.1.1 - Lucene 5.3.1,  -Xms28g -Xmx28g mlockall = true
- 6 times consecutively for each query
- Intel(R) Xeon(R) CPU           X5650  @ 2.67GHz, 2 physical cpu's, 12 cores w hyperthreading, 24 processors in total
- 64GB ram, 1TB SSD 
- segments : https://gist.github.com/jrots/d7020441bf33dca4b67a , 4 shards with 30 segments on each shard
</comment><comment author="jimczi" created="2016-01-14T18:05:06Z" id="171724590">And what is the average time when you remove the range filter/post_filter ? The post_filter vs filter are very similar in terms of implementation, the post_filter has to scan the inverted lists also. 
</comment><comment author="clintongormley" created="2016-01-15T11:23:43Z" id="171937435">@jrots It seems there may be a misunderstanding of what the `post_filter` does in Elasticsearch.  The post filter is simply a filter that is applied to the query results AFTER aggregations have been calculated - it has nothing to do with executing the filter via the inverted index vs fielddata.

That said, something seems odd about how the query is executed - if the range query is the most costly, the bool query should figure that out and change the execution order to apply the terms queries first.  We'll investigate.

Two questions:
- Is the `birthdate` field mapped as a string or as a date?
- Why do the number of results differ for the two queries? Are you actively indexing?
</comment><comment author="jimczi" created="2016-01-15T16:05:04Z" id="172001365">&gt; Why do the number of results differ for the two queries? Are you actively indexing?

@clintongormley I guess that the number of results differs because the post-filtered documents are not taken into account in the total hits. I opened https://github.com/elastic/elasticsearch/issues/16021 because I don't know if it's expected or not.
</comment><comment author="clintongormley" created="2016-01-15T16:44:59Z" id="172011690">@jimferenczi no - these counts should be the same whether post_filter is used or not.  post_filter just affects which documents the aggs see.
</comment><comment author="jrots" created="2016-01-15T17:03:34Z" id="172016772">I was still actively indexing at that moment - now that's done and I've also ran: force merge max_segments=1 to be sure that we're measuring the right things here. 

with range on birthdate : 

``` json
{"took":872,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":18892}}
{"took":875,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":18892}}
{"took":109,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":18892}}
```

If I change the range of the birthdate with one day f.e. it's again consecutive 800ms+ 

leaving out the birthdate completely : 

``` json
{"took":37,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":39062}}
{"took":32,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":39062}}
```

etc.. 
even changing some of the params in the query, 

running with range on birthdate in post_filter,initial + consecutive 

``` json
{"took":401,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":18892}}
{"took":54,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":18892}}
{"took":53,"timed_out":false,"_shards":{"total":4,"successful":4,"failed":0},"hits":{"total":18892}}
```

birthdate is stored as : 

``` json
          "birthdate" : {
            "type" : "date",
            "format" : "YYYY-MM-dd"
          },
```

Wonder if I don't need time precision on my dates (like a birthdate f.e.), 
if I'm not better of storing these dates as a fixed int number =&gt; floor(( (epochtime) - (timestamp birthdate) )/60/60/24)
This way it would generate less terms in the lucene index, the precision_step would also be 8 in that case, delivering faster responses. 
</comment><comment author="clintongormley" created="2016-01-15T17:14:58Z" id="172019754">&gt; I was still actively indexing at that moment

OK that explains the varying counts.

&gt; Wonder if I don't need time precision on my dates (like a birthdate f.e.), if I'm not better of storing these dates as a fixed int number =&gt; floor(( (epochtime)  (timestamp birthdate) )/60/60/24)

Before going there... query execution order has had a big rewrite, and it sounds like it is choosing a suboptimal order.  @jimferenczi is going to dive into it to see if he can find something that can be improved.

thanks for all the info
</comment><comment author="jimczi" created="2016-01-15T20:36:06Z" id="172086242">edited version of the comment which was wrong and misleading.
@jrots I'll try to reproduce.
</comment><comment author="jimczi" created="2016-01-18T15:00:21Z" id="172552757">@jrots I've tried to reproduce with a 40M documents index, 5 shards, each document contain one date field with a random date chosen between "1950-01-01" to "2000-12-31" and two long fields with different cardinalities (one big and one small).
 I compared the post_filter+boolean query vs one big boolean query and the results are always the same, there is no gain with the post_filter. I checked with different type of range queries (big range like 1950 =&gt; 2000 and small ones like your example).  
When you test the performance you can have some variations depending on the activity of your machine in terms of cpu and ram. Are you sure that you stopped the indexing and the queries when you checked the performance of each query ? Can you try to re-run your tests with more than 3 trials per query ?
</comment><comment author="clintongormley" created="2016-01-18T18:11:05Z" id="172609890">@jrots could you provide a more complete recreation? or would it be possible to upload your index somewhere so that we could try it out?
</comment><comment author="jrots" created="2016-01-19T19:45:43Z" id="172964504">Ok I'll have a look to have a testable index that doesn't contain too much private info
</comment><comment author="jrots" created="2016-02-01T10:07:38Z" id="177888722">Hi guys, 
sorry for the late reply.I retried the post_filter and filter on date ranges and it indeed have the same results.. (both quite slow to be honest). Must've been the indexing I was doing at that time that caused the fluctuation in search response times. 

I think if you have a lot of documents and are doing a wide numeric range there can be a big slowdown. .

I fixed my problem by rewriting the following range query  :

```
                    "range": {
                                "birthdate": {
                                    "gte": "1967-01-28",
                                    "lte": "1996-01-28"
                                }
                            }
```

to : 

```
"filter": {
                "bool": {
                    "should": [
                        {
                            "range": {
                                "birthdate": {
                                    "gte": "1967-01-28",
                                    "lte": "1967-12-31"
                                }
                            }
                        },
                        {
                            "range": {
                                "birthdate": {
                                    "gte": "1996-01-01",
                                    "lte": "1996-01-28"
                                }
                            }
                        },
                        {
                            "terms": {
                                "country_birthdate_string": [
                                    "be1-1968",
                                    "be1-1969",
                                    "be0-1970",
                                    "be0-1980",
                                    "be1-1990",
                                    "be1-1991",
                                    "be1-1992",
                                    "be1-1993",
                                    "be1-1994",
                                    "be1-1995"
                                ]
                            }
                        }
```

&lt;country&gt;0_&lt;year rounded 0&gt; =&gt; spans 10 years
&lt;country&gt;1 =&gt; spans 1 year

I added a transform to the index mapping that generates these keywords automatically for me: 

```
    "mappings" : {
      "person" : {
        "transform" : {
          "inline" : "ctx._source[\"country_birthdate_string\"] = [ctx._source[\"country\"] + \"0_\"+(int)Math.floor(Integer.parseInt(ctx._source[\"birthdate\"].substring(0,4))/10)*10, ctx._source[\"country\"] + \"1_\"+ctx._source[\"birthdate\"].substring(0,4)];",
          "lang" : "groovy"
        },
        "_all" : {
          "enabled" : false
        },
        "properties" : {
          "birthdate" : {
            "type" : "date",
            "format" : "YYYY-MM-dd"
          },
          "country" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "country_birthdate_string" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
```

I have a speed up times 10 doing it like this for my use case. 
Maybe worth adding something like this higher up in lucene internal/ revisit the numeric tree lookup for large ranges. . 
Regards
J
</comment><comment author="clintongormley" created="2016-03-01T12:34:33Z" id="190704428">The new point field encoding coming in 5.0 should improve this situation.  Closing
</comment><comment author="ChristopheBoucaut" created="2017-04-06T10:19:06Z" id="292131208">Hi @jrots.

Did you test your use case with ES 5.* ? Can you confirm the improve ?

I have the same issue with ES 5.3 and I don't found a "native solution" in ES.

Regards.</comment><comment author="tvernum" created="2017-06-28T06:03:36Z" id="311564782">@hwb1992 Please ask this question on our [discuss forum](https://discuss.elastic.co/) where we can provide better support.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation: Missing quotes in quoted code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15993</link><project id="" key="" /><description>The documented syntax is incorrect, it appears to be missing " " around the age which causes an error.

I propose to change:

  "doc": { "name": "Jane Doe", "age": 20 }
to
  "doc": { "name": "Jane Doe", "age": "20" }

I hope this helps

McParty
</description><key id="126694156">15993</key><summary>Documentation: Missing quotes in quoted code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sarlacpit</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2016-01-14T16:41:44Z</created><updated>2016-01-15T12:30:24Z</updated><resolved>2016-01-15T11:28:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T16:57:51Z" id="171702988">@sarlacpit Why do you think the age should be quoted? it's valid as it is, unless I'm missing something?
</comment><comment author="sarlacpit" created="2016-01-14T18:29:36Z" id="171731608">Hi Clinton,

Thank you for the response.

I am embarrassed to say, the error is mine. I am sorry to have wasted everyone's time with this.
I was getting an error and when I quoted the 20 the error disappeared - I tried it several times.

Not being a 'programmer', as such, I am assuming the " " implies a string as opposed to a integer? if so would you consider some brief explanation  as to the different data types being used?

Many Thanks
</comment><comment author="clintongormley" created="2016-01-15T11:28:44Z" id="171938204">@sarlacpit this section of the definitive guide may help you: https://www.elastic.co/guide/en/elasticsearch/guide/current/mapping-intro.html
</comment><comment author="sarlacpit" created="2016-01-15T12:30:23Z" id="171949018">Thanks :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Cleanup initialization code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15992</link><project id="" key="" /><description>- Folded IngestModule into NodeModule
- Renamed IngestBootstrapper to IngestService
- Let NodeService construct IngestService and removed the Guice annotations
- Let IngestService implement Closable
</description><key id="126680580">15992</key><summary>[Ingest] Cleanup initialization code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-14T15:41:20Z</created><updated>2016-01-15T12:35:39Z</updated><resolved>2016-01-15T12:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-14T16:51:25Z" id="171699474">left a few comments, looks very good to me already though
</comment><comment author="martijnvg" created="2016-01-15T09:28:58Z" id="171911918">@javanna I've renamed the variables.
</comment><comment author="javanna" created="2016-01-15T11:01:01Z" id="171933678">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>move grok patterns loading out of GrokProcessorFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15991</link><project id="" key="" /><description>also added ingest-geoip entries where missing in the plugin manager as it wil become an official plugin.
</description><key id="126677924">15991</key><summary>move grok patterns loading out of GrokProcessorFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-14T15:28:26Z</created><updated>2016-01-14T16:25:30Z</updated><resolved>2016-01-14T16:25:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-14T16:00:33Z" id="171682465">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check cluster health in integration test wait condition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15990</link><project id="" key="" /><description>With this commit we do not check only if an endpoint is up but
we also check that the cluster status is green. Previously,
builds sporadically failed to pass this condition.

@rjernst could you please have a look at this?
</description><key id="126671284">15990</key><summary>Check cluster health in integration test wait condition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T15:01:34Z</created><updated>2016-01-18T07:46:49Z</updated><resolved>2016-01-15T08:15:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-14T18:19:26Z" id="171728343">I think we can do this more simply, similar to how it was in 2.x?

We can change this in the current code:
`ant.get(src: "http://${node.httpUri()}",`

to this:
`ant.get(src: "http://${node.httpUri()}/_cluster/health?wait_for_nodes=${node.numNodes}",`
</comment><comment author="danielmitterdorfer" created="2016-01-15T07:57:51Z" id="171896150">@rjernst: Ah, much simpler. I'd also add `wait_for_status=green`.
</comment><comment author="rjernst" created="2016-01-15T07:59:44Z" id="171896373">Well, there are no indexes, so I think waiting for green doesn't matter?
</comment><comment author="danielmitterdorfer" created="2016-01-15T08:01:15Z" id="171896578"> thought this might be a bit more robust as I've experienced status red shortly in my local tests (but then I did not specify the `wait_for_nodes` parameter...)
</comment><comment author="rjernst" created="2016-01-15T08:03:43Z" id="171897015">IIRC green just means "do all indexes have primary and replicas"
</comment><comment author="danielmitterdorfer" created="2016-01-15T08:08:26Z" id="171897678">I've pushed the change now. The number of nodes in the cluster is not defined in `Node` but directly in `ClusterConfiguration`. Apart from that I've also removed the log statement.
</comment><comment author="rjernst" created="2016-01-15T08:11:12Z" id="171898019">LGTM
</comment><comment author="danielmitterdorfer" created="2016-01-15T08:16:17Z" id="171898750">Thanks for the review @rjernst!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add per-index setting to limit number of nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15989</link><project id="" key="" /><description>Indexing a document with 100 nested fields actually indexes 101 documents as each nested
document is indexed as a separate document. To safeguard against ill-defined mappings
the number of nested fields that can be defined per index has been limited to 50. This
default limit can be changed with the index setting `index.mapping.nested_fields.limit`.

Closes #14983
</description><key id="126670642">15989</key><summary>Add per-index setting to limit number of nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Mapping</label><label>breaking</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T14:59:23Z</created><updated>2016-01-20T13:56:39Z</updated><resolved>2016-01-19T09:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-14T15:09:49Z" id="171667559">This looks good to me. One thing I'm wondering is whether we should only apply the limit to indices created on or after 2.3 in order to not fail recoveries of existing indices if they happen to cross the limit (unless maybe it's something that we want to do so that they know they are in a bad situation). cc @clintongormley 
</comment><comment author="clintongormley" created="2016-01-14T16:32:21Z" id="171691832">@jpountz i agree that this should be for new indices (&gt;= 2.3)
</comment><comment author="ywelsch" created="2016-01-15T11:37:32Z" id="171939941">Thinking a bit more about this, I noticed that the case where we update the new settings to a value lower than the current number of nested fields is not covered. After discussing with @jpountz and @s1monw, two possible ways to think about the nested fields limit came up:

1) The setting `index.mapping.nested_fields.limit` guarantees that the number of nested fields in the mapping is never above that limit. This means that updating `index.mapping.nested_fields.limit` to a value lower than the current number of nested fields must fail the index settings update.

or 

2) The setting `index.mapping.nested_fields.limit` guarantees that adding new nested fields fails if the number of nested fields goes over the current limit. Updating `index.mapping.nested_fields.limit` to a value lower than the current number of nested fields always succeeds.

In case we go with 2), I would drop the requirement that this only applies to new indices (&gt;= 2.3).

@jpountz @clintongormley thoughts?
</comment><comment author="clintongormley" created="2016-01-15T12:57:17Z" id="171953282">Good point - I think I prefer option 2
</comment><comment author="jpountz" created="2016-01-15T13:42:38Z" id="171964298">I think 2 is more practical too.
</comment><comment author="ywelsch" created="2016-01-15T15:52:48Z" id="171998065">I agree that option 2 is better.

Pushed a new set of changes, can you have a look again @jpountz ?

Instead of adding an(other) ad-hoc parameter to merge, I created a MergeOptions class (maybe we should call it ValidationOptions?)

Other options might be added there. In `MetaDataMappingService.PutMappingExecutor.applyRequest` I spotted for example a validation check that checks if _parent field points to an already existing type.
</comment><comment author="jpountz" created="2016-01-15T16:35:42Z" id="172008991">I am not sure I like giving an option to bypass this check. I would rather like to have a new enum parameter in the merge method that gives the context of the mapping update (type creation, mapping update or restoring an existing mapping) and apply the nested count check whenever it makes sense (type creation and mapping updates). I expect that it would also allow us to get rid of the `applyDefault` parameter since the defaults only need to be applied upon type creation.
</comment><comment author="jpountz" created="2016-01-15T16:36:48Z" id="172009267">If I'm not mistaken it would also work for the parent check which should only be applied for mapping updates.
</comment><comment author="ywelsch" created="2016-01-18T12:49:59Z" id="172519503">The reason I chose to set the options externally is that the options for such a context enum seem too complex to me. Looking at the current code on master, I identify 9 call sites:
- creating a new index (2 calls, case distinction on default mapping)
- receiving cluster state update from master with new/updated mapping (1 call)
- adding an alias (to parse the filter) (2 calls, case distinction on default mapping)
- check compatibility during possible upgrade of index metadata (e.g. when restoring from an old snapshot) (1 call)
- refresh of mapping triggered by node (1 call)
- update of mapping triggered by user or by system (e.g. with dynamic mappings) (2 calls, one where we create the mapping from the state before we update and another where we merge the updated mapping)

I'm a bit hesitant on moving these case distinctions into MapperService. @jpountz how should we proceed?
</comment><comment author="ywelsch" created="2016-01-19T09:07:34Z" id="172782062">@jpountz Thanks for the change in #16059. I have updated this PR accordingly. Can you have another look?
</comment><comment author="jpountz" created="2016-01-19T09:12:16Z" id="172783101">LGTM
</comment><comment author="jpountz" created="2016-01-19T14:51:17Z" id="172876098">For the record, this setting was also made updatable via 6ca79095df46ae3bb9f599b00b7b56674c2c991e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>timeInQueue for pending_tasks growing too fast?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15988</link><project id="" key="" /><description>Hello, on 2.1.1 3 nodes cluster seeing `timeInQueue` growing faster than it should?

1) start three node cluster whose pending_tasks queue grows right after starting
2) call GET _cat/pending_tasks every second

```
abonuccelli@w530 ~ $ while true;do date;curl -XGET -u admin:r1ng3r -k 'https://w530:9200/_cat/pending_tasks';sleep 1;done
jue ene 14 15:36:23 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:24 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:25 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:26 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:28 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:29 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:30 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:31 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:32 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:33 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:34 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:35 CET 2016
curl: (7) Failed to connect to w530 port 9200: Connection refused
jue ene 14 15:36:36 CET 2016
5 11.3m NORMAL local-gateway-elected-state                                                                                          
7    6m URGENT zen-disco-join(join from node[{node3}{zPm--nb4Q1GyDfUgPRki0w}{192.168.1.105}{w530/192.168.1.105:9302}{master=true}]) 
6 10.7m HIGH   cluster_reroute(async_shard_fetch)                                                                                   
jue ene 14 15:36:37 CET 2016
8 14.2s HIGH cluster_reroute(async_shard_fetch) 
9  1.4s HIGH cluster_reroute(async_shard_fetch) 
jue ene 14 15:36:38 CET 2016
 9 19.4m HIGH   cluster_reroute(async_shard_fetch) 
10  9.3m URGENT shard-started ([logstash-unparsed-2016.01.14][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[23], s[INITIALIZING], a[id=6bPUppNYR96GpmRzRA2lVA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
11  9.1m URGENT shard-started ([logstash-unparsed-2016.01.14][2], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[26], s[INITIALIZING], a[id=tKqikFUkTqmHtr4SwQKmzQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
12  1.4m URGENT shard-started ([logstash-auth-2016.01.14][2], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[22], s[INITIALIZING], a[id=0uNZSK5bTjav1G5U462xPQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store]     
13  1.1m URGENT shard-started ([logstash-auth-2016.01.14][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[21], s[INITIALIZING], a[id=-FSh28JvTw6CzPlT-rKslQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store]     
14    1m URGENT shard-started ([logstash-unparsed-2016.01.14][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[26], s[INITIALIZING], a[id=A6yx_FkqRb6BRmNyn4v9dA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
15   47s URGENT shard-started ([logstash-auth-2016.01.14][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[22], s[INITIALIZING], a[id=7LEi3mQMRlOebCBTVITYqQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store]     
jue ene 14 15:36:39 CET 2016
17  5.1m URGENT shard-started ([logstash-auth-2016.01.12][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[40], s[INITIALIZING], a[id=oRhDl4tWQjyXrHjYtyyFfw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store]     
18  3.9m URGENT shard-started ([logstash-auth-2016.01.13][2], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[32], s[INITIALIZING], a[id=WM9zpWx5T4ujXxPdskbuOA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store]     
19  3.9m URGENT shard-started ([logstash-unparsed-2016.01.13][1], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[36], s[INITIALIZING], a[id=NZmkrLKfTMKT4a4iSfGtBQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
20  3.1m URGENT shard-started ([logstash-auth-2016.01.13][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=7_fdmALYRYCE6Hs1Ai0Lfg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store]     
22  1.2m URGENT shard-started ([.marvel-es-data][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[26], s[INITIALIZING], a[id=2E_4eG24S8m1mUDSg1NLCQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store]              
21  1.2m URGENT shard-started ([.watch_history-2016.01.05][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[29], s[INITIALIZING], a[id=tUm4Rt7XQhK7zJTRMqawFg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store]    
16 20.8m HIGH   _add_listener_ 
jue ene 14 15:36:40 CET 2016
29 16.5m URGENT shard-started ([logstash-syslog-2016.01.12][1], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[38], s[INITIALIZING], a[id=Cz2-XhQJSBCkUtLTU_XoYg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
30 16.2m URGENT shard-started ([logstash-syslog-2016.01.12][1], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[38], s[INITIALIZING], a[id=Cz2-XhQJSBCkUtLTU_XoYg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started] 
31 14.5m URGENT shard-started ([logstash-syslog-2016.01.12][2], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=W3HDLC-nTWm33uJt9mCvyQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
33  8.2m URGENT shard-started ([logstash-unparsed-2016.01.12][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[37], s[INITIALIZING], a[id=70JqKeAaTgO9HGqX0UYaWg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
36 47.8s URGENT shard-started ([.watch_history-2016.01.12][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[34], s[INITIALIZING], a[id=L9Wpm4MlQtGuhq2Ps7-3Gg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
32   12m URGENT shard-started ([logstash-auth-2016.01.12][2], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[33], s[INITIALIZING], a[id=jAEM0xJMSFCAw7xAp3Zh5g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
34    8m URGENT shard-started ([logstash-auth-2016.01.12][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[33], s[INITIALIZING], a[id=aQYnVPBWTx24BR5L-H9egQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
35    8m URGENT shard-started ([logstash-unparsed-2016.01.12][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=nFYleWmMQumM9tnc-58TrA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
16 44.9m HIGH   _add_listener_                                                                                                                               
jue ene 14 15:36:42 CET 2016
46  9.3m URGENT shard-started ([logstash-unparsed-2016.01.10][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=8US-Te0YSQCym42iTKKAjw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
47  3.9m URGENT shard-started ([logstash-unparsed-2016.01.10][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=8US-Te0YSQCym42iTKKAjw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started] 
49  2.4m URGENT shard-started ([logstash-syslog-2016.01.10][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=3NzmZFu6S8yYVgkir8_Hfg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
48  2.6m URGENT shard-started ([logstash-syslog-2016.01.10][2], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=VGbLKgt7RYu0WmSuVg3ZDQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
53 25.6s URGENT shard-started ([.watch_history-2016.01.10][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=hykDFlrTQnOVJjY6WPSR4g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]    
50  1.7m URGENT shard-started ([logstash-unparsed-2016.01.10][2], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=rn21EPXSQbOw2YKx_bTilg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
51  1.6m URGENT shard-started ([logstash-unparsed-2016.01.10][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[34], s[INITIALIZING], a[id=yrciN2zZQPaY0XFWkpwUGQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
52  1.3m URGENT shard-started ([.watch_history-2016.01.10][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=hykDFlrTQnOVJjY6WPSR4g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
16    1h HIGH   _add_listener_ 
54 25.6s URGENT shard-started ([logstash-unparsed-2016.01.10][2], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=rn21EPXSQbOw2YKx_bTilg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started] 
jue ene 14 15:36:43 CET 2016
64  8.4m URGENT shard-started ([logstash-syslog-2016.01.09][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=RnkOosAyRpiaOGvX0hBpTQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
65    6m URGENT shard-started ([logstash-unparsed-2016.01.08][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[40], s[INITIALIZING], a[id=I5Q3OFfpSZ-1inpSGLiwZQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
66  5.6m URGENT shard-started ([logstash-unparsed-2016.01.08][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[44], s[INITIALIZING], a[id=AyqMhVCHRpeSdGGZblgKig], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
69  1.2m URGENT shard-started ([.watch_history-2016.01.09][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=4yoPNJg8RjSSSpcMNwolHA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
67  5.5m URGENT shard-started ([logstash-syslog-2016.01.09][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=RnkOosAyRpiaOGvX0hBpTQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started] 
68  5.1m URGENT shard-started ([logstash-syslog-2016.01.08][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[34], s[INITIALIZING], a[id=gWL8rqL5RICFBlOPbXXVlg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
16  1.4h HIGH   _add_listener_                                                                                                                               
70 44.1s URGENT shard-started ([logstash-syslog-2016.01.09][1], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=z21y4FyJTeGab0hhunzPFg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
jue ene 14 15:36:44 CET 2016
78 12.7m URGENT shard-started ([.watch_history-2016.01.07][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[38], s[INITIALIZING], a[id=HnIbZsKmRw2F811HF42Hbw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
79 11.7m URGENT shard-started ([.watch_history-2016.01.07][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[38], s[INITIALIZING], a[id=HnIbZsKmRw2F811HF42Hbw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started] 
81  8.8m URGENT shard-started ([logstash-syslog-2016.01.07][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[36], s[INITIALIZING], a[id=3qO53WStT9CXgbkFnUksiA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
80  9.3m URGENT shard-started ([logstash-syslog-2016.01.07][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[34], s[INITIALIZING], a[id=axCQvobnTLC9PvJPjWAMGw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
83  6.2m URGENT shard-started ([.watch_history-2016.01.08][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=hXmEuKK6Sy2HWm-rHEDAew], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
87   14s URGENT shard-started ([test-geo][1], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[85], s[INITIALIZING], a[id=AGzIixN_RROIhjgaVgQ0GA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery from store] 
82  6.3m URGENT shard-started ([logstash-syslog-2016.01.08][2], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[34], s[INITIALIZING], a[id=tz9YgDHwT7yvXKEXQOX66A], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
84  2.8m URGENT shard-started ([.watches][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[47], s[INITIALIZING], a[id=snXyoQs_QeaA06FhtMKQrg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
85 40.5s URGENT shard-started ([logstash-unparsed-2016.01.07][0], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[35], s[INITIALIZING], a[id=nzKhYoHqSa6v5mjLWQ-kyg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery from store] 
86 38.6s URGENT shard-started ([.watches][0], node[zPm--nb4Q1GyDfUgPRki0w], [P], v[47], s[INITIALIZING], a[id=snXyoQs_QeaA06FhtMKQrg], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [master {node2}{I_iHcd03Q3aX2KXBVsjMhA}{192.168.1.105}{192.168.1.105:9301}{master=true} marked shard as initializing, but shard state is [POST_RECOVERY], mark shard as started]                  
16  1.7h HIGH   _add_listener_                                                                                                                              
jue ene 14 15:36:45 CET 2016
101  2.9m URGENT shard-started ([test-idx][1], node[IjwKfD1UQMqtdKXPsLR8Cw], [P], v[90], s[INITIALIZING], a[id=0BHaiJdMQcSt2NeZJM09pA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.661Z]]), reason [after recovery from store]                                                                                                                     
102  1.8m URGENT shard-started ([logstash-unparsed-2016.01.13][1], node[zPm--nb4Q1GyDfUgPRki0w], [R], v[38], s[INITIALIZING], a[id=j9ubCTjyS8qIamqigOblLQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.664Z]]), reason [after recovery (replica) from node [{node1}{IjwKfD1UQMqtdKXPsLR8Cw}{192.168.1.105}{w530/192.168.1.105:9300}{master=true}]] 
103 35.3s URGENT shard-started ([logstash-unparsed-2016.01.14][2], node[zPm--nb4Q1GyDfUgPRki0w], [R], v[28], s[INITIALIZING], a[id=12I5tGEJRzC6y65FTq4hdw], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-01-14T14:36:35.665Z]]), reason [after recovery (replica) from node [{node1}{IjwKfD1UQMqtdKXPsLR8Cw}{192.168.1.105}{w530/192.168.1.105:9300}{master=true}]] 
 16  2.1h HIGH   _add_listener_                                                                            
 97 12.7m HIGH   cluster_reroute(async_shard_fetch)                                                        
jue ene 14 15:36:47 CET 2016
jue ene 14 15:36:48 CET 2016
jue ene 14 15:36:49 CET 2016
jue ene 14 15:36:51 CET 2016
jue ene 14 15:36:52 CET 2016
jue ene 14 15:36:53 CET 2016
jue ene 14 15:36:54 CET 2016
jue ene 14 15:36:55 CET 2016
jue ene 14 15:36:56 CET 2016
jue ene 14 15:36:58 CET 2016
jue ene 14 15:36:59 CET 2016
jue ene 14 15:37:00 CET 2016
jue ene 14 15:37:01 CET 2016
jue ene 14 15:37:02 CET 2016
jue ene 14 15:37:03 CET 2016
jue ene 14 15:37:04 CET 2016
jue ene 14 15:37:06 CET 2016
jue ene 14 15:37:07 CET 2016
jue ene 14 15:37:08 CET 2016
jue ene 14 15:37:09 CET 2016
jue ene 14 15:37:10 CET 2016
jue ene 14 15:37:11 CET 2016
jue ene 14 15:37:12 CET 2016
jue ene 14 15:37:14 CET 2016
jue ene 14 15:37:15 CET 2016
jue ene 14 15:37:16 CET 2016
jue ene 14 15:37:17 CET 2016
jue ene 14 15:37:18 CET 2016
jue ene 14 15:37:19 CET 2016
jue ene 14 15:37:20 CET 2016
jue ene 14 15:37:21 CET 2016
jue ene 14 15:37:22 CET 2016
jue ene 14 15:37:23 CET 2016
jue ene 14 15:37:25 CET 2016
jue ene 14 15:37:26 CET 2016
jue ene 14 15:37:27 CET 2016
jue ene 14 15:37:28 CET 2016
jue ene 14 15:37:29 CET 2016
jue ene 14 15:37:30 CET 2016
jue ene 14 15:37:31 CET 2016
jue ene 14 15:37:32 CET 2016
jue ene 14 15:37:33 CET 2016
jue ene 14 15:37:35 CET 2016
jue ene 14 15:37:36 CET 2016
108 12.9m URGENT delete-index [.shield_audit_log-2016.01.14] 
109 12.5m HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:37 CET 2016
108 33.5m URGENT delete-index [.shield_audit_log-2016.01.14] 
109 33.2m HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:38 CET 2016
108 52.8m URGENT delete-index [.shield_audit_log-2016.01.14] 
109 52.5m HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:39 CET 2016
108 1.1h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 1.1h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:40 CET 2016
108 1.4h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 1.4h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:41 CET 2016
108 1.7h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 1.7h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:42 CET 2016
108 2.1h URGENT delete-index [.shield_audit_log-2016.01.14] 
109   2h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:44 CET 2016
108 2.4h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 2.3h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:45 CET 2016
108 2.7h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 2.7h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:46 CET 2016
108 3h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 3h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:47 CET 2016
108 3.3h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 3.3h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:48 CET 2016
108 3.6h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 3.6h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:49 CET 2016
108 3.9h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 3.9h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:50 CET 2016
108 4.2h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 4.2h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:51 CET 2016
108 4.5h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 4.5h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:52 CET 2016
108 4.8h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 4.8h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:53 CET 2016
108 5.1h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 5.1h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:55 CET 2016
108 5.4h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 5.4h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:56 CET 2016
108 5.7h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 5.7h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:57 CET 2016
108 6.1h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 6.1h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:58 CET 2016
108 6.4h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 6.4h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:37:59 CET 2016
108 6.7h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 6.7h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:38:00 CET 2016
108 7h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 7h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:38:01 CET 2016
108 7.3h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 7.3h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:38:02 CET 2016
108 7.6h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 7.6h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:38:04 CET 2016
108 7.9h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 7.9h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:38:05 CET 2016
108 8.2h URGENT delete-index [.shield_audit_log-2016.01.14] 
109 8.2h HIGH   cluster_reroute(async_shard_fetch)          
jue ene 14 15:38:06 CET 2016
108  8.5h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 10.6m URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109  8.5h HIGH   cluster_reroute(async_shard_fetch)                                  
jue ene 14 15:38:07 CET 2016
108  8.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 29.7m URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109  8.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111  7.8m HIGH   _add_listener_                                                      
jue ene 14 15:38:08 CET 2016
108  9.2h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 48.4m URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109  9.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111 26.5m HIGH   _add_listener_                                                      
jue ene 14 15:38:09 CET 2016
108  9.5h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  1.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109  9.5h HIGH   cluster_reroute(async_shard_fetch)                                  
111 44.9m HIGH   _add_listener_                                                      
jue ene 14 15:38:10 CET 2016
108 9.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 1.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 9.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111   1h HIGH   _add_listener_                                                      
jue ene 14 15:38:11 CET 2016
108 10.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  1.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 10.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111  1.3h HIGH   _add_listener_                                                      
jue ene 14 15:38:12 CET 2016
108 10.4h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110    2h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 10.4h HIGH   cluster_reroute(async_shard_fetch)                                  
111  1.6h HIGH   _add_listener_                                                      
jue ene 14 15:38:14 CET 2016
108 10.7h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  2.3h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 10.7h HIGH   cluster_reroute(async_shard_fetch)                                  
111  1.9h HIGH   _add_listener_                                                      
jue ene 14 15:38:15 CET 2016
108  11h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 2.6h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109  11h HIGH   cluster_reroute(async_shard_fetch)                                  
111 2.2h HIGH   _add_listener_                                                      
jue ene 14 15:38:16 CET 2016
108 11.3h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  2.9h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 11.3h HIGH   cluster_reroute(async_shard_fetch)                                  
111  2.6h HIGH   _add_listener_                                                      
jue ene 14 15:38:17 CET 2016
108 11.6h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  3.2h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 11.6h HIGH   cluster_reroute(async_shard_fetch)                                  
111  2.9h HIGH   _add_listener_                                                      
jue ene 14 15:38:18 CET 2016
108 11.9h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  3.5h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 11.9h HIGH   cluster_reroute(async_shard_fetch)                                  
111  3.2h HIGH   _add_listener_                                                      
jue ene 14 15:38:19 CET 2016
108 12.2h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  3.9h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 12.2h HIGH   cluster_reroute(async_shard_fetch)                                  
111  3.5h HIGH   _add_listener_                                                      
jue ene 14 15:38:20 CET 2016
108 12.6h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  4.2h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 12.6h HIGH   cluster_reroute(async_shard_fetch)                                  
111  3.8h HIGH   _add_listener_                                                      
jue ene 14 15:38:21 CET 2016
108 12.9h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  4.5h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 12.9h HIGH   cluster_reroute(async_shard_fetch)                                  
111  4.1h HIGH   _add_listener_                                                      
jue ene 14 15:38:22 CET 2016
108 13.2h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  4.8h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 13.2h HIGH   cluster_reroute(async_shard_fetch)                                  
111  4.4h HIGH   _add_listener_                                                      
jue ene 14 15:38:24 CET 2016
108 13.5h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  5.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 13.5h HIGH   cluster_reroute(async_shard_fetch)                                  
111  4.7h HIGH   _add_listener_                                                      
jue ene 14 15:38:25 CET 2016
108 13.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  5.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 13.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111    5h HIGH   _add_listener_                                                      
jue ene 14 15:38:26 CET 2016
108 14.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  5.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 14.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111  5.4h HIGH   _add_listener_                                                      
jue ene 14 15:38:27 CET 2016
108 14.4h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110    6h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 14.4h HIGH   cluster_reroute(async_shard_fetch)                                  
111  5.7h HIGH   _add_listener_                                                      
jue ene 14 15:38:28 CET 2016
108 14.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  6.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 14.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111    6h HIGH   _add_listener_                                                      
jue ene 14 15:38:29 CET 2016
108 15.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  6.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 15.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111  6.3h HIGH   _add_listener_                                                      
jue ene 14 15:38:30 CET 2016
108 15.4h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110    7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 15.4h HIGH   cluster_reroute(async_shard_fetch)                                  
111  6.6h HIGH   _add_listener_                                                      
jue ene 14 15:38:31 CET 2016
108 15.7h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  7.3h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 15.7h HIGH   cluster_reroute(async_shard_fetch)                                  
111  6.9h HIGH   _add_listener_                                                      
jue ene 14 15:38:33 CET 2016
108  16h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 7.6h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109  16h HIGH   cluster_reroute(async_shard_fetch)                                  
111 7.2h HIGH   _add_listener_                                                      
jue ene 14 15:38:34 CET 2016
108 16.3h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  7.9h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 16.3h HIGH   cluster_reroute(async_shard_fetch)                                  
111  7.5h HIGH   _add_listener_                                                      
jue ene 14 15:38:35 CET 2016
108 16.6h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  8.2h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 16.6h HIGH   cluster_reroute(async_shard_fetch)                                  
111  7.8h HIGH   _add_listener_                                                      
jue ene 14 15:38:36 CET 2016
108 16.9h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  8.5h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 16.9h HIGH   cluster_reroute(async_shard_fetch)                                  
111  8.2h HIGH   _add_listener_                                                      
jue ene 14 15:38:37 CET 2016
108 17.2h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  8.8h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 17.2h HIGH   cluster_reroute(async_shard_fetch)                                  
111  8.5h HIGH   _add_listener_                                                      
jue ene 14 15:38:38 CET 2016
108 17.5h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  9.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 17.5h HIGH   cluster_reroute(async_shard_fetch)                                  
111  8.8h HIGH   _add_listener_                                                      
jue ene 14 15:38:39 CET 2016
108 17.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  9.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 17.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111  9.1h HIGH   _add_listener_                                                      
112    4m HIGH   _add_listener_                                                      
jue ene 14 15:38:40 CET 2016
108 18.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110  9.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 18.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111  9.4h HIGH   _add_listener_                                                      
112 22.2m HIGH   _add_listener_                                                      
jue ene 14 15:38:41 CET 2016
108 18.4h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 10.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 18.4h HIGH   cluster_reroute(async_shard_fetch)                                  
111  9.7h HIGH   _add_listener_                                                      
112 40.4m HIGH   _add_listener_                                                      
jue ene 14 15:38:43 CET 2016
108 18.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 10.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 18.7h HIGH   cluster_reroute(async_shard_fetch)                                  
111   10h HIGH   _add_listener_                                                      
112 58.6m HIGH   _add_listener_                                                      
jue ene 14 15:38:44 CET 2016
108 19.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 10.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 19.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111 10.3h HIGH   _add_listener_                                                      
112  1.2h HIGH   _add_listener_                                                      
jue ene 14 15:38:45 CET 2016
108 19.4h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110   11h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 19.4h HIGH   cluster_reroute(async_shard_fetch)                                  
111 10.6h HIGH   _add_listener_                                                      
112  1.6h HIGH   _add_listener_                                                      
jue ene 14 15:38:46 CET 2016
108 19.7h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 11.3h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 19.7h HIGH   cluster_reroute(async_shard_fetch)                                  
111 10.9h HIGH   _add_listener_                                                      
112  1.9h HIGH   _add_listener_                                                      
jue ene 14 15:38:47 CET 2016
108   20h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 11.6h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109   20h HIGH   cluster_reroute(async_shard_fetch)                                  
111 11.3h HIGH   _add_listener_                                                      
112  2.2h HIGH   _add_listener_                                                      
jue ene 14 15:38:48 CET 2016
108 20.3h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 11.9h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 20.3h HIGH   cluster_reroute(async_shard_fetch)                                  
111 11.6h HIGH   _add_listener_                                                      
112  2.5h HIGH   _add_listener_                                                      
jue ene 14 15:38:49 CET 2016
108 20.6h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 12.2h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 20.6h HIGH   cluster_reroute(async_shard_fetch)                                  
111 11.9h HIGH   _add_listener_                                                      
112  2.8h HIGH   _add_listener_                                                      
jue ene 14 15:38:50 CET 2016
108 20.9h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 12.5h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 20.9h HIGH   cluster_reroute(async_shard_fetch)                                  
111 12.2h HIGH   _add_listener_                                                      
112  3.1h HIGH   _add_listener_                                                      
jue ene 14 15:38:51 CET 2016
108 21.2h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 12.8h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 21.2h HIGH   cluster_reroute(async_shard_fetch)                                  
111 12.5h HIGH   _add_listener_                                                      
112  3.4h HIGH   _add_listener_                                                      
jue ene 14 15:38:53 CET 2016
108 21.5h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 13.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 21.5h HIGH   cluster_reroute(async_shard_fetch)                                  
111 12.8h HIGH   _add_listener_                                                      
112  3.7h HIGH   _add_listener_                                                      
jue ene 14 15:38:54 CET 2016
108 21.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 13.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 21.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111 13.1h HIGH   _add_listener_                                                      
112    4h HIGH   _add_listener_                                                      
jue ene 14 15:38:55 CET 2016
108 22.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 13.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 22.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111 13.4h HIGH   _add_listener_                                                      
112  4.3h HIGH   _add_listener_                                                      
jue ene 14 15:38:56 CET 2016
108 22.5h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 14.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 22.5h HIGH   cluster_reroute(async_shard_fetch)                                  
111 13.7h HIGH   _add_listener_                                                      
112  4.7h HIGH   _add_listener_                                                      
jue ene 14 15:38:57 CET 2016
108 22.8h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 14.4h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 22.8h HIGH   cluster_reroute(async_shard_fetch)                                  
111   14h HIGH   _add_listener_                                                      
112    5h HIGH   _add_listener_                                                      
jue ene 14 15:38:58 CET 2016
108 23.1h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 14.7h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 23.1h HIGH   cluster_reroute(async_shard_fetch)                                  
111 14.3h HIGH   _add_listener_                                                      
112  5.3h HIGH   _add_listener_                                                      
jue ene 14 15:38:59 CET 2016
108 23.4h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110   15h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 23.4h HIGH   cluster_reroute(async_shard_fetch)                                  
111 14.6h HIGH   _add_listener_                                                      
112  5.6h HIGH   _add_listener_                                                      
jue ene 14 15:39:00 CET 2016
108 23.7h URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 15.3h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109 23.7h HIGH   cluster_reroute(async_shard_fetch)                                  
111 14.9h HIGH   _add_listener_                                                      
112  5.9h HIGH   _add_listener_                                                      
jue ene 14 15:39:01 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 15.6h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 15.3h HIGH   _add_listener_                                                      
112  6.2h HIGH   _add_listener_                                                      
jue ene 14 15:39:03 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 15.9h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 15.6h HIGH   _add_listener_                                                      
112  6.5h HIGH   _add_listener_                                                      
jue ene 14 15:39:04 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 16.2h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 15.9h HIGH   _add_listener_                                                      
112  6.8h HIGH   _add_listener_                                                      
jue ene 14 15:39:05 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
110 16.5h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 16.2h HIGH   _add_listener_                                                      
112  7.1h HIGH   _add_listener_                                                      
jue ene 14 15:39:06 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
113 13.8m URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 16.5h HIGH   _add_listener_                                                      
112  7.4h HIGH   _add_listener_                                                      
jue ene 14 15:39:07 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
113 32.6m URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 16.8h HIGH   _add_listener_                                                      
112  7.7h HIGH   _add_listener_                                                      
jue ene 14 15:39:08 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
113 50.8m URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 17.1h HIGH   _add_listener_                                                      
112    8h HIGH   _add_listener_                                                      
jue ene 14 15:39:09 CET 2016
108    1d URGENT delete-index [.shield_audit_log-2016.01.14]                         
113  1.1h URGENT create-index [.shield_audit_log-2016.01.14], cause [auto(bulk api)] 
109    1d HIGH   cluster_reroute(async_shard_fetch)                                  
111 17.4h HIGH   _add_listener_                                                      
112  8.3h HIGH   _add_listener_                                                      
^C
abonuccelli@w530 ~ $ 
```
</description><key id="126667669">15988</key><summary>timeInQueue for pending_tasks growing too fast?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Stats</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0</label><label>v2.0.1</label><label>v2.0.2</label><label>v2.1.0</label><label>v2.1.1</label></labels><created>2016-01-14T14:45:02Z</created><updated>2016-01-15T13:01:50Z</updated><resolved>2016-01-15T13:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T16:30:22Z" id="171691349">Confirmed on 2.2.0-SNAPSHOT - within a few seconds it is reporting timeInQueue of several hours!
</comment><comment author="jasontedor" created="2016-01-14T16:43:01Z" id="171696616">@clintongormley Should be [dividing by `1000000` instead of `1000`](https://github.com/elastic/elasticsearch/blob/a954e4e8e5f235e0279ed38609a2bbd2f0abaf68/core/src/main/java/org/elasticsearch/common/util/concurrent/PrioritizedRunnable.java#L45) (so the value being returned is microseconds, not milliseconds). Or even better, just use built-in methods to convert nanoseconds to milliseconds. I'll throw up a quick pull request.
</comment><comment author="jasontedor" created="2016-01-14T17:02:49Z" id="171705208">@clintongormley I opened #15995.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Azure Storage emulator for integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15987</link><project id="" key="" /><description>If available on the machine we are running our tests (which is something we can detect by checking `storageConnection` system property), we can use the [Azure Storage emulator](https://azure.microsoft.com/en-us/documentation/articles/storage-use-emulator/) to run snapshot and restore tests which are closer to the test we run today.

According to the [source code](https://github.com/Azure/azure-storage-java/blob/master/microsoft-azure-storage-test/src/com/microsoft/azure/storage/TestHelper.java#L277-L293) and the [documentation](https://azure.microsoft.com/en-us/documentation/articles/storage-configure-connection-string/), the `storageConnection` should contain something like:

```
Account name: devstoreaccount1
Account key: Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==
```
</description><key id="126664707">15987</key><summary>Use Azure Storage emulator for integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>adoptme</label><label>test</label></labels><created>2016-01-14T14:28:28Z</created><updated>2016-07-25T14:07:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping for alphabetical order with location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15986</link><project id="" key="" /><description>I have a bug in combination with symfony2, distance search and alphabetical sorting. Please take a look on the description on stackoverflow: http://stackoverflow.com/questions/34725089/elasticsearch-mapping-for-alphabetical-order-with-location

Thanks for helping!
</description><key id="126662376">15986</key><summary>Mapping for alphabetical order with location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2016-01-14T14:15:10Z</created><updated>2016-01-14T16:20:22Z</updated><resolved>2016-01-14T16:20:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T16:20:21Z" id="171688472">Hi @antondachauer 

The best place to ask questions like these is in the forum: http://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Execution service enhancement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15985</link><project id="" key="" /><description>Use `BiConsumer` instead of `Consumer` to pass down the failed index request with throwable.
</description><key id="126652297">15985</key><summary>[Ingest] Execution service enhancement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-14T13:12:51Z</created><updated>2016-01-14T13:22:27Z</updated><resolved>2016-01-14T13:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-14T13:16:18Z" id="171641717">thanks, I wasn't aware a BiConsumer existed, LGTM!
</comment><comment author="javanna" created="2016-01-14T13:17:00Z" id="171641835">I am also wondering if we still need all those callbacks in the execution service. Seems like things could be simplified further, maybe worth having a look at this later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix incorrect file-based example in script_fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15984</link><project id="" key="" /><description>When following the example I get error message _must specify a script in script fields_. The file parameter as well as the rest must be folded inside JSON object under "script".
</description><key id="126650755">15984</key><summary>Fix incorrect file-based example in script_fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pdudits</reporter><labels><label>docs</label></labels><created>2016-01-14T13:05:16Z</created><updated>2016-01-14T13:17:42Z</updated><resolved>2016-01-14T13:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pdudits" created="2016-01-14T13:06:41Z" id="171638900">CLA has been signed few seconds after PR was created
</comment><comment author="clintongormley" created="2016-01-14T13:17:42Z" id="171641967">thanks @pdudits 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Remove dedicated ingest threadpool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15983</link><project id="" key="" /><description>To main concern with the dedicated ingest TP is that there are already many TPs and in the case with beefy nodes we would many more threads. In the case ingest isn't used the all these threads are just idle.
</description><key id="126648768">15983</key><summary>[Ingest] Remove dedicated ingest threadpool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-14T12:51:46Z</created><updated>2016-01-17T12:09:52Z</updated><resolved>2016-01-14T16:52:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-14T13:14:28Z" id="171641415">I am not sure this is the right decision on the lung run, but I am ok with simplifying things for now. One day we can always add back a specific thread pool if we need it. LGTM
</comment><comment author="clintongormley" created="2016-01-17T12:09:52Z" id="172318172">Closed in 63ee2224f771d23874bbc6e081b606618bcf2b73
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Azure repository with only one primary account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15982</link><project id="" key="" /><description>Using a single azure account is now rejected.
This commit fixes this issue and adds a test for it.

This regression was introduced with #13779. Hopefully no elasticsearch version has been released since then.

Needs to be merged in 2.2, 2.x and master branches.
</description><key id="126648735">15982</key><summary>Fix Azure repository with only one primary account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>blocker</label><label>non-issue</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T12:51:31Z</created><updated>2016-01-17T12:04:47Z</updated><resolved>2016-01-14T13:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-14T12:52:33Z" id="171636422">I marked it as a blocker.

@imotov @tlrx Could you review it please? 
</comment><comment author="tlrx" created="2016-01-14T13:04:33Z" id="171638514">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_analyze` API hangs with unknown analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15981</link><project id="" key="" /><description>When calling the `_analyze` API
- with an inexistent `analyzer`
- without specifying the `index` parameter

the endpoint does not return and a `NullPointerException` is logged

```
[2016-01-14 13:18:51,292][ERROR][transport                ] [Sunpyre] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@2463bc3a]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="126644213">15981</key><summary>`_analyze` API hangs with unknown analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels /><created>2016-01-14T12:20:53Z</created><updated>2016-01-14T12:31:04Z</updated><resolved>2016-01-14T12:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T12:31:03Z" id="171632013">Fixed by #15447
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Client ES 2.1 Java Hash Map key removal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15980</link><project id="" key="" /><description>In my java project i use several java objects which contain java HashMaps among other fields. All serialization is performed correctly up the java client comunication with the node but something goes wrong upon key removal.  Everytime i remove a key from a java HashMap the key removal is not reflected on the stored document. Adding or updating keys on the java HashMap is performed correctly. Furthermore, interacting directly with the ES APIs won't result in this bug. Everything is ok.

So my best guess is that the java client detects a NoOp on the update request after i remove i json property although this happens only for properties derived from hashmaps. I'd exclude the Jackson Object Mapper fault since the generated json is correct.

What am i doing wrong?

I wrote a simple java testcase to show this behaviour

``` java


        //create an object mapper
        mapper = new ObjectMapper();
        mapper.configure(Feature.WRITE_DATES_AS_TIMESTAMPS, true);
        mapper.configure(DeserializationConfig.Feature.FAIL_ON_UNKNOWN_PROPERTIES, false);
        mapper.configure(SerializationConfig.Feature.REQUIRE_SETTERS_FOR_GETTERS, true);

        //create a hashmap with one key
        Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;();
        map.put("key", "value");

        //index the document
        //sent json is: '{"key":"value"}'
        this.client.prepareIndex("restaurants", "Restaurant", "id1")
        .setSource(mapper.writeValueAsString(map)).execute().actionGet();

        //remove the key from map and update the document
        //sent json is: '{}'
        map.remove("key");
        this.client.prepareUpdate("restaurants", "Restaurant", "id1")
        .setDoc(mapper.writeValueAsString(map)).execute().actionGet();

        //retrieve the document, no key expected  
        //returned json is: '{"key":"value"}'
        GetResponse response = this.client.prepareGet("restaurants", "Restaurant", "id1").execute()
        .actionGet();

        //test fails
        Map&lt;String,String&gt; updatedMap = mapper.readValue(response.getSourceAsString(), new HashMap&lt;String,String&gt;().getClass());
        assertTrue(!updatedMap.containsKey("key"));
```
</description><key id="126641462">15980</key><summary>Java Client ES 2.1 Java Hash Map key removal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">serxhio</reporter><labels><label>:CRUD</label></labels><created>2016-01-14T12:03:38Z</created><updated>2016-01-14T17:24:34Z</updated><resolved>2016-01-14T13:30:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T12:30:18Z" id="171631883">@nik9000 could you have a look at this please?
</comment><comment author="javanna" created="2016-01-14T13:30:06Z" id="171645247">@serxhio I don't think noop is a problem here. You cannot remove fields when providing a partial document with the update api. You can only add new ones or replace existing ones. The existing document is merged with the partial one, so if you provide an empty map, it doesn't mean that the existing document should become empty after the update. The only way to do what you need is to use a script, as shown in our docs [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#_scripted_updates). Closing as this is the expected behaviour of the update api, feel free to reopen if there's anything I missed.
</comment><comment author="serxhio" created="2016-01-14T14:56:06Z" id="171664116">Ah ok. I think i misunderstood the docs. I though the update would completely replace the existing json with the updated json. In my real project, i do the following steps:
retrieve the original json
deserialize it in a java object
remove a field from the hashmap
serialize it back in json
update the existing json with the new. 

So, i don't understand why my update would be considered a partial update since i'm providing the whole document with just one single field removed. The thing is that the update-REST-API is performing a full update while the java client performs a partial update (assuming the same jsons are used). Maybe there is an unkown setting for the client?

Thank you very much for your support by the way  :)
</comment><comment author="javanna" created="2016-01-14T15:00:58Z" id="171665293">If you use the index api instead, the existing doc will get completely replaced with the new one, seems like this is what you need given that you already load the old document and modify it. The update api works the same on the REST layer and java layer (in fact the REST layer uses the java api underneath). If you see some difference there, sounds like a bug so please post a recreation but I do hope that is not the case.
</comment><comment author="serxhio" created="2016-01-14T15:05:14Z" id="171666427">That's awesome, so i can use the IndexRequest in order to perform a full update. That will work for me.
Thank you all very much, if fixes some troubles for me.
</comment><comment author="javanna" created="2016-01-14T15:15:03Z" id="171669054">you are welcome @serxhio . if you have other questions you can join us at https://discuss.elastic.co/ .
</comment><comment author="serxhio" created="2016-01-14T17:07:42Z" id="171706510">Hello, i remembered i measured the differences between the java client and the rest apis by hand. In order to bring evidences i managed (i'm sorry for any ugly code, i was really in a hurry :) ) to quickly create a test case that shows these difference in updates. I hope i'm wrong  :)

There are two tests that perform the same tasks: index,update,get.
The java client returns a different result respect to the rest apis.

[ESPartialUpdates.zip](https://github.com/elastic/elasticsearch/files/90712/ESPartialUpdates.zip)
</comment><comment author="javanna" created="2016-01-14T17:24:33Z" id="171713180">please join us on [discuss](https://discuss.elastic.co/), this doesn't seem like a bug but more questions on how to use the elasticsearch apis. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Following error coming after start the ES2.1.1 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15979</link><project id="" key="" /><description>$ ./elasticsearch 
[2016-01-14 15:18:50,490][INFO ][node                     ] [es_f1_01] version[2.1.1], pid[25850], build[40e2c53/2015-12-15T13:05:55Z]
[2016-01-14 15:18:50,490][INFO ][node                     ] [es_f1_01] initializing ...
Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/plugins/AbstractPlugin
Likely root cause: java.lang.ClassNotFoundException: org.elasticsearch.plugins.AbstractPlugin
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
    at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:789)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
    at java.net.FactoryURLClassLoader.loadClass(URLClassLoader.java:789)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
    at org.elasticsearch.plugins.PluginsService.loadPluginClass(PluginsService.java:380)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:348)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:146)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
</description><key id="126619471">15979</key><summary>Following error coming after start the ES2.1.1 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sabarnath</reporter><labels /><created>2016-01-14T10:02:14Z</created><updated>2016-01-14T11:12:31Z</updated><resolved>2016-01-14T11:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sabarnath" created="2016-01-14T10:03:09Z" id="171593044">Note : i have added my own plugins too.
</comment><comment author="dadoonet" created="2016-01-14T10:31:26Z" id="171600798">What happens if you don't install your plugin?
Are you using the official distribution? Or are you building your own elasticsearch version?
</comment><comment author="sabarnath" created="2016-01-14T10:45:00Z" id="171609560">yes . I tried without our plugins we face some other issues,

 like below
Note : We are upgrade the 1.5.1 into 2.1.1 , I just copied existing(1.5.1) data folder into current(2.1.1) version

[2016-01-14 15:51:50,734][ERROR][gateway                  ] [es_f1_01] failed to read local state, exiting...
java.lang.IllegalStateException: unable to upgrade the mappings for the index [medzo_vendor_thing], reason: [Mapper for [payload] conflicts with existing mapping in other types[mapper [payload.explanation] has different [store] values, mapper [payload.explanation] has different [omit_norms] values, cannot change from disable to enabled, mapper [payload.explanation] has different [analyzer]]]
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:339)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.upgradeIndexMetaData(MetaDataIndexUpgradeService.java:116)
    at org.elasticsearch.gateway.GatewayMetaState.pre20Upgrade(GatewayMetaState.java:228)
    at org.elasticsearch.gateway.GatewayMetaState.&lt;init&gt;(GatewayMetaState.java:87)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.IllegalArgumentException: Mapper for [payload] conflicts with existing mapping in other types[mapper [payload.explanation] has different [store] values, mapper [payload.explanation] has different [omit_norms] values, cannot change from disable to enabled, mapper [payload.explanation] has different [analyzer]]
    at org.elasticsearch.index.mapper.MapperService.checkNewMappersCompatibility(MapperService.java:363)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:319)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:265)
    at org.elasticsearch.cluster.metadata.MetaDataIndexUpgradeService.checkMappingsCompatibility(MetaDataIndexUpgradeService.java:333)
    ... 39 more
</comment><comment author="sabarnath" created="2016-01-14T10:52:38Z" id="171610998">Seems , We had own analyzer in setting of 1.5.1 not supporting current version .

Following  error mapper : mapper [payload.explanation]

"explanation": {

```
"store": true,
"norms": {
    "enabled": false
},
"analyzer": "vmWhitespaceAnalyzer",
"type": "string"
```

## }

See this is our own custom analyzer : 

"vmWhitespaceAnalyzer":{
                    "type":"pattern",
                    "flags":"DOTALL",
                    "lowercase":"true",
                    "pattern":"\s+",
                    "stopwords":"_none_"
                }
</comment><comment author="dadoonet" created="2016-01-14T11:12:31Z" id="171615250">So without your plugin, elasticsearch is able to go further than:

```
[2016-01-14 15:18:50,490][INFO ][node ] [es_f1_01] initializing ...
```

As I can now see:

```
[2016-01-14 15:51:50,734][ERROR][gateway ] [es_f1_01] failed to read local state, exiting...
```

Which happens after plugin initialization.

So I'm closing this ticket because as you can see your plugin seems to be incorrect here. May be it was not upgraded???
Feel free to follow up on discuss.elastic.co.

About the other issue you are seeing, I'd also recommend asking on discuss as we and the community can better help you there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `optimize_single_shard` settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15978</link><project id="" key="" /><description>This optimization has existed for years with no complaints, I suggest that we
remove the settings that allow to disable it.
</description><key id="126619160">15978</key><summary>Remove `optimize_single_shard` settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T10:00:42Z</created><updated>2016-02-26T13:58:43Z</updated><resolved>2016-02-26T13:58:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-02-22T18:44:31Z" id="187313637">LGTM
</comment><comment author="javanna" created="2016-02-22T18:44:49Z" id="187313744">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove index.flush_on_close entirely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15977</link><project id="" key="" /><description>This undocumented setting was mainly used for testing and a safety net for
a new flushOnClose feature back in 1.x. We can now safely remove this setting.
The test usage now uses a mock plugin setting to achive the same.
</description><key id="126614652">15977</key><summary>Remove index.flush_on_close entirely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-14T09:36:17Z</created><updated>2016-01-14T11:54:06Z</updated><resolved>2016-01-14T11:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-14T09:36:35Z" id="171586085">@jimferenczi another one for you ;)
</comment><comment author="jimczi" created="2016-01-14T10:00:34Z" id="171592407">LGTM. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Azure Storage client to 4.0.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15976</link><project id="" key="" /><description>We are using `2.0.0` today but Azure team now recommends:

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
    &lt;artifactId&gt;azure-storage&lt;/artifactId&gt;
    &lt;version&gt;4.0.0&lt;/version&gt;
&lt;/dependency&gt;
```

Release notes from 2.0.0:
## 2015.10.05 Version 4.0.0
- Removed deprecated table AtomPub support.
- Removed deprecated constructors which take service clients in favor of constructors which take credentials.
- Added support for "Add" permissions on Blob SAS.
- Added support for "Create" permissions on Blob and File SAS.
- Added support for IP Restricted SAS and Protocol SAS.
- Added support for Account SAS to all services.
- Added support for Minute and Hour Metrics to FileServiceProperties and added support for File Metrics to CloudAnalyticsClient.
- Removed deprecated startCopyFromBlob() on CloudBlob. Use startCopy() instead.
- Removed deprecated Credentials and StorageKey classes. Please use the appropriate methods on StorageCredentialsAccountAndKey instead.
## 2015.09.16 Version 3.1.0
- Fixed a bug in table where a select on a non-existent field resulted in a null reference exception if the corresponding field in the TableEntity was not nullable.
- Fixed a bug in table where JsonParser was automatically closing the response stream before it was completely drained causing socket exhaustion.
- Fixed a bug in StorageCredentialsAccountAndKey.updateKey(String) which prevented valid keys from being set.
- Added CloudBlobContainer.listBlobs(final String, final boolean) method.
- Fixed a bug in blob where using AccessConditions on block blob uploads larger than 64MB done with the upload\* methods or block blob uploads done openOutputStream with would fail if the blob did not already exist.
- Added support for setting a proxy per request. Proxy can be set on an OperationContext instance and will be used when that instance is passed to the request method.
## 2015.08.04 Version 3.0.0
- Added support for SAS to the Azure File service.
- Added support for Append Blob.
- Added support for Access Control Lists (ACL) to File Shares.
- Added support for getting and setting of CORS rules to File service.
- Added support for ShareStats to File Shares.
- Added support for copying an Azure File to another Azure File or a Block Blob asynchronously, and aborting Azure File copy operations asynchronously.
- Added support for copying a Blob to an Azure File asynchronously.
- Added support for setting a maximum quota property on a File Share.
- Removed deprecated AuthenticationScheme and its getter and setter. In the future only SharedKey will be used.
- Removed deprecated getter/setters for all request option properties on the service clients. Please use the default request options getter/setters instead.
- Removed getSubDirectoryReference() for blob directories and file directories. Use getDirectoryReference() instead. 
- Removed getEntityClass() in TableQuery. Please use getClazzType() instead.
- Added client-side verification for lease duration and break periods.
- Deprecated the setters in table for timestamp as this property is only modifiable by the service.
- Deprecated startCopyFromBlob() on CloudBlob. Use startCopy() instead.
- Deprecated the Credentials and StorageKey classes. Please use the appropriate methods on StorageCredentialsAccountAndKey instead.
- Deprecated constructors which take service clients in favor of constructors which take credentials.
- Fixed a bug where the DateBackwardCompatibility flag was not applied if set on the CloudTableClient default request options.
- Changed library behavior to retry all exceptions thrown when parsing a response object.
- Changed behavior to stop removing query parameters passed in with the resource URI if that URI contains a SAS token. Some query parameters such as comp, restype, snapshot and api-version will still be removed.
- Added support for logging StringToSign to SharedKey and SAS.
- **Added a connect timeout to prevent hangs when establishing the network connection.**
- **Made performance enhancements to the BlobOutputStream class.**
## 2015.05.26 Version 2.2.0
- Fixed a bug where maximum execution time was ignored for file, queue, and table services.
- **Changed the socket timeout to be set to the service side timeout plus 5 minutes when maximum execution time is not set.**
- **Changed the socket timeout to default to 5 minutes rather than infinite when neither service side timeout or maximum execution time are set.**
- Fixed a bug where MD5 was calculated for commitBlockList even though UseTransactionalMD5 was set to false.
- Fixed a bug where selecting fields that did not exist returned an error rather than an EntityProperty with a null value.
- Fixed a bug where table entities with a single quote in their partition or row key could be inserted but not operated on in any other way.
## 2015.04.01 Version 2.1.0
- Fixed a bug for all listing API's where next() would sometimes throw an exception if hasNext() had not been called even if there were more elements to iterate on.
- Added sequence number to the blob properties. This is populated for page blobs.
- Creating a page blob sets its length property.
- Added support for page blob sequence numbers and sequence number access conditions. 
- Fixed a bug in abort copy where the lease access condition was not sent to the service.
- Fixed an issue in startCopyFromBlob where if the URI of the source blob contained certain non-ASCII characters they would not be encoded appropriately. This would result in Authorization failures.
- Fixed a small performance issue in XML serialization.
- Fixed a bug in BlobOutputStream and FileOutputStream where flush added data to a request pool rather than immediately committing it to the Azure service.
- Refactored to remove the blob, queue, and file package dependency on table in the error handling code.
- Added additional client-side logging for REST requests, responses, and errors.
</description><key id="126604435">15976</key><summary>Upgrade Azure Storage client to 4.0.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label><label>upgrade</label></labels><created>2016-01-14T08:28:56Z</created><updated>2016-02-29T14:18:11Z</updated><resolved>2016-02-29T14:18:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-14T08:47:30Z" id="171573972">I updated the ticket description and highlighted some of enhancement which could help us fixing #12567
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show force merge/optimize progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15975</link><project id="" key="" /><description>Are we able to expose this somehow, maybe pending tasks?
At the moment it just runs until finished, but I'm hoping there may be a way to figure out some kind of progress percentage, even a rough one.

I realise this is all a little vague, but I don't really know if it's possible, just that it'd be nice to have :p
</description><key id="126589682">15975</key><summary>Show force merge/optimize progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>discuss</label><label>enhancement</label></labels><created>2016-01-14T06:41:08Z</created><updated>2016-01-14T12:04:39Z</updated><resolved>2016-01-14T12:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T12:04:39Z" id="171627320">If it can be done, it'll be via the task management API.  Closing in favour of https://github.com/elastic/elasticsearch/issues/15117
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_recovery api shows negative metrics after translog replay</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15974</link><project id="" key="" /><description>Tested on 2.1.1:

```
      "translog" : {
        "recovered" : 13,
        "total" : -1,
        "percent" : "-1.0%",
        "total_on_start" : -1,
        "total_time_in_millis" : 340
      },
```

I have 13 entries in the translog for a shard, once it is done replaying on startup, if I hit the _recovery api and look at the translog section for the shard, it shows 13 recovered.  But the percent and total metrics are negative which is misleading.
</description><key id="126579613">15974</key><summary>_recovery api shows negative metrics after translog replay</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-14T05:20:00Z</created><updated>2016-02-08T11:01:06Z</updated><resolved>2016-02-08T11:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="micpalmia" created="2016-01-14T12:58:37Z" id="171637487">I also have recently seen this, on 2.1.0, on three shards stuck in `TRANSLOG` stage after an unfortunate index `_open`/`_close` procedure went bad.

Unfortunately, I did not fully understand what happened, but seeing those negative metrics in the `translog` section lead me to think the the index must have been corrupted.
</comment><comment author="durera" created="2016-01-17T17:34:29Z" id="172356066">I've been seeing similar since we moved to 2.1 along with length recovery times when we hit translog stage

```
translog: {
  recovered: 16207,
  total: -1,
  percent: "-1.0%",
  total_on_start: -1,
  total_time_in_millis: 1511096
}
```
</comment><comment author="robison" created="2016-02-06T04:54:53Z" id="180681584">Same. Been seeing exceptionally long recovery times for just a handful of not incredibly large shards.

```
  "translog": {
    "recovered": 2413666,
    "total": -1,
    "percent": "-1.0%",
    "total_on_start": -1,
    "total_time": "4.5h",
    "total_time_in_millis": 16535797
  },
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update bulk.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15973</link><project id="" key="" /><description>The `close` method doesn't wait for any remaining bulk requests to complete and exists immediately.

It looks like `close` exits immediately not exists immediately.
</description><key id="126579586">15973</key><summary>Update bulk.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DynamicScope</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-01-14T05:19:28Z</created><updated>2016-01-18T10:59:34Z</updated><resolved>2016-01-18T10:59:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T12:02:17Z" id="171626960">Hi @DynamicScope 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="DynamicScope" created="2016-01-17T01:39:08Z" id="172282925">FYI, I signed the CLA.
</comment><comment author="clintongormley" created="2016-01-18T10:59:34Z" id="172497600">thanks @DynamicScope 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fuzziness=auto doesn't work in query string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15972</link><project id="" key="" /><description>https://gist.github.com/masaruh/55deca377b6ade5847de
(on 1.x, it's always `~2`)

Probably because it always uses fixed value https://github.com/elastic/elasticsearch/blob/v2.1.1/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java#L141

Probably it should be something like?

```
        if (fuzzySlop.image.length() == 1) {
            return getFuzzyQuery(qfield, termImage, settings.getFuzziness().asDistance(termImage));
        }
```
</description><key id="126577870">15972</key><summary>fuzziness=auto doesn't work in query string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2016-01-14T05:05:10Z</created><updated>2016-09-02T14:33:50Z</updated><resolved>2016-09-02T14:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T12:31:14Z" id="190702796">We should probably change the default fuzziness ( eg `foo~`) to use `AUTO` instead of `2`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce node handshake</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15971</link><project id="" key="" /><description>This commit introduces a handshake when initiating a light
connection. During this handshake, node information, cluster name, and
version are received from the target node of the connection. This
information can be used to immediately validate that the target node is
a member of the same cluster, and used to set the version on the
stream. This will allow us to extend APIs that are used during initial
cluster recovery without a major version change.

Closes #9061 
</description><key id="126569969">15971</key><summary>Introduce node handshake</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Discovery</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-01-14T03:30:21Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-05-05T00:06:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-14T03:31:57Z" id="171519543">@bleskes I'm not happy with some decisions that were forced, but it's easier to discuss code written than code not written so let's use this as a starting point?
</comment><comment author="nik9000" created="2016-01-14T03:48:21Z" id="171521104">&gt; This will allow us to extend APIs that are used during initial
&gt; cluster recovery with a major version change.

s/with/without/ ?
</comment><comment author="jasontedor" created="2016-01-14T03:50:42Z" id="171521399">&gt; s/with/without/ ?

@nik9000 Thanks! I edited in place.
</comment><comment author="nik9000" created="2016-01-14T03:56:19Z" id="171521903">&gt; @nik9000 Thanks! I edited in place.

thanks!
</comment><comment author="nik9000" created="2016-02-10T14:33:22Z" id="182398969">This seems like a nice thing to get into 3.0. Is there anything I can do to help?
</comment><comment author="dakrone" created="2016-04-06T17:19:57Z" id="206472834">Ping @bleskes and @jasontedor whether this is still something we want in 5.0? It's breaking so we need to get it in if we want it before 5.0 is out.
</comment><comment author="clintongormley" created="2016-04-14T18:49:45Z" id="210096546">@bleskes any chance you could review this?
</comment><comment author="jasontedor" created="2016-04-20T13:54:17Z" id="212434473">@bleskes This has been rebased on master and I've removed the part that previously troubled me. :smile:
</comment><comment author="jasontedor" created="2016-05-03T12:07:27Z" id="216508413">@nik9000 I think that I've responded to all of your feedback here. Do you mind taking another look?
</comment><comment author="nik9000" created="2016-05-04T19:50:35Z" id="216981470">Left a minor question. Otherwise LGTM.
</comment><comment author="jasontedor" created="2016-05-05T00:06:56Z" id="217043764">Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Be able to specify translog path in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15970</link><project id="" key="" /><description>It would be nice to be able to place translog on a different filesystem from the Elasticsearch data files.  So  upon restart of a node, replaying translog and writing data files won't hit the same disk.
</description><key id="126561354">15970</key><summary>Be able to specify translog path in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sherry-ger</reporter><labels><label>:Translog</label><label>discuss</label><label>feature</label></labels><created>2016-01-14T02:13:06Z</created><updated>2016-01-15T10:41:27Z</updated><resolved>2016-01-15T10:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T11:57:11Z" id="171625387">I don't like this idea for a few reasons.  The first is that we're now managing two directory trees which may or may not overlap, so you have to be extra careful when creating them, cleaning them up or moving them around.  The chance of introducing bugs here is high.

The second is that we have just moved away from splitting shards across multiple paths to ensuring that a single shard is contained entirely on a single disk.  The reason for this was to avoid the loss of one path corrupting all of your shards.  If users have a separate path just for translogs then we're back to the same situation.

My feeling is: if your disks aren't keeping up, then get SSDs or add more hardware.
</comment><comment author="clintongormley" created="2016-01-15T10:41:26Z" id="171930335">Discussed this in FixItFriday - @s1monw pointed out two important issues:
- An entire shard needs to belong to a single path so that it if any part fails, the whole shard fails.
- When nodes are shut down, they flush to disk, meaning that there is no translog to replay on startup.  Only in case of disastrous shutdown do translogs need to be replayed.  Introducing the complexity of multiple paths for this outlier is not worth it.

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ban DNS lookups with forbidden apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15969</link><project id="" key="" /><description>followup to #15851

The idea here is that only a very few places really need to do DNS lookups, we should prevent it happening by accident. Abuse cases of InetAddress.getByName for other purposes (e.g. parsing addresses, getting the loopback address) can be done instead with other safer methods. 

Especially parsing addresses, its better to use `InetAddresses.forString`, which won't do a lookup but just throw an exception if the address is wrong.

This cleans up the tests around this for the most part. There are some exceptions where we just `SuppressForbidden`, such as tests for InetAddresses.forString itself, which intentionally do round-trip testing against InetAddress, etc.
</description><key id="126555853">15969</key><summary>ban DNS lookups with forbidden apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2016-01-14T01:29:32Z</created><updated>2016-01-14T07:45:02Z</updated><resolved>2016-01-14T02:20:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-14T02:20:49Z" id="171508517">I'm closing this, its not worth it to me if it results in more complex code. I just want to prevent mistakes.
</comment><comment author="rmuir" created="2016-01-14T05:30:27Z" id="171535831">I opened this to try to improve the situation: https://github.com/policeman-tools/forbidden-apis/issues/96

I definitely think we should not restructure our real code in unnatural ways (e.g. factoring out strange methods) to try to satisfy a static analysis tool. The code should be as simple as possible and structured in the way that makes it easiest to understand. I've seen this a few times lately and we gotta do something about it, we already have too many things tugging at the quality of the code, we don't need forbidden apis added to that too.
</comment><comment author="dadoonet" created="2016-01-14T07:40:19Z" id="171563703">Even if you close it, I added a few comments. I think we should do it.
</comment><comment author="rmuir" created="2016-01-14T07:45:02Z" id="171564550">I am just unhappy with the current solution. Honestly, I was on the fence to even make a pull request about it, because its a tricky thing to contain. So when nik didn't like it either, it just convinced me, you know with forbidden-apis as it is now, this isn't a good tradeoff to make. 

Maybe we can better fine-grain the annotation and revisit. But ive seen strange code recently that was factored in a way to reduce the forbidden-apis stuff, hell i know i've even suggested it before, but in general I really think its a bad tradeoff.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Introduce DeDot Processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15968</link><project id="" key="" /><description>fixes #15944.
</description><key id="126549136">15968</key><summary>[Ingest] Introduce DeDot Processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-14T00:30:54Z</created><updated>2016-01-15T19:37:12Z</updated><resolved>2016-01-15T19:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-14T09:13:52Z" id="171581300">@talevy Left a few comments. Can you also add some basic docs?
</comment><comment author="jimczi" created="2016-01-14T10:30:14Z" id="171600199">&gt; should this recursively search and replace across all fields?
&gt; I left it to only search and replace across top-level fields. 

Is this something we don't want to provide for the first iteration ? I don't see why we should not recursively check for dots in inner map or list.
</comment><comment author="javanna" created="2016-01-14T13:18:44Z" id="171642147">&gt; Is this something we don't want to provide for the first iteration ? I don't see why we should not recursively check for dots in inner map or list.

yes we want to provide this, it's part of the PR already
</comment><comment author="javanna" created="2016-01-15T11:22:07Z" id="171937182">left a comment around testing, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Fix some todos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15967</link><project id="" key="" /><description /><key id="126532461">15967</key><summary>[Ingest] Fix some todos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-13T22:36:44Z</created><updated>2016-01-14T09:15:26Z</updated><resolved>2016-01-14T09:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-13T22:45:47Z" id="171461363">LGTM
</comment><comment author="javanna" created="2016-01-14T07:55:35Z" id="171565950">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize which MockPlugins are used for IT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15966</link><project id="" key="" /><description>in 2.x we randomly swap mocks in/out. This was lost on the way towards a better build...Now it's coming back with even more randomization
</description><key id="126527234">15966</key><summary>Randomize which MockPlugins are used for IT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T22:07:10Z</created><updated>2016-01-13T22:58:51Z</updated><resolved>2016-01-13T22:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-13T22:18:01Z" id="171454893">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Misleading error message for geo_point parse failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15965</link><project id="" key="" /><description>Even without enabling `geohash` functionality for the `geo_point` type, if it fails to parse a string in the format of `"&lt;lat&gt;,&lt;lon&gt;"`, then it throws a misleading error:

```
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: the character '.' is not a valid geohash character
```

It would be extremely useful to show the:
- Name of the malformed field
- The value of the malformed field (in addition to the reason the field failed to parse)

In my case, a field was accidentally mapped as a `geo_point` when it was meant to be an IP address, which made it hard to determine which field was the cause.

``` json
{
  "ipv4_src_addr" : "127.0.0.1"
}
```
</description><key id="126527051">15965</key><summary>Misleading error message for geo_point parse failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-13T22:06:05Z</created><updated>2017-03-15T06:41:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mortonsykes" created="2017-03-15T06:41:41Z" id="286654733">Hi i am new to Elastic Search and would be interested in taking this issue. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove updatability of `index.flush_on_close`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15964</link><project id="" key="" /><description>`index.flush_on_close` is a test setting and doesn't need to be updatable.

Relates to #15955

ping @jimferenczi @nik9000 
</description><key id="126497057">15964</key><summary>Remove updatability of `index.flush_on_close`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T19:41:19Z</created><updated>2016-01-13T21:55:28Z</updated><resolved>2016-01-13T21:55:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-13T19:48:50Z" id="171412441">LGTM, thanks for the  followup !
</comment><comment author="nik9000" created="2016-01-13T21:12:05Z" id="171434060">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update plugins.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15963</link><project id="" key="" /><description>--install is no longer a valid command
the new command is simply install without--

bin/plugin --install mobz/elasticsearch-head
should be changed to
bin/plugin install mobz/elasticsearch-head

All other plugin installation commands on this page should be changed similarly.

propose updating commands to reflect this change

The BigDesk plugin does not work with Elasticsearch 2.1.1.  
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip.  
BigDesk plugin examples should be removed.  
</description><key id="126486007">15963</key><summary>Update plugins.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CSUMBmWall</reporter><labels /><created>2016-01-13T18:45:45Z</created><updated>2016-01-13T19:22:57Z</updated><resolved>2016-01-13T19:22:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-13T19:22:57Z" id="171405346">It still correct on 1.4 branch which is the branch you were looking at.

But thanks for the effort!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix MatchedQueriesFetchSubPhase's consumption of DocIdSetIterator.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15962</link><project id="" key="" /><description>Closes #15949
</description><key id="126482855">15962</key><summary>Fix MatchedQueriesFetchSubPhase's consumption of DocIdSetIterator.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v1.7.5</label></labels><created>2016-01-13T18:31:16Z</created><updated>2016-01-14T12:09:08Z</updated><resolved>2016-01-14T08:48:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-13T20:33:32Z" id="171424323">LGTM
</comment><comment author="jpountz" created="2016-01-13T21:09:28Z" id="171433182">@martijnvg actually I had to push another commit to fix a test failure. Could you look at it, it's unclear to me why named queries on top hits suddenly work.
</comment><comment author="martijnvg" created="2016-01-13T21:47:00Z" id="171446720">I think my assumption that it didn't work was just wrong and it not working was caused by this bug. With `docId != matchedDocId` it wouldn't check the nested docs and skip over them too soon this isn't the case with `docId &lt; matchedDocId`.
</comment><comment author="jpountz" created="2016-01-13T21:49:05Z" id="171447762">Ok, so that's two bugs in one fix. :) Thanks for looking.
</comment><comment author="martijnvg" created="2016-01-13T21:50:57Z" id="171448260">thanks for fixing :) luckily this code is a bit simpler in 2.x/master branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loss data while using bulk request to create a new index at first time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15961</link><project id="" key="" /><description>Version: 2.1.1, Build: 40e2c53/2015-12-15T13:05:55Z, JVM: 1.8.0_60

``` json
message.json

{"index":{"_index":"test","_type":"message","_id":"1"}}
{"integer":1}
{"index":{"_index":"test","_type":"message","_id":"2"}}
{"integer":1.0}

```

``` bash
#1. Delete every thing
curl -XDELETE "http://localhost:9200/*"

#2. Bulk request
curl -s -XPOST localhost:9200/_bulk --data-binary @message.json

```

``` json
{
    "errors": true,
    "items": [
        {
            "index": {
                "_id": "1",
                "_index": "test",
                "_type": "message",
                "error": {
                    "reason": "Merge failed with failures {[mapper [integer] of different type, current_type [double], merged_type [long]]}",
                    "type": "merge_mapping_exception"
                },
                "status": 400
            }
        },
        {
            "index": {
                "_id": "2",
                "_index": "test",
                "_shards": {
                    "failed": 0,
                    "successful": 1,
                    "total": 2
                },
                "_type": "message",
                "_version": 1,
                "status": 201
            }
        }
    ],
    "took": 40
}
```

``` bash
[2016-01-14 01:25:30,179][INFO ][cluster.metadata         ] [Swarm] [test] deleting index
[2016-01-14 01:25:31,405][INFO ][cluster.metadata         ] [Swarm] [test] creating index, cause [auto(bulk api)], templates [], shards [5]/[1], mappings [message]
[2016-01-14 01:25:31,425][INFO ][cluster.metadata         ] [Swarm] [test] update_mapping [message]
[2016-01-14 01:25:31,428][DEBUG][action.admin.indices.mapping.put] [Swarm] failed to put mappings on indices [[test]], type [message]
MergeMappingException[Merge failed with failures {[mapper [integer] of different type, current_type [double], merged_type [long]]}]
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$2.execute(MetaDataMappingService.java:392)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-01-14 01:25:31,429][DEBUG][action.bulk              ] [Swarm] [test][3] failed to execute bulk item (index) index {[test][message][1], source[{"integer":1}]}
MergeMappingException[Merge failed with failures {[mapper [integer] of different type, current_type [double], merged_type [long]]}]
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$2.execute(MetaDataMappingService.java:392)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

``` json
curl "http://localhost:9200/test/_search?pretty"

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "message",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"integer":1}
    } ]
  }
}
```
</description><key id="126470582">15961</key><summary>Loss data while using bulk request to create a new index at first time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yzprofile</reporter><labels /><created>2016-01-13T17:26:52Z</created><updated>2016-01-14T12:50:40Z</updated><resolved>2016-01-14T09:12:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-14T09:09:39Z" id="171580540">This does not qualify as a data loss given that only one write was acknowledged (see the error in the bulk response).

However there seems to be a problem indeed as the second document should have picked the mapping definition generated by the first document instead of trying to map the field as a double.
</comment><comment author="clintongormley" created="2016-01-14T09:12:43Z" id="171581099">@jpountz These two docs go to different shards, so the second doc was mapped before the first.  If you run this a few times, you'll see the order changing.  Nothing to fix here.
</comment><comment author="jpountz" created="2016-01-14T09:25:02Z" id="171583795">Oh yes indeed!
</comment><comment author="yzprofile" created="2016-01-14T10:12:16Z" id="171594826">``` json
POST /test/
{
  "mappings": {
    "_default_": {
      "properties": {
        "integer": {
          "type": "double"
        }
      }
    }
  }
}
```

It will work well if I create mapping before sending bulk request.

And there will not be an error: "merge failed".

If the "double"  support to index "long" data ,  Why don't make it compatible?

Thanks.
</comment><comment author="clintongormley" created="2016-01-14T12:25:27Z" id="171631059">Hmmm this is very tricky.  One shard succeeds in adding a `double` field, then the other tries to add an `integer` field, which causes a merge conflict (because the field already exists and is of a different datatype).

If you were to reindex the integer document, it would be coerced to a double and accepted, but it'd be really hard to add this "retry" logic into the mapping code.  I think there are two practical solutions here:
- map your fields ahead of time, or
- ensure that fields with the same name contain the same datatype (ie pass `1.0` instead of `1`)
</comment><comment author="yzprofile" created="2016-01-14T12:50:40Z" id="171636027">Thanks for your time to explain, Issue close. 

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Does one Index and one type support multiple types or formates?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15960</link><project id="" key="" /><description>Does one Index and one type support multiple types or formates?

For example:

``` json
POST /maptest/
{
  "mappings": {
    "_default_": {
      "properties": {
        "time": {
          "type": "date",
          "format": "yyyy-MM-dd HH:mm:ss.SSS",
          "format": "yyyy-MM-dd HH:mm:ss"
        }
      }
    }
  }
}
```

There is a field has two formats under one index/type.
</description><key id="126452924">15960</key><summary>Does one Index and one type support multiple types or formates?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yzprofile</reporter><labels /><created>2016-01-13T16:08:55Z</created><updated>2016-01-13T16:22:29Z</updated><resolved>2016-01-13T16:14:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-13T16:14:15Z" id="171346240">Multiple formats can be [specified using `||`](https://github.com/elastic/elasticsearch/blob/0c08d796c5bf32991b32cead4b594b4e6198d183/core/src/main/java/org/elasticsearch/common/joda/Joda.java#L234-L254) since #169 was closed.

For future reference, questions are best asked on [the Elastic Discourse forums](https://discuss.elastic.co); GitHub is for filing feature requests and bug reports.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Clarify that null_value only modifies the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15959</link><project id="" key="" /><description>The `null_value` field does not modify `_source`, rather it only sets the indexed value and therefore makes it searchable.

I've found that a lot of people expect to retrieve the "set" value even though it's only indexed that way.
</description><key id="126450647">15959</key><summary>[DOCS] Clarify that null_value only modifies the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>docs</label><label>enhancement</label></labels><created>2016-01-13T15:58:08Z</created><updated>2016-01-15T12:38:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove dead fielddata code.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15958</link><project id="" key="" /><description>This became dead when we removed in-memory fielddata for numeric fields.
</description><key id="126439909">15958</key><summary>Remove dead fielddata code.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T15:13:28Z</created><updated>2016-01-13T17:33:36Z</updated><resolved>2016-01-13T17:33:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-13T17:11:50Z" id="171367132">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Adding node.max_local_storage_nodes setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15957</link><project id="" key="" /><description>This setting was missing from the docs, so I added it. However, I also
completely rewrote the nodes documentation page because it was mostly
talking about client nodes with some issues, without ever discussing
master nodes, or even tribe nodes. All nodes should be listed on a
"nodes" documentation page.

Fixes #15903
Fixed #14429
</description><key id="126438634">15957</key><summary>[DOCS] Adding node.max_local_storage_nodes setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Settings</label><label>docs</label></labels><created>2016-01-13T15:07:21Z</created><updated>2016-01-30T17:15:14Z</updated><resolved>2016-01-30T17:15:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-01-13T16:34:57Z" id="171354313">@clintongormley Want to take a look, since you so gracefully asked me to write it?
</comment><comment author="clintongormley" created="2016-01-14T09:09:42Z" id="171580554">Big improvement, i've left some suggestions
</comment><comment author="pickypg" created="2016-01-18T19:06:07Z" id="172624602">Thanks! @clintongormley Updated. Let me know if you don't like any of the changes.
</comment><comment author="clintongormley" created="2016-01-19T12:26:17Z" id="172836422">Looking great.  I'm just wondering about the settings section at the end.  I think we are documenting a number of settings that are already documented elsewhere, no?  I'd prefer to just mention "important settings" and link to the appropriate sections.
</comment><comment author="pickypg" created="2016-01-19T17:17:00Z" id="172923104">The entire `Network Configuration` section is a bit redundant due to other documentation, but I added specifically those settings because they are each critical to normal setup and questions that users have about nodes. I also like that it helps to explain the purpose of each port, as most users are generally confused about which one gets used and why.

I will probably drop the bit about `http.enabled` as that's a bit advanced.
</comment><comment author="clintongormley" created="2016-01-29T20:32:41Z" id="176956177">@pickypg would be good to get this merged
</comment><comment author="clintongormley" created="2016-01-30T17:15:14Z" id="177251267">Closed by c0a7f8889781f342c6c79696de904bc4ef47314b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Operation counter for IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15956</link><project id="" key="" /><description>Relates to the request in #15900.

Container that represents a resource with reference counting capabilities. Provides operations to suspend acquisition of new references. Useful for resource management when resources are intermittently unavailable.

Using a fair semaphore is based on the following (tested) observations:
- as long as permits can be freely acquired, no noticeable performance difference between fair and non-fair version of semaphore.
- when suspending acquisition of new references, it is important that the process acquiring all locks does not starve. This can only be guaranteed by fair semaphore.
- the given implementation does 10 million (acquire, release) operations per second on my laptop (tested with 50 concurrent threads). Should be fast enough ;-)
</description><key id="126427093">15956</key><summary>Operation counter for IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T14:19:57Z</created><updated>2016-02-02T15:03:31Z</updated><resolved>2016-02-02T09:01:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-15T06:50:16Z" id="171885120">Thanks @ywelsch. I left some very minor comments.
</comment><comment author="ywelsch" created="2016-01-15T08:51:43Z" id="171904455">Pushed a new change set addressing comments. Please have another look @bleskes.
</comment><comment author="bleskes" created="2016-01-17T07:47:17Z" id="172298839">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move all dynamic settings and their config classes to the index level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15955</link><project id="" key="" /><description>Today we maintain a lot of settings on the shard level which are all index level settings.
In order to cut over to the new settings API where we register update listener we have to move
all of them on to the index level otherwise we need a way to un-register listeners which is error-prone
and requires additional handling when shards are closed. It's simpler and also more accurate to handle all of
them on the index level where we can trash the entire registry for update listener once the index goes out of scope.

Related to #6732
</description><key id="126418730">15955</key><summary>Move all dynamic settings and their config classes to the index level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T13:34:01Z</created><updated>2016-01-14T09:16:08Z</updated><resolved>2016-01-13T19:35:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-13T14:08:37Z" id="171300978">LGTM
</comment><comment author="nik9000" created="2016-01-13T14:59:45Z" id="171321118">LGTM. @jimferenczi has a point about maybe removing INDEX_FLUSH_ON_CLOSE, probably worth its own issue. Left a silly comment about naming that can come later/never.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make get alias expand to open and closed indices by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15954</link><project id="" key="" /><description>This change affects get alias, get aliases as well as cat aliases. They all return closed indices too by default. get alias and get aliases also allow to return open indices only through the `expand_wildcards` option (set it to `open`).

Closes #14982
</description><key id="126416728">15954</key><summary>make get alias expand to open and closed indices by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Aliases</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T13:21:22Z</created><updated>2016-01-14T13:24:40Z</updated><resolved>2016-01-14T13:24:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-14T09:39:03Z" id="171586563">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make RescoreBuilder and nested QueryRescorer Writable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15953</link><project id="" key="" /><description>Adding serialization capabilities to RescoreBuilder and make all QueryRescorer implement NamedWritable, also requiring all implementations of RescoreBuilder.Rescorer to implement
equals() and hashCode.

In addition, the current rescore mode enumeration is pulled out to a separate class to make sharing of constants easier between the query builders XContent rendering coder and the parser.

Relates to #15559
</description><key id="126411531">15953</key><summary>Make RescoreBuilder and nested QueryRescorer Writable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T12:48:53Z</created><updated>2016-01-14T14:55:18Z</updated><resolved>2016-01-14T14:55:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-13T15:53:09Z" id="171339919">@cbuescher left a couple of comments but it looks good
</comment><comment author="cbuescher" created="2016-01-14T14:35:35Z" id="171659063">@colings86 I think I found a way to adress your comments, using ParseFields will be part of the follow-up PR that adds fromXContent() and build(). Anything else to add or are you okay with merging this?
</comment><comment author="colings86" created="2016-01-14T14:52:08Z" id="171663049">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid circular reference in exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15952</link><project id="" key="" /><description>Don't set the suppressed Exception in Translog.closeOnTragicEvent(Exception ex) if it is an
AlreadyClosedException. ACE is thrown by the TranslogWriter and as cause might
contain the Exception that we add the suppressed ACE to. We then end up with a
circular reference where Exception A has a suppressed Exception B that has as cause A.
This would cause a stackoverflow when we try to serialize it.
For a more detailed description see #15941

closes #15941
</description><key id="126411253">15952</key><summary>Avoid circular reference in exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Translog</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-13T12:47:03Z</created><updated>2016-01-22T11:10:03Z</updated><resolved>2016-01-13T15:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-13T13:20:38Z" id="171287771">LGTM - left a minor one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allowing dots in field names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15951</link><project id="" key="" /><description>As part of the Great Mapping Refactoring (#8870), we had to reject field names containing dots (https://github.com/elastic/elasticsearch/pull/12068), eg:

```
{ 
  "foo.bar": "val1",  
  "foo": {
    "bar": "val2"
  }
}
```

The behaviour was undefined and resulted in ambiguities when trying to reference fields with the dot notation used in queries and aggregations.

Removing support for dots has caused pain for a number of users and especially as Elasticsearch is being used more and more for the metrics use case (where dotted fields are common), we should consider what we can do to improve this situation.  Now that mappings are much stricter (and immutable), it becomes feasible to revisit the question of whether to allow dots to occur in field names.  
# Replace dots with another character

The first and simplest solution is to simply replace dots in field names with another character (eg `_`) as is done by the [Logstash de_dot filter](https://www.elastic.co/guide/en/logstash/current/plugins-filters-de_dot.html) and which will be supported natively in Elasticsearch by the [node ingest `de_dot` processor](https://github.com/elastic/elasticsearch/issues/15944).
# Treat dots as paths

Another solution would be to treat fields with dots in them as "paths" rather than field names.  In other words, these two documents would be equivalent:

```
{ "foo.bar": "value" }
{ "foo": { "bar": "value" }}
```

To use an edge case as an example, the following document:

```
{
  "foo.bar" : {
    "baz": "val1"
  },
  "foo": {
    "bar.baz": "val2"
  }
```

  }

would result in the following mapping:

```
{
  "properties": {
    "foo": {
      "type": "object",
      "properties": {
        "bar": {
          "type": "object",
          "properties": {
            "baz": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```

The lucene field would be called `foo.bar.baz` and would contain the terms `["val1", "val2]`.  Stored fields or doc values (for supported datatypes), would both contain `["val1", "val2"]`.
## Issues with this approach

This solution works well for search and aggregations, but leaves us with two incongruities:
### `_source=`

The first occurs when using the `_source=` parameter to do source filtering on the response. The reason for this is that the `_source` field is stored as provided - it is not normalized before being stored  For instance:

```
GET _search?_source=foo.*
```

would return:  

```
{
  "foo.bar" : {
    "baz": "val1"
  },
  "foo": {
    "bar.baz": "val2"
  }
```

  }

rather than:

```
{
  "foo": {
    "bar": {
      "baz": [
        "val1",
        "val2"
      ]
    }
  }
}
```
### Update requests

The second occurs during update requests, which uses the `_source` as a map-of-maps.  Running an update like:

```
POST index/type/id/_update
{
  "doc": {
    "foo": {
      "bar": {
        "baz": "val3"
      }
    }
  }
}
```

could result (depending on how it is implemented) in any of the following:  

Version 1:

```
{
  "foo": {
    "bar": {
      "baz": "val3"
    }
  }
}
```

Version 2: 

```
{
  "foo": {
    "bar": {
      "baz": [
        "val1",
        "val2",
        "val3"
      ]
    }
  }
}
```

Version 3: 

```
{
  "foo.bar": {
    "baz": "val1"
  },
  "foo": {
    "bar.baz": "val2",
    "bar": {
      "baz": "val3"
    }
  }
}
```
</description><key id="126410546">15951</key><summary>Allowing dots in field names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>discuss</label><label>Meta</label></labels><created>2016-01-13T12:42:18Z</created><updated>2016-11-22T10:06:26Z</updated><resolved>2016-05-16T08:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2016-01-13T16:39:33Z" id="171356629">I spoke to @rjernst, and it might be simpler to disallow inconsistent dots in fields. So the first full path dots we allow, and the second one, we reject (similar to what we do with conflict on types). As an example: first allow `{"x.y" : { "z" : "test" } }`, then reject something like `{"x" : {"y.z" : "test" }}`

If we can do this, it might also be simpler to do it in 2.x, and it is more constraint compared to the above solution, and later we can extend (if we need to) to implement the above.
</comment><comment author="rjernst" created="2016-01-13T19:37:01Z" id="171409418">@kimchy I don't think that will actually work, because of how we parse values and just append them. I don't think we could distinguish, without some nasty-ish logic, if a field is just appending to an existing field, or another field with the same path (and same goes for inside the mapper service itself with storing the mappers).

@clintongormley There are two things that bug me. First, why do we have _source filtering at all? We already have stored fields which can serve that same purpose (returning a subset of the document on search).  The second thing that bugs me is that your example works at all. We allow duplicate values for a field to append instead of error? That is leniency at its best: I don't know of any json parsers that emit arrays as duplicate keys (at least not by default), which means the user is probably serializing themselves, and very likely has a bug in their serialization.  I don't think we should support either of those features, but dropping _source filtering would at least remove your concern so we could do the dots-as-paths option?
</comment><comment author="nik9000" created="2016-01-13T21:10:18Z" id="171433388">I wonder if we can delay a decision on how to handle updates by not supporting dots in the document merge use case. You can always use scripts to be totally clear there if you need.
</comment><comment author="jpountz" created="2016-01-13T21:38:40Z" id="171442736">&gt; I don't think we could distinguish, without some nasty-ish logic, if a field is just appending to an existing field, or another field with the same path

Actually this should already work today. The second document will trigger a dynamic mapping update that will be rejected since the mapping would have two mappers that have the same path: #15243
</comment><comment author="clintongormley" created="2016-01-14T10:00:26Z" id="171592348">&gt; Actually this should already work today. The second document will trigger a dynamic mapping update that will be rejected since the mapping would have two mappers that have the same path: #15243 

If we go with treating dots as paths ,then this won't work correctly, eg a document containing both forms (eg `{ foo: { bar.baz:..}},{foo.bar:{ baz...}}}` will be rejected if indexed first, but if indexed after a document containing just one form, it will be accepted.

&gt; First, why do we have _source filtering at all?

@rjernst because users want to be able to get back what they put in, and to be able to distinguish between values such as:
- `""`
- `null`
- `[]`
- `"val"`
- `["val"]`
- `["val", null]`

You can't do this with stored fields.

&gt;  The second thing that bugs me is that your example works at all. We allow duplicate values for a field to append instead of error? That is leniency at its best: I don't know of any json parsers that emit arrays as duplicate keys (at least not by default), which means the user is probably serializing themselves, 

Where do you see duplicate keys?  

```
{
  "foo.bar" : {
    "baz": "val1"
  },
  "foo": {
    "bar.baz": "val2"
  }
}
```

The above is perfectly valid JSON - no duplicate keys there.  The fact that `foo: bar.baz: val` and `foo.bar: baz: val` end up being added to the same lucene field `foo.bar.baz` is just an artefact of the way we could translate dots to paths.
</comment><comment author="clintongormley" created="2016-01-14T10:07:47Z" id="171593937">&gt; I don't think that will actually work, because of how we parse values and just append them. I don't think we could distinguish, without some nasty-ish logic, if a field is just appending to an existing field, or another field with the same path (and same goes for inside the mapper service itself with storing the mappers).

The only way I can see this working is as follows.  Fields with dots are mapped with dots, so `{ "foo": { "bar.baz": "val" }}` would be mapped as:

```
{
  "properties": {
    "foo": {
      "type": "object",
      "properties": {
        "bar.baz": {
          "type": "string"
        }
      }
    }
  }
}
```

When adding a new field:

```
If the field name contains dots (eg `foo.bar.baz`):
    Does the first part of the field (`foo`) exist as a field in the mapping already?
        If yes: conflict
        If no: add field as `foo.bar.baz`
Else (field name does not contain dots, eg `foo`)
    Do any fields exist in the mapping which start with `foo.`?
        If yes: conflict
        If no:  add field as `foo`
```

This logic would prevent conflicting paths from being added.

When looking up a field (eg `foo.bar.baz`) in search/aggs etc:

```
Does `foo` exist?
Does `foo.bar` exist?
Does `foo.bar.baz` exist?
```
</comment><comment author="clintongormley" created="2016-01-14T10:12:38Z" id="171594879">By the way, the decision about dots also affects the node ingest plugin, which treats dots as steps in a path hierarchy, and has no support for escaping.  This may not be a problem as long as the `de_dot` processor runs first but otherwise, it'll suffer from similar issues to those described above.
</comment><comment author="rjernst" created="2016-01-18T20:51:54Z" id="172650175">@clintongormley The logic you described there for adding new fields and searching is exactly why I don't like that approach. That is much more complicated than what we have today (and I especially don't like that the lookup of a field for search becomes linear on the object level of the field).

I am still convinced that doing `dots-as-path` is the correct choice. There are really two sides to users pain here, the first is dynamic mappings, and the second is explicit mappings. In the first case, I believe we can implement it within the document parsing that we have (which is where dynamic mappings are determined), and can be done independently of the second case (which I believe is harder, but still doable).

As for your concerns about `_source`, my thoughts are as follows:
1. Source filtering should be viewed as a regex on the full path of a field, so I think returning the source as-is for all fields with a full path that match the regex is correct (so it should return your first example).
2. Update requests should work as they do today, which I believe merges the new document with the previous version. Therefore your "version 3" should be what happens. I do also think that we shouldn't be so concerned with returning a normalized view, but that can/should be explored in a separate issue.

@jpountz has expressed a concern with this approach and the edge cases it brings, in particular with nested fields. I think in the case, for example, where `foo` is already a nested field, we will need to reject `foo.bar` as a field, since it should require that `foo` is an object field. But I think that is completely doable and testable.

While discussing with @jpountz he also made me realize escaping might be simpler than I originally thought. However, I still think this `dots-as-path` approach is correct for a couple reasons:
1. It works with existing 1.x indexes for upgrade.
2. It does not complicate the api, or have possible confusion for users who pass in `foo.bar` in there document, but then later find they must query with `foo\.bar`.
</comment><comment author="clintongormley" created="2016-01-19T12:57:13Z" id="172845167">&gt; As for your concerns about _source, my thoughts are as follows:

Agreed on both counts.  

&gt; But I think that is completely doable and testable.

Good to hear.  As long as the implemented solution is known to deal with the edge cases correctly, I'm happy.

&gt; It works with existing 1.x indexes for upgrade.

Note, mappings in 1.x indices have field names like:

```
"foo": {
    "type": "object",
    "properties": {
        "bar.baz": {....
```

So that structure would need to be updated on upgrade to:

```
"foo": {
    "type": "object",
    "properties": {
        "bar": {
            "type": "object",
            "properties": {
                "baz": ...
```
</comment><comment author="GlenRSmith" created="2016-02-18T20:56:05Z" id="185915898">Any update on the likelihood of implementing something around this?
</comment><comment author="jpountz" created="2016-02-22T18:20:19Z" id="187303832">@GlenRSmith I know @rjernst is currently exploring treating dots in field names as sub objects.
</comment><comment author="felixbarny" created="2016-02-24T14:13:30Z" id="188271846">Another use case where I need dots in field names is for tracking request parameters. I currently store them like this:

```
"params": {
  "param1": "value1",
  "param2": "value2" 
}
```

I don't really have control over the names of the request parameters so the only option is to de_dot the parameter names. But then I can't use the stored information to reproduce/replay the captured request. Converting the parameters into

```
"params": [
  "key": "param1",
  "value": "value1"
]
```

isn't an option either, because I want do aggregations on specific parameters in Grafana.

Yet another use case of mine is that I store configuration parameters in Elasticsearch where the config keys are field names and contain dots.

So a big +1 from my side.
</comment><comment author="ryanmaclean" created="2016-03-02T22:34:47Z" id="191469504">I hate to pile on, but our use case is identical to @felixbarny's. Perhaps I'm not understanding the need to treat these as sub-objects, but if that could be an option (even if it were the default) it would be much better than the _only_ way to handle fields that contain dots. 
</comment><comment author="bneff" created="2016-03-14T21:57:24Z" id="196538840">I also have a use case similar to @felixbarny.  
</comment><comment author="felixbarny" created="2016-05-16T08:41:52Z" id="219376992">Nice! Could you explain/document how this works now?
</comment><comment author="bneelima84" created="2016-07-15T06:39:16Z" id="232873089">Hi, I am using 5.0.0.4 alpha release and tried to create an index with the below mapping (which has dots in field names); 

`{"mappings" : {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"def" : {&#160;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"_all": {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"enabled": false
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;},
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"_source": {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"enabled": false
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;},
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"properties" : {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"Id" : { "type" : "string", "index" : "not_analyzed", "store" : true },

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"first.Name" : { "type" : "string", "index" : "not_analyzed" },
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"Last.Name" : { "type" : "string", "index" : "not_analyzed" },
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"Middle.Name" : { "type" : "string", "index" : "not_analyzed" },
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;"Qual" : { "type" : "string", "index" : "not_analyzed" }
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}
&#160;&#160;&#160;&#160;}
&#160;}`

Bu this fails as below:
{
"error":&#160;&#160;&#160;
{
"root_cause":&#160;&#160;&#160;
[
1]&#160;&#160;
0: &#160;&#160;&#160;
{
"type":&#160;"mapper_parsing_exception"
"reason":&#160;"Field name [Middle.Name] cannot contain '.'"

## }

## 

"type":&#160;"mapper_parsing_exception"
"reason":&#160;"Failed to parse mapping [def]: Field name [Middle.Name] cannot contain '.'"
"caused_by":&#160;&#160;&#160;
{
"type":&#160;"mapper_parsing_exception"
"reason":&#160;"Field name [Middle.Name] cannot contain '.'"

## }

## }

"status":&#160;400
}

Am I missing anything ? 
</comment><comment author="rjernst" created="2016-07-15T07:31:38Z" id="232883086">The current support for dot in field names is for dynamic mappings and document parsing. When specifying mappings directly, you will still need to split up the fields recursively. I opened #19443 to address this.
</comment><comment author="cdenneen" created="2016-08-04T21:21:34Z" id="237687347">Can dots in field names be patched in 2.3.x. Otherwise it will require 1.x -&gt; 2.x (re-work to undo all the dots in field names), then 2.x -&gt; 5.x (allow dots back).
Currently this is a real show stopper in upgrading ES past 1.7 since can't use upgrade path and 1.x-&gt;5.x isn't supported.
</comment><comment author="s1monw" created="2016-08-04T21:33:29Z" id="237690416">@cdenneen we are looking into possible solutions I will update the issue when we have more to say.
</comment><comment author="jpountz" created="2016-08-04T21:55:08Z" id="237695618">@cdenneen I would like to clarify (it might be clear to you but not necessarily for other readers) that data will need to be reindexed anyway between 1.x and 5.x since elasticsearch only supports one major version back, and the version that matters in that case is the version that was used to create the index. So 5.x will not be able to read any index created in 1.x.
</comment><comment author="cdenneen" created="2016-08-04T22:07:34Z" id="237698300">@s1monw thanks

@jpountz yes that's why i was saying upgrade path 1.x-&gt;5.x isn't supported but 1.x-&gt;2.x and 2.x-&gt;5.x is... but in order to do that you'd have to undo the dot fields for the 2.x upgrade and then put them back in 5.x after that upgrade... so unless there is a 1.x-&gt;5.x upgrade path I would think there needs to be a 2.x patch to support this to allow the upgrade to work (stepping up the major versions)
</comment><comment author="GlenRSmith" created="2016-08-04T22:13:25Z" id="237699491">@cdenneen I think you're missing the point. A 2.x patch wouldn't help you. Indices created in 1.x can't be read in 5.x. Full stop. Not even if you had no conflicts and upgraded to 2.x first.
</comment><comment author="cdenneen" created="2016-08-05T01:24:20Z" id="237732214">Glen,

Upgrades from 1.x -&gt; 2.x wouldn't convert the index to 2.x standard so you
could do a 5.x upgrade later?

-Chris
</comment><comment author="jpountz" created="2016-08-05T07:25:33Z" id="237775868">@cdenneen No. An index that lives in a 2.x cluster but was created with 1.x cannot be upgraded to 5.x.
</comment><comment author="clintongormley" created="2016-08-11T15:35:27Z" id="239198855">@cdenneen just to clarify, if we get support for dots in fields into 2.4, you'd be able to upgrade to 2.4, reindex to a new index, the upgrade to 5.x

An alternate route would be to create a new 5.x cluster, then use reindex-from-remote to pull the indices you want to take with you into 5.x directly.
</comment><comment author="pktxu" created="2016-11-09T16:41:12Z" id="259461205">@clintongormley graylog is affected by this, is it still being considered for inclusion in an hypothetic 2.4.2 release?
</comment><comment author="rjernst" created="2016-11-09T16:43:05Z" id="259461700">Support for dots in field names was added in 2.4.0:
https://www.elastic.co/guide/en/elasticsearch/reference/2.4/dots-in-names.html
</comment><comment author="aslakhellesoy" created="2016-11-22T10:06:26Z" id="262198875">Solution is here: https://www.elastic.co/guide/en/elasticsearch/reference/2.4/dots-in-names.html</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeout settings (default to 5 minutes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15950</link><project id="" key="" /><description>By default, azure does not timeout. This commit adds support for a timeout settings which defaults to 5 minutes.
It's a timeout **per request** not a global timeout for a snapshot request.

It can be defined globally, per account or both. Defaults to `5m`.

``` yml
cloud:
    azure:
        storage:
            timeout: 10s
            my_account1:
                account: your_azure_storage_account1
                key: your_azure_storage_key1
                default: true
            my_account2:
                account: your_azure_storage_account2
                key: your_azure_storage_key2
                timeout: 30s
```

In this example, timeout will be 10s for `my_account1` and 30s for `my_account2`.

Backport of #15080 in 2.x branch.

Related to #14277.

(cherry picked from commit 96b3166c6da24c6d8ecd22e5a38c39d087cd8f18)
</description><key id="126392391">15950</key><summary>Add timeout settings (default to 5 minutes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>enhancement</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-13T10:52:24Z</created><updated>2016-01-19T17:35:44Z</updated><resolved>2016-01-19T16:52:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-13T10:53:07Z" id="171253118">@imotov Assigning to you. It's a backport of #15080 in 2.x branch.
</comment><comment author="dadoonet" created="2016-01-18T15:22:18Z" id="172558060">Discussed with @imotov today.

As seen in #12567, this PR alone won't fix anything at it sounds although se define timeout settings, azure storage client does not throw any exception (see tests in https://github.com/Azure/azure-storage-java/issues/71).
It could have been fixed in azure client 3.0.0 but the tests again don't seem to prove it.

At the very least, we won't merge this PR alone but only at the same time as we fix #15976.

Let's wait for the moment for Azure Storage Java SDK team's answer...
</comment><comment author="dadoonet" created="2016-01-19T11:46:52Z" id="172826672">@imotov You can move forward on this review as #16084 will fix the timeout issue we have seen so far.
Could you review it ASAP as it would be fantastic to have it in 2.0.4, 2.1.3 and 2.2.0?

cc @clintongormley 
</comment><comment author="imotov" created="2016-01-19T15:51:13Z" id="172894868">Left a minor comment. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nullpointer using named query when there are nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15949</link><project id="" key="" /><description>It is possible for a null pointer to happen when using a named query when you also have nested docs (seen on 1.4.3, I can't reproduce on 2.1.1 but given the number of things that have to be in place to reproduce I can't be that confident that it doesn't exist somewhere in the latest version).

A script to reproduce it can be found here: https://gist.github.com/tstibbs/92de2206531ca37e0764

The last command in the gist is a search request that should result in a null pointer, I see something like the following in the logs:

```
[2016-01-12 16:51:30,174][DEBUG][action.search.type       ] [Black Fox] [myindex][0], node[WYV4NA6SRNyRXepgS8QEQQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@43cc9843]
java.lang.NullPointerException
    at org.apache.lucene.search.DisjunctionScorer.advance(DisjunctionScorer.java:155)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.advance(ConstantScoreQuery.java:278)
    at org.elasticsearch.search.fetch.matchedqueries.MatchedQueriesFetchSubPhase.addMatchedQueries(MatchedQueriesFetchSubPhase.java:108)
    at org.elasticsearch.search.fetch.matchedqueries.MatchedQueriesFetchSubPhase.hitExecute(MatchedQueriesFetchSubPhase.java:80)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:372)
    at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:333)
    at org.elasticsearch.search.action.SearchServiceTransportAction$11.call(SearchServiceTransportAction.java:330)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2016-01-12 16:51:30,174][DEBUG][action.search.type       ] [Black Fox] All shards failed for phase: [query_fetch]
```

I'm not sure if everything in the script is strictly required, but the following three things all seem to be required in order to reproduce it:
- A large number of (possibly empty) documents &#8211; see org.apache.lucene.search.ConstantScoreAutoRewrite:91.
- Multiple matching docs on the same shard.
- Multiple matching documents that have nested children &#8211; the nested docs cause issues in [MatchedQueriesFetchSubPhase](https://github.com/elastic/elasticsearch/blob/1.4/src/main/java/org/elasticsearch/search/fetch/matchedqueries/MatchedQueriesFetchSubPhase.java#L108) where the filterIterator can be advanced beyond NO_MORE_DOCS (I&#8217;m not sure if that&#8217;s allowed or not, but the org.apache.lucene.search.DisjunctionScorer.advance method assumes it isn&#8217;t).
</description><key id="126381945">15949</key><summary>Nullpointer using named query when there are nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels><label>:Nested Docs</label><label>bug</label><label>discuss</label></labels><created>2016-01-13T09:57:23Z</created><updated>2016-01-14T16:36:16Z</updated><resolved>2016-01-14T16:36:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-13T11:25:50Z" id="171260031">I'm able to reproduce this in 1.7.3, but not in 2.0.0  or beyond.  @jpountz @martijnvg do you know for sure whether this has been fixed?
</comment><comment author="jpountz" created="2016-01-13T14:20:34Z" id="171303688">I don't know when it was fixed but I think I found the bug: in 1.7 MatchedQueriesFetchSubPhase might call DocIdSetIterator.advance after it already returned NO_MORE_DOCS, which is illegal.
</comment><comment author="clintongormley" created="2016-01-13T17:18:41Z" id="171369118">Is this no longer the case in 2.x? In which case we can close?
</comment><comment author="jpountz" created="2016-01-13T17:20:05Z" id="171369466">This rule still exists in 2.x but the code is fine and doesn't call advance on an exhausted iterator. Do we want to get this fixed in 1.7?
</comment><comment author="clintongormley" created="2016-01-13T17:22:33Z" id="171370523">@jpountz depends how big a job it is i suppose? 
</comment><comment author="jpountz" created="2016-01-13T17:33:20Z" id="171373353">It looks simple. I'll give it a try and give up if it's more complicated than I expected.
</comment><comment author="jpountz" created="2016-01-14T16:36:16Z" id="171693408">Fixed via #15962
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Always require units for bytes and time settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15948</link><project id="" key="" /><description>With #11437 we made units required, but left an emergency escape hatch (the `settings_require_units` setting, defaulting to `true`) as an insurance policy in case something was wrong with the back-compat logic to upgrade cluster settings...

I think for 3.0 we should remove this escape hatch and always, unconditionally require units.
</description><key id="126380523">15948</key><summary>Always require units for bytes and time settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-13T09:50:51Z</created><updated>2016-01-13T11:27:00Z</updated><resolved>2016-01-13T10:17:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-13T09:59:27Z" id="171237942">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTP Response Header Cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15947</link><project id="" key="" /><description>Cleans up HTTP response headers before sending over the wire.
</description><key id="126333673">15947</key><summary>HTTP Response Header Cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label></labels><created>2016-01-13T03:36:07Z</created><updated>2016-01-20T14:34:50Z</updated><resolved>2016-01-19T18:46:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-01-13T10:14:43Z" id="171241861">I suppose not using URLEncoder is a performance thing, there have been reports over the years that it is slow and allowcates a fair share of memory, but I do not know if this is still the case with more recent java versions. Google implemented their `FastURLEncoder` in their HTTP API clients and guava has something called `UrlEscapers` nowadays IIRC. I suppose this one is very similar.

Apart from that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTP Response Header cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15946</link><project id="" key="" /><description /><key id="126327860">15946</key><summary>HTTP Response Header cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>bug</label><label>review</label></labels><created>2016-01-13T02:45:23Z</created><updated>2016-01-17T11:38:55Z</updated><resolved>2016-01-13T03:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Nonunique terms are being accounted multiple times in terms aggregation of child/nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15945</link><project id="" key="" /><description>My domain is the following: a parent doc has some child docs with some string field. I want to calculate a frequency of parents having particular field in the child doc. When this particular field doesn't have duplicates amond same parent - results of query below are OK. But when some parent has multiple children with same value of the field, calculation sums this duplicates and gives wrong results. When I perform equal search query it gives me right count of parent docs. 

```
    curl -XPUT 127.0.0.1:9200/test_index -d '
    {
      "mappings": {
        "test_parent": {
            "properties": {
                "test_filter_field": {
                    "type": "string"
                }
            }
        },
        "test_child": {
          "_parent": {
            "type": "test_parent"
          },
          "properties": {
              "test_non_nested_field": {
                  "type": "string"
              },
              "test_nested_field": {
                  "type": "nested",
                  "properties": {
                    "nested_filter_field": {
                        "type": "string"
                    },
                    "nested_term_field": {
                        "type": "string"
                    }
                  }
              }
          }
        }
      }
    }'

    curl -XPUT 127.0.0.1:9200/test_index/test_parent/parent1 -d '
    {
        "test_filter_field" : "1"
    }'

    curl -XPUT 127.0.0.1:9200/test_index/test_child/child4?parent=parent1 -d '
    {
        "test_non_nested_field": "non_nested_term_field11"
    }'

    curl -XPUT 127.0.0.1:9200/test_index/test_child/child5?parent=parent1 -d '
    {
        "test_non_nested_field": "non_nested_term_field11"
    }'

    curl -XPOST 127.0.0.1:9200/test_index/test_parent/_search?pretty=true -d '
    {
      "aggs": {
        "top": {
          "filter": {
            "term": {
              "test_filter_field": "1"
            }
          },
          "aggs": {
            "children.nested.filter": {
              "children": {
                "type": "test_child"
              },
              "aggs": {
                "field": {
                  "terms": {
                    "field": "test_child.test_non_nested_field"
                  }
                }
              }
            }
          }
        }
      }
    }
    '
```

It gives me 

```
    "aggregations" : {
    "top" : {
      "doc_count" : 1,
      "children.nested.filter" : {
        "doc_count" : 2,
        "field" : {
          "doc_count_error_upper_bound" : 0,
          "sum_other_doc_count" : 0,
          "buckets" : [ {
            "key" : "non_nested_term_field11",
            "doc_count" : 2  &lt;------ I expect it here to give me 1
          } ]
        }
      }
    }
    }
```

Here is equal search query, which gives correct result

```
    curl -XPOST 127.0.0.1:9200/test_index/test_parent/_search?pretty=true -d '
    {
        "query": {
            "has_child": {
                "query": {
                    "multi_match": {
                        "query": "non_nested_term_field11",
                        "fields": [
                            "test_non_nested_field"
                        ]
                    }
                },
                "type": "test_child"
            }
        }
    }
```

response

```
    {

      "hits" : {
        "total" : 1,  &lt;--- obviously correct
        "max_score" : 1.0,
        "hits" : [
            {
              "_index" : "test_index",
              "_type" : "test_parent",
              "_id" : "parent1",
              "_score" : 1.0,
              "_source": {
                    "test_filter_field" : "1"
              }
            }
        ]
      }
    }
```
</description><key id="126325834">15945</key><summary>Nonunique terms are being accounted multiple times in terms aggregation of child/nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serj-p</reporter><labels /><created>2016-01-13T02:27:18Z</created><updated>2016-01-13T11:08:38Z</updated><resolved>2016-01-13T11:08:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-13T11:08:38Z" id="171255892">Hi @serj-p 

The right place to ask questions like these is in the forums: http://discuss.elastic.co/

We reserve the github issues list for bug reports and feature requests

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add De-dot Processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15944</link><project id="" key="" /><description>Since fields with dots in the name are disallowed from being indexed, it is useful to have an ingest processor that can help replace those `.`s into some other character (e.g. `_`).
</description><key id="126324373">15944</key><summary>[Ingest] Add De-dot Processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2016-01-13T02:17:35Z</created><updated>2017-06-23T06:48:19Z</updated><resolved>2016-01-15T21:34:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-01-13T07:04:38Z" id="171197616">@martijnvg @javanna 
I started the work for this here: https://github.com/elastic/elasticsearch/compare/feature/ingest...talevy:ingest-dedot-processor?expand=1
but I have a few questions about its functionality.

should this recursively search and replace across all fields? should we supply a list of fields?

I left it to only search and replace across top-level fields. I also noticed some behavior that I thought was unexpected within the mustache templating against documents with dots in them. I added two tests to demonstrate the existing behavior. 
</comment><comment author="javanna" created="2016-01-13T08:28:40Z" id="171214799">I left a couple of comments on your commit. I think we should look for fields that contain dots recursively. I had a look at the mustache tests and left comments about those too.
</comment><comment author="eskibars" created="2016-01-14T23:44:10Z" id="171818976">+1 to doing this recursively
</comment><comment author="talevy" created="2016-01-15T21:34:24Z" id="172099656">processor merged into `feature/ingest` branch. closing. 
</comment><comment author="animageofmine" created="2017-06-23T06:44:47Z" id="310585804">@talevy 
What happened to dedot processor? Was this removed or replaced with something else?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient: Add exception when using plugin.types, to help migration to addPlugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15943</link><project id="" key="" /><description>closes #15693
</description><key id="126289239">15943</key><summary>TransportClient: Add exception when using plugin.types, to help migration to addPlugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-12T22:10:27Z</created><updated>2016-01-13T11:03:07Z</updated><resolved>2016-01-12T23:50:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-12T22:31:07Z" id="171081199">@rjernst Left a couple questions, otherwise LGTM.
</comment><comment author="rjernst" created="2016-01-12T23:49:56Z" id="171102693">I moved the check as suggested.
</comment><comment author="rjernst" created="2016-01-12T23:50:50Z" id="171102870">Pushed to 2.2 as well: b82c86a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor documentation updates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15942</link><project id="" key="" /><description>A couple of very small documentation updates. 
</description><key id="126282668">15942</key><summary>Minor documentation updates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">inqueue</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-01-12T21:37:24Z</created><updated>2016-01-13T19:52:57Z</updated><resolved>2016-01-13T06:38:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-01-12T22:52:32Z" id="171088173">LGTM. thanks for the fixes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Circular reference in Exception when checkpoint writing fails </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15941</link><project id="" key="" /><description>We can end up with a circular reference in an Exception A, which has as suppressed Exception B which has as cause A again if an Exception is thrown in the try block of `Translog.ensureSynced()` (Translog.java#L562).

This is easy to see if you pick https://github.com/brwe/elasticsearch/commit/bb108994d9b4929481f2f74b3e01bc8c0f756b18 and run the `TranslogTests`.

The way this happens is so:
1. We call `Translog.ensureSynced()` but unfortunately when `TranslogWriter.sync()` is called (via `current.syncUpTo()`) the writing of the checkpoint fails (TranslogWriter.java#L151) with Exception A. This causes `TranslogWriter.tragedy` to be set to Exception A before it is thrown (`TranslogWriter.closeWithTragicEvent()`). 
2. Because of Exception A we now get to the catch clause in `Translog.ensureSynced()`  where we call `Translog.closeOnTragicEvent()`  and pass as parameter the Exception A _that we just set as tragic exception_  (`TranslogWriter.tragedy`).
3. In `Translog.closeOnTragicEvent()` we call `close()` but that will fail too because we closed the translog already before (because of the tragic event A). We will get Exception B which is an `AlreadyClosedException` with cause `TranslogWriter.tragedy` (Exception A). (It is created in `TranslogWriter.ensureOpen()`). 
4. Because of Exception B now we get to the catch clause of `Translog.closeOnTragicEvent()` and have
   - `inner` Exception B, the `AlreadyClosedException` with cause `TranslogWriter.tragedy` (Exception A)
   - method parameter Exception (`ex`)  which is  `TranslogWriter.tragedy` (Exception A)

We the set B as suppressed Exception for the parameter exception A and therefore end up with a circular reference A that has suppressed B that has cause A.

As a result, when we try to serialize this, we get a stackoverflow.

This can be seen also in this build failure: http://build-us-00.elastic.co/job/es_core_21_metal/326/

If this explanation is too confusing let me know. 
</description><key id="126280049">15941</key><summary>Circular reference in Exception when checkpoint writing fails </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2016-01-12T21:27:02Z</created><updated>2016-01-13T15:28:04Z</updated><resolved>2016-01-13T15:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-13T08:27:31Z" id="171214632">thanks so much @brwe for tracking this down. I know exactly what kind of time-sink this is/can be. The explanation makes perfect sense to me after reading it a couple of times :). I think the fix is relatively straight forward, no? I mean we should no add `AlreadyClosedExceptions` as suppressed exceptions when we call `Translog#close()` inside `Translog#closeOnTragicEvent(Throwable), what do you think?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot failover/retry on failed shard if a good copy is available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15940</link><project id="" key="" /><description>Scenario reported by the field is the following.

Periodically, snapshot fails (partial) against a specific shard. 

```
[2015-11-10 07:20:37,413][WARN ][snapshots ] [node_name] [[index_name][1]] [snapshot:20151110t071646z] failed to create snapshot 
org.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [index_name][1] Failed to snapshot 
at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:100) 
at org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:871) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745) 
Caused by: org.elasticsearch.index.engine.FlushFailedEngineException: [index_name][1] Flush failed 
at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:715) 
at org.elasticsearch.index.engine.InternalEngine.snapshotIndex(InternalEngine.java:846) 
at org.elasticsearch.index.shard.IndexShard.snapshotIndex(IndexShard.java:772) 
at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:83) 
... 4 more 
Caused by: org.apache.lucene.index.CorruptIndexException: [index_name][1] Preexisting corrupted index [corrupted_JwkJ91qoSs2cbwcrhNb0iA] caused by: CorruptIndexException[verification failed : calculated=14wqrat stored=n88qsu] 
org.apache.lucene.index.CorruptIndexException: verification failed : calculated=14wqrat stored=n88qsu 
at org.elasticsearch.index.store.Store$VerifyingIndexInput.verify(Store.java:1507) 
at org.elasticsearch.index.store.Store.verify(Store.java:505) 
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshotFile(BlobStoreIndexShardRepository.java:568) 
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:507) 
at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:140) 
at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:85) 
at org.elasticsearch.snapshots.SnapshotsService$5.run(SnapshotsService.java:871) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
at java.lang.Thread.run(Thread.java:745)

at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:602) 
at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:583) 
at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:150) 
at org.elasticsearch.index.engine.InternalEngine.flush(InternalEngine.java:709) 
... 7 more
```

This tends to happen when there is a corruption against a large segment.  Since we have to read the entire segment to check for corruption, for large segments, we do not check for corruption until one of the following operations are performed today (snapshot, merge, relocation, peer recovery (when we copy segments over from a primary shard to a replica shard).  So this can happen when the cluster is green and snapshot will then detect that a segment is bad and the recovery process will kick in to try to recover the shard from a replica, etc..

If there is a good copy available, snapshot will succeed on that shard on the next scheduled snapshot run.  However, for the snapshot operation that was previously issued, there is currently not a failover/retry mechanism to retry the snapshot once recovery is successful, or try snapshot-ing from a copy of the shard instead.  

Discussed with @imotov , a solution to this will be complex and we can revisit after the [task management api](https://github.com/elastic/elasticsearch/issues/15117) has been implemented in the future so we can keep track of long running jobs.
</description><key id="126278040">15940</key><summary>Snapshot failover/retry on failed shard if a good copy is available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>enhancement</label><label>stalled</label></labels><created>2016-01-12T21:19:20Z</created><updated>2016-01-12T21:19:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a fixed-point mapping type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15939</link><project id="" key="" /><description>Adds a new `fixed` mapping type, which provides storage of real-valued numbers in a `long` via an associated scaling factor.  Uses a base-10 scaling factor because base-2 can give strange results (to a human) when searching, due to it not lining up with discrete digits.

I felt very much like a bull in a china shop here... unclear if all the right things were modified/overridden, or if there is a cleaner way to do this.

Related to #13625
</description><key id="126267857">15939</key><summary>Add a fixed-point mapping type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Mapping</label><label>feature</label></labels><created>2016-01-12T20:24:33Z</created><updated>2016-04-06T22:13:49Z</updated><resolved>2016-04-06T21:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-13T09:40:28Z" id="171231956">I only skimmed through the patch but I think you are on the right track. I think we still need to sort out the following questions:
- what should the API be, ie should users be able to pass a numer of decimal digits, decimal bits or even arbitrary scaling factors?
- how can we make conversions efficient? In particular the conversion from fixed-point to double since it could run billions of times for a single search request
- should we round down or to the closest value?

@rmuir Maybe you have opinions on this?

&gt; base-2 can give strange results (to a human) when searching, due to it not lining up with discrete digits

Why is it an issue? Don't floats have the same problem since they can't represent most values accurately?
</comment><comment author="polyfractal" created="2016-01-13T15:18:13Z" id="171326170">&gt; Why is it an issue? Don't floats have the same problem since they can't represent most values accurately?

Floats do, but it's a lot less noticeable in my opinion, since it happens at very extreme values.  For most values that people actually work with, the error is unnoticeable.  E.g. If I search for `1.23`, I don't expect `1.25` to come back as a hit.

It may just be me, but I find base-2 very unintuitive for the end-user:
- A user cares about the number of digits to preserve, not the number of bits.  Specifying `2 digits` is conceptually easier to understand than `6 bits`
- Specifying 2 digits always gives you two digits.  Specifying 5 bits gives you 1-2 digits of precision.  So we can either let users specify `num_bits` and hope they understand how it lines up with base-10, or we can let users specify `num_digits` and over-estimate the number of bits required so we can guarantee that number of digits.
- Since base-2 only accurately represents powers of 2, I think we'll get a lot of complaints about accuracy because humans think in base-10, and the error is noticeable when you are only saving two digits worth of the fractional portion.  It's not technically a bug, but it is confusing
</comment><comment author="rmuir" created="2016-01-13T20:24:08Z" id="171422085">&gt; what should the API be, ie should users be able to pass a numer of decimal digits, decimal bits or even arbitrary scaling factors?
&gt;     how can we make conversions efficient? In particular the conversion from fixed-point to double since it could run billions of times for a single search request
&gt;     should we round down or to the closest value?
&gt; 
&gt; @rmuir Maybe you have opinions on this?

It depends on the goals. I will say this: if its named `fixed` then immediately what springs to mind to me is a true fixed-point type, e.g. something you can use for financial data and all the use cases along with that. This is a fair amount of work, it means mathematical operations etc have to work, it means we can never involve `double`, etc. Think about all the cases like even a script used in an aggregation doing a sum: not all script engines even have bigdecimal support. So I personally think its a challenging task.

On the other hand, if we just want an optimized way to encode a `double`, but with some restraints on range/precision, that e.g. takes advantage of things like GCD compression already implemented in docvalues, then maybe we treat it just like that, and figure out an alternative name?
</comment><comment author="jpountz" created="2016-01-13T21:47:47Z" id="171447191">One thing I was thinking about: decimal encoding has the benefit that it would more likely leverage gcd compression in doc values. For instance if 3 decimal places are configured but only one is actually used then Lucene will figure out all values share 100 as a common factor and compress accordingly. This also means that we might not have to make the scale configurable: we could pick a fixed scale that should be enough for common use-cases and then rely on Lucene for using the right number of bits?
</comment><comment author="polyfractal" created="2016-01-13T23:34:27Z" id="171471852">&gt; On the other hand, if we just want an optimized way to encode a double, but with some restraints on range/precision, that e.g. takes advantage of things like GCD compression already implemented in docvalues, then maybe we treat it just like that, and figure out an alternative name?

Yeah, this was my goal personally.  I just want better compression for when you don't need the fully dynamic range of doubles.  I even have a big, fat warning in the docs that this is not equivalent to `Decimal` and similar currency types, since it always casts back to Double for arithmetic.  Just a storage optimization really.

++ to renaming to something more clear, I have no attachment to `fixed`. :)  Random ideas:
- `variable_precision`: to illustrate that you can change the precision. But it sounds like each value may have its own precision or something
- `real_storage`: e.g. real-valued number optimized for storage (yuck)
- `truncated_real`: e.g. real-valued with truncated precision
- `compressed_real`, `compressed_float`
- `scaled_real`, `scaled_float`

I dunno, those are all terrible. 

@jpountz: Not sure I understand.  Do you mean to pick a scale that provides, say, 6 digits of sigfigs and simply use that?  I think that's fine on the fractional side of things (since Lucene will compress down unused bits), but it arbitrarily limits the range for not much change in complexity?  Maybe i'm misunderstanding?
</comment><comment author="rmuir" created="2016-01-13T23:49:31Z" id="171474540">OK, it makes a lot more sense if we just try to do the optimized thing, and defer true `fixed`. I figure as far as naming goes, ill throw out `sloppy` as an option. 

If we do it this way, then we use DoubleDocValues and other apis just like now, its just that how we encode the `double` itself is different. Like an alternative floating point encoding.  The main alternative would be to use an IEEE-half float (https://en.wikipedia.org/wiki/Half-precision_floating-point_format)

The advantage here is a wider range than a half-float, so larger numbers can be represented. We can also simplify a little bit, simply reject values like NaN/Inf/ and so on.

The main disadvantage over existing float/half-float is, uncertainty over how much space is really being saved (especially if its configurable, which i would avoid if possible). Those standardized formats are simpler to think about and you know what space is required.

So it would be good to see numbers, where less precision is used and it really saves space. Keep in mind in 3.0, dynamically mapped floating point values already go to `float` instead of `double` so they are only using 32-bits per value.
</comment><comment author="rmuir" created="2016-01-13T23:59:12Z" id="171476394">Also when thinking about some alternative format like this, keep in mind the bits-per-value "schedule" used by docvalues (https://github.com/apache/lucene-solr/blob/6f15d0282f17ab49ab434c54605f4b94f6c4d037/lucene/core/src/java/org/apache/lucene/util/packed/DirectWriter.java#L154-L156)

We round up to values that are efficient to encode/decode, so it does not make sense to think about saving a bit or two per value in general: e.g. using 13 bits per value is really no better than 16.
</comment><comment author="polyfractal" created="2016-01-14T14:32:40Z" id="171658377">Makes sense, lemme run some real-world tests under varying conditions to see how much space is actually saved (will be easy to compare against float/double, and easy enough to extrapolate to half-float)

I think we should still add half-float too, I opened a ticket for it a while ago too: https://github.com/elastic/elasticsearch/issues/13626
</comment><comment author="polyfractal" created="2016-01-15T20:58:44Z" id="172090909">Alright, did some tests.  Summary is that I think @jpountz's suggestion was right: we should consider limiting this to ~3 digits and not allow customization.  In that configuration it can save considerable space for certain use-cases.

Testing methodology was:
1. Create index with `fixed`, `double` and `float` mapped (with `decimal_places` between 1-10, depending on the test)
2. Index 1 million docs
3. Refresh, explicit flush, wait for all merges to stop
4. Measure disk size of the three fields
5. Repeat steps 2-4

The data being indexed was: `Math.sin((i / 1000)) * 100 + (normal(0,1) * 3.0)`.  E.g. a sine wave with a periodicity of ~160 docs, an amplitude of 100 and gaussian noise added to each point so they aren't identical each period.  I figured this was a good tradeoff to approximate "real" data, since values trend in the same direction, have some noise but isn't completely random.

Values are sent to Elasticsearch as a string (`"value" : "1.23"`) so that each type coerces on it's own.  Sizes are inclusive of all components (postings, norms, doc_values, etc).

Fixed point compresses better for lower precision, as expected.  At one SigFig it's about twice as good as a float:
![1](https://cloud.githubusercontent.com/assets/1224228/12364550/46434028-bb9d-11e5-80b5-72112b849ef8.png)

The compression advantage decreases linearly until we hit 7 SigFigs, at which point it's about identical to a float:
![7](https://cloud.githubusercontent.com/assets/1224228/12364584/78e5b254-bb9d-11e5-8275-98bce8bd504a.png)

And at 8+ SigFigs it is larger and no longer useful if your goal is to save space:
![8](https://cloud.githubusercontent.com/assets/1224228/12364602/9a7f565e-bb9d-11e5-90a0-c9d48238bc6d.png)

Predictably, if you increase the dynamic range of the data to +/- 10,000 (e.g. `Math.sin((i / 1000)) * 10000 + (normal(0,1) * 3.0)`, the gap closes a lot quicker and `fixed` is equivalent to `float` by four SigFigs:
![4](https://cloud.githubusercontent.com/assets/1224228/12364641/cbbd49ec-bb9d-11e5-9f61-d69aad5b0bd5.png)

And a very narrow range the opposite ocurrs, giving you more sigfigs before it runs out of steam.  

So I think the general conclusion here is that we can probably set this to ~3 SigFigs and it'll work nicely for most people that are storing real-values but don't need extreme precision or a very large range (e.g. server metrics and the like).
</comment><comment author="jpountz" created="2016-01-18T12:42:48Z" id="172518330">I re-read this discussion and figured out I had not really compiled this comment:

&gt; On the other hand, if we just want an optimized way to encode a double, but with some restraints on range/precision, that e.g. takes advantage of things like GCD compression already implemented in docvalues, then maybe we treat it just like that, and figure out an alternative name?

If the significant digits are what we are after then maybe we should keep using the original double/float/half-float representation indeed and allow users to configure how many bits of the mantissa they want to keep. At indexing time we would zero out the last bits that we don't care about (and doc values would compress with gcd compression) and at search time we would still just have to call Float.intBitsToFloat. The only search-time overhead would be an integer multiplication due to gcd (de)compression. I assume that it is what was meant?

For instance (assuming my math is correct) then we would have 3 significant digits for 16 bits of storage on a float, and for 12 bits of storage on a half-float.

We don't even need to make things configurable, we could just register field types for the numbers that round nicely, eg. "float3" (3 significant figures for 16 bits) and "double4" (4 significant figures for 24 bits, maybe even 5 if we round instead of rounding down).
</comment><comment author="rjernst" created="2016-01-18T19:53:17Z" id="172634175">I am strongly against something like "float3" and "double4" as field types. I think we need to minimize the field types we have, as there is a real cost for a user to find what data type to specify, as opposed to tweaking settings for a field type to optimize storage/query/whatever.
</comment><comment author="rmuir" created="2016-01-18T22:42:49Z" id="172673553">&gt; At indexing time we would zero out the last bits that we don't care about (and doc values would compress with gcd compression) and at search time we would still just have to call Float.intBitsToFloat. The only search-time overhead would be an integer multiplication due to gcd (de)compression. I assume that it is what was meant?

I wasn't trying to imply any particular implementation, more just how we think about it. I was hoping we could narrow the issue to "lets create an alternative encoding that saves space at the expense of range/precision" but otherwise the API is the same as float/double.

I think if this is our goal, we should still consider the IEEE half-float. There would be less surprises, its standardized, maybe even hardware support in the future (e.g. intel cpus can do conversion of vectors at least). It seems like the lowest hanging fruit, that would very easily only use 16bpv.

Its true we could take advantage of docvalues and try to invent something that is maybe more "interesting" (e.g. if a larger range is needed, will use 24bpv or whatever), but maybe we should start with the standard stuff? We have to be a little careful inventing our own encoding of this stuff, and also we should take care for it to not be too fragile.

But it might be easier as a start to just add a "simple" 16bpv option and think about how we want the apis to work, e.g. address ryan's concerns and so on. 
</comment><comment author="polyfractal" created="2016-01-19T01:14:45Z" id="172697186">My main concern with the half-float is that it makes a trade-off which is less useful to space-conscious users.  It trades range for greater precision near zero, whereas I think most users would prefer greater range with less overall precision.  Someone who values precision won't be picking a half-float (or fixed-point) anyway.  

The half-float's max value is only 65,504 and integers &gt; 2048 start rounding.  By time you hit 16,385 you're rounding by multiples of 16, which is pretty significant.  Half-floats really shine closer to zero; normal's down to 0.000061 and subnormal down to 0.0000000596046.

I'm not arguing _against_ a half-float, but I think it's tangential to the goal of "compact float with a few significant digits".   But I could be wrong, that's just my gut feeling :)

I don't really have enough experience to comment on `float3` or equivalent...but lopping bits off scares me a little.  Floating points are already super unintuitive for mortals like myself, I feel like configurable bits on the mantissa is just asking for user confusion and problems as rounding errors become more apparent :)
</comment><comment author="rmuir" created="2016-01-19T01:27:13Z" id="172698614">&gt; I'm not arguing against a half-float, but I think it's tangential to the goal of "compact float with a few significant digits". But I could be wrong, that's just my gut feeling :)

I'm not arguing for it really either. But at least it should be a nice basis for comparisons? The main advantage to me is still, its already a standard, and so those properties about it ("the tradeoffs") are already understood. Like you can easily look them up and you know if its what you want.

Obviously we can try to make something ourselves, but IMO its more complex, e.g. we have to implement different tradeoffs we feel are better, document what the format is and properties about it, put more effort into encode/decode, and so on.

Finally, there is something to be said for "well somehow they made this an IEEE standard for small floats" when trying to figure out what the best tradeoffs should be. I'm not saying the primary use cases necessarily overlap with ours at all (eg graphics/gaming), but we should keep in mind a variety of possible use cases. For things like scoring factors (e.g. `log(PageRank)`), half-float may be a reasonable choice.
</comment><comment author="dakrone" created="2016-04-06T17:21:05Z" id="206473314">Ping @polyfractal, what's the word on this? It's a couple of months old now
</comment><comment author="polyfractal" created="2016-04-06T18:06:51Z" id="206494963">I think there was sufficient concern raised in this thread that it shouldn't move forward until there is a half float implementation to compare against, especially since adding a whole new data type is a pretty big deal.

I just haven't had time to work on a half-float implementation, and honestly, it may be beyond my ability to do well.  I've never implemented something like that before.

Perhaps just close for now until I (or someone else) can scrounge up time to implement IEEE-half float for a direct comparison?
</comment><comment author="dakrone" created="2016-04-06T21:21:40Z" id="206575516">Sounds good, I'm going to close this for now, we can always use the re-open button if we want!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mpercolate api should serialise start time </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15938</link><project id="" key="" /><description>PR for #15908 

(when back porting to other branches I'll add version checks for serialisation)
</description><key id="126266233">15938</key><summary>mpercolate api should serialise start time </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T20:16:53Z</created><updated>2016-01-15T08:46:05Z</updated><resolved>2016-01-15T08:25:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-14T10:25:19Z" id="171597977">@javanna I've updated this PR.
</comment><comment author="javanna" created="2016-01-14T10:30:00Z" id="171600091">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add proper ingest methods to Client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15937</link><project id="" key="" /><description>Now that ingest is part of core we can add specific put/get/delete/simualtePipeline methods to the Client interface which is nice for java api users
</description><key id="126252719">15937</key><summary>add proper ingest methods to Client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-12T19:12:55Z</created><updated>2016-01-13T08:46:20Z</updated><resolved>2016-01-13T08:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-01-12T19:21:56Z" id="171022943">:+1: LGTM
</comment><comment author="martijnvg" created="2016-01-13T08:40:50Z" id="171216709">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exceptions and Infinite Loop Checking</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15936</link><project id="" key="" /><description>New Features:
- Exceptions (throw, try/catch)
- Infinite Loop Checking
- Iterators added to the API

Fixes:
- Improved loop path analysis
- Fixed set/add issue with shortcuts in lists
- Fixed minor promotion issue with incorrect type
</description><key id="126243761">15936</key><summary>Exceptions and Infinite Loop Checking</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>feature</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T18:32:40Z</created><updated>2016-02-01T21:04:16Z</updated><resolved>2016-01-19T17:50:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-12T20:27:24Z" id="171044255">I wonder if it'd be possible/even a good idea to separate the code that collects statement metadata from the code that enforces limits rather than putting them all in Analyzer.
</comment><comment author="jdconrad" created="2016-01-12T20:39:58Z" id="171047340">@nik9000 On that last statement, I'm not 100% clear on what you mean?  Can you give me an example here on what you'd separate exactly?  Thanks.
</comment><comment author="nik9000" created="2016-01-12T20:46:19Z" id="171049322">&gt; @nik9000 On that last statement, I'm not 100% clear on what you mean? Can you give me an example here on what you'd separate exactly? Thanks.

I might make a `Validator` and call it with all different metadatas. The job of building the metadata is hard enough, I figure it might make it easier to read if the code actually checking the metadata is in its own spot. It might make things harder to read though, so it might be a horrible idea.

Or you could make another visitor implementation that walks the parse tree after the `Validator` is run and checks the metadata - that is probably silly because it means you have to keep the metadata around a second time, but its an idea.

Its not at all needed for this PR. Its just something that struck me while I was reading all the ins and outs of building the metadata.
</comment><comment author="jdconrad" created="2016-01-12T20:53:16Z" id="171051253">@nik9000, I'd definitely have to give that more thought.  I'd prefer to avoid walking the tree a third time just because of compile-time overhead.
</comment><comment author="jdconrad" created="2016-01-12T20:54:04Z" id="171051470">Also, I agree with all the comments about needing comments.  I know the code is fairly raw still, and needs a lot of clean up.  It obviously isn't all going to happen at once here :).
</comment><comment author="nik9000" created="2016-01-12T21:07:58Z" id="171054728">&gt; @nik9000, I'd definitely have to give that more thought. I'd prefer to avoid walking the tree a third time just because of compile-time overhead.

Yeah. I'm all for that.

&gt; It obviously isn't all going to happen at once here :).

Sure! I'd love javadoc on the new members but don't bother with old members. Or merge without it and add javadoc later. Its all the same to me.
</comment><comment author="jdconrad" created="2016-01-19T01:53:27Z" id="172705677">Updated with the requested tests.  Made some of the more minor changes that were requested.  JavaDoc'd several files as a start to some of the internal documentation.  As mentioned in many prior comments I'm going to avoid doing any major refactoring of the metadata in this PR as that is something that requires some significant work and is outside the scope of this PR.
</comment><comment author="jdconrad" created="2016-01-19T02:27:14Z" id="172713011">@nik9000 Thanks for the thorough reviews here.  I will work through this iteration tomorrow.  And that spelling mistake on my part is just embarrassing :).
</comment><comment author="nik9000" created="2016-01-19T02:47:22Z" id="172720673">This is awesome and you should merge it as fast as you can click that button. I left some comments but none of them need to be addressed in this PR. I'm really fond of your docs on the Metadata fields.
</comment><comment author="nik9000" created="2016-01-19T17:02:19Z" id="172918451">@jdconrad for your tour of unwrap I'd start [here](https://github.com/elastic/elasticsearch/blob/c50b22f95f48119213156bdb634925e1be91b794/plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java#L187). Javascript also has an implementation of this but everything else just returns what its passed.
</comment><comment author="nik9000" created="2016-01-19T17:02:49Z" id="172918657">LGTM
</comment><comment author="nik9000" created="2016-01-19T17:02:57Z" id="172918725">:shipit: 
</comment><comment author="nik9000" created="2016-01-19T17:03:06Z" id="172918802">what is that a picture of?!
</comment><comment author="jdconrad" created="2016-01-19T17:50:36Z" id="172931975">If that is a detective squirrel, that's awesome.  Thanks again @nik9000 .  Forgot to add the closes to the commit message so closing this manually as it's been committed.
</comment><comment author="clintongormley" created="2016-01-20T14:25:15Z" id="173219368">Merged into master in 2249a640bb2730c37a23a0d3a39e6d6c92257ef0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog corruption if process is killed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15935</link><project id="" key="" /><description>This looks like #9699, but I'm seeing it with 2.1.1.

Here's the stacktrace:

```
[2016-01-12 09:57:05,517][INFO ][gateway                  ] [Googam] recovered [2] indices into cluster_state
[2016-01-12 09:57:33,204][WARN ][indices.cluster          ] [Googam] [[audiolevel-2016.01.12][0]] marking and sending shard failed due to [failed recovery]
[audiolevel-2016.01.12][[audiolevel-2016.01.12][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /Users/jmason/Projects/sme/elasticsearch-2.1.1/data/elasticsearch/nodes/0/indices/audiolevel-2016.01.12/0/translog/translog-33.tlog]; nested: EOFException[read past EOF. pos [107951305] length: [4] end: [107951305]];
  at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
  at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
  at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: [audiolevel-2016.01.12][[audiolevel-2016.01.12][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /Users/jmason/Projects/sme/elasticsearch-2.1.1/data/elasticsearch/nodes/0/indices/audiolevel-2016.01.12/0/translog/translog-33.tlog]; nested: EOFException[read past EOF. pos [107951305] length: [4] end: [107951305]];
  at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:178)
  at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
  at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
  at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
  at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
  at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
  at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
  ... 5 more
Caused by: [audiolevel-2016.01.12][[audiolevel-2016.01.12][0]] EngineException[failed to recover from translog]; nested: ElasticsearchException[unexpected exception reading from translog snapshot of /Users/jmason/Projects/sme/elasticsearch-2.1.1/data/elasticsearch/nodes/0/indices/audiolevel-2016.01.12/0/translog/translog-33.tlog]; nested: EOFException[read past EOF. pos [107951305] length: [4] end: [107951305]];
  at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)
  at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:175)
  ... 11 more
Caused by: ElasticsearchException[unexpected exception reading from translog snapshot of /Users/jmason/Projects/sme/elasticsearch-2.1.1/data/elasticsearch/nodes/0/indices/audiolevel-2016.01.12/0/translog/translog-33.tlog]; nested: EOFException[read past EOF. pos [107951305] length: [4] end: [107951305]];
  at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:102)
  at org.elasticsearch.index.translog.TranslogReader.access$000(TranslogReader.java:46)
  at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:297)
  at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)
  at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
  at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
  ... 12 more
Caused by: java.io.EOFException: read past EOF. pos [107951305] length: [4] end: [107951305]
  at org.elasticsearch.common.io.Channels.readFromFileChannelWithEofException(Channels.java:102)
  at org.elasticsearch.index.translog.ImmutableTranslogReader.readBytes(ImmutableTranslogReader.java:84)
  at org.elasticsearch.index.translog.TranslogReader.readSize(TranslogReader.java:91)
  ... 17 more
```
</description><key id="126238551">15935</key><summary>Translog corruption if process is killed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">masonjm</reporter><labels><label>:Translog</label><label>bug</label><label>feedback_needed</label></labels><created>2016-01-12T18:05:01Z</created><updated>2016-02-06T21:17:25Z</updated><resolved>2016-01-21T10:42:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-13T10:42:34Z" id="171251121">@masonjm Is this reproducible?  (It is possible that your translog was genuinely corrupted).  Also what file system are you on?
</comment><comment author="s1monw" created="2016-01-13T10:46:47Z" id="171251909">@masonjm did you just upgrade you cluster or that node? It was fixed in `2.1.1` but if you hit it during shutdown before you upgrade there is not much the fix will do for you
</comment><comment author="s1monw" created="2016-01-13T14:03:46Z" id="171299529">the relevant change is this https://github.com/elastic/elasticsearch/pull/15420
</comment><comment author="s1monw" created="2016-01-21T10:42:06Z" id="173533480">closing - no feedbac
</comment><comment author="theluxury" created="2016-02-06T06:45:54Z" id="180707782">I just got this error with 2.1.1. Did not just upgrade, had 2.1.1 from the start. 

```
[2016-02-06 06:45:25,330][WARN ][cluster.action.shard     ] [ip-172-31-2-14] [reddit_filtered_2009_12]&gt;[3] received shard failed for [reddit_filtered_2009_12][3], node[r_W2uH9hQfm0kscdKpLocQ], [P], v[129], s[INITIALIZING], a[id=xBGkNrapRJqWdUEsHSLlhA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-02-06T06:45:25.251Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2009_12/3/translog/translog-10.tlog: Too many open files]; ]], indexUUID [Sue13-EfTLic4-qhW4vdfw], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2009_12/3/index/_c.si: Too many open files]; ]
[reddit_filtered_2009_12][[reddit_filtered_2009_12][3]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2009_12/3/index/_c.si: Too many open files];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [reddit_filtered_2009_12][[reddit_filtered_2009_12][3]] EngineCreationFailureException[failed to create engine]; nested: FileSystemException[/home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2009_12/3/index/_c.si: Too many open files];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:156)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    ... 5 more
Caused by: java.nio.file.FileSystemException: /home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2009_12/3/index/_c.si: Too many open files
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:91)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
    at java.nio.channels.FileChannel.open(FileChannel.java:287)
    at java.nio.channels.FileChannel.open(FileChannel.java:335)
    at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:82)
    at org.apache.lucene.store.FileSwitchDirectory.openInput(FileSwitchDirectory.java:186)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:89)
    at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:89)
    at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:109)
    at org.apache.lucene.codecs.lucene50.Lucene50SegmentInfoFormat.read(Lucene50SegmentInfoFormat.java:82)
    at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:362)
    at org.apache.lucene.index.IndexFileDeleter.&lt;init&gt;(IndexFileDeleter.java:170)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:948)
    at org.elasticsearch.index.engine.InternalEngine.createWriter(InternalEngine.java:1090)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:150)
    ... 11 more
[2016-02-06 06:45:25,332][WARN ][index.translog           ] [ip-172-31-2-14] [reddit_filtered_2010_01][4] failed to delete temp file /home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2010_01/4/translog/translog-7895359354389472541.tlog
java.nio.file.NoSuchFileException: /home/ubuntu/elasticsearch-2.1.1/data/coke-zero-crazy-delicious/nodes/0/indices/reddit_filtered_2010_01/4/translog/translog-7895359354389472541.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1126)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:327)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="s1monw" created="2016-02-06T18:31:38Z" id="180830492">&gt; I just got this error with 2.1.1. Did not just upgrade, had 2.1.1 from the start.

this looks like a totally different problem. It seems like you exhausted all your filehandles? can you take a look at this https://www.elastic.co/guide/en/elasticsearch/guide/current/_file_descriptors_and_mmap.html and make sure your system is configured correctly?
</comment><comment author="theluxury" created="2016-02-06T19:32:32Z" id="180843972">Yeah, from http://www.cyberciti.biz/faq/linux-increase-the-maximum-number-of-open-files/, I put `at /proc/sys/fs/file-max` and get back a huge number (1632405). Then I looked at the documents you linked and tried `sysctl -w vm.max_map_count=262144` but still get the same error.

FWIW my ES was working fine yesterday, then I installed Marvel (but couldn't get it to start since I only have ES 2.1.1), uninstalled Marvel, kill - 9 my ES, and now I have this error. 
</comment><comment author="s1monw" created="2016-02-06T19:45:50Z" id="180845671">when the node is running can I get the output of `curl -XGET 'http://localhost:9200/_nodes/stats/os,process?pretty=true'`
</comment><comment author="theluxury" created="2016-02-06T19:55:09Z" id="180846991">```
{
  "cluster_name" : "coke-zero-crazy-delicious",
  "nodes" : {
    "rt5Be5zxSLy15WtdbAVwbg" : {
      "timestamp" : 1454788460095,
      "name" : "ip-172-31-2-14",
      "transport_address" : "ec2-52-35-132-98.us-west-2.compute.amazonaws.com/172.31.2.14:9300",
      "host" : "172.31.2.14",
      "ip" : [ "ec2-52-35-132-98.us-west-2.compute.amazonaws.com/172.31.2.14:9300", "NONE" ],
      "os" : {
        "timestamp" : 1454788460096,
        "load_average" : 1.42,
        "mem" : {
          "total_in_bytes" : 16827494400,
          "free_in_bytes" : 12977397760,
          "used_in_bytes" : 3850096640,
          "free_percent" : 77,
          "used_percent" : 23
        },
        "swap" : {
          "total_in_bytes" : 0,
          "free_in_bytes" : 0,
          "used_in_bytes" : 0
        }
      },
      "process" : {
        "timestamp" : 1454788460096,
        "open_file_descriptors" : 4091,
        "max_file_descriptors" : 4096,
        "cpu" : {
          "percent" : 71,
          "total_in_millis" : 94250
        },
        "mem" : {
          "total_virtual_in_bytes" : 23121223680
        }
      }
    }
  }
```
</comment><comment author="s1monw" created="2016-02-06T20:23:38Z" id="180853821">```
        "open_file_descriptors" : 4091,
        "max_file_descriptors" : 4096,
```

I guess your settings are not applied.
</comment><comment author="masonjm" created="2016-02-06T20:46:43Z" id="180862368">The original bug report was a fresh install of 2.1.1 on OSX with Java 7. It was triggered by doing a hard reset on the computer.

Since then I moved to 2.1.4 running on a VirtualBox'd Ubuntu 14.04 install with Java 8. I've gone through the same hard reset sequence several times without issue. I don't know if the problem is fixed in 2.1.4, or if it was related to that specific setup.
</comment><comment author="theluxury" created="2016-02-06T21:09:27Z" id="180864013">Cool, I found out how to up that, and it's not giving me the error anymore. I restarted and am getting some index not found errors. If I check for /_cluster/health, I get 

```
{
  "cluster_name" : "coke-zero-crazy-delicious",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 6,
  "number_of_data_nodes" : 6,
  "active_primary_shards" : 158,
  "active_shards" : 158,
  "relocating_shards" : 0,
  "initializing_shards" : 28,
  "unassigned_shards" : 857,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 15.148609779482264
}
```

 Anyway to make this go faster?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add load averages to OS stats on FreeBSD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15934</link><project id="" key="" /><description>This commit adds load averages to the OS stats on FreeBSD. For these
stats to be available, linprocfs must be available and mounted at
/compat/linux/proc.

Closes #15917
</description><key id="126227113">15934</key><summary>Add load averages to OS stats on FreeBSD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T17:13:06Z</created><updated>2016-01-14T01:11:23Z</updated><resolved>2016-01-14T01:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-12T17:14:30Z" id="170979166">I tested this on a FreeBSD VM (`FreeBSD  11.0-CURRENT FreeBSD 11.0-CURRENT #0 r293245: Wed Jan  6 21:12:34 UTC 2016     root@releng2.nyi.freebsd.org:/usr/obj/usr/src/sys/GENERIC  amd64`), but if someone (@dakrone?, @rmuir?) has a real FreeBSD system they would be willing to test on, that would be helpful. :)
</comment><comment author="dakrone" created="2016-01-13T18:38:24Z" id="171392438">I'm traveling right now but when I get a chance I'll test this on FreeBSD, thanks @jasontedor 
</comment><comment author="dakrone" created="2016-01-14T01:08:47Z" id="171490499">Confirmed that this works:

```
hinmanm@freebsd:~ % curl -s 'localhost:9200/_nodes/stats/os?human' | jq '.nodes'
{
  "JnAf_UaPT_KK1MHlWsBNUA": {
    "timestamp": 1452733681138,
    "name": "Stephen Strange",
    "transport_address": "127.0.0.1:9300",
    "host": "127.0.0.1",
    "ip": [
      "127.0.0.1:9300",
      "NONE"
    ],
    "os": {
      "timestamp": 1452733681138,
      "cpu": {
        "percent": -1,
        "load_average": [
          0.38,
          0.36,
          0.31
        ]
      },
      "mem": {
        "total": "1.9gb",
        "total_in_bytes": 2118909952,
        "free": "90.8mb",
        "free_in_bytes": 95297536,
        "used": "1.8gb",
        "used_in_bytes": 2023612416,
        "free_percent": 4,
        "used_percent": 96
      },
      "swap": {
        "total": "2gb",
        "total_in_bytes": 2147483648,
        "free": "2gb",
        "free_in_bytes": 2147483648,
        "used": "0b",
        "used_in_bytes": 0
      }
    }
  }
}
```

LGTM
</comment><comment author="jasontedor" created="2016-01-14T01:11:01Z" id="171490812">Thanks @dakrone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move RefreshTask into IndexService and use since task per index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15933</link><project id="" key="" /><description>`refresh_interval` is a per index setting but we interpret and maintain it per shard. This
change moves the refresh task outside of IndexShard to the IndexService where it logically belongs
and reuses scheduling infrastructure used for translog fsync (async commit).

This change will use the same task for all shards of an index while previously we used on thread/task
per shard to refresh. This will also prevent too many concurrent refreshes if there are many indices and
shards allocated on a single node.
</description><key id="126221608">15933</key><summary>Move RefreshTask into IndexService and use since task per index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T16:47:35Z</created><updated>2016-01-15T06:42:21Z</updated><resolved>2016-01-13T12:19:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-12T17:35:01Z" id="170984877">LGTM, it's great to push "async-ness" higher up in the stack.
</comment><comment author="s1monw" created="2016-01-12T20:09:55Z" id="171038937">@mikemccand I pushed a new commit with a lot more tests and simpler task initialization.
</comment><comment author="mikemccand" created="2016-01-12T20:16:51Z" id="171041256">LGTM, thanks!
</comment><comment author="bleskes" created="2016-01-14T17:38:10Z" id="171717960">This is a great change. We little side effect is added debug logging each 1s / 5s. Imho this should be TRACE:

```
[2016-01-14 19:28:53,990][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:28:54,995][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:28:55,965][DEBUG][index                    ] [Sam Wilson] [index] scheduling translog_sync every 5s
[2016-01-14 19:28:56,000][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:28:57,003][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:28:58,007][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:28:59,012][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:29:00,017][DEBUG][index                    ] [Sam Wilson] [index] scheduling refresh every 1s
[2016-01-14 19:29:00,970][DEBUG][index                    ] [Sam Wilson] [index] scheduling translog_sync every 5s
```

I believe some of these were actually introduce in #14121 (great change), but it's hard to tell because the history is full of little commits that don't refer to relevant PR (argh  git merge). /cc @mikemccand 
</comment><comment author="mikemccand" created="2016-01-14T17:43:22Z" id="171719252">@bleskes I now dub you the logging policeman!

But I don't think #14121 added these debug logs?

I'm sorry for polluting git history with lots of little commits not pointing to the eventual PR ... I guess this means when I do local commits I should at least reference the ... issue I'm working against?
</comment><comment author="bleskes" created="2016-01-14T17:49:28Z" id="171720772">&gt; @bleskes I now dub you the logging policeman!

LOL. Challenge accepted. It's important :)

&gt; I guess this means when I do local commits I should at least reference the ... issue I'm working against?

I'm a fan of squash-rebase before merging where I change the commit message to include a reference to the PR (known at that time). I really don't like how hard it has become to trace code lines to PR/Issues. I know this can start a flame war - let's please not have it here ;) 
</comment><comment author="jasontedor" created="2016-01-14T18:04:40Z" id="171724498">&gt; I really don't like how hard it has become to trace code lines to PR/Issues.

I agree that it would be better if it were easier. But it can be done with a little work at the command line. Pretending that you think 0a2a7f2f7a2b19b4cbd5b954f028d9b5a9201fd9 is the culprit (just a random commit grabbed from #14121):

``` bash
$ comm -1 -2 \
&gt; &lt;(git rev-list 0a2a7f2f7a2b19b4cbd5b954f028d9b5a9201fd9..master --ancestry-path) \
&gt; &lt;(git rev-list 0a2a7f2f7a2b19b4cbd5b954f028d9b5a9201fd9..master --first-parent) \
&gt; | tail -1
b4a095d430f592d0a0e4f27d5beab7aed84a3441
$ git log --pretty=oneline b4a095d430f592d0a0e4f27d5beab7aed84a3441 | head -1
b4a095d430f592d0a0e4f27d5beab7aed84a3441 Merge pull request #14121 from mikemccand/fair_indexing_buffers
```

which gets us to #14121.

Additionally, the [GitHub search](https://github.com/search?q=0a2a7f2f7a2b19b4cbd5b954f028d9b5a9201fd9&amp;type=Issues&amp;utf8=&#10003;) usually gets it there for me as well (the key is to search issues).
</comment><comment author="s1monw" created="2016-01-14T21:11:25Z" id="171780937">@bleskes FYI https://github.com/elastic/elasticsearch/commit/903d0ff96ce662ddf4f2d6d27a753c6ab8a9fbeb
</comment><comment author="bleskes" created="2016-01-15T06:42:21Z" id="171883716">@jasontedor thanks for git voodoo. I'll use it if need be. Personally - I just want git blame (and it's integration with IntelliJ ) to work

@s1monw re:

&gt; @bleskes FYI 903d0ff

Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Modify load average format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15932</link><project id="" key="" /><description>This commit modifies the load_average in the node stats API response
to be an object containing the one-minute, five-minute and
fifteen-minute load averages as fields (if those values are
available). Additionally, this commit modifies the cat nodes API
response to format the one-minute, five-minute and fifteen-minute load
averages as null if any of the respective values are not available.

Relates #15907
</description><key id="126214395">15932</key><summary>Modify load average format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T16:15:15Z</created><updated>2016-01-18T16:59:20Z</updated><resolved>2016-01-18T16:59:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-13T10:34:36Z" id="171249566">This solution seems very specific.  @tlrx was working on a more generic solution here https://github.com/elastic/elasticsearch/pull/13708
</comment><comment author="jasontedor" created="2016-01-14T13:55:09Z" id="171650165">@clintongormley Recapping a few conversations we've had through other channels.

&gt; This solution seems very specific.

It is, but it's [restoring a minor feature](https://github.com/elastic/elasticsearch/blob/v1.7.4/src/main/java/org/elasticsearch/monitor/os/OsStats.java#L148-L162) that was in the 1.x line of Elasticsearch when Elasticsearch made available the one-minute, five-minute, and fifteen-minute load averages. In the 2.x line of Elasticsearch, only the one-minute load average is available and is presented as a number. In #15907, the five-minute and fifteen-minute load averages were brought back, but presenting the values as an array without the ability to change the format. This was guided initially by the default from the 1.x line, but I'm now thinking the object format is more useful than the array format.

&gt; @tlrx was working on a more generic solution here #13708

For now, this is stalled by other priorities and unlikely to make the next major release of Elasticsearch.

Given this, I think that either of the following outcomes is reasonable.
1. take the pull request as is, but add a `TODO` to revisit pending #13708 
2. modify the `load_average` field to present it as

``` json
   load_average: {
     "1m": 6.21,
     "5m": 7.01,
     "15m": 6.33
   }
```

by default and _not_ have the option to present them as an array

@clintongormley What do you think?
</comment><comment author="clintongormley" created="2016-01-14T16:14:18Z" id="171686410">I like the object notation - makes more sense to me
</comment><comment author="jasontedor" created="2016-01-15T17:23:46Z" id="172023486">&gt; I like the object notation - makes more sense to me

@clintongormley Agree, so I think it should be the default, but where does that leave us on the ability to format as an array given that it was possible in 1.x but lost in 2.x and has caused some [frustration](https://github.com/elastic/elasticsearch/pull/14741#issuecomment-170680502)?
</comment><comment author="clintongormley" created="2016-01-18T09:17:09Z" id="172471790">@jasontedor my personal take would be to support the object format only.  The array format isn't useful for anybody.  @nickcraver would you agree?
</comment><comment author="NickCraver" created="2016-01-18T11:10:10Z" id="172499557">The array is what we'd expect (and always how you see it in a system) - but if it varies across platforms then an object makes more sense. However, the more important thing is is **stability** in these monitoring APIs since they must be used across minor and major versions in all the places they are used. To support one value being formatted as an array, then an integer, then an object is nuts for static deserialization.

In porting our monitoring to support 2.0, this was 1 of 2 breaking changes in that regard. I don't think the stability is given as much attention as it deserves on these issues.
</comment><comment author="jasontedor" created="2016-01-18T12:28:24Z" id="172515876">@clintongormley I've updated this pull request to remove the array format and only format the load average as an object (values that are not available will not appear in the response).

On OS X:

```
$ curl -sS -XGET localhost:9200/_nodes/stats/os \
&gt; | jq '.nodes[].os.cpu.load_average'

{
  "1m": 0.55712890625
}
```

On Linux:

```
$ curl -sS -XGET localhost:9200/_nodes/stats/os \
&gt; | jq '.nodes[].os.cpu.load_average'

{
  "1m": 2.81,
  "5m": 7.12,
  "15m": 7.2
}
```
</comment><comment author="clintongormley" created="2016-01-18T13:34:01Z" id="172528160">&gt; In porting our monitoring to support 2.0, this was 1 of 2 breaking changes in that regard. I don't think the stability is given as much attention as it deserves on these issues.

Yes I get that - we didn't think it through clearly enough, but we can't change history so we're looking for the best path forward.

&gt; I've updated this pull request to remove the array format and only format the load average as an object

@jasontedor I think this is good - it works across any system, including indexing back into Elasticsearch itself.  thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend tracking of parent tasks to master node, replication and broadcast actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15931</link><project id="" key="" /><description>Now MasterNodeOperations, ReplicationAllShards, ReplicationSingleShard,  BroadcastReplication and BroadcastByNode actions keep track of their parent tasks.
</description><key id="126193852">15931</key><summary>Extend tracking of parent tasks to master node, replication and broadcast actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T14:42:21Z</created><updated>2016-01-29T09:23:38Z</updated><resolved>2016-01-28T17:54:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-19T15:41:29Z" id="172891916">@imotov I left some comments - looks good
</comment><comment author="imotov" created="2016-01-20T16:36:04Z" id="173264108">@s1monw I merged ChildTask and Task. Since child task is almost universally applicable concern, I think it makes sense and it significantly simplifies things for task cancellation as well. 
</comment><comment author="imotov" created="2016-01-21T17:53:46Z" id="173652971">@s1monw I tried to inject static listeners during construction but it was getting too convoluted. Moreover, while I was struggling with it, I realized that I might not actually need these listeners in the production code after all. I think I came up with a much better way to implement the notifications that I need and I will show this mechanism in the next PR. For this PR, I kept dynamic registration of listeners but I moved it from TaskManager to a newly created MockTaskManager, which I can enable in tests when I need to. What do you think?
</comment><comment author="s1monw" created="2016-01-28T13:50:53Z" id="176193132">lets do it! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date format field comparing problem..</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15930</link><project id="" key="" /><description>here is my xshell log.
the problem is gte and lte does not have the same behivor in comparing using two different date format

[BEGIN] 2016/1/12 22:01:27
[root@4-216 ~]# curl -XDELETE 192.168.4.216:9280/test_red/t6
{"error":"TypeMissingException[[_all] type[[t6]] missing: No index has the type.]","status":404}[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# curl -XPUT 192.168.4.216:9280/test_red/t6/_mapping -d '

&gt; ```
&gt;           {
&gt;               "t6":{
&gt;                   "_all":{
&gt;                       "auto_boost" : true,
&gt;                       "index_analyzer" : "ik"
&gt;                   },
&gt;                   "properties" : {
&gt;                       "start_time" : {
&gt;                         "type" : "date",
&gt;                         "format" : "YYYY-MM-dd"
&gt;                       }
&gt;                   }
&gt;               }
&gt;           }'
&gt; ```
&gt; 
&gt; {"acknowledged":true}[root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_mapping?pretty
&gt; {
&gt;   "test_red" : {
&gt;     "mappings" : {
&gt;       "t6" : {
&gt;         "_all" : {
&gt;           "auto_boost" : true,
&gt;           "index_analyzer" : "ik"
&gt;         },
&gt;         "properties" : {
&gt;           "start_time" : {
&gt;             "type" : "date",
&gt;             "format" : "YYYY-MM-dd"
&gt;           }
&gt;         }
&gt;       }
&gt;     }
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XPOST 192.168.4.216:9280/test_red/t6/1 -d '{
&gt;                 "start_time":"2016-01-02"
&gt;             }'
&gt; {"_index":"test_red","_type":"t6","_id":"1","_version":1,"created":true}[root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XPOST 192.168.4.216:9280/test_red/t6/2?pretty -d '{
&gt;                 "start_time":"2016-01-03"
&gt;             }'
&gt; {
&gt;   "_index" : "test_red",
&gt;   "_type" : "t6",
&gt;   "_id" : "2",
&gt;   "_version" : 1,
&gt;   "created" : true
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XPOST 192.168.4.216:9280/test_red/t6/3?pretty -d '{
&gt;                 "start_time":"2016-01-04"
&gt;             }'
&gt; {
&gt;   "_index" : "test_red",
&gt;   "_type" : "t6",
&gt;   "_id" : "3",
&gt;   "_version" : 1,
&gt;   "created" : true
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 3,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "1",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-02"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "2",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-03"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "3",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-04"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# # ive had three document indexed just now
&gt; [root@4-216 ~]# # now i will  try to make some filter test
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# ^C
&gt; [root@4-216 ~]# ^C
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd",
&gt;                             "lte": "2016-01-03"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 2,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "1",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-02"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "2",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-03"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd",
&gt;                             "gte": "2016-01-03"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 2,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "2",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-03"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "3",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-04"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# ############# see, 2016-01-03 is matched in the two filter query, one is lte filter, and anther is gte filter
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# #### the date format in filter is YYYY-MM-dd
&gt; [root@4-216 ~]# #### now lets see what will i get with filter format YYYY-MM-dd HH:mm:ss
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd HH:mm:ss",
&gt;                             "lte": "2016-01-03 00:00:00"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 2,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "1",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-02"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "2",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-03"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd HH:mm:ss",
&gt;                             "lte": "2016-01-03 01:00:00"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 2,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "1",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-02"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "2",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-03"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd HH:mm:ss",
&gt;                             "gte": "2016-01-03 14:00:00"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 1,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "3",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-04"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd HH:mm:ss",
&gt;                             "gte": "2016-01-03 00:00:00"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 1,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 2,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "2",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-03"
&gt;             }
&gt;     }, {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "3",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-04"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# #### do you see the difference ? 
&gt; [root@4-216 ~]# #### when i use lte filter to match less and equal to 2016-01-03 01:00:00, the date 2016-01-03 is matched
&gt; [root@4-216 ~]# #### when i use gte filter to match greater and equal to 2016-01-03 14:00:00, the date 2016-01-03 is not matched
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# #### when i use gte filter to match greater and equal to 2016-01-03 00:00:00, the date 2016-01-03 is  matched
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# #### now lets see gte filter to match greater and equal to 2016-01-03 01:00:00.   which can be compared with the lte filter  to match less and equal to 2016-01-03 01:00:00
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
&gt;                 "filter":{
&gt;                     "range":{
&gt;                         "start_time":{
&gt;                             "format": "YYYY-MM-dd HH:mm:ss",
&gt;                             "gte": "2016-01-03 01:00:00"
&gt;                         }
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {
&gt;   "took" : 2,
&gt;   "timed_out" : false,
&gt;   "_shards" : {
&gt;     "total" : 5,
&gt;     "successful" : 5,
&gt;     "failed" : 0
&gt;   },
&gt;   "hits" : {
&gt;     "total" : 1,
&gt;     "max_score" : 1.0,
&gt;     "hits" : [ {
&gt;       "_index" : "test_red",
&gt;       "_type" : "t6",
&gt;       "_id" : "3",
&gt;       "_score" : 1.0,
&gt;       "_source":{
&gt;                 "start_time":"2016-01-04"
&gt;             }
&gt;     } ]
&gt;   }
&gt; }
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# #### see, it doesnt match
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# ### its still has the problem after i update the date filed format from "YYYY-MM-dd" to "YYYY-MM-dd HH:mm:ss"
&gt; [root@4-216 ~]# ### is this regular????????
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# ####### now lets update the format
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XPUT 192.168.4.216:9280/test_red/_mapping/t6 -d '
&gt;             {
&gt;                 "_all":{
&gt;                     "auto_boost" : true,
&gt;                     "index_analyzer" : "ik"
&gt;                 },
&gt;                 "properties" : {
&gt;                     "start_time" : {
&gt;                       "type" : "date",
&gt;                       "format" : "YYYY-MM-dd HH:mm:ss"
&gt;                     }
&gt;                 }
&gt;             }'
&gt; {"acknowledged":true}[root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# 
&gt; [root@4-216 ~]# curl -XPUT 192.168.4.216:9280/test_red/_mapping/t6 -d '
&gt;             {
&gt;                 "_all":{
&gt;                     "auto_boost" : true,
&gt;                     "index_analyzer" : "ik"
&gt;                 },
&gt;                 "properties" : {
&gt;                     "start_time" : {
&gt;                       "type" : "date",
&gt;                       "format" : "YYYY-MM-dd HH:mm:ss"
&gt;                     }
&gt;                 }
&gt; [root@4-216 ~]# ####### now lets update the format

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "gte": "2016-01-03 01:00:00"
                        }
                    }
                }
[root@4-216 ~]# #### now lets see gte filter to match greater and equal to 2016-01-03 01:00:00.   which can be compared with the lte filter  to match less and equal to 2016-01-03 01:00:00

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "gte": "2016-01-03 00:00:00"
                        }
                    }
                }
                            "gte": "2016-01-03 14:00:00"
                        }
                    }
                }
                            "lte": "2016-01-03 01:00:00"
                        }
                    }
                }
                            "lte": "2016-01-03 00:00:00"
                        }
                    }
                }
[root@4-216 ~]# #### now lets see what will i get with filter format YYYY-MM-dd HH:mm:ss

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "lte": "2016-01-03 00:00:00"
                        }
                    }
                }
[root@4-216 ~]# #### now lets see what will i get with filter format YYYY-MM-dd HH:mm:ss

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd",
                            "gte": "2016-01-03"
                        }
                    }
                }
                            "lte": "2016-01-03"
                        }
                    }
                }
[root@4-216 ~]# # now i will  try to make some filter test

[root@4-216 ~]# curl -XPOST 192.168.4.216:9280/test_red/t6/3?pretty -d '{
                "start_time":"2016-01-04"
[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd",
                            "lte": "2016-01-03"
                        }
                    }
                }
            }'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-02"
            }
    }, {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-03"
            }
    } ]
  }
}
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd",
                            "lte": "2016-01-03"
                        }
                    }
                }
                            "gte": "2016-01-03"
                        }
                    }
                }
            }'
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-03"
            }
    }, {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-04"
            }
    } ]
  }
}
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# ######### we got 2016-01-03 both
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd",
                            "gte": "2016-01-03"
                        }
                    }
                }
                            "lte": "2016-01-03"
                        }
                    }
                }
[root@4-216 ~]# curl -XPUT 192.168.4.216:9280/test_red/_mapping/t6 -d '
            {  
                "_all":{
                    "auto_boost" : true,
                    "index_analyzer" : "ik"
                },  
                "properties" : {
                    "start_time" : {
                      "type" : "date",
                      "format" : "YYYY-MM-dd HH:mm:ss"
                    }
                }
[root@4-216 ~]# ####### now lets update the format

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "gte": "2016-01-03 01:00:00"
                        }
                    }
                }
[root@4-216 ~]# #### now lets see gte filter to match greater and equal to 2016-01-03 01:00:00.   which can be compared with the lte filter  to match less and equal to 2016-01-03 01:00:00

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "gte": "2016-01-03 01:00:00"
                        }
                    }
                }
[root@4-216 ~]# #### now lets see gte filter to match greater and equal to 2016-01-03 01:00:00.   which can be compared with the lte filter  to match less and equal to 2016-01-03 01:00:00

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "gte": "2016-01-03 00:00:00"
                        }
                    }
                }
                            "gte": "2016-01-03 14:00:00"
                        }
                    }
                }
                            "lte": "2016-01-03 01:00:00"
                        }
                    }
                }
            }'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-02"
            }
    }, {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-03"
            }
    } ]
  }
}
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "lte": "2016-01-03 01:00:00"
                        }
                    }
                }
[root@4-216 ~]# ######### we got 2016-01-03 both

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd",
                            "gte": "2016-01-03"
                        }
                    }
                }
                            "lte": "2016-01-03"
                        }
                    }
                }
[root@4-216 ~]# curl -XPUT 192.168.4.216:9280/test_red/_mapping/t6 -d '
            {  
                "_all":{
                    "auto_boost" : true,
                    "index_analyzer" : "ik"
                },  
                "properties" : {
                    "start_time" : {
                      "type" : "date",
                      "format" : "YYYY-MM-dd HH:mm:ss"
                    }
                }
[root@4-216 ~]# ####### now lets update the format

[root@4-216 ~]# curl -XGET 192.168.4.216:9280/test_red/t6/_search?pretty -d '{
                "filter":{
                    "range":{
                        "start_time":{
                            "format": "YYYY-MM-dd HH:mm:ss",
                            "gte": "2016-01-03 01:00:00"
                        }
                    }
                }
            }'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test_red",
      "_type" : "t6",
      "_id" : "3",
      "_score" : 1.0,
      "_source":{
                "start_time":"2016-01-04"
            }
    } ]
  }
}
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# #### we got 2016-01-03 only once...
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 
[root@4-216 ~]# 

[END] 2016/1/12 22:16:50
</description><key id="126193551">15930</key><summary>date format field comparing problem..</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q11112345</reporter><labels><label>:Dates</label><label>feedback_needed</label></labels><created>2016-01-12T14:41:00Z</created><updated>2016-01-14T02:19:04Z</updated><resolved>2016-01-12T14:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T14:50:39Z" id="170936953">Hi @q11112345 

First, could you tell us what version of ES you are using? Second, could you reformat your post to provide a simple runnable example that I can copy and paste to try out, rather than having to recreate everything you did?
</comment><comment author="dadoonet" created="2016-01-12T14:54:29Z" id="170937908">Question was asked here: https://discuss.elastic.co/t/how-does-the-updating-put-mapping-api-works-to--all-field/38803

I suggest we follow up on discuss instead?
</comment><comment author="clintongormley" created="2016-01-12T14:56:06Z" id="170938384">Makes sense. btw @q11112345, deleting mappings is no longer supported, and may be involved with the issue you have.
</comment><comment author="q11112345" created="2016-01-13T01:52:47Z" id="171127898">ok. im sorry. i cant open gist.github.com. im in china.
how about give me a email-address.
version im using is 1.5.0. i forget telling you.
</comment><comment author="clintongormley" created="2016-01-13T11:04:21Z" id="171255195">@q11112345 A number of issues have been fixed around dates and ranges since then.  You should upgrade
</comment><comment author="q11112345" created="2016-01-14T02:19:04Z" id="171508004">alright. thank you. this is not a urgent.  i have to discuss with my leader about upgrading.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add convenience method for capturing and clearing requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15929</link><project id="" key="" /><description>This commit adds convenience methods to o.e.t.t.CapturingTransport
that enables capturing requests and clearing the captured requests
with a single method. This is to simplify a common pattern in tests of
capturing requests, and then clearing the captured requests.

Closes #15897
</description><key id="126189721">15929</key><summary>Add convenience method for capturing and clearing requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T14:22:56Z</created><updated>2016-03-10T18:36:34Z</updated><resolved>2016-01-12T14:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-12T14:31:14Z" id="170927844">LGTM
</comment><comment author="jasontedor" created="2016-01-12T14:43:59Z" id="170933759">@nik9000 Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent results using tribe node with transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15928</link><project id="" key="" /><description>Hi! I am using a tribe node with the transport client to connect to 2 clusters. When I look at **_cat/indices** I see consistent indexes. When I use the transport client with the below **does the specified index exist?** request I get true 50% of the time and false 50% of the time although the index does in fact exist.

Code:

``` java
public boolean indexExists(String index, TransportClient client) {
    return client.admin().indices().exists(new IndicesExistsRequest(index)).actionGet().isExists();
}
```

YML settings:

```
tribe:
    t1:
        cluster.name: clusterone
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: clusterone-master-a,clusterone-master-b,clusterone-master-c
    t2:
        cluster.name: clustertwo
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: clustertwo-master-a,clustertwo-master-b,clustertwo-master-c
```
</description><key id="126187859">15928</key><summary>Inconsistent results using tribe node with transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirajpatel</reporter><labels><label>:Tribe Node</label><label>feedback_needed</label></labels><created>2016-01-12T14:13:12Z</created><updated>2016-01-13T15:55:16Z</updated><resolved>2016-01-13T15:48:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-12T14:36:16Z" id="170930183">how do you initialize your transport client? does it point to the tribe node or to nodes that belong to the two clusters?
</comment><comment author="nirajpatel" created="2016-01-12T15:15:50Z" id="170943515">The transport client was initialized to point to the tribe node. I actually setup 3 tribe nodes so I added all 3 using the **addTransportAddress** method to my TransportClient.
</comment><comment author="javanna" created="2016-01-12T15:22:25Z" id="170945516">how come you have three tribe nodes? not sure how that would work, I would expect you to have a single tribe node and the transport client pointing to that one. I am thinking you get different answers because of this.
</comment><comment author="nirajpatel" created="2016-01-12T15:25:40Z" id="170946418">Okay, let me see if that is the reason! (I have 3 in the case that one of them goes down)
</comment><comment author="nirajpatel" created="2016-01-12T18:20:20Z" id="171001176">@javanna I did 100 iterations and here are the results for if the TransportClient sees that the index exists

false
false
false
true
true
true
true
false
false
true
false
false
false
true
true
true
true
false
false
true
false
false
false
true
true
true
true
false
....

Meanwhile the REST API is consistent.
</comment><comment author="javanna" created="2016-01-12T18:46:48Z" id="171009691">Can you please post the code that initialized your `TransportClient` ? which node do you send the request to through the REST layer?
</comment><comment author="nirajpatel" created="2016-01-12T19:35:48Z" id="171027854">Sure!

``` java
private String tribeClusterName = "tribe-cluster-name";
private String tribeClusterHosts = "tribe-node-a";
private int tribeClusterPort = 9300;

public TransportClient tribeTransportClient() {
    TransportClient transportClient = new TransportClient(clientSettings(tribeClusterName, true));
    for (String dataAddress : tribeClusterHosts.split(",")) {
        TransportAddress transportAddress = new InetSocketTransportAddress(dataAddress, tribeClusterPort);
        transportClient.addTransportAddress(transportAddress);
    }
    return transportClient;
}

private Settings clientSettings(String clusterName, boolean sniff) {
    return ImmutableSettings.settingsBuilder()
        .put("cluster.name", clusterName)
        .put("client.transport.sniff", sniff)
        .build();
}
```

I send the request to tribe-node-a:9200/_cat/indices through the REST interface.
</comment><comment author="javanna" created="2016-01-13T07:46:49Z" id="171206983">hi @nirajpatel can you try and remove sniffing please? I am afraid that it will make the transport client point to the tribe node plus all of the different nodes in the two clusters, which would explain your mixed results.
</comment><comment author="nirajpatel" created="2016-01-13T15:48:15Z" id="171337732">@javanna that worked!! thank you :+1: 
</comment><comment author="javanna" created="2016-01-13T15:55:16Z" id="171340796">thanks for getting back to us @nirajpatel !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for loading a plugin in a client node is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15927</link><project id="" key="" /><description>In 2.1.1 (and, I think, all 2.X versions), there's no support for loading plugins to a client-only node.

For example, there's no (obvious) way to load plugins when doing something like:

``` java
Node node = NodeBuilder.nodeBuilder()
        .clusterName("my-cluster")
        .settings(settings)
        .client(true)
        .build();
```

An `addPlugin` method on NodeBuilder along the lines of `TransportClient.addPlugin` would be helpful.

The work-around is to override Node, duplicate the environment/version constructor logic, and provide your custom list of plugins to load:

``` java
private static class PluginEnabledNode extends Node {
    public PluginEnabledNode(Settings settings, Collection&lt;Class&lt;? extends Plugin&gt;&gt; plugins) {
        super(InternalSettingsPreparer.prepareEnvironment(settings, null),
                Version.CURRENT,
                plugins);
    }
}
```

And then you can directly instantiate a `PluginEnabledNode` (being careful to set "node.client" and "cluster.name" in your settings)

``` java
Node node = new PluginEnabledNode(settings, ImmutableList.&lt;Class&lt;? extends Plugin&gt;&gt;of(CloudAwsPlugin.class)).start()
```

Note to anyone attempting this work-around -- my initial attempt resulted in a very large number of Guice related injection errors when the Node was initializing.  These were due to a library version conflict between what was used by my project and what was used by the plugin I was adding.  In my case, I had an older version of Jackson and the plugin was using a new version that relied on methods that didn't exist in the older version.
</description><key id="126184048">15927</key><summary>Support for loading a plugin in a client node is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbaggott</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-01-12T13:52:24Z</created><updated>2016-07-27T19:45:54Z</updated><resolved>2016-01-18T22:01:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dbaggott" created="2016-01-12T13:55:31Z" id="170918425">Also [discussed here](https://discuss.elastic.co/t/cloud-aws-for-es-2-1-1-in-maven/38924/5).
</comment><comment author="jprante" created="2016-01-12T14:10:05Z" id="170922498">@dbaggott `NodeBuilder` has become unusable for plugins.

See https://github.com/elastic/elasticsearch/issues/15060
and https://github.com/elastic/elasticsearch/issues/13212
and https://github.com/elastic/elasticsearch/pull/15107
and https://discuss.elastic.co/t/add-plugins-from-classpath-in-embedded-elasticsearch-client-node/36269
</comment><comment author="dadoonet" created="2016-01-12T14:12:45Z" id="170923151">I think this is important in cases where users want to use a Client Node to connect to AWS or Azure for example. Those users need to add a cloud plugin.
We should either support it out of the box, either document how they can do it, either remove support for Client Node...

Thoughts?
</comment><comment author="clintongormley" created="2016-01-12T14:27:07Z" id="170926689">A client only node is just a node, with a config directory etc.  What's the issue with installing the plugin into the `plugins` directory like you'd do on any other node?
</comment><comment author="dadoonet" created="2016-01-12T14:30:24Z" id="170927497">Because you are starting a Node programmatically which is in Maven/Java:
- add dependencies to `elasticsearch`, `cloud-aws`.
- start the node in Java.

As a Java developper I would not expect having to run a single command like `bin/plugin` when I want to use a Java client.
</comment><comment author="clintongormley" created="2016-01-12T14:31:34Z" id="170927985">Then use a transport client, not a client node. 
</comment><comment author="dadoonet" created="2016-01-12T14:32:37Z" id="170928420">So we should deprecate Client Node and don't document it in the Java API doc anymore IMO.
</comment><comment author="dbaggott" created="2016-01-12T15:07:23Z" id="170941383">@jprante, thanks for all the pointers!  I spent some time searching closed issues but didn't manage to find those.  The context is very helpful, I didn't realize the history that I was stumbling into and it's helpful in understanding where everyone is coming from and why the discussion is so heated!

@dadoonet, thanks for clarifying the use case!

@clintongormley, w/r/t your first suggestion, I'll look into that.  As @dadoonet alluded to, the mechanics of doing that are very cumbersome.  But I'm gathering that it's the only "officially supported" mechanism?

@clintongormley, if one were to switch from a client node to a transport node aren't there important functional ramifications in turns of "smart load balancing" etc that are being lost?

@clintongormley, @jprant, @dadoonet, (the yuckiness of pushing internal ES details into my app aside), is the "extend a Node" hack fundamentally broken?  And apologies if that's already discussed in one of the links, I'm still working through the discussions.

I realize you all are very deep into the 2.X series but for someone who is just coming at it and upgrading from 1.7 to 2.X, the "right thing" to do in this situation is _very opaque_.  As a user of ES, I'm very motivated to do the right thing, I'm just still working through what that is/how to do it!  So, a strong +1 for providing guidance on what the right thing is and a strong +1 for making it practical to do the right thing.  Of all the breaking changes in 2.X that we're dealing with, this is the one that's the least documented and that I'm spending the most time on.  I realize that it's only a subset of users so (understandably) not a top priority but for anyone who is in this situation, it's painful.
</comment><comment author="dbaggott" created="2016-01-12T15:09:45Z" id="170941974">(And, I forgot to say, @clintongormley, thanks for the suggestions!)
</comment><comment author="clintongormley" created="2016-01-13T10:32:23Z" id="171249132">@dbaggott You're right that things have changed a lot in this area in the 2.x series, and that this has been discussed in a number of places, and also that the change can be quite painful (for which i apologise).  Just know that it isn't a change we made lightly.

i tried to explain the reasoning behind it here: https://discuss.elastic.co/t/add-plugins-from-classpath-in-embedded-elasticsearch-client-node/36269/5?u=clinton_gormley

&gt; if one were to switch from a client node to a transport node aren't there important functional ramifications in turns of "smart load balancing" etc that are being lost?

The transport client still handle load balancing. It's slightly dumber in that it doesn't have the cluster state and so sends requests to "The Cluster" rather than directly to the node which holds the relevant data, so yes, there can be an extra network hop.  This is the same way that all of the HTTP clients work.

Previously we recommended embedding a node client into your application but we have moved away from this recommendation - it is fraught with issues.  Really, the only way to use a node client (if at all) is to run a standalone (not embedded) one on your local app server. Your application uses the transport client to talk to the node client on localhost, and the node client then sends the request to the right node in the cluster.

Honestly, I'm not convinced that this setup is worth it.  One of the issues is: let's say you have 1,000 app servers... suddenly you have 1,000 extra nodes in your cluster, all of which need to receive cluster state updates!  Cluster state updates wait for acknowledgement, so suddenly you are subject to the latency of the slowest node out of an extra 1,000.
</comment><comment author="dbaggott" created="2016-01-13T14:51:26Z" id="171316721">@clintongormley, thanks for the helpful reply and for the link which I had read (and now read again).  Your caveat about large numbers of apps aside, running a local node client and talking to it on localhost is the most practical suggestion I've heard (at least for our development/deployment ecosystem).  I also see that that's a variation on [what's recommended in the 2.1 docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-node.html).  Our use of a Node-based client over a TransportClient is entirely motivated by the perceived benefits of being cluster-intimate.  If I can find the bandwidth, I'll test how much of a difference it actually makes for us.

Preface: I'm not trying to argue for any course of action or convince anyone to do anything differently, I'm just trying to understand:  On the surface of things, it doesn't _seem_ entirely coherent to 1. have a library api that provides facilities for building a transport client with any arbitrary 3rd party plugin; 2. have that same api only support building client nodes without plugins.  In short, what would be the fundamental difference between running these two lines of code from an external application?

``` java
TransportClient.builder().settings(settings).addPlugin(MaliciousPlugin.class).build();
NodeBuilder.nodeBuilder().settings(settings).addPlugin(MaliciousPlugin.class).build().client();
```

In either case, the malicious plugin code running in the external app could represent itself as a node and join the cluster with malicious intent, no?  Ie, just because it's being loaded by a TransportClient, doesn't prevent the plugin from acting with evil intent.  And the issues w/r/t JarHell and class loaders apply to both?  Or am I missing something in my assumptions or thinking?

Again, thank you for taking the time to respond!
</comment><comment author="clintongormley" created="2016-01-14T08:34:35Z" id="171571791">&gt; In either case, the malicious plugin code running in the external app could represent itself as a node and join the cluster with malicious intent, no? 

No, that's the difference.  A transport client is **not** a node. It doesn't join the cluster. It is just a lightweight client that talks to the cluster.  
</comment><comment author="dbaggott" created="2016-01-14T18:23:46Z" id="171729758">I get that distinction -- I was trying to suggest that, from a security perspective, the malicious plugin is malicious and so will, by definition, do things it's not supposed to do (like open up ports to emulate a node and join the cluster).

I'm guessing what I'm missing is the assumption that the ES cluster is completely locked down from a network perspective and you expect that a client application that's only running a TransportClient would _only_ have access the cluster's http port whereas a client application running an embedded node would necessarily have broader network access to the cluster...  In the former case, it's considered "safe" _for the ES cluster_ to allow loading of a 3rd party plugin since they can only interact with the cluster via the http protocol which is hardened.  That, of course, provides no guarantees for the security of the _client application/server_ itself in the face of a malicious plugin that was loaded via the TransportClient.

In the perfect world, am I correct in thinking that a transport client should ideally support loading of plugins using a similarly secure security mechanism as the standard `bin/plugin install` mechanism?  I appreciate that that is probably a low priority and might contain difficulties that make it unattractive and/or impractical.  But, if that were the case, then loading a plugin via the TransportClient would be safer and loading plugins via embedded nodes would then be a non-issue...
</comment><comment author="rjernst" created="2016-01-18T22:01:52Z" id="172666157">@dbaggott Malicious code is malicious code. With it, from a TransportClient, it could do anything that your app allows code to do. We can't lock down your jvm with SecurityManager as we do in Elasticsearch when you build a transport client. If you wish to run your own SecurityManager, please do, it is highly encouraged! Note that this has nothing to do with what a Node can do within the cluster.

As for the issue here, in master, the Node ctor that takes plugins is now public (and NodeBuilder has been removed). If you _really_ must do so, you can use the Node ctor as you did NodeBuilder before, but this is essentially unsupported, may break in any release, and for the reasons that have already been listed in this issue, is highly discouraged. It is much better to use a client, whether talking with a remove cluster, or a local node that is connected to the cluster.
</comment><comment author="denov" created="2016-07-27T19:18:42Z" id="235691070">the docs on - https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/client-connected-to-client-node.html  don't seem to line up with this discussion.   i'm in the process of doing a 1.4 to 2.3 upgrade and have found the docs a bit confusing.  IMO, it shouldn't be hard to find some docs on how to configure a java app running on EC2.  
</comment><comment author="rjernst" created="2016-07-27T19:25:01Z" id="235692716">@denov See the docs for the upcoming 5.0 release, maybe that clarifies?
https://www.elastic.co/guide/en/elasticsearch/client/java-api/master/client-connected-to-client-node.html
</comment><comment author="denov" created="2016-07-27T19:29:29Z" id="235693843">@rjernst thanks for the link, but no.  that pages also says 'think about discovery plugins for example', i'm thinking.... but can't find the how.

I'm just looking on how to load the cloud-aws https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-aws.html for my app can discover the nodes running ES.

in 1.x  i just needed the plugin in the classpath and it was all good.  now that's not the case.  
</comment><comment author="rjernst" created="2016-07-27T19:45:54Z" id="235698182">@denov Please ask questions like this on https://discuss.elastic.co. There are multiple past discussions there that should help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>redirect ingest requests to an ingest node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15926</link><project id="" key="" /><description>Rather than failing the request, when a node with node.ingest set to false receives an index or bulk request with a pipeline id, it should try to redirect the request to another node with node.ingest set to true. If there are no node with ingest set to true based on the current cluster state, an exception will be returned and the request will fail. Note that in case there are no ingest nodes and bulk has a pipeline id  specified only for a subset of index requests, the whole bulk will fail.
</description><key id="126163892">15926</key><summary>redirect ingest requests to an ingest node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-12T11:58:07Z</created><updated>2016-01-12T16:16:33Z</updated><resolved>2016-01-12T16:16:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-12T11:58:43Z" id="170890222">The current PR has unit tests, but some integration test with multiple nodes may need to be added, I am on the fence about that. Maybe a qa multinode test.
</comment><comment author="javanna" created="2016-01-12T15:28:48Z" id="170947274">I pushed new commits, this should be ready
</comment><comment author="martijnvg" created="2016-01-12T16:08:50Z" id="170959621">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to install scripting plugins from source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15925</link><project id="" key="" /><description>Looking at docs here: https://www.elastic.co/guide/en/elasticsearch/plugins/current/lang-python.html

This assumes you have access to the internet. The server I need to install this on does not have access to anything outside our local network (company security measures). 

Can we get instructions on how to build from source?

It's hinted at in the old repository: https://github.com/elastic/elasticsearch-lang-python although not actually explained.

Is it still possible with the new location of the plugins?
</description><key id="126162003">15925</key><summary>How to install scripting plugins from source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NDevox</reporter><labels /><created>2016-01-12T11:48:27Z</created><updated>2016-01-13T10:37:42Z</updated><resolved>2016-01-12T13:12:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-12T13:12:43Z" id="170907517">In 2.x branches:

``` sh
git clone https://github.com/elastic/elasticsearch.git
git checkout v2.1.1
cd elasticsearch/plugins/lang-python
mvn install
```

The plugin is built in `target/releases`.

But you don't need that to install the plugin. You can also download a packaged version from our download service.

For example here: https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/lang-python/2.1.1/lang-python-2.1.1.zip
</comment><comment author="NDevox" created="2016-01-12T16:37:35Z" id="170968155">@dadoonet do you think it is worth adding this to the docs?
</comment><comment author="dadoonet" created="2016-01-12T16:53:04Z" id="170972740">I don't know. Running `bin/plugin -v` gives the exact URL which is used to download the file locally.
Also note that maven central has also those files: http://search.maven.org/#search%7Cga%7C1%7Clang-python

@clintongormley WDYT? Should we document how is organized our download service for such a case?
Or add a `--do-not-install` / `--just-download` option to the plugin manager?
</comment><comment author="clintongormley" created="2016-01-13T10:37:42Z" id="171250150">Perhaps we can add a line to the docs for each plugin saying "This plugin can be downloaded for offline install from xxxxx", and just use an asciidoc `:plugin_url:` replacement to keep the URLs up to date.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify MapperService#searchFilter(...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15924</link><project id="" key="" /><description>This method has a lot of logic that deals with the fact if type field isn't indexed. Since the the type field is locked down (like other meta fields), the type field is now always indexed. So it is time to clean this up.

It also has logic if only one type has been specified, which can be removed because BooleanQuery optimises when there is only one clause.

There is also logic to hide percolator queries. In order to further simplify this method I think we should not do this anymore? A percolator query is just a document and should be returned like other documents. If it percolator queries shouldn't be returned then the query defined in the search request should just be modified to do so. We should implicitly exclude these docs from the search results any more.
</description><key id="126156995">15924</key><summary>Simplify MapperService#searchFilter(...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2016-01-12T11:19:42Z</created><updated>2016-01-20T21:27:58Z</updated><resolved>2016-01-20T21:27:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-12T16:28:24Z" id="170965247">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix MapperService#searchFilter(...)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15923</link><project id="" key="" /><description>Search filter should wrap the types filters in a separate boolean as should clauses

So that a document must either match with one of the types and the non nested clause.

Closes #15757
</description><key id="126155788">15923</key><summary>Fix MapperService#searchFilter(...)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T11:12:45Z</created><updated>2016-01-22T11:10:03Z</updated><resolved>2016-01-13T08:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-12T13:19:52Z" id="170909162">LGTM. I'd have liked to see a test that verifies what can be found but with our desire to limit integration tests I think it'd be a pain. I think it'd be more obvious what its fixing.
</comment><comment author="jpountz" created="2016-01-12T13:41:57Z" id="170914719">LGTM2. I think that as a follow-up we should look into simplifying this method and:
- remove the `useTermsFilter` optimization: given that TermsFilter rewrites to a BooleanQuery when there are less then 16 terms anyway it doesn't change anything
- refactor the code so that we don't have 3 completely different branches for the no type, one type and several types cases
</comment><comment author="martijnvg" created="2016-01-12T14:15:02Z" id="170923720">thanks @nik9000 @jpountz! I opened #15924 to simplify this method.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Review feedback and several cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15922</link><project id="" key="" /><description>@javanna @martijnvg FYI
</description><key id="126154924">15922</key><summary>Review feedback and several cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Ingest</label></labels><created>2016-01-12T11:07:25Z</created><updated>2016-01-13T11:04:07Z</updated><resolved>2016-01-13T11:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-12T11:08:03Z" id="170877872">I will do another review once we got rid of the index as a storage - looks pretty good already guys!
</comment><comment author="martijnvg" created="2016-01-12T12:46:08Z" id="170901174">+1 for the cleanups. We will address the TODO's in followup PRs.
</comment><comment author="s1monw" created="2016-01-12T14:55:49Z" id="170938257">@martijnvg should i just merge it then?
</comment><comment author="javanna" created="2016-01-12T15:19:42Z" id="170944676">yes @s1monw merge please, we will take it from here.
</comment><comment author="martijnvg" created="2016-01-12T16:16:36Z" id="170961774">@s1monw yes, +1 to merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function_score ignores functions if inner query is filter-only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15921</link><project id="" key="" /><description>Working on _Elasticsearch 2.1.1_ - sorry in advance for the probably inaccurate terminology.

When the `query` section of a `function_score` query would not normally rank results (e.g., a `bool` query only containing `filter` and `must_not` clauses), the function score query ranks `0` all elements regardless of the listed `functions`.

Moving the filtering to an outside query works as expected.
In Elasticsearch 1.x, a `filtered` query only containing the `filter` section gets scored normally.

``` bash
curl -XPUT localhost:9200/test/test/1 -d '
{"rank": 1}'
curl -XPUT localhost:9200/test/test/2 -d '
{"rank": 2}'
curl -XPUT localhost:9200/test/test/3 -d '
{"rank": 3}'

# scores everything 0
curl -XPOST localhost:9200/test/test/_search -d '
{
   "query": {
      "function_score": {
         "query": {
            "bool": {
               "filter": {
                  "range": {
                     "rank": {
                        "gte": 2
                     }
                  }
               }
            }
         },
         "functions": [
            {
               "field_value_factor": {
                  "field": "rank"
               }
            }
         ]
      }
   }
}'

# scores appropriately
curl -XPOST localhost:9200/test/test/_search -d '
{
   "query": {
      "bool": {
         "filter": {
            "range": {
               "rank": {
                  "gte": 2
               }
            }
         },
         "must": {
            "function_score": {
               "functions": [
                  {
                     "field_value_factor": {
                        "field": "rank"
                     }
                  }
               ]
            }
         }
      }
   }
}'
```
</description><key id="126128478">15921</key><summary>function_score ignores functions if inner query is filter-only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label></labels><created>2016-01-12T08:53:49Z</created><updated>2016-01-13T16:27:06Z</updated><resolved>2016-01-13T16:27:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T13:25:11Z" id="170910814">It appears the filter returns the score as `0` instead of `1`, which doesn't seem right.  @jpountz ?
</comment><comment author="jpountz" created="2016-01-12T14:58:01Z" id="170938990">The first query gives a score of 0 to all docs because there are no scoring clauses. In order to get a score of 1 you would need to add a `must` clause with a `match_all` query, or more simply since there is a single filter, wrap it under a `constant_score` query:

```
curl -XPOST localhost:9200/test/test/_search -d '
{
   "query": {
      "function_score": {
         "query": {
            "constant_score": {
               "range": {
                  "rank": {
                     "gte": 2
                  }
               }
            }
         },
         "functions": [
            {
               "field_value_factor": {
                  "field": "rank"
               }
            }
         ]
      }
   }
}'
```
</comment><comment author="clintongormley" created="2016-01-12T15:01:44Z" id="170939956">@jpountz this is surprising... filtered docs in 1.x were all given a score of 1.0.  Is this not possible with `bool.filter`?  If not, then this should be documented.
</comment><comment author="jpountz" created="2016-01-12T16:01:52Z" id="170957636">We could automatically add a match_all MUST clause if there are no positive clauses in order to get a score of one. But isn't it more natural to get null scores if there are no scoring clauses?
</comment><comment author="micpalmia" created="2016-01-12T16:09:30Z" id="170959806">Regardless of whether it's more natural to get a score of 1 or 0 in case there are no scoring clauses, a `match_all` clause is the default if the `query` clause inside the `function_score` query is not provided. It's thus surprising, when implementing a query with function scoring, that adding a few filtering query _removes_ the scoring functionality.
</comment><comment author="jpountz" created="2016-01-12T17:27:30Z" id="170982772">But you did not just add filters, you overrode the default `match_all` query with another query that returns scores that are equal to 0.

Maybe what really needs to be done is further documenting the 2.0 upgrade notes about the `filtered` query. They already state that 

```
GET _search
{
  "query": {
    "filtered": {
      "query": {
        "match": {
          "text": "quick brown fox"
        }
      },
      "filter": {
        "term": {
          "status": "published"
        }
      }
    }
  }
}
```

needs to become

```
GET _search
{
  "query": {
    "bool": {
      "must": {
        "match": {
          "text": "quick brown fox"
        }
      },
      "filter": {
        "term": {
          "status": "published"
        }
      }
    }
  }
}
```

Maybe they should also document that

```
GET _search
{
  "filtered": {
    "filter": {
      "term": {
        "status": "published"
      }
    }
  }
}
```

needs to become

```
GET _search
{
  "query": {
    "constant_score": {
      "filter": {
        "term": {
          "status": "published"
        }
      }
    }
  }
}
```
</comment><comment author="micpalmia" created="2016-01-12T21:03:17Z" id="171053633">I think transforming an unscored 1.x query into a 2.x `constant_score` query is very much unnecessary most of the time. I happily transformed all the production queries that were filter-only into boolean filters without issues, because the output score was irrelevant.

The thing that I was personally missing (and that I think is not clear at all from the docs) is that unscored queries stopped returning a score of 1 for all documents and now return a score of 0. And this obviously has important consequences in a context in which scores get multiplied by each other, like in the `function_score` query.

Thank for your help!
</comment><comment author="clintongormley" created="2016-01-13T10:58:34Z" id="171254208">I've added the documentation here: https://github.com/elastic/elasticsearch/commit/33e7bb6fba5bf0a28d3fb5462cff931be579c366

Just wondering if the function score query should always have an implicit `match_all`, to make it function the way it did before.
</comment><comment author="clintongormley" created="2016-01-13T16:27:06Z" id="171350358">&gt; Just wondering if the function score query should always have an implicit match_all, to make it function the way it did before.

To put this remark into context, I was thinking of having an implicit `match_all` if the user specified the `filter` parameter in the `function_score` query, but that parameter no longer exists in 2.0, so my remark doesn't make much sense. 

Given that the docs have been added, I think we can close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong "index.routing.allocation.enable" value prevents all allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15920</link><project id="" key="" /><description>(This happens on 1.7.3 and 2.1.0)

On single node cluster, do this

```
PUT recoverytest

PUT recoverytest/_settings
{
  "index.routing.allocation.enable": "true"
}

```

and restart node.
You get this exception and no shard is allocated.

```
[2016-01-12 17:04:52,871][ERROR][cluster.routing          ] [Blue Shield] unexpected failure during [cluster_reroute(async_shard_fetch)], current state version [2]
java.lang.IllegalArgumentException: Illegal allocation.enable value [TRUE]
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider$Allocation.parse(EnableAllocationDecider.java:189)
    at org.elasticsearch.cluster.routing.allocation.decider.EnableAllocationDecider.canAllocate(EnableAllocationDecider.java:91)
    at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)
    at org.elasticsearch.gateway.PrimaryShardAllocator.buildNodesToAllocate(PrimaryShardAllocator.java:179)
    at org.elasticsearch.gateway.PrimaryShardAllocator.allocateUnassigned(PrimaryShardAllocator.java:96)
    at org.elasticsearch.gateway.GatewayAllocator.allocateUnassigned(GatewayAllocator.java:125)
    at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:72)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:175)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:155)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:141)
    at org.elasticsearch.cluster.routing.RoutingService$2.execute(RoutingService.java:176)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

I think one wrong value in index level setting shouldn't prevent other indices' shards allocation.
Could be dup of #6732 since this shouldn't happen if you can't set wrong value.
</description><key id="126123028">15920</key><summary>Wrong "index.routing.allocation.enable" value prevents all allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels /><created>2016-01-12T08:14:37Z</created><updated>2017-06-20T14:54:17Z</updated><resolved>2016-01-12T13:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T13:22:40Z" id="170910088">Fixed by https://github.com/elastic/elasticsearch/pull/15278
</comment><comment author="haiguo" created="2016-01-12T13:45:02Z" id="170915385">What we saw is that entire cluster recover can be stopped by a bad setting on a single index.  Master node will stop directly recovery when an IllegalArgumentException is thrown.  Is that fixed in 15278?
</comment><comment author="clintongormley" created="2016-01-12T13:48:42Z" id="170916733">I missed the fact that this is an index setting rather than a cluster setting. Index settings are still being worked on, but yes, the validation of index settings will fix this.  This will be fixed once https://github.com/elastic/elasticsearch/issues/6732 is closed
</comment><comment author="promiseofcake" created="2017-06-20T14:54:16Z" id="309783978">What can we do if we are currently in this state short of upgrading? (2.4.2)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High disk watermark exceeded on one or more nodes, rerouting shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15919</link><project id="" key="" /><description>I am running Elasticsearch, and Kibana on Windows and using Synology NAS as storage for Elasticsearch. For few days, Elasticsearch started behaving weird; therefore, I checked elasticsearch.log and found the following errors:

[WARN ][cluster.routing.allocation.decider] [Desmond Pitt] high disk watermark [0b] exceeded on [O2-Ef7fET9S_MJNAL-q_yA][Desmond Pitt] free: -1b[100%], shards will be relocated away from this node
[WARN ][cluster.routing.allocation.decider] [Desmond Pitt] high disk watermark [0b] exceeded on [O2-Ef7fET9S_MJNAL-q_yA][Desmond Pitt] free: -1b[100%], shards will be relocated away from this node

[INFO ][cluster.routing.allocation.decider] [Desmond Pitt] high disk watermark exceeded on one or more nodes, rerouting shards
DEBUG][action.bulk ] [Desmond Pitt] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
DEBUG][action.bulk ] [Desmond Pitt] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]

I have already run this command GET _node/stats and found the following issue:

fs":{"timestamp":1452402078885,"total":{},"data":[{"path":"\abc-synology1.xyz.edu\syslog\elasticsearch\nodes\0"}]}

Which I guess, shows the file system fails to report disk usage, which confuses the high water mark check.

If I am right, how can I resolve it or what else could be the issue?

By the way, I am referred here by Elastic Search forums: https://discuss.elastic.co/t/high-disk-watermark-exceeded-on-one-or-more-nodes-rerouting-shards/38506/8
</description><key id="126099798">15919</key><summary>High disk watermark exceeded on one or more nodes, rerouting shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">najamss</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2016-01-12T05:11:50Z</created><updated>2016-09-21T03:59:27Z</updated><resolved>2016-01-17T17:19:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-12T07:17:17Z" id="170821002">Thanks @najamss . I think the question here is what we want to do when a node doesn't report the file disk usage. 
- Do we see it as a requirement? In which case, should we test at node start and refuse to start if the stats are not there? Also, if a node doesn't report usage during operation, do we relocate all shards away as we seem to be doing now (but with better logging)?
- Do we accept this as acceptable hickups ? if so, we should adapt the disk threshold allocator to accept this.

@dakrone thoughts?
</comment><comment author="dakrone" created="2016-01-12T17:31:11Z" id="170983922">&gt; Do we see it as a requirement? In which case, should we test at node start and
&gt; refuse to start if the stats are not there?

I don't think we should view it as a hard requirement, I think it should
probably warn about it and self-disable the AllocationDecider if there are no
disk usages reported, what do you think?

&gt; Also, if a node doesn't report usage during operation, do we relocate all
&gt; shards away as we seem to be doing now (but with better logging)?

Are you talking about if it previously reported the disk statistics but suddenly
stopped?

Regardless, if a node doesn't report usage we should not relocate all shards
away. In that case we should warn heavily that a user is entering unknown
territory and we cannot protect them against disk getting full.

&gt; Do we accept this as acceptable hickups ? if so, we should adapt the disk
&gt; threshold allocator to accept this.

I don't think we should try to plan for a case where disk stats are reported
correctly, and the magically fail to report (I think that should be handled
better by the OS!). It sounds like here that this is an all-or-nothing case, we
either have the fs numbers or we don't at all.

I'm open to suggestions though, what do you think @bleskes?
</comment><comment author="najamss" created="2016-01-13T05:24:52Z" id="171173527">Hello Folks,

Honestly speaking I am not getting either of y'all. I am a beginner to ELK stack and found that maybe it's easy to deploy but very difficult to maintain, it requires a full time employee to manage it which is true for any other commercial product. Solution was running fine before but suddenly it started behaving like this. Now it's throwing weird errors like:

][cluster.action.shard     ] [Aireo] [logstash-2016.01.08][1] received shard failed for [logstash-2016.01.08][1], node[79PYnn4uROy7KOZjakxDrQ], [P], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-01-13T05:23:28.862Z], details[shard failure [failed recovery][IndexShardGatewayRecoveryException[[logstash-2016.01.08][1] failed recovery]; nested: ElasticsearchException[failed to obtain write log pre translog recovery]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@\synology1.abc.xyz.edu\syslog\elasticsearch\nodes\0\indices\logstash-2016.01.08\1\index\write.lock]; ]]], indexUUID [sI1jLxDERgmVtx2aqGhT3w], reason [shard failure [failed recovery][IndexShardGatewayRecoveryException[[logstash-2016.01.08][1] failed recovery]; nested: ElasticsearchException[failed to obtain write log pre translog recovery]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@\synology1.abc.xyz.edu\syslog\elasticsearch\nodes\0\indices\logstash-2016.01.08\1\index\write.lock]; ]]
</comment><comment author="bleskes" created="2016-01-14T17:11:45Z" id="171708347">@najamss your issue is different - something prevents the node from acquiring the write lock (something is using that folder, or the NAS is acting up in more ways than just failing to report stats)  if you didn't solve the issue by now can you open a new issue so we can discuss it there. Please also let us know what version of ES you are using and what happened before.

@dakrone  re

&gt; Regardless, if a node doesn't report usage we should not relocate all shards
&gt; away. In that case we should warn heavily that a user is entering unknown
&gt; territory and we cannot protect them against disk getting full.

Agreed. That is my instinct as well.

&gt;  It sounds like here that this is an all-or-nothing case, we
&gt; either have the fs numbers or we don't at all.

I'm not sure. @najamss reports it used to be OK and suddenly stopped. 

In any case this:

```
[WARN ][cluster.routing.allocation.decider] [Desmond Pitt] high disk watermark [0b] exceeded on [O2-Ef7fET9S_MJNAL-q_yA][Desmond Pitt] free: -1b[100%], shards will be relocated away from this node
```

feels like the wrong response to missing FS stats from a node: `"fs":{"timestamp":1452402078885,"total":{},"data":[{"path":"\abc-synology1.xyz.edu\syslog\elasticsearch\nodes\0"}]}`
</comment><comment author="dakrone" created="2016-01-14T17:57:56Z" id="171722853">&gt; In any case this:

```
[WARN ][cluster.routing.allocation.decider] [Desmond Pitt] high disk watermark [0b] exceeded on [O2-Ef7fET9S_MJNAL-q_yA][Desmond Pitt] free: -1b[100%], shards will be relocated away from this node
```

&gt; feels like the wrong response to missing FS stats from a node:
&gt; "fs":{"timestamp":1452402078885,"total":{},"data":[{"path":"\abc-synology1.xyz.edu\syslog\elasticsearch\nodes\0"}]}

So part of the problem is that `FsInfo.Path` defines these:

```
long total = -1;
long free = -1;
long available = -1;
```

However, because filesystems are crazy, some of these are actually valid values!
I have seen systems with filesystems that report negative amounts for the
free/available values (I don't think I've seen them for the `total`, so that's
something we can check).

I think something we can do is check at least the `total &lt; 0` and just ignore
updating the stats in the event that it's a transient error.
</comment><comment author="bleskes" created="2016-01-17T08:15:42Z" id="172299592">Thanks lee. Does #16001 mean we can close this?
</comment><comment author="dakrone" created="2016-01-17T17:19:05Z" id="172354425">@bleskes yeah I think so, closing this
</comment><comment author="SalahAdDin" created="2016-09-21T03:54:55Z" id="248504856">I have the same problem with the official docker image:

``` bash
elasticsearch_1  | [2016-09-20 05:09:32,905][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:09:32,905][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:10:02,905][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:10:32,905][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:10:32,905][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:11:02,906][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:11:32,906][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:11:32,906][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:12:02,907][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:12:32,907][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:12:32,907][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:13:02,908][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:13:32,908][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:13:32,908][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
web_1            | Performing system checks...
web_1            | 
web_1            | System check identified no issues (0 silenced).
web_1            | September 19, 2016 - 21:39:09
web_1            | Django version 1.10.1, using settings '''
web_1            | Starting development server at http://0.0.0.0:8000/
web_1            | Quit the server with CONTROL-C.
elasticsearch_1  | [2016-09-20 05:14:02,908][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:14:32,909][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:14:32,909][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:15:02,909][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:15:32,910][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:15:32,910][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:16:02,910][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:16:32,911][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:16:32,911][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:17:02,911][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:17:32,912][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:17:32,912][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:18:03,036][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:18:33,037][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:18:33,037][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:19:03,037][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:19:33,038][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:19:33,038][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:20:03,038][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:20:33,038][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:20:33,038][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:21:03,039][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:21:33,039][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:21:33,039][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:22:03,040][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:22:33,040][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:22:33,040][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:23:03,040][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:23:33,041][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:23:33,041][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:24:03,041][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:24:33,042][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:24:33,042][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:25:03,042][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:25:33,043][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:25:33,043][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:26:03,043][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:26:33,043][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:26:33,043][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:27:03,044][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:27:33,044][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:27:33,044][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:28:03,045][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:28:33,045][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:28:33,045][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:29:03,046][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:29:33,046][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:29:33,046][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:30:03,046][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:30:33,047][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:30:33,047][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:31:03,047][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:31:33,048][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:31:33,048][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:32:03,048][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:32:33,049][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:32:33,049][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:33:03,049][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:33:33,049][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:33:33,049][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:34:03,050][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:34:33,050][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:34:33,050][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:35:03,051][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:35:33,051][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:35:33,051][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:36:03,052][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:36:33,052][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:36:33,052][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:37:03,053][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:37:33,053][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:37:33,053][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:38:03,053][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:38:33,054][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:38:33,054][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:39:03,054][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:39:33,055][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:39:33,055][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:40:03,055][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:40:33,056][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:40:33,056][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:41:03,056][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:41:33,056][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:41:33,056][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:42:03,057][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:42:33,057][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:42:33,057][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:43:03,058][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:43:33,058][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:43:33,058][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:44:03,058][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:44:33,059][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:44:33,059][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:45:03,059][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:45:33,060][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:45:33,060][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:46:03,060][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:46:33,061][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:46:33,061][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:47:03,061][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:47:33,062][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:47:33,062][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:48:03,062][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:48:33,063][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:48:33,063][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:49:03,063][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:49:33,063][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:49:33,063][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:50:03,064][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:50:33,064][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:50:33,064][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:51:03,065][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:51:33,065][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:51:33,065][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:52:03,065][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:52:33,066][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:52:33,066][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:53:03,066][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:53:33,067][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:53:33,067][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:54:03,067][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:54:33,067][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:54:33,068][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:55:03,068][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:55:33,068][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:55:33,068][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:56:03,069][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:56:33,069][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:56:33,069][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:57:03,070][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:57:33,070][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:57:33,070][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:58:03,070][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:58:33,071][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:58:33,071][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 05:59:03,071][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:59:33,072][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 05:59:33,072][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:00:03,072][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:00:33,072][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:00:33,072][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:01:03,073][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:01:33,073][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:01:33,073][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:02:03,074][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:02:33,074][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:02:33,074][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:03:03,075][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:03:33,075][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:03:33,075][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:04:03,076][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:04:33,076][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:04:33,076][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:05:03,076][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:05:33,077][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:05:33,077][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:06:03,077][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:06:33,078][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:06:33,078][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:07:03,078][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:07:33,078][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:07:33,078][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:08:03,079][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:08:33,079][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:08:33,079][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:09:03,080][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:09:33,080][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:09:33,080][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:10:03,080][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:10:33,081][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:10:33,081][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:11:03,081][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:11:33,082][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:11:33,082][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:12:03,082][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:12:33,082][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:12:33,083][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:13:03,083][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:13:33,083][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:13:33,083][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:14:03,084][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:14:33,084][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:14:33,084][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:15:03,085][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:15:33,085][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:15:33,085][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:16:03,085][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:16:33,086][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:16:33,086][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:17:03,086][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:17:33,087][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:17:33,087][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:18:03,087][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:18:33,088][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:18:33,088][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:19:03,088][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:19:33,088][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:19:33,088][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:20:03,089][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:20:33,089][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:20:33,089][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:21:03,090][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:21:33,090][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:21:33,090][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:22:03,090][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:22:33,091][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:22:33,091][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:23:03,091][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:23:33,092][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:23:33,092][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:24:03,092][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:24:33,092][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:24:33,092][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:25:03,093][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:25:33,093][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:25:33,093][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:26:03,094][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:26:33,094][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:26:33,094][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:27:03,094][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:27:33,095][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:27:33,095][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:28:03,095][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:28:33,096][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:28:33,096][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:29:03,096][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:29:33,096][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:29:33,097][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:30:03,097][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:30:33,097][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:30:33,097][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:31:03,098][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:31:33,098][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:31:33,098][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:32:03,099][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:32:33,099][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:32:33,099][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:33:03,100][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:33:33,100][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:33:33,100][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:34:03,101][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:34:33,101][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:34:33,101][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:35:03,102][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:35:33,102][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:35:33,102][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:36:03,102][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:36:33,103][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:36:33,103][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:37:03,103][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:37:33,104][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:37:33,104][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:38:03,104][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:38:33,104][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:38:33,105][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:39:03,105][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:39:33,105][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:39:33,105][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:40:03,106][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:40:33,106][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:40:33,106][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:41:03,137][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:41:33,137][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:41:33,137][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:42:03,138][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:42:33,138][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:42:33,138][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:43:03,139][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:43:33,139][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:43:33,139][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:44:03,139][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:44:33,244][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:44:33,244][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:45:03,245][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:45:33,245][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:45:33,245][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:46:03,246][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:46:33,246][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:46:33,246][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:47:03,247][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:47:33,247][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:47:33,247][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:48:03,247][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:48:33,248][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:48:33,248][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:49:03,248][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:49:33,249][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:49:33,249][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:50:03,249][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:50:33,250][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:50:33,250][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:51:03,251][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:51:33,251][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:51:33,251][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:52:03,252][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:52:33,252][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:52:33,252][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:53:03,253][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:53:33,253][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:53:33,253][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:54:03,253][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:54:33,254][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:54:33,254][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:55:03,254][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:55:33,255][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:55:33,255][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:56:03,255][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:56:33,255][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:56:33,255][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:57:03,256][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:57:33,256][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:57:33,256][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:58:03,257][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:58:33,257][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:58:33,257][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 06:59:03,258][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:59:33,258][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 06:59:33,258][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 07:00:03,258][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 07:00:33,259][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 07:00:33,259][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 07:01:03,259][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 07:01:33,260][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 07:01:33,260][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 07:02:03,260][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 07:02:33,260][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 07:02:33,260][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 22:30:09,169][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 22:30:39,170][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
elasticsearch_1  | [2016-09-20 22:30:39,170][INFO ][cluster.routing.allocation.decider] [Carlos Lobo] rerouting shards: [high disk watermark exceeded on one or more nodes]
elasticsearch_1  | [2016-09-20 22:31:09,170][WARN ][cluster.routing.allocation.decider] [Carlos Lobo] high disk watermark [90%] exceeded on [v1-xs1R8TbaUh3RKm1EznQ][Carlos Lobo][/usr/share/elasticsearch/data/elasticsearch/nodes/0] free: 4.7gb[5%], shards will be relocated away from this node
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AccessControlException in JarHell#checkJarHell when running embedded for testing purposes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15918</link><project id="" key="" /><description>Our tests are run with a security manager enabled.

Another developer (who apparently disabled the security manager instead of reporting this) added a test which tries to start an elasticsearch node, which then tries to load plugins. Turns out that this calls `JarHell#checkJarHell`, which then pokes around the classpath in some fashion. The resulting `AccessControlException` is then presumably caught, but the code appears to then assume that there is "jar hell", rather than assuming that there isn't, which could possibly be the source of the issue.

This is occurring in v2.0.1.

Stack trace:

```
java.lang.IllegalStateException: failed to load bundle [] due to jar hell
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:342)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:113)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:144)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)
    at ElasticSearchTestUtils.createNode(ElasticSearchTestUtils.java:62)
    at ElasticSearchTestUtils$1.&lt;init&gt;(ElasticSearchTestUtils.java:75)
    at ElasticSearchTestUtils.createClient(ElasticSearchTestUtils.java:73)
    at TestElasticSearchTextAnalysisService.setUp(TestElasticSearchTextAnalysisService.java:337)
    (omitting JUnit frames)
Caused by: java.security.AccessControlException: access denied ("java.io.FilePermission" "/Applications/IntelliJ IDEA 15 CE.app/Contents/lib/idea_rt.jar" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:211)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:150)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:173)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:340)
    ... 18 more
```
</description><key id="126099048">15918</key><summary>AccessControlException in JarHell#checkJarHell when running embedded for testing purposes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trejkaz</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2016-01-12T05:07:27Z</created><updated>2016-02-14T12:20:58Z</updated><resolved>2016-02-03T08:53:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T13:18:47Z" id="170908935">@rjernst could you take a look at this?
</comment><comment author="trejkaz" created="2016-02-03T00:12:07Z" id="178901772">I'm guessing that's a no?
</comment><comment author="s1monw" created="2016-02-03T08:53:31Z" id="179099471">I think you have to fix your classpath to not include this jar file otherwise you won't get passed the security manager.
</comment><comment author="trejkaz" created="2016-02-03T08:59:22Z" id="179104043">I'm not sure it's possible, but I guess I could file a bug with IDEA to see if they will take their runtime off there, but I'm not confident that they will cooperate.

Surely from the elasticsearch side there is a way to implement that check without using JarFile directly, but I couldn't find a way when I did a quick check (JarURLConnection#getJarFile throws the same exception.)

At the very least, though, I think elasticsearch should catch SecurityException when this happens and allow it to proceed.
</comment><comment author="s1monw" created="2016-02-03T11:50:10Z" id="179182834">I run in idea as well and it's totally possible
</comment><comment author="trejkaz" created="2016-02-04T00:43:08Z" id="179553242">I'm not finding the option... I have dug pretty deep.
</comment><comment author="jasontedor" created="2016-02-04T01:19:23Z" id="179561723">&gt; I'm not finding the option... I have dug pretty deep.

Start by removing everything but the bare minimum from the SDKs (File -&gt; Project Structure -&gt; Platform Settings -&gt; SDKs -&gt; Classpath). There is no reason for an IntelliJ jar to be on the classpath. Take a look at the PR #13465 that removed the leniency in this situation and specifically a [minimal set of jars](https://github.com/elastic/elasticsearch/pull/13465#issuecomment-142124441) provided there.
</comment><comment author="trejkaz" created="2016-02-04T02:32:25Z" id="179583890">At the moment, the jars for the SDK are:

```
jdk1.8.0_51.jdk/Contents/Home/jre/lib/charsets.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/jce.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/jfr.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/jsse.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/management-agent.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/resources.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/rt.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/cldrdata.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/dnsns.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/localedata.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/morelocales.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/nashorn.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/resourcebundlefix.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/sunec.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar
jdk1.8.0_51.jdk/Contents/Home/jre/lib/ext/zipfs.jar
```

Which is bigger than their list, but most of what we have extra is actually in use. (I'm not sure about management-agent. We use management heavily, so I'm scared to remove it, but maybe it's a particular thing we don't specifically use.)

So idea_rt.jar was never explicitly on the classpath. But when I run from IDEA, it magically gets added on the front of the classpath anyway.

Supposedly, adding idea.no.launcher=true to idea.properties stops it doing this, but I'm finding that it doesn't work. I still get it on the front of the classpath.

That aside, what if our security policy said you weren't allowed to read from rt.jar? Surely you would be handling that situation incorrectly as well, so I still assert that the correct handling for a SecurityException is that you are supposed to catch it and proceed in a sensible fashion. The current behaviour is not really acceptable.
</comment><comment author="jasontedor" created="2016-02-04T02:36:02Z" id="179584779">Nothing will work if you try to forbid rt.jar, it is _the_ Java runtime.
</comment><comment author="trejkaz" created="2016-02-04T02:45:42Z" id="179587985">False.

Only dodgy code which tries to scrape the classpath for jar files and then open each one directly would fail in such a scenario. Well-behaved code which looks up resources the proper way from the classpath still works. Elasticsearch's `JarHell` class is far from well-behaved code, which is why it is causing problems here.

Learn how Java does things, and then fix your shit.
</comment><comment author="trejkaz" created="2016-02-04T03:32:00Z" id="179599005">Demonstration, for anyone interested in learning how things actually work.

`build.sh`:

```
#!/bin/bash

COMPILE_CLASSPATH=elasticsearch-2.2.0.jar
RUNTIME_CLASSPATH=${COMPILE_CLASSPATH}:lucene-core-5.4.1.jar

javac -classpath ${COMPILE_CLASSPATH} WellBehavedDemo.java

# Deliberately uses == to be extra restrictive.
java -classpath .:${RUNTIME_CLASSPATH} \
 -Djava.security.manager \
 -Djava.security.policy==my.policy \
 WellBehavedDemo
```

`WellBehavedDemo.java`:

```
import org.elasticsearch.bootstrap.JarHell;

class WellBehavedDemo {
    public static void main(String[] args) throws Exception {
        ClassLoader classLoader = WellBehavedDemo.class.getClassLoader();

        System.out.print("Loading classes from JRE...");
        classLoader.loadClass("java.lang.String");
        System.out.println(" works!");

        System.out.print("Reading resources from JRE...");
        classLoader.getResources("/java/lang/String.class");
        System.out.println(" works!");

        System.out.print("Loading classes from extra jars...");
        classLoader.loadClass("org.apache.lucene.util.Version");
        System.out.println(" works!");

        System.out.print("Reading resources from extra jars...");
        classLoader.getResources("/META-INF/services/org.apache.lucene.codecs.Codec");
        System.out.println(" works!");

        System.out.print("Running JarHell... ");
        JarHell.checkJarHell();
        System.out.println(" works!");
    }
}
```

Notice that this loads both classes and resources, and uses getResources() for this, which can return multiple. This can be used for classfiles too, the only catch being that JarHell wants a way to list the directories. I'm not sure there is a good way to do that without breaking rules.

I was hoping to demonstrate this with a minimal (1-line) policy file, but it turns out Elasticsearch also behaves badly when you reject its access to system properties as well, and additionally is using system properties for things which it should be using public APIs for instead. Please consider all these as appended to the bug report and feel free to come up with worthless excuses for why you won't fix those bugs either.

`my.policy`:

```
grant {

  // Elasticsearch doesn't cope well with denials to read system properties either, so we'll just humour
  // it to get it through to the specific failure I reported. It turns out many libraries have this particular
  // issue, so we already granted it to everything.

  // So here's a summary of ES's transgressions up to the issue I actually reported.

  // via org.elasticsearch.common.logging.Loggers.&lt;clinit&gt;
  // Should have caught the SecurityException and treated it as if the property wasn't set.
  permission java.util.PropertyPermission "es.logger.prefix", "read";

  // via org.elasticsearch.bootstrap.JarHell.parseClassPath
  // Should have looked at the ClassLoader instead, because java.class.path sometimes only contains the classpath for the container.
  permission java.util.PropertyPermission "java.class.path", "read";

  // via org.elasticsearch.bootstrap.JarHell.parseClassPath
  // Should have used File.pathSeparator instead.
  permission java.util.PropertyPermission "path.separator", "read";

  // via org.elasticsearch.bootstrap.JarHell.parseClassPath
  // Should have used File.separator instead.
  permission java.util.PropertyPermission "file.separator", "read";

  // via org.elasticsearch.bootstrap.JarHell.checkJarHell
  // I'm not sure what they're smoking here. It should not be necessary to read the location of Java!
  permission java.util.PropertyPermission "java.home", "read";

  // via org.elasticsearch.bootstrap.JavaVersion.&lt;clinit&gt;
  // Should have caught the SecurityException and treated it the same as the property being missing.
  permission java.util.PropertyPermission "java.specification.version", "read";

  // via org.elasticsearch.bootstrap.JarHell.checkJarHell
  // Current directory is probably fine, as long as it's only reading.
  // In real situations we use absolute paths anyway, which removes this issue.
  // Could possibly just modify the shell script to get around it, actually.
  permission java.util.PropertyPermission "user.dir", "read";
  permission java.io.FilePermission "${user.dir}", "read";

  // via org.elasticsearch.bootstrap.JarHell.checkJarHell
  // This next one is exactly the idea_rt.jar issue.
  // It just happens to be the first thing in the classpath that isn't Elasticsearch itself, whereas when running from IDEA,
  // idea_rt.jar is always first.
  //permission java.io.FilePermission "${user.dir}/lucene-core-5.4.1.jar", "read";
};
```

Output:

```
bucket:security_demo tester$ ./well_behaved_demo.sh 
Loading classes from JRE... works!
Reading resources from JRE... works!
Loading classes from extra jars... works!
Reading resources from extra jars... works!
Running JarHell... Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/Users/tester/test/security_demo/lucene-core-5.4.1.jar" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:211)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:150)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:174)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:87)
    at WellBehavedDemo.main(WellBehavedDemo.java:24)
```

Notice how the way `JarHell` is doing things is rejected by the security policy, even though accessing the same things using the proper APIs works fine.
</comment><comment author="clintongormley" created="2016-02-13T23:32:04Z" id="183771169">@trejkaz Instead of just insulting us, why not send a PR instead?
</comment><comment author="trejkaz" created="2016-02-14T08:47:13Z" id="183853213">The pull request I would have submitted (removing the JarHell checker in its entirety) presumably wouldn't be accepted anyway.
</comment><comment author="clintongormley" created="2016-02-14T12:20:58Z" id="183881321">Clearly not.  But you said you had ways to improve it, so submit that instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add loadaverage support for freebsd</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15917</link><project id="" key="" /><description>Should be very easy to add based on https://github.com/elastic/elasticsearch/pull/15907, just that we want to read from linprocfs if its available (`/compat/linux/proc/loadavg`)
</description><key id="126096958">15917</key><summary>add loadaverage support for freebsd</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Stats</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-12T04:46:04Z</created><updated>2016-01-14T01:11:22Z</updated><resolved>2016-01-14T01:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-12T17:18:39Z" id="170980410">+1!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>master node can not rejoin cluster after restart on es2.0.0. but other nodes can rejoin cluster after restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15916</link><project id="" key="" /><description>i have a es cluster with ten nodes  five  dedicated master[A,B,C,D,E] and five datanode,when i retsart
the master A ,other dedicated master elect another master B,but when the A start,it cannot find master B.but B can find A,and put A into the cluster.
# the error like this

&gt; [2016-01-12 11:39:45,812][DEBUG][action.admin.cluster.state] [efe-es-03] no known master node, scheduling a retry
&gt; [2016-01-12 11:39:45,891][DEBUG][action.admin.cluster.state] [efe-es-03] no known master node, scheduling a retry
&gt; [2016-01-12 11:39:46,490][INFO ][rest.suppressed          ] /.kibana/config/_search Params: {index=.kibana, type=config}
&gt; ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];]
&gt;         at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:154)
&gt;         at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:144)
# the new master add A into cluster

&gt; [2016-01-12 10:57:49,394][INFO ][cluster.service          ] [efe-es-03] added {{efe-es-01}{9H7bnUbASVerOMdEOlWikg}{10.39.128.30}{10.39.128.30:9300}{rack=10.39.128.30, max_local_storage_nodes=1, master=true},}, reason: zen-disco-join(join from node[{efe-es-01}{9H7bnUbASVerOMdEOlWikg}{10.39.128.30}{10.39.128.30:9300}{rack=10.39.128.30, max_local_storage_nodes=1, master=true}])
# but A can't get nodes info others can get nodes info including A

&gt; curl localhost:8080/_cat/nodes
</description><key id="126094352">15916</key><summary>master node can not rejoin cluster after restart on es2.0.0. but other nodes can rejoin cluster after restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wisre</reporter><labels /><created>2016-01-12T04:18:24Z</created><updated>2016-01-12T06:44:16Z</updated><resolved>2016-01-12T06:44:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wisre" created="2016-01-12T04:20:05Z" id="170785372"># my discover config like this

&gt;  discovery.zen.ping.unicast.hosts: ["A", "B","C","D","E"]
&gt;  discovery.zen.minimum_master_nodes: 3
&gt;  discovery.zen.ping.multicast.enabled: false
</comment><comment author="dadoonet" created="2016-01-12T06:44:16Z" id="170814749">Please join us on discuss.elastic.co. We can help there.

Also provide your full logs there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add I/O statistics on Linux</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15915</link><project id="" key="" /><description>This commit adds I/O statistics on Linux. These statistics are read from
/proc/self/io and include at a total level for the process the number of
characters (bytes) read and written (could be satisfied by the page
cache), number of read and write system, and number of bytes read and
written (that hit the storage layer).

Closes #15296
</description><key id="126092423">15915</key><summary>Add I/O statistics on Linux</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-01-12T04:01:53Z</created><updated>2016-11-07T17:35:38Z</updated><resolved>2016-05-17T20:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-12T04:06:51Z" id="170780254">On my Fedora system this gives statistics that look like

``` json
.
.
.
"nodes" : { 
  "LslDO6WMRS2chP4EWlCY1w" : {
    .
    .
    .
    "fs" : {
      .
      .
      .
      "io_stats" : {
        "read_bytes" : 36864,
        "write_bytes" : 28913664
      }
    }
  }
}
```
</comment><comment author="rmuir" created="2016-01-12T04:48:58Z" id="170790118">Do we really need the character device and syscalls stats, or can we just limit it to the `_bytes` statistics which are for the storage layer, which I think people are most interested in, and its more minimal and more likely to be something we could support elsewhere if need be?
</comment><comment author="jasontedor" created="2016-01-12T04:52:33Z" id="170791167">&gt; Do we really need the character device and syscalls stats, or can we just limit it to the _bytes statistics which are for the storage layer, which I think people are most interested in, and its more minimal and more likely to be something we could support elsewhere if need be?

@rmuir I added the system calls because people were looking for the number of disk ops and I think that the closest we can get (on the process level) is with the number of sys calls? I'm fine dropping the character device stats though.
</comment><comment author="rmuir" created="2016-01-12T05:01:51Z" id="170792016">I don't think thats the meaning. As i understand it rchar/wchar/syscr/syscw are the "high level" counters, e.g. each read() operation increments those, regardless of any caching. and the number read goes against the counters. And kernel documentation mentions that they include things like tty io, so who knows what other things they include (sockets, etc, not sure).

On the other hand read_bytes/write_bytes is the number that were actually turned into i/o (not cached) caused by this process. It happens at a lower level, and I think that is what people are most interested in, and "most accurate" for us.
</comment><comment author="rmuir" created="2016-01-12T05:15:59Z" id="170797498">&gt; @rmuir I added the system calls because people were looking for the number of disk ops and I think that the closest we can get (on the process level) is with the number of sys calls?

I really think its misleading. showing cached reads is gonna be super confusing, because its not at all the number of io operations. You just don't have that per-process, thats why sigar was picking it up from /proc/diskstats (https://www.kernel.org/doc/Documentation/iostats.txt)

I don't think we should try to make something up, just because people want it. If we want to provide the numbers that sigar was giving before, then we should just use /proc/diskstats. 
</comment><comment author="jasontedor" created="2016-01-12T13:21:37Z" id="170909591">&gt; I really think its misleading.

@rmuir Agree. While I did try to document the meaning in my commit, I agree with you that it's best to just not include these numbers to not sow confusion.

&gt; If we want to provide the numbers that sigar was giving before, then we should just use /proc/diskstats.

Those stats are since system boot and not per-process, and so I think that we should not use them. I've pared this down to just physical read and write bytes in d2df60c6801fcf0636be7c4a498dc525a36a125b.
</comment><comment author="jasontedor" created="2016-01-15T20:55:34Z" id="172090264">@rmuir Any further thoughts on this?
</comment><comment author="rmuir" created="2016-01-17T19:29:31Z" id="172369280">My concerns are only around the semantics: will this stat be useful? or will it only get deprecated later because its not quite what people want.

This change gives per-process read/write bytes.

if instead, we use /proc/diskstats, then its similar to what sigar provided, and can give number of reads/writes but also can be treated as IOPS if needed, because each device has a size for that, etc, etc.

but if we do it per-process, this is all we got. 

I get the impression from this issue that people are looking for "per-process IOPS" but lets stop wasting our time with impossible fantasies, so the question is, will this be useful?
</comment><comment author="dakrone" created="2016-04-06T17:21:39Z" id="206473501">ping @jasontedor, what's the word on this?
</comment><comment author="jasontedor" created="2016-04-06T17:22:57Z" id="206473959">&gt; ping @jasontedor, what's the word on this?

@dakrone It's on my radar to focus on again soon with the intention of hitting 5.0.
</comment><comment author="jasontedor" created="2016-04-28T17:21:07Z" id="215500569">@rmuir I've updated the PR to include IOPS, read/write throughput, queue length (really resident length) and await times. Do you mind taking a look?
</comment><comment author="jasontedor" created="2016-04-28T17:55:31Z" id="215510150">These stats now appear like so:

``` json
.
.
.
"nodes" : { 
  "mpxCcBekQgajWfIX5QEfBQ" : {
    .
    .
    .
    "fs" : {
      .
      .
      .
      "io_stats" : {             
        "devices" : [ {
          "device_name" : "dm-2",
          "iops" : 1261.3776,
          "reads_per_second" : 0.0,
          "writes_per_second" : 1261.3776,
          "read_kilobytes_per_second" : 0.0,
          "write_kilobytes_per_second" : 4333.8613,
          "average_request_size_in_kilobytes" : 3.435816,
          "average_resident_requests" : 1.645188,
          "average_await_time_in_milliseconds" : 1.2995245,
          "average_read_await_time_in_milliseconds" : 0.0,
          "average_write_await_time_in_milliseconds" : 1.2995245
        } ]
      }
    }
  }
}
```
</comment><comment author="jasontedor" created="2016-05-11T21:32:03Z" id="218596617">@rmuir I pushed d83a95b81bcbed787b0a8cedd3647831dd6c1a89. Do you mind reviewing again?
</comment><comment author="kimchy" created="2016-05-12T16:30:11Z" id="218811988">@jasontedor it looks great! on suggestion, but this is subjective, I would opt to just report the disk stats as is, and not zero them from process start, this will ease debugging the feature (same as /proc ...) if it doesn't work, and anyhow we would want to display the rate of change, so zeroing is less needed and keeps it simpler
</comment><comment author="rtoma" created="2016-09-01T11:03:25Z" id="244046559">Team, isn't it strange to have a single ES JVM present stats sourced from the OS?

These OS stats are influenced by all running proceses, not just that ES JVM. Imagine running multiple ES JVM's: their disk.io_stats stats will be similar but their actual I/O load is not.same.

In ES 2.x the fs.data.\* metrics were dropped and I miss them. As they showed the I/O activity of a single JVM.

Or am I misreading the sourcecode? If you want I can make a new issue for this.
</comment><comment author="jasontedor" created="2016-11-07T17:35:38Z" id="258904645">&gt; As they showed the I/O activity of a single JVM.

This isn't correct, the stats were OS-level then too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Smarter request routing which takes recent node latency into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15914</link><project id="" key="" /><description>Hi! I was wondering if there was any ability to blacklist a node while it is recovering. Trying to route GETs, indexing and search to a recovering index can cause the node to fall out of the cluster or can cause the cluster to pause altogether. Is there a way to blacklist a node using the TransportClient that way no traffic is routed to a particular node (or anything similar)? Thanks!
</description><key id="126087774">15914</key><summary>Smarter request routing which takes recent node latency into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nirajpatel</reporter><labels><label>:Search</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2016-01-12T03:13:08Z</created><updated>2017-03-31T09:29:00Z</updated><resolved>2017-03-31T09:29:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T12:57:18Z" id="170903784">Hi @nirajpatel 

it's not possible, and I've been trying to think if it makes sense to add. It has complications (eg what happens if all nodes are recovering from each other?).  Really, it shouldn't be necessary.  Recovery should be throttled so there are sufficient resources on the box to handle search requests etc as well.  (This also means providing enough hardware so that your nodes are not at breaking point already, and ensuring that there aren't abusive search requests - something we're trying help with in https://github.com/elastic/elasticsearch/issues/11511)
</comment><comment author="nirajpatel" created="2016-01-12T14:18:42Z" id="170924663">Thank you so much for responding @clintongormley! That certainly makes sense. I was thinking more on the transport client side while the cluster is recovering we could at least throttle requests. 

**Recovery should be throttled so there are sufficient resources on the box to handle search requests etc as well.**
Recovery being throttled would mean higher search, GET, indexing times for an extended period of time correct? Wouldn't you almost want to favor recovering quickly over servicing incoming requests normally? I guess it's a personal tradeoff but if the option existed in the transport client the user could make that decision. What are your thoughts? Thank you!
</comment><comment author="clintongormley" created="2016-01-15T11:23:54Z" id="171937466">We discussed this in FixItFriday - blacklisting a node is too black or white.  Instead a better option would be to have a smarter request routing mechanism than the current round-robin that takes recent node latency into account.
</comment><comment author="colings86" created="2017-03-31T09:29:00Z" id="290665024">This is the same idea as https://github.com/elastic/elasticsearch/issues/3890 so will close in favour of it.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate span_near's collect_payloads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15913</link><project id="" key="" /><description>Its a noop in Lucene.
</description><key id="126087702">15913</key><summary>Deprecate span_near's collect_payloads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Query DSL</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T03:12:48Z</created><updated>2016-03-10T18:55:13Z</updated><resolved>2016-01-12T14:56:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-12T03:13:51Z" id="170770947">@jpountz can you have a look at this? Its not clear to me _exactly_ how you'd load payloads now but this way isn't it.
</comment><comment author="jpountz" created="2016-01-12T14:01:15Z" id="170920076">You would load payloads by passing Postings.PAYLOADS to SpanWeight.getSpans().
</comment><comment author="jpountz" created="2016-01-12T14:04:59Z" id="170921125">Just left a minor comment, otherwise LGTM
</comment><comment author="nik9000" created="2016-01-12T14:24:42Z" id="170926121">&gt; You would load payloads by passing Postings.PAYLOADS to SpanWeight.getSpans().

Got it. I'll make the change you recommend and then merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs explanation for unassigned shard reason codes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15912</link><project id="" key="" /><description>Added the description for the unassigned shard reason codes.
Plus the formatting fix suggested by @clintongormley.

Closes #14001
</description><key id="126084220">15912</key><summary>Docs explanation for unassigned shard reason codes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fforbeck</reporter><labels><label>docs</label></labels><created>2016-01-12T02:39:45Z</created><updated>2016-01-12T12:28:03Z</updated><resolved>2016-01-12T12:28:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fforbeck" created="2016-01-12T02:43:08Z" id="170765383">Hi @clintongormley
I created another PR with the formatting changes you requested.

Please, take a look if possible.

Thank you! 
</comment><comment author="clintongormley" created="2016-01-12T12:28:03Z" id="170896253">thanks @fforbeck - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop using deprecated constructors for queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15911</link><project id="" key="" /><description>DisjunctionMaxQuery and BooleanQuery
</description><key id="126084208">15911</key><summary>Stop using deprecated constructors for queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-12T02:39:38Z</created><updated>2016-01-12T14:47:33Z</updated><resolved>2016-01-12T14:47:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-12T14:05:56Z" id="170921404">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] fix NPE when Grok matches expression, but no captures defined.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15910</link><project id="" key="" /><description>As was found here: https://github.com/elastic/elasticsearch/issues/15901#issuecomment-170666936.

Grok would throw a NPE when the expression matched, but no captures are found. In this case, Grok should do nothing to the document, but it should also succeed.
</description><key id="126080414">15910</key><summary>[Ingest] fix NPE when Grok matches expression, but no captures defined.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-01-12T02:04:46Z</created><updated>2016-01-12T20:03:00Z</updated><resolved>2016-01-12T20:02:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-12T08:48:30Z" id="170840955">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail hot_threads in a better way if unsupported by JDK</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15909</link><project id="" key="" /><description>Currently if thread cpu time is not supported (for instance, on
operating systems such as FreeBSD), an `IllegalStateException` is thrown
in `HotThreads#innerDetect()` that causes the API to return a useless
response.

This changes the check to be earlier, substituting a message for the
hot_threads output (in case some nodes _do_ support it).

Additionally, if an exception is thrown during the hot_threads
generation it is now logged and the best effort output is returned.
</description><key id="126067197">15909</key><summary>Fail hot_threads in a better way if unsupported by JDK</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-01-12T00:05:48Z</created><updated>2016-04-06T10:38:42Z</updated><resolved>2016-04-04T22:57:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-12T00:14:08Z" id="170739619">changes looks good to me. see my comment though, if we want to clean this up then maybe change the check to 'if (threadBean.isThreadCpuTimeEnabled() == false` and remove all the save/restore stuff. If its not enabled by default then there is something to worry about, and this code is all dead today.
</comment><comment author="dakrone" created="2016-01-12T02:53:15Z" id="170767328">Pushed more commits to this that remove the save restore and only checks if enabled.
</comment><comment author="dakrone" created="2016-01-15T20:34:09Z" id="172085753">Pushed another commit for this that returns just the string, @rmuir how do you feel about putting the exception in the hot_threads (as I did), do you think it should elide the output as it was previously doing?
</comment><comment author="rmuir" created="2016-01-15T20:47:24Z" id="172088515">honestly my concern is that we have explicit code to check for the case where this isn't enabled by the operating system (where just the string is returned), so some other exception must be a real problem (e.g. bug in our logic, security misconfiguration, whatever). Like its the unexpected case, and it means something is wrong.

I dont have a strong opinion on it, i just feel like a hard failure is exactly what you want here vs just another string?
</comment><comment author="rmuir" created="2016-01-15T20:51:23Z" id="172089347">practically i guess im just worried about test coverage like TestHotThreadsDontFail. This thing is tricky to test, if that is all we are testing and we make all the logic lenient, then how good are our tests?
</comment><comment author="dakrone" created="2016-01-18T22:43:20Z" id="172673638">&gt; i just feel like a hard failure is exactly what you want here vs just another string?

I dunno, in this case, "hard failure" is just returning nothing for that particular node, so I was thinking showing an explicit failure to the user would be better than that. If you think it'd be better to return an empty response though, I can remove that part.
</comment><comment author="rmuir" created="2016-01-18T22:59:55Z" id="172676175">&gt; I dunno, in this case, "hard failure" is just returning nothing for that particular node, so I was thinking showing an explicit failure to the user would be better than that. If you think it'd be better to return an empty response though, I can remove that part.

Why wouldnt we return 500?
</comment><comment author="rjernst" created="2016-01-18T22:59:59Z" id="172676187">&gt; I dunno, in this case, "hard failure" is just returning nothing for that particular node

Why wouldn't you fail the outter request if a node fails? If just one node in your cluster doesn't support hotthreads, but the rest do, this seems like a serious problem a user should be aware of? It would mean they are running completely different _vendors_ jvms on different nodes?

I also share the concern over testing. We really need to test this error case better.
</comment><comment author="dakrone" created="2016-01-18T23:05:11Z" id="172677055">&gt; Why wouldnt we return 500?
&gt; Why wouldn't you fail the outter request if a node fails? If just one node in your cluster doesn't support hotthreads, but the rest do, this seems like a serious problem a user should be aware of?

We don't recommend it, but why disallow it for customers running mixed-OS clusters? ie, what if I run Windows master nodes but FreeBSD data nodes? If you feel strongly enough, I guess I can change it to fail the request utterly if a single node happens not to have `isThreadCpuTimeSupported` true.

&gt; I also share the concern over testing. We really need to test this error case better.

Sure, I agree with this, I don't want to change the test until we determine what the behavior should be though.
</comment><comment author="dakrone" created="2016-02-21T19:56:54Z" id="186898110">@rjernst @rmuir any thoughts on my latest response? I'm okay either way (fail hard if any nodes are unsupported, or return the failure as a string in the response in case _some_ nodes are supported) I just want to get which one I should implement.
</comment><comment author="rjernst" created="2016-02-21T22:55:42Z" id="186938791">I would go with failing hard if any nodes do not support it.
</comment><comment author="dakrone" created="2016-04-04T22:01:50Z" id="205516237">@rjernst changed this to just fail hard if thread cpu time is not supported. It looks like a big change, but it's actually just removing code that tries to set the cpu time to enabled and reset it afterwards. As @rmuir said, if it's not enabled by default something is wrong and we should fail it hard. Other than that, I didn't actually change the hot threads implementation code.
</comment><comment author="rjernst" created="2016-04-04T22:37:29Z" id="205524235">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mpercolate does not function properly in elasticsearch 2.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15908</link><project id="" key="" /><description>The mpercolate call appears to be broken in 2.1.1 in clusters with more than one node. The following sequence works perfectly on my laptop but fails in two separate three node clusters (in "green" state). Sometimes the call succeeds on one of the three machines but not on the others. I could not find any errors in the logs.
PUT http://localhost:9200/test
[mapping.txt](https://github.com/elastic/elasticsearch/files/86558/mapping.txt)
POST http://localhost:9200/test/.percolator/Certificate_Expiration_Warning_Medium
[percolator.txt](https://github.com/elastic/elasticsearch/files/86560/percolator.txt)
POST http://localhost:9200/test/_mpercolate
[mpercolate.txt](https://github.com/elastic/elasticsearch/files/86562/mpercolate.txt)
Expected Output:
[output.txt](https://github.com/elastic/elasticsearch/files/86566/output.txt)
Actual Output:
[actual.txt](https://github.com/elastic/elasticsearch/files/86577/actual.txt)
</description><key id="126065732">15908</key><summary>mpercolate does not function properly in elasticsearch 2.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rossbrower</reporter><labels><label>:Percolator</label><label>bug</label></labels><created>2016-01-11T23:54:29Z</created><updated>2016-01-15T08:25:56Z</updated><resolved>2016-01-15T08:25:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T11:53:29Z" id="170888637">Simple recreation: 

Start two nodes, run this:

```
PUT test
{  
   "settings":{  
      "number_of_shards":1,
      "number_of_replicas":1
   },
   "mappings":{  
      "cert_localmachine_my":{  
         "properties":{  
            "cert_localmachine_my":{  
               "properties":{  
                  "NotAfter":{  
                     "type":"date",
                     "format":"strict_date_optional_time||epoch_millis"
                  }
               }
            }
         }
      }
   }
}
POST /test/.percolator/Certificate_Expiration_Warning_Medium
{  
   "query":{  
      "bool":{  
         "filter":{  
            "range":{  
               "cert_localmachine_my.NotAfter":{  
                  "lt":"now+90d"
               }
            }
         }
      }
   }
}


POST test/_mpercolate
{"percolate":{"type":"cert_localmachine_my"}}
{"doc":{"cert_localmachine_my":{"NotAfter":"2015-07-21T10:28:01-07:00"}}}
```

Then restart one node and run the mpercolate command a few times.  Sometimes it matches sometimes it doesn't.
</comment><comment author="martijnvg" created="2016-01-12T17:43:42Z" id="170987371">The problem is not related to the node restart. It also doesn't match before the restart. The problem is that the mpercolate api doesn't serialise the start time of the request to other nodes that participate in mpercolate execution. I'll open a pr for that. 

@rabx88 Can you confirm that this problem only occurs with range query/filters that use `now`?
</comment><comment author="rossbrower" created="2016-01-12T19:07:57Z" id="171018204">That is correct. We tried a few other examples using combinations of term filters and it worked as expected. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reintroduce five-minute and fifteen-minute load averages on Linux</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15907</link><project id="" key="" /><description>This commit reintroduces the five-minute and fifteen-minute load stats
on Linux, and changes the format of the load_average field back to an
array.

Relates #12049, relates #14741
</description><key id="126057932">15907</key><summary>Reintroduce five-minute and fifteen-minute load averages on Linux</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T22:59:21Z</created><updated>2016-01-12T04:53:32Z</updated><resolved>2016-01-12T04:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-11T23:03:42Z" id="170726230">Makes sense to me though someone else should look at it too.
</comment><comment author="rmuir" created="2016-01-11T23:21:27Z" id="170730000">Do you think its cleaner to remove some `#ifdef LINUX`/security code here and just unconditionally add a FilePermission read for `/proc/loadavg` to `security.policy`?

Its not really going to do any harm on other OSes, and I think it makes it easier as a static permission this way (e.g., to factor out this monitoring stuff to a module with these permissions and more isolation, monitoring stuff is a great candidate for that). Having it in this file makes that refactoring more clear, because someone just has to move it to the `plugin-security.policy` file when they try to do that.

In general, we add most filesystem permissions dynamically with java code: not because we want to, but because its our only choice since the paths are configurable in e.g. elasticsearch.yml... but I always prefer the policy files if there is any way, even if its a little less secure! In this case the path is well-known/not-configurable, so that is the way I would do it.
</comment><comment author="rmuir" created="2016-01-11T23:48:55Z" id="170734806">I would remove the constant too, in this case it just causes a loss of locality. Think if i want to add freebsd support here, I see it like this:

```
security.policy:
// load average statistics on linux
grant FilePermission "/proc/loadavg", "read";
// load average statistics on freebsd
grant FilePermission "/compat/linux/proc/loadavg", "read";
```

Then we only have system-dependent code in one place: the monitoring code itself... makes it easier to move out :) and I think we would prefer to keep Environment simpler and limited to "core" stuff.
</comment><comment author="jasontedor" created="2016-01-12T00:37:20Z" id="170743370">&gt; Do you think its cleaner to remove some #ifdef LINUX/security code here and just unconditionally add a FilePermission read for /proc/loadavg to security.policy?

@rmuir I do think it's cleaner, but I was trying to following the principle of least privilege any only grant those permissions on systems where it will actually be used. I pushed f7c7f8eb5231f7f317691a8a30f4594191730458 to make this permission static and defined in security.policy.
</comment><comment author="jasontedor" created="2016-01-12T00:40:52Z" id="170743872">&gt; I would remove the constant too, in this case it just causes a loss of locality.

@rmuir I moved this constant to a new class to hold these in f7c7f8eb5231f7f317691a8a30f4594191730458. I added new class to holding these because more are coming (e.g., `/proc/[pid]/io`).
</comment><comment author="rmuir" created="2016-01-12T00:41:15Z" id="170743993">&gt; @rmuir I do think it's cleaner, but I was trying to following the principle of least privilege any only grant those permissions on systems where it will actually be used. I pushed f7c7f8e to make this permission static and defined in security.policy.

Yeah, I see the rationale. Its a tough tradeoff, but in cases like this I think simplicity should win. Otherwise monitoring code could get too complicated to factor out and we miss the forest for the trees, because if we can pull it out into a module, then a ton of stuff including this (like messing around with arbitrary threads, environment variable access, sysprops, ...) can be moved to just that module: so only the 1% code that needs it has it. 
</comment><comment author="jasontedor" created="2016-01-12T00:41:36Z" id="170744096">&gt; I do think it's cleaner, but I was trying to following the principle of least privilege any only grant those permissions on systems where it will actually be used.

@rmuir Hmm...now I'm thinking about `/proc/[pid]/io` and wondering if we only want to grant this for the Elasticsearch pid which requires a dynamic setup of the property?
</comment><comment author="rmuir" created="2016-01-12T00:41:53Z" id="170744147">&gt; @rmuir Hmm...now I'm thinking about /proc/[pid]/io and wondering if we only want to grant this for the Elasticsearch pid which requires a dynamic setup of the property?

that is why there is /proc/self
</comment><comment author="jasontedor" created="2016-01-12T00:42:19Z" id="170744209">&gt; that is why there is /proc/self

@rmuir :)
</comment><comment author="rmuir" created="2016-01-12T00:44:22Z" id="170744546">&gt; I added new class to holding these because more are coming (e.g., /proc/[pid]/io).

Well, see my freebsd example: in this case we have a totally different "de-facto" path (/compat/linux/...), and it asks you to set this up when you install openjdk (not sure if it will fail hard without it). Anyway I just don't think these constants being in a separate file helps much. To me its too much indirection.

I guess to me this is more intuitive, since it prevents system-dependent stuff from being sprayed everywhere:

```
if (Constants.Linux) {
   processLoadAverage("/proc/loadavg");
} else if (Constants.FreeBSD) {
   processLoadAverage("/compat/linux/proc/loadavg");
} ...
```

What does the separate file with constants buy us?
</comment><comment author="jasontedor" created="2016-01-12T00:46:13Z" id="170744834">&gt; What does the separate file with constants buy us?

@rmuir What it buys is not spraying these paths in a bunch of different places which lets us use `@SuppressForbidden` in one place instead of many.
</comment><comment author="rmuir" created="2016-01-12T00:48:51Z" id="170745214">But we don't truly need a separate file for that right? If we go with my pseudocode we have:

```
@SuppressForbidden(reason = "access absolute paths for operating system statistics")
private void something() {
  if (Constants.Linux) {
   processLoadAverage(PathUtils.get("/proc/loadavg"));
  } else if (Constants.FreeBSD) {
   processLoadAverage(PathUtils.get("/compat/linux/proc/loadavg"));
  } ...
}
```

I think this is just as good, and `processLoadAverage(Path loadFile)` with the try/catch and parsing logic wouldn't need any suppression, its agnostic to where the file is coming from. If its a static method it also makes unit testing it very easy (just make fake files).
</comment><comment author="jasontedor" created="2016-01-12T01:09:54Z" id="170748521">&gt; But we don't truly need a separate file for that right?

@rmuir Nope. I pushed 67d1785cbb06ba0944960c30ed61e298f8e42723 and think this is ready for a final review?
</comment><comment author="rmuir" created="2016-01-12T01:52:41Z" id="170756613">I do think its a lot better. So the last thing is this `DEFAULT_LOAD_AVERAGE`. I'm really not a fan of this..., its static, but there is nothing to enforce immutability of the thing (regardless of double or pojo representation), so besides being a dangerous optimization, I'm not sure that optimizing this actually matters. 

If we really want to keep this optimization can it be at least moved to the method where its returned from, have a better name, and be private, like `private static final double[] LOAD_AVERAGE_UNAVAILABLE` ? 
</comment><comment author="jasontedor" created="2016-01-12T02:03:41Z" id="170758334">@rmuir I pushed 5fb5ed8f39c1c617d4fe91f7d8c690ad657c2528 to just allocate new instances when the load averages are not available. If Java had immutable primitive arrays I'd stay with a private static instance but you won me over.
</comment><comment author="rmuir" created="2016-01-12T04:34:51Z" id="170786887">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Move pipeline configuration to the cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15906</link><project id="" key="" /><description>Move the pipeline configuration from the dedicated index to the cluster state.
</description><key id="126056394">15906</key><summary>[Ingest] Move pipeline configuration to the cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-11T22:53:19Z</created><updated>2016-01-13T22:02:43Z</updated><resolved>2016-01-13T22:02:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-12T17:19:40Z" id="170980664">I left some comments, would love @s1monw to have a look too and see if we can improve this further.
</comment><comment author="martijnvg" created="2016-01-13T13:16:26Z" id="171286892">@javanna @s1monw I've updated the PR.
</comment><comment author="s1monw" created="2016-01-13T13:29:02Z" id="171289847">thanks for the tests @martijnvg 
</comment><comment author="martijnvg" created="2016-01-13T16:12:51Z" id="171345870">@javanna I've updated the PR. I think it is closed to getting merged.
</comment><comment author="javanna" created="2016-01-13T17:39:17Z" id="171375046">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is cloud-aws for 2.X available in maven for java client nodes?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15905</link><project id="" key="" /><description>I have a java-based client node that uses cloud-aws for EC2 discovery and I'm trying to upgrade it from 1.7 to 2.1.1.

I'm unable to find any 2.X-compatible versions of the plugin in maven. Am I missing it? If I want to incorporate EC2-based discovery of an 2.1.1 cluster into a java client, how is it recommended that I proceed?
</description><key id="126055908">15905</key><summary>Is cloud-aws for 2.X available in maven for java client nodes?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbaggott</reporter><labels /><created>2016-01-11T22:51:14Z</created><updated>2016-01-11T23:00:46Z</updated><resolved>2016-01-11T23:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dbaggott" created="2016-01-11T23:00:46Z" id="170725387">Yes, it is:

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
    &lt;artifactId&gt;cloud-aws&lt;/artifactId&gt;
    &lt;version&gt;2.1.1&lt;/version&gt;
&lt;/dependency&gt;
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw exception if content type could not be determined in Update API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15904</link><project id="" key="" /><description>If the content type could not be determined, the UpdateRequest would still try to parse the content instead
of throwing the standard ElasticsearchParseException.  This manifests when
passing illegal JSON in the request body that does not begin with a '{'.
By trying to parse the content from an unknown request body content type,
the UpdateRequest was throwing a null pointer exception.  This has been
fixed to throw an ElasticsearchParseException, to be consistent with the
behavior of all other requests in the face of undecipherable request
content types.

Closes #15822
</description><key id="126055151">15904</key><summary>Throw exception if content type could not be determined in Update API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T22:47:22Z</created><updated>2016-01-12T16:13:54Z</updated><resolved>2016-01-12T16:13:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-01-11T22:48:14Z" id="170717321">@jpountz not sure if you have the time, but this PR needs a code review!  Also, should it be backported to v2.2.0 as well?
</comment><comment author="jpountz" created="2016-01-12T14:25:42Z" id="170926359">LGTM!

&gt; Also, should it be backported to v2.2.0 as well?

+1 to push it to 2.2 as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Document node.max_local_storage_nodes setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15903</link><project id="" key="" /><description>`node.max_local_storage_nodes` is an extremely useful setting that prevents more than the defined number of nodes (e.g., `1`) from sharing the same `path.data`.

I'm open to suggestions on where it should be documented.
</description><key id="126052000">15903</key><summary>[DOCS] Document node.max_local_storage_nodes setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Settings</label><label>docs</label></labels><created>2016-01-11T22:29:27Z</created><updated>2016-01-30T17:14:31Z</updated><resolved>2016-01-30T17:14:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T10:44:29Z" id="170871584">Thanks for volunteering.  I'd put it on this page: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>histogram query results a deadlock in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15902</link><project id="" key="" /><description>ElasticSearch version 1.7.3 running with Oracle Java 8.

When ran this query 

```
 {
  "size" : 1000,
  "query" : {
    "range" : {
      "eventTimestamp" : {
        "from" : 1452543896208,
        "to" : 1452544796208,
        "include_lower" : true,
        "include_upper" : true
      }
    }
  },
  "aggregations" : {
    "/series(eventtimestamp)" : {
      "date_histogram" : {
        "field" : "eventTimestamp",
        "interval" : "10ms",
        "min_doc_count" : 0,
        "order" : {
          "_key" : "asc"
        },
        "extended_bounds" : {
          "min" : 0,
          "max" : 1452544262018
        }
      }
    }
  }
}
```

ElasticSearch froze and observed a possible deadlock using yourkit. Filter is selected for 15 mins on a interval of 10ms. 
&lt;img width="1380" alt="screen shot 2016-01-11 at 12 51 11 pm" src="https://cloud.githubusercontent.com/assets/1412516/12246831/96db4156-b865-11e5-9dce-670d5f87012e.png"&gt;

Although min/max diff is unrealistically huge, ES should simply timeout.

Now I replaced min max diff to reasonably low value but not within the filter range. ES still freezes. 

```
 {
  "size" : 1000,
  "query" : {
    "range" : {
      "eventTimestamp" : {
        "from" : 1452552455074,
        "to" : 1452552755074,
        "include_lower" : true,
        "include_upper" : true
      }
    }
  },
  "aggregations" : {
    "/series(eventtimestamp)" : {
      "date_histogram" : {
        "field" : "eventTimestamp",
        "interval" : "10ms",
        "min_doc_count" : 0,
        "order" : {
          "_key" : "asc"
        },
        "extended_bounds" : {
          "min" : 210,
          "max" : 1000
        }
      }
    }
  }
}
```

![heapusage](https://cloud.githubusercontent.com/assets/1412516/12249272/89c2371e-b873-11e5-9cf4-e8a732e7b192.png)
&lt;img width="589" alt="cpuusage" src="https://cloud.githubusercontent.com/assets/1412516/12249279/96c3ebe2-b873-11e5-9dbf-f81ad977e1e0.png"&gt;
[es_deadlock_histogram_stackdump.txt](https://github.com/elastic/elasticsearch/files/86527/es_deadlock_histogram_stackdump.txt)

Ideally elasticsearch should only return buckets with overlapping range from filter (or range present in output result set) and input min/max bounds.
</description><key id="126038389">15902</key><summary>histogram query results a deadlock in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">puneetjaiswal</reporter><labels /><created>2016-01-11T21:17:07Z</created><updated>2016-01-12T10:42:02Z</updated><resolved>2016-01-12T10:42:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-12T10:42:02Z" id="170871109">You are creating 145255275507 buckets... which is using up all your memory.  The threads are frozen because garbage collection is thrashing.  We will be dealing with this by limiting the number of buckets that can created.  Closing in favour of https://github.com/elastic/elasticsearch/issues/14046
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[WIP] [Ingest] _simulate error messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15901</link><project id="" key="" /><description>This issue will contain comments on the ingest _simulate error reporting functionality. This is an ongoing exploration, and I'll add a comment for each unexpected behavior.
</description><key id="126020323">15901</key><summary>[WIP] [Ingest] _simulate error messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">BigFunger</reporter><labels><label>:Ingest</label></labels><created>2016-01-11T19:39:01Z</created><updated>2016-04-04T14:56:06Z</updated><resolved>2016-04-04T14:55:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BigFunger" created="2016-01-11T19:39:11Z" id="170666309">## grok scenario 1

**input**

```
{
  "pipeline": {
    "description": "_description",
    "processors": [
      {
          "grok" : {
              "field": "",
              "pattern" : "",
              "processor_id" : "processor_1"
          }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_type": "type",
      "_id": "id",
      "_source": {
        "_raw": ""
      }
    }
  ]
}
```

**output**

```
{
  "docs": [
    {
      "processor_results": [
        {
          "processor_id": "processor_1",
          "error": {
            "root_cause": [
              {
                "type": "illegal_argument_exception",
                "reason": "path cannot be null nor empty"
              }
            ],
            "type": "illegal_argument_exception",
            "reason": "path cannot be null nor empty"
          }
        }
      ]
    }
  ]
}
```

**comments**
I don't know what "path cannot be null nor empty" refers to.
</comment><comment author="BigFunger" created="2016-01-11T19:40:19Z" id="170666600">## grok scenario 2

**input**

```
{
  "pipeline": {
    "description": "_description",
    "processors": [
      {
          "grok" : {
              "field": "_raw",
              "pattern" : "",
              "processor_id" : "processor_1"
          }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_type": "type",
      "_id": "id",
      "_source": {
        "_raw": ""
      }
    }
  ]
}
```

**output**

```
{
  "docs": [
    {
      "processor_results": [
        {
          "processor_id": "processor_1",
          "error": {
            "root_cause": [
              {
                "type": "null_pointer_exception",
                "reason": null
              }
            ],
            "type": "null_pointer_exception",
            "reason": null
          }
        }
      ]
    }
  ]
}
```

**comments**
I expect the [reason] property to be populated
null_pointer_exception means nothing to me in context.
</comment><comment author="BigFunger" created="2016-01-11T19:40:57Z" id="170666787">## grok scenario 3

**input**

```
{
  "pipeline": {
    "description": "_description",
    "processors": [
      {
          "grok" : {
              "field": "_raw",
              "pattern" : "test",
              "processor_id" : "processor_1"
          }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_type": "type",
      "_id": "id",
      "_source": {
        "_raw": ""
      }
    }
  ]
}
```

**output**

```
{
  "docs": [
    {
      "processor_results": [
        {
          "processor_id": "processor_1",
          "error": {
            "root_cause": [
              {
                "type": "illegal_argument_exception",
                "reason": "Grok expression does not match field value: []"
              }
            ],
            "type": "illegal_argument_exception",
            "reason": "Grok expression does not match field value: []"
          }
        }
      ]
    }
  ]
}
```

**comments**
Do we want a grok expression that doesn't match anything to throw an exception?
</comment><comment author="BigFunger" created="2016-01-11T19:41:31Z" id="170666936">## grok scenario 4

**input**

```
{
  "pipeline": {
    "description": "_description",
    "processors": [
      {
          "grok" : {
              "field": "_raw",
              "pattern" : "test",
              "processor_id" : "processor_1"
          }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_type": "type",
      "_id": "id",
      "_source": {
        "_raw": "test"
      }
    }
  ]
}
```

**output**

```
{
  "docs": [
    {
      "processor_results": [
        {
          "processor_id": "processor_1",
          "error": {
            "root_cause": [
              {
                "type": "null_pointer_exception",
                "reason": null
              }
            ],
            "type": "null_pointer_exception",
            "reason": null
          }
        }
      ]
    }
  ]
}
```

**comments**
I expect the [reason] property to be populated
null_pointer_exception means nothing to me in context.
I also didn't anticipate this NPE at all.
</comment><comment author="talevy" created="2016-01-12T01:06:03Z" id="170747725">scenario 2, 4 is a bug in Grok parsing. This occurs when there is a regex match, but no captures to iterate through in order to extract into new fields
</comment><comment author="talevy" created="2016-01-12T01:06:36Z" id="170747808">scenario 3: yes, we want an exception to be thrown
</comment><comment author="talevy" created="2016-01-12T01:11:17Z" id="170748726">Scenario 1: this is a slight issue with how we evaluate field names. Since the field name was empty, it blows up :)
</comment><comment author="talevy" created="2016-01-12T01:12:08Z" id="170748845">@BigFunger thanks for submitting these, I will clean up scenarios 1,2,4. Scenario 3 will probably remain the same
</comment><comment author="talevy" created="2016-01-12T01:59:09Z" id="170757601">actually. looks like Scenario 1 is expected. `path` is just not very helpful in this context. it means the path defined in `field`.
</comment><comment author="talevy" created="2016-04-04T14:55:21Z" id="205334680">Closing this since the NPEs have been fixed. There is still a known issue around the lack of context when one is unable to find a field path. This is not as straightforward to resolve since knowledge of the field name for this path value is lacking.

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/ingest/core/IngestDocument.java#L553 

``` java
if (Strings.isEmpty(path)) {
    throw new IllegalArgumentException("path cannot be null nor empty");
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Primary relocation handoff</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15900</link><project id="" key="" /><description>When primary relocation completes, a cluster state is propagated that deactivates the old primary and marks the new primary as active. As cluster state changes are not applied synchronously on all nodes, there can be a time interval where the relocation target has processed the cluster state and believes to be the active primary and the relocation source has not yet processed the cluster state update and still believes itself to be the active primary. This PR ensures that, before completing the relocation, the relocation source deactivates writing to its store and delegates requests to the relocation target.

The change is motivated as follows:

1) We need to ensure that we only start writing data into the new primary once all the writes into the old primary have been completely replicated (among others to the new primary). This ensures that the new primary operates on the proper document version numbers. Document versions are increased when writing to the primary and then used on the replica to make sure that newer documents are not overridden by older documents (in the presence of concurrent replication). A scenario for this would be: Write document with id "K" and value "X" to old primary (gets version 1) and replicate it to new primary as well as replica. Assume that another document with id "K" but value "Y" is written on the new primary before the new primary gets the replicated write of "K" with value "X". Unaware of the other write it will then assign the same version number (namely 1) to the document with value "Y" and replicate it to the replica. Depending on the order in which replicated writes from old and new primary arrive at the replica, it will then either store "X" or "Y", which means that the new primary and the replica can become out of sync.

2) We have to ensure that no new writes are done on the old primary once we start writing into the new primary. This helps with the following scenario. Assume primary relocation completes and master broadcasts cluster state which now only contains the new primary. Due to the distributed nature of Elasticsearch, cluster states are not applied in full synchrony on all nodes. For a brief moment nodes in the cluster have a different view of which node is the primary. In particular, it's possible that the node holding the old primary (node A) still believes to be the primary whereas the node holding the new primary (node B) believes to be the primary as well. If we send a document to node B, it will get indexed into the new primary and acknowledged (but won't exist on the old primary). If we then issue a delete request for the same document to the node A (which can happen if we send requests round-robin to nodes), then that node will not find the document in its old primary and fail the request.

This PR (in combination with #19013) implements the following solution:

Before completing the relocation, node A (holding the primary relocation source) deactivates writing to its shard copy (and temporarily puts all new incoming requests for that shard into a queue), then waits for all ongoing operations to be fully replicated. Once that is done, it delegates all new incoming requests to node B (holding the new primary) and also sends all the elements in the queue there. It uses a special action to delegate requests to node B, which bypasses the standard reroute phase when accepting requests as standard rerouting is based on the current cluster state on the node. At that moment, indexing requests that directly go to the node B will still be rerouted back to node A with the old primary. This means that node A is still in charge of indexing, but will use the physical shard copy on node B to do so. Node B finally asks the master to activate the new primary (finish the relocation). The master then broadcasts a new cluster state where the old primary on node A is removed and the new primary on node B is active. It doesn't matter now in which order the cluster state is applied on the nodes A and B:
1) If the cluster state is first applied on the node B, both nodes will send their index requests to the shard copy that is on node B.
2) If the cluster state is first applied on node A, requests to node A will be rerouted to node B and requests to node B will be rerouted to node A. To prevent redirect loops during the time period where cluster states on node A and node B differ, #16274 makes requests that are coming from node A wait on node B until B has processed the cluster state where relocation is completed.

supersedes #15532
</description><key id="126017174">15900</key><summary>Primary relocation handoff</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T19:22:36Z</created><updated>2016-09-24T01:49:05Z</updated><resolved>2016-02-02T09:01:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-11T19:41:16Z" id="170666874">Thanks @ywelsch . Can we open a PR with just the operation counter, extracted to it's own java file? We can work on the primary relocation after wards. Also I would love it if we can keep the counter naming to the universe out of IndexShard (for now). Let's try to keep the change small. 
</comment><comment author="ywelsch" created="2016-01-25T12:40:58Z" id="174494979">@bleskes I've updated the PR according to our discussion and considered the following 4 scenarios:
- source node and target node on cluster state before relocation target is marked as started and shard on source node not yet marked as RELOCATED. This means that source node knows it is active primary but not yet relocated and target node knows it is primary and relocation target. Index requests to source node are indexed on source node and replicated to target node. Index requests to target node are rerouted to source node.
- source node and target node on cluster state before relocation target is marked as started and shard on source node marked as RELOCATED. This means that source node knows it is relocated and target node knows it is a primary relocation target. Index requests to source node are sent in primary phase to target node and replicated back to source node. Index requests to target are rerouted back to source node.
- source node on cluster state before shard of relocation target is marked as started and target node on cluster state with shard marked as started. This means that source node knows it is still active primary, but its shard has been marked as RELOCATED. Target node knows it is active primary as well. Index requests to source node are sent in primary phase to target node but not replicated back to source node. Index requests to target node are indexed directly on target node and not replicated to source node.
- source node on cluster state where relocation target is marked as started and target node on cluster state where it is not yet started. This means that source node has closed its shard. Requests to source node are rerouted to target node. Requests to target node are rerouted to source node which are rerouted to target node and back and forth. This is addressed by subsequent patch.
</comment><comment author="bleskes" created="2016-01-28T13:35:48Z" id="176188017">Thanks @ywelsch . this looks solid. I left some comments, mostly around simplifying things...
</comment><comment author="ywelsch" created="2016-01-28T18:42:44Z" id="176332022">@bleskes I've updated the PR again. Can you have another look? Reviewing this time might be easier by commenting directly on the newly added commits.

After all is done, I will put SuspendableRefContainer and its tests into a standalone commit.
</comment><comment author="bleskes" created="2016-01-30T09:29:55Z" id="177128288">Thanks @ywelsch . Looks great. I left very minor comments and one important one on Replication Action tests.  I'd love it if @s1monw gives this a look  as well (mostly around the indexshard universe).
</comment><comment author="bleskes" created="2016-01-30T09:31:15Z" id="177129019">&gt; Reviewing this time might be easier by commenting directly on the newly added commits.

Sadly github looses those when you delete the branch and they are not a first class citizen in the UI.

&gt; After all is done, I will put SuspendableRefContainer and its tests into a standalone commit.

No need now. Thanks. Would have just made this PR (slightly) easier to review
</comment><comment author="ywelsch" created="2016-02-01T11:07:38Z" id="177917183">squashed / rebased on current master.
</comment><comment author="s1monw" created="2016-02-01T11:32:09Z" id="177927694">@ywelsch this looks awesome. I left some comments
</comment><comment author="s1monw" created="2016-02-01T13:05:15Z" id="177964745">I did another round with minor comments LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ban Serializable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15899</link><project id="" key="" /><description>1. Uses forbidden patterns to prevent things from referencing
   java.io.Serializable or from mentioning serialVersionUID.
2. Uses -Xlint:-serial so we don't have to hear from javac that we aren't
   declaring serialVersionUID on any classes that we make that happen to extend
   Serializable.
3. Remove Serializable and serialVersionUID declarations.

I didn't use forbidden apis because it doesn't look like it has a way to ban
explicitly implementing Serializable. If you try to ban Serializable with
forbidden apis you end up banning all Exceptions and all Strings.

Closes #15847
</description><key id="126012184">15899</key><summary>Ban Serializable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T18:58:56Z</created><updated>2016-01-12T15:04:50Z</updated><resolved>2016-01-12T15:04:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-11T22:50:58Z" id="170719111">Rebased!
</comment><comment author="jpountz" created="2016-01-12T14:40:15Z" id="170931740">TI like it but I'm unsure how ok it is to modify files under `org/elasticsearch/common/inject`. @s1monw any opinion on this?
</comment><comment author="nik9000" created="2016-01-12T14:47:53Z" id="170935863">&gt; I'm unsure how ok it is to modify files under org/elasticsearch/common/inject

We've done it a bunch of times. It'd be nice to just remove them but that'll be tons of work. Things like https://github.com/elastic/elasticsearch/pull/15761 are a start but there is a long way to go.
</comment><comment author="jpountz" created="2016-01-12T14:50:15Z" id="170936841">OK, fine by me then. LGTM!
</comment><comment author="nik9000" created="2016-01-12T14:53:23Z" id="170937636">\o/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ChannelReference and simplify Views</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15898</link><project id="" key="" /><description>Currently we use ref counting to manage the life cycles of a translog file. This was done to allow the creation of view and snapshots, making sure that the underlying files are available. This commit takes a simpler route based on the observation that a snapshot doesn't need to have it's own life cycle but rather can lift on the lifecycle of it's parent (translog or view)[1].  Instead of ref counter, the channel is owned by the writer or reader pointing at it. Views are just a min generation indication and Snapshot get access to a channel they do not own (and thus are not closable).  This means ChannelReference and it's call backs on close are gone.

Also, I took the opportunity to clean up legacy translog readers we don't need in master.

[1] If code fails to adhere to this assumption it will get a channel already closed exception.
</description><key id="126007183">15898</key><summary>Remove ChannelReference and simplify Views</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T18:31:50Z</created><updated>2016-01-20T12:14:21Z</updated><resolved>2016-01-19T08:19:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-11T19:57:30Z" id="170671438">@bleskes thank you for working on this. I love the fact that we don't do ref counting anymore. I left a bunch of comments but it looks awesome. I also think @mikemccand should take a look at this.
</comment><comment author="bleskes" created="2016-01-12T10:21:45Z" id="170865807">Thanks @s1monw . Flight is delayed so I addressed all your comments :)
</comment><comment author="mikemccand" created="2016-01-12T11:21:03Z" id="170880457">I really like this simplification!  It's analogous to how in Lucene when we clone an `IndexInput` it's also not separately tracked but rather relies on its original `IndexInput` to remain open.

I left a bunch of small comments, many of which are likely pre-existing issues ... we can postpone/separate them from this simplification if it's a problem.
</comment><comment author="bleskes" created="2016-01-17T14:30:26Z" id="172329609">Thanks @mikemccand . I pushed another set of changes. Can you take another look?
</comment><comment author="mikemccand" created="2016-01-18T10:23:54Z" id="172489575">Thanks @bleskes, I left a few minor comments, most of them pre-existing issues so feel free to defer to separate issue, but otherwise LGTM: this is a nice simplification.
</comment><comment author="bleskes" created="2016-01-18T19:42:15Z" id="172632098">@mikemccand @s1monw thanks. I pushed some more commits addressing your feedback
</comment><comment author="mikemccand" created="2016-01-18T23:41:18Z" id="172683062">LGTM, thanks @bleskes, this is a great change!
</comment><comment author="bleskes" created="2016-01-19T08:27:37Z" id="172774072">Pushed to master. Thanks @mikemccand and @s1monw , these are tricky reviews :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add CapturingTransport#captureRequestsAndClear</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15897</link><project id="" key="" /><description>We use the pattern of 

``` java
CapturingTransport.CapturedRequest[] capturedRequests = transport.capturedRequests();
transport.clear()
```

where `transport` is a `CapturingTransport` throughout our test suite. This pattern could be simplified by adding a dedicated `CapturingTransport#captureRequestsAndClear` method.
</description><key id="125997403">15897</key><summary>Add CapturingTransport#captureRequestsAndClear</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>enhancement</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T17:38:26Z</created><updated>2016-01-12T14:44:23Z</updated><resolved>2016-01-12T14:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove retry logic from IndicesClusterStateService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15896</link><project id="" key="" /><description>IndicesClusterStateService currently manages its own retries when managing shard state. Channel related failures will now be managed in ShardStateAction (#15748, #15895) so the retry logic should be refactored there to only handle non-channel failures.
</description><key id="125997002">15896</key><summary>Remove retry logic from IndicesClusterStateService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.4.4</label></labels><created>2016-01-11T17:36:05Z</created><updated>2017-06-27T10:28:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ShardStateAction#shardStarted should manage retries on channel exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15895</link><project id="" key="" /><description>ShardStateAction#shardFailure was modified in #15748 to manage retries on channel exceptions, and otherwise signal success or catastrophic failure to the requester. A similar mechanism should be built for ShardStateAction#shardStarted.
</description><key id="125996719">15895</key><summary>ShardStateAction#shardStarted should manage retries on channel exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T17:34:18Z</created><updated>2016-01-19T12:05:45Z</updated><resolved>2016-01-19T12:05:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix blended terms take 2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15894</link><project id="" key="" /><description>Closes #15860
</description><key id="125996540">15894</key><summary>Fix blended terms take 2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T17:33:23Z</created><updated>2016-01-15T15:30:29Z</updated><resolved>2016-01-15T14:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-11T17:35:39Z" id="170627920">@s1monw I was a bit overzealous with that last one. I ended up merging it without running all the tests. Something about a weekend in between. I reverted it because it was pretty destructive. Anyway, this is the code from the last one in first commit and code to fix the fallout of the first commit in the second. Mostly it works around funkiness with setting an analyzer and using numeric or date fields. 
</comment><comment author="nik9000" created="2016-01-12T15:49:52Z" id="170953579">I just blasted all the tags saying backport this very far into the past after talking with some other Elastic employees. This has been like this for a long time and its a fiddly change so I'd like it to soak in integration testing. Thus I'm targeting just master and 2.x branches. So the 3.0 and 2.3 releases.
</comment><comment author="jpountz" created="2016-01-13T08:32:11Z" id="171215322">LGTM. I agree this fix is to subtle to be backported to bugfix releases.
</comment><comment author="nik9000" created="2016-01-15T15:30:29Z" id="171991892">And cherry picked to 2.x: 1c95b75b466b6dc152959c55e36d833c9cace8f1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when running simple query_string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15893</link><project id="" key="" /><description>I'm running a homebrew'ed v2.1.1 setup and after upgrading from v2.0.x I get a NPE when running a simple string query like this: 

``` bash
curl -XGET 'http://localhost:9200/collectiveaccess/ca_objects/_search' -d '{
  "query": {
    "query_string": {
      "query": "test"
    }
  }
}'
"error":{"root_cause":[{"type":"null_pointer_exception","reason":null}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"collectiveaccess","node":"NeHq0D0nSDSv-97EzhLGqQ","reason":{"type":"null_pointer_exception","reason":null}}]},"status":500}
```

I tried nuking and recreating the index but to no avail. This mapping/search combination still works fine in v2.0.x. I looked over the [breaking changes](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking-changes-2.1.html) in the v2.1.0 change log but I don't think any of that affects what we're doing here.

Below is a snippet from the log and my mapping and index settings. Let me know if you need anything else.

```
[2016-01-11 12:05:00,424][DEBUG][action.search.type       ] [Human Robot] [collectiveaccess][2], node[h48rbBL6SB6kNjcl9jrzvQ], [P], v[6], s[STARTED], a[id=Cf_NvjlpS9i3-7DdD-MWGA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7d7461d2] lastShard [true]
RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-11 12:05:00,424][DEBUG][action.search.type       ] [Human Robot] [collectiveaccess][1], node[h48rbBL6SB6kNjcl9jrzvQ], [P], v[6], s[STARTED], a[id=TAvE76wPT9GjkcinF8zStw]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7d7461d2]
RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-11 12:05:00,429][DEBUG][action.search.type       ] [Human Robot] All shards failed for phase: [query]
RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-11 12:05:00,424][DEBUG][action.search.type       ] [Human Robot] [collectiveaccess][0], node[h48rbBL6SB6kNjcl9jrzvQ], [P], v[6], s[STARTED], a[id=ZD_2_wvQRmO2xl_-DXYDdQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7d7461d2] lastShard [true]
RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-11 12:05:00,424][DEBUG][action.search.type       ] [Human Robot] [collectiveaccess][3], node[h48rbBL6SB6kNjcl9jrzvQ], [P], v[6], s[STARTED], a[id=FBn1HlPHTFCtPPs9ky6T2g]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7d7461d2] lastShard [true]
RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-11 12:05:00,424][DEBUG][action.search.type       ] [Human Robot] [collectiveaccess][4], node[h48rbBL6SB6kNjcl9jrzvQ], [P], v[6], s[STARTED], a[id=3Asjj9bXT5C0aQ7btw_sqw]: Failed to execute [org.elasticsearch.action.search.SearchRequest@7d7461d2] lastShard [true]
RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    ... 10 more
[2016-01-11 12:05:00,432][INFO ][rest.suppressed          ] /collectiveaccess/ca_objects/_search Params: {index=collectiveaccess, type=ca_objects}
Failed to execute phase [query], all shards failed; shardFailures {[h48rbBL6SB6kNjcl9jrzvQ][collectiveaccess][0]: RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[h48rbBL6SB6kNjcl9jrzvQ][collectiveaccess][1]: RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[h48rbBL6SB6kNjcl9jrzvQ][collectiveaccess][2]: RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[h48rbBL6SB6kNjcl9jrzvQ][collectiveaccess][3]: RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException; }{[h48rbBL6SB6kNjcl9jrzvQ][collectiveaccess][4]: RemoteTransportException[[Human Robot][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException; }
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:228)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onFailure(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.ActionListenerResponseHandler.handleException(ActionListenerResponseHandler.java:46)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: ; nested: NullPointerException;
    at org.elasticsearch.ElasticsearchException.guessRootCauses(ElasticsearchException.java:382)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.guessRootCauses(SearchPhaseExecutionException.java:152)
    at org.elasticsearch.action.search.SearchPhaseExecutionException.getCause(SearchPhaseExecutionException.java:99)
    at java.lang.Throwable.printStackTrace(Throwable.java:665)
    at java.lang.Throwable.printStackTrace(Throwable.java:721)
    at org.apache.log4j.DefaultThrowableRenderer.render(DefaultThrowableRenderer.java:60)
    at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
    at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
    at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
    at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
    at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
    at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
    at org.apache.log4j.Category.callAppenders(Category.java:206)
    at org.apache.log4j.Category.forcedLog(Category.java:391)
    at org.apache.log4j.Category.log(Category.java:856)
    at org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo(Log4jESLogger.java:125)
    at org.elasticsearch.common.logging.support.AbstractESLogger.info(AbstractESLogger.java:90)
    at org.elasticsearch.rest.BytesRestResponse.convert(BytesRestResponse.java:131)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:96)
    at org.elasticsearch.rest.BytesRestResponse.&lt;init&gt;(BytesRestResponse.java:87)
    at org.elasticsearch.rest.action.support.RestActionListener.onFailure(RestActionListener.java:60)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.raiseEarlyFailure(TransportSearchTypeAction.java:316)
    ... 10 more
Caused by: java.lang.NullPointerException
    at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
    at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
    at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
    at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
    at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    ... 3 more
```

```
{
  "collectiveaccess" : {
    "mappings" : {
      "ca_objects" : {
        "properties" : {
          "ca_collection_labels/collection_id" : {
            "type" : "double"
          },
          "ca_collection_labels/name" : {
            "type" : "string"
          },
          "ca_collections/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_collections/type_id" : {
            "type" : "string"
          },
          "ca_entities/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_entities/type_id" : {
            "type" : "string"
          },
          "ca_entity_labels/displayname" : {
            "type" : "string"
          },
          "ca_entity_labels/entity_id" : {
            "type" : "double"
          },
          "ca_entity_labels/forename" : {
            "type" : "string"
          },
          "ca_entity_labels/middlename" : {
            "type" : "string"
          },
          "ca_entity_labels/surname" : {
            "type" : "string"
          },
          "ca_item_tags/tag" : {
            "type" : "string"
          },
          "ca_list_item_labels/item_id" : {
            "type" : "double"
          },
          "ca_list_item_labels/item_id_content_ids" : {
            "type" : "string"
          },
          "ca_list_item_labels/name_plural" : {
            "type" : "string"
          },
          "ca_list_item_labels/name_plural_content_ids" : {
            "type" : "string"
          },
          "ca_list_item_labels/name_singular" : {
            "type" : "string"
          },
          "ca_list_item_labels/name_singular_content_ids" : {
            "type" : "string"
          },
          "ca_list_items/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_list_items/item_id" : {
            "type" : "double"
          },
          "ca_list_items/type_id" : {
            "type" : "string"
          },
          "ca_loan_labels/loan_id" : {
            "type" : "double"
          },
          "ca_loan_labels/name" : {
            "type" : "string",
            "boost" : 100.0
          },
          "ca_loans/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_loans/loan_id" : {
            "type" : "double"
          },
          "ca_loans/type_id" : {
            "type" : "string"
          },
          "ca_object_labels/name" : {
            "type" : "string"
          },
          "ca_object_labels/name_content_ids" : {
            "type" : "string"
          },
          "ca_object_labels/name_sort" : {
            "type" : "string"
          },
          "ca_object_labels/name_sort_content_ids" : {
            "type" : "string"
          },
          "ca_object_representations/md5" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "ca_object_representations/media_content" : {
            "type" : "string"
          },
          "ca_object_representations/mimetype" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "ca_object_representations/original_filename" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "ca_object_representations/type_id" : {
            "type" : "string"
          },
          "ca_objects/aat" : {
            "type" : "string"
          },
          "ca_objects/aat_content_ids" : {
            "type" : "string"
          },
          "ca_objects/access" : {
            "type" : "double"
          },
          "ca_objects/access_content_ids" : {
            "type" : "string"
          },
          "ca_objects/audio_format" : {
            "type" : "string"
          },
          "ca_objects/audio_format_content_ids" : {
            "type" : "string"
          },
          "ca_objects/collection_reference" : {
            "type" : "string"
          },
          "ca_objects/copyrightStatement_content_ids" : {
            "type" : "string"
          },
          "ca_objects/coverageDates" : {
            "type" : "date",
            "ignore_malformed" : true,
            "format" : "date_time_no_millis"
          },
          "ca_objects/coverageDates_text" : {
            "type" : "string"
          },
          "ca_objects/coverageNotes" : {
            "type" : "string"
          },
          "ca_objects/coverageNotes_content_ids" : {
            "type" : "string"
          },
          "ca_objects/currency_test" : {
            "type" : "double"
          },
          "ca_objects/currency_test_content_ids" : {
            "type" : "string"
          },
          "ca_objects/currency_test_currency" : {
            "type" : "string"
          },
          "ca_objects/date" : {
            "type" : "string"
          },
          "ca_objects/date_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dc_dates_types" : {
            "type" : "string"
          },
          "ca_objects/dc_dates_types_content_ids" : {
            "type" : "string"
          },
          "ca_objects/deaccession_date" : {
            "type" : "date",
            "ignore_malformed" : true,
            "format" : "date_time_no_millis"
          },
          "ca_objects/deaccession_notes" : {
            "type" : "string"
          },
          "ca_objects/deaccession_notes_content_ids" : {
            "type" : "string"
          },
          "ca_objects/deleted" : {
            "type" : "boolean"
          },
          "ca_objects/deleted_content_ids" : {
            "type" : "string"
          },
          "ca_objects/description" : {
            "type" : "string"
          },
          "ca_objects/description_content_ids" : {
            "type" : "string"
          },
          "ca_objects/description_source" : {
            "type" : "string"
          },
          "ca_objects/description_source_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dimensions" : {
            "type" : "string"
          },
          "ca_objects/dimensions_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dimensions_height_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dimensions_length_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dimensions_thickness_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dimensions_weight_content_ids" : {
            "type" : "string"
          },
          "ca_objects/dimensions_width_content_ids" : {
            "type" : "string"
          },
          "ca_objects/duration" : {
            "type" : "double"
          },
          "ca_objects/duration_content_ids" : {
            "type" : "string"
          },
          "ca_objects/entity_reference" : {
            "type" : "string"
          },
          "ca_objects/entity_reference_2" : {
            "type" : "string"
          },
          "ca_objects/external_link" : {
            "type" : "string"
          },
          "ca_objects/external_link_content_ids" : {
            "type" : "string"
          },
          "ca_objects/formatNotes" : {
            "type" : "string"
          },
          "ca_objects/formatNotes_content_ids" : {
            "type" : "string"
          },
          "ca_objects/gen_format" : {
            "type" : "string"
          },
          "ca_objects/gen_format_content_ids" : {
            "type" : "string"
          },
          "ca_objects/georeference" : {
            "type" : "geo_shape",
            "precision" : "3.0m"
          },
          "ca_objects/georeference_content_ids" : {
            "type" : "string"
          },
          "ca_objects/georeference_text" : {
            "type" : "string"
          },
          "ca_objects/hier_object_id" : {
            "type" : "double"
          },
          "ca_objects/hier_object_id_content_ids" : {
            "type" : "string"
          },
          "ca_objects/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_objects/idno_content_ids" : {
            "type" : "string"
          },
          "ca_objects/image_format" : {
            "type" : "string"
          },
          "ca_objects/image_format_content_ids" : {
            "type" : "string"
          },
          "ca_objects/informationservice" : {
            "type" : "string"
          },
          "ca_objects/informationservice_content_ids" : {
            "type" : "string"
          },
          "ca_objects/integer_test" : {
            "type" : "long"
          },
          "ca_objects/integer_test_content_ids" : {
            "type" : "string"
          },
          "ca_objects/internal_notes" : {
            "type" : "string"
          },
          "ca_objects/internal_notes_content_ids" : {
            "type" : "string"
          },
          "ca_objects/is_deaccessioned" : {
            "type" : "boolean"
          },
          "ca_objects/is_deaccessioned_content_ids" : {
            "type" : "string"
          },
          "ca_objects/language" : {
            "type" : "string"
          },
          "ca_objects/language_content_ids" : {
            "type" : "string"
          },
          "ca_objects/lcsh_terms" : {
            "type" : "string"
          },
          "ca_objects/lcsh_terms_content_ids" : {
            "type" : "string"
          },
          "ca_objects/lot_id" : {
            "type" : "double"
          },
          "ca_objects/lot_id_content_ids" : {
            "type" : "string"
          },
          "ca_objects/meas_line1" : {
            "type" : "long"
          },
          "ca_objects/meas_line1_content_ids" : {
            "type" : "string"
          },
          "ca_objects/measurement_notes_content_ids" : {
            "type" : "string"
          },
          "ca_objects/parent_id" : {
            "type" : "double"
          },
          "ca_objects/parent_id_content_ids" : {
            "type" : "string"
          },
          "ca_objects/rights" : {
            "type" : "string"
          },
          "ca_objects/rightsHolder_content_ids" : {
            "type" : "string"
          },
          "ca_objects/rightsText_content_ids" : {
            "type" : "string"
          },
          "ca_objects/rights_content_ids" : {
            "type" : "string"
          },
          "ca_objects/source_id" : {
            "type" : "string"
          },
          "ca_objects/source_id_content_ids" : {
            "type" : "string"
          },
          "ca_objects/status" : {
            "type" : "double"
          },
          "ca_objects/status_content_ids" : {
            "type" : "string"
          },
          "ca_objects/text_format" : {
            "type" : "string"
          },
          "ca_objects/text_format_content_ids" : {
            "type" : "string"
          },
          "ca_objects/tgn" : {
            "type" : "string"
          },
          "ca_objects/tgn_content_ids" : {
            "type" : "string"
          },
          "ca_objects/type_id" : {
            "type" : "string"
          },
          "ca_objects/type_id_content_ids" : {
            "type" : "string"
          },
          "ca_objects/ubio" : {
            "type" : "string"
          },
          "ca_objects/ubio_content_ids" : {
            "type" : "string"
          },
          "ca_objects/ulan" : {
            "type" : "string"
          },
          "ca_objects/ulan_container_content_ids" : {
            "type" : "string"
          },
          "ca_objects/ulan_content_ids" : {
            "type" : "string"
          },
          "ca_objects/url_entry_content_ids" : {
            "type" : "string"
          },
          "ca_objects/url_line1" : {
            "type" : "long"
          },
          "ca_objects/url_line1_content_ids" : {
            "type" : "string"
          },
          "ca_objects/url_line2" : {
            "type" : "long"
          },
          "ca_objects/url_line2_content_ids" : {
            "type" : "string"
          },
          "ca_objects/url_source_content_ids" : {
            "type" : "string"
          },
          "ca_objects/video_format" : {
            "type" : "string"
          },
          "ca_objects/video_format_content_ids" : {
            "type" : "string"
          },
          "ca_objects/wiki_content_ids" : {
            "type" : "string"
          },
          "ca_objects/wikipedia" : {
            "type" : "string"
          },
          "ca_objects/wikipedia_content_ids" : {
            "type" : "string"
          },
          "ca_occurrence_labels/name" : {
            "type" : "string"
          },
          "ca_occurrence_labels/occurrence_id" : {
            "type" : "double"
          },
          "ca_occurrences/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_occurrences/type_id" : {
            "type" : "string"
          },
          "ca_place_labels/name" : {
            "type" : "string"
          },
          "ca_place_labels/place_id" : {
            "type" : "double"
          },
          "ca_places/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_places/type_id" : {
            "type" : "string"
          },
          "ca_representation_annotation_labels/name" : {
            "type" : "string"
          },
          "ca_representation_annotation_labels/name_sort" : {
            "type" : "string"
          },
          "ca_sets/set_code" : {
            "type" : "string",
            "index" : "not_analyzed"
          },
          "ca_sets/set_id" : {
            "type" : "double"
          },
          "ca_sets/type_id" : {
            "type" : "string"
          },
          "ca_storage_location_labels/location_id" : {
            "type" : "double"
          },
          "ca_storage_location_labels/name" : {
            "type" : "string"
          },
          "ca_storage_locations/idno" : {
            "type" : "string",
            "boost" : 100.0,
            "analyzer" : "keyword_lowercase"
          },
          "ca_storage_locations/type_id" : {
            "type" : "string"
          },
          "created" : {
            "type" : "date",
            "format" : "date_time_no_millis"
          },
          "created/administrator" : {
            "type" : "date",
            "format" : "strict_date_optional_time||epoch_millis"
          },
          "modified" : {
            "type" : "date",
            "format" : "date_time_no_millis"
          }
        }
      }
    }
  }
}
```

```
$ curl -XGET 'http://localhost:9200/collectiveaccess/_settings?pretty=1'
{
  "collectiveaccess" : {
    "settings" : {
      "index" : {
        "creation_date" : "1452531593932",
        "analysis" : {
          "analyzer" : {
            "keyword_lowercase" : {
              "filter" : "lowercase",
              "tokenizer" : "keyword"
            }
          }
        },
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "uuid" : "b3cWKAkRQam8ezDP5wQzXA",
        "version" : {
          "created" : "2010199"
        }
      }
    }
  }
}
```
</description><key id="125992854">15893</key><summary>NPE when running simple query_string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">stefankeidel</reporter><labels><label>:Core</label><label>bug</label><label>feedback_needed</label></labels><created>2016-01-11T17:14:08Z</created><updated>2016-01-11T19:11:46Z</updated><resolved>2016-01-11T19:11:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-11T19:07:35Z" id="170655117">Heya @skeidel 

You say you've nuked and recreated this index and you're seeing the same thing?  I tried creating the index with the same settings/mappings, but it wasn't enough to trigger the NPE.  Any chance you could provide (just enough) documents to recreate the problem?
</comment><comment author="clintongormley" created="2016-01-11T19:11:46Z" id="170656241">Never mind - already fixed in https://github.com/elastic/elasticsearch/pull/15506
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cardinality aggregation should not reserve a fixed amount of memory per bucket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15892</link><project id="" key="" /><description>The cardinality aggregation currently reserves a fixed amount of memory per bucket. While this makes memory management easier, this also has the downside of making buckets that don't collect many docs very memory-intensive.
</description><key id="125991998">15892</key><summary>Cardinality aggregation should not reserve a fixed amount of memory per bucket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label></labels><created>2016-01-11T17:09:42Z</created><updated>2017-05-16T08:27:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2017-04-27T13:07:26Z" id="297707923">This has been encountered by a user in https://github.com/elastic/elastisearch/issues/24359#issuecomment-297684702 since the parent terms aggregation was using global ordinals and this was causing a huge number of buckets to be pre-allocated
</comment><comment author="jay-dihenkar" created="2017-04-29T11:27:52Z" id="298163342">+1 : to fix this. ES May go OOM immediately if a user runs this kind of query while testing as mentioned in #24359 and it may be serious if env is PROD.</comment><comment author="colings86" created="2017-05-02T08:00:20Z" id="298540706">Just to clarify, when the mentioned bug occurs ES trips the circuit breaker and fails the request with a circuit breaker exception rather than ES throwing a OutOfMemoryError and bringing down the node. This is not to say that this isn't a bug or that this bug is less severe, I just want to clarify the behaviour of this bug</comment><comment author="JeffBolle" created="2017-05-15T16:28:12Z" id="301528389">I must have been doing something wrong, (maybe my circuit breakers were set incorrectly), but when I encountered this the OOM exception brought down ES on the impacted node. #24582 </comment><comment author="jay-dihenkar" created="2017-05-15T17:00:34Z" id="301537265">I would like to confirm here that on versions we tested: v5.2.2 and v5.3.0/1 - ES goes OOM even before the circuit breaker could react.</comment><comment author="JeffBolle" created="2017-05-15T17:50:46Z" id="301551626">@jay-dihenkar Thanks for confirming, I was running 5.4.0.
@colings86 The circuit breakers do NOT kick in causing just the request to fail.  The thread exits (please reference my logs in #24582), and in my case, Elasticsearch becomes unresponsive on the node. In my case this happened on every node in my cluster, I assume because they all had shards for this query, and it brought my cluster down. </comment><comment author="colings86" created="2017-05-16T08:18:56Z" id="301710187">@JeffBolle ok noted, I had not seen #24582 and was basing my comment on the stack trace from #24359 which is a circuit breaker exception. So as well as not reserving a fixed amount of memory per bucket in the cardinality aggregation we should also make sure that the cardinality and terms aggregations do a better job of reporting the amount of memory they are about to allocate to the circuit breaker</comment><comment author="jay-dihenkar" created="2017-05-16T08:27:31Z" id="301712123">I was able to capture the exception in #24359 only after a tweak to circuit breakers limit. Circuit breaker only came into play when I tweaked the settings as follows (after trial and errors): 
```
indices.breaker.total.limit: 50%
indices.breaker.request.limit: 50%
indices.requests.cache.size: 30%
indices.breaker.request.overhead: 550
```
Defaults didn't help prevent the crash.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update crud.asciidoc, nouns mixed up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15891</link><project id="" key="" /><description>Flipping nouns appropriately
</description><key id="125991576">15891</key><summary>Update crud.asciidoc, nouns mixed up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sromocki</reporter><labels><label>docs</label></labels><created>2016-01-11T17:07:48Z</created><updated>2016-01-11T19:00:43Z</updated><resolved>2016-01-11T19:00:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-11T19:00:43Z" id="170653326">thanks @sromocki 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes TopHitsBuilder in place of TopHitsAggregator.Factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15890</link><project id="" key="" /><description>This follows on from #15821 and requires that PR is merged first
</description><key id="125973054">15890</key><summary>Removes TopHitsBuilder in place of TopHitsAggregator.Factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-01-11T15:49:09Z</created><updated>2016-01-14T16:46:36Z</updated><resolved>2016-01-14T14:57:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-14T13:43:32Z" id="171647795">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch windows service crashes when stopping it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15889</link><project id="" key="" /><description>When stopping Elasticsearch 2.1 windows service on windows 2008 R1, windows 2008 R2' windows 2012 and windows10, the following Application Error is logged in the Windows event log indicating an access violation in Java. I stopped it both using the service.bat file, and from services.msc. 
As you can see below I am using jdk 8 update 45.
From what I have seen Elasticsearch keeps working fine when starting it again, but this crash is still not supposed to happen.

There is also an open question about it in stackoverflow - http://stackoverflow.com/questions/28966966/elasticsearch-shutdown-crash-when-run-as-windows-service

Faulting application name: elasticsearch-service-x64.exe, version: 1.0.15.0, time stamp: 0x51543b9d
Faulting module name: jvm.dll, version: 25.45.0.2, time stamp: 0x55428cbf
Exception code: 0xc0000005
Fault offset: 0x0000000000213298
Faulting process id: 0x2b7c
Faulting application start time: 0x01d14c83385f6463
Faulting application path: C:\temp\elasticsearch-2.1.1\elasticsearch-2.1.1\bin\elasticsearch-service-x64.exe
Faulting module path: c:\Program Files\Java\jdk1.8.0_45\jre\bin\server\jvm.dll
Report Id: 860f851d-b5d6-46ff-8a83-c05f54445d92
Faulting package full name: 
Faulting package-relative application ID: 
</description><key id="125969344">15889</key><summary>Elasticsearch windows service crashes when stopping it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">famini</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2016-01-11T15:31:10Z</created><updated>2017-05-26T12:25:40Z</updated><resolved>2016-03-01T12:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T12:25:14Z" id="190698779">Based on http://www.oracle.com/technetwork/java/javase/crashes-137240.html this sounds like a bug in the JVM.  I can only suggest upgrading the JVM, and if this issue persists, reporting the issue to Oracle.
</comment><comment author="prabhurajreddy" created="2017-03-02T11:23:43Z" id="283627967">The issue still persists in the latest elastic search 5.2.2 and jdk1.8.0_91. It crashes during the service stop. 
Below is the windows error 

Faulting application name: elasticsearch-service-x64.exe, version: 1.0.15.0, time stamp: 0x51543b9d
Faulting module name: jvm.dll, version: 25.71.0.1, time stamp: 0x56fe319b
Exception code: 0xc0000005
Fault offset: 0x0000000000215498
Faulting process id: 0xad38
Faulting application start time: 0x01d29341d48a1bf5
Faulting application path: C:\temp\elasticsearch-5.2.2\bin\elasticsearch-service-x64.exe
Faulting module path: C:\Program Files\Java\jdk1.8.0_91\jre\bin\server\jvm.dll
Report Id: 5c68cf80-260c-4f9c-a8fb-e4b6dc0dc69f
Faulting package full name: 
Faulting package-relative application ID:  </comment><comment author="mgruben" created="2017-03-21T13:50:48Z" id="288083805">Ditto:

```
Faulting application name: elasticsearch-service-x64.exe, version: 1.0.15.0, time stamp: 0x51543b9d 
Faulting module name: jvm.dll, version: 24.65.0.4, time stamp: 0x53d28874 
Exception code: 0xc0000005 
Fault offset: 0x00000000001d0128 
Faulting process id: 0x610 
Faulting application start time: 0x01d27e5ae4eb53d3 
Faulting application path: C:\Program Files\AccessData\ElasticSearch\bin\elasticsearch-service-x64.exe 
Faulting module path: C:\Program Files\AccessData\ElasticSearch\jdk\jre\bin\server\jvm.dll 
Report Id: cb60e70b-0aec-11e7-8103-000e1eb547e0 
Faulting package full name: 
Faulting package-relative application ID: 
```</comment><comment author="jasontedor" created="2017-03-21T13:56:08Z" id="288085411">@mgruben You're using a Java 7 runtime (I can tell from the Hotspot version 24.65.0.4). Also, it's best to post topics on the [forum](https://discuss.elastic.co) for general help and questions rather than posting on closed issues.</comment><comment author="mgruben" created="2017-03-21T14:07:14Z" id="288088798">@jasontedor Noted, I'll work with our vendor to see if we can upgrade to the most current JVM (currently 8u121).  
Thanks!</comment><comment author="hordemark" created="2017-05-16T03:49:00Z" id="301667844">@mgruben  I encountered this issue, 
My elasticsearch is 1.7.3 and JRE is 1.8.131. 
But still have problem. Do you have a workaround?
Thanks!</comment><comment author="clintongormley" created="2017-05-26T12:25:39Z" id="304270146">&gt; My elasticsearch is 1.7.3 and JRE is 1.8.131.
But still have problem. Do you have a workaround?

Upgrade</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes the simple metric builders in place of AggFactory implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15888</link><project id="" key="" /><description>This follows on from https://github.com/elastic/elasticsearch/pull/15821 and requires that PR is merged first
</description><key id="125964294">15888</key><summary>Removes the simple metric builders in place of AggFactory implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-01-11T15:06:40Z</created><updated>2016-01-14T16:46:40Z</updated><resolved>2016-01-14T14:56:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-14T13:40:45Z" id="171647227">I left some comments. This change also makes me think that we should move factories to their own classes in all cases since it's weird today to have to import Aggregator impls which are internal to the server side.
</comment><comment author="colings86" created="2016-01-14T14:46:55Z" id="171661686">@jpountz thanks for the review. I think you are right with the generics not being needed so I have removed them from the Cardinality, Missing and ValuesSource Aggregator Factories (which are all the metric factory classes which had been made generic)

I also agree with moving the factories to their own classes and had already intended to do that but I wanted to do it at the end of the refactoring of the aggregations so it makes the diffs easier to review. Does that work for you?
</comment><comment author="jpountz" created="2016-01-14T14:51:12Z" id="171662812">Totally!
</comment><comment author="jpountz" created="2016-01-14T14:51:31Z" id="171662879">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[WIP] Make FieldSortBuilder serializable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15887</link><project id="" key="" /><description>Warning: This is early work in progress posted for feedback.

Final goal as described in #15178 is to make all sorting serializable. The initial state of this PR however only adds the Writable interface to SortBuilder, fixes FieldSortBuilder and adds a roundtrip serialisation test (mostly inspir^Wcopied from the highlighter refactoring) for it.

What's missing: 
- Fix other classes implementing SortBuilder and add respective unit tests.
- Migrate the logic that generates the actual Lucene sort objects over from SortParseElement and friends.

@cbuescher if you have time for feedback (as in "yeah, that looks like a good way forward" or as in "no, stop - before going out and changing another handful of classes let's fix issues x, y, and z with the general approach first") that would be appreciated.
</description><key id="125940685">15887</key><summary>[WIP] Make FieldSortBuilder serializable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label><label>:Search Refactoring</label><label>WIP</label></labels><created>2016-01-11T13:10:11Z</created><updated>2016-01-21T19:12:30Z</updated><resolved>2016-01-21T15:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-01-12T12:30:42Z" id="170897126">Forgot to include one piece of IMHO important information in the initial comment: I didn't copy the original xcontent parsing code over*, so this will need quite some testing if kept as is.

. \* Original code resides (if I read the code correctly) in SortParseElement, but parses more than what the FieldSortBuilder generates. I tried pulling the logic apart but after a couple of hours/days gave up as it seemed too hairy to do in the first round at least. Maybe I'm missing something obvious here - maybe doing the other sort builders turns something obvious up?
</comment><comment author="cbuescher" created="2016-01-12T15:26:54Z" id="170946733">@MaineC had a look at the current changes and I also found it hard to understand what the SortParseElement currently does (pre-refactoring), so +1 on not trying to pull that apart, rather try to replicate the same parser logic in each SortBuilders new fromXContent() method (that can get quiet tedious, I imagine). As far as testing goes, maybe when you have the fromXContent and build() methods (returning a SortField), then hopefully there's a way of checking whether the old SortParseElement adds the same SortField to the search context as the new SortBuilder produces via its build() method. 
</comment><comment author="MaineC" created="2016-01-13T12:49:37Z" id="171281323">@cbuescher thanks for your feedback.

&gt; As far as testing goes, maybe when you have the fromXContent and build() methods (returning a 
&gt; SortField), then hopefully there's a way of checking whether the old SortParseElement adds the same 
&gt; SortField to the search context as the new SortBuilder produces via its build() method.

Yes, that's what I had in mind as well. As mentioned f2f the only thing that worries me with that approach would be how to best cover backwards compatibility. If we base our tests just on the json that is being generated we won't cover cases that currently parse well but are deprecated.
</comment><comment author="MaineC" created="2016-01-13T14:59:25Z" id="171321026">After looking into the GeoDistanceSortBuilder yesterday afternoon and this morning I came across another issue with XContent generation/parsing roundtrip testing. After talking via zoom with @cbuescher earlier today summarizing our discussion here, so also @colings86 can have a look.

While the issue appeared in the XContent roundtrip test first, it seems to point to a bigger issue down the road. The explanation is a bit lengthy though, so please bear with me. Here goes:

When converting a FieldScoreBuilder to XContent this is what I get with the original code in pretty printed json:

```
"FgDAl" : {
    "order" : "asc",
    "missing" : "_first",
    "nested_filter" : {
        "term" : {
            "ZXfgYMnv" : {
                "value" : "nnd",
                "boost" : 7.2023077
            }
        }
    }
}
```

For it to be valid json I add another pair of curly braces in the test, pass it to the newly written XContent parser - all fine.

Now for the GeoDistanceScoreBuilder this is the XContent that gets generated:

```
"_geo_distance" : {
    "XBe" : [ "qp3ttuc5hnvh", "7mceczpz6jj9", "1pw7spkm9c3x", "g04huu185y27" ],
    "unit" : "m",
    "distance_type" : "arc",
    "reverse" : true,
    "nested_path" : "DrptiKA",
    "coerce" : false,
    "ignore_malformed" : false
}
```

The structure is slightly different - note the reference to "_geo_distance" above. Reason: While FieldScores are being directly parsed in SortParseElement, for geo distance sorting parsing is delegated here: https://github.com/elastic/elasticsearch/blob/76fa9023b6378681de34672d2e94227e8b464cfd/core/src/main/java/org/elasticsearch/search/sort/SortParseElement.java#L142

This difference caused a glitch in the XContent roundtrip unit test: While I could send the json generated by FieldScoreBuilder directly into the parsing method, the code I extracted into fromXContent method from the GeoDistanceScoreParser class would break when confronted with json that still contains the name of the parser (rightly so, because outside the test that would have been consumed by SortParseElement before being handed to the Parser).

I could easily add an exception in how the test is being executed for GeoDistanceScoreBuilder (and ScriptScoreBuilder which probably looks similar).

However when pulling the current parsing code apart and moving the separate code snippets closer to their respective builders we will run into a problem: We need to figure out which builder's parser to call. Easy for the GeoDistanceScoreBuilder, it has a name attached. When keeping the current query structure what we'd need to do for the FieldScoreBuilder is to try and parse the structure, if we encounter something we don't know, we would assume it's a fieldname and assume we need to use the FieldScoreBuilder's parse method. Problem: At that point in time we already consumed the fieldname from the json snippet.

Solution 1: Write the parse method signature such that we can pass the already parsed field name into it.

Solution 2: Instead of generating the FieldScoreBuilder object withing the parse method, construct it internally, initialise it with the fieldname and give that pre-initialised version to the parser.

Solution 3: Always require the sort entries to be named. However that would mean breaking the currently supported query DSL. Not sure if we want that @clintongormley 

@colings86 @cbuescher Not sure which of the three we should choose, or if there's another solution that is better.

Not sure if the above writeup was clear enough to understand - happy to add any clarifications if needed.
</comment><comment author="colings86" created="2016-01-13T16:11:16Z" id="171345417">I think I understand the problem correctly, and my gut feeling is to go for solution 1. We can pass `_geo_distance` to the GeoDistanceSortBuilder (did you mean SortBuilder above, you have written ScoreBuilder but this is for sorting right?) to it's parse method and ignore it if necessary so that the calling class doesn't need to know the details of the implementation.
</comment><comment author="MaineC" created="2016-01-14T11:29:21Z" id="171618142">Ok - makes sense.

And yeah, I meant to write SortBuilder.
</comment><comment author="MaineC" created="2016-01-21T15:06:51Z" id="173599009">Closing in favour of #16151 and #16127 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove use of request headers/context for pipeline id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15886</link><project id="" key="" /><description>Remove use of request headers/context for pipeline id in favour of instance members added to IndexRequest and BulkRequest

Now that the ingest infra is part of es core we can remove some code that was required by the plugin and have a better integration with es core. We allow to specify the pipeline id in bulk and index as a request parameter, we have a REST filter that parses it and adds it to the relevant action request. That is not required anymore, as we can add this logic to RestIndexAction and RestBulkAction directly, no need for a filter. Also, we can allow to specify a pipeline id for each index requests in a bulk request. The small downside of this is that the ingest filter has to go over each item of a bulk request, all the time, to figure out whether they have a pipeline id.
</description><key id="125931829">15886</key><summary>remove use of request headers/context for pipeline id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-11T12:23:56Z</created><updated>2016-01-11T18:05:30Z</updated><resolved>2016-01-11T18:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-11T16:33:02Z" id="170608292">@martijnvg  I pushed a couple of new commits, can you have another look please?
</comment><comment author="martijnvg" created="2016-01-11T17:00:48Z" id="170618187">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated links on scripting reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15885</link><project id="" key="" /><description>The links in https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-scripting.html seem to point to deprecated repos (it is noted that they will only receive bug fixes).

I've changed them to point to their current locations.

This pull in my opinion is unfinished as there are a few questions on #15883 which could do with thinking on first.
</description><key id="125931350">15885</key><summary>Updated links on scripting reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NDevox</reporter><labels><label>docs</label></labels><created>2016-01-11T12:20:26Z</created><updated>2016-01-12T13:38:52Z</updated><resolved>2016-01-12T13:28:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-11T18:48:18Z" id="170650005">Thanks for the PR @NDevox 

If you could make the changes I've suggested, I'll be happy to merge it in
</comment><comment author="NDevox" created="2016-01-11T20:49:18Z" id="170685553">Makes a lot of sense to me, I'll make the changes - and as pointed out in #15883 I'll remove MVEL as well.
</comment><comment author="NDevox" created="2016-01-11T20:58:28Z" id="170687801">Note - I've not used asciidocs before, so wasn't sure if `{plugins}` is useable in a link - so simply put in the full link for now.

If that is wrong let me know and I will update.
</comment><comment author="clintongormley" created="2016-01-12T10:34:10Z" id="170868862">@NDevox yes you should use `{plugins}`, the replacement is defined in this file: https://raw.githubusercontent.com/elastic/elasticsearch/master/docs/reference/index.asciidoc
</comment><comment author="NDevox" created="2016-01-12T10:37:21Z" id="170869806">@clintongormley updated.
</comment><comment author="clintongormley" created="2016-01-12T13:28:34Z" id="170911579">thanks @NDevox 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated links on scripting reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15884</link><project id="" key="" /><description>The links in https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-scripting.html seem to point to deprecated repos (it is noted that they will only receive bug fixes).

I've changed them to point to their current locations.

This pull in my opinion is unfinished as there are a few questions on #15883 which could do with thinking on first.
</description><key id="125929486">15884</key><summary>Updated links on scripting reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NDevox</reporter><labels /><created>2016-01-11T12:09:07Z</created><updated>2016-01-11T12:19:54Z</updated><resolved>2016-01-11T12:19:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="NDevox" created="2016-01-11T12:13:27Z" id="170523277">Signed CLA
</comment><comment author="NDevox" created="2016-01-11T12:19:54Z" id="170524275">I'll close and open another one to see that the CLA's pass.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting plugin guidance seems out of date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15883</link><project id="" key="" /><description>The links currently point to deprecated repositories (they are noted to only receive bugfixes).

example:

https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-scripting.html

Python points to:
https://github.com/elastic/elasticsearch-lang-python

which in turn tells you to look at:
https://github.com/elastic/elasticsearch/tree/master/plugins/lang-python

I appreciate the links might still point there due to the guidance for installing the plugins being held there. But if they are deprecated isn't it better to move the guidance and redirect the links to the current folder?

As an aside, MVEL is still on the master branch despite being removed from 2.0 onwards. Should this be kept?
</description><key id="125929342">15883</key><summary>Scripting plugin guidance seems out of date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NDevox</reporter><labels><label>docs</label></labels><created>2016-01-11T12:07:54Z</created><updated>2016-01-12T13:32:05Z</updated><resolved>2016-01-12T13:32:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="NDevox" created="2016-01-11T12:21:02Z" id="170524445">First PR failed the CLA check so signed and opened another.
</comment><comment author="NDevox" created="2016-01-11T12:22:34Z" id="170524696">Worth noting that the readme at the deprecated repo (https://github.com/elastic/elasticsearch-lang-python) talks about building from source and points to out-of-date resources so is fairly misleading. Especially considering this repo hasn't seen development for 4 months whereas the up-to-date repo has seen development in the last few days.
</comment><comment author="clintongormley" created="2016-01-11T18:48:57Z" id="170650170">I'd remove the MVEL link as well - it's long gone :)

thanks
</comment><comment author="NDevox" created="2016-01-11T20:59:19Z" id="170688028">removed in pull #15885 
</comment><comment author="NDevox" created="2016-01-12T13:32:05Z" id="170912521">Closed by #15885 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check lenient_expand_open after aliases have been resolved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15882</link><project id="" key="" /><description>We fail today with ClusterBlockExceptions if an alias expands to a closed index
during search since we miss to check the index option down the road after we expanded
aliases.

Closes #13278
</description><key id="125923415">15882</key><summary>Check lenient_expand_open after aliases have been resolved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Aliases</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-11T11:28:56Z</created><updated>2016-01-22T11:10:03Z</updated><resolved>2016-01-11T12:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-11T11:43:18Z" id="170516564">LGTM thanks a lot @s1monw 
</comment><comment author="martijnvg" created="2016-01-11T12:08:44Z" id="170521736">LGTM. Thanks @s1monw for fixing this.
</comment><comment author="s1monw" created="2016-01-11T13:05:18Z" id="170542800">pushed to all relevant branches...
</comment><comment author="oferbar" created="2016-01-11T20:35:20Z" id="170681946">Hi, is this also fixed for 1.6 or just 2.x?
Many thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Initialize translog before scheduling the sync to disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15881</link><project id="" key="" /><description>Otherwise Translog.current might be null when we call syncNeeded().

I saw this in the log of  a failed test here: http://build-us-00.elastic.co/job/es_core_21_metal/326/consoleText
Unfortunately it is not the reason for the test failure.
</description><key id="125921314">15881</key><summary>Initialize translog before scheduling the sync to disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Translog</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-11T11:13:21Z</created><updated>2016-01-22T11:10:03Z</updated><resolved>2016-01-11T13:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-11T11:16:51Z" id="170511641">good catch. LGTM. Doubting about the extra test.
</comment><comment author="s1monw" created="2016-01-11T11:25:35Z" id="170512950">LGTM too I agree the test is optional but not much of a cost either
</comment><comment author="brwe" created="2016-01-11T11:26:09Z" id="170513049">what now? remove it or not?
</comment><comment author="s1monw" created="2016-01-11T11:26:25Z" id="170513103">flip a coin
</comment><comment author="bleskes" created="2016-01-11T11:28:01Z" id="170513391">When in doubt - less is better :)

&gt; On 11 Jan 2016, at 12:26, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; flip a coin
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="brwe" created="2016-01-11T12:43:53Z" id="170535450">I flipped a coin but then was too clumsy to catch it...so I tried if we have failures in unit tests without that one and there were many so I decided to remove this test. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch fails to start tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15880</link><project id="" key="" /><description>Hi folks,

I'm trying to start tribe node using following config:

```
network.host: 10.63.19.34
tribe:
    e6001:
        #path.conf: /home/elk/etc/tribe-client
        cluster.name: es-test
        discovery.zen.ping.timeout: 100s
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.63.19.36", "10.63.19.37"]
    e6002:
        #path.conf: /home/elk/etc/tribe-client
        cluster.name: es-testold
        discovery.zen.ping.timeout: 100s
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.hosts: ["10.63.19.34"]
    blocks:
        write:    true
        metadata: true
```

This config resides in the file `~/etc/elasticsearch_tribe/elasticsearch.yml`. I'm starting it using following command:

```
/home/elk/running/elasticsearch/bin/elasticsearch  --path.conf /home/elk/etc/elasticsearch_tribe
```

Elasticsearch fails with following output:

2.1.1

```
/home/elk/running/elasticsearch/bin/elasticsearch  --path.conf /home/elk/etc/elasticsearch_tribe
[2016-01-11 18:14:07,952][WARN ][bootstrap                ] unable to install syscall filter: prctl(PR_GET_NO_NEW_PRIVS): Invalid argument
[2016-01-11 18:14:08,312][INFO ][node                     ] [Great Gambonnos] version[2.1.1], pid[21083], build[40e2c53/2015-12-15T13:05:55Z]
[2016-01-11 18:14:08,312][INFO ][node                     ] [Great Gambonnos] initializing ...
[2016-01-11 18:14:08,686][INFO ][plugins                  ] [Great Gambonnos] loaded [elasticsearch-analysis-ik], sites []
[2016-01-11 18:14:11,782][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/mydict.dic
[2016-01-11 18:14:11,783][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/single_word_low_freq.dic
[2016-01-11 18:14:11,787][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/ext_stopword.dic
[2016-01-11 18:14:12,705][INFO ][node                     ] [Great Gambonnos/e6001] version[2.1.1], pid[21083], build[40e2c53/2015-12-15T13:05:55Z]
[2016-01-11 18:14:12,706][INFO ][node                     ] [Great Gambonnos/e6001] initializing ...
[2016-01-11 18:14:13,080][INFO ][plugins                  ] [Great Gambonnos/e6001] loaded [elasticsearch-analysis-ik], sites []
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/home/elk/running/elasticsearch/config/hunspell" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(UnixFileAttributeViews.java:49)
    at sun.nio.fs.UnixFileSystemProvider.readAttributes(UnixFileSystemProvider.java:144)
    at sun.nio.fs.LinuxFileSystemProvider.readAttributes(LinuxFileSystemProvider.java:99)
    at java.nio.file.Files.readAttributes(Files.java:1737)
    at java.nio.file.Files.isDirectory(Files.java:2192)
    at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:127)
    at org.elasticsearch.indices.analysis.HunspellService.&lt;init&gt;(HunspellService.java:102)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.tribe.TribeClientNode.&lt;init&gt;(TribeClientNode.java:35)
    at org.elasticsearch.tribe.TribeService.&lt;init&gt;(TribeService.java:140)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    &lt;&lt;&lt;truncated&gt;&gt;&gt;
```

2.1.0

```
/home/elk/running/elasticsearch/bin/elasticsearch  --path.conf /home/elk/etc/elasticsearch_tribe
[2016-01-11 18:17:05,196][WARN ][bootstrap                ] unable to install syscall filter: prctl(PR_GET_NO_NEW_PRIVS): Invalid argument
[2016-01-11 18:17:05,562][INFO ][node                     ] [Lasher] version[2.1.0], pid[22340], build[72cd1f1/2015-11-18T22:40:03Z]
[2016-01-11 18:17:05,562][INFO ][node                     ] [Lasher] initializing ...
[2016-01-11 18:17:06,120][INFO ][plugins                  ] [Lasher] loaded [license, marvel-agent, ${elasticsearch.plugin.name}], sites [kopf]
[2016-01-11 18:17:09,358][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/mydict.dic
[2016-01-11 18:17:09,366][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/single_word_low_freq.dic
[2016-01-11 18:17:09,372][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/ext_stopword.dic
Exception in thread "main" java.security.AccessControlException: access denied ("java.io.FilePermission" "/home/elk/running/elasticsearch/config/elasticsearch.yml" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at sun.nio.fs.UnixPath.checkRead(UnixPath.java:795)
    at sun.nio.fs.UnixFileSystemProvider.checkAccess(UnixFileSystemProvider.java:290)
    at java.nio.file.Files.exists(Files.java:2385)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:86)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:135)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.tribe.TribeService.&lt;init&gt;(TribeService.java:140)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

```
java -version
java version "1.8.0_66"
Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)
cat /etc/debian_version
7.9
```

Please advise how this can be resolved.

Thinks
</description><key id="125913435">15880</key><summary>elasticsearch fails to start tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zcola</reporter><labels /><created>2016-01-11T10:19:57Z</created><updated>2016-01-27T09:24:30Z</updated><resolved>2016-01-11T16:55:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-11T16:55:12Z" id="170616652">The problem is due to the fact that your configuration tries to load files (/home/elk/running/elasticsearch/config/hunspell, /home/elk/running/elasticsearch/config/elasticsearch.yml) from outside of your conf directory (/home/elk/etc/elasticsearch_tribe), which is forbidden for security reasons.
</comment><comment author="clintongormley" created="2016-01-11T18:59:07Z" id="170652900">@jpountz i'm just wondering if the tribe node itself is trying to load the hunspell dictionary?
</comment><comment author="rjernst" created="2016-01-11T21:17:10Z" id="170692407">@clintongormley Yes, the tribe node is a "normal" node right now (still initializes all other node services, include the analysis module, which creates the hunspell service). It also seems here path.conf was not read at the right time, since something in the 2.1.0 case tried reading elasticsearch.yml from the default conf dir.
</comment><comment author="zcola" created="2016-01-12T01:14:10Z" id="170749128">@jpountz But the Starting method is used to start the `data node` or `master node` can be started successfully. And I did not `/home/elk/running/elasticsearch/config/hunspell` this file, hunspell this error does not occur in `2.1.1`, not `2.1.0`.
</comment><comment author="zcola" created="2016-01-12T01:16:33Z" id="170749896">@rjernst Your advice?
</comment><comment author="zcola" created="2016-01-12T02:21:11Z" id="170761040"> `--path.conf` parameter is not used can be started successfully, but can only start from the default configuration path location

```
 /home/elk/running/elasticsearch/bin/elasticsearch
home/elk/running/elasticsearch/bin/elasticsearch
[2016-01-12 10:18:38,017][WARN ][bootstrap                ] unable to install syscall filter: prctl(PR_GET_NO_NEW_PRIVS): Invalid argument
[2016-01-12 10:18:38,288][INFO ][node                     ] [Arishem the Judge] version[2.1.0], pid[25632], build[72cd1f1/2015-11-18T22:40:03Z]
[2016-01-12 10:18:38,289][INFO ][node                     ] [Arishem the Judge] initializing ...
[2016-01-12 10:18:38,827][INFO ][plugins                  ] [Arishem the Judge] loaded [license, marvel-agent, ${elasticsearch.plugin.name}], sites [kopf]
[2016-01-12 10:18:41,745][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/mydict.dic
[2016-01-12 10:18:41,745][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/single_word_low_freq.dic
[2016-01-12 10:18:41,752][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/ext_stopword.dic
[2016-01-12 10:18:42,300][INFO ][node                     ] [Arishem the Judge/e6001] version[2.1.0], pid[25632], build[72cd1f1/2015-11-18T22:40:03Z]
[2016-01-12 10:18:42,300][INFO ][node                     ] [Arishem the Judge/e6001] initializing ...
[2016-01-12 10:18:42,464][INFO ][plugins                  ] [Arishem the Judge/e6001] loaded [license, marvel-agent, ${elasticsearch.plugin.name}], sites [kopf]
[2016-01-12 10:18:43,045][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/mydict.dic
[2016-01-12 10:18:43,046][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/single_word_low_freq.dic
[2016-01-12 10:18:43,049][INFO ][ik-analyzer              ] [Dict Loading]ik/custom/ext_stopword.dic
[2016-01-12 10:18:43,120][INFO ][node                     ] [Arishem the Judge/e6001] initialized
[2016-01-12 10:18:43,125][INFO ][node                     ] [Arishem the Judge/e6002] version[2.1.0], pid[25632], build[72cd1f1/2015-11-18T22:40:03Z]
[2016-01-12 10:18:43,125][INFO ][node                     ] [Arishem the Judge/e6002] initializing ...
```
</comment><comment author="clintongormley" created="2016-01-27T09:24:30Z" id="175506443">@zcola i finally understand what you mean.  I've opened #16253 to address this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change link for HDFS support to plugins docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15879</link><project id="" key="" /><description>I came to this change when I read https://github.com/elastic/elasticsearch/pull/15591

HDFS plugin link has not been updated when we moved HDFS to elasticsearch repository (#15192).
</description><key id="125896591">15879</key><summary>Change link for HDFS support to plugins docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository HDFS</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-01-11T08:43:56Z</created><updated>2016-01-11T10:07:59Z</updated><resolved>2016-01-11T09:20:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-11T08:44:33Z" id="170468369">@clintongormley @costin Could you review this small doc update?
</comment><comment author="costin" created="2016-01-11T08:59:02Z" id="170472361">LGTM. Should be backported to 2.3 as well.
</comment><comment author="dadoonet" created="2016-01-11T09:23:28Z" id="170481834">@costin Thanks. Merged in master. Regarding 2.x, are you sure the plugin is there? I looked at https://github.com/elastic/elasticsearch/tree/2.x/plugins but can't see it. https://github.com/elastic/elasticsearch/tree/2.x/docs/plugins neither.
</comment><comment author="costin" created="2016-01-11T09:57:11Z" id="170493447">Nevermind - 2.x is still 2.2 (I think) hence why it is not there yet.
</comment><comment author="dadoonet" created="2016-01-11T10:07:59Z" id="170497225">I don't think you backported yet your plugin to 2.x branch. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RED in Elasticsearch status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15878</link><project id="" key="" /><description>Why Elasticsearch in a determinated moment pass to red state whithout an aparent problem?

I have the heap size well dimensionated and after I have three nodes that they are communicating between them with Unicast.

It was working for a month without any problem and it was indexing all time.
</description><key id="125890242">15878</key><summary>RED in Elasticsearch status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juandasgandaras</reporter><labels /><created>2016-01-11T07:52:50Z</created><updated>2016-01-11T08:49:22Z</updated><resolved>2016-01-11T08:49:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-11T08:49:22Z" id="170469077">The place to look for the answer is in the logs.  Questions like these should be asked on the forum http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make IndexingMemoryController private to IndicesService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15877</link><project id="" key="" /><description>This commit detaches the IndexingMemoryController from guice and moves
it's creation and closing into IndicesService where it logically belongs.
</description><key id="125840687">15877</key><summary>Make IndexingMemoryController private to IndicesService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-10T19:45:37Z</created><updated>2016-01-11T09:07:02Z</updated><resolved>2016-01-10T20:33:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-10T20:04:09Z" id="170386935">LGTM
</comment><comment author="jasontedor" created="2016-01-10T20:07:23Z" id="170387585">Yay! LGTM.
</comment><comment author="bleskes" created="2016-01-11T09:07:02Z" id="170476603">late to the game , but +1 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for precise cardinality aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15876</link><project id="" key="" /><description>While current cardinality aggregation works fast and supports very large datases it is not precise. There are many use cases where approximation is not acceptable and where cardinality is sufficiently low (and known) to allow a simple deterministic algorithms to work and to provide precise result with reasonable time and memory use.

It would be very useful if elastic supported a secondary simple algorithm to provide precise cardinality calculation and let users decide which one to use depending on their needs
</description><key id="125835704">15876</key><summary>Support for precise cardinality aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-01-10T18:13:53Z</created><updated>2017-03-09T18:27:47Z</updated><resolved>2016-03-01T12:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T21:31:37Z" id="170396153">@roytmana what about using the `precision_threshold` for these low cardinality fields?  See https://www.elastic.co/guide/en/elasticsearch/reference/2.1/search-aggregations-metrics-cardinality-aggregation.html#_precision_control
</comment><comment author="roytmana" created="2016-01-11T15:21:19Z" id="170585352">@clintongormley two issues
1. No matter how high it was, for me it was not 100% precise (difference was by 1 document) even when I set it to be higher than actual cardinality
2. When I have it as a nested aggregation and set a fairly high threshold (because there are few buckets in parent agg that are much bigger than others) and the parent aggregation generates fair number of buckets I ended up with memory circuit breaker kicking in. So in general case when nesting etc it can end up more memory intensive (bacause it is fixed to high number independently of its bucket) that a very trivial algorithm of accumulating unique values  
</comment><comment author="jpountz" created="2016-01-11T17:04:43Z" id="170619161">Accurate cardinalities can't be reasonably computed in a distributed index as it would require to forward all unique values to the coordinating node. Since we don't want to have features that only work in a single-shard setup, I'm afraid this means we can't support this feature.

Just to check that this is not a bug in the current impl: how many unique values does your field have?
</comment><comment author="jpountz" created="2016-01-11T17:10:23Z" id="170621329">&gt; when nesting etc it can end up more memory intensive (bacause it is fixed to high number independently of its bucket) that a very trivial algorithm of accumulating unique values 

I'm interested in exploring this problem however. I opened #15892.
</comment><comment author="roytmana" created="2016-01-11T19:00:43Z" id="170653323">@jpountz I understand but would like to make another plea :-) since my customers are rather unhappy abut it  

Elastic does forward shard search results or aggregations to coordinating node for merging why not the unique values? If a user needs precise count and that's the only way to do it, he/she would find the cost in processing/memory/networking acceptable and ensure the cardinality is not out of reasonable range before using this algorithm. As of now there is no way to do it at all no matter at what the cost is.

Even if you make the existing algorithm more memory efficient, it still does not guarantee exactness even what looks like with thresholds higher than actual cardinality. Is that the correct understanding (it is what I seem to observe anyways)? 
</comment><comment author="roytmana" created="2016-01-11T19:07:58Z" id="170655219">I know it is probably not practical to do but here is a silly question anyway: 
In a single node multi-shard scenario some algorithms such as this or guaranteed result of term agg when sorted by count could be entirely practical and fairly easy to implement. Would elastic care to support some better algorithms/guarantees for single node multishard scenario? This scenario covers in very many cases traditional data mart solution where amount of data is sizeable but not huge and would fit to a powerful multicore server just fine but typical data mart users needs exact values in majority of cases and thus approximating algorithms or missing value in term aggregation because of shard count "conflict" are not an option
</comment><comment author="jpountz" created="2016-01-14T09:33:08Z" id="171585373">&gt; Even if you make the existing algorithm more memory efficient, it still does not guarantee exactness even what looks like with thresholds higher than actual cardinality

True. Actually even below the treshold the count could be wrong, it's just much more unlikely and the error should be low.

&gt; Would elastic care to support some better algorithms/guarantees for single node multishard scenario?

We have a policy that all features that we implement need to work and scale in a distributed environment, so I'm afraid we would not want to do this.

For the record, it is possible to compute exact counts on client side by initiating a scroll and maintaining a set of all unique field values that have been seen.
</comment><comment author="roytmana" created="2016-01-14T18:33:53Z" id="171732629">If ibhad to resort to that I will term aggregate on that field and then count entries this plays well being a subagg at least. It is ironic that elastic would support that but not cardinality :-) I guess I can look into pipeline and see if I can replace terms with the count
</comment><comment author="clintongormley" created="2016-03-01T12:16:26Z" id="190692526">&gt; We have a policy that all features that we implement need to work and scale in a distributed environment, so I'm afraid we would not want to do this.

Closing
</comment><comment author="polkovnikov-ph" created="2016-07-05T10:20:01Z" id="230441408">I have the same issue and would like to ask how cardinality aggregation's `precision_threshold` works. Are you using Bloom filter (or one of its variations)? Does `precision_threshold` stand for its size?

_Edit._ [Here's an article](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/40671.pdf) on HyperLogLog++ that is used in ES. But I still don't understand what is `precision_threshold`. An article says "... for precision 14 and use LinearCounting to the left ...", so it's in order of tens, not thousands. Should it be set to a maximal expected value of aggregation?
</comment><comment author="jpountz" created="2016-08-18T15:41:17Z" id="240764479">@polkovnikov-ph the internal HLL++ parameter is a bit opaque so we used something slightly more meaningful. Here is how we translate from the `precision_threshold` to HLL++'s precision if you are interested: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/metrics/cardinality/HyperLogLogPlusPlus.java#L67
</comment><comment author="yuanlianxi" created="2016-08-29T11:03:37Z" id="243094893">@jpountz what about the precision_threshold max value be configurable in the future ES release
</comment><comment author="aalexgabi" created="2016-11-24T18:39:45Z" id="262830515">@jpountz We currently have the same issue with one little caveat: we want to compute the cardinality for a field value that is guaranteed to be on the same shard because the routing is based on it. More precisely we want to compute the cardinality for each parent id.

</comment><comment author="xzer" created="2017-01-23T10:22:54Z" id="274449889">I got a workaround for this case:

```json
{
    "byFullListScripting": {
      "terms": {
        "field": "groupId",
        "shard_size": Integer.MAX_VALUE,
        "size": Integer.MAX_VALUE
      },
      "aggs": {
        "cntScripting": {
          "scripted_metric": {
            "map_script": "targetId='u'+doc['cntTargetId']; if (_agg[targetId] == null) { _agg[targetId] = 1}",
            "reduce_script": "map=[:]; for (a in _aggs){ map.putAll(a) }; return map.size()"
          }
        }
      }
}
```

Some comments:

1.  inline script must be true in config file or use filed script.
1.  if the "cntTargetId" is a string rather than a number, it is not necessary to plus 'u' to the id, which is just tell the script engine that our key is a map key rather than an array index.
1. unfortunately ES does not support ordering by scripted sub aggregation, you need to sort the returned bucket list by yourself.
1. to get precise aggregation you have to set shard_size and size to Integer.MAX_VALUE to make all the data to be retrieved.
1. **It is really very expensive operation, take the responsibility by yourself**


</comment><comment author="DWilches" created="2017-03-09T18:26:17Z" id="285436872">Same here. Without an alternative to exact cardinality, I would need to implement this as a `terms` aggregation and then counting the results. Of course, just for fields I know have a relatively small cardinality.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup IndexingOperationListeners infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15875</link><project id="" key="" /><description>This commit reduces the former ShardIndexinService to a simple stats/metrics
class, moves IndexingSlowLog to the IndexService level since it can be shared
across shards of an index and is now hidden behind IndexingOperationListener.

IndexingOperationListener is now a first class citizen in IndexShard and is passed
in from IndexService.
</description><key id="125780859">15875</key><summary>Cleanup IndexingOperationListeners infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-09T20:54:07Z</created><updated>2016-01-10T19:22:45Z</updated><resolved>2016-01-10T19:22:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-09T20:55:01Z" id="170280375">I added one nocommit since we apparently never call clear on the indexing stats, I wonder if we should?
</comment><comment author="mikemccand" created="2016-01-10T09:52:21Z" id="170330443">LGTM, this is a great cleanup!  Thanks @s1monw. I love how slow log and stats counting are just listeners now, and you just pass all listeners up front on init instead of add/remove listener.

&gt; we apparently never call clear on the indexing stats

This has come up before: #9693 ... I don't think it has to block this change?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of IndexWriter#isLocked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15874</link><project id="" key="" /><description>This commit removes and now forbids use of
`org.apache.lucene.index.IndexWriter#isLocked` as this method was
deprecated in [LUCENE-6508](https://issues.apache.org/jira/browse/LUCENE-6508). The deprecation is due to the fact that
checking if a lock is held before acquiring that lock is subject to a
[time-of-check-to-time-of-use race condition](https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use). There were three uses of
`IndexWriter#isLocked` in the code base:
- a logging statement in `o.e.i.e.InternalEngine` where we are already in
  an exceptional condition that the lock was held; in this case,
  logging whether or not the directory is locked is superfluous
- in `o.e.c.l.u.VersionsTests` where we were verifying that a write lock
  is released upon closing an `IndexWriter`; in this case, the check is
  not needed as successfully closing an `IndexWriter` releases its
  write lock
- in `o.e.t.s.MockFSDirectoryService` where we were verifying that a
  directory is not write-locked before (implicitly) trying to obtain
  such a write lock in `org.apache.lucene.index.CheckIndex#&lt;init&gt;` (this
  is the exact type of a situation that is subject to a race
  condition); in this case we can proceed by just (implicitly) trying
  to obtain the write lock and failing if we encounter a
  `LockObtainFailedException`

Closes #15846
</description><key id="125777083">15874</key><summary>Remove and forbid use of IndexWriter#isLocked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-09T19:37:46Z</created><updated>2016-03-10T18:53:01Z</updated><resolved>2016-01-10T13:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-09T20:04:27Z" id="170275157">+1, nice cleanup. this method spells trouble!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> inconsistent num of document between _cat/indices and &lt;index&gt;/_search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15873</link><project id="" key="" /><description>i inserted one document in elasticsearch.
when i search in indices it showing

http://localhost:9200/_cat/indices?v
health status   index          pri    rep     docs.count     docs.deleted     store.size             pri.store.size 
green  open   mylogs        5      1         2                  0                        72.4kb                     72.4kb  

when searching in that index

http://localhost:9200/mylogs/_search

took: 1,
timed_out: false,
_shards: {
total: 5,
successful: 5,
failed: 0
},
hits: {
total: 1,
max_score: 1,
hits: [
{}]
</description><key id="125756620">15873</key><summary> inconsistent num of document between _cat/indices and &lt;index&gt;/_search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">veeruborra</reporter><labels><label>:CAT API</label></labels><created>2016-01-09T12:33:18Z</created><updated>2016-01-15T12:50:13Z</updated><resolved>2016-01-15T12:47:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-09T13:23:21Z" id="170239813">What version of Elasticsearch are you running, and can you provide a minimal reliable reproduction as this does not reproduce for me?

``` bash
$ curl -XGET localhost:9200/
{
  "name" : "Nick Fury",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.1.1",
    "build_hash" : "40e2c53a6b6c2972b3d13846e450e66f4375bd71",
    "build_timestamp" : "2015-12-15T13:05:55Z",
    "build_snapshot" : false,
    "lucene_version" : "5.3.1"
  },
  "tagline" : "You Know, for Search"
}
$ curl -XDELETE localhost:9200/_all?pretty=1
{
  "acknowledged" : true
}
$ curl -XGET localhost:9200/_cat/nodes?v
host      ip        heap.percent ram.percent cpu load node.role master name
127.0.0.1 127.0.0.1            6          99   6 1.69 d         m      Nick Fury
127.0.0.1 127.0.0.1            2          99   6 1.69 d         *      Thog
$ curl -XPOST localhost:9200/foo/bar/1?pretty=1 -d '{ "foo" : "bar" }'
{
  "_index" : "foo",
  "_type" : "bar",
  "_id" : "1",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
$ curl -XGET localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
green  open   foo     5   1          1            0      6.8kb          3.4kb
$ curl -XGET localhost:9200/foo/_search?pretty=1
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "foo",
      "_type" : "bar",
      "_id" : "1",
      "_score" : 1.0,
      "_source" : {
        "foo" : "bar"
      }
    } ]
  }
}
```
</comment><comment author="veeruborra" created="2016-01-10T09:45:13Z" id="170329882">{
name: "Leo",
cluster_name: "elasticsearch",
version: {
number: "2.1.1",
build_hash: "40e2c53a6b6c2972b3d13846e450e66f4375bd71",
build_timestamp: "2015-12-15T13:05:55Z",
build_snapshot: false,
lucene_version: "5.3.1"
},
tagline: "You Know, for Search"
}

iam using elasticsearch in php and indexing exception in elasticsearch when exception occurs.

For each single exception,the count in the indices showing two.But when searching in that particular 
index it showing only one document.
</comment><comment author="clintongormley" created="2016-01-10T10:07:28Z" id="170331466">@veeruborra could you also provide a curl recreation demonstrating this problem?  I'm unable to recreate this either
</comment><comment author="veeruborra" created="2016-01-10T12:13:51Z" id="170340153">I am using elasticsearch-php for inserting document in my index.

 $params =
            [
                'index' =&gt; index_logs,
                'type' =&gt; index_type,
                'client' =&gt; [
                    'ignore' =&gt; [400, 404],
                    'verbose' =&gt; true,
                    'timeout' =&gt; 10,
                    'connect_timeout' =&gt; 10
                ],
                'body' =&gt; $data
            ];
        $client = ClientBuilder::create()-&gt;build();
        $apiexception = $client-&gt;create($params);

when exception occurs the error data is stored in $data variable..
</comment><comment author="jasontedor" created="2016-01-10T13:13:22Z" id="170343739">@veeruborra We are looking for a [small script and instructions that reliably reproduces the issue](https://en.wikipedia.org/wiki/Minimal_Working_Example) so that we can investigate. Without this information, we are in the dark about what you're seeing and what the cause is.
</comment><comment author="jasontedor" created="2016-01-15T12:47:05Z" id="171951646">Closing due to no feedback.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove "geohash" json object parsing for geo_point fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15872</link><project id="" key="" /><description>Taken from issue #15179 this simple improvement will remove the ability to specify a geohash encoded geopoint using the `{ "my_field": { "geohash": "s7ws01wyd7ws"}}` format. Since `geo_point` parsing already supports `"my_field" : "s7ws01wyd7ws"` its superfluous.
</description><key id="125738523">15872</key><summary>Remove "geohash" json object parsing for geo_point fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-09T05:00:27Z</created><updated>2016-03-16T17:38:03Z</updated><resolved>2016-03-16T17:38:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-03-16T17:38:03Z" id="197448822">resolved by #15871
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove .geohash suffix from GeoDistanceQuery and GeoDistanceRangeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15871</link><project id="" key="" /><description>Occasionally the .geohash suffix in Geo{Distance|DistanceRange}Query would conflict with a mapping that defines a sub-field by the same name. This occurs often with nested and multi-fields when a mapping defines a geo_point sub-field using the field name "geohash". Since the QueryParser already handles parsing geohash encoded geopoints, without requiring the ".geohash" suffix, the suffix parsing can be removed altogether.

This PR removes the .geohash suffix parsing, adds explicit test coverage for the nested query use-case, and adds random distance queries to the nested query test suite.

closes #15179 
</description><key id="125738371">15871</key><summary>Remove .geohash suffix from GeoDistanceQuery and GeoDistanceRangeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>v2.2.2</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-09T04:54:34Z</created><updated>2016-03-09T15:36:33Z</updated><resolved>2016-03-09T15:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-26T12:16:29Z" id="189251580">@nknize: I left a few comments but apart from that LGTM. I'd just be interested how you think about the API issue in `RandomQueryBuilder`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix nested multi-value query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15870</link><project id="" key="" /><description>The doc has a typo. It currently says 

&gt; This document would incorrectly match a query for alice AND smith:

and the query is for alice AND white.
</description><key id="125709607">15870</key><summary>fix nested multi-value query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjruizes</reporter><labels><label>docs</label></labels><created>2016-01-08T22:42:05Z</created><updated>2016-01-10T09:06:53Z</updated><resolved>2016-01-10T09:05:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjruizes" created="2016-01-08T22:46:34Z" id="170148661">I signed the CLA-- not sure why the check still says I haven't.
</comment><comment author="clintongormley" created="2016-01-10T09:06:53Z" id="170328702">thanks @rjruizes - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix blended terms for non-strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15869</link><project id="" key="" /><description>It had some funky errors, like lenient:true not working and queries with
two integer fields blowing up if there was no analyzer defined on the
query. This throws a bunch more tests at it and rejiggers how non-strings
are handled so they don't wander off into scary QueryBuilder-land unless
they have a nice strong analyzer to protect them.

Closes #15860
</description><key id="125703060">15869</key><summary>Fix blended terms for non-strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-01-08T21:58:03Z</created><updated>2016-01-11T18:54:53Z</updated><resolved>2016-01-11T13:19:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-08T22:00:29Z" id="170139873">@jreinke this should fix your problem. If you can read the Java API check my tests and make sure. I for sure could reproduce both the shift value thing and the number format exception thing with them.

@s1monw for review. If we're going to do a 1.7.5 should I mark try and backport this there on Monday? Its been a long, long time since I touched 1.x so I have no clue how hard it'll be but I'm 95% sure this issue is there too.
</comment><comment author="s1monw" created="2016-01-08T22:44:03Z" id="170148232">LGTM

&gt; @s1monw for review. If we're going to do a 1.7.5 should I mark try and backport this there on Monday? Its been a long, long time since I touched 1.x so I have no clue how hard it'll be but I'm 95% sure this issue is there too.

yeah I think we should backport it if we can. I think it should be rather simple this code didn't change much at all. You gotta play with `patch -p2` how exciting 
</comment><comment author="nik9000" created="2016-01-11T13:30:39Z" id="170549425">Reverted! It broke some tests. I'll fix the tests and push.
</comment><comment author="clintongormley" created="2016-01-11T18:53:04Z" id="170651293">I've removed the version labels.  Please add back once you've pushed a fix
</comment><comment author="nik9000" created="2016-01-11T18:54:53Z" id="170651788">&gt; I've removed the version labels. Please add back once you've pushed a fix

Oh! Sure. I've actually recreated the PR with the fix as #15894. Its complex enough it worth re-reviewing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Rename ingest plugin to ingest-geoip plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15868</link><project id="" key="" /><description>This plugin now only contains the geoip processor.
</description><key id="125701638">15868</key><summary>[Ingest] Rename ingest plugin to ingest-geoip plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-08T21:48:59Z</created><updated>2016-01-08T22:19:04Z</updated><resolved>2016-01-08T22:19:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-01-08T22:08:41Z" id="170141569">+1 for this naming
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>script_lang not supported [groovy] when putting indexed script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15867</link><project id="" key="" /><description>In pom:

```
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;1.7.1&lt;/version&gt;
        &lt;/dependency&gt;
```

ES 1.7.1 has `script.inline: on` set:

Using a NodeClient connection:

```
        node = NodeBuilder.nodeBuilder().settings(settings).node();
        client = node.client();
```

Tried 2 ways to put an indexed script.

Example 1:

```
 PutIndexedScriptResponse putIndexedScriptResponse =
        client.preparePutIndexedScript()
                .setId("dummy_script_name")
                .setScriptLang("groovy")
                .setSource("\"script\" : \"_score * doc['some_field'].value\"")
                .get();
```

Example 2:

```
PutIndexedScriptResponse putIndexedScriptResponse = client.preparePutIndexedScript(GroovyScriptEngineService.NAME, "dummy_script_name", "\"script\" : \"_score * doc['some_field'].value\"").get();
```

They both return the same exception:

```
Exception in thread "main" org.elasticsearch.ElasticsearchIllegalArgumentException: script_lang not supported [groovy]
    at org.elasticsearch.script.ScriptService.validateScriptLanguage(ScriptService.java:343)
    at org.elasticsearch.script.ScriptService.putScriptToIndex(ScriptService.java:395)
    at org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction.doExecute(TransportPutIndexedScriptAction.java:54)
    at org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction.doExecute(TransportPutIndexedScriptAction.java:36)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:98)
    at org.elasticsearch.client.support.AbstractClient.putIndexedScript(AbstractClient.java:260)
    at org.elasticsearch.action.indexedscripts.put.PutIndexedScriptRequestBuilder.doExecute(PutIndexedScriptRequestBuilder.java:189)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)
    at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:73)
    at com.elasticsearch.pius.PutIndexedScript.main(PutIndexedScript.java:59)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
```

Rest request works:

```
POST /_scripts/groovy/dummy_script_name
{
     "script": "_score * doc['some_field'].value"
}
```

Is this a bug?  Or are there steps missing that should be documented in our guide?
</description><key id="125685072">15867</key><summary>script_lang not supported [groovy] when putting indexed script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Indexed Scripts/Templates</label><label>:Java API</label><label>discuss</label><label>docs</label></labels><created>2016-01-08T20:14:35Z</created><updated>2016-05-24T10:06:59Z</updated><resolved>2016-05-24T10:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-08T20:18:11Z" id="170112581">I think you need to add:

``` xml
        &lt;dependency&gt;
            &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;
            &lt;artifactId&gt;groovy-all&lt;/artifactId&gt;
            &lt;version&gt;2.4.4&lt;/version&gt;
        &lt;/dependency&gt;
```

As it's marked as optional.
</comment><comment author="dadoonet" created="2016-01-08T20:19:09Z" id="170112788">For example, see doc: https://www.elastic.co/guide/en/elasticsearch/client/java-api/1.7/_metrics_aggregations.html#java-aggs-metrics-scripted-metric
</comment><comment author="ppf2" created="2016-01-08T20:34:28Z" id="170116421">Thanks @dadoonet for the tip !  I am thinking it will be helpful to also add this caveat of adding the `groovy-all-&lt;version&gt;.jar` to the Java client project to the main scripting module page [here](https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting.html).  Thoughts? :)
</comment><comment author="rjernst" created="2016-01-08T20:50:11Z" id="170120280">The problem is this line in the stack trace:

```
at org.elasticsearch.script.ScriptService.validateScriptLanguage(ScriptService.java:343)
```

Why is the client trying to validate the script language? The client should pass the script along to the system and let it do any validation necessary.
</comment><comment author="rjernst" created="2016-01-08T20:52:42Z" id="170120751">Seems like the root of the problem there is there are no apis for indexed scripts, it is essentially something that is translated client side into an index request (to the indexed scripts index). Instead, there should really be CRUD operations for scripts, and those translate into indexing operations on the server side.
</comment><comment author="rjernst" created="2016-01-08T20:54:17Z" id="170121059">Oh we do have CRUD ops! So I think we just need to change the client to call those, instead of indexing directly into the indexed scripts?
</comment><comment author="rjernst" created="2016-01-08T20:58:58Z" id="170122184">Ah, I think I see the real root of the problem. The node client performs all actions _locally_ (on itself). Really I think it should be sending these requests to the cluster, assuming _nothing_ about the "node client" itself? But i think this comes back to a core problem with the node client: it is a node in the cluster, so the cluster thinks it should work like any other node.
</comment><comment author="dadoonet" created="2016-01-09T10:40:36Z" id="170224661">I think we have a similar issue with `mustache`. See https://discuss.elastic.co/t/template-query-on-embedded-server-results-in-script-lang-not-supported-mustache/38765
So if we document it, we need to add documentation for https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.1/java-specialized-queries.html#java-query-dsl-template-query as well.
</comment><comment author="clintongormley" created="2016-05-24T10:06:59Z" id="221224352">Indexed scripts have been replaced with stored scripts.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread pool size should not be a global (cluster update API) setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15866</link><project id="" key="" /><description>Spinoff from #15585 

Today you can e.g. set `threadpool.bulk.size` across the cluster, but this is dangerous, because it really should be node level setting instead (maybe you have "hot" nodes with many cores, and "cold" nodes with fewer cores).
</description><key id="125680031">15866</key><summary>Thread pool size should not be a global (cluster update API) setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-08T19:45:42Z</created><updated>2016-06-02T14:43:44Z</updated><resolved>2016-06-02T13:50:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-06-02T13:50:52Z" id="223297427">Closed in favor of #18613
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sanity check for Gradle esplugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15865</link><project id="" key="" /><description>During development of a plugin based on the Gradle `elasticsearch.esplugin`, @beiske noticed a dependency of `RestSpecHack` on the `idea` plugin (which was not explicitly applied in his project). There are currently no sanity checks in place in our infrastructure to check whether the plugin functionality properly works for future third-party plugins based on `elasticsearch.esplugin`.

This PR adds the most simple sanity check, an integration test that applies `elasticsearch.esplugin` against the following sample plugin project:

```
plugins {
  id 'elasticsearch.esplugin'
}

esplugin {
  description 'Dummy description.'
  classname 'org.elasticsearch.plugin.dummy.DummyPlugin'
}
```
</description><key id="125674959">15865</key><summary>Sanity check for Gradle esplugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>build</label><label>test</label></labels><created>2016-01-08T19:17:27Z</created><updated>2016-07-12T08:39:20Z</updated><resolved>2016-07-11T18:39:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-18T20:02:47Z" id="172638457">This looks ok, although I'm not so happy about all the manual work for tests...especially passing through a classpath....  Separate from the tests, I think we should just always apply idea/eclipse plugins? It doesn't hurt users if they dont use them, since they would never run the `idea` or `eclipse` tasks.
</comment><comment author="ywelsch" created="2016-01-18T20:32:55Z" id="172646589">Applying idea / eclipse by default is ok for me.

About the integration test, I am still unsure whether we should put it in by default (unit tests would be less of an issue). I will probably take it out of the PR and go with applying idea / eclipse. We might resurrect testing of buildSrc at a later point in time then. wdyt @rjernst ?
</comment><comment author="rjernst" created="2016-01-18T21:01:23Z" id="172652889">Sounds good.
</comment><comment author="rjernst" created="2016-01-18T21:02:52Z" id="172653161">And for the record, I don't think the startup time is something to be concerned with on a fresh checkout. After the first build, if no changes are made, there is no added cost for buildSrc. It is better to stop the build early for a problem with plugins in buildSrc, rather than continue on and get possibly confusing errors/behavior in the build.
</comment><comment author="dakrone" created="2016-04-06T17:22:25Z" id="206473824">ping @ywelsch is this still an issue? It's a couple of months old now
</comment><comment author="dakrone" created="2016-07-11T16:58:08Z" id="231796217">@ywelsch ping again about this
</comment><comment author="ywelsch" created="2016-07-11T18:39:45Z" id="231825532">Now that `buildSrc` is also represented as sub-project `build-tools`, one of my concerns regarding this PR (running tests while starting up build) is gone. As I'm currently working on other things I'll close this PR for now but might revisit it again later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix initial sizing of BytesStreamOutput.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15864</link><project id="" key="" /><description>It currently tries to align to the page size (16KB) by default. However, this
might waste a significant memory (if many BytesStreamOutputs are allocated)
and is also useless given that BytesStreamOutput does not recycle (on the
contrary to ReleasableBytesStreamOutput). So the initial size has been changed
to 0.

Closes #15789
</description><key id="125673792">15864</key><summary>Fix initial sizing of BytesStreamOutput.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v1.7.5</label><label>v2.0.2</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T19:10:15Z</created><updated>2016-01-11T08:15:13Z</updated><resolved>2016-01-11T08:15:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-08T19:10:40Z" id="170096006">cc @s1monw 
</comment><comment author="s1monw" created="2016-01-08T19:25:17Z" id="170099229">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove checks for versions that we don't test against anyway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15863</link><project id="" key="" /><description /><key id="125661215">15863</key><summary>remove checks for versions that we don't test against anyway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2016-01-08T17:59:01Z</created><updated>2016-01-08T18:45:30Z</updated><resolved>2016-01-08T18:45:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-08T18:07:43Z" id="170076759">LGTM. Yay!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of j.u.c.ThreadLocalRandom</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15862</link><project id="" key="" /><description>This commit removes and now forbids all uses of
java.util.concurrent.ThreadLocalRandom across the codebase. The
underlying issue with ThreadLocalRandom is that it can not be
seeded. This means that if ThreadLocalRandom is used in production code,
then tests that cover any code path containing ThreadLocalRandom will be
prevented from being reproducible by use of ThreadLocalRandom. Instead,
using org.elasticsearch.common.random.Randomness#get will give
reproducible sources of random when running under tests and otherwise
still give an instance of ThreadLocalRandom when running as production
code.

Closes #15294
</description><key id="125654552">15862</key><summary>Remove and forbid use of j.u.c.ThreadLocalRandom</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T17:25:07Z</created><updated>2016-01-08T18:12:39Z</updated><resolved>2016-01-08T17:59:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-08T17:31:59Z" id="170066569">LGTM. Its convenient that this directly resolves back to ThreadLocalRandom outside of tests. It makes it simpler to review.
</comment><comment author="jasontedor" created="2016-01-08T18:01:36Z" id="170075420">Thanks @nik9000.
</comment><comment author="nik9000" created="2016-01-08T18:12:39Z" id="170078031">:+1:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dead o.e.c.m.UnboxedMathUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15861</link><project id="" key="" /><description>This commit removes the dead UnboxedMathUtils from the codebase.
</description><key id="125649616">15861</key><summary>Remove dead o.e.c.m.UnboxedMathUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T17:00:43Z</created><updated>2016-01-08T17:23:30Z</updated><resolved>2016-01-08T17:23:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-08T17:06:27Z" id="170058023">LGTM
</comment><comment author="nik9000" created="2016-01-08T17:07:23Z" id="170058402">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi_match query gives java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15860</link><project id="" key="" /><description>Hi team,

I get an error using a `multi_match` query with `cross_fields` type and a numeric query.
I'm using v2.1.1 on OSX, installed via Homebrew.

**Index basic data**

```
curl -XPUT http://localhost:9200/blog/post/1?pretty=1 -d '{"foo":123, "bar":"xyzzy", "baz":456}'
```

**Use a `multi_match` query with `cross_fields` type and a numeric query**

```
curl -XGET http://localhost:9200/blog/post/_search?pretty=1 -d '{"query": {"multi_match": {"type": "cross_fields", "query": "100", "lenient": true, "fields": ["foo", "bar", "baz"]}}}'
```

**Error**

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "Illegal shift value, must be 0..63; got shift=2147483647"
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "blog",
      "node" : "0TxGVVWsSu2qX63hZdOv2w",
      "reason" : {
        "type" : "illegal_argument_exception",
        "reason" : "Illegal shift value, must be 0..63; got shift=2147483647"
      }
    } ]
  },
  "status" : 400
}
```

**Note that the error does not appear if I specify only 1 numeric field in search.**

**Stack trace**

```
Caused by: java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647
    at org.apache.lucene.util.NumericUtils.longToPrefixCodedBytes(NumericUtils.java:147)
    at org.apache.lucene.util.NumericUtils.longToPrefixCoded(NumericUtils.java:121)
    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.getBytesRef(NumericTokenStream.java:163)
    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.clone(NumericTokenStream.java:217)
    at org.apache.lucene.analysis.NumericTokenStream$NumericTermAttributeImpl.clone(NumericTokenStream.java:148)
    at org.apache.lucene.util.AttributeSource$State.clone(AttributeSource.java:54)
    at org.apache.lucene.util.AttributeSource.captureState(AttributeSource.java:281)
    at org.apache.lucene.analysis.CachingTokenFilter.fillCache(CachingTokenFilter.java:96)
    at org.apache.lucene.analysis.CachingTokenFilter.incrementToken(CachingTokenFilter.java:70)
    at org.apache.lucene.util.QueryBuilder.createFieldQuery(QueryBuilder.java:223)
    at org.apache.lucene.util.QueryBuilder.createBooleanQuery(QueryBuilder.java:87)
    at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:178)
    at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:55)
    at org.elasticsearch.index.search.MultiMatchQuery.access$000(MultiMatchQuery.java:42)
    at org.elasticsearch.index.search.MultiMatchQuery$QueryBuilder.parseGroup(MultiMatchQuery.java:118)
    at org.elasticsearch.index.search.MultiMatchQuery$CrossFieldsQueryBuilder.buildGroupedQueries(MultiMatchQuery.java:198)
    at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:86)
    at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:163)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:257)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:303)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:206)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:201)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:831)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:651)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:617)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
```
</description><key id="125648180">15860</key><summary>multi_match query gives java.lang.IllegalArgumentException: Illegal shift value, must be 0..63; got shift=2147483647</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jreinke</reporter><labels><label>:Search</label><label>bug</label></labels><created>2016-01-08T16:52:27Z</created><updated>2016-01-15T14:54:59Z</updated><resolved>2016-01-15T14:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-08T18:33:44Z" id="170084617">Fun times. Reproduces in master. I'll work on fixing it there and backporting it after that.
</comment><comment author="jreinke" created="2016-01-08T20:01:37Z" id="170108115">FYI, I have reported a similar bug with the same symptoms (1 numeric field included is ok but 2+ numeric fields give `number_format_exception`). Don't know if it could be related.

https://github.com/elastic/elasticsearch/issues/3975#issuecomment-167577538
</comment><comment author="nik9000" created="2016-01-08T20:22:56Z" id="170113564">Good timing! I've figure out what is up and I've started on a solution. I've only got about two more hours left to work on it so I might not have anything before the weekend but, yeah, I'll have something soon.

The `lenient: true` issue is similar so I'll work on it while I'm in there.
</comment><comment author="nik9000" created="2016-01-11T17:27:25Z" id="170625825">Had to revert the change. I'll get it in there though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update aliases.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15859</link><project id="" key="" /><description>fix typo
</description><key id="125642462">15859</key><summary>Update aliases.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paulmallet</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-01-08T16:20:33Z</created><updated>2016-01-11T08:50:31Z</updated><resolved>2016-01-11T08:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T10:04:00Z" id="170331239">Hi @paulmallet 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="paulmallet" created="2016-01-11T08:44:11Z" id="170468332">done
</comment><comment author="clintongormley" created="2016-01-11T08:50:31Z" id="170469433">thanks @paulmallet 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minimum_should_match for one word query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15858</link><project id="" key="" /><description>The following is a search query. The term full_name uses a synonym analyzer. 
The synonym expand file has the synonym: rice,arroz. 
Now if we run the validate query we get the below results:

```
GET /products_v2/product/_validate/query?explain
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "full_name": {
              "query": "rice",
              "fuzziness": 1,
              "minimum_should_match": "3&lt;90%"
            }
          }
        }
      ],
      "should": [
        {
          "match_phrase": {
            "name": {
              "query": "rice",
              "slop": 50
            }
          }
        }
      ]
    }
  }
}

{
   "valid": true,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "explanations": [
      {
         "index": "products_v2",
         "valid": true,
         "explanation": "filtered(+((full_name:arroz~1 full_name:rice~1)~2) (name:arroz name:rice))-&gt;cache(_type:product)"
      }
   ]
}
```

Both rice and arroz are position 1. The minimum_should_match doesn't get applied to the position but the number of words in position 1.

Let's say the synonym file has another synonym :  Brown Sugar, brownsugar.
The following is the result of the validate query calling brownsugar

```
{
   "valid": true,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "explanations": [
      {
         "index": "products_v4",
         "valid": true,
         "explanation": "filtered(+(((full_name:brownsugar~1 full_name:brown~1) full_name:sugar~1)~2) name:\"(brownsugar brown) sugar\"~50)-&gt;cache(_type:product)"
      }
   ]
}
```

In this case the minimum to match is applied based on positions. brownsugar and brown are position 1, sugar is position 2. The minimum_should_match gets applied as 2.

If minimum_should_match gets applied based on the number of positions there are, I believe the above example for rice should have minimum_should_match should be 1 and not 2. This is a problem in the way ES handles minimum_should_match for one word synonyms.
</description><key id="125639035">15858</key><summary>Minimum_should_match for one word query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jzbahrai</reporter><labels><label>:Query DSL</label><label>bug</label><label>discuss</label></labels><created>2016-01-08T16:05:27Z</created><updated>2016-01-19T14:07:32Z</updated><resolved>2016-01-19T14:07:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T10:02:34Z" id="170331182">Simpler recreation here:

```
PUT t
{
  "settings": {
    "analysis": {
      "analyzer": {
        "syns": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "syns"
          ]
        }
      },
      "filter": {
        "syns": {
          "type": "synonym",
          "synonyms": [
            "arroz,rice",
            "brown sugar,brownsugar"
          ]
        }
      }
    }
  },
  "mappings": {
    "t": {
      "properties": {
        "full_name": {
          "type": "string",
          "analyzer": "syns"
        }
      }
    }
  }
}

GET t/_validate/query?explain
{
  "query": {
    "match": {
      "full_name": {
        "query": "rice",
        "minimum_should_match": 2
      }
    }
  }
}

GET t/_validate/query?explain
{
  "query": {
    "match": {
      "full_name": {
        "query": "brown sugar",
        "minimum_should_match": 2
      }
    }
  }
}
```

Not sure what can be done for the second multi-word synonym example (as multi-word synonyms are really problematic) but it does feel like the first example should not apply the min-should-match to stacked tokens.

@mikemccand @jpountz what do you think=
</comment><comment author="jpountz" created="2016-01-11T08:50:05Z" id="170469231">+1
</comment><comment author="jimczi" created="2016-01-11T10:37:40Z" id="170503174">@clintongormley I changed the behavior of minimum should match to fix this issue: https://github.com/elastic/elasticsearch/issues/15521. In 2.2 and master we always honor the value even if the final number of optional clauses (after expansion) is smaller. This is why your first example returns no result. For the multi word synonym example the problem is similar but the resolution would be different simply because the generated query is wrong:

```
"explanation": "((full_name:brown full_name:brownsugar) full_name:sugar)"
```
</comment><comment author="jpountz" created="2016-01-11T16:49:41Z" id="170614804">@jimferenczi Maybe I'm the one confused but I think the issue is different here: we want the number of SHOULD clauses to be equal to the number of unique positions instead of the number of tokens?
</comment><comment author="jzbahrai" created="2016-01-11T17:06:08Z" id="170619794">I am also a little unsure as to what @jimferenczi is saying. I want the minimum_should_match for rice,arroz to be equal to 1 as it should be based on the number of positions (in this case 1), but it is 2. It is correct for the multiword brown sugar, brownsugar as it is 2 and the number of positions in that case is 2.
</comment><comment author="jimczi" created="2016-01-11T18:13:59Z" id="170638474">@jzbahrai @jpountz I just answered regarding @clintongormley's recreation where the minimum should match is statically set to 2. I agree that there is a bug with the example used by @jzbahrai where minimum should match is set with "3&lt;90%". I'll take a look.
</comment><comment author="jimczi" created="2016-01-11T18:39:21Z" id="170647673">The bug is when there is only one position but multiple clauses in this position. it is related to a small optimization that is done to reduce the number of clauses when there is only one position. In such case we build only one level of boolean query and the optional clauses are all at the root level, this level is then used to compute the number of optional clauses in the query. For instance the query "rice" is expanded into:
(rice arrow) =&gt; 2 optional clauses at the root level.
but to correctly compute the number of positions we should generate another level to indicate that the query contains only one position:
((rice arrow)) =&gt; 1 optional clause at the root level and 2 optional clauses at the inner level.
I'll work on a fix tomorrow.
</comment><comment author="jimczi" created="2016-01-12T09:05:22Z" id="170845076">The bug/problem is at the Lucene level. I opened https://issues.apache.org/jira/browse/LUCENE-6972, @jpountz can you take a look ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Makes the AggregatorFactory and PipelineAggregatorFactory setter methods chainable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15857</link><project id="" key="" /><description /><key id="125626148">15857</key><summary>Makes the AggregatorFactory and PipelineAggregatorFactory setter methods chainable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-01-08T14:53:28Z</created><updated>2016-01-11T12:50:12Z</updated><resolved>2016-01-11T12:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-08T17:11:48Z" id="170060138">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document index deletion using comma separated indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15856</link><project id="" key="" /><description /><key id="125621638">15856</key><summary>Document index deletion using comma separated indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcbachmann</reporter><labels><label>docs</label></labels><created>2016-01-08T14:23:53Z</created><updated>2016-01-11T00:05:31Z</updated><resolved>2016-01-10T09:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T09:48:47Z" id="170330116">thanks @marcbachmann - merged
</comment><comment author="marcbachmann" created="2016-01-11T00:05:21Z" id="170407910">Nice. thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Make grok processor available by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15855</link><project id="" key="" /><description>Moved the grok processor to its own module, so that it will available out-of-the-box, while its dependencies are isolated.

In order to made ingest processor available via a module. The grok patterns file have to be embedded in the plugin jar file, because modules can't ship with default config files. While I made this change I removed the ability to specify additional custom patterns via the config directory. My reasoning here is that custom patterns can already be specified via the pipeline config, which is much easier than managing custom files on all nodes in a cluster.
</description><key id="125612861">15855</key><summary>[Ingest] Make grok processor available by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2016-01-08T13:25:30Z</created><updated>2016-01-11T13:17:44Z</updated><resolved>2016-01-08T21:00:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-08T13:32:54Z" id="170004824">&gt; While I made this change I removed the ability to specify additional custom patterns via the config directory. My reasoning here is that custom patterns can already be specified via the pipeline config, which is much easier than managing custom files on all nodes in a cluster.

++
</comment><comment author="javanna" created="2016-01-08T13:42:10Z" id="170007031">left a few comments, looks good though, maybe @s1monw would like to have a look too.
</comment><comment author="talevy" created="2016-01-08T17:16:12Z" id="170061882">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeout settings (default to 5 minutes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15854</link><project id="" key="" /><description>As we did for Azure repository (#15080), I think we should also add a configurable timeout to S3 repository (default to 5 minutes).

AWS javadoc says:

Socket Timeout:

```
     * Returns the amount of time to wait (in milliseconds) for data to be
     * transfered over an established, open connection before the connection
     * times out and is closed. A value of 0 means infinity, and isn't
     * recommended.
```

Request Timeout:

```
     * Returns the amount of time to wait (in milliseconds) for the request to complete before
     * giving up and timing out. A non-positive value means infinity.
```

The default AWS SDK timeouts are:

``` java
/** The default timeout for reading from a connected socket. */
public static final int DEFAULT_SOCKET_TIMEOUT = 50 * 1000;

/** The default timeout for a request. This is disabled by default, being vended as an opt-in setting. */
public static final int DEFAULT_REQUEST_TIMEOUT = 0;
```

Should we add a Request Timeout or having default Socket Timeout is enough to stop idle threads?
</description><key id="125600195">15854</key><summary>Add timeout settings (default to 5 minutes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>discuss</label></labels><created>2016-01-08T12:10:23Z</created><updated>2016-01-14T12:07:46Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-08T12:10:39Z" id="169982226">@imotov @tlrx What do you think?
</comment><comment author="imotov" created="2016-01-13T01:53:51Z" id="171128092">@dadoonet  do you think 5 minutes is long enough in this case? How did you choose 5 minutes?
</comment><comment author="dadoonet" created="2016-01-13T06:29:36Z" id="171183220">This is what we did for azure.
</comment><comment author="imotov" created="2016-01-14T02:51:00Z" id="171514377">I am not really sure how we picked 5 minutes for azure either :)
</comment><comment author="dadoonet" created="2016-01-14T08:00:07Z" id="171566603">It came formerly from @craigwi... :) 

Thinking of it more, may be we should consider that the biggest file would be 2Tb for instance and measure how long it takes on S3 or Azure to copy such a file? Then multiply this by 1.5 or 2... 
</comment><comment author="clintongormley" created="2016-01-14T12:07:46Z" id="171627836">Is the timeout for the whole request? Or just since the last time data was passed down the socket?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix CreateIndexIT#testCreateAndDeleteIndexConcurrently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15853</link><project id="" key="" /><description>The test fails in different ways since we can hit timeouts and shards not being available on retry
which we simply didn't expect before but are valid.

Closes #15312
Closes #14512
</description><key id="125586331">15853</key><summary>Fix CreateIndexIT#testCreateAndDeleteIndexConcurrently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label></labels><created>2016-01-08T10:46:35Z</created><updated>2016-03-15T03:30:44Z</updated><resolved>2016-03-15T03:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix tribe blocks documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15852</link><project id="" key="" /><description>Closes #15806
</description><key id="125564213">15852</key><summary>Fix tribe blocks documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T08:19:54Z</created><updated>2016-01-22T11:10:02Z</updated><resolved>2016-01-13T17:06:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-12T20:46:59Z" id="171049464">LGTM.
</comment><comment author="ywelsch" created="2016-01-13T17:10:35Z" id="171366815">thanks for the review, @jasontedor !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ingest] Don't do DNS lookups from GeoIpProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15851</link><project id="" key="" /><description>There is no need to involve DNS in this!
</description><key id="125539273">15851</key><summary>[ingest] Don't do DNS lookups from GeoIpProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-01-08T04:16:22Z</created><updated>2016-01-08T18:17:21Z</updated><resolved>2016-01-08T18:17:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-08T06:35:57Z" id="169910117">Lgtm. Good catch.
</comment><comment author="s1monw" created="2016-01-08T08:13:38Z" id="169928440">grrrr... can we make this a forbidden API and only use it where it's justified / suppressed for a reason?
</comment><comment author="bleskes" created="2016-01-08T08:21:44Z" id="169929864">++

&gt; On 08 Jan 2016, at 09:13, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; grrrr... can we make this a forbidden API and only use it where it's justified / suppressed for a reason?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="martijnvg" created="2016-01-08T08:34:47Z" id="169933871">+1 to remove

I thought it would be useful for cases hostnames are provided instead of ip addresses. Although this is maybe unlikely, I thought it would help.
</comment><comment author="martijnvg" created="2016-01-08T08:41:18Z" id="169935441">and in the case just an ip address is provided no actual dns lookup is being done.
</comment><comment author="rmuir" created="2016-01-08T08:49:12Z" id="169937519">If the user has data with hostnames, they have a real problem on their hands to resolve these in any efficient way, we can't do it "on the side".

Using InetAddress is not a good solution, it will result in memory leaks (we must assume each host is cached infinitely). In general with any massive use of DNS (via InetAddress, JNDI, etc) we have to worry about craziness like exhaustion of entropy on the machine (securerandom is used for DNS port randomization to "help" address spoofing), bogus data (e.g. NXDOMAIN hijacking), tons of network traffic, throttling, triggering alarms, etc.

I am aware that doing DNS operations on millions of log records, etc is sometimes something people want. But DNS is a very complex serious beast, its definitely a distributed database query, and should always be explicit. Doing it for lots of data is even more complicated if you want it to be anywhere near correct or efficient. 
</comment><comment author="rmuir" created="2016-01-08T08:52:35Z" id="169938007">&gt; and in the case just an ip address is provided no actual dns lookup is being done.

Right, my specific concern is right now it will silently do tons of DNS in cases where an ip address is not provided, e.g. `HostNameLookups On` (http://httpd.apache.org/docs/2.4/mod/core.html#hostnamelookups). We should fail rather than silently be slow in this case!
</comment><comment author="martijnvg" created="2016-01-08T09:00:22Z" id="169939230">Thanks for the explanation. I underestimated the slowness / costs here. LGTM
</comment><comment author="martijnvg" created="2016-01-08T09:01:35Z" id="169939402">like @s1monw suggested, we should make this is a forbidden api.
</comment><comment author="rmuir" created="2016-01-08T18:16:44Z" id="170078988">I'll followup with a forbidden api patch for master. I'm concerned about false positive rate, but I totally agree with the trap (esp. considering the trappiness of InetAddress here, which is why @jasontedor added a parsing method that doesn't go to DNS).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle some deprecation warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15850</link><project id="" key="" /><description>Suppress lots of them with comments. Fix a few.
</description><key id="125537172">15850</key><summary>Handle some deprecation warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T03:50:20Z</created><updated>2016-01-11T19:23:48Z</updated><resolved>2016-01-11T19:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-08T06:41:19Z" id="169910633">Lgtm. Thx @nik9000!
</comment><comment author="rjernst" created="2016-01-08T06:43:24Z" id="169910855">LGTM too.
</comment><comment author="nik9000" created="2016-01-08T13:52:08Z" id="170009474">Pushed another commit that just drops support for the pre-1.0 versions rather than suppresses the warnings.
</comment><comment author="rjernst" created="2016-01-08T16:25:23Z" id="170046849">The minimum version is Lucene 5 for master (or rather that is the intention in master, with an eventual 6.0 release of lucene). If we are assuming no release of master can happen without lucene 6, then removing any conditions checking before Lucene 5 should be ok.
</comment><comment author="nik9000" created="2016-01-08T16:29:33Z" id="170047855">I was mixed up when I last mentioned versions. In master we have to be able to open up indices from 2.x  which means Lucene 5.2.1 and onwards so anything older than that is fine to remove. Right? So we can remove the old checksum support entirely.
</comment><comment author="rjernst" created="2016-01-08T16:32:44Z" id="170048592">Yes, although I would not look for the specific minor release, but only the major version (while you are correct we released 2.0 with 5.2.1, I think it is much simpler to think about major versions alone).
</comment><comment author="nik9000" created="2016-01-08T16:35:24Z" id="170049239">Right. I'm working on these deprecation and other Xlint things to "relax" so I'll probably put a bow on this PR and set it aside and take up the hunt for the rest of the stuff in another PR soonish. I'll remove that 4.4 ngram filter and remove my `SuppressWarnings` on `FIRST_LUCENE_CHECKSUM_VERSION` so it'll be real obvious the next time someone goes to hunt deprecations. They (or I) can remove the old checksum support then.
</comment><comment author="nik9000" created="2016-01-08T18:30:40Z" id="170083583">And done. @rjernst can you have another look?
</comment><comment author="rjernst" created="2016-01-08T19:17:57Z" id="170097597">Looks good, one more place I noticed, at least in the files changed here.
</comment><comment author="nik9000" created="2016-01-08T22:18:45Z" id="170143478">Fixed that one last place and a few others I caught that were using Elasticsearch index version.
</comment><comment author="nik9000" created="2016-01-08T22:19:18Z" id="170143573">I'll rerun the test suite one last time and merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated script parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15849</link><project id="" key="" /><description>The notes in the code say that the script parsing will go in 2.0 but its still there. I mean `SCRIPT_LANG`, `SCRIPT_FILE`, `SCRIPT_ID`, and `SCRIPT_INLINE`.
</description><key id="125533401">15849</key><summary>Remove deprecated script parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Scripting</label><label>adoptme</label><label>non-issue</label></labels><created>2016-01-08T03:09:05Z</created><updated>2016-05-07T15:38:11Z</updated><resolved>2016-05-07T15:38:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-08T03:10:42Z" id="169880718">@colings86 I see your name coming up in git-blame on this code. Is this something you can do easily? Like, if its not that much work.
</comment><comment author="clintongormley" created="2016-03-01T11:54:41Z" id="190686751">Related to https://github.com/elastic/elasticsearch/issues/13729
</comment><comment author="clintongormley" created="2016-05-07T15:38:07Z" id="217645613">Duplicate of https://github.com/elastic/elasticsearch/issues/13729
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove -Xlint:-deprecation from all but core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15848</link><project id="" key="" /><description>Gets a few other Xlints while I'm in there but mostly just deprecation. For the most part I isolate the deprecation and leave a comment. Either the comment is an admission of why we still need to use the deprecated method or a link to an issue.

Closes #15829
</description><key id="125529430">15848</key><summary>Remove -Xlint:-deprecation from all but core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T02:34:21Z</created><updated>2016-01-11T21:55:13Z</updated><resolved>2016-01-11T21:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-08T06:50:41Z" id="169911602">LGTM
</comment><comment author="dadoonet" created="2016-01-08T09:13:45Z" id="169942803">Does this change mean that we won't see anymore in the IDE when we are using deprecated fields/methods?
I found it was useful seeing that and reporting that when coding.
</comment><comment author="nik9000" created="2016-01-08T13:38:23Z" id="170006278">&gt; Does this change mean that we won't see anymore in the IDE when we are using deprecated fields/methods?

It means you won't see the warnings in the scopes that I've annotated with `@SuppressWarnings("deprecation")` but you'll still see them if you use them in a new place. To me `@SuppressWarnings("deprecation")` means "I know I'm using deprecated methods in here and I'm ok with that. So I tried to keep the scope of those annotations pretty small. I didn't do it very well in some cases though.

I think if we want something to remind us to remove the deprecated fields on upgrade we can add a unit test that fails if the version is too great. That way when we bump the version number we **know** we have to remove the code. We might not be able to remove it in the commit that bumps the version numbers but we can `@AwaitsFix` the test then if we have to. Right now we forget to remove deprecated stuff.
</comment><comment author="dadoonet" created="2016-01-08T14:07:45Z" id="170012348">It looks like a fantastic plan to me. That's great to have unit test as reminder... :)
</comment><comment author="nik9000" created="2016-01-08T16:38:06Z" id="170049860">@dadoonet I removed the deprecated settings in this PR. Do you see any others that I should make one of those tests for? I can do that as part of this PR so I don't feel bad about suppressing the warnings.
</comment><comment author="dadoonet" created="2016-01-08T16:45:48Z" id="170051936">@nik9000 I guess that you can remove in master:
- `AzureStorageService#ACCOUNT_DEPRECATED` 
- `AzureStorageService#KEY_DEPRECATED` 

As they have been deprecated in 2.2.0 with #13779
</comment><comment author="nik9000" created="2016-01-11T19:24:35Z" id="170661674">@dadoonet done!
</comment><comment author="dadoonet" created="2016-01-11T21:30:55Z" id="170696275">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal: Use -Xlint:-serial everywhere</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15847</link><project id="" key="" /><description>So far as I can tell `-Xlint:serial` warns you about not defining `serialVersionUID` on classes that implement `Serializable`. For the most part for us that is just `Exception`s since we don't use Java serialization. I think we're ok with not implementing `serialVersionUID` so I think we should just turn that one off on all projects rather than on a case by case basic like we do now.

A third option is to `@SuppressWarnings` about it on every exception but I don't like that idea much.
</description><key id="125528956">15847</key><summary>Proposal: Use -Xlint:-serial everywhere</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T02:29:24Z</created><updated>2016-01-12T15:04:50Z</updated><resolved>2016-01-12T15:04:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-10T13:38:56Z" id="170344977">+1 to use `-Xlint:-serial`.
</comment><comment author="bleskes" created="2016-01-11T09:12:56Z" id="170477983">Exceptions don't use java serialization anymore as well.  It looks like we only use Seriazlizable in injection code now. Does that fail linting? I also wonder if we should add serializable to the fobidden API.. 
</comment><comment author="s1monw" created="2016-01-11T09:20:21Z" id="170480522">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate from IndexWriter.isLocked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15846</link><project id="" key="" /><description>IndexWriter.isLocked is deprecated with advice to just grab the lock instead. So we probably ought to do that.
</description><key id="125527833">15846</key><summary>Migrate from IndexWriter.isLocked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-08T02:21:46Z</created><updated>2016-01-10T13:32:41Z</updated><resolved>2016-01-10T13:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Migrate from LuceneTestCase.getBaseTempDirForTestClass</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15845</link><project id="" key="" /><description>Its deprecated and the warnings on the deprecation seem pretty stern. Either they are wrong or we shouldn't be using it.
</description><key id="125527410">15845</key><summary>Migrate from LuceneTestCase.getBaseTempDirForTestClass</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>test</label></labels><created>2016-01-08T02:18:40Z</created><updated>2016-09-14T17:46:17Z</updated><resolved>2016-09-14T17:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Migrate from RandomizedTest.isNightly()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15844</link><project id="" key="" /><description>Nightly looks like it is [going away](https://github.com/randomizedtesting/randomizedtesting/issues/218). We should migrate away from it before it disappears out from under us.
</description><key id="125526814">15844</key><summary>Migrate from RandomizedTest.isNightly()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label></labels><created>2016-01-08T02:12:48Z</created><updated>2016-11-04T07:43:02Z</updated><resolved>2016-11-04T07:42:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-11-04T07:42:53Z" id="258363416">I first looked into adapting to this change, but it turns out the method is staying after all. See https://github.com/randomizedtesting/randomizedtesting/issues/218#issuecomment-252193168 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch from MSOffice to Office properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15843</link><project id="" key="" /><description>The mapper attachement plugin uses `MSOffice` properties which are apparently deprecated and have been replaced by Tika's `Office` properties. I'm not sure why, but whatever. Anyway, we should figure out the new right way before we suffer from more bit rot.
</description><key id="125523852">15843</key><summary>Switch from MSOffice to Office properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Ingest Attachment</label><label>adoptme</label><label>enhancement</label><label>v5.4.4</label></labels><created>2016-01-08T01:41:40Z</created><updated>2017-06-27T10:28:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Move pipelines from .ingest index to cluster state?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15842</link><project id="" key="" /><description>Right now the pipelines are stored in the .ingest index, but maybe we can move the pipeline configuration to the cluster state? This does have advantages:
- The whole configuration management we have to pushes the pipelines to nodes can be removed and instead ingest can rely on cluster state updates.
- No need to ensure that mapping/settings are applied when the ingest index is created  or manage an index template. Because we are in control what goes into the cluster state via the put and delete pipeline apis.

Beyond the source of the pipeline nothing else is stored (inverted index or docvalues) and the cluster state can hold that too. Also it is likely that there will not be many pipelines (&gt;1000).

Before this was tricky because how ingest was designed: 
- pipelines changes were propagated asynchronously to other ingest nodes. 
- ingest was a plugin and may not be installed on the elected master node. 
</description><key id="125502940">15842</key><summary>[Ingest] Move pipelines from .ingest index to cluster state?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2016-01-07T22:49:56Z</created><updated>2016-01-13T22:03:16Z</updated><resolved>2016-01-13T22:03:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-08T17:20:08Z" id="170063004">After discussing this question there is another requirement that should be taken into account, namely security. Not all pipelines may be used by everyone and if pipeline configuration would be moved to the cluster state there wouldn't be a mechanism how security can easily be enforced. If pipeline configuration is in an index security can be enforced.
</comment><comment author="clintongormley" created="2016-01-10T09:13:19Z" id="170328906">Discussing this online we decided not to move pipelines to the cluster state
</comment><comment author="martijnvg" created="2016-01-11T23:44:31Z" id="170734054">Reopening, using the cluster state to store pipelines makes the ingest code much cleaner and simpler. If security is needed at some point then this can be achieved by adding this to the cluster state infrastructure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed broken link on from-size documentation page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15841</link><project id="" key="" /><description>See https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-from-size.html for the current bug.

&lt;img width="870" alt="from___size" src="https://cloud.githubusercontent.com/assets/9599/12183613/bdac0186-b544-11e5-99ec-a306ae7b69e6.png"&gt;
</description><key id="125491310">15841</key><summary>Fixed broken link on from-size documentation page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simonw</reporter><labels><label>docs</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T21:44:22Z</created><updated>2016-01-08T22:52:15Z</updated><resolved>2016-01-08T22:49:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T22:39:54Z" id="169829974">hey @simonw can you sign the CLA so I can pull it in? If you wanna get rid of your github handle you let me know, right?! ;)
</comment><comment author="simonw" created="2016-01-08T21:07:18Z" id="170123731">Done.
</comment><comment author="s1monw" created="2016-01-08T22:49:09Z" id="170149227">merged thx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>licence-2.1.1.zip FileNotFound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15840</link><project id="" key="" /><description>`./bin/plugin install -v licence`

&gt; -&gt; Installing licence...
&gt; Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/licence/2.1.1/licence-2.1.1.zip ...
&gt; Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/licence/2.1.1/licence-2.1.1.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/licence/2.1.1/licence-2.1.1.zip];
&gt; ERROR: failed to download out of all possible locations..., use --verbose to get detailed information

Nested link reveals: -
This XML file does not appear to have any style information associated with it.
</description><key id="125464464">15840</key><summary>licence-2.1.1.zip FileNotFound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SpaceShipDev</reporter><labels /><created>2016-01-07T19:09:05Z</created><updated>2016-01-07T19:58:55Z</updated><resolved>2016-01-07T19:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="djschny" created="2016-01-07T19:11:33Z" id="169777069">It appears license is misspelled. Try rerunning as:

`./bin/plugin install -v license`
</comment><comment author="SpaceShipDev" created="2016-01-07T19:12:11Z" id="169777223">This is horrific, lets pretend this never happened.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add update-by-query job id metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15839</link><project id="" key="" /><description>This request is to add (by default preferably) the ability to add/update a field on a document that is a unique ID associated with the reindex job being run. This has the following use cases for example:
1. As a reindex job is running, you can track it's progress simply by writing a filtered query to see how many documents have that ID on it.
2. If some documents were not updated due to conflicts (since it is my understanding we are not retrying on conflict), this will allow a person to determine these documents and take appropriate manual or other action post reindex job completion.
3. If the reindex job failed half way through (due to whatever reason), one would be able to know how far it got, what documents to run over again for the rerun, etc.
4. Later on when task management is in place, automatic rerunning of the job can continue on another node and we would know what set of documents to resume on.

In lieu of this being available, one could accomplish this by script but of course it would be manual up to each person to do. However I believe this is something good to have on by default as it helps promote best practices. One could also disable it when calling a reindex API. Some configurable options that come to mind:
- `job_id_stamping` - defaults to `true` and controls whether the above described behavior happens
- `job_id_field` - specify the name of the field to use to store the job id in. defaults to something like `reindex_job_id`
- `job_id_value` - specify the value to place in the `job_id_field` of a document when it is updated. Defaults to something unique like a uuid, task id, or datetime stamp.

While this may not make it in the first iteration, this is important metadata to have and making sure we think about scenarios like this as we continue to iterate on the tool is important.
</description><key id="125464421">15839</key><summary>add update-by-query job id metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Reindex API</label><label>discuss</label></labels><created>2016-01-07T19:08:41Z</created><updated>2016-03-01T11:47:16Z</updated><resolved>2016-03-01T11:47:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-01T11:47:16Z" id="190685173">(This discussion is really about update-by-query rather than reindex - title updated to reflect that)

While the job_id approach might work for some use cases, I think that it is too blunt a tool and ignores the fact that multiple update-by-query requests could be run in parallel on the same index.  Instead, I think the scripted approach (in combination with filters) is the correct one.

Update scripts should be written to take parallel changes into account, eg:

```
if (ctx._source.name == 'Jon') {
    ctx._source.name = 'John'
}
```

This can be rerun with a filter on name == 'Jon', but the logic depends entirely on how the application should behave.  Providing just one rigid way of doing this would be insufficient and, I think, confusing.

&gt; As a reindex job is running, you can track it's progress simply by writing a filtered query to see how many documents have that ID on it.

Status can already be tracked via task mgmt.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Replace ingest template with create index if missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15838</link><project id="" key="" /><description>Removed index template and instead create `.ingest` with settings and mapping when we find out that the index is missing during the put pipeline call.
</description><key id="125453505">15838</key><summary>[Ingest] Replace ingest template with create index if missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2016-01-07T18:08:55Z</created><updated>2016-01-08T09:36:55Z</updated><resolved>2016-01-08T09:36:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-08T09:21:05Z" id="169944248">I think Ryan raised valid concerns, things need to be re-evaluated now that ingest is part core. Probably moving to cluster state makes sense, but let's discuss it on #15842 and get this in for now, which is a good improvement already and should fix some of the test failures we are experiencing. LGTM besides the two comments I left.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move ingest rest spec and tests to core with basic processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15837</link><project id="" key="" /><description>Basic processors that don't have any external dependencies and don't require special permissions are moved to es core. This allows also to move ingest rest spec and corresponding tests to es core too.

Geo ip and grok processors stay in the plugin for now.

Also node.ingest is set to true by default on any node.

Adapted also the message returned for when a node with ingest set to false receives an index or bulk request with a pipeline id.
</description><key id="125452832">15837</key><summary>Move ingest rest spec and tests to core with basic processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-07T18:04:28Z</created><updated>2016-01-08T10:28:56Z</updated><resolved>2016-01-08T10:28:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-07T19:15:23Z" id="169777975">The move looks ok but I left a couple questions.
</comment><comment author="martijnvg" created="2016-01-08T09:41:35Z" id="169947741">LGTM
</comment><comment author="javanna" created="2016-01-08T09:49:57Z" id="169949844">Two followups left here:
- redirect ingest requests to ingest nodes rather than failing when a node with ingest set to false receives a bulk/index request with a pipeline id
- convert IngestClientIt to use ESSmokeClientTestCase
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BitSetFilterCache duplicates its content.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15836</link><project id="" key="" /><description>We have a bug that makes all per-index bitset caches store bitsets for all
indices. In the case that you have many indices, which is fairly common with
time-based data, this could translate to a lot of wasted memory.

Closes #15820
</description><key id="125451389">15836</key><summary>BitSetFilterCache duplicates its content.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T17:56:27Z</created><updated>2016-01-10T09:39:04Z</updated><resolved>2016-01-08T08:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-07T17:59:51Z" id="169757150">LGTM
</comment><comment author="jpountz" created="2016-01-07T18:00:38Z" id="169757344">@martijnvg could you have a look since you're familiar with this cache?
</comment><comment author="martijnvg" created="2016-01-07T20:18:10Z" id="169793828">LGTM 
Maybe we should back port this also to 1.7 branch? This is a bad bug.
</comment><comment author="jpountz" created="2016-01-08T09:26:27Z" id="169945106">Good point. I'll backport to 1.7 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When using a whitespace tokenizer the stop words filter doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15835</link><project id="" key="" /><description>When using the whitespace tokenizer, the stop words filter doesn't work. Here are the CURL commands to replicate:

```
        PUT /my_index
        {
          "settings": {
            "index": {
              "number_of_shards": 1,
              "analysis": {
                  "analyzer": {
                     "fulltext":{
                    "type":"custom",
                    "tokenizer":"whitespace",
                    "filter": ["english_stop"]
                  }
                  },
                 "filter":{
                    "english_stop":{
                       "type":"stop",
                       "stopwords":"_english_"
                    }
                  }
                 }
              }
            }
          }
        }
```

I need to be able to use the whitespace tokenizer because I'm also using the word_delim filter which turns terms like "wi-fi" to wifi, if I use the standard tokenizer, I will lose this ability.
</description><key id="125426202">15835</key><summary>When using a whitespace tokenizer the stop words filter doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels /><created>2016-01-07T16:19:48Z</created><updated>2016-01-08T17:40:35Z</updated><resolved>2016-01-07T16:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T16:22:53Z" id="169714262">please use our discuss forum for questions like this - https://discuss.elastic.co/
</comment><comment author="imranazad" created="2016-01-08T00:46:42Z" id="169853568">@s1monw I think this is a technical issue, the stop words filter just simply will not work when using the whitespace tokenizer.
</comment><comment author="s1monw" created="2016-01-08T08:25:19Z" id="169930788">it works as expected, whitespace tokenizer doesn't lowercase and stopwords are lowercased. please ask these questions on the discuss forjum
</comment><comment author="imranazad" created="2016-01-08T12:28:41Z" id="169985213">@s1monw The issue isn't that the stopwords aren't lower cased the issue is that the stop words are not filtered when using the whitespace tokenizer.
</comment><comment author="dakrone" created="2016-01-08T15:56:48Z" id="170038749">@imranazad what @s1monw is saying is that it may appear that stopwords are not being removed because the english stopwords list only includes words that are lowercase:

# Create an index

```
DELETE /ws
{}

POST /ws
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "analysis": {
        "analyzer": {
          "wsplusstop":{
            "type":"custom",
            "tokenizer":"whitespace",
            "filter": ["english_stop"]
          }
        },
        "filter":{
          "english_stop":{
            "type":"stop",
            "stopwords":"_english_"
          }
        }
      }
    }
  },
  "mappings": {
    "doc": {
      "properties": {
        "body": {
          "type": "string",
          "analyzer": "wsplusstop"
        }
      }
    }
  }
}
```

# Index docs

```
POST /ws/doc/1
{"body": "bill the bat"}

POST /ws/doc/2
{"body": "Bob The Builder"}

POST /ws/_refresh
{}
```

Results:

```
{"_index":"ws","_type":"doc","_id":"1","_version":1,"_shards":{"total":1,"successful":1,"failed":0},"created":true}
{"_index":"ws","_type":"doc","_id":"2","_version":1,"_shards":{"total":1,"successful":1,"failed":0},"created":true}
{"_shards":{"total":1,"successful":1,"failed":0}}
```

# See how the data is analyzed with `fielddata_fields`

```
GET /ws/_search?pretty
{
  "query": {
    "match_all": {}
  },
  "fielddata_fields": ["body"]
}

{
  "took" : 40,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "ws",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0,
      "_source":{"body": "bill the bat"},
      "fields" : {
        "body" : [ "bat", "bill" ]
      }
    }, {
      "_index" : "ws",
      "_type" : "doc",
      "_id" : "2",
      "_score" : 1.0,
      "_source":{"body": "Bob The Builder"},
      "fields" : {
        "body" : [ "Bob", "Builder", "The" ]
      }
    } ]
  }
}
```

Notice the lowercase `the` was removed but the capitalized `The` was not.

In the future though, as @s1monw mentioned, please ask these types of questions
in the discussion forums.
</comment><comment author="imranazad" created="2016-01-08T17:40:35Z" id="170069478">@dakrone Ah I see. Thank you, yes of course, absolutely.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripted metric aggregation docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15834</link><project id="" key="" /><description>it appears the docs on the scripted metric aggregation are obsolete/wrong
on ES 1.6 the init of a new prop on _agg, if done as documented(1), will throw with something like 

```
GroovyScriptExecutionException[MissingPropertyException[No such property: transactions for class: Script93]]
```

if the the prop is accessed directly and init (2), all works as expected

(1)

```
init_script" : "_agg['transactions'] = []"
```

(2)

```
init_script" : "_agg.transactions = []"
```
</description><key id="125423012">15834</key><summary>Scripted metric aggregation docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">albertocsm</reporter><labels><label>docs</label></labels><created>2016-01-07T16:04:26Z</created><updated>2016-03-01T11:28:37Z</updated><resolved>2016-03-01T11:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T10:10:50Z" id="170331620">@colings86 could you look at this please?
</comment><comment author="colings86" created="2016-01-11T11:24:21Z" id="170512773">I can reproduce this in 1.6.1, 1.7.3, and 2.1.1. I'm not sure what changed though as the init_script in (1) worked when the scripted_metric was created. Maybe @dakrone, @pickypg or @jdconrad knows what might have changed in the groovy scripting engine to cause this and whether it was intended or if this is a bug?
</comment><comment author="clintongormley" created="2016-03-01T11:28:37Z" id="190677551">This is a test bug :)

When pasting a curl statement into the console, the single quotes in `"_agg['transactions'] = []"` need to be escaped.

Running the same commands in Sense works just fine.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add timeout settings (default to 5 minutes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15833</link><project id="" key="" /><description>We need to backport #15080 in 2.x branch.

By default, azure does not timeout. This commit adds support for a timeout settings which defaults to 5 minutes.
It's a timeout **per request** not a global timeout for a snapshot request.

It can be defined globally, per account or both. Defaults to `5m`.

``` yml
cloud:
    azure:
        storage:
            timeout: 10s
            my_account1:
                account: your_azure_storage_account1
                key: your_azure_storage_key1
                default: true
            my_account2:
                account: your_azure_storage_account2
                key: your_azure_storage_key2
                timeout: 30s
```

In this example, timeout will be 10s for `my_account1` and 30s for `my_account2`.

Related to https://github.com/elastic/elasticsearch/issues/10564#issuecomment-169690968.
</description><key id="125419633">15833</key><summary>Add timeout settings (default to 5 minutes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>enhancement</label></labels><created>2016-01-07T15:48:48Z</created><updated>2016-01-25T13:33:18Z</updated><resolved>2016-01-25T13:33:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="craigwi" created="2016-01-13T04:12:25Z" id="171160545">in case anyone needs a super short term solution, I implemented the most basic solution (the default 5min timeout) in https://github.com/craigwi/elasticsearch-cloud-azure/releases/tag/v2.8.3-craigwi.

Craig.
</comment><comment author="dadoonet" created="2016-01-25T13:33:17Z" id="174510550">Closed by #15950 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sync translog to disk after recovery from primary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15832</link><project id="" key="" /><description>Otherwise if that node is shutdown and restarted it might will have lost all operations
that were in the translog.

This might be responsible for build failures in the bwc tests like here: http://build-us-00.elastic.co/job/es_core_2x_window-2008/437/consoleText
</description><key id="125416323">15832</key><summary>sync translog to disk after recovery from primary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Recovery</label><label>blocker</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-07T15:32:32Z</created><updated>2016-01-07T15:54:56Z</updated><resolved>2016-01-07T15:54:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T15:35:12Z" id="169697349">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation for global search timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15831</link><project id="" key="" /><description>This commit adds documentation for the global search timeout setting
&#8220;search.default_search_timeout&#8221;.

Relates #12211
</description><key id="125413616">15831</key><summary>Add documentation for global search timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T15:20:30Z</created><updated>2016-02-14T18:41:07Z</updated><resolved>2016-01-07T16:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T15:56:09Z" id="169705754">LGTM
</comment><comment author="skearns64" created="2016-01-07T17:02:57Z" id="169730144">This is great! Should we also add the setting to the Modules section of the docs, where some other settings are documented? (e.g. a sub-section of https://www.elastic.co/guide/en/elasticsearch/reference/current/modules.html) 
</comment><comment author="jasontedor" created="2016-02-14T18:41:07Z" id="183947799">&gt; Should we also add the setting to the Modules section of the docs, where some other settings are documented?

I don't think so; unless I'm mistaken there aren't currently any modules that it naturally fits into, and I don't think we should create one just for this setting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog base flushes can be disabled after replication relocation or slow recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15830</link><project id="" key="" /><description>#10624 decoupled translog flush from ongoing recoveries. In the process, the translog creation was delayed to moment the engine is created (during recovery, after copying files from the primary). On the other side, TranslogService, in charge of translog based flushes, starts a background checker as soon as the shard is allocated. That checker performs it's first check after 5s expected the translog to be there. However, if the file copying phase of the recovery takes &gt;5s (likely!) or local recovery is slow, the check can run into an exception and never recover. The end result is that the translog based flush is completely disabled.

Note that this is mitigated but shard inactivity which triggers synced flush after 5m of no indexing.

Also - this is already fixed in master, where we don't have this background check,

Closes #15814
</description><key id="125407875">15830</key><summary>Translog base flushes can be disabled after replication relocation or slow recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label></labels><created>2016-01-07T14:48:50Z</created><updated>2016-01-07T16:56:02Z</updated><resolved>2016-01-07T16:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T15:51:44Z" id="169703957">left some comments - thanks boaz
</comment><comment author="bleskes" created="2016-01-07T16:36:29Z" id="169720413">@s1monw thanks. Pushed an update
</comment><comment author="s1monw" created="2016-01-07T16:38:55Z" id="169721089">this looks great LGTM
</comment><comment author="bleskes" created="2016-01-07T16:55:57Z" id="169727174">I pushed this to all the branched. Thanks @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated ec2 proxy settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15829</link><project id="" key="" /><description>The following settings have been deprecated for a while and can likely be removed in 3.0:
- cloud.aws.proxy_host --&gt; cloud.aws.proxy.host
- cloud.aws.proxy_port --&gt; cloud.aws.proxy.port
- cloud.aws.ec2.proxy_host --&gt; cloud.aws.ec2.proxy.host
- cloud.aws.ec2.proxy_port --&gt; cloud.aws.ec2.proxy.port
</description><key id="125399649">15829</key><summary>Remove deprecated ec2 proxy settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Discovery EC2</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T14:03:09Z</created><updated>2016-01-11T21:55:13Z</updated><resolved>2016-01-11T21:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-07T14:55:47Z" id="169685924">These settings have been deprecated in 2.2.0 which has not been released yet. See #15352

That being said, I agree that we can potentially remove these settings in master branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make MetaData parsing less lenient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15828</link><project id="" key="" /><description>Today this simply ignores everything that is not recognized.
</description><key id="125396636">15828</key><summary>Make MetaData parsing less lenient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T13:45:49Z</created><updated>2016-01-10T11:13:43Z</updated><resolved>2016-01-07T14:20:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-07T13:47:41Z" id="169668300">LGTM. Add a test that it blows up if you send it garbage?
</comment><comment author="jpountz" created="2016-01-07T14:09:48Z" id="169674533">@nik9000 I added a test as suggested
</comment><comment author="nik9000" created="2016-01-07T14:17:43Z" id="169676728">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove IcuCollationTokenFilterFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15827</link><project id="" key="" /><description>ICUCollationKeyFilter has been removed from Lucene 5 but its sitting around in Elasticsearch in the ICU plugin and just marked as deprecated. We should come up with a strategy to migrate from it to ICUCollationDocValuesField like the docs say to do.
</description><key id="125396275">15827</key><summary>Remove IcuCollationTokenFilterFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Analysis ICU</label><label>enhancement</label><label>low hanging fruit</label><label>v5.4.4</label></labels><created>2016-01-07T13:43:44Z</created><updated>2017-06-27T10:28:26Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T11:11:56Z" id="170335172">Related to https://github.com/elastic/elasticsearch/issues/13149
</comment><comment author="Tenzil" created="2016-05-25T09:49:04Z" id="221525712">As you said deprecated, How can i use ICU . is there any alternative way is it brought in Elastic Search ?. Or just tell how can i moves alphanumeric values to down of the result sets
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect field in mapping exception while indexing a document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15826</link><project id="" key="" /><description>We are  trying to index a document:
{u'App ID': u'53fca404af36ab0e4c966503',
 u'App Name': u'Test',
 'Signed Up Date': 'Tue Aug 26 2014 20:43:08 GMT+0530 (IST)',
 u'_id': '5661506c6905a8404581a775',
 u'cr_t': datetime.datetime(2015, 12, 4, 8, 35, 56, 772000),
 u'uid': u'53fca404af36ab0e4c966503'}

In the index Mapping:
"Signed Up Date" is of type long

The value which we are trying to index is not a string:
"Tue Aug 26 2014 20:43:08 GMT+0530 (IST)"

The problem is the exception which I am getting is:
TransportError(400, u'RemoteTransportException[[user-node3][inet[/172.31.33.23:9300]][indices:data/write/index]]; nested: MapperParsingException[failed to parse [moe_geo_ip_lnglat.Signed Up Date]]; nested: NumberFormatException[For input string: "Tue Aug 26 2014 20:43:08 GMT+0530 (IST)"]; ')

The field name in the exception comes as -
 moe_geo_ip_lnglat.Signed Up Date.

"moe_geo_ip_lnglat"  and  "Signed Up Date" are two separate fields in the index.

The Exception should only contain "Signed Up Date" as the field name it failed to parse.

Further Information:
moe_geo_ip_lnglat is a geo_point field which is mapped as a geo_point  using template.

Also we also tried the same indexing in another Elasticsearch cluster which doesn't have any such template and it works fine.

Also this incorrect field name exception is also happening with other fields as well with moe_geo_ip_lnglat coming as the parent field.
</description><key id="125393530">15826</key><summary>Incorrect field in mapping exception while indexing a document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shashank-moengage</reporter><labels /><created>2016-01-07T13:26:05Z</created><updated>2016-01-08T09:27:11Z</updated><resolved>2016-01-08T09:27:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-08T09:27:11Z" id="169945234">Please join us on discuss.elastic.co. We can help you there.

Also try to format your code correctly so it's more readable.
Also try to provide a script which reproduces your problem.

From what you wrote, the only thing I can tell is that: Yes, `Tue Aug 26 2014 20:43:08 GMT+0530 (IST)` is not a number. :) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Catch tragic even inside the checkpoint method rather than on the caller side</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15825</link><project id="" key="" /><description>We call this method in the buffered and unbuffered writer but never catch tragic
event exceptions on the unbuffered one. This change moves the check into the checkpoint method
to streamline the tragic event handling.

Note this is not a problem in master since it has only one impl
</description><key id="125392302">15825</key><summary>Catch tragic even inside the checkpoint method rather than on the caller side</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label></labels><created>2016-01-07T13:17:18Z</created><updated>2016-01-07T14:31:28Z</updated><resolved>2016-01-07T14:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T13:18:25Z" id="169660985">this was discovered by a beefed up test here: http://build-us-00.elastic.co/job/es_core_22_small/27/testReport/junit/org.elasticsearch.index.translog/TranslogTests/testFatalIOExceptionsWhileWritingConcurrently/ and here http://build-us-00.elastic.co/job/es_core_21_centos/900/testReport/junit/org.elasticsearch.index.translog/TranslogTests/testFatalIOExceptionsWhileWritingConcurrently/
</comment><comment author="bleskes" created="2016-01-07T14:30:15Z" id="169679586">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log uncaught exceptions from scheduled once tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15824</link><project id="" key="" /><description>`ScheduledThreadPoolExecutor` allows you to schedule tasks to run once or periodically at the future. If such a task throws an exception, that exception is caught and reported in the future that `ScheduledThreadPoolExecutor#schedule` returns. However, we typically do not capture the future / do not test it for errors. This results in exception being swallowed and not reported. To mitigate this we now wrap any command in a LoggingRunnable  (already used for periodic tasks).  Also, RunnableCommand is changed not to swallow exception but percolate them further for reporting by the future.

Discovered while researching #15814 
</description><key id="125388328">15824</key><summary>Log uncaught exceptions from scheduled once tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>:Logging</label><label>blocker</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T12:50:41Z</created><updated>2016-02-03T12:14:32Z</updated><resolved>2016-01-07T13:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T12:51:29Z" id="169655309">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to retrieve in an aggregations request only pipeline aggs results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15823</link><project id="" key="" /><description>I have explained this matter in several other issues, but it's time to make it a ticket on its own.
Very often programmers find themselves in the need to post-process the aggregation results computed by _Elasticsearch_. Since October 2015 the pipeline aggregations are officially avaliable to everyone, so a bunch of use cases can now be handled by just crafting a more elaborate search query.

That's **very good**, but not enough yet, because clients need also to be off-loaded in terms of network traffic and memory. For now, they receive and are forced to load from the network reply the results of +completely uninteresting+, intermediate aggregations.

Ideally, we should have control on the aggregation level whether its results should be returned or not (a `prune` property accepted by all aggregations would be just fine). Alternatively, one could also prune from the results the aggregations used in pipeline aggregations via a search-wide flag called (say) `prunePipelinedAggs` with three possible values:
- _`false`_: _default_, for backwards compatibility (but I would vote for _`basic`_ as default value)
- _`basic`_: suppresses only the results of basic aggregations that serve as source data for pipeline aggs; the results of "unrefined" aggregations remain untouched, in the reply
- _`all`_: suppresses the results of all aggregations (both basic and pipeline) that serve as source data for pipeline aggs

Wording may vary, but you get the idea. Particularly "basic" is not an established term.
</description><key id="125386341">15823</key><summary>Ability to retrieve in an aggregations request only pipeline aggs results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acarstoiu</reporter><labels /><created>2016-01-07T12:37:00Z</created><updated>2016-06-04T03:53:21Z</updated><resolved>2016-01-10T11:09:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T11:09:36Z" id="170334939">@acarstoiu Use response filtering to return just the aggs that you want.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering
</comment><comment author="acarstoiu" created="2016-01-14T13:25:16Z" id="171643877">Yes, I found this on my own in June last year, see [this comment](https://github.com/elastic/elasticsearch/issues/9876#issuecomment-113288211). But that's just a workaround, isn't it?!
</comment><comment author="shuangshui" created="2016-05-26T06:58:48Z" id="221792792">@acarstoiu  Me to need this feature.
In my case the bucket is 50 Mb big, but it's no use to me; I just want the bucket size (returned by the reducer)
</comment><comment author="acarstoiu" created="2016-05-26T09:38:34Z" id="221824817">Use a pipeline aggregation and follow the @clintongormley's link. 
</comment><comment author="shuangshui" created="2016-05-28T05:10:53Z" id="222290684">@acarstoiu  thanks. this do solve my problem 
but still is needs to sort the bucket and load all of them in to the server mem and return part of the result. This is indeed not a very efficient solution? dont know if i'm right
</comment><comment author="acarstoiu" created="2016-05-28T15:36:00Z" id="222314515">**Off topic**: please stop using a mechanical translator and learn the language. English has _by far the simplest grammar_ among the European languages, it's the best you can get in terms of simplicity (and I know the Chinese grammar is way simpler).

And now to the matter: yes, it is less than optimal, that's why this issue exists (albeit closed - @clintongormley has yet to explain why).
</comment><comment author="clintongormley" created="2016-06-01T14:06:29Z" id="223003002">@acarstoiu kindly refrain from castigating other users about their level of English.  
</comment><comment author="acarstoiu" created="2016-06-02T07:34:42Z" id="223217391">Well, I did ponder whether to write that or not, but I honestly believe it helps the guy a lot more than being "politically correct", a term invented in the west. Try using a mechanical translator for _un &#537;ut &#238;n fund &#238;nseamn&#259; un pas &#238;nainte_ :v:
</comment><comment author="shuangshui" created="2016-06-04T03:53:21Z" id="223734401">@acarstoiu @clintongormley  thanks a lot for your kindness.I'll improve my English although I was not using google translator.so embarrassing.
**On the topic**, when doing aggregation in Es, it will sort the bucket by doc_count by default.
In my case the query usually involves 1 million distinct values to group by. So it's very slow(about 20 seconds) . I'm wondering if there is a way to turn off the ordering/sorting in aggregation, which I think will speed up my query a lot&#65281;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.NullPointerException while updating document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15822</link><project id="" key="" /><description>Using this docker container: https://hub.docker.com/_/elasticsearch/
Version: 2.1.0

Elastic throws NullPointerException while I'm trying to update single document using inline script

**First GET document**

```
curl -XGET 'http://172.18.0.3:9200/counters-20160107/counters/20160107T1100:1:3:2?routing=1:2&amp;pretty'
{
  "_index" : "counters-20160107",
  "_type" : "counters",
  "_id" : "20160107T1100:1:3:2",
  "_version" : 1,
  "_routing" : "1:2",
  "found" : true,
  "_source":{"company":2,"time":1452167502000,"counters":{...}}
}
```

**And update**

```
curl -XPOST 'http://172.18.0.3:9200/counters-20160107/counters/20160107T1100:1:3:2/_update?routing=1:2' -d@/root/tmp/update
{"error":{"root_cause":[{"type":"null_pointer_exception","reason":null}],"type":"null_pointer_exception","reason":null},"status":500}
```

**Content of /root/tmp/update**

```
"script" : {
  "inline": "ctx._source.company = cid",
  "params": {
    "cid": "1"
  }
}
```

**Log**

```
tail -f /var/log/elasticsearch/elasticsearch.log
[2016-01-07 12:26:25,986][INFO ][rest.suppressed          ] /counters-20160107/counters/20160107T1100:1:3:2/_update Params: {routing=1:2, index=counters-20160107, id=20160107T1100:1:3:2, type=counters}
java.lang.NullPointerException
```
</description><key id="125386180">15822</key><summary>java.lang.NullPointerException while updating document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">zim32</reporter><labels><label>:CRUD</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-07T12:36:07Z</created><updated>2016-01-12T16:13:54Z</updated><resolved>2016-01-12T16:13:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-07T14:13:06Z" id="169675258">Do you have a stack trace in the logs below the NullPointerException?
</comment><comment author="zim32" created="2016-01-07T14:27:11Z" id="169678638">No
</comment><comment author="clintongormley" created="2016-01-10T11:24:08Z" id="170336310">This NPE is caused by the malformed request body (missing opening `{`).  A simple recreation:

```
curl -XPOST "http://localhost:9200/t/t/1/_update" -d'
"doc": {}
'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed AvgBuilder and ExtendedStatsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15821</link><project id="" key="" /><description> This is in place of AvgAggregator.Factory and ExtendedStatsAggregator.Factory which are now the objects to use in the Java API for these aggregations
</description><key id="125381076">15821</key><summary>Removed AvgBuilder and ExtendedStatsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2016-01-07T12:00:25Z</created><updated>2016-01-12T11:00:57Z</updated><resolved>2016-01-12T10:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-11T16:08:38Z" id="170598527">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>memory leak in BitsetFilterCache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15820</link><project id="" key="" /><description>When we have many indices and part of them contains nested type, we can find that the heap is cost by BitsetFilterCache very quickly. It can be reproduced on ES2.1 under the default setting. Here is the steps:
1. create two indices "index1" and "index2"
2. add type "tweet" with nested type "comments" to "index1"
3. add a new tweet doc to "index1"
4. wait until it finishes the refresh
5. dump the heap, you can find that the "loadedFilters" field of BitsetFilterCache of "index2" contains the query result of "index1".

And the root cause is:

BitSetProducerWarmer does not check the index name in the method BitSetProducerWarmer#warmNewReaders. And IndicesWarmer notifies every BitSetProducerWarmer even the indexShard does not belong to the index of the BitSetProducerWarmer
</description><key id="125378052">15820</key><summary>memory leak in BitsetFilterCache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jamesjin</reporter><labels /><created>2016-01-07T11:41:37Z</created><updated>2016-01-12T14:01:29Z</updated><resolved>2016-01-08T08:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-07T16:05:42Z" id="169708259">This is a very good observation, I could reproduce it.
</comment><comment author="ycombinator" created="2016-01-12T14:01:28Z" id="170920169">Hi @jamesjin, thanks for reporting this issue! I work in Developer Relations at Elastic. I understand it must've taken you a significant bit of time to reproduce it and figure out the root cause. Thank you for doing that. As a token of our appreciation, we'd like to send you a small thank you gift. Could you send us a quick "hello" email us at developerrelations [at] elastic.co and we'll figure out how best to get you the gift there? Thanks again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to rollback writer on close (NPE)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15819</link><project id="" key="" /><description>Got this NPE today on 1 of our data nodes which then proceeded to recovery.
ES 2.1, OS Win2008 R2

```
[2016-01-07 11:17:22,718][WARN ][index.engine             ] [data-nd2] [.marvel-es-2016.01.07][0] failed to rollback writer on close
java.lang.NullPointerException
[2016-01-07 11:17:22,723][WARN ][index.engine             ] [data-nd2] [.marvel-es-2016.01.07][0] failed engine [refresh failed]
java.lang.NullPointerException
[2016-01-07 11:17:22,724][WARN ][index.shard              ] [data-nd2] [.marvel-es-2016.01.07][0] Failed to perform scheduled engine refresh
[.marvel-es-2016.01.07][[.marvel-es-2016.01.07][0]] RefreshFailedEngineException[Refresh failed]; nested: NullPointerException;
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:686)
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
[2016-01-07 11:17:22,724][WARN ][indices.cluster          ] [data-nd2] [[.marvel-es-2016.01.07][0]] marking and sending shard failed due to [engine failure, reason [refresh failed]]
java.lang.NullPointerException
```
</description><key id="125377378">15819</key><summary>Failed to rollback writer on close (NPE)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skundrik</reporter><labels /><created>2016-01-07T11:37:15Z</created><updated>2016-01-07T14:48:17Z</updated><resolved>2016-01-07T14:48:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T12:51:05Z" id="169655237">I don't have any idea how that NullPointerException showed up without a stacktrace to be honest! @rmuir any idea?
</comment><comment author="skundrik" created="2016-01-07T14:25:29Z" id="169678297">I came across this article which talks about JVM optimisation of NullPointerException. It might be worth adding this as a standard start-up option.

http://jawspeak.com/2010/05/26/hotspot-caused-exceptions-to-lose-their-stack-traces-in-production-and-the-fix/
</comment><comment author="skundrik" created="2016-01-07T14:38:30Z" id="169681572">Based on the above  I went to see if I can find the first occurrence of it and came up with this which seems to indicate that `stats2` doesn't contain the `readerCoreKey` any more.

```
[2015-12-23 11:47:49,949][WARN ][index.engine             ] [data-nd2] [logstash-2015.12.23-v1][1] failed to rollback writer on close
java.lang.NullPointerException
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$1.onDocIdSetEviction(IndicesQueryCache.java:158)
    at org.apache.lucene.search.LRUQueryCache.clearCoreCacheKey(LRUQueryCache.java:313)
    at org.apache.lucene.search.LRUQueryCache$1.onClose(LRUQueryCache.java:276)
    at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:168)
    at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:157)
    at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:175)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.ReadersAndUpdates.dropReaders(ReadersAndUpdates.java:182)
    at org.apache.lucene.index.IndexWriter$ReaderPool.dropAll(IndexWriter.java:603)
    at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2075)
    at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2033)
    at org.elasticsearch.index.engine.InternalEngine.closeNoLock(InternalEngine.java:993)
    at org.elasticsearch.index.engine.Engine.failEngine(Engine.java:540)
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:685)
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-12-23 11:47:49,949][WARN ][index.engine             ] [data-nd2] [logstash-2015.12.23-v1][1] failed engine [refresh failed]
java.lang.NullPointerException
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$1.onDocIdSetEviction(IndicesQueryCache.java:158)
    at org.apache.lucene.search.LRUQueryCache.clearCoreCacheKey(LRUQueryCache.java:313)
    at org.apache.lucene.search.LRUQueryCache$1.onClose(LRUQueryCache.java:276)
    at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:168)
    at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:157)
    at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:175)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:359)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.IndexReader.close(IndexReader.java:403)
    at org.apache.lucene.index.FilterDirectoryReader.doClose(FilterDirectoryReader.java:134)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:130)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:274)
    at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:189)
    at org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:678)
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-12-23 11:47:49,969][WARN ][index.shard              ] [data-nd2] [logstash-2015.12.23-v1][1] Failed to perform scheduled engine refresh
[logstash-2015.12.23-v1][[logstash-2015.12.23-v1][1]] RefreshFailedEngineException[Refresh failed]; nested: NullPointerException;
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:686)
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$1.onDocIdSetEviction(IndicesQueryCache.java:158)
    at org.apache.lucene.search.LRUQueryCache.clearCoreCacheKey(LRUQueryCache.java:313)
    at org.apache.lucene.search.LRUQueryCache$1.onClose(LRUQueryCache.java:276)
    at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:168)
    at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:157)
    at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:175)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:359)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.IndexReader.close(IndexReader.java:403)
    at org.apache.lucene.index.FilterDirectoryReader.doClose(FilterDirectoryReader.java:134)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:130)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:274)
    at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:189)
    at org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:678)
    ... 5 more
[2015-12-23 11:47:49,989][WARN ][indices.cluster          ] [data-nd2] [[logstash-2015.12.23-v1][1]] marking and sending shard failed due to [engine failure, reason [refresh failed]]
java.lang.NullPointerException
    at org.elasticsearch.indices.cache.query.IndicesQueryCache$1.onDocIdSetEviction(IndicesQueryCache.java:158)
    at org.apache.lucene.search.LRUQueryCache.clearCoreCacheKey(LRUQueryCache.java:313)
    at org.apache.lucene.search.LRUQueryCache$1.onClose(LRUQueryCache.java:276)
    at org.apache.lucene.index.SegmentCoreReaders.notifyCoreClosedListeners(SegmentCoreReaders.java:168)
    at org.apache.lucene.index.SegmentCoreReaders.decRef(SegmentCoreReaders.java:157)
    at org.apache.lucene.index.SegmentReader.doClose(SegmentReader.java:175)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.StandardDirectoryReader.doClose(StandardDirectoryReader.java:359)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.index.IndexReader.close(IndexReader.java:403)
    at org.apache.lucene.index.FilterDirectoryReader.doClose(FilterDirectoryReader.java:134)
    at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:253)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:130)
    at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:58)
    at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:274)
    at org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:189)
    at org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)
    at org.elasticsearch.index.engine.InternalEngine.refresh(InternalEngine.java:678)
    at org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:615)
    at org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:1255)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="s1monw" created="2016-01-07T14:45:25Z" id="169683365">great, that helps a lot
</comment><comment author="jpountz" created="2016-01-07T14:48:16Z" id="169684142">Fixed in 2.1.1 #15202
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic Analyzer in Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15818</link><project id="" key="" /><description>Dear ElasticSearch Team,

I have a custom analyzer ( custom_asciifolding is "mapping" type =&gt; ["&#225;=&gt;a", "&#233;=&gt;e" ... etc] ): 

... "analyzer": {
   "standard_custom": {
        "type": "custom",
        "char_filter": ["custom_asciifolding"],
        "filter": ["lowercase"],
        "tokenizer": "standard"
} ...

I have a contact register indexed with name property (value: Rub&#233;n Recacha)

When I put analyzer parameter in query string, does not work correctly:

{
  "query": {
    "query_string": {
      "query": "ruben*",
      "analyzer": "standard_custom"
    }
  }
}

==&gt; 0 results

{
  "query": {
    "query_string": {
      "query": "rub&#233;n*",
      "analyzer": "standard_custom"
    }
  }
}

==&gt; 1 results

It is a bug?

How I could to do a query changing the analyzer dynamically?

Thank you very much,

Best Regards,
Rub&#233;n Recacha.
</description><key id="125366038">15818</key><summary>Dynamic Analyzer in Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2016-01-07T10:24:01Z</created><updated>2016-01-07T11:01:47Z</updated><resolved>2016-01-07T11:01:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2016-01-07T10:30:22Z" id="169623352">I use the version 1.4.4 but I think is not relevant in this case.

Regards.
</comment><comment author="dadoonet" created="2016-01-07T11:01:47Z" id="169630732">Please use discuss.elastic.co.

Wildcards are not analyzed. Have a look at https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html and `analyze_wildcard` option in case it helps.

If you have any other question, please join us on discuss.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for GET _aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15817</link><project id="" key="" /><description>`GET /{index}/_aliases/{name}` is supposed to filter aliases by index and/or by name.

But if you want to filter only by alias `name`, it gives you back all indices:

``` sh
curl -XDELETE localhost:9200/foo,bar?pretty
curl -XPUT localhost:9200/foo?pretty
curl -XPUT localhost:9200/bar?pretty
curl -XPOST localhost:9200/_aliases?pretty -d '{
    "actions" : [
        { "add" : { "index" : "foo", "alias" : "myalias" } }
    ]
}'

# Correct
curl -XGET localhost:9200/_aliases?pretty
# {
#   "bar" : {
#     "aliases" : { }
#   },
#   "foo" : {
#     "aliases" : {
#       "myalias" : { }
#     }
#   }
# }

# Correct
curl -XGET localhost:9200/foo/_aliases?pretty
# {
#   "foo" : {
#     "aliases" : {
#       "myalias" : { }
#     }
#   }
# }

# Incorrect
curl -XGET localhost:9200/_aliases/myalias?pretty
# {
#   "bar" : {
#     "aliases" : { }
#   },
#   "foo" : {
#     "aliases" : {
#       "myalias" : { }
#     }
#   }
# }


# Correct
curl -XGET localhost:9200/_alias?pretty
# {
#   "bar" : {
#     "aliases" : { }
#   },
#   "foo" : {
#     "aliases" : {
#       "myalias" : { }
#     }
#   }
# }

# Correct
curl -XGET localhost:9200/foo/_alias?pretty
# {
#   "foo" : {
#     "aliases" : {
#       "myalias" : { }
#     }
#   }
# }

# Correct
curl -XGET localhost:9200/_alias/myalias?pretty
# {
#   "foo" : {
#     "aliases" : {
#       "myalias" : { }
#     }
#   }
# }
```

We don't document `GET _aliases` anymore so may be we should remove its support as we can do the same thing with `GET _alias` API.

Might be related to https://github.com/elastic/elasticsearch/issues/4743
Related to https://github.com/elastic/elasticsearch-js/issues/331
</description><key id="125365915">15817</key><summary>Remove support for GET _aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2016-01-07T10:23:12Z</created><updated>2016-01-13T12:55:11Z</updated><resolved>2016-01-13T12:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T11:03:56Z" id="170334558">There used to be a difference between GET _alias and GET _aliases (the details of which escape me) which Kibana depended on, but I think this has been resolved.  @rashidkpc could you confirm?
</comment><comment author="rashidkpc" created="2016-01-10T20:54:02Z" id="170393150">@spalger you were looking at this the other day. Where did we decide to go with it?
</comment><comment author="spalger" created="2016-01-10T21:05:23Z" id="170393845">I was looking at this from the perspective of removing it from the client, not sure how it relates to Kibana.
</comment><comment author="javanna" created="2016-01-13T11:41:50Z" id="171263126">this appears to be a duplicate of #13906. +1 for removing asap, especially given that filtering aliases by name doesn't work.
</comment><comment author="dadoonet" created="2016-01-13T12:55:11Z" id="171282282">Thank you @javanna. For whatever reason I did not find this issue. Closing this then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct documentation of discovery.zen.ping.unicast.hosts regarding support for port ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15816</link><project id="" key="" /><description>The documentation of discovery.zen.ping.unicast.hosts explicitly mentions port-ranges but fails to mention that they are _not supported_ and only the first port is checked.
There are several Issues/PRs relating to changing this but none have made it yet so please document that port-range format is allowed but not supported and that if a user wishes to address multiple ports they must do explicitly

```
server1:9300,server1:9301
```

https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html
#8833 will allow the first two ports to be scanned. Older items relating to this are #99, #7561, #7090, #12999, elastic/elasticsearch-cloud-aws#197
</description><key id="125345006">15816</key><summary>Correct documentation of discovery.zen.ping.unicast.hosts regarding support for port ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drekbour</reporter><labels><label>:Network</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-01-07T08:01:37Z</created><updated>2016-03-01T11:20:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-07T08:18:53Z" id="169592543">@drekbour Thanks for opening this issue. Do you want to contribute a doc PR to fix that?
</comment><comment author="clintongormley" created="2016-01-10T10:59:41Z" id="170334402">We should probably also throw an exception when a user specifies a port range.
</comment><comment author="clintongormley" created="2016-03-01T11:20:12Z" id="190671863">I've updated the documentation, but we should still throw an exception when the user specifies a port range.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove a few more Xlint skips</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15815</link><project id="" key="" /><description /><key id="125320245">15815</key><summary>Remove a few more Xlint skips</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T04:32:17Z</created><updated>2016-01-07T13:34:28Z</updated><resolved>2016-01-07T13:34:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-07T08:01:19Z" id="169590298">LGTM
</comment><comment author="s1monw" created="2016-01-07T08:11:40Z" id="169591565">thank you @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog size not effectively limited by index.translog.flush_threshold_size?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15814</link><project id="" key="" /><description>I have a logstash index in my ES 2.1.1 cluster (2 nodes):

```
{
  "logstash-2016.01.07" : {
    "settings" : {
      "index" : {
        "refresh_interval" : "30s",
        "number_of_shards" : "8",
        "translog" : {
          "flush_threshold_ops" : "100000",
          "flush_threshold_size" : "1gb",
          "sync_interval" : "5s",
          "durability" : "async"
        },
        "query" : {
          "default_field" : "body"
        },
        "creation_date" : "1452124822948",
        "store" : {
          "compress" : {
            "stored" : "true"
          }
        },
        "number_of_replicas" : "1",
        "uuid" : "XXXXX",
        "version" : {
          "created" : "2010199"
        }
      }
    }
  }
}
```

So, the translogs should be limited in size.  However:

```
On one node:

# du -sh /disk*/logstash/nodes/0/indices/logstash-2016.01.07/*/translog/*.tlog
3.1G    /disk10/logstash/nodes/0/indices/logstash-2016.01.07/2/translog/translog-1.tlog
40M /disk11/logstash/nodes/0/indices/logstash-2016.01.07/1/translog/translog-35.tlog
44M /disk12/logstash/nodes/0/indices/logstash-2016.01.07/3/translog/translog-35.tlog
44M /disk1/logstash/nodes/0/indices/logstash-2016.01.07/0/translog/translog-35.tlog
3.0G    /disk2/logstash/nodes/0/indices/logstash-2016.01.07/7/translog/translog-1.tlog
3.0G    /disk3/logstash/nodes/0/indices/logstash-2016.01.07/4/translog/translog-1.tlog
39M /disk4/logstash/nodes/0/indices/logstash-2016.01.07/6/translog/translog-35.tlog
50M /disk5/logstash/nodes/0/indices/logstash-2016.01.07/5/translog/translog-35.tlog
```

and the other:

```
# du -sh /disk*/logstash/nodes/0/indices/logstash-2016.01.07/*/translog/*.tlog
2.7G    /disk11/logstash/nodes/0/indices/logstash-2016.01.07/6/translog/translog-1.tlog
3.1G    /disk12/logstash/nodes/0/indices/logstash-2016.01.07/0/translog/translog-1.tlog
30M /disk1/logstash/nodes/0/indices/logstash-2016.01.07/7/translog/translog-35.tlog
38M /disk2/logstash/nodes/0/indices/logstash-2016.01.07/4/translog/translog-35.tlog
2.9G    /disk3/logstash/nodes/0/indices/logstash-2016.01.07/5/translog/translog-1.tlog
45M /disk6/logstash/nodes/0/indices/logstash-2016.01.07/2/translog/translog-35.tlog
3.0G    /disk7/logstash/nodes/0/indices/logstash-2016.01.07/3/translog/translog-1.tlog
3.0G    /disk9/logstash/nodes/0/indices/logstash-2016.01.07/1/translog/translog-1.tlog
```

So it seems the translog size is not being limited to the 1GB set in flush_threshold_size?  Or do I misunderstand something here?
</description><key id="125318316">15814</key><summary>Translog size not effectively limited by index.translog.flush_threshold_size?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aquarapid</reporter><labels /><created>2016-01-07T04:12:21Z</created><updated>2016-01-07T17:18:39Z</updated><resolved>2016-01-07T17:18:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-07T08:37:12Z" id="169597778">hey @aquarapid I am curious, something looks not right here. Can I get the output of `localhost:9200/_cat/shards` by any chance? And can you also paste `localhost:9200/_stats/merge,refresh,store,translog,indexing` ?
</comment><comment author="bleskes" created="2016-01-07T09:33:16Z" id="169608875">also , do you see this in the logs anywhere? 

```
logger.warn("failed to flush shard on translog threshold", e);
```
</comment><comment author="aquarapid" created="2016-01-07T17:15:19Z" id="169735198">I did not see the translog fail flush message in the logs.

I do have an ongoing recovery of shards; which I pause during the day to not interfere with indexing.  The cluster contains 10TB or so of data (+replication).  At the moment there are 36 unassigned shards remaining.

shards and stats attached (slightly obfuscated):

[stats.txt](https://github.com/elastic/elasticsearch/files/81450/stats.txt)
[shards.txt](https://github.com/elastic/elasticsearch/files/81449/shards.txt)
</comment><comment author="bleskes" created="2016-01-07T17:18:39Z" id="169737136">thanks @aquarapid . We spend some time on this and believe to have found the issue (see #15830) . That Pr explains the issue if you don't have short periods of 5m without indexing.  I'm going to close this for now, please let me know if that's not the case.

Thanks so much for reporting this. It led us to find an important bug. You can use the _flush api to force the translog to be trimmed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Xlint:-override,-fallthrough,-static</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15813</link><project id="" key="" /><description>Adds `@SuppressWarnings("fallthrough")` in two places where the fallthrough
is used to implement well known hashing algorithms.
</description><key id="125317160">15813</key><summary>Remove Xlint:-override,-fallthrough,-static</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T03:57:16Z</created><updated>2016-01-07T13:34:40Z</updated><resolved>2016-01-07T13:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-07T08:46:23Z" id="169599475">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Standardize some methods on varargs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15812</link><project id="" key="" /><description>Right now we define the same sort of methods as taking String arrays and
string varargs. We should standardize on one and varargs is easier to
call so lets use varargs!
</description><key id="125305639">15812</key><summary>Standardize some methods on varargs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T02:02:28Z</created><updated>2016-01-07T02:42:13Z</updated><resolved>2016-01-07T02:42:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-07T02:05:00Z" id="169522737">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanups for Def</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15811</link><project id="" key="" /><description>Manually I:
1. Added some missing raw types warnings suppressions.
2. Removed some unused unchecked cast warning suppressions.
3. Added &lt;?&gt; to Class.

I let my IDE:
1. Remove unneeded casts.
2. Reorder imports (just ignore these, everyone does).
</description><key id="125302458">15811</key><summary>Cleanups for Def</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Scripting</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T01:30:36Z</created><updated>2016-03-10T18:56:21Z</updated><resolved>2016-01-07T16:03:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-07T01:33:29Z" id="169518498">I'm just getting cross-eyed over warnings so I'm doing small cleanups. This one was very large because of the removal of "unneeded" casts. All the tests pass and when I scanned the changes they seems correct but I'm not sure if they are all sane.
</comment><comment author="jdconrad" created="2016-01-07T15:55:38Z" id="169705619">LGTM.  Thanks for doing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add on_failure support for verbose _simulate execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15810</link><project id="" key="" /><description>updates the `_simulate?verbose` execution to properly support on_failure/Compound processors.

This includes changes to the `Processor` interface to introduce a notion of a `processor_id`. This is an optional property that is currently only used by the verbose-simulate to help users tag processors in the pipelines with ids so that their executions can be traced within a verbose-simulate response.

here is a sample query and response: https://gist.github.com/talevy/7b77e78a41e173a7dac1
</description><key id="125298292">15810</key><summary>[Ingest] Add on_failure support for verbose _simulate execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2016-01-07T00:53:44Z</created><updated>2016-01-15T23:04:02Z</updated><resolved>2016-01-15T23:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-08T08:56:11Z" id="169938527">Left two comment on the processor interface. Other then that I think this is the right direction.
</comment><comment author="BigFunger" created="2016-01-11T18:21:17Z" id="170640912">@talevy I am getting an unexpected result when running _simulate against a grok processor when I don't supply a 'field' property. I assumed I would get an error back. The input and output data that I used can be found [here](https://gist.github.com/BigFunger/0aa04577654e09918e35):
</comment><comment author="talevy" created="2016-01-11T23:55:53Z" id="170736379">updated
</comment><comment author="javanna" created="2016-01-13T08:39:47Z" id="171216518">I left a comment on null values. I am not a big fan of adding the getId method to the Processor interface if we use null in the majority of the cases. If the id is useful we should have it there, but have a good value for it all the time and not allow null. For instance we could automatically generate an id when it's not provided, using a simple counter (first processor has id "processor0", second one "processor1" etc.). Do you think this is feasible?
</comment><comment author="talevy" created="2016-01-13T21:11:28Z" id="171433919">@javanna updated to use _real_ processorids.

two things that I would like feedback on:
- the `getId` for `CompoundProcessor`. I just take a hash of the processors/onfailureprocessors. I don't think this value is that important.
- using `UUID` to generate a default processor_id instead of keeping counting state (I am impartial to which is better since order is not very meaningful when `on_failure`s exist).
</comment><comment author="martijnvg" created="2016-01-13T22:24:36Z" id="171456448">&gt; the getId for CompoundProcessor. I just take a hash of the processors/onfailureprocessors. I don't think this value is that important.

agreed, this id is never exposed to the outside world.

&gt; using UUID to generate a default processor_id instead of keeping counting state (I am impartial to which is better since order is not very meaningful when on_failures exist).

I think the random part is a bit too much here. Maybe we should have a valid 'null id'? Or the getId() can return Optional&lt;String&gt; instead? I think the optional pipeline will also be used for stored pipelines, esp. when simulating docs in a stored pipeline.
</comment><comment author="talevy" created="2016-01-13T22:27:35Z" id="171457132">&gt; I think the random part is a bit too much here. Maybe we should have a valid 'null id'? Or the getId() can return Optional instead? I think the optional pipeline will also be used for stored pipelines, esp. when simulating docs in a stored pipeline.

I think I prefer @javanna's stance against `null`ish values. I also agree that random is a bit much. I will update to the sequential ID approach. what do you think of that @martijnvg?
</comment><comment author="martijnvg" created="2016-01-13T22:31:31Z" id="171458009">Agreed, sequential incrementing auto generated ids are better than random ones.
</comment><comment author="martijnvg" created="2016-01-15T08:11:04Z" id="171898002">@talevy I like the change from making the processor id just a simple tag. This makes it clear it is optional and we don't need the id generator part anymore. Left one question about the api design, but this PR looks good now.
</comment><comment author="javanna" created="2016-01-15T10:59:03Z" id="171933335">left a few comments too, looks good besides those
</comment><comment author="martijnvg" created="2016-01-15T22:54:59Z" id="172119811">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove some raw from ActionRequest subclasses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15809</link><project id="" key="" /><description>Renames "T" used in many subclasses of ActionRequest to "Self" and tightens
the type bounds.
</description><key id="125292803">15809</key><summary>Remove some raw from ActionRequest subclasses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-07T00:07:08Z</created><updated>2016-01-13T14:43:58Z</updated><resolved>2016-01-13T14:43:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-07T00:09:11Z" id="169504871">All the raw types warnings are getting to me. This feels like a first step to clean them up.

I think the name `Self` is more clear in this case than `T`. `T` just makes me think "generic type that I don't really care about" while `Self` makes me think "the type that `this` has".
</comment><comment author="nik9000" created="2016-01-12T16:00:51Z" id="170957366">@jasontedor do you have an opinion on this? I know you are +1 on warning squashing in general, but what about this particular change?
</comment><comment author="jasontedor" created="2016-01-12T20:15:44Z" id="171040937">@nik9000 I'm good with the change to address the raw types, but the change of the generic parameter type name is [breaking with Java convention](http://docs.oracle.com/javase/tutorial/java/generics/types.html) and I'd prefer to just keep `T` if you're okay with that?
</comment><comment author="nik9000" created="2016-01-12T20:41:03Z" id="171047629">&gt; I'd prefer to just keep T if you're okay with that?

`T` is really confusing to me because its so overused. To me it just means "some generic type that invariable isn't documented". I at least get `E`, `K`, `V`, and even `N` from the java convention. That is one of the java conventions I'm happy to ignore intentionally. We actually do it in lots of places, like `TransportAction` and `ActionListener`.
</comment><comment author="jasontedor" created="2016-01-12T20:44:36Z" id="171048575">&gt; We actually do it in lots of places, like `TransportAction` and `ActionListener`.

Fair enough, but is `Request` okay instead of `Self` then?
</comment><comment author="nik9000" created="2016-01-12T20:46:39Z" id="171049393">&gt; Fair enough, but is `Request` okay instead of `Self` then?

Sure.
</comment><comment author="nik9000" created="2016-01-13T14:41:50Z" id="171311537">@jasontedor renamed!
</comment><comment author="jasontedor" created="2016-01-13T14:43:37Z" id="171312123">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't create new EngineSearcher for version lookup when indexing each document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15808</link><project id="" key="" /><description>Today we use `acquireSearcher` which in turn makes an `EngineSearcher` for each doc we need to retrieve the version for (when it's not in the live version map) ... this is maybe non-trivial garbage ... I think instead we should just pull `IndexSearcher` ourselves?
</description><key id="125287666">15808</key><summary>Don't create new EngineSearcher for version lookup when indexing each document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label></labels><created>2016-01-06T23:32:52Z</created><updated>2016-01-10T11:02:11Z</updated><resolved>2016-01-07T10:04:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T23:38:55Z" id="169498159">The new code being more complex, I'd like to be sure the reduced garbage creation is noticeable?
</comment><comment author="mikemccand" created="2016-01-07T00:22:27Z" id="169507393">OK I will test to see if there's any measurable change to indexing perf ...
</comment><comment author="mikemccand" created="2016-01-07T10:04:42Z" id="169615521">OK, I tested at defaults (worst case for GC) and any impact is under the measurement noise ... so I won't make this change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase query on field mapped as long does not filter results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15807</link><project id="" key="" /><description>This issue was raised by a user in irc. They were trying to filter on a field in kibana with no success. Upon digging a bit deeper we found that his field was mapped as a long. When I tried to create a filter in kibana on that field, none of the results were filtered out. Kibana creates filters on long fields as phrase queries.

To see the issue input these commands into sense.

```
DELETE /xamox

POST /xamox
{
  "mappings": {
    "test": {
      "properties": {
        "long_field": {
          "type": "long"
        },
        "double_field": {
          "type": "double"
        }
      }
    }
  }
}

POST /xamox/test
{
  "long_field": 0.253388538525881,
  "double_field": 0.253388538525881
}

POST /xamox/test
{
  "long_field": 0.640691024131749,
  "double_field": 0.640691024131749
}

POST /xamox/test
{
  "long_field": 0.0878632360473135,
  "double_field": 0.0878632360473135
}

#basic query
GET /xamox/test/_search

#phrase query on long
POST /xamox/test/_search
{
  "query": {
    "match": {
      "long_field": {
        "query":0.640691024131749,
        "type":"phrase"
      }
    }
  }
}

#phrase query on double
POST /xamox/test/_search
{
  "query": {
    "match": {
      "double_field": {
        "query":0.640691024131749,
        "type":"phrase"
      }
    }
  }
}
```
</description><key id="125285391">15807</key><summary>Phrase query on field mapped as long does not filter results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BigFunger</reporter><labels /><created>2016-01-06T23:19:23Z</created><updated>2016-01-07T00:36:32Z</updated><resolved>2016-01-07T00:36:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-07T00:36:32Z" id="169509716">The phrase type is not the problem here. It works as expected, the problem is the coercion that is done on long mappings by default. This means that if your input contains a floating point it will be truncated to a long. As a result all the "long_field" in your example documents are all indexed under the same term: 0. This is why your query on long_field returns all the documents, it is internally translated into:

```
"match": {
   "long_field": {
      "query":0,
      "type":"phrase"
  }
} 
```

The solution for your problem would be to change the mapping to use double instead of long.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid Tribe node setting tribe.blocks.write.indices in documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15806</link><project id="" key="" /><description>The tribe node setting, to block writes to specific indices, is invalid in the docs for all versions (https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-tribe.html): 

&gt; tribe.blocks.indices.write: hk_,ldn_

The valid setting (as found in code) and tested is spelled:

&gt; tribe.blocks.write.indices: hk_,ldn_
</description><key id="125281765">15806</key><summary>Invalid Tribe node setting tribe.blocks.write.indices in documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thomasquatorze</reporter><labels /><created>2016-01-06T22:53:04Z</created><updated>2016-01-13T17:06:05Z</updated><resolved>2016-01-13T17:06:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Version.LATEST instead of Lucene.VERSION</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15805</link><project id="" key="" /><description>There was a TODO for it.
</description><key id="125279302">15805</key><summary>Version.LATEST instead of Lucene.VERSION</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T22:37:08Z</created><updated>2016-01-06T23:25:48Z</updated><resolved>2016-01-06T23:25:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T23:05:35Z" id="169490527">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Installation Elasticsearch 2.1.1 as root </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15804</link><project id="" key="" /><description>I installed Elasticsearch 2.1.1 as root  and getting error

"Exception in thread "main" java.lang.RuntimeException: don't run elasticsearch as root"

If I create a ES_USER=esuser  - what should be the group permission etc it should have so that I can start od Elasticsearch as a ES_USER.

Can someone provide a sample ES_USER ?
</description><key id="125268701">15804</key><summary>Installation Elasticsearch 2.1.1 as root </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">satapsa</reporter><labels /><created>2016-01-06T21:35:32Z</created><updated>2016-01-06T22:07:41Z</updated><resolved>2016-01-06T22:07:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T22:07:41Z" id="169477959">Please ask such questions at http://discuss.elastic.co (with more details if possible, in particular how you installed elasticsearch). We only use github issues for bugs / feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] throw exception when invalid locale is provided to the date processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15803</link><project id="" key="" /><description>We should throw an exception when `locale` is not valid. Currently, this is silently ignored and set to the `""` locale. by switching to the `Locale.Builder`, an exception is thrown and can be caught.
</description><key id="125262215">15803</key><summary>[Ingest] throw exception when invalid locale is provided to the date processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>review</label></labels><created>2016-01-06T21:00:47Z</created><updated>2016-01-11T18:57:01Z</updated><resolved>2016-01-11T18:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-08T09:24:39Z" id="169944831">good catch LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow Query/Filter Cache to be disabled by index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15802</link><project id="" key="" /><description>For most users, having the filter cache being specified across the nodes makes a ton of sense, particularly for hot/cold architectures.

However, at extreme scales, this is sometimes not enough unless you go the extra mile with your cluster to setup a hot/warm/cold (ad nauseam) setup to allow an extra tier(s) of nodes to setup. For a lot of use cases, this extra level of node management adds a lot of complexity that only grows with the size of the cluster.

With that in mind, it would be useful to restrict (on/off only) the memory caches that get used by any index. This would enable curation to disable caches for any given index as it ages, thereby reducing its overall burden as a step before closing them.
</description><key id="125253346">15802</key><summary>Allow Query/Filter Cache to be disabled by index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Cache</label><label>feature</label><label>low hanging fruit</label><label>v5.0.0-alpha2</label></labels><created>2016-01-06T20:12:23Z</created><updated>2016-04-11T16:16:15Z</updated><resolved>2016-04-11T16:06:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T12:30:38Z" id="170341181">I understand each word individually, but I have no idea what they mean together :)  What exactly are you suggesting?
</comment><comment author="pickypg" created="2016-01-10T19:36:51Z" id="170384189">Allow an index to explicitly disable caching, so that old indices can avoid caching.
</comment><comment author="clintongormley" created="2016-01-15T11:16:36Z" id="171936374">Discussed in FixItFriday - we should add an index-level setting allowing the query cache to be disabled per index.
</comment><comment author="jpountz" created="2016-01-22T17:26:07Z" id="173984703">This is actually already possible to do by setting `index.queries.cache.type: none` in the index settings. Should we just document it?
</comment><comment author="pickypg" created="2016-01-22T17:32:51Z" id="173986754">Yeah, I think that covers it pretty well. :)
</comment><comment author="jpountz" created="2016-04-11T16:09:35Z" id="208429144">Finally we decided to go with `index.queries.cache.enabled: false`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use CyclicBarriers for sychronizing driver and test threads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15801</link><project id="" key="" /><description>This commit modifies some tests to use CyclicBarriers to correctly and
simply sychronize driver and test threads.
</description><key id="125252527">15801</key><summary>Use CyclicBarriers for sychronizing driver and test threads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T20:09:30Z</created><updated>2016-01-07T01:09:55Z</updated><resolved>2016-01-07T01:09:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-06T20:38:02Z" id="169454370">Makes sense to me. I'd prefer to record exceptions in the inner threads in some AtomicReference and then wrap and rethrow if its non-null after the threads are done rather than fail inside the thread because you don't get the stack trace. Its a pain not to have a stack trace!
</comment><comment author="nik9000" created="2016-01-07T01:01:57Z" id="169513254">LGTM
</comment><comment author="jasontedor" created="2016-01-07T01:09:55Z" id="169514542">Thanks for the review @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.1.1 doesn't automatically start on Ubuntu 16.04</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15800</link><project id="" key="" /><description>I have been testing 16.04 with Logstash, Kibana and ElasticSearch and so far, Logstash and Kibana (I'm using the init.d file from Kibana 4.1.1) start just fine with the init.d file at boot time.

But ElasticSearch doesn't start at boot time and it doesn't even run the file at boot. However, if I login on the system and manually start it using 'service elasticsearch start', it works just fine.

I tested the exact same scenario with 14.04 and it works just fine and starts at boot. So, there is something Ubuntu 16.04 doesn't like in the init.d file.

Download URL for Ubuntu 16.04:
- http://cdimage.ubuntu.com/ubuntu-server/daily/
</description><key id="125249467">15800</key><summary>Elasticsearch 2.1.1 doesn't automatically start on Ubuntu 16.04</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ThomasdOtreppe</reporter><labels /><created>2016-01-06T19:52:10Z</created><updated>2017-04-21T09:52:50Z</updated><resolved>2016-01-06T21:38:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-06T19:56:16Z" id="169440681">Do the instructions in at the [repository](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html) page for making it start automatically work for you? The Elasticearch deb doesn't register itself to start on boot intentionally even though that is the normal thing for debs to do because we think that behavior is creepy in a distributed system.
</comment><comment author="ThomasdOtreppe" created="2016-01-06T21:38:02Z" id="169470833">I was doing the `update-rc.d elasticsearch defaults 95 10` that has been working forever until 16.04 that switched to systemd. The systemctl command worked perfectly.

Sorry about that.
</comment><comment author="gsouf" created="2017-04-21T09:52:50Z" id="296147887">Hi @nik9000  
The link you sent is not working anymore, would you like to update it please?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Paginate from _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15799</link><project id="" key="" /><description>Currently the `from` key on a request takes an integer and paging picks up after chopping off that number of results from the top of the queued result set.

This works well for static sets. However in high write load situations, in which we're sorting by time, it becomes a problem. We may end up with missing results that it seems should be there, but have a hard time expressing where we want the results to start.

Take for example, a logging case in which I want to see the 10 records before and the 10 records after some event.

I could read the time from the event, and do a sort in both directions. However I'd have to hope that 10 things didn't happen at the same time, common in say, error scenarios where a number of errors (eg, one per shard in a distributed system, all happen at exactly the same time. If that was the case there's a very good chance that my record wouldn't actually appear in the results. 

Of course we could say the time resolution wasn't high enough, or that events would be of arbitrary order anyway, but the goal here really is to make sure that our "context" event is included and the results around it are the same as they would be in any previous request.

Ideally I could request the following, assuring that I get `event24587302` as the first result in the chronologically sorted list

```
{
 sort: { "@timestamp" : "desc" }, // And fire another with "asc" at the same time, in an _msearch
 from: "event24587302"
}
```

Stems from issues in https://github.com/elastic/kibana/issues/275
</description><key id="125237735">15799</key><summary>Paginate from _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-01-06T18:51:25Z</created><updated>2016-02-29T19:11:46Z</updated><resolved>2016-02-13T22:19:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-06T20:51:23Z" id="169457385">That seems hard to achieve considering the way the query is executed. In a multi shards environment you would need at least another round trip to retrieve the timestamp value associated with the from _id. Additionally this does not solve the case where the timestamps are equals. If your problem is just to resolve the case where the timestamps are equals then you can use the script sort, something like:

```
"sort": [
    {
      "timestamp": {
        "order": "desc"
      }
    },
    {
      "_script": {
        "type": "number",
        "script": {
          "inline": "if (_fields['_id'].value == fromId) { return 0 } else { return 1}",
          "params": {
            "fromId": "$eventId"
          }
        },
        "order": "asc"
      }
    }
  ] 
```

Unfortunately this would be very slow because the _id fields is stored but has no doc values. You could activate doc values for the field but the memory usage would make the feature very costly.
</comment><comment author="jpountz" created="2016-01-06T22:11:11Z" id="169478686">related to #8192
</comment><comment author="clintongormley" created="2016-01-10T11:50:46Z" id="170337942">As long as your UIDs increase, you could do the following to get the 10 results before and after a particular document:

```
POST t/t/_bulk
{"index": {"_id": "1"}}
{"timestamp":"2016-01-01T00:00:00Z"}
{"index": {"_id": "2"}}
{"timestamp":"2016-01-01T00:00:00Z"}
{"index": {"_id": "3"}}
{"timestamp":"2016-01-01T00:00:00Z"}
{"index": {"_id": "4"}}
{"timestamp":"2016-01-01T00:00:00Z"}
{"index": {"_id": "5"}}
{"timestamp":"2016-01-01T11:11:11Z"}
```

Get 10 docs after doc 3:

```
GET _search
{
  "query": {
    "bool": {
      "filter": {
        "bool": {
          "should": [
            {
              "range": {
                "timestamp": {
                  "gt": "2016-01-01T00:00:00Z"
                }
              }
            },
            {
              "bool": {
                "must": [
                  {
                    "term": {
                      "timestamp": "2016-01-01T00:00:00Z"
                    }
                  },
                  {
                    "range": {
                      "_uid": {
                        "gt": "t#3"
                      }
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  "sort": [
    {
      "timestamp": "asc"
    },
    {
      "_uid": "asc"
    }
  ]
}
```

Get 10 docs before doc 3:

```
GET _search
{
  "query": {
    "bool": {
      "filter": {
        "bool": {
          "should": [
            {
              "range": {
                "timestamp": {
                  "lt": "2016-01-01T00:00:00Z"
                }
              }
            },
            {
              "bool": {
                "must": [
                  {
                    "term": {
                      "timestamp": "2016-01-01T00:00:00Z"
                    }
                  },
                  {
                    "range": {
                      "_uid": {
                        "lt": "t#3"
                      }
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  "sort": [
    {
      "timestamp": "desc"
    },
    {
      "_uid": "desc"
    }
  ]
}
```
</comment><comment author="dtr2" created="2016-01-17T16:37:01Z" id="172347659">Is it possible to have a kibana search string to show 10 lines of context around line with _id="X"
Looking at the IDs, they don't seem numeric to me.
</comment><comment author="simianhacker" created="2016-02-02T23:08:44Z" id="178878061">@clintongormley This solution is not very realistic for a real world (distributed) system that is not single threaded
</comment><comment author="clintongormley" created="2016-02-13T22:19:14Z" id="183765461">@simianhacker i don't follow why?  Also, the new `search_after` feature (https://github.com/elastic/elasticsearch/pull/16125) makes this even easier.  I think with `search_after` implemented, there is nothing else to do here.
</comment><comment author="rashidkpc" created="2016-02-29T18:58:56Z" id="190333171">@clintongormley I can't think of a way to ensure unique increasing UIDs across distributed writers, eg, logstash. Is there an elasticsearch option to ensure that?
</comment><comment author="rashidkpc" created="2016-02-29T19:03:28Z" id="190335246">Or does that not matter with `search_after`?
</comment><comment author="jimczi" created="2016-02-29T19:11:27Z" id="190338908">@rashidkpc it doesn't matter with search_after as long as the UIDs are unique.
</comment><comment author="clintongormley" created="2016-02-29T19:11:46Z" id="190338987">Doesn't matter.  The main sort is on (eg) timestamp, the UID is used purely as a tie breaker for documents that have the same timestamp
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexShardRoutingTable#activeShards should include relocation targets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15798</link><project id="" key="" /><description>Currently `IndexShardRoutingTable#activeShards` does not include relocation targets. This means that when a relocation target is needed (e.g., [in `TransportReplicationAction`](https://github.com/elastic/elasticsearch/blob/3b192cfc7488a02387501a598fe177234d4149ae/core/src/main/java/org/elasticsearch/action/support/replication/TransportReplicationAction.java#L851)) we have to create the target relocation shard object. Instead, if `IndexShardRoutingTable#activeShards` included relocation targets, that could be cached and we can simplify places where the relocation target is needed.

Relates #15791 via a [comment](https://github.com/elastic/elasticsearch/pull/15791/files#r48978243) from @bleskes.
</description><key id="125237517">15798</key><summary>IndexShardRoutingTable#activeShards should include relocation targets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-06T18:50:10Z</created><updated>2016-12-12T17:03:33Z</updated><resolved>2016-12-12T17:03:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-12-12T17:03:33Z" id="266487649">The relocation target shard is now auto-cached by the relocation source object.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support multi level nested filters inside sort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15797</link><project id="" key="" /><description>I have hotel docs which have multiple rooms and each room has day wise price and inventory.
hotel &gt; room &gt; priceInventory

I have filters on all three levels. Say hotels with 5 star rating, rooms with "free wifi" amenity and having inventory of 2 for dates d1-d2 and between price p1-p2.

`For sorting, I want to calculate minimum price from all matched nested docs. But this isn't possible with regular sort, as I cannot specify filters on multiple nested levels inside it.` So I have to utilize script sorting. But inside a script accessing nested level fields is only possible with the _source field which is non-performant as described in https://github.com/elastic/elasticsearch/issues/15767.

Converting this to single nesting level and copying room attributes in every invPrice doesn't work either 
due to loss of grouping.

Mapping:

``` json
        "hotel": {
            "properties": {
               "name": {
                  "type": "string",
               },
               "id": {
                  "type": "string",
               },
               "roomTypes": {
                  "type": "nested",
                  "properties": {
                     "roomCode": {
                        "type": "string",
                        "index": "not_analyzed",
                     },
                     "roomType": {
                        "type": "string",
                        "index": "not_analyzed",
                     },
                     "amenities": {
                          "type": "string",
                          "index": "not_analyzed",
                     },
                     "invPrice": {
                          "type": "nested",
                          "properties": {
                             "date": {
                                "type": "date",
                                "format": "dateOptionalTime",
                             },
                             "inventory": {
                                "type": "integer",
                             },
                             "price": {
                                "type": "double",
                             }
                          }
                       }
                  }
               },
               "starRating": {
                  "type": "short",
               }
            }
        }
```
</description><key id="125233399">15797</key><summary>Support multi level nested filters inside sort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sumitjainn</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-01-06T18:26:38Z</created><updated>2016-09-01T12:36:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T12:25:37Z" id="170341005">@sumitjainn Would you mind expanding your example to include a copy-paste runnable recreation of what you are doing today, so that we have something concrete to play with?
</comment><comment author="sumitjainn" created="2016-01-11T07:00:06Z" id="170450076">@clintongormley  Here is an example query which fails

``` json
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "bool": {
          "must": [
            {
              "nested": {
                "path": "rooms",
                "query": {
                  "filtered": {
                    "query": {
                      "term": {
                        "amenities": "wifi"
                      }
                    },
                    "filter": {
                      "bool": {
                        "must": [
                          {
                            "nested": {
                              "path": "rooms.priceInventory",
                              "query": {
                                "bool": {
                                  "must": [
                                    {
                                      "range": {
                                        "price": {
                                          "gte": 200,
                                          "lte": 950
                                        }
                                      }
                                    },
                                    {
                                      "term": {
                                        "date": 1450290600000
                                      }
                                    }
                                  ]
                                }
                              }
                            }
                          },
                          {
                            "nested": {
                              "path": "rooms.priceInventory",
                              "query": {
                                "bool": {
                                  "must": [
                                    {
                                      "range": {
                                        "price": {
                                          "gte": 200,
                                          "lte": 950
                                        }
                                      }
                                    },
                                    {
                                      "term": {
                                        "date": 1450377000000
                                      }
                                    }
                                  ]
                                }
                              }
                            }
                          }
                        ]
                      }
                    }
                  }
                }
              }
            }
          ]
        }
      }
    }
  },
  "sort": [{
    "rooms.priceInventory.price": {
      "order": "desc",
      "mode":"max",
      "nested_filter":{
          "range": {
             "rooms.priceInventory.price": {
               "gte": 20,
               "lte": 950
             }
           }
        }
    }
  }]
 }
```

I cannot specify filters at multiple levels in nested_filter, such that complete main query (sans the top level) cannot be repeated. This sorts on price of rooms which don't have wifi amenity.
</comment><comment author="vikasdp" created="2016-08-29T19:30:46Z" id="243229493">@sumitjainn - under `nested_filter{}` don't you need an entire query that you have in the main (top) level query clause for this to work? https://www.elastic.co/guide/en/elasticsearch/guide/current/nested-sorting.html 

cc @clintongormley 
</comment><comment author="sumitjainn" created="2016-09-01T12:36:17Z" id="244065234">@vikasdp No, multi level/path nesting does not work in the sort clause.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test for boundary chars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15796</link><project id="" key="" /><description>This is a forward port of the test for #15795. The fix isn't required but the test will help!
</description><key id="125228643">15796</key><summary>Add test for boundary chars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T17:57:35Z</created><updated>2016-01-06T23:26:39Z</updated><resolved>2016-01-06T23:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T23:08:47Z" id="169491066">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix boundary chars in java api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15795</link><project id="" key="" /><description>Closes #15792
</description><key id="125228319">15795</key><summary>Fix boundary chars in java api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>:Java API</label><label>bug</label><label>review</label><label>v2.3.0</label></labels><created>2016-01-06T17:56:15Z</created><updated>2016-03-16T19:43:04Z</updated><resolved>2016-01-06T23:26:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T23:08:33Z" id="169491028">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Doc] Fix french analyzer elision token filter doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15794</link><project id="" key="" /><description>Fix #15774, kudos to @jpountz for the solution :+1:
</description><key id="125223711">15794</key><summary>[Doc] Fix french analyzer elision token filter doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienalexandre</reporter><labels><label>docs</label></labels><created>2016-01-06T17:28:56Z</created><updated>2016-01-06T17:39:27Z</updated><resolved>2016-01-06T17:39:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Don't override originalQuery with request filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15793</link><project id="" key="" /><description>These filters leak into highlighting and probably other places and cause things like the type name to be highlighted when using requireFieldMatch=false. We could have special hacks to keep them out of highlighting but it feals better to keep them out of any variable named "originalQuery".

Closes #15689
</description><key id="125221185">15793</key><summary>Don't override originalQuery with request filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T17:15:42Z</created><updated>2016-01-12T18:11:09Z</updated><resolved>2016-01-12T16:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-12T14:49:53Z" id="170936749">@javanna is this something you can review? I'm hunting reviewers....
</comment><comment author="javanna" created="2016-01-12T15:17:28Z" id="170944132">I found this a bit scary at first as it adds a new method to be used to retrieve the query that we previously relied on (the filtered one) while the default one (parsedQuery) is not filtered anymore. That said I looked at where we call what and it seems ok, also if tests are green I suppose we are good :) LGTM then
</comment><comment author="nik9000" created="2016-01-12T16:08:23Z" id="170959494">&gt; a bit scary

Yeah! I believe it! Its weird to have these three, but I'm not sure of a better way to do this. I think we lucked out that this worked properly before the query/filter merger.
</comment><comment author="nik9000" created="2016-01-12T17:30:46Z" id="170983816">Backporting.....
</comment><comment author="nik9000" created="2016-01-12T17:30:59Z" id="170983870">I'm so not used to maven....
</comment><comment author="nik9000" created="2016-01-12T18:11:09Z" id="170998675">And cherry-picked to 2.x: 6ddd233a0283e12634a89a566f34ab645c4dba20 53d41a19f28448ec737dcf41595207c04bd565a0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchRequestBuilder.setHighlighterBoundaryChars doesn't work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15792</link><project id="" key="" /><description>Using the java API, I'm trying to use setHighlighterBoundaryChars to change the boundary chars of the fast vector highlighter, but it seems that it is broken.

```
...
char[] chars = new char[]{'\n', '.'};
request.setHighlighterBoundaryChars(chars);
System.out.println(request.toString());
...
```

prints 

```
...
  "highlight" : {
    "boundary_max_scan" : 200,
    "boundary_chars" : "[C@550ec6a1",
...
```

According to [the doc](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search-request-highlighting.html#boundary-characters), the "boundary_chars" field should be a string containing all the boundary chars.
###### Possible fix

Looking at the code of HighlightBuilder.java:368 in 2.2: 

```
// Current
builder.field("boundary_chars", boundaryChars); // boundaryChars is a char array

// Fixed
builder.field("boundary_chars", new String(boundaryChars)); // might work better
```
</description><key id="125219880">15792</key><summary>SearchRequestBuilder.setHighlighterBoundaryChars doesn't work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">julamb</reporter><labels><label>:Highlighting</label><label>v2.3.0</label></labels><created>2016-01-06T17:08:41Z</created><updated>2016-01-10T12:02:38Z</updated><resolved>2016-01-10T12:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-06T17:43:26Z" id="169400553">Confirmed the bug. It looks like your fix will work too. This is not a problem in master though. I'll open up a PR to fix it once the test suite finishes running.
</comment><comment author="julamb" created="2016-01-06T18:00:24Z" id="169405072">Cool! Actually I just noticed that there's another use of the boundaryChars char array at HighlightBuilder.java:430.
</comment><comment author="nik9000" created="2016-01-06T18:04:17Z" id="169405944">&gt; Cool! Actually I just noticed that there's another use of the boundaryChars char array at HighlightBuilder.java:430.

Good catch. I'll fix that one too.
</comment><comment author="nik9000" created="2016-01-06T19:45:27Z" id="169437763">&gt; Good catch. I'll fix that one too.

Done and waiting in the PR.
</comment><comment author="clintongormley" created="2016-01-10T12:02:38Z" id="170339394">Fixed by #15795
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only fail the relocation target when a replication request on it fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15791</link><project id="" key="" /><description>This commit addresses an issue when handling a failed replication
request against a relocating target shard. Namely, if a replication
request fails against the target of a relocation we currently fail both
the source and the target. This leads to an unnecessary
recovery. Instead, only the target of the relocation should be failed.

Closes #15790
</description><key id="125213674">15791</key><summary>Only fail the relocation target when a replication request on it fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T16:38:13Z</created><updated>2016-01-09T20:24:38Z</updated><resolved>2016-01-06T17:56:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-06T17:27:38Z" id="169396712">@bleskes I pushed c291c1714295dd0ab7024f50518fc39fa21d742d to address your feedback on `TransportReplicationActionTests#runReplicateTest`.
</comment><comment author="bleskes" created="2016-01-06T17:50:16Z" id="169402224">awesome. LGTM. Can we open an issue/work on that comment regarding building the target routing for each request? just making sure it's not lost :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed replication requests on relocating shards causes unnecessary recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15790</link><project id="" key="" /><description>When executing a replication request against a shard that is relocating, the request is routed to the target of the relocation. However, if that request fails we currently fail the source of the relocation. This correctly causes the target of the relocation to be marked as failed but since it leads to both the source and the target being marked as failed, it leads to an unnecessary recovery.
</description><key id="125191567">15790</key><summary>Failed replication requests on relocating shards causes unnecessary recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>blocker</label><label>bug</label></labels><created>2016-01-06T14:58:11Z</created><updated>2016-01-10T12:19:35Z</updated><resolved>2016-01-06T17:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>High heap usage on get field mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15789</link><project id="" key="" /><description>Environment:
- 62 indices
- 50+ types
- Java 7.0_67
- JVM arguments `-Xms8G -Xmx8G`
- Elasticsearch version 1.7.5 (branch 1.7)

Steps:
1. Start Elasticsearch
2. Execute the following query `http://localhost:9200/logstash/_mapping/field/*` (produced by Kibana)
3. Wait the response
4. Run the same query again

Here is the result (screenshot from JVisualVM);
![mapping-memory-leak-confirmed](https://cloud.githubusercontent.com/assets/333276/12142867/b68a64c0-b47b-11e5-9f04-5fefbe9b6754.JPG)

As you can see the first execution of the query took about 1G of memory and the second execution of the query took about 2.5G (more than 2 times more memory than the first execution).
I tried to force GC but no memory is free up.

Using MAT, I can see that the memory is filled with `FieldMappingMetaData`.
I'm not sure if this is intended but I `GetFieldMappingsResponse` keeps a reference of the mapping: https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java#L39

I didn't try on Elasticsearch 2.x but the code is more or less the same.
</description><key id="125173350">15789</key><summary>High heap usage on get field mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mogztter</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label></labels><created>2016-01-06T13:07:11Z</created><updated>2016-03-16T19:38:44Z</updated><resolved>2016-01-11T08:15:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T14:08:10Z" id="169331410">&gt; I tried to force GC but no memory is free up.

This could be because elasticsearch disables explicit GCs. What happens if you keep calling this API, does Elasticsearch run out of memory?
</comment><comment author="Mogztter" created="2016-01-06T15:39:03Z" id="169361619">&gt; This could be because elasticsearch disables explicit GCs. What happens if you keep calling this API, does Elasticsearch run out of memory?

Yes, if I execute the query one more time, Elasticsearch hangs forever. Most of the time when there's activity on the node, Elasticsearch terminates abruptly (OutOfMemory).
</comment><comment author="Mogztter" created="2016-01-06T16:08:28Z" id="169371633">If you want I can upload a heap dump
</comment><comment author="jpountz" created="2016-01-06T16:20:22Z" id="169376033">That might help, thanks.
</comment><comment author="Mogztter" created="2016-01-08T10:56:03Z" id="169963306">Here's a hprof file: https://drive.google.com/file/d/0B7t8xIk1T8NjejFLVzY1Z1ZzTDg/view?usp=sharing (~ 3Go)
</comment><comment author="Mogztter" created="2016-01-08T12:26:57Z" id="169984995">I removed all breakpoints on Intellij and now the graph looks like this:
![mapping-memory-garbage](https://cloud.githubusercontent.com/assets/333276/12197649/2399269e-b609-11e5-9439-9e305104f8d0.JPG)
_somehow the debugging on Intellij blocked the garbage collection_

Still the memory usage is huge and can threaten the stability of the cluster with big spike of heap used and concurrents queries can achieve the same result (ie. Elasticsearch hangs forever or becomes unresponsive for minutes):

![mapping-memory-unresponsive](https://cloud.githubusercontent.com/assets/333276/12197843/fc143a62-b60a-11e5-8ad8-061b6f8be46f.JPG)
_Elasticsearch was unresponsive between 13:17 and 13:22_

Is there a circuit breaker available to prevent this operation from causing an `OutOfMemoryError` ?
</comment><comment author="s1monw" created="2016-01-08T14:10:56Z" id="170013081">@Mogztter thanks for the heapdump I looked at it and found on JNI Global references holding on to the memory, any chance you are running your JVM in debug mode? I also wonder if you can run with `-Xcheck:jni` and separately can you try running this with the latest JVM ie a java 8 JVM? Are you using any kind of native plugins or something like this?
</comment><comment author="s1monw" created="2016-01-08T14:11:57Z" id="170013548">ooh nevermind then I didn't see your latest reply!!!
</comment><comment author="s1monw" created="2016-01-08T14:26:19Z" id="170017340">I looked at this again  with @jpountz and we found a smoking gun... @jpountz will take care of a fix. Thanks for opening this @Mogztter 
</comment><comment author="jpountz" created="2016-01-08T14:31:06Z" id="170018251">The issue appears to be that we over-allocate the data-structure that we use to store the serialized representation of the field mappings, which can become an issue if there are many fields. I'll look into it.
</comment><comment author="s1monw" created="2016-01-08T14:36:02Z" id="170019218">@clintongormley I think we should make this a release blocker as well? @jpountz WDYT? this is 1.7.5
</comment><comment author="jpountz" created="2016-01-08T14:37:38Z" id="170019551">We already need to make a 1.7.5 anyway because of the bitset cache bug so that works for me.
</comment><comment author="s1monw" created="2016-01-08T19:26:22Z" id="170099493">@Mogztter once @jpountz has pushed #15864 to 1.7 can you give it a try just to confirm the situation is better after that?
</comment><comment author="Mogztter" created="2016-01-08T20:25:14Z" id="170113995">@jpountz Thanks for the quick fix :+1: 
@s1monw Sure, unfortunately I won't be able to confirm the improvement before Tuesday.
</comment><comment author="s1monw" created="2016-01-10T21:40:43Z" id="170396729">&gt; @s1monw Sure, unfortunately I won't be able to confirm the improvement before Tuesday.

Tuesday is more than soon enough :) thanks
</comment><comment author="jpountz" created="2016-01-11T08:39:51Z" id="170467689">The fix has been propagated to all branches.
</comment><comment author="Mogztter" created="2016-01-12T08:45:44Z" id="170840468">@jpountz Thanks!
@s1monw Yes the situation is _way_ better:

![heap-after](https://cloud.githubusercontent.com/assets/333276/12258694/c89e80a8-b910-11e5-81fe-2550b2ced052.JPG)

Now the memory taken is about 250 Mo and stable on each call :+1: 
</comment><comment author="jpountz" created="2016-01-12T08:46:40Z" id="170840626">:+1:  Thanks a lot for confirming.
</comment><comment author="s1monw" created="2016-01-12T09:32:02Z" id="170850857">awesome! thanks for reporting @Mogztter 
</comment><comment author="Mogztter" created="2016-01-21T12:00:11Z" id="173549556">@jpountz Hello Adrien, I saw that all `v1.7.5` issues are closed :+1: any ETA for Elasticsearch 1.7.5 ? thanks
</comment><comment author="jpountz" created="2016-01-22T18:16:52Z" id="173998262">I can't promise anything but it should go out next week.
</comment><comment author="Mogztter" created="2016-02-02T21:52:46Z" id="178846134">Yay 1.7.5 is out, upgrading now, thanks :smile: 
</comment><comment author="s1monw" created="2016-02-03T08:54:21Z" id="179100039">@Mogztter thank YOU!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Never delete translog-N.tlog file when creation fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15788</link><project id="" key="" /><description>We today delete the translog-N.tlog file if any subsequent operation fails
but we might actually be in a good state if for instance the creation of the writer
failes after we sucessfully baked the new translog generation into the checkpoint. In this situation
we used to delete the translog-N.tlog file and failed on the next recovery of the translog with a
NoSuchFileException | FileNotFoundException just like in https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336

This commit changes the behavior and cleans up that limbo state on recovery if we already have a generation+1 file written but not baked into
the checkpoint we remove that file but only if the previous ckp file has already been renamed otherwise we know we can't be in this state.
</description><key id="125166403">15788</key><summary>Never delete translog-N.tlog file when creation fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T12:18:11Z</created><updated>2016-01-06T13:31:26Z</updated><resolved>2016-01-06T13:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-06T12:32:59Z" id="169314063">LGTM. Left some minor comments. Great catch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>move all the processors under the same package org.elasticsearch.ingest.processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15787</link><project id="" key="" /><description>Forked from https://github.com/elastic/elasticsearch/pull/15769#issuecomment-169268344
</description><key id="125161238">15787</key><summary>move all the processors under the same package org.elasticsearch.ingest.processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-06T11:38:19Z</created><updated>2016-01-06T12:28:35Z</updated><resolved>2016-01-06T12:28:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-06T11:56:48Z" id="169307800">LGTM
</comment><comment author="s1monw" created="2016-01-06T12:17:00Z" id="169310637">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Never call a listerner under lock in InternalEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15786</link><project id="" key="" /><description>We has a postIndex|DeleteUnderLock listener callback to load percolator
queries which is entirely private to the index shard in the meanwhile. Yet,
it still calls an external callback while holding an indexing lock which is scary
since we have no control over how long the operation could possibly take.

This commit decouples the percolator registry entirely from the ShardIndexingService
by pessimistically fetching percolator documents from the the engine using realtime get.
Even in situations where the same document is changed concurrently we will eventually end up
in the correct state without loosing an update. This also moves the index throtteling stats directly into
the engine to entirely remove the need for the dependency between InternalEngine and ShardIndexingService.
</description><key id="125152944">15786</key><summary>Never call a listerner under lock in InternalEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T10:40:52Z</created><updated>2016-01-06T12:23:33Z</updated><resolved>2016-01-06T12:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-06T10:53:44Z" id="169296559">LGTM, this is a nice cleanup!
</comment><comment author="martijnvg" created="2016-01-06T10:57:52Z" id="169297194">&gt; Even in situations where the same document is changed concurrently we will eventually end up
&gt; in the correct state without loosing an update. 

This is a better tradeoff. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add PathHierarchy type back to path_hierarchy tokenizer for backward compatibility with 1.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15785</link><project id="" key="" /><description>Relates to #15756
</description><key id="125148049">15785</key><summary>Add PathHierarchy type back to path_hierarchy tokenizer for backward compatibility with 1.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Analysis</label><label>regression</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-06T10:18:58Z</created><updated>2016-01-11T11:31:54Z</updated><resolved>2016-01-06T13:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T10:22:34Z" id="169285861">Is there a way that we could only support it for indices created before 2.0 to ensure that new indices will use the lowercase version?
</comment><comment author="jpountz" created="2016-01-06T10:26:57Z" id="169287471">Maybe we could register a subclass of PathHierarchyTokenizerFactory whose constructor fails if the version in the index settings is &gt;= 2.0? The error message could even point to the replacement to use?
</comment><comment author="jpountz" created="2016-01-06T12:24:06Z" id="169311997">Hmm after all we will have to do it for others analysis components (eg. edgeNGram) so let's push it as-is and work on a migration path as part of #8988
</comment><comment author="s1monw" created="2016-01-06T13:51:05Z" id="169327838">@ywelsch this went into 2.2 as well can you label it accordingly?
</comment><comment author="ywelsch" created="2016-01-06T13:53:14Z" id="169328601">@s1monw done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15784</link><project id="" key="" /><description /><key id="125130377">15784</key><summary>2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">backtonet</reporter><labels /><created>2016-01-06T07:55:07Z</created><updated>2016-01-06T08:10:28Z</updated><resolved>2016-01-06T08:10:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-06T08:10:28Z" id="169263185">I guess this was opened by mistake... closing... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin not work on es2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15783</link><project id="" key="" /><description>Hi,

there are some errors when i use plugin to install something, please look the info below.

```
[root@iZ25g5gvvtgZ elasticsearch]# bin/plugin install https://github.com/elastic/elasticsearch/tree/master/plugins/analysis-smartcn
-&gt; Installing from https://github.com/elastic/elasticsearch/tree/master/plugins/analysis-smartcn...
Trying https://github.com/elastic/elasticsearch/tree/master/plugins/analysis-smartcn ...
Downloading ............................................................................DONE
Verifying https://github.com/elastic/elasticsearch/tree/master/plugins/analysis-smartcn checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip

```

Thanks
</description><key id="125128308">15783</key><summary>plugin not work on es2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lizhecome</reporter><labels /><created>2016-01-06T07:34:03Z</created><updated>2016-01-06T08:24:46Z</updated><resolved>2016-01-06T07:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-06T07:35:39Z" id="169258471">You are not following the user guide to install the plugin.

```
bin/plugin install analysis-smartcn
```

Please ask questions on discuss.elastic.co.
</comment><comment author="lizhecome" created="2016-01-06T07:42:29Z" id="169259266">the result is the same 

```
[root@iZ25g5gvvtgZ elasticsearch]# bin/plugin install elastic/elasticsearch-analysis-smartcn
-&gt; Installing elastic/elasticsearch-analysis-smartcn...
Trying https://github.com/elastic/elasticsearch-analysis-smartcn/archive/master.zip ...
Downloading ......................DONE
Verifying https://github.com/elastic/elasticsearch-analysis-smartcn/archive/master.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip
```
</comment><comment author="dadoonet" created="2016-01-06T07:43:15Z" id="169259360">Did I wrote `bin/plugin install elastic/elasticsearch-analysis-smartcn`?
</comment><comment author="lizhecome" created="2016-01-06T07:54:14Z" id="169261127">sorry, but there is still an error when install medcl/elasticsearch-analysis-pinyin, could you please take a look? thanks.

```
[root@iZ25g5gvvtgZ elasticsearch]# bin/plugin install medcl/elasticsearch-analysis-pinyin
-&gt; Installing medcl/elasticsearch-analysis-pinyin...
Trying https://github.com/medcl/elasticsearch-analysis-pinyin/archive/master.zip ...
Downloading ........................................................................................................................................................DONE
Verifying https://github.com/medcl/elasticsearch-analysis-pinyin/archive/master.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip
```
</comment><comment author="dadoonet" created="2016-01-06T08:24:46Z" id="169265134">Please ask questions on discuss.elastic.co.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>3.0 Mapping API field wildcard not working?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15782</link><project id="" key="" /><description>I'm surprised by the difference between the output of these two commands. Is that a bug?

``` sh
curl 'http://localhost:9200/logstash-*/_mapping?pretty' # returns all fields
curl 'http://localhost:9200/logstash-*/_mapping/field/*?pretty' # returns nothing
```
</description><key id="125080439">15782</key><summary>3.0 Mapping API field wildcard not working?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">spalger</reporter><labels><label>bug</label></labels><created>2016-01-06T00:00:58Z</created><updated>2016-01-06T21:30:39Z</updated><resolved>2016-01-06T21:30:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T10:15:38Z" id="169284361">It looks like one. I'll dig.
</comment><comment author="jpountz" created="2016-01-06T17:38:40Z" id="169399435">@spalger I could not reproduce the issue on a fresh master build, do you have steps that I could follow to reproduce it?
</comment><comment author="spalger" created="2016-01-06T18:22:58Z" id="169410170">Hmm, I suppose it's possible that the reason I wasn't seeing any fields is because I didn't have any documents indexed... That's the only way that I can reproduce that now....
</comment><comment author="jpountz" created="2016-01-06T21:27:50Z" id="169468433">@spalger Then may I close?
</comment><comment author="spalger" created="2016-01-06T21:30:39Z" id="169469115">Thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for weighted average in the 'avg' aggregator based an extensi&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15781</link><project id="" key="" /><description>Support for weighted average in the 'avg' aggregator. See a full description of the feature implemented here https://github.com/elastic/elasticsearch/issues/15731.

This commit only contains the implementation of the first approach based on the script attribute.

Below an example of script returning a value and a weight for each document matching the query. These two values are returned as a map and used by the AvgAggregator class to compute the weighted average accordingly.  

{"avg" : { "script" : "[value: doc['latency'].value, weight: doc['transaction'].value]" }}

Laurent
</description><key id="125079729">15781</key><summary>Support for weighted average in the 'avg' aggregator based an extensi&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lquerel</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2016-01-05T23:54:43Z</created><updated>2016-06-24T10:01:17Z</updated><resolved>2016-06-24T10:01:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T21:48:39Z" id="169473401">I'm not against adding a way to compute weighted averages, but this implementation is too hacky in my opinion.
</comment><comment author="lquerel" created="2016-01-06T23:52:36Z" id="169501783">Thank for your response. 
I agree that some modifications in ScriptDoubleValues are a little bit hacky :). But this is the only way that I found to keep additional values returned by the script. Can you provide some guidance to improve that and I'll update this pull request. 
</comment><comment author="dakrone" created="2016-04-06T21:46:22Z" id="206584833">@jpountz do you have any guidance to provide for this PR?
</comment><comment author="jpountz" created="2016-06-24T10:01:17Z" id="228307010">Discussed in Fixit Friday: we are not happy with this way that weighted averages are exposed, but we will keep the issue open to try to assess whether the community needs this feature and explore how it could be exposed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.1.1 bloats log files if particular index has a broken transaction log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15780</link><project id="" key="" /><description>This is a spin off the problem discussed here: https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336.

If ES 2.1.1 encounters broken transaction log during index recovery - as listed in the above discussion, it starts quite rapidly to fill up the log files - at about 10GB/hour rate. This can fill up the disk space and cause for additional instability. f

I think logging strategy for such errors should be less aggressive and utilize [Truncated Exponential Backoff](https://github.com/elastic/elasticsearch/issues/15779) approach.
</description><key id="125050749">15780</key><summary>ES 2.1.1 bloats log files if particular index has a broken transaction log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haizaar</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2016-01-05T21:01:50Z</created><updated>2016-04-15T22:00:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-06T09:26:57Z" id="169275461">@haizaar did you have any replicas or was this the single shard copy available? 
</comment><comment author="haizaar" created="2016-01-08T21:54:59Z" id="170138769">I had one replica (it's `.kibana` index with default settings). 
</comment><comment author="bleskes" created="2016-01-08T22:42:29Z" id="170147977">@haizaar thanks. Was that replica assigned and active? I think you mentioned a single node?  If you've had another replica it would have taken over. Sadly if the master has no other options it just keeps trying and that spams the log. Agreed we should do better. 
</comment><comment author="haizaar" created="2016-01-09T07:15:19Z" id="170203214">It was a single node scenario. The replica was not assigned and did not take over for some reason. 
</comment><comment author="surry" created="2016-04-15T22:00:21Z" id="210663254">We saw something similar with log files growing huge when we hit the maximum number of documents for an index. We had a 20 GB log for one day that was filled with indexing exceptions related to that limit because we kept trying to index new documents. Thankfully this was at the end of the month and a new index was created a few days later.

I wouldn't say that's an Elasticsearch issue, but I thought it might fall into the same bucket as the fix hinted at above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It would be useful to have a way to ignore broken transaction log on index recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15779</link><project id="" key="" /><description>This is a spin off the problem discussed here: https://discuss.elastic.co/t/cannot-recover-index-because-of-missing-tanslog-files/38336

I have an index with broken transaction log data. The data itself in the index appears to be fine as reported by Lucene tool. However ES 2.1.1 refuses to bring the index up because of broken translog. 

Since the actual index data is there, I think there should be a way (a tool?) to instruct ES to disregard the transaction log file and to start over.
</description><key id="125049671">15779</key><summary>It would be useful to have a way to ignore broken transaction log on index recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haizaar</reporter><labels><label>:Recovery</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-05T20:56:00Z</created><updated>2016-05-24T09:10:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-06T09:32:02Z" id="169276349">@haizaar to be clear - throwing away the transaction log means a likely data loss. In your case it's pretty much guaranteed (because of the translog generation your lucene index points at). I'm fine with having a way to allow users to confirm this loss and open the lucene index but we need to be clear about the implications. 

Also note that usually another shard will be used to recover the primary.  This request is about having a way to recover where all copies have been lost or have corruptions, in the specific case that one of those corruptions is in the translog.
</comment><comment author="haizaar" created="2016-01-08T21:58:08Z" id="170139418">Yes, I fully understand the implications. However if you ever get to this situation (no replicas and no adequate backups) I prefer to have a choice between full data loss (like it is now) and partial data loss. And I much prefer the partial one.

I hope I read you right.
</comment><comment author="bleskes" created="2016-01-08T22:37:26Z" id="170147062">&gt; I hope I read you right.

You do :)
</comment><comment author="s1monw" created="2016-01-08T22:46:22Z" id="170148620">&gt; I have an index with broken transaction log data. The data itself in the index appears to be fine as reported by Lucene tool. However ES 2.1.1 refuses to bring the index up because of broken translog.

I am curious, you have yet another broken t-log? What's the problem this time?
</comment><comment author="haizaar" created="2016-01-09T07:14:07Z" id="170203173">No. I opened this bug as a spin off from the original discussion we had before.
</comment><comment author="dakrone" created="2016-05-23T17:46:48Z" id="221043483">Coming back to this after working with someone who had a corrupt (though empty) translog and wanted to keep the data in the shard as it was the only copy (a single primary, no replica). I think we should either have a way to ignore a corrupted translog (with a ton of warnings and all that) or a tool that can put an empty translog in place so a shard can recover. Otherwise, it's frustrating in the case of accidental translog corruption or truncation to lose the entire shard's worth of data.
</comment><comment author="clintongormley" created="2016-05-24T09:10:45Z" id="221210738">Agreed - we need a command line tool to perform expert operations such as this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleted document count and total space usage is growing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15778</link><project id="" key="" /><description>Elastic search version elasticsearch-1.7.0

Before executing optimization
GET _cat/indices?v
health status index               pri rep docs.count   docs.deleted store.size pri.store.size
green  open   indexname       5   1   12451854      3122422    101.6gb         50.5gb

After executing the following command 

POST indexname/_optimize?only_expunge_deletes=true
health status index                pri rep docs.count   docs.deleted store.size pri.store.size
green  open   indexname       5   1   12451854      575567    68.6gb         34.5gb

Why the space reclaiming is not happening while deleting it?

I am using JDBC importer (elasticsearch-jdbc-1.7.0.0) to recreate the document instead of updating. It recreates around 1500000 documents in  a day.

I was able to get much better performance after the executing the above step for optimization
</description><key id="125046041">15778</key><summary>Deleted document count and total space usage is growing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">binishabraham</reporter><labels /><created>2016-01-05T20:36:40Z</created><updated>2016-01-05T20:43:26Z</updated><resolved>2016-01-05T20:39:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-05T20:39:10Z" id="169126284">The handling of deletes in Elasticsearch is covered well in [Lucene's Handling of Deleted Documents](https://www.elastic.co/blog/lucenes-handling-of-deleted-documents) on the [Elastic engineering blog](https://www.elastic.co/blog).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Safe cluster state task notifications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15777</link><project id="" key="" /><description>This commit addresses an issue where a cluster state task listener
throwing an exception could prevent other listeners from being notified,
and could prevent the executor from receiving notifications that a new
cluster state was published. Additionally, this commit also addresses a
similar issue for executors handling cluster state publication
notifications.
</description><key id="125045761">15777</key><summary>Safe cluster state task notifications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>blocker</label><label>enhancement</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-05T20:34:50Z</created><updated>2016-01-05T21:56:39Z</updated><resolved>2016-01-05T21:56:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-05T20:38:38Z" id="169126158">Oh nice! LGTM.
</comment><comment author="bleskes" created="2016-01-05T21:34:11Z" id="169140409">LGTM
</comment><comment author="jasontedor" created="2016-01-05T21:56:39Z" id="169146320">Thanks for reviewing @nik9000 and @bleskes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace ContextAndHeaders with a ThreadPool based ThreadLocal implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15776</link><project id="" key="" /><description>ContextAndHeaders has a massive impact on the core infrastructure since it has to
be manually passed on to all relevant places across threads/network calls etc. For the same reason
it's also very error prone and easily forgotten on potentially relevant APIs.

The new ThreadContext is associated with a ThreadPool (node or transport client) and ensures that
headers and context registered on a current thread are inherited to new threads spawned, send across
the network to be deserialized on the receiver end as well as restored on the response handling thread
once the response is received.
</description><key id="125000819">15776</key><summary>Replace ContextAndHeaders with a ThreadPool based ThreadLocal implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-05T16:30:26Z</created><updated>2016-08-11T11:15:03Z</updated><resolved>2016-01-27T16:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-05T16:30:54Z" id="169051704">please note that I still have to add javadocs etc. and documentation but  I want to get general feedback here first
</comment><comment author="nik9000" created="2016-01-05T22:12:52Z" id="169150326">+1 on the concept of moving context to ThreadLocal and +1 on dealing with those at the ThreadPool level. The actual change I'll need to read more carefully.
</comment><comment author="s1monw" created="2016-01-13T10:55:01Z" id="171253497">@uboness @jaymode @javanna @nik9000  I updated this PR and removed the _WIP_ label. I think it's ready for review. /cc @clintongormley 
</comment><comment author="rjernst" created="2016-01-13T23:39:08Z" id="171472737">This looks good to me. One suggestion would be to document a little more on _how_ to use the ThreadContext in its' javadocs (eg like the try-with-resources style stashing that is used everywhere). It took me a while to understand that in the flow of reading this PR.
</comment><comment author="s1monw" created="2016-01-14T21:20:18Z" id="171783132">&gt; One suggestion would be to document a little more on how to use the ThreadContext in its' javadocs (eg like the try-with-resources style stashing that is used everywhere). It took me a while to understand that in the flow of reading this PR.

good call I will adress this tomorrow
</comment><comment author="s1monw" created="2016-01-15T08:37:53Z" id="171902262">@rjernst added more javadocs
</comment><comment author="rjernst" created="2016-01-15T10:21:18Z" id="171926135">Thanks for the docs @s1monw. LGTM.
</comment><comment author="s1monw" created="2016-01-19T20:07:00Z" id="172970751">@jaymode I pushed some updates can you take a look
</comment><comment author="jaymode" created="2016-01-27T15:31:38Z" id="175690353">LGTM
</comment><comment author="clintongormley" created="2016-01-28T10:14:56Z" id="176105244">@s1monw you marked this as breaking but I don't see a note in the breaking changes docs.  I presume this breaks the Java API for plugins? Would you mind adding a note please?
</comment><comment author="s1monw" created="2016-01-28T11:09:45Z" id="176125904">@clintongormley ok
</comment><comment author="ruckc" created="2016-08-01T23:40:02Z" id="236743051">It would be nice if this linked to the Breaking Changes location... can't seem to find the replacement for Java API consumers of ContentAndHeader methods...
</comment><comment author="ruckc" created="2016-08-01T23:46:41Z" id="236744620">Hmm, @s1monw did this get documented in breaking changes?  I can't find it in https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_java_api_changes.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent peer recovery from node with older version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15775</link><project id="" key="" /><description>@brwe found an issue uncovered by the backward compatibility tests (http://build-us-00.elastic.co/job/es_core_2x_window-2012/328/) where we try to relocate a replica to an old node although the primary is on a new one.

I could track the issue down to a bug in the `NodeVersionAllocationDecider` which did not take this scenario into account (it compared node versions of old and new replica node, not node of primary shard which is used for peer recovery).
</description><key id="124993489">15775</key><summary>Prevent peer recovery from node with older version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>blocker</label><label>bug</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-05T15:52:23Z</created><updated>2016-01-06T09:08:01Z</updated><resolved>2016-01-06T09:08:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-05T16:09:48Z" id="169046169">this LGTM and I think this should go into 2.2, 2.1, 2.0 and 1.7 branches
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"french" pre-configured language analyzer wrong about elision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15774</link><project id="" key="" /><description>Elasticsearch come with pre-configured language analyzer and I think [the :fr:  french one](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/analysis-lang-analyzer.html#french-analyzer) is wrong.

Filters are in this order:
- "french_elision" (configured with "d", "c", "jusqu", "lorsqu", "puisqu"...)
- "lowercase"
- "french_stop"
- "french_stemmer"

But, as "french_elision" is not case insensitive, prefix may be ignored if they have an uppercase letter.
- So the sentence `Jusqu'o&#249; aller ?` gives two tokens: `jusqu'ou` and `aler`.
- If we inverse "french_elision" and "lowercase", we get: `o&#249;` and `aler`.

Here is a gist to play: https://found.no/play/gist/27c387bf6e6e7e064b1b#analysis

I can't think of any good reason behind this order, but I'm no expert in language analysis :neckbeard: 
I wish I had reported this issue before 2.0, but anyway, hope it can be fixed in the future. 

Cheers!
</description><key id="124990670">15774</key><summary>"french" pre-configured language analyzer wrong about elision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienalexandre</reporter><labels><label>:Analysis</label><label>adoptme</label><label>bug</label></labels><created>2016-01-05T15:38:41Z</created><updated>2016-01-06T17:39:26Z</updated><resolved>2016-01-06T17:39:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T14:09:54Z" id="169331756">+1
</comment><comment author="jpountz" created="2016-01-06T14:46:22Z" id="169341986">Actually I think we need to fix the docs to replace:

```
        "french_elision": {
        "type":         "elision",
            "articles": [ "l", "m", "t", "qu", "n", "s",
                          "j", "d", "c", "jusqu", "quoiqu",
                          "lorsqu", "puisqu"
                        ]
        }
```

with

```
        "french_elision": {
        "type":         "elision",
            "articles": [ "l", "m", "t", "qu", "n", "s",
                          "j", "d", "c", "jusqu", "quoiqu",
                          "lorsqu", "puisqu"
                        ],
            "articles_case": true
        }
```

which is how this analyzer is defined in Lucene.
</comment><comment author="damienalexandre" created="2016-01-06T16:36:21Z" id="169381856">Right! Just tested on a fresh ES 2.1, the core "french" analyzer is fine :tada: 

Glad it's only a documentation issue :relaxed: Didn't notice before the core "french" was ok because I  always tweak analyzers (to add come icu filters / tokenizer, etc) using the doc as a bootstrap.

I would be glad to send a PR if you want :+1: 
</comment><comment author="jpountz" created="2016-01-06T16:40:55Z" id="169382945">@damienalexandre that would be great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding Multiple Types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15773</link><project id="" key="" /><description>Is there any way to add values in a column which a comprises of Double and Long Values and Retrieve them back Using Spark Java ESRDD.

Also I don't want Elastic Search to Upper cast a long value to double and type cast double to long ?
I have tried using ignore_malformed because first value can be double value or a long value.
</description><key id="124986125">15773</key><summary>Adding Multiple Types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chirag2528</reporter><labels /><created>2016-01-05T15:13:49Z</created><updated>2016-01-05T22:55:21Z</updated><resolved>2016-01-05T22:55:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-01-05T22:55:21Z" id="169160665">Questions like these are better asked on http://discuss.elastic.co. Github is reserved for bug reports and feature requests.

Fields in elasticsearch must be a single data type.  You can store longs in a double field, or vice versa, but you will lose precision in the process.  It sounds like you are using dynamic mappings, which is why you care about the "first value". Instead, you can explicitly set your mappings, and the values in your documents will (by default) be coerced to that type (ie passing a long value, but with field mapped as double).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NPE in TestClusterService when waiting indefinitely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15772</link><project id="" key="" /><description>When waiting indefinitely for a new cluster state in a test,
TestClusterService#add will throw a NullPointerException if the timeout
is null. Instead, TestClusterService#add should guard against a null
timeout and not even attempt to add a notification for the timeout
expiring. Note that the usage of null is the agreed upon contract for
specifying an indefinite wait from ClusterStateObserver.
</description><key id="124981765">15772</key><summary>Fix NPE in TestClusterService when waiting indefinitely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-05T14:54:06Z</created><updated>2016-01-05T14:57:08Z</updated><resolved>2016-01-05T14:57:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-05T14:56:10Z" id="169023629">LGTM
</comment><comment author="jasontedor" created="2016-01-05T14:56:42Z" id="169023912">The `NullPointerException` occurs in [`ThreadPool#schedule`](https://github.com/elastic/elasticsearch/blob/d48af9a155d87bdfc589fdb8d0ebfd62d72b1564/core/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L360) where `TimeValue#millis` is invoked to setup the timeout notification. However, the guard should be in `TestClusterService#add` because in this case no notification for a timeout should even be registered.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify TranslogWriter to always write to a stream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15771</link><project id="" key="" /><description>We used to write into an in-memory buffer and if necessary also allow reading
from the memory buffer if the some translog locations that are not flushed to
the channel need to be read. This commit hides all writing behind a buffered output
stream and if ncecessary flushes all buffered data to the channel for reading. This allows
for several simplifcations like reusing javas build in BufferedOutputStream and removes the
need for read write locks on the translog writer. All thread safety is now achived using
the synchronized primitive.
</description><key id="124977174">15771</key><summary>Simplify TranslogWriter to always write to a stream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-05T14:27:54Z</created><updated>2016-01-05T15:32:08Z</updated><resolved>2016-01-05T15:28:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-05T14:50:01Z" id="169021100">LGTM, I love this simplification!
</comment><comment author="jpountz" created="2016-01-05T14:53:59Z" id="169022410">:+1: 
</comment><comment author="bleskes" created="2016-01-05T15:32:08Z" id="169035683">awesome. LGTM3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creating a snapshot of a single index does not work properly from node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15770</link><project id="" key="" /><description>I try to create a snapshot of a single index using the `indices` parameter within the official node.js module, but I always end up with getting a complete snapshot of the cluster.

``` coffeescript
@es.snapshot.create( 
   { snapshot: "#{uid}", indices: index, repository: @config.esrepo, include_global_state: false }
   ( err, created ) -&gt; 
      ...
      ...
)
```

The above coffeescript transiling to something like this:

``` javascript
_this.es.snapshot.create({
            snapshot: uid,
            indices: index,
            repository: _this.config.esrepo,
            include_global_state: false
          }, function(err, created) {
              .....
          });
```

Is this due to a bug or missing feature in the es client? The `indices` parameter is being described within the [API](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/modules-snapshots.html#_snapshot), but no within the [JS client API](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/api-reference.html#api-snapshot-create) ...?
</description><key id="124975956">15770</key><summary>Creating a snapshot of a single index does not work properly from node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">exinferis</reporter><labels /><created>2016-01-05T14:21:26Z</created><updated>2016-01-10T13:02:21Z</updated><resolved>2016-01-10T13:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T13:02:21Z" id="170343286">It looks like you are missing the `body` parameter in your JS request
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move ingest api to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15769</link><project id="" key="" /><description>Move interfaces and classes that define the ingest api to core. The reason for this is that this way any plugin can plug in its own `Processor` implementation without having to depend on another plugin (the ingest plugin). The runtime, which contains the rest/action filters, the pipeline CRUD apis and processor impls, is still a plugin though, which will later become a module.
</description><key id="124966963">15769</key><summary>Move ingest api to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-05T13:32:54Z</created><updated>2016-01-07T15:04:16Z</updated><resolved>2016-01-07T15:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-05T16:14:08Z" id="169047388">this is ready for reviews, TODOs have been addressed, not a WIP anymore.
</comment><comment author="javanna" created="2016-01-05T17:31:16Z" id="169072609">@s1monw @rmuir @uboness can you have a look please?
</comment><comment author="rjernst" created="2016-01-05T20:44:08Z" id="169127400">I left a couple comments. I also don't understand why anything is tied to mustache directly (in the existing branch). Even the ingest index template is not dependent on script templates at all (it doesn't even have any substitutions).
</comment><comment author="martijnvg" created="2016-01-05T20:54:49Z" id="169130878">&gt; I also don't understand why anything is tied to mustache directly

Maybe I misunderstand this, but ingest templating isn't tied directly to mustache. The only template service implementation is tied to ScriptService which delegates to mustache module.
</comment><comment author="s1monw" created="2016-01-06T08:45:56Z" id="169268344">it's maybe unrelated but can we please prevent `plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/` from having a gazillion sub-packages that all have a single class like `/foo/FooProcessor` honestly I think this entire thing only needs a single package afterall maybe 2 but that should be it
</comment><comment author="martijnvg" created="2016-01-06T08:54:18Z" id="169269858">&gt; it's maybe unrelated but can we please prevent plugins/ingest/src/main/java/org/elasticsearch/ingest/processor/ from having a gazillion sub-packages

+1 lets change that
</comment><comment author="javanna" created="2016-01-07T14:57:04Z" id="169686224">This PR is ready to merge, few TODOs are left but they will be addressed in a followup PR:
- update docs according to current state (the infra is not a plugin anymore) and behaviour (node.ingest only affects whether ingestion is enabled on index and bulk api, all the ingest related rest actions are always enabled.
- index template: remove ingest index template: create the index manually instead before the initial put pipeline request, and check for consistency of the index template every time we store pipelines. (re-enable SimpleIndexTemplateIT disabled tests as well)
- move basic processors with no dependency to es core. At that point we will also be able to move the rest spec and most of the rest tests, the ones that rely on basic processors only, to core. Only grok and geoip will stay in the plugin at that point and only their specific rest tests will stay there.
- add specific pipeline methods to Client: client.putPipeline etc.
</comment><comment author="martijnvg" created="2016-01-07T15:03:13Z" id="169687661">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use separate searchers for "search visibility" vs "move indexing buffer to disk"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15768</link><project id="" key="" /><description>Spinoff from #14121...

Today, when ES detects it's using too much heap vs the configured indexing buffer (default 10% of JVM heap) it opens a new searcher to force Lucene to move the bytes to disk, clear version map, etc.

But this has the unexpected side effect of making newly indexed/deleted documents visible to future searches, which is not nice for users who are trying to prevent that, e.g. #3593.

As @uschindler suggested in that issue, I think ES should have two separate searchers from the engine: one for search visibility, only ever refreshed according to the user's wishes, and another, used internally for freeing up heap, version map lookups, etc.  Lucene will be efficient about this, sharing segment readers across those two searchers.

I haven't started on this (need to finish #14121 first!) so if someone wants to take it, please feel free!
</description><key id="124950682">15768</key><summary>Use separate searchers for "search visibility" vs "move indexing buffer to disk"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label></labels><created>2016-01-05T11:49:08Z</created><updated>2016-05-07T15:38:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-06T14:46:27Z" id="169342010">I'll try to tackle this ... it doesn't look too hard, given the changes in #14121 which already begins separate "write indexing buffer to disk" from "refresh".
</comment><comment author="mikemccand" created="2016-01-06T18:56:38Z" id="169419465">Note that with this change, a refresh only happens when the user expects it to: on the periodic (default: every 1 second) interval, or when refresh API is explicitly invoked.

But this is a biggish change to ES's behavior vs today, e.g. `flush`, `forceMerge`, moving indexing buffers to disk because they are too big, etc. does NOT refresh, and a good number of tests are angry because of this ... so I'm slowly inserting `refresh()` for such tests.

It also has implications for transient disk usage, since ES will "secretly" refresh less often, meaning we hold segments, which may now be merged or deleted, open for longer.  Users who disable `refresh_interval` (set to -1) need to be careful to invoke refresh API at important times (after `flush` or `forceMerge`).

Still I think it is important we make ES's semantics/behavior crisp and well defined: `refresh`, and `refresh` alone, makes recent index changes visible to searches.  No other operation should do this as an "accidental" side effect.
</comment><comment author="bleskes" created="2016-01-06T19:11:02Z" id="169423766">Just a note - since ES moves shard around on it&#8217;s own will, if we want to support this, we&#8217;ll have to make sure shard relocation (i.e., copy all the files) maintains this semantics. This will be tricky for many reasons - for example, the user may issue a refresh command when the target shard is not yet ready to receive it (engine closed). Today we refresh the target at the end of every recovery for this reason.  Have a &#8220;refresh when I say and only when I say&#8221; is much more complicated then the current &#8220;refresh at least when I say&#8221; (but whenever you want as well) semantics. I&#8217;m not sure it&#8217;s worth the complexity imho.

&gt; On 06 Jan 2016, at 19:56, Michael McCandless notifications@github.com wrote:
&gt; 
&gt; Note that with this change, a refresh only happens when the user expects it to: on the periodic (default: every 1 second) interval, or when refresh API is explicitly invoked.
&gt; 
&gt; But this is a biggish change to ES's behavior vs today, e.g. flush, forceMerge, moving indexing buffers to disk because they are too big, etc. does NOT refresh, and a good number of tests are angry because of this ... so I'm slowly inserting refresh() for such tests.
&gt; 
&gt; It also has implications for transient disk usage, since ES will "secretly" refresh less often, meaning we hold segments, which may now be merged or deleted, open for longer. Users who disable refresh_interval (set to -1) need to be careful to invoke refresh API at important times (after flush or forceMerge).
&gt; 
&gt; Still I think it is important we make ES's semantics/behavior crisp and well defined: refresh, and refresh alone, makes recent index changes visible to searches. No other operation should do this as an "accidental" side effect.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="mikemccand" created="2016-01-06T22:40:34Z" id="169485486">Thanks @bleskes, I agree this is too difficult to achieve "perfectly", and I think recovery should be unchanged here (refresh when the shard is done recovering).

Similarly, primary and each replica are in general searching slightly different of a shard today, i.e. when each refreshes every 1s by default, it's a different set of indexed docs that become visible, in general.

File-based replication would make this easier ;)

So I think those should remain out of scope, here, and we should still state that ES is a "refresh at least when I say", but with this issue "less often when I don't say" than today.

Or are you saying we shouldn't even try to make any change here, i.e. leave the engine doing a normal search-visible refresh when e.g. it wants to free up heap used by version map?
</comment><comment author="s1monw" created="2016-01-11T20:22:12Z" id="170678499">&gt; So I think those should remain out of scope, here, and we should still state that ES is a "refresh at least when I say", but with this issue "less often when I don't say" than today.

+1 to this - I think for the replication case we should refresh since we have to but for stuff like clearing version maps etc. we can improve the situation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create in memory cache for  _source field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15767</link><project id="" key="" /><description>I am using _source field in my sort script to sort docs based on complex logic involving multi level nested fields. But accessing _source from disk for each matched document is performing poorly, giving me ~ 3 sec response times.

It would me very efficient to have a cache for this field, so that costly disk accesses can be avoided on every request. It would cost more memory, but would solve performance problem for many advanced sorting use cases.
</description><key id="124930376">15767</key><summary>Create in memory cache for  _source field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sumitjainn</reporter><labels /><created>2016-01-05T09:47:58Z</created><updated>2016-04-28T07:15:17Z</updated><resolved>2016-01-06T10:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T10:18:35Z" id="169285094">Sorry but this is exactly why we have doc values so we won't add a cache for the _source document.
</comment><comment author="jpountz" created="2016-01-06T10:19:59Z" id="169285372">Maybe you could model your documents in such a way that you don't need the source anymore?
</comment><comment author="sumitjainn" created="2016-01-06T15:59:47Z" id="169368170">Here's my use case: I have hotel docs which have multiple rooms and each room has day wise price and inventory.
hotel &gt; room &gt; priceInventory

I have filters on all three levels. Say hotels with 5 star rating, rooms with "free wifi" amenity and having inventory of 2 for dates d1-d2 and between price p1-p2. 

Now for sorting, I want to calculate minimum price from all matched nested docs. But this isn't possible with regular sort, as I cannot specify filters on multiple nested levels inside it. So I have to utilize script sorting. But inside a script accessing nested level fields is only possible with the _source field.

Doc values cannot be used here. Caching _source would improve sort efficiency drastically, and will even improve efficiency of other operations where _source needs to be loaded. Indeed, it would be very useful to have this feature for many elasticsearch use cases.

Mapping:

``` json
        "hotel": {
            "properties": {
               "name": {
                  "type": "string",
               },
               "id": {
                  "type": "string",
               },
               "roomTypes": {
                  "type": "nested",
                  "properties": {
                     "roomCode": {
                        "type": "string",
                        "index": "not_analyzed",
                     },
                     "roomType": {
                        "type": "string",
                        "index": "not_analyzed",
                     },
                     "amenities": {
                          "type": "string",
                          "index": "not_analyzed",
                     },
                     "invPrice": {
                          "type": "nested",
                          "properties": {
                             "date": {
                                "type": "date",
                                "format": "dateOptionalTime",
                             },
                             "inventory": {
                                "type": "integer",
                             },
                             "price": {
                                "type": "double",
                             }
                          }
                       }
                  }
               },
               "starRating": {
                  "type": "short",
               }
            }
        }
```
</comment><comment author="jpountz" created="2016-01-06T16:55:38Z" id="169386873">@sumitjainn You could denormalize your data in order to have a single level of nesting:

```
        "hotel": {
            "properties": {
               "name": {
                  "type": "string",
               },
               "id": {
                  "type": "string",
               },
               "roomPrices": {
                  "type": "nested",
                  "properties": {
                     "roomCode": {
                        "type": "string",
                        "index": "not_analyzed",
                     },
                     "roomType": {
                        "type": "string",
                        "index": "not_analyzed",
                     },
                     "amenities": {
                          "type": "string",
                          "index": "not_analyzed",
                     },
                     "date": {
                        "type": "date",
                        "format": "dateOptionalTime",
                     },
                     "inventory": {
                        "type": "integer",
                     },
                     "price": {
                        "type": "double",
                     }
                  }
               },
               "starRating": {
                  "type": "short",
               }
            }
        }
```

This will force to duplicate `roomType`, `amenities` , etc. for every price. But it will also make your queries much faster sinch they will have to perform a single join instead of two and you will be able to use nested sorting: https://www.elastic.co/guide/en/elasticsearch/guide/current/nested-sorting.html
</comment><comment author="sumitjainn" created="2016-01-06T17:14:32Z" id="169392949">@jpountz It was one of the approaches we tried, but sadly it doesn't work. The reason is the loss of association between room and invPrice. So the query to match only hotels for rooms which have inventory on 2 dates d1 and d2 (not across different rooms), is not possible. This is direct result of the fact that the grouping facilitated though nesting of all invPrice of one room under it, has been given away, while flattening.
</comment><comment author="jpountz" created="2016-01-06T17:43:17Z" id="169400520">Ok I see. I don't have a good alternative then :(
</comment><comment author="sumitjainn" created="2016-01-06T17:58:25Z" id="169404585">Is it not a valid feature request then? Wouldn't it benefit to have a source cache for many use cases apart from this? Can it atleast be made open for discussion?
</comment><comment author="jpountz" created="2016-01-06T18:17:26Z" id="169408891">@sumitjainn I'm open for discussion as of how to solve this kind of problem, however I am -1 on the _source cache idea. Maybe you can open a new issue that explains the problem that you are trying to solve and links to the discussion that happened here?
</comment><comment author="sumitjainn" created="2016-01-06T18:30:02Z" id="169411966">@jpountz I have created the issue, but could you please elaborate on your negative vote on caching? Does having doc values preclude any need or possibility of a _source cache? 
</comment><comment author="jpountz" created="2016-01-06T21:16:26Z" id="169463525">Elasticsearch has two ways to read field values:
- doc values to read a couple field values from many documents, which is typically useful for sorting/aggregations
- stored fields/_source to read as many field values as you want from a couple of documents, which is typically useful to get summaries for the top matches

So it doesn't make sense to try to optimize stored fields for the use-case that doc values have been designed for. Additionally I don't see how a cache would help for this problem.
</comment><comment author="sumitjainn" created="2016-04-28T07:15:17Z" id="215331891">@jpountz I have stumbled upon another issue due to non-caching of _source:

I have top level hotel doc with large number of nested docs (around 30k) with the total doc size ~ 35 MB. Now fetching any field in the response (nested or otherwise), gives me a response time of 2-6 secs with only ~20 matching docs. Response time is always &gt; 1 sec even with &lt;5 matching docs.

Now when I used multiple stored fields instead of _source, my response times dropped to ~ 50ms. I have tested this on machine with 16 GB free memory to be used by OS for File System cache. 

So, isn't this too much of a performance penalty to bear? Also, the es documentation is inaccurate, as it heavily recommends _source over stored fields, while not spelling out its limitations, found thus. 

As to your last point, while we have changed our schema since then,  I don't think doc values solves the use case I had mentioned. Also I don't see why caching _source wouldn't help, wouldn't reading from memory be faster than from disk?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add serialization support for more important IOExceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15766</link><project id="" key="" /><description>Several IOExceptions are always wrapped in an NotSerializableWrapper which is
annoying to read. These exceptions are important to get right across the network
and we should support the important ones that indicate problems on the Filesystem.

This commit also adds general support for IOException to preserve the parent type
across the network if no specific type is serializable.
</description><key id="124927655">15766</key><summary>Add serialization support for more important IOExceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-05T09:33:39Z</created><updated>2016-01-10T12:59:31Z</updated><resolved>2016-01-05T11:00:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-01-05T09:33:46Z" id="168947214">@mikemccand FYI
</comment><comment author="javanna" created="2016-01-05T09:39:02Z" id="168948703">LGTM
</comment><comment author="mikemccand" created="2016-01-05T10:04:18Z" id="168959471">LGTM, thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `_gce_` network when not using discovery gce</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15765</link><project id="" key="" /><description>For now we support `_gce_` only if discovery is set to `gce` and all information about GCE is provided (project_id and zone).
But in some cases, people would like to only bind to `_gce_` on a single node (without any elasticsearch cluster).

They could access the machine then from other machines running inside the same project.

This commit adds a new GceMetadataService which is started as soon as the plugin is started so GceNameResolver can use it to resolve `_gce`.

Closes #15724.

(cherry picked from commit 9871e41)
</description><key id="124925182">15765</key><summary>Allow `_gce_` network when not using discovery gce</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery GCE</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-01-05T09:21:05Z</created><updated>2016-07-28T15:20:13Z</updated><resolved>2016-07-28T15:19:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-05T09:22:45Z" id="168943303">@imotov As discussed in #15726 I cherry picked my code and applied it to master branch.
@bleskes Could you review this please?
</comment><comment author="bleskes" created="2016-01-06T11:49:25Z" id="169306849">I think we if we break GceComputeService into two functions (network + discovery) we should have those clearly named. It seems GceComputeService needs to become GceDiscoveryService in it's current implementation. 
</comment><comment author="dadoonet" created="2016-02-20T06:55:16Z" id="186528875">@bleskes Sorry it took me a loooong time to apply your changes...

&gt; I think we if we break GceComputeService into two functions (network + discovery) we should have those clearly named. It seems GceComputeService needs to become GceDiscoveryService in it's current implementation.

I don't think we should change the names here. The two service classes we have here are responsible for:
- Calling GCE API (GceComputeService)
- Getting GCE instance metadata (GceMetadataService)

The discovery is done in `GceUnicastHostsProvider` which uses `GceComputeService` to get the list of the IP addresses running in GCE.

WDYT?
</comment><comment author="dadoonet" created="2016-03-02T18:00:54Z" id="191351267">@bleskes Before you can review it, I need to merge the master changes in it first. Some code changed. I'll ping you when ready to review.
</comment><comment author="dakrone" created="2016-04-06T17:23:52Z" id="206474608">@dadoonet is this still waiting on changes in master?
</comment><comment author="dadoonet" created="2016-04-29T14:58:15Z" id="215744815">@bleskes I finally ended up rebasing everything on master so it's ready for review and hopefully merged! :)

Thanks for your patience!
</comment><comment author="dadoonet" created="2016-05-27T06:43:18Z" id="222072194">@bleskes Do you have time to review this again?
</comment><comment author="bleskes" created="2016-06-02T12:19:04Z" id="223274686">thx @dadoonet . I left some comments. please ping if something needs clarification
</comment><comment author="dadoonet" created="2016-06-03T16:40:56Z" id="223629903">Thanks a lot @bleskes!
I pushed some other commits to address your remarks.
I need your help with this one I was not able to apply: https://github.com/elastic/elasticsearch/pull/15765#discussion_r65528384
</comment><comment author="dadoonet" created="2016-07-21T08:30:35Z" id="234190114">@bleskes I just added a new commit. (yeah I know: it took time) 

I was not able for now to add the network name resolver in the NetworkService ctor. 
The NetworkService is built in [Node class](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/node/Node.java#L266) with:

``` java
final NetworkService networkService = new NetworkService(settings);
```

So I'm unsure on how I could deal with that.

For now, I added a method which can add to the existing network service the network name resolver.

Let me know what you think.
</comment><comment author="dadoonet" created="2016-07-21T09:41:22Z" id="234206059">@rjernst I added this 5e57febe53c6961b65701d3f74e6af91559018d4 to address your comment.
I think I will come with a follow up PR to also remove:

``` java
    public void onModule(DiscoveryModule discoveryModule) {
        discoveryModule.addDiscoveryType(EC2, ZenDiscovery.class);
        discoveryModule.addUnicastHostProvider(EC2, AwsEc2UnicastHostsProvider.class);
    }
```

And add new methods in the DiscoveryPlugin interface instead.
</comment><comment author="rjernst" created="2016-07-21T16:45:59Z" id="234313061">@dadoonet Thanks for trying to move this to the new plugin model. But I don't think we should leave it in a state of adding a new use of onModule unless there is a blocker issue for the next release.
</comment><comment author="dadoonet" created="2016-07-25T14:01:56Z" id="234961657">@rjernst I'm not adding a new use of `onModule` here. It was there already and I don't touch it (yet).
I think this should come as a follow up for this PR.

Do you think we should block this PR until we changed the Discovery plugins to the new model? I gave a first look at what is needed to achieve this goal and it looks a bit complicated. 

That's why I'd prefer merging this PR first. I opened it in January and would love to have it merged.

WDYT?
</comment><comment author="rjernst" created="2016-07-26T15:37:50Z" id="235307974">@dadoonet Sorry, you are right, I misread the change. But this change is still half-way. The NetworkService should take these on construction and have them be immutable, instead of still having a register method. 
</comment><comment author="dadoonet" created="2016-07-26T16:27:07Z" id="235323952">@rjernst Thanks for the advice. Makes a lot of sense. I pushed a new commit.
</comment><comment author="rjernst" created="2016-07-27T10:25:33Z" id="235547320">LGTM, I left 2 very minor additional comments.
</comment><comment author="dadoonet" created="2016-07-27T11:37:34Z" id="235560506">Thank you @rjernst! 

I also added a [latest commit](https://github.com/elastic/elasticsearch/pull/15765/commits/fb9bad23de1febf5ed84a9da286d27f7d0d1d7af). I forgot to address one of @bleskes comment.

Do you mind giving a final review on this?
</comment><comment author="bleskes" created="2016-07-28T14:47:13Z" id="235917269">the two last two commits look good to me 
</comment><comment author="dadoonet" created="2016-07-28T15:20:13Z" id="235927943">\o/ Fantastic guys! Thank you so much @rjernst and @bleskes for the help on this! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>segment memory usage control[feature request]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15764</link><project id="" key="" /><description>let's say I have many many indeces, nowsdays all of term dictonaries are loaded into memory and thus cost lots of memory. so my request is if we can use a fixed size memory pool to manage this part of memory.
</description><key id="124915125">15764</key><summary>segment memory usage control[feature request]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2016-01-05T07:54:37Z</created><updated>2016-01-15T12:54:41Z</updated><resolved>2016-01-15T12:54:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T17:47:16Z" id="169401484">I don't see how this would help?
</comment><comment author="jpountz" created="2016-01-07T18:11:13Z" id="169759909">&gt; nowsdays all of term dictonaries are loaded into memory

By the way this is not correct. Only a tiny portion of it is loaded into memory.
</comment><comment author="makeyang" created="2016-01-08T06:59:02Z" id="169912455">my cluster has 50 data nodes and more than 1000 indices. 
when I execute:
GET _cat/nodes?v&amp;h=port,sm
below is the response:
port       sm 
9300    1.1gb 
9300  963.9mb 
9300    1.1gb 
9300      1gb 
9300  934.3mb 
9300      1gb 
9300  885.9mb 
9300    1.1gb 
9300      1gb 
9300      1gb 
9300      1gb 
9300  973.9mb 
9300    1.1gb 
9300  941.4mb 
9300  964.3mb 
9300    894mb 
9300  933.2mb 
9300    1.1gb 
9300 1000.6mb 
9300  886.1mb 
9300    1.1gb 
9300      1gb 
9300 1019.3mb 
9300   1003mb 
9300    1.3gb 
9300  989.1mb 
9300 1022.6mb 
9300  945.8mb 
9300  986.5mb 
9300    1.2gb 
9300  893.7mb 
9300  938.5mb 
9300      1gb 
9300      1gb 
9300  966.5mb 
9300  944.1mb 
9300  915.8mb 
9300  925.3mb 
9300      1gb 
9300      1gb 
9300      1gb 
9300  919.6mb 
9300      1gb 
9300      1gb 
9300      1gb 
9300  896.4mb 
9300    1.2gb 
9300      1gb 
9300  879.9mb 
9300  819.9mb 

as our indices are becoming more and more, sm part will cost more and more memory. so this feature is make this part of memory consumption predictable and controable.
</comment><comment author="makeyang" created="2016-01-15T11:07:50Z" id="171934820">@jpountz  any updates?
</comment><comment author="clintongormley" created="2016-01-15T12:54:41Z" id="171952819">As @jpountz said: 

&gt; I don't see how this would help?

You can see how much memory is being used by each segment with:

```
GET _stats/segments
```

or in detail:

```
GET _segments?verbose
```

You will see that the term dictionary doesn't actually use that much memory.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Fix/simulate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15763</link><project id="" key="" /><description>add documentation

(and fix typo in rest test)
</description><key id="124867520">15763</key><summary>[Ingest] Fix/simulate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>docs</label><label>review</label></labels><created>2016-01-05T00:11:39Z</created><updated>2016-01-05T15:21:42Z</updated><resolved>2016-01-05T15:21:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-05T08:21:36Z" id="168932560">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close recovered translog readers if createWriter fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15762</link><project id="" key="" /><description>If we fail to create a writer all recovered translog readers are not
closed today which causes all open files to leak.

Closes #15754

@jasontedor @mikemccand can you take a look
</description><key id="124856442">15762</key><summary>Close recovered translog readers if createWriter fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T22:43:43Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2016-01-05T07:23:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-01-05T01:09:43Z" id="168860757">LGTM
</comment><comment author="jasontedor" created="2016-01-05T04:04:23Z" id="168889452">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't use guice for QueryParsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15761</link><project id="" key="" /><description>1. Gets guice out of the business of building ScoreFunctionParsers and
   QueryParsers.
2. Moves QueryParser registration to SearchModule
3. Moves NamedWriteableRegistry construction out of guice and into Node and
   TransportClient.
4. Moves shape registration into SearchModule so now all named writeable
   registration is done in the SearchModule.

This is breaking for plugin authors. Instead of declaring new QueryParser
like:

``` java
public void onModule(IndicesModule module) {
  module.registerQueryParser(NewQueryParser.class);
}
```

you do it like:

``` java
public void onModule(SearchModule module) {
  module.registerQueryParser(NewQueryParser::new);
}
```

The QueryParser's argument no longer come from @Inject, now they come from
the declaration in the plugin. The above example is for a no-arg QueryParser.
Most of the QueryParsers in Elasticsearch are no-arg.

ScoreFunctionParsers have a similar but slightly different change. This:

``` java
public void onModule(SearchModule module) {
  module.registerFunctionScoreParser(NewFunctionScoreParser.class);
}
```

becomes

``` java
public void onModule(SearchModule module) {
  module.registerFunctionScoreParser(new NewFunctionScoreParser());
}
```

Since all known ScoreFunctionParsers have no arg constructors its simpler to
just build them at registration time rather than specify a supplier that is
used to build them later.
</description><key id="124850511">15761</key><summary>Don't use guice for QueryParsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T22:09:07Z</created><updated>2016-07-29T12:08:38Z</updated><resolved>2016-01-18T22:39:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-05T09:42:58Z" id="168949372">looks good! what exactly needs to be discussed here? :) Can we we highlight the changes needed for plugin developers that implement their own query parsers (e.g. just add the code snippet to the description of the issue)?
</comment><comment author="nik9000" created="2016-01-05T13:20:15Z" id="168995700">&gt; looks good! what exactly needs to be discussed here? :)

I dunno. I added it because I wasn't really sure how well it'd be taken. We can't construct all the shared dependencies at the Node level but this one felt ok. We'll need a way for plugins to get access to these dependencies. I was _thinking_ the way to do that is to make them available on the modules in public methods. But that discussion can wait I think.

&gt; Can we we highlight the changes needed for plugin developers that implement their own query parsers (e.g. just add the code snippet to the description of the issue)?

Yeah. I think I can squash and do that.
</comment><comment author="nik9000" created="2016-01-05T22:23:37Z" id="169152849">Squashed and edited the commit description to describe the breaking changes to plugin authors. I've also added `breaking` and `:Plugins` to make it obvious that this is breaking to plugin authors. I've also edited the PR description to include the breaking changes.
</comment><comment author="nik9000" created="2016-01-18T19:50:45Z" id="172633737">@javanna its been a while - does this still look good to you? I'm going to rebase it so we have a chance of being able to merge it.
</comment><comment author="javanna" created="2016-01-18T22:33:12Z" id="172671579">sorry @nik9000 this had gotten off my radar. Had another look, LGTM ! cool stuff!
</comment><comment author="nik9000" created="2016-01-18T22:39:39Z" id="172673014">&gt; sorry @nik9000 this had gotten off my radar. Had another look, LGTM ! cool stuff!

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove fuzzy query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15760</link><project id="" key="" /><description>**Note**: This is the original comment stating some alternatives when it was not yet clear on how to proceed. As you can read below, we decided on option 1

As a term-level query, a fuzzy query will not run through the analysis stage. This can produce surprising results and I'd like to put this up for discussion whether we can do something about it. 

Consider this example:

```
PUT /people
{
   "mappings": {
      "person": {
         "properties": {
            "name": {
                "type": "string"
            }
         }
      }
   }
}
```

```
POST /_bulk
{"index":{"_index":"people","_type":"person"}}
{"name":"Marcy"}
{"index":{"_index":"people","_type":"person"}}
{"name":"Darcy"}
```

```
GET /_search
{
   "query": {
      "fuzzy": {
         "name": {
            "term": "Marcy",
            "fuzziness": "1"
         }
      }
   }
}
```

One could expect that this query returns "Marcy" and "Darcy" as the Levenshtein distance to the search term "Marcy" is 0 and 1 respectively. However, as person names are analyzed fields, the index contains "marcy" and "darcy", which explains why we just match one document with the provided fuzziness.

There are multiple options how we could tackle this.
### Option 1: Remove fuzzy query

The docs already state that [fuzzy queries are not that useful](https://www.elastic.co/guide/en/elasticsearch/guide/master/fuzzy-scoring.html). So one option could be to remove support for fuzzy queries and just expose them only indirectly e.g. in suggesters.
### Option 2: Analyze fuzzy query terms

We _could_ run search terms of a fuzzy query through an analyzer. While it will work out in the example case above, there are other cases where this will lead to really strange results. Consider this case:

```
GET /books/_analyze?analyzer=english
{
    "field": "content",
    "text": "atypical"
}
```

which produces the token `atyp`. Now a user might do a fuzzy search on this field, searching for "typical". Would we run this through the same analyzer, this would produce the token `typic` which obviously will not fuzzy-match. This is even more puzzling.

The above case has already shown that analyzing query terms can also produce unexpected results. I think it boils down to the fact that there is an imbalance between what's indexed and what's searched.
### Option 3: Improve docs

The [Term Query docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html) already point out the problem but this is maybe too disconnected from the fuzzy query docs so users don't see that this warning also applies to fuzzy queries.

So we could add specific examples that demonstrate this case and link back to the term query docs for the explanation.
</description><key id="124845493">15760</key><summary>Remove fuzzy query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Search</label></labels><created>2016-01-04T21:41:32Z</created><updated>2017-01-10T08:35:36Z</updated><resolved>2016-12-12T11:09:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-01-05T03:59:58Z" id="168887937">The fuzzy query is a sort of term query, it generates all possible matching terms that are within the maximum edit distance of the specified term. There is no analysis involved. If you want to do a fuzzy query with an analyzer you can already use the match query with the fuzziness parameter: https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html#query-dsl-match-query-fuzziness
Though I agree that analysis and fuzziness can produce strange things, some warnings in the docs (of the match query) could be useful.
</comment><comment author="jimczi" created="2016-01-05T04:04:30Z" id="168889460">... and the Levenshtein distance of "darcy" and "Marcy" is 1 so your example should return both results ;)
</comment><comment author="danielmitterdorfer" created="2016-01-05T14:46:17Z" id="169020265">Ah, changing the example strings really bit me. If you index "Trezy" and "Trezi" and search for "Trezy" you will get back only one hit: "Trezy".
</comment><comment author="danielmitterdorfer" created="2016-01-15T08:12:07Z" id="171898144">So based on the discussion so far I'd suggest we improve the docs for fuzzy queries (option 3).
</comment><comment author="danielmitterdorfer" created="2016-01-15T12:13:44Z" id="171945862">We discussed this issue on FixItFriday and decided to go for option 1.

The current plan is to deprecate fuzzy queries in 2.x. and to remove them in 3.0. We should clearly state in the docs we still support fuzziness in match queries and the completion suggester.
</comment><comment author="danielmitterdorfer" created="2016-01-15T12:16:55Z" id="171946310">Changed the ticket title to reflect our decision in the FixItFriday meeting.
</comment><comment author="danielmitterdorfer" created="2016-01-27T08:43:08Z" id="175484447">We deprecate fuzzy query with 3.0.0 (see #16211) and will remove it with 4.0.0. I leave myself assigned as a reminder until we start working on 4.0.0.
</comment><comment author="rjernst" created="2016-01-27T15:05:05Z" id="175672772">Why did this switch from deprecation in 2.x to 3.0?
</comment><comment author="danielmitterdorfer" created="2016-01-27T15:14:17Z" id="175679953">The original intent was to deprecate it with 2.3 and remove it with 3.0 (which would have required two separate PRs) but Adrien suggested we remove it with 4.0 in #16121. To do this consistently with our development model, I redid the change on master for 3.0 in #16211. Before backporting it, I approached Clint and he suggested that a deprecation period of a complete major version cycle is long enough. Hence, I did not backport to 2.x.
</comment><comment author="dmr" created="2016-01-27T18:51:10Z" id="175790218">I like the fuzzy query but I understand the reasons to drop it. Will there be a plugin with that functionality or how can the functionality be achieved without fuzzy query?
</comment><comment author="danielmitterdorfer" created="2016-01-28T07:42:42Z" id="176036392">Hi @dmr,

currently, we have not planned to reimplement this is a plugin. The [fuzzy query docs for 3.0](https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-fuzzy-query.html) are already available online, but I reproduce it here as a summary:
- There will be no exact replacement for string fields but note that we still support [fuzziness for match queries](https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-match-query.html#query-dsl-match-query-fuzziness)
- For date and numeric fields you can use [range queries](https://www.elastic.co/guide/en/elasticsearch/reference/master/query-dsl-range-query.html).

Does that help?
</comment><comment author="dmr" created="2016-01-28T08:33:37Z" id="176052449">Thank you for the quick reply. You answer helps in the sense that I understand that there is no replacement but we have a few customers who started to like that feature.

I plan to keep up with elasticsearch versions so I need a replacement that I can offer our clients.
</comment><comment author="clintongormley" created="2016-01-28T09:15:08Z" id="176077166">@dmr the replacement is the match query with the `fuzziness` parameter.  What functionality do you think will be missed?
</comment><comment author="dmr" created="2016-02-02T20:59:23Z" id="178815975">I just didn't know about this parameter, thank you, fuzziness is perfect for me!
</comment><comment author="cbuescher" created="2016-10-18T12:16:08Z" id="254489737">@danielmitterdorfer since you are assigned to this, is there already an open PR for this? If not, I can open one, I just started removing `fuzzy` on master because it has some deprecation comments and came across this issue.
</comment><comment author="danielmitterdorfer" created="2016-10-18T12:18:42Z" id="254490189">@cbuescher No, I didn't start with this yet. Go ahead. :)
</comment><comment author="javanna" created="2016-11-29T14:09:20Z" id="263579264">I came up with a PR for this, but it turns out that we may have to discuss it again. See https://github.com/elastic/elasticsearch/pull/21851#issuecomment-263559249 . Marking for discussion. We have to go back and un-deprecate if we want to keep this query around.</comment><comment author="danielmitterdorfer" created="2016-11-30T07:13:34Z" id="263800534">@javanna As Clint has mentioned in https://github.com/elastic/elasticsearch/pull/21851#issuecomment-263576787, we only considered `fuzzy` queries (on top-level) in our original discussion but did not think about `fuzzy` queries as part of `span` queries. IIRC within the scope that we discussed, we did not find compelling reasons to keep `fuzzy` around and that's why we decided to remove it. If we want to keep the functionality mentioned in the review of #21851, then we should undeprecate it. I hope that sheds some light on this issue.</comment><comment author="clintongormley" created="2016-11-30T11:11:06Z" id="263847543">The `fuzzy` query today falls under the section called [Term level queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/term-level-queries.html) which explains that the queries in that section are all low level term-based queries that don't use analysis.
We could possibly reiterate that point on the fuzzy query page itself, and point people to the match query with fuzziness instead.

But given that the match query can't be used in span queries where the fuzzy query CAN be used, I think we have no option but to keep it.</comment><comment author="danielmitterdorfer" created="2016-12-02T10:11:14Z" id="264419448">We discussed this again in Fix-It Friday and will it un-deprecate it for the reasons that Clint has stated above.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed example query for Native Java plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15759</link><project id="" key="" /><description>Based on Elasticsearch v2.1.1 source code, the correct way to use a custom native Java plugin is to use "inline" in the query instead of "id".

E.g.:

```
GET _search
{
    "fields": [
       "_boost"
    ], 
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "functions": [
        {
          "script_score": {
            "script": {
                "inline": "my_script",
                "lang" : "native"
            }
          }
        }
      ]
    }
  }
}
```
</description><key id="124827019">15759</key><summary>Fixed example query for Native Java plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">MohammedJabarullah</reporter><labels><label>docs</label><label>v2.2.1</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T19:56:49Z</created><updated>2016-03-09T10:38:36Z</updated><resolved>2016-02-01T13:40:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MohammedJabarullah" created="2016-01-04T20:01:57Z" id="168789449">I've signed the CLA.
</comment><comment author="jpountz" created="2016-01-05T13:23:45Z" id="168996227">@colings86 could you help confirm this is actually a documentation bug and not a bug in parsing?
</comment><comment author="colings86" created="2016-01-11T14:17:57Z" id="170564057">@MohammedJabarullah thanks for catching this and for submitting a PR. I left a minor comment, if you could update the PR when you have a minute I can get this merged in.
</comment><comment author="MohammedJabarullah" created="2016-01-15T12:47:01Z" id="171951641">@colings86 You're welcome and thanks for accepting my patch. I'm glad I could contribute to the project that I use at work.
</comment><comment author="colings86" created="2016-02-01T13:55:31Z" id="177981335">Merged into master and manually fixed in 2.x and 2.2 branches. Thanks again for the contribution @MohammedJabarullah 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>suggest_mode is per shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15758</link><project id="" key="" /><description /><key id="124825920">15758</key><summary>suggest_mode is per shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Suggesters</label><label>docs</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T19:51:21Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2016-01-04T20:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-04T19:52:49Z" id="168785425">Sorry for the whitespace changes. My editor eats trailing whitespace and I hate trailing whitespace enough to not work to stop it. You can open up the changes with `?w=true` on the url and only get the actual, real edits.
</comment><comment author="nik9000" created="2016-01-04T19:53:00Z" id="168785538">Ping @clintongormley and @s1monw to make sure this all makes sense.
</comment><comment author="s1monw" created="2016-01-04T20:00:05Z" id="168789021">LGTM - thanks nik
</comment><comment author="nik9000" created="2016-01-04T21:02:41Z" id="168807788">Merged to master and pushed to the 2.x, 2.2, and 2.1 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Types filter doesn't work for multiple types (EDIT - with nested fields)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15757</link><project id="" key="" /><description>Client and Server on 2.1.0
This is superficially similar to https://github.com/elastic/elasticsearch/issues/2218.

I have an index called index1 with many types.
curl 'http://127.0.0.1:9200/index1/type1/_search?q=test
returns one result
curl 'http://127.0.0.1:9200/index1/type2/_search?q=test
returns one result.
These are different results with the correct type.
curl 'http://127.0.0.1:9200/index1/foo,bar,whatever/_search?q=test
returns both results
curl 'http://127.0.0.1:9200/index1/type1,foo/_search?q=test
returns results of type1 and type2 (both results)

This index and its mappings were created using the Java API. I cannot recreate this issue when using curl with a new index and mappings.
</description><key id="124814217">15757</key><summary>Types filter doesn't work for multiple types (EDIT - with nested fields)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">TheHound</reporter><labels><label>:Nested Docs</label><label>bug</label></labels><created>2016-01-04T18:42:48Z</created><updated>2016-01-13T08:51:03Z</updated><resolved>2016-01-13T08:45:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-01-04T20:00:56Z" id="168789211">Can you reproduce consistently with the Java API?  If so, can you provide an example snippet in a gist or the like?
</comment><comment author="TheHound" created="2016-01-05T10:19:48Z" id="168964334">Hi, apologies for the misleading issue above I have finally reproduced with curl (I had missed something from my mapping previously). This seems to be cause by nested fields. In the last step outlined below there are 2 results. If the mapping for entityContent is removed then there are no results (this is correct)

N.B. Not sure if this helps or not but this is a regression from 1.7.3 caught by one of our tests as part of me upgrading to 2.1.0

Steps:

```
#Create Index
curl -XPUT "http://localhost:9200/index1"

#Create mappings
curl -XPUT --data @index1.type1.json "http://localhost:9200/index1/type1/_mapping"
curl -XPUT --data @index1.type2.json "http://localhost:9200/index1/type2/_mapping"

#Add Data
curl -XPUT --data @index1.data.json "http://localhost:9200/index1/type1/2"
curl -XPUT --data @index1.data.json "http://localhost:9200/index1/type2/2"

#Creates 2 hits
curl "http://localhost:9200/index1/a,b/_search?q=test"
```

Files:
index1.type1.json

```
{
    "type1" : {
        "properties" : {
            "content" : {
                "type" : "string"
            },
            "entityContent" : {
                "type" : "nested",
                "properties" : {
                    "field1" : {
                        "type" : "string"
                    }
                }
            }
        }
    }
}
```

index1.type2.json

```
{
    "type2" : {
        "properties" : {
            "content" : {
                "type" : "string"
            }
        }
    }
}
```

index1.data.json

```
{
    "content" : "This is a Test."
}
```
</comment><comment author="TheHound" created="2016-01-05T17:13:36Z" id="169066063">Having a quick read through the elasticsearch code base I assume this is going wrong on line 534 of 
https://github.com/elastic/elasticsearch/blob/1a47226d9af373e4627cd360ad8a8a7b93ea7882/core/src/main/java/org/elasticsearch/index/mapper/MapperService.java though not familiar enough to say for certain.
</comment><comment author="clintongormley" created="2016-01-10T12:59:12Z" id="170343132">Hi @TheHound 

I don't understand this issue at all, or how what you see is different from what you expect.  The example you give above returns two documents, which is what I'd expect to get.  You're searching the `_all` field for the word `test`, which matches both documents.
</comment><comment author="TheHound" created="2016-01-11T09:07:26Z" id="170476761">Hi @clintongormley 

I would expect the path part of the URL /a,b/ to act as a type filter. Since those types do not exist I would expect no results.

So:

```
curl 'http://127.0.0.1:9200/index1/type1/_search?q=test
```

Only returns 1 hit. I would therefore expect this to only return 1 hit:

```
curl 'http://127.0.0.1:9200/index1/type1,type3/_search?q=test
```

But instead returns 2, 1 for type1 and 1 for type2.
</comment><comment author="clintongormley" created="2016-01-11T13:29:04Z" id="170549121">OK gotcha, here's a recreation:

```
PUT index1

PUT index1/type1/_mapping
{
  "type1": {
    "properties": {
      "entityContent": {
        "type": "nested",
        "properties": {
          "field1": {
            "type": "string"
          }
        }
      }
    }
  }
}

PUT index1/type1/1
{
    "content" : "This is a Test."
}

PUT index1/type2/2
{
    "content" : "This is a test."
}
```

Querying with a known type plus an unknown type returns both documents incorrectly:

```
GET index1/type1,type3/_search?q=test
```

The explanation for this query (when there is a nested document mapped):

```
GET index1/type1,type3/_validate/query?q=test&amp;explain
```

is:

```
"+_all:test #ConstantScore(+ConstantScore(ConstantScore(_type:type1) _type:type3 +(+*:* -_type:__*)))"
```

However, if you repeat the explanation without the nested doc mapping, you get:

```
"+_all:test #ConstantScore(+ConstantScore(ConstantScore(_type:type1) _type:type3))"
```
</comment><comment author="martijnvg" created="2016-01-13T08:51:03Z" id="171218317">@TheHound Thanks for reporting this issue! I'll back port it to all 2.x branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PathHierarchy vs path_hierarchy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15756</link><project id="" key="" /><description>In versions of Elasticsearch prior to 2.0 the documentation for the path_hierarchy analyzer says the 'type' field has these requirements. 

```
Required. Must be set to PathHierarchy (case-sensitive).
```

https://www.elastic.co/guide/en/elasticsearch/reference/1.7/analysis-pathhierarchy-tokenizer.html

In 2.0+ that is now incorrect and if you have an index that uses type: 'PathHierarchy' the upgrade process from 1.7.\* to 2.\* fails on each shard of the index. It requires it to be type: 'path_hierarchy'. 

We have a user that has an index with many billions of records in it that uses this analyzer as specified by the 1.7 documentation. This index is so large it is infeasible to reindex to simply change this part of the mapping. Is this an intentional change in 2._? Is there any way to patch the mapping attached to the shards so that an upgrade to 2._ is possible?

Note: this issue is not flagged by the migration plugin. We were testing the upgrade on a test cluster and only discovered the issue when the shards attempted to upgrade. It just logs the following stack repeatedly for each shard and effectively corrupts the index since other indices had upgraded successfully while this type fails. In that scenario the cluster can't be reverted without data loss.

```
[2015-12-14 18:42:56,825][WARN ][indices.cluster          ] [node] [[logs-2015.12.11][3]] marking and sending shard failed due to [failed to create index]
[logs-2015.12.11] IndexCreationException[failed to create index]; nested: IllegalArgumentException[Unknown Tokenizer type [PathHierarchy] for [field_tokens]];
    at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:362)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:307)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:176)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Unknown Tokenizer type [PathHierarchy] for [field_tokens]
    at org.elasticsearch.index.analysis.AnalysisModule.configure(AnalysisModule.java:267)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:61)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:233)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:105)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:143)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:159)
    at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
    at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:358)
    ... 8 more
```
</description><key id="124804731">15756</key><summary>PathHierarchy vs path_hierarchy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kstaken</reporter><labels><label>blocker</label><label>bug</label><label>v2.2.0</label></labels><created>2016-01-04T17:42:40Z</created><updated>2016-01-06T13:51:20Z</updated><resolved>2016-01-06T13:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-01-05T16:35:46Z" id="169053327">This is indeed a bug. We will provide a fix for the next release. If you can't wait for the next release, you might try the following before upgrading to 2.x (try it first on a test cluster, and make backups before proceeding): Close indices. Update settings of indices by changing PathHierarchy to path_hierarchy. Open indices. That should be it. You can then upgrade to 2.x.

Example (tested on 1.7.4):

```

# This is the example index we start with
curl -X POST http://localhost:9200/myindex/ -d'
{
  "settings": {
    "analysis": {
      "analyzer": {
        "path-analyzer": {
          "type": "custom",
          "tokenizer": "path-tokenizer"
        }
      },
      "tokenizer": {
        "path-tokenizer": {
          "type": "PathHierarchy",
          "delimiter": "."
        }
      }
    }
  },
  "mappings": {
    "test": {
      "properties": {
        "text": {
          "type": "string",
          "analyzer": "path-analyzer"
        }
      }
    }
  }
}'

# Close index
curl -X POST http://localhost:9200/myindex/_close

# Update settings by changing PathHierarchy to path_hierarchy
curl -X PUT http://localhost:9200/myindex/_settings -d'
{
  "settings": {
    "analysis": {
      "analyzer": {
        "path-analyzer": {
          "type": "custom",
          "tokenizer": "path-tokenizer"
        }
      },
      "tokenizer": {
        "path-tokenizer": {
          "type": "path_hierarchy",
          "delimiter": "."
        }
      }
    }
  }
}'

# reopen index
curl -X POST http://localhost:9200/myindex/_open
```
</comment><comment author="ywelsch" created="2016-01-06T13:51:19Z" id="169327924">Fixed in #15785
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use egrep instead of grep -E for Solaris</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15755</link><project id="" key="" /><description>/usr/bin/grep on Solaris doesn't support -E.  /usr/xpg4/bin/grep does support -E, but it was considered cleaner to use egrep which corresponds to grep -E on Linux and supports enough of the same regex syntax on Solaris.  egrep also works on Mac OS X.

Closes #15628
</description><key id="124799685">15755</key><summary>Use egrep instead of grep -E for Solaris</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">droberts195</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T17:15:17Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2016-01-06T21:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-01-06T21:18:59Z" id="169464845">Merged thanks! I'll backport this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reproducible SearchWithRandomIOExceptionsIT failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15754</link><project id="" key="" /><description>This test rarely fails and is often not reproducible, but I had a new failure on my local CI on my feature branch that I found reliably reproduces on master too:

``` bash
$ gradle \
&gt; :core:clean \
&gt; :core:integTest \
&gt; -Dtests.seed=7B8A12D17560A5D \
&gt; -Dtests.class=org.elasticsearch.search.basic.SearchWithRandomIOExceptionsIT \
&gt; -Dtests.method=testRandomDirectoryIOExceptions
```

Take note that the output logs approach 45 MB in size.

I ran `git-bisect` and it looks like this issue was introduced with fcfd98e9e89231d748ae66c81791b0b08b0c6200.
</description><key id="124776788">15754</key><summary>Reproducible SearchWithRandomIOExceptionsIT failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>bug</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T15:17:05Z</created><updated>2016-01-05T07:23:11Z</updated><resolved>2016-01-05T07:23:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-04T15:25:44Z" id="168705420">@jasontedor was this a ci run? if so, can you add a link? also, stack trace would be good, just to know at a glance what failed...
</comment><comment author="jasontedor" created="2016-01-04T15:29:09Z" id="168706443">@bleskes It failed on my local CI, not the public CI and the entire log output is 45 MB. Here's a relevant snippet of the logs:

```
[2016-01-03 14:26:38,400][WARN ][org.elasticsearch.index.engine] [node_s0] [test][1] failed engine [index]
java.io.IOException: a random IOException (_0.fdx)
    at org.apache.lucene.store.MockDirectoryWrapper.maybeThrowIOException(MockDirectoryWrapper.java:445)
    at org.apache.lucene.store.MockIndexOutputWrapper.writeBytes(MockIndexOutputWrapper.java:151)
    at org.apache.lucene.store.MockIndexOutputWrapper.writeByte(MockIndexOutputWrapper.java:127)
    at org.apache.lucene.store.DataOutput.writeInt(DataOutput.java:70)
    at org.apache.lucene.codecs.CodecUtil.writeHeader(CodecUtil.java:91)
    at org.apache.lucene.codecs.CodecUtil.writeIndexHeader(CodecUtil.java:134)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.&lt;init&gt;(CompressingStoredFieldsWriter.java:116)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsWriter(CompressingStoredFieldsFormat.java:128)
    at org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat.fieldsWriter(Lucene50StoredFieldsFormat.java:183)
    at org.apache.lucene.codecs.asserting.AssertingStoredFieldsFormat.fieldsWriter(AssertingStoredFieldsFormat.java:49)
    at org.apache.lucene.index.DefaultIndexingChain.initStoredFieldsWriter(DefaultIndexingChain.java:81)
    at org.apache.lucene.index.DefaultIndexingChain.startStoredFields(DefaultIndexingChain.java:258)
    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:295)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:234)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1477)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1256)
    at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:407)
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:358)
    at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:503)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnReplica(TransportIndexAction.java:187)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:169)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:64)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:380)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:286)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:283)
    at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-01-03 14:26:38,403][WARN ][org.elasticsearch.action.index] [node_s1] [test][1] failed to perform indices:data/write/index[r] on node {node_s0}{TCOt3YwhSvefWkM7AUfErQ}{local}{local[59]}[mode=&gt;local]
RemoteTransportException[[node_s1][local[60]][indices:data/write/index[r]]]; nested: IndexFailedEngineException[Index failed for [type#5]]; nested: NotSerializableExceptionWrapper[a random IOException (_0.fdx)];
Caused by: [test][[test][1]] IndexFailedEngineException[Index failed for [type#5]]; nested: NotSerializableExceptionWrapper[a random IOException (_0.fdx)];
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:363)
    at org.elasticsearch.index.shard.IndexShard.index(IndexShard.java:503)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnReplica(TransportIndexAction.java:187)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:169)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:64)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:380)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:286)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:283)
    at org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:296)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: NotSerializableExceptionWrapper[a random IOException (_0.fdx)]
    at org.apache.lucene.store.MockDirectoryWrapper.maybeThrowIOException(MockDirectoryWrapper.java:445)
    at org.apache.lucene.store.MockIndexOutputWrapper.writeBytes(MockIndexOutputWrapper.java:151)
    at org.apache.lucene.store.MockIndexOutputWrapper.writeByte(MockIndexOutputWrapper.java:127)
    at org.apache.lucene.store.DataOutput.writeInt(DataOutput.java:70)
    at org.apache.lucene.codecs.CodecUtil.writeHeader(CodecUtil.java:91)
    at org.apache.lucene.codecs.CodecUtil.writeIndexHeader(CodecUtil.java:134)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.&lt;init&gt;(CompressingStoredFieldsWriter.java:116)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsWriter(CompressingStoredFieldsFormat.java:128)
    at org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat.fieldsWriter(Lucene50StoredFieldsFormat.java:183)
    at org.apache.lucene.codecs.asserting.AssertingStoredFieldsFormat.fieldsWriter(AssertingStoredFieldsFormat.java:49)
    at org.apache.lucene.index.DefaultIndexingChain.initStoredFieldsWriter(DefaultIndexingChain.java:81)
    at org.apache.lucene.index.DefaultIndexingChain.startStoredFields(DefaultIndexingChain.java:258)
    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:295)
    at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:234)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:450)
    at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1477)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1256)
    at org.elasticsearch.index.engine.InternalEngine.innerIndex(InternalEngine.java:407)
    at org.elasticsearch.index.engine.InternalEngine.index(InternalEngine.java:358)
    ... 13 more
```

@bleskes Let me know if you want me to share the entire log output somewhere, but since this seems to reproduce 100% of the time with this seed I don't think that will be needed?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deleting an alias from / deletes the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15753</link><project id="" key="" /><description>Consider

index=foo
alias: foo/bar

to delete an alias manual says to http DELETE host/foo/_alias/bar.

I mistakenly did:

DELETE host/bar

It deleted the alias AND the index.  Wow.  ES should not allow you to remove an alias and take the index down with it.....
</description><key id="124774783">15753</key><summary>deleting an alias from / deletes the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">boschmayhem</reporter><labels /><created>2016-01-04T15:04:46Z</created><updated>2016-01-04T15:09:46Z</updated><resolved>2016-01-04T15:09:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-04T15:09:46Z" id="168701681">Duplicates #2318.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix indent in example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15752</link><project id="" key="" /><description>Previously it would look like if `warnings` key is nested under `errors`.
</description><key id="124773217">15752</key><summary>Fix indent in example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmile</reporter><labels><label>docs</label></labels><created>2016-01-04T14:55:52Z</created><updated>2016-01-05T13:37:06Z</updated><resolved>2016-01-05T13:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolate api shouldn't be allowed to dynamically add fields to mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15751</link><project id="" key="" /><description>Today if a document is percolated and a field in the document being percolated doesn't exist in the mapping then that missing field is dynamically added to the mapping. To use case for this is that a skeleton document could be used to generate a mapping, but since this just adds fields with default settings this isn't doing correctly in many scenarios and percolator queries end up not being matched.

This "dynamically adding of fields" feature should be removed.
</description><key id="124773125">15751</key><summary>Percolate api shouldn't be allowed to dynamically add fields to mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T14:55:17Z</created><updated>2016-01-26T09:48:25Z</updated><resolved>2016-01-26T09:48:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-04T15:04:13Z" id="168700473">+1
</comment><comment author="martijnvg" created="2016-01-06T21:36:58Z" id="169470603">I started to look at this and it is a bit harder then I though. Basically during parsing of the percolate document we use a reference to an existing DocumentMapper and during parsing if a new field is discovered that DocumentMapper is updated. For the percolator (and term vectors api) we kind of want a read only copy of the DocumentMapper and if the document being parsed holds unknown fields it should thrown an exception.

We could change the `dynamic` property on the root mapper, because that option is updatable, but that isn't really an option, because DocumentMapper instances are being shared.
</comment><comment author="jpountz" created="2016-01-07T18:16:29Z" id="169761100">&gt; during parsing of the percolate document we use a reference to an existing DocumentMapper and during parsing if a new field is discovered that DocumentMapper is updated

Hmm I don't think this is correct. Document parsing returns a parsed document and a mapping update, so we would just have to ignore the mapping update I think?

``` diff
diff --git a/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java b/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
index 6733ebd..bdea794 100644
--- a/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
+++ b/core/src/main/java/org/elasticsearch/percolator/PercolateDocumentParser.java
@@ -49,14 +49,12 @@ public class PercolateDocumentParser {
     private final HighlightPhase highlightPhase;
     private final SortParseElement sortParseElement;
     private final AggregationPhase aggregationPhase;
-    private final MappingUpdatedAction mappingUpdatedAction;

     @Inject
-    public PercolateDocumentParser(HighlightPhase highlightPhase, SortParseElement sortParseElement, AggregationPhase aggregationPhase, MappingUpdatedAction mappingUpdatedAction) {
+    public PercolateDocumentParser(HighlightPhase highlightPhase, SortParseElement sortParseElement, AggregationPhase aggregationPhase) {
         this.highlightPhase = highlightPhase;
         this.sortParseElement = sortParseElement;
         this.aggregationPhase = aggregationPhase;
-        this.mappingUpdatedAction = mappingUpdatedAction;
     }

     public ParsedDocument parse(PercolateShardRequest request, PercolateContext context, MapperService mapperService, QueryShardContext queryShardContext) {
@@ -98,9 +96,6 @@ public class PercolateDocumentParser {
                         if (docMapper.getMapping() != null) {
                             doc.addDynamicMappingsUpdate(docMapper.getMapping());
                         }
-                        if (doc.dynamicMappingsUpdate() != null) {
-                            mappingUpdatedAction.updateMappingOnMasterSynchronously(request.shardId().getIndex(), request.documentType(), doc.dynamicMappingsUpdate());
-                        }
                         // the document parsing exists the "doc" object, so we need to set the new current field.
                         currentFieldName = parser.currentName();
                     }
```
</comment><comment author="martijnvg" created="2016-01-18T16:00:55Z" id="172569997">@jpountz right this change is straight forward. I do wonder if we should fail the percolate request if the document to be percolated contains unmapped fields to avoid unexpected behaviour? If so, should have an `ignore_unmapped` setting like we have with sorting?
</comment><comment author="jpountz" created="2016-01-18T16:19:37Z" id="172575350">I am wondering that this should not be necessary: since we require that fields are mapped when indexing queries, then we would only get dynamic mapping updates for fields that are not used by queries and thus do not matter for matching?
</comment><comment author="martijnvg" created="2016-01-18T18:12:30Z" id="172610150">Agreed, we shouldn't worry about unused fields.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Client API should use String instead of Text</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15750</link><project id="" key="" /><description>For similar reasons to #15729, our client API should use the well-known String class where it today uses Text.
</description><key id="124761327">15750</key><summary>Client API should use String instead of Text</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2016-01-04T13:47:26Z</created><updated>2017-03-13T22:03:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Jordanwmk" created="2017-03-13T06:26:53Z" id="286024887">Hi, I am new to OSS but I would like to have a go at resolving this issue</comment><comment author="nik9000" created="2017-03-13T06:33:20Z" id="286025639">Since we're moving away from the `TransportClient` in general I don't think this is a good on. Honestly we should probably close this one in anticipation of https://github.com/elastic/elasticsearch/issues/23331</comment><comment author="nik9000" created="2017-03-13T06:33:28Z" id="286025657">@jpountz, what do you think about closing this?</comment><comment author="javanna" created="2017-03-13T22:03:13Z" id="286258002">I think that this is still valid given that the Java high level REST client is reusing the java api classes. Would be nice to not use anything that depends on lucene in there.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete documents with matching query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15749</link><project id="" key="" /><description>Hi, 

Prior to 2.0 I used to execute this kind of request to delete documents with specific content:

curl -XDELETE 'http://localhost:9200/logstash-*/_query?pretty' -d '
{
"query": { "match": { "Message": "My test message" } }
}'

Since ES 2.0, I followed the documentation and notice that DELETE has been moved to plugin, well, but the syntaxt seems the same as : 
https://www.elastic.co/guide/en/elasticsearch/plugins/2.0/delete-by-query-usage.html

But now i've got response like that:

No handler found for uri [/logstash-*/_query?pretty] and method [DELETE]

I've tried a lot of things, adding filters an so on, the GET is ok but can't suppress any documents.

Is something else to know about the new plugin or any bugs ?

Thanks a lot for help 
</description><key id="124755726">15749</key><summary>Delete documents with matching query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ddes</reporter><labels /><created>2016-01-04T13:02:29Z</created><updated>2016-01-04T13:28:28Z</updated><resolved>2016-01-04T13:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-04T13:20:33Z" id="168675045">Please ask your questions on discuss.elastic.co.

Did you install the plugin, restart all nodes?
</comment><comment author="ddes" created="2016-01-04T13:27:42Z" id="168676186">Plugin installed but nodes not restarted.

I'll ask on discuss.elastic.co. thanks,

I was just wondering if any known bugs on this.
</comment><comment author="dadoonet" created="2016-01-04T13:28:28Z" id="168676303">So yes you need to restart the nodes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait for new master when failing shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15748</link><project id="" key="" /><description>This commit handles the situation when we are failing a shard and either
no master is known, or the known master left while failing the shard. We
handle this situation by waiting for a new master to be reelected, and
then sending the shard failed request to the new master.

Relates #14252
</description><key id="124752777">15748</key><summary>Wait for new master when failing shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T12:36:58Z</created><updated>2016-02-16T16:45:34Z</updated><resolved>2016-01-17T15:50:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-04T12:37:58Z" id="168668777">Note that this pull request contains refactorings from #15735 and #15736 that I extracted into separate pull requests.
</comment><comment author="jasontedor" created="2016-01-07T02:29:43Z" id="169526636">@bleskes This is ready for review now.
</comment><comment author="bleskes" created="2016-01-08T14:06:02Z" id="170012015">Discussed with @jasontedor . Decided to try folding all _channel_ related retries (connection, master loss and any unknown exception) into the ShardStateAction , as opposed to dealing with that in TransportReplicationAction.
</comment><comment author="jasontedor" created="2016-01-08T22:52:37Z" id="170149816">@bleskes I pushed f49435c78b8a9574d54997492e5305840b610c57 to implement what we have discussed.
</comment><comment author="bleskes" created="2016-01-11T09:52:19Z" id="170492281">I think this looks great and is the right way to go. Left some comments. I'm doubtful if we need the timeout mechanism - it just adds complexity. Let's talk this one over.

Also - can we a open follow up issues to deal with shard started and remove the retry logics from IndicesClusterService ? (will make that class simpler).
</comment><comment author="jasontedor" created="2016-01-11T17:39:21Z" id="170628829">&gt; Also - can we a open follow up issues to deal with shard started

I opened #15895.

&gt; and remove the retry logics from IndicesClusterService ? (will make that class simpler).

I opened #15896.
</comment><comment author="jasontedor" created="2016-01-11T19:55:06Z" id="170670501">@bleskes I think this is ready for a final another review round.
</comment><comment author="jasontedor" created="2016-01-12T13:40:02Z" id="170914274">@bleskes I pushed more commits.
</comment><comment author="bleskes" created="2016-01-15T07:13:15Z" id="171888710">Thanks @jasontedor . Looks great. I left some comments w.r.t testing. A couple of things I miss in that area:

1) A simple test for success - without a retry/exceptions.
2) Validation of the message sent to the master - we learned from bitter experience that we should test that ... :)
</comment><comment author="jasontedor" created="2016-01-15T12:06:30Z" id="171944843">&gt; Validation of the message sent to the master - we learned from bitter experience that we should test that 

@bleskes Can you elaborate what you mean?
</comment><comment author="bleskes" created="2016-01-15T12:24:46Z" id="171947546">I meant that we should check the message is the right class and that it contains the shard routing we gave it.

On 15 jan. 2016 2:06 PM +0200, Jason Tedornotifications@github.com, wrote:

&gt; &gt; Validation of the message sent to the master - we learned from bitter experience that we should test that
&gt; 
&gt; @bleskes(https://github.com/bleskes)Can you elaborate what you mean? I have some commits locally I was planning for another PR (checking if the shard is in the routing table, checking if the shard is a replica not being failed by the primary, checking if the shard is a primary not being failed by the owning node) but want to be sure we're thinking the same thing.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/pull/15748#issuecomment-171944843).
</comment><comment author="jasontedor" created="2016-01-15T14:31:24Z" id="171975424">&gt; 1) A simple test for success - without a retry/exceptions.
&gt; 2) Validation of the message sent to the master - we learned from bitter experience that we should test that ... :)

@bleskes Pushed a new test in efb142613ffc38b30db81cf7b01e5c2d24f1baff.
</comment><comment author="jasontedor" created="2016-01-15T16:39:09Z" id="172010088">@bleskes I think this pull request is ready for another retry loop. ;)
</comment><comment author="bleskes" created="2016-01-17T07:36:53Z" id="172298523">LGTM. Left one little comment that doesn't need another cycle. Thanks @jasontedor !
</comment><comment author="jasontedor" created="2016-01-17T15:51:15Z" id="172343178">Thanks for another very thorough and helpful review @bleskes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make index type and id optional in simulate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15747</link><project id="" key="" /><description>Default values are _index, _type and _id.

Closes #15711
</description><key id="124749794">15747</key><summary>make index type and id optional in simulate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2016-01-04T12:18:49Z</created><updated>2016-01-04T12:25:22Z</updated><resolved>2016-01-04T12:25:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-01-04T12:22:51Z" id="168665464">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `missing` on terms aggs work with all execution modes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15746</link><project id="" key="" /><description>There are two bugs:
- the 'global_ordinals_low_cardinality' mode requires a fielddata-based impl so
  that it can extract the segment to global ordinal mapping
- the 'global_ordinals_hash' mode abusively casts to the values source to a
  fielddata-based impl while it is not needed

Closes #14882
</description><key id="124731475">15746</key><summary>Make `missing` on terms aggs work with all execution modes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>blocker</label><label>bug</label><label>review</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-01-04T10:21:54Z</created><updated>2016-01-06T08:32:42Z</updated><resolved>2016-01-06T08:32:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-01-05T16:11:11Z" id="169046605">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Misleading docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15745</link><project id="" key="" /><description>The docs are misleading as adding a document would look like this:

``` bash
curl -XPOST 'http://localhost:9200/index/pin?pretty' -d '
{
    "location" : {
         "lat" : 46.8,
         "lon" : -71.2
   }
}'
```

The submitted JSON does not contain the pin node.

Here is a stackoverflow question on the same topic: http://stackoverflow.com/a/21663582/2106834
</description><key id="124729630">15745</key><summary>Misleading docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">foaly-nr1</reporter><labels /><created>2016-01-04T10:10:58Z</created><updated>2016-01-10T17:16:11Z</updated><resolved>2016-01-10T17:16:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="foaly-nr1" created="2016-01-04T10:21:10Z" id="168633651">Just signed the CLA as well.
</comment><comment author="jpountz" created="2016-01-05T13:28:41Z" id="168997101">I guess the confusion comes from the fact that 'pin' is the name of an outer object  in the docs. Then maybe we should remove references to 'pin' entirely? (even in the mappings definitions)
</comment><comment author="foaly-nr1" created="2016-01-05T14:35:19Z" id="169017799">[Other parts of the docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html) show the request to the server for both mapping definitions and documents. That'd be less confusing and still show the full information.
</comment><comment author="clintongormley" created="2016-01-10T13:04:24Z" id="170343365">This has been fixed in the 2.0 docs https://www.elastic.co/guide/en/elasticsearch/reference/2.1/geo-point.html#geo-point
</comment><comment author="foaly-nr1" created="2016-01-10T17:16:11Z" id="170372059">Excellent thanks, @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_stats broken after closing index with alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15744</link><project id="" key="" /><description>If an aliased index is closed then the _stats endpoint returns an error

```
curl http://localhost:9200/_stats

{"error":{"root_cause":[{"type":"index_closed_exception","reason":"closed","index":"test_index"}],"type":"index_closed_exception","reason":"closed","index":"test_index"},"status":403}
```

This does not happen if the alias is removed first. I think that this should not error, it should just exclude the closed index from the stats as normal?
</description><key id="124720953">15744</key><summary>/_stats broken after closing index with alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brainsiq</reporter><labels><label>:Aliases</label><label>:Stats</label></labels><created>2016-01-04T09:18:41Z</created><updated>2016-01-13T10:29:13Z</updated><resolved>2016-01-13T10:29:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T17:27:54Z" id="170372902">Related to #15730 and #14982 
</comment><comment author="javanna" created="2016-01-13T10:29:13Z" id="171247899">Same as described in https://github.com/elastic/elasticsearch/issues/15730#issuecomment-171243705 .

The exception returned by `GET _stats` has been resolved by #15882, given that not specifying indices is considered a wildcard expressions. `GET _stats?expand_wildcards=open,closed` does return an exception as the `expand_wildcards` option is honoured.

If you do specify an alias that points to some closed index, you get back an exception by default: `GET alias/_stats` but `ignore_unavailable` allows to control whether closed indices should be ignored or not: `GET alias/_stats?ignore_unavailable=true` doesn't return exception.

I think we can close this one, the actual bug was same as #13278 which was fixed by #15882 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Labelling in Kibana charts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15743</link><project id="" key="" /><description>There should be an option to label the metrics being applied to the pie charts, bar charts, data table, etc., so that the labels appear in the visualisations instead of the entire metric operation on a particular field. 
</description><key id="124714142">15743</key><summary>Labelling in Kibana charts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SukritiMehrotra</reporter><labels /><created>2016-01-04T08:19:40Z</created><updated>2016-01-04T08:26:32Z</updated><resolved>2016-01-04T08:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-01-04T08:26:32Z" id="168609310">I think you meant opening that feature request in https://github.com/elastic/kibana/issues ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>is it possible to patch index.max_result_window config and feature into 1.7?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15742</link><project id="" key="" /><description>it's really helpful for es memory issue, so I'd like to ask if it is possible to patch index.max_result_window config and feature into 1.7
</description><key id="124709402">15742</key><summary>is it possible to patch index.max_result_window config and feature into 1.7?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-01-04T07:23:57Z</created><updated>2016-01-05T01:31:23Z</updated><resolved>2016-01-04T13:57:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-04T13:57:00Z" id="168682820">I'm not sure there will be another release of 1.7 and if there is I'm not sure its a good idea to add something like this to it. I understand your motivation though! I just don't think it'd be ok to add something that'd break existing behavior to a patch level release. I'm going to close this with that reasoning. Sorry!
</comment><comment author="makeyang" created="2016-01-05T01:31:23Z" id="168863857">fully understand. thanks for reply.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"match"  with "query" parameter accepts list input and gives wrong results </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15741</link><project id="" key="" /><description>There is an inconsistency in `match` query in simple form and with `query` parameter. The latter gives an erroneous result with a list input whereas the former refuses the list input with parse error.

```
GET /.../.../_search
{
    "query": {
        "match": {
            "skills": ["python", "ruby"]
        }
    }
}
```

results an parse error, as expected.

```
GET /.../.../_search
{
    "query": {
        "match": {
            "skills": {
                "query": ["python", "ruby"]
            }
        }
    }
}
```

gives the same output as with `"query": ["xxx","yyy", "ruby"]` or `"query": ["ruby"]`, considering ONLY the last item of the list and  ignoring the rest.
</description><key id="124697611">15741</key><summary>"match"  with "query" parameter accepts list input and gives wrong results </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">asldevi</reporter><labels><label>:Query DSL</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-01-04T05:11:51Z</created><updated>2016-09-13T11:20:14Z</updated><resolved>2016-09-13T11:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T17:25:07Z" id="170372694">This is broke in 2.2, but may already be fixed in master?
</comment><comment author="czak" created="2016-01-25T20:10:55Z" id="174645562">I can confirm the above incorrect behavior on both 2.1.1 and 2.2.

On current `master` (4c1e93bd89c), both forms raise errors, albeit slightly differently:

#### Shorthand form

``` sh
curl -X POST 'http://localhost:9200/company/employee/_search?pretty=true' -d '
{
    "query": {
        "match": {
            "skills": ["python", "ruby"]
        }
    }
}'
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_state_exception",
      "reason" : "Can't get text on a START_ARRAY at 5:13"
    } ],
    "type" : "illegal_state_exception",
    "reason" : "Can't get text on a START_ARRAY at 5:13"
  },
  "status" : 500
}
```

#### With "query" param

``` sh
curl -X POST 'http://localhost:9200/company/employee/_search?pretty=true' -d '
{
    "query": {
        "match": {
            "skills": {
                "query": ["python", "ruby"]
            }
        }
    }
}'
{
  "error" : {
    "root_cause" : [ {
      "type" : "parsing_exception",
      "reason" : "[match] unknown token [START_ARRAY] after [query]",
      "line" : 6,
      "col" : 17
    } ],
    "type" : "parsing_exception",
    "reason" : "[match] unknown token [START_ARRAY] after [query]",
    "line" : 6,
    "col" : 17
  },
  "status" : 400
}
```
</comment><comment author="Hazy-Sunshine" created="2016-08-04T01:47:12Z" id="237429352">you should use terms to match mutiply items.
GET /.../.../_search
{
    "query": {
        "terms": {
            "skills":  ["python", "ruby"] 
        }
    }
}
</comment><comment author="javanna" created="2016-09-13T11:20:14Z" id="246650834">fixed in 2.4 branch with https://github.com/elastic/elasticsearch/commit/6e5260d2c493a1461f9180b5004eceddce1e89c9.

Also added specific tests in master: https://github.com/elastic/elasticsearch/commit/7894eba2b3fd17bd035b91ec02c1205ef738d6e0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix syntax in the request params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15740</link><project id="" key="" /><description>add a missing double quote
</description><key id="124696616">15740</key><summary>fix syntax in the request params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asldevi</reporter><labels><label>docs</label></labels><created>2016-01-04T04:54:17Z</created><updated>2016-01-05T13:29:53Z</updated><resolved>2016-01-05T13:29:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add double quotation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15739</link><project id="" key="" /><description>#15737

thank you.
</description><key id="124690133">15739</key><summary>add double quotation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">omiend</reporter><labels><label>docs</label></labels><created>2016-01-04T03:20:30Z</created><updated>2016-01-05T13:21:51Z</updated><resolved>2016-01-05T13:21:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add double quotation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15738</link><project id="" key="" /><description>#15737

thank you.
</description><key id="124689946">15738</key><summary>add double quotation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">omiend</reporter><labels /><created>2016-01-04T03:16:56Z</created><updated>2016-01-04T03:19:29Z</updated><resolved>2016-01-04T03:19:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>should add double quotation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15737</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/agg-metadata.html
![image](https://cloud.githubusercontent.com/assets/4135267/12082895/f1d07e7a-b2dc-11e5-934a-e9262bc63bac.png)

elasticsearch/docs/reference/aggregations/misc.asciidoc

`== Aggregation Metadata`

Before

```
[source,js]
--------------------------------------------------
{
    ...
    aggs": {
        "titles": {
            "terms": {
                "field": "title"
            },
            "meta": {
                "color": "blue"
            },
        }
    }
}
--------------------------------------------------
```

After

```
[source,js]
--------------------------------------------------
{
    ...
    "aggs": {
        "titles": {
            "terms": {
                "field": "title"
            },
            "meta": {
                "color": "blue"
            },
        }
    }
}
--------------------------------------------------
```
</description><key id="124689923">15737</key><summary>should add double quotation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">omiend</reporter><labels><label>docs</label></labels><created>2016-01-04T03:16:29Z</created><updated>2016-01-08T06:24:32Z</updated><resolved>2016-01-06T17:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-06T17:48:26Z" id="169401785">Fixed via #15739
</comment><comment author="omiend" created="2016-01-08T06:24:32Z" id="169909025">thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make cluster state external to o.e.c.a.s.ShardStateAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15736</link><project id="" key="" /><description>This commit modifies the handling of cluster states in
o.e.c.a.s.ShardStateAction so that all necessary state is obtained
externally to the ShardStateAction#shardFailed and
ShardStateAction#shardStarted methods. This refactoring permits the
removal of the ClusterService field from ShardStateAction.
</description><key id="124679533">15736</key><summary>Make cluster state external to o.e.c.a.s.ShardStateAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-01-03T23:51:49Z</created><updated>2016-01-04T21:22:17Z</updated><resolved>2016-01-04T21:22:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-03T23:58:21Z" id="168556380">The primary motivator for this change is ongoing work for the next step of #14252 (handling when we send a shard failure to a node that we thought was the master but is no longer the master).
</comment><comment author="nik9000" created="2016-01-04T14:09:33Z" id="168686270">Makes sense to me but I suspect @bleskes will give final review.
</comment><comment author="bleskes" created="2016-01-04T15:40:13Z" id="168709927">Left some very minor comments. All in the spirit of clean ups - which is what this Pr seems to be about...
</comment><comment author="jasontedor" created="2016-01-04T16:27:11Z" id="168723949">@bleskes Thanks for the comments and additional suggestions. I've pushed more commits to address.
</comment><comment author="bleskes" created="2016-01-04T21:01:25Z" id="168807502">LGTM. Left one minor naming suggestion, no need for another review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor master node change predicate for reuse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15735</link><project id="" key="" /><description>This commit migrates a ClusterStateObserver.ChangePredicate for
detecting a master node change into a separate class for reuse
elsewhere.
</description><key id="124661673">15735</key><summary>Refactor master node change predicate for reuse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-01-03T17:51:08Z</created><updated>2016-01-06T18:58:20Z</updated><resolved>2016-01-06T18:58:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-03T17:52:07Z" id="168523419">This predicate will be reused in the next step of #14252 (handling when we send a shard failure to a node that we thought was the master but is no longer the master).
</comment><comment author="bleskes" created="2016-01-06T13:06:32Z" id="169319397">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logfile being spammed with "NoSuchFileException"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15734</link><project id="" key="" /><description>As I was just trying to access my ElasticSearch instance, I got prompted with several 503 errors. After accessing the server and trying to check the logs, I saw that my logfile was 1.8 GB big and consisted mainly of errors such as the following:

```
[2016-01-02 13:56:18,601][WARN ][cluster.action.shard     ] [Brynocki] [admin_dashboard][0] received shard failed for [admin_dashboard][0], node[hIMImTGASSmIHX_bUAVa8w], [P], v[39], s[INITIALIZING], a[id=c51WUYPuSIOiSe2Pox-SHA], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-01-02T12:56:15.006Z], details[failed recovery, failure IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833]; ]], indexUUID [XE89emCJQeOLqRItz7LAgw], message [failed recovery], failure [IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833]; ]
[admin_dashboard][[admin_dashboard][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833];
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:254)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: [admin_dashboard][[admin_dashboard][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833];
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:178)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        ... 5 more
Caused by: [admin_dashboard][[admin_dashboard][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833];
        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:175)
        ... 11 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833];
        at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1636)
        at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:299)
        at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)
        at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
        at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
        ... 12 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0xc1ef600a, got: 0x30373833]
        at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1593)
        at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1626)
        ... 17 more
[2016-01-02 13:56:18,813][WARN ][index.translog           ] [Brynocki] [admin_dashboard][0] failed to delete temp file /var/lib/elasticsearch/onlyonce_statistics/nodes/0/indices/admin_dashboard/0/translog/translog-3301628599119632219.tlog
java.nio.file.NoSuchFileException: /var/lib/elasticsearch/onlyonce_statistics/nodes/0/indices/admin_dashboard/0/translog/translog-3301628599119632219.tlog
        at sun.nio.fs.UnixException.translateToIOException(Unknown Source)
        at sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)
        at sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(Unknown Source)
        at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
        at java.nio.file.Files.delete(Unknown Source)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
```

I can't seem to find out how to recover from this problem
</description><key id="124589751">15734</key><summary>Logfile being spammed with "NoSuchFileException"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Cyberuben</reporter><labels><label>:Translog</label></labels><created>2016-01-02T12:59:16Z</created><updated>2016-01-10T19:12:09Z</updated><resolved>2016-01-10T19:12:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-03T20:12:00Z" id="168537323">What version are you running? I'm thinking it could be #14695
</comment><comment author="Cyberuben" created="2016-01-03T20:31:29Z" id="168538847">That'd be the following:

```
"version": {
    "number": "2.1.0",
    "build_hash": "72cd1f1a3eee09505e036106146dc1949dc5dc87",
    "build_timestamp": "2015-11-18T22:40:03Z",
    "build_snapshot": false,
    "lucene_version": "5.3.1"
  }
```

I shouldn't forget to mention that the issue was "resolved" by deleting my index, but I don't think this is the appropriate solution. I'm only working with test data so deleting and recreating my index solved it for me.
</comment><comment author="jpountz" created="2016-01-03T20:48:49Z" id="168539920">The "failed to delete temp file" part would be addressed by #14872 I think. However the other messages look to me like there is a genuine corruption of the translog file. @s1monw does it ring a bell to you?
</comment><comment author="clintongormley" created="2016-01-10T17:22:49Z" id="170372574">Did you have a disk full event? I think it is related to https://github.com/elastic/elasticsearch/issues/15333
</comment><comment author="Cyberuben" created="2016-01-10T17:36:36Z" id="170373390">The disk was full after this spammed my log file, yes, but I don't think a full disk was the cause. I had roughly 16GB of log files, and I only had 20GB diskspace.
</comment><comment author="s1monw" created="2016-01-10T19:12:03Z" id="170382735">This was fixed in 2.1.1 as clint said https://github.com/elastic/elasticsearch/issues/15333 is the relevant issue. The log spamming is also fixed  #14872 - I will close it now since all issues have been resolved in 2.1.1 and 2.0.2 @Cyberuben thanks for reporting this it's super important to get as much info as possible if stuff like this happens.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Faster terms filter when cached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15733</link><project id="" key="" /><description>Our ES queries using terms lookups with many security labels are slow.  In theory query performance (with a cached filter) shouldn't vary with the number of looked-up terms, especially since in our tests we fill the terms with bogus terms that don't match any documents.  (So the resulting bitset should be identical to the one generated from the original terms list.)

The slowdown results from incidental / accidental bits in the code.  The parsing code implements the terms lookup.  It basically works as follows:
(1) Fetch the terms by lookup or from cache.
(2) Instantiate the TermsFilter.
(3) Fetch the cached filter (if cached, otherwise use the filter from step (2)).

(The query execution code is not an issue.)

In theory steps (1) and (3) should be instantaneous when cached, which turns out to be true for (3) and mostly true for (1).  Of course, step (2) is totally redundant when step (3) is cached.

Is step (2) fast?  It turns out not.  The TermsFilter constructor turns out to be horribly slow when the terms array is large.  It copies the array, sorts it, and iterates over it.  There are some other copies that happen outside the constructor, etc.  So as mentioned we can eliminate step (2) entirely.  Or really, defer it until step (3) gets a cache miss.

Eliminating step (2) fixes 90% of the problem (makes the query say 5 - 10x faster).  Then it turns out there's another copy in step (1) that we can also eliminate, which makes the query another 5 - 10x faster.  I'll re-run some numbers locally but if I recall correctly I had a 13-second query with 200k lookups down to under 100 ms.

Two other related items:
- The terms lookup cache needs to be bigger say 4 gb.  There's another big slowdown without that.
- The developer can specify a cache key to the terms filter (to any filter actually).  The terms lookups are cached using the same key.  This leads to redundant caching of the lookups because the lookups should be keyed by the lookup parameters (lookup table, type, id, and field).  But the terms filter itself should generally be keyed by the field being filtered on + the lookup parameters.  If you are filtering against 50 different fields using the same lookup, as we do, then you end up with 50 copies of the lookup in the lookup cache.  We should use the default lookup cache key (which is just the lookup options) instead, allowing the lookup cache to cache 50x more lookups.  That's also in this PR.
</description><key id="124519478">15733</key><summary>Faster terms filter when cached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tksfz</reporter><labels /><created>2016-01-01T00:11:18Z</created><updated>2016-01-10T17:39:21Z</updated><resolved>2016-01-10T17:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tksfz" created="2016-01-01T00:13:20Z" id="168265579">It goes from ~9200ms to ~55ms.
</comment><comment author="clintongormley" created="2016-01-10T17:39:21Z" id="170373534">Hi @tksfz 

Thanks for the PR but queries/filters have changed completely in 2.x.  There are no more filters, there are no cache keys etc.  Query caching is now handled by Lucene rather than Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature/deferred terms filter 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15732</link><project id="" key="" /><description>Our ES queries using terms lookups with many security labels are slow.  In theory query performance (with a cached filter) shouldn't vary with the number of looked-up terms, especially since in our tests we fill the terms with bogus terms that don't match any documents.  (So the resulting bitset should be identical to the one generated from the original terms list.)

The slowdown results from incidental / accidental bits in the code.  The parsing code implements the terms lookup.  It basically works as follows:
(1) Fetch the terms by lookup or from cache.
(2) Instantiate the TermsFilter.
(3) Fetch the cached filter (if cached, otherwise use the filter from step (2)).

(The query execution code is not an issue.)

In theory steps (1) and (3) should be instantaneous when cached, which turns out to be true for (3) and mostly true for (1).  Of course, step (2) is totally redundant when step (3) is cached.

Is step (2) fast?  It turns out not.  The TermsFilter constructor turns out to be horribly slow when the terms array is large.  It copies the array, sorts it, and iterates over it.  There are some other copies that happen outside the constructor, etc.  So as mentioned we can eliminate step (2) entirely.  Or really, defer it until step (3) gets a cache miss.

Eliminating step (2) fixes 90% of the problem (makes the query say 5 - 10x faster).  Then it turns out there's another copy in step (1) that we can also eliminate, which makes the query another 5 - 10x faster.  I'll re-run some numbers locally but if I recall correctly I had a 13-second query with 200k lookups down to under 100 ms.

Two other related items:
- The terms lookup cache needs to be bigger say 4 gb.  There's another big slowdown without that.
- The developer can specify a cache key to the terms filter (to any filter actually).  The terms lookups are cached using the same key.  This leads to redundant caching of the lookups because the lookups should be keyed by the lookup parameters (lookup table, type, id, and field).  But the terms filter itself should generally be keyed by the field being filtered on + the lookup parameters.  If you are filtering against 50 different fields using the same lookup, as we do, then you end up with 50 copies of the lookup in the lookup cache.  We should use the default lookup cache key (which is just the lookup options) instead, allowing the lookup cache to cache 50x more lookups.  That's also in this PR.
</description><key id="124519337">15732</key><summary>Feature/deferred terms filter 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tksfz</reporter><labels /><created>2016-01-01T00:10:27Z</created><updated>2016-01-01T00:10:51Z</updated><resolved>2016-01-01T00:10:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Extend the Avg Aggregation to support weighted average</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15731</link><project id="" key="" /><description>The current Avg Aggretation only supports the simple form of average (ie: sum(values) / num of values). In order to support weighted average (ie: sum( value \* weight) / sum (weight)) in an efficient way, I suggest the following changes in the API.

**First implementation without modification of the existing API**
The script property in the avg object can return a number of a list of numbers. My proposal is to also support a map containing 2 fixed properties: value and weight (or values and weights if multiple values) (. For example:

``` javascript
{"avg" : { "script" : "[value: doc['latency'].value, weight: doc['transaction'].value]" }}
```

I prototyped this solution with only 2 modifications. A first modification in the ScriptDoubleValues class to take into account this map. A second modification in the AvgAggregator class to apply the right formula accordingly to the context.

**Second implementation requiring minor modifications of the existing API (more efficient because no script)**
We can add a new property 'weight' to define the field to use in the document.

``` javascript
{ "avg" : { "field" : "latency",  "weight" : "transaction"} } 
```

I didn't prototype this solution because I'm still not sure which DoubleValues implementation to use to get efficiently the value and the weight. If you have some advices don't hesitate.

A third option is to use scripted metrics but until now it's not possible to sort the result of scripted metrics so that doesn't work for me. I created a pull request to add this support but I'm still waiting approval. But anyway a solution based on scripted metrics will be less efficient. 

This proposal should be easy to implement and I'm pretty sure that many users will be happy with this extended version of the avg aggregation. 
</description><key id="124513482">15731</key><summary>Extend the Avg Aggregation to support weighted average</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lquerel</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-12-31T23:00:04Z</created><updated>2017-07-28T20:24:08Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-03T20:16:23Z" id="168537639">Actually this is typically the kind of use-case for which the `avg` aggregation supports scripts. Any reason why you're not happy with scripts?
</comment><comment author="lquerel" created="2016-01-03T20:56:22Z" id="168540292">The weighted average is equals to sum(value-i \* weight-i) / sum(weight-i). Value and Weight are dynamic values extracted from a set of documents. Currently the script only returns the value for each document so I don't see any mathematical way to compute easily a weighted average without the weight for each document knowing that currently the result of the script is summed and divided at the end by the number of values (and not the sum of weights). 
</comment><comment author="lquerel" created="2016-01-05T23:55:51Z" id="169172672">I sent a pull request with the implementation of the first approach based on the script attribute.

https://github.com/elastic/elasticsearch/pull/15781

For info, this pull request solves this issue too: https://github.com/elastic/elasticsearch/issues/8486
</comment><comment author="jpountz" created="2016-01-06T22:03:54Z" id="169476965">&gt; The weighted average is equals to sum(value-i \* weight-i) / sum(weight-i). Value and Weight are dynamic values extracted from a set of documents. Currently the script only returns the value for each document so I don't see any mathematical way to compute easily a weighted average without the weight for each document knowing that currently the result of the script is summed and divided at the end by the number of values (and not the sum of weights).

Right, this involves some client-side logic, something like (not tested):

```
{
  "aggs": {
    "weighted_sum": {
      "sum": {
        "script": {
          "inline": "doc['value'].value * doc['weight'].value"
        }
      }
    },
    "sum_of_weights": {
      "sum": {
        "field": "weight"
        }
      }
    }
  }
}
```

And then you would need to divide the `weighted_sum` value by `sum_of_weights`. I can understand that this is less convenient that that eg you can't use it with kibana so I'm open to discussing adding a way to do weighted averages easily. But I would like to make sure there is enough demand for it before (so that the feature is worth maintaining) and have a discussion about what to do if either the source of values or weights is missing in a document or multi-valued. If we can't come with a nice API then I'd rather not implement the feature. I'll mark this issue as `discuss` so that others chime in.
</comment><comment author="lquerel" created="2016-01-06T23:43:18Z" id="169499607">Thank for your response.

With this approach, how can you order the result of the weighted average? With my proposal it's done directly by the coordinator node with the following query.

```
{
  "aggs": {
    "myAggr": {
      "terms": {
        "field": "dimension",
        "order": { "weightedAgv": "desc"}
      },  
      "aggs": {
          "weightedAvg": {"avg" : { "script" : "[value: doc['latency'].value, weight: doc['transaction'].value]" }}
       }
    }
}
```

Except if I'm wrong, but with your approach I need to sort client side and means potentially loading a lot of data. Right ? 
IMO it's a second good reason (with kibana) to support this feature natively. I will be very happy to help if needed. 

Multi-valued values/weights are implemented in my pull request as follow:

**Standard average**

```
{ "avg" : { "script" : "doc['latency'].value" }} } 
```

By default the weight is equals to 1.

**Weighted average (single-valued)**

```
{ "avg" : { "script" : "[value: doc['latency'].value, weight: doc['transaction'].value]" }}} 
```

**Weighted average (multi-valued)**

```
{ "avg" : { "script" : "[values: doc['latency'].value, weights: doc['transaction'].value]" }}} 
```

Weights is optional, if not provided then by default weight=1 for each value.
If weights.size() &lt; values.size() then by default weight=1 for each value without corresponding weight.
Otherwise we use the corresponding weight.

General remark: when the weight field is missing in the original doc then by default weight=1.

For the second approach exposed in my initial proposal, the idea is the same. In the current API the property 'field' is used to identify the value or the list of values used to compute the average. I suggest to introduce an optional property 'weight' to identify the value of the list of values used to determine all weights.

Is it good/general enough to implement the feature? Thanks in advance for your response.
</comment><comment author="elephantum" created="2016-02-19T21:52:26Z" id="186425342">+1 for any reasonable solution. Weighted average is a common case in any business-related dashboard. 
</comment><comment author="jpountz" created="2016-04-06T17:58:45Z" id="206489094">Thinking about it a bit more: the only reason to not do it on client side is to be able to order by sub aggregation. However sorting by sub aggregation has the drawback of making counts very inaccurate, I am actually more thinking of deprecating the ability to sort by sub aggregations and by asending count because of that. So I don't think we should add a weighted average aggregation (or add weighting support to the avg agg) just to be able to sort by it.

&gt; If weights.size() &lt; values.size() then by default weight=1 for each value without corresponding weight.

This won't work since elasticsearch does not maintain the order of values in fielddata: they always come in ascending order. So if your original document has `{value: [5, 1], weight: [2,3]}`, we will actually use a weight of 2 for 1 and 3 for 5.
</comment><comment author="jpountz" created="2016-06-24T10:08:01Z" id="228308158">We just discussed it in Fixit Friday. We like the feature but would like to verify that there is traction for this feature before implementating it. Also we need to figure out a clear API, especially about what happens with multi-valued fields or missing values.
</comment><comment author="lquerel" created="2016-06-24T16:35:15Z" id="228394899">Good news.

Proposal: add a new aggregation to compute weighted average.

```
{ 
    "weigthed-avg" : { 
        "field-value" : string,      [mandatory]
        "field-weight": string,     [mandatory]
        "missing-value": int,      [optional, default 0]
        "missing-weight": int     [optional, default 1]
    } 
}
```

The management of multi-values fields was already described before in this thread.

Weighted average computed as: sum(field-value[i] \* field-weight[i]) / sum(field-weight[i])
</comment><comment author="kylevarisco" created="2016-11-03T21:03:15Z" id="258273101">I'd love to see this in ElasticSearch.  In our business (commodity trading/marketing/logistics), weighted averages are the main metric that we use - weighted average sales prices, weighted average transport costs, etc.  I think this would be a very useful addition - currently we are having to calculate sums in Elastic, and then pull those results to a middle tier where the weighted averages are calculated.
</comment><comment author="smartlocating" created="2016-11-20T14:30:59Z" id="261781611">@lquerel @jpountz what is your current recommendation for getting a single-value weighted average? 
</comment><comment author="hilalgur" created="2016-12-05T11:54:21Z" id="264836065">Hi,
I have same problem. I want to weighted average on ES-5.0.0. I did something:
```
GET titubb56k/ihale/_search 
{
  "aggs": {
    "myAggr": {
      "terms": {
        "field": "UrunNo",
        "order": { "weightedAvg": "desc"}
      },  
      "aggs": {
          "weightedAvg": { "avg" : { "script" : "[values: doc['BirimFiyat'].value, weights: doc['Adet'].value]" }}
       }    }  }  }
```
and I have error:

```
{
  "error": {
    "root_cause": [
      {
        "type": "parsing_exception",
        "reason": "Unexpected token VALUE_STRING [script] in [weightedAvg].",
        "line": 9,
        "col": 49
      }
    ],
    "type": "parsing_exception",
    "reason": "Unexpected token VALUE_STRING [script] in [weightedAvg].",
    "line": 9,
    "col": 49
  },
  "status": 400
}
```
What is the problem? Or weighted average is possible on ES-5.0.0</comment><comment author="hilalgur" created="2016-12-07T10:48:04Z" id="265416008">I did,
```

POST ortalama/avg/_search
{
  "size": 3,
    "query": {
    "term": {
      "IdareKodu":  "4"
      }
    },

  "aggs": {
    "gunluk_satis": {
      "terms": {
        "field": "IdareKodu"
      },
      "aggs": {
        "pay": {
          "sum": {
            "script": {
              "lang": "expression",
              "inline": "doc['BirimFiyat'] * doc['Adet']"
            }
          }
        },
       
        "payda": {
          "sum": {
            "script": {
              "lang": "expression",
              "inline": "doc['Adet']"
            }
          }
        },
        "sonuc": {
                    "bucket_script": {
                        "buckets_path": {
                          "Pay": "pay",
                          "Payda": "payda"
                        },
                        "script": "params.Pay / params.Payda"
                    }
                }
      }
    }
  }
}

```

That worked</comment><comment author="lquerel" created="2017-03-09T07:05:51Z" id="285272459">@jpountz what is the conclusion?</comment><comment author="mandarincreative" created="2017-05-01T17:04:54Z" id="298375248">Any update on a native-ish weighted Average?
</comment><comment author="darrencruse" created="2017-07-28T20:24:08Z" id="318753341">Just a note that I'm disappointed this hasn't made it in - I'm working for an agriculture company and it's common to do calculations that are weighted by the area of farmers fields.

i.e. the search is matching attributes in the indexed documents (representing fields) to select a subset of all possible fields, and the field documents have other values to average, along with the area of each field.  So the goal is to do averages on the values weighting them more for bigger fields and less for smaller fields, with different searches used for the selection (might be region of the country, type of crop being grown etc. etc).

(giving the detail JIC there's still a thought this isn't a common need - maybe I'm biased because of my problem but I think this really *is* a common need, and also hoping feedback if people think this can be done client side - I've racked my brain about this and I don't see how it can be?) 

Anyway I'd gotten a little excited that I'd landed on the "bucket_script" solution like @hilalgur showed above.

Then disappointed again when it seems there no way to get a Kibana visualization to use a query that needs a "bucket_script" (seems it only supports plain "script" and it seems to *have* to generate the ES query itself it doesn't seem possible to completely tell it the ES query you want it to use AFAICT).

atm I'm thinking we might have to ditch Kibana and go with doing charts ourself atop the ES queries which use "bucket_script".   Seems ashame though Kibana seems pretty nice in a lot of ways otherwise.

 

</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query processing against aliases with closed indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15730</link><project id="" key="" /><description>I am separating out the 2 functionalities being discussed as part of #14982 with the creation of this new ticket.  For #14982, we will focus on the GET _alias and _cat/aliases reporting of index/alias association.

For this ticket, the focus is on the error message the user will receive if a query is sent to an alias that has a reference to a closed index.

https://github.com/elastic/elasticsearch/issues/14982#issuecomment-165981661

&gt; There has been a suggestion to remove any aliases automatically when closing an index, but I think this is not the correct behaviour. If a user wants this, then they can simply DELETE {index}/_alias/\* before closing the index.

Per the feedback in the other ticket, it sounds like the current behavior is expected, i.e. if any one of the indices referenced by an alias is closed, all queries to the alias will return a 403 error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "index_closed_exception",
            "reason": "closed",
            "index": "test"
         }
      ],
      "type": "index_closed_exception",
      "reason": "closed",
      "index": "test"
   },
   "status": 403
}
```

Feedback we received from the field is that in order to workaround the above, the admins have to always remember to remove the to-be-closed indices from the aliases prior to closing. Otherwise, this can create an outage when end users attempt to query against the alias (instead of getting back results from the indices that are not closed, they get an immediate error message).

Since the current behavior is by design, I am creating this request as an enhancement to discuss if this is something we can automate for the end user (eg. maybe have the close api remove the index from all aliases), or if we can provide a configurable option for users to determine if they want aliases with closed indices to return a 403, or allow queries against the open indices to function and return results, etc..
</description><key id="124496478">15730</key><summary>Query processing against aliases with closed indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Aliases</label><label>enhancement</label></labels><created>2015-12-31T19:12:11Z</created><updated>2016-02-23T23:35:22Z</updated><resolved>2016-01-13T11:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="allthedrones" created="2016-01-05T15:11:20Z" id="169029611">Given the prevalence of advice to use aliases as an abstraction to prevent client applications from being tightly coupled to indexing strategy, I would advocate for at least having the _option_ for a low-drag experience in which closed members of an alias are automatically bypassed. I agree with feedback that it would be incorrect to transparently remove indices from an alias upon close unless they were guaranteed to be transparently restored when re-opened. In every other sense, closing an index seems to represent a state of suspended animation, and the strongly established expectation is that it will be returned to its previous state upon open.

If [optionally] silently bypassing closed indices within an alias is not feasible, it would be nice if the close (and open) API allowed the specification of options to condense the operations: specifying the removal of aliases at close, and re-enrollment into aliases at open. I can see this being useful for a few other settings as well (for instance, I often `open` an index and immediately change the replica count ... would be really slick to specify that as part of the open call). 

Still, as a matter of first preference, if a new user takes the common advice to point their client app(s) at aliases, which are billed as a transcendent/aggregate layer above indices, I think it's potentially confusing &amp; frustrating if changing a single component index results in an outage for the entire alias.
</comment><comment author="clintongormley" created="2016-01-10T13:40:13Z" id="170345039">@javanna makes me wonder if we should have an `expand_aliases` option https://www.elastic.co/guide/en/elasticsearch/reference/2.1/multi-index.html
</comment><comment author="javanna" created="2016-01-13T09:49:38Z" id="171234808">@clintongormley if I read correctly #13278 and the corresponding fix (#15882), we are now checking the state of the indexes after alias resolution and honouring the current indices options, which were not previously applied to aliases. expland_wildcards is now used to expand aliases too. It seems like there is no need for a new option but maybe we need to rename the option as it's getting applied to aliases too. Relates to #9438 at this point. 
</comment><comment author="javanna" created="2016-01-13T10:16:05Z" id="171242111">&gt; we are now checking the state of the indexes after alias resolution and honouring the current indices options

I take this back, I had misunderstood, we check the index state if the alias matches a wildcard expression, not when the alias is explicitly specified. I find this confusing, digging a bit more to find out what we can do about it.
</comment><comment author="javanna" created="2016-01-13T10:21:01Z" id="171243705">`GET /alia*/_search` will match the alias "alias" and resolve by default to the open indices only. It honours `expand_wildcards`.

`GET /alias/_search` will resolve to open and closed indices by default and return an exception, but ignore_unavailable allows to control whether unavailable (closed too) indices should be ignored.

`GET /alias/_search?ignore_unavailable=true` resolves to the open indices only.

This behaviour doesn't seem that bad to me. Maybe the default for search should be to ignore unavailable? That would be a breaking change though.
</comment><comment author="clintongormley" created="2016-01-13T11:29:27Z" id="171260579">Thanks @javanna . I'd forgotten about `ignore_unavailable`.  I don't think this should be the default though.  If a user searches against an alias with closed indices, they should be made aware of it, rather than the situation being silently ignored.

I think the workaround is already in place, and there is nothing more to do here.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Client API should use java collections APIs instead of HPPC's</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15729</link><project id="" key="" /><description>Follow-up of https://github.com/elastic/elasticsearch/pull/15727: consumers of the Java client API have to get familiar with hppc (and its cursors for instance) while the same information could be exposed with regular Java collections.
</description><key id="124474539">15729</key><summary>Client API should use java collections APIs instead of HPPC's</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-31T14:12:03Z</created><updated>2016-11-28T09:09:18Z</updated><resolved>2016-11-28T07:56:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-31T14:15:41Z" id="168202316">++
</comment><comment author="nik9000" created="2016-01-04T13:49:16Z" id="168679926">I wonder if its important that the actual `Client` interface in Elasticsearch do this. There ought to be a REST based java client and **it** certainly should do this.
</comment><comment author="jpountz" created="2016-01-04T13:55:06Z" id="168682042">Yeah when I write "client API" this should read "what users use to communicate with elasticsearch". Which is the Client interface today, but may well change in the future. It is totally fine to use HPPC in internal code when appropriate.
</comment><comment author="javanna" created="2016-11-28T07:56:05Z" id="263205243">I think we can close this as we are developing the Java high level REST client. There we won't have any hppc dependency. The java api will become internal as soon as the high level REST client is mature enough to replace it.</comment><comment author="jpountz" created="2016-11-28T09:09:18Z" id="263218418">Hurray!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return an aggregated view of all mappings/properties of all types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15728</link><project id="" key="" /><description>Currently we can retrieve all mappings and properties of all types with `_cluster/state` but the response size is proportional to the number of indices.

Most of the time (and I think this is mandatory in Elastcicsearch 2.x) field mappings are consistent between types, so we should be able to create an aggregated view that way the response size will be proportional to the number of properties and types (but not to the number of indices).

.API proposal to retrieve an aggregated view of all mappings

```
http://localhost:9200/_cluster/state/metadata?level=mappings
```

``` json
{
  "cluster_name": "superheroes",
  "metadata": {
    "mappings": {
      "hadoop-hdfs": {
        "_source": {},
        "dynamic_templates": [],
        "_size": {},
        "properties": {},
        "_all": {}
      },
      "jboss-boot": {
        "_source": {},
        "dynamic_templates": [],
        "_size": {},
        "properties": {},
        "_all": {}
      },
      "haproxy-error": {
        "_source": {},
        "dynamic_templates": [],
        "_size": {},
        "properties": {},
        "_all": {}
      },
      "haproxy-info": {
        "_source": {},
        "dynamic_templates": [],
        "_size": {},
        "properties": {},
        "_all": {}
      }
    }
  }
}
```

Following the same principle, here is the query to create an aggregated view of all properties:

.API proposal to retrieve an aggregated view of all properties

```
http://localhost:9200/_cluster/state/metadata?level=properties
```

``` json
{
  "cluster_name": "superheroes",
  "metadata": {
    "properties": {
      "tags": {
        "type": "string",
        "copy_to": [
          "tag"
        ]
      },
      "application": {
        "analyzer": "application_analyzer",
        "type": "string"
      },
      "class": {
        "index": "not_analyzed",
        "type": "string"
      },
      "@version": {
        "index": "no",
        "type": "string"
      },
      "message": {
        "analyzer": "log_analyzer",
        "type": "string"
      },
      "@timestamp": {
        "format": "date_time",
        "doc_values": true,
        "type": "date"
      }
    }
  }
}
```

If it's necessary we could also report fields/properties conflicts between types (or even indices):

.Conflict on `application` property between two types

``` json
"conflicts": [
  {
    "property": "application",
    "first_type": "hadoop-hdfs",
    "other_type": "jboss-boot",
    "parameter": ["analyzer"]
  }
]
```

I think this will greatly improve the performance of administration/data visualization tool like Kibana or Head plugin when the cluster have many indices (&gt; 100).

What do you think ?
</description><key id="124473567">15728</key><summary>Return an aggregated view of all mappings/properties of all types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mogztter</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-12-31T14:04:34Z</created><updated>2016-04-14T09:49:59Z</updated><resolved>2016-04-14T09:49:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-31T14:16:01Z" id="168202342">Note that we are also considering removing types or at least divorcing mappings from types. See #15613.
</comment><comment author="Mogztter" created="2016-01-08T12:31:31Z" id="169985788">@jpountz I think this is a good idea but in the mean time are you in favor of this proposal ? Will you support a pull request with this feature ?
</comment><comment author="clintongormley" created="2016-03-01T09:16:49Z" id="190628602">HI @Mogztter 

What exactly is your use case for this?  @rashidkpc is asking for something similar in Kibana, but slightly different.  They would like an API which accepts zero or more indices and zero or more types and returns an array of distinct fields (an array, because some fields with the same name will be different in different indices) listing:
- the field type
- whether the field is searchable
- whether the field is aggregatable

Would an API like this cover your requirements too?
</comment><comment author="Mogztter" created="2016-03-01T12:43:50Z" id="190708414">&gt; What exactly is your use case for this?

I don't really have a use case, my goal is to improve performance of Kibana and Head by returning just the right level of information.

&gt; Would an API like this cover your requirements too?

I think so, as long as the result is an array of **distinct** fields.
</comment><comment author="Bargs" created="2016-04-07T14:53:16Z" id="206940185">This feature is even more important for Kibana with the change from string -&gt; text in ES 5.0. Since `text` fields have fielddata disabled by default, half of a Kibana user's string fields are going to throw a nasty error when the user attempts to aggregate on them (see more about this in https://github.com/elastic/kibana/issues/6769). Having an API that gives us a canonical list of fields with an isAggregatable boolean for each one would be a boon for Kibana users.
</comment><comment author="rashidkpc" created="2016-04-07T16:52:29Z" id="206990292">@clintongormley This is really important for us, really need this sooner than later.
</comment><comment author="spalger" created="2016-04-07T16:53:01Z" id="206990498">Couldn't agree more with @Bargs. With the change to string fields, and the fact that text is not aggregatable by default, this has become extremely important for Kibana
</comment><comment author="jpountz" created="2016-04-07T16:58:14Z" id="206993403">I thought it used to be the same before since the logstash template would disable fielddata on string fields? (to encourage to use the `.raw` field)
</comment><comment author="rashidkpc" created="2016-04-07T17:05:29Z" id="206996294">Only a subset of Kibana users use logstash, and logstash didn't make that change until Nov 2015
</comment><comment author="jpountz" created="2016-04-07T17:24:38Z" id="207009277">Is it something that could be done with the field mappings API? For instance, the sense recreation below:

```
PUT index
{
  "mappings": {
    "t": {
      "properties": {
        "f": {
          "type": "text"
        },
        "g": {
          "type": "text",
          "fielddata": true
        },
        "h": {
          "type": "keyword"
        },
        "i": {
          "type": "float"
        }
      }
    }
  }
}

GET index/_mapping/*/field/*?include_defaults=true&amp;filter_path=**.doc_values,**.index,**.fielddata
```

returns

```
{
  "index": {
    "mappings": {
      "t": {
        "f": {
          "mapping": {
            "f": {
              "index": true,
              "doc_values": false,
              "fielddata": false
            }
          }
        },
        "g": {
          "mapping": {
            "g": {
              "index": true,
              "doc_values": false,
              "fielddata": true
            }
          }
        },
        "h": {
          "mapping": {
            "h": {
              "index": true,
              "doc_values": true
            }
          }
        },
        "i": {
          "mapping": {
            "i": {
              "index": true,
              "doc_values": true
            }
          }
        }
      }
    }
  }
}
```

Fields that are searchable are those that have `index: true` and fields that are aggregatable are those that have either `doc_values: true` or `fielddata: true`.
</comment><comment author="rashidkpc" created="2016-04-07T17:30:21Z" id="207014933">It could be, but we'd much rather have a property that told us if the field was aggregatable. It used to be that any field with `index: true` could be aggregated, that changed. We tried coming up with rules in the past, but they were unreliable. There are also fields such as _type, _id, etc for which the rules are unclear. We really need to know whether or not, for sure, elasticsearch is going to allow aggregations to run on the field in a straight forward manner. 
</comment><comment author="rashidkpc" created="2016-04-07T17:34:07Z" id="207017060">@clintongormley in response to your point on #12817:

&gt; Another thought that came up in FixItFriday: HTTP compression should greatly reduce the amount of data being sent over the wire (given that there is so much repetition in the mappings). Unfortunately, HTTP compression is disabled by default (see #1482) . @kimchy can you remember the details?
&gt; 
&gt; I tested this out with 10 indices containing the same mapping of twenty fields, and it reduced a GET _mapping from 5589 bytes to 209 bytes... Sounds like this could be worth doing.

HTTP compression would likely be a wash at best. The issue is the size of the object being deserialized, not the actual bytes going over the wire. The browser's JSON deserializer tends to have issue deserializing single objects over a couple megs, which isn't uncommon for users with many fields in many indices. 
</comment><comment author="jpountz" created="2016-04-07T18:14:03Z" id="207033629">&gt; There are also fields such as _type, _id, etc for which the rules are unclear.

This is something that we could fix by using the same index/doc_values/fielddata convention as other fields. For instance I think it would be fine if the `_index` field reported that it is indexed and has doc values when `include_defaults=true`.
</comment><comment author="rashidkpc" created="2016-04-07T18:30:18Z" id="207039451">The big problem here is simply tracking the rules and knowing when they change. Since elasticsearch already knows the rules for when things are aggregatable, it would be best if there was an API to convey a fields abstract capabilities instead of the client needing to discern them from its enabled features.
</comment><comment author="clintongormley" created="2016-04-14T09:49:59Z" id="209855842">Closing in favour of https://github.com/elastic/elasticsearch/issues/17750
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation for Java API update/get settings API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15727</link><project id="" key="" /><description>Closes #10941.
</description><key id="124470976">15727</key><summary>Add documentation for Java API update/get settings API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-31T13:27:23Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2015-12-31T13:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-31T13:27:44Z" id="168193670">@jpountz And another one... :) 
</comment><comment author="jpountz" created="2015-12-31T13:47:14Z" id="168195237">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `_gce_` network when not using discovery gce</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15726</link><project id="" key="" /><description>For now we support `_gce_` only if discovery is set to `gce` and all information about GCE is provided (project_id and zone).
But in some cases, people would like to only bind to `_gce_` on a single node (without any elasticsearch cluster).

They could access the machine then from other machines running inside the same project.

This commit adds a new GceMetadataService which is started as soon as the plugin is started so GceNameResolver can use it to resolve `_gce`.

Closes #15724.
</description><key id="124459473">15726</key><summary>Allow `_gce_` network when not using discovery gce</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>enhancement</label><label>review</label></labels><created>2015-12-31T11:02:41Z</created><updated>2016-05-27T06:44:49Z</updated><resolved>2016-05-27T06:44:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-31T11:03:29Z" id="168170479">@imotov Could you review it please? 
Note that I did that against 2.x branch. If merged, I'll port it to master branch.
</comment><comment author="imotov" created="2016-01-04T18:47:24Z" id="168764970">I think we typically open PRs against master and then backport to 2.0, not other way around. Left a cosmetic comment. LGTM, but it might make sense for someone who is more familiar with GCE to take another look.
</comment><comment author="dadoonet" created="2016-01-05T09:30:30Z" id="168945842">Thanks @imotov.

@bleskes It's all yours now! :) Same as #15765 but on 2.x branch.
</comment><comment author="dadoonet" created="2016-04-29T15:56:44Z" id="215775557">If no one objects, I'll probably close this PR as it will be most likely merged in master.
Not sure if we want to have that "feature" in 2.4?
</comment><comment author="dadoonet" created="2016-05-27T06:44:49Z" id="222072429">As I mentioned a month ago, I'm closing this one.
It will be merged in master with https://github.com/elastic/elasticsearch/pull/15765
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to filter out subsections from metadata in cluster state API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15725</link><project id="" key="" /><description>Cluster state API can be really verbose even when `metadata` response filter is used.
It would be great if we could filter out subsections from `metadata`. For instance:

```
http://localhost:9200/_cluster/state/metadata.indices.settings,metadata.indices.state
```

This will filter out `metadata.templates`, `metadata.indices.mappings` and `metadata.indices.aliases`:

```
http://localhost:9200/_cluster/state/metadata.indices.settings,metadata.indices.state/twitter
```

``` json
{
  "cluster_name": "social-network",
  "metadata": {
    "indices": {
      "twitter": {
        "state": "open",
        "settings": {
          "index": {
            "creation_date": "1448705217713",
            "uuid": "gLEtWQ21RGC2D8PKeI-SgQ",
            "number_of_replicas": "1",
            "analysis": {
              "char_filter": {},
              "filter": {},
              "analyzer": {}
            },
            "number_of_shards": "3",
            "query": {
              "default_field": "message"
            },
            "refresh_interval": "5s",
            "version": {
              "created": "1070399"
            }
          }
        }
      }
    }
  }
}
```

Another option is to provide or extend an existing API to make `state` attribute available. AFAIK `"state:" "open|close"` is only available on the cluster state API. This information is missing from the "legacy" index status API but also from the new index stats API and index recovery API.

With `state` information available on the get settings API, the following query is (almost) equivalent with my proposal:

```
http://localhost:9200/twitter/_settings
```

``` json
{
  "twitter": {
    "state": "open",
    "settings": {
      "index": {
        "creation_date": "1448705217713",
        "uuid": "gLEtWQ21RGC2D8PKeI-SgQ",
        "number_of_replicas": "1",
        "analysis": {
          "char_filter": {},
          "filter": {},
          "analyzer": {}
        },
        "number_of_shards": "3",
        "query": {
          "default_field": "message"
        },
        "refresh_interval": "5s",
        "version": {
          "created": "1070399"
        }
      }
    }
  }
}
```
</description><key id="124455700">15725</key><summary>Allow to filter out subsections from metadata in cluster state API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mogztter</reporter><labels /><created>2015-12-31T10:20:20Z</created><updated>2015-12-31T11:18:36Z</updated><resolved>2015-12-31T11:18:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-31T10:56:19Z" id="168169801">Would response filtering work for this? https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_response_filtering
</comment><comment author="Mogztter" created="2015-12-31T11:18:36Z" id="168174817">Indeed, I didn't know about this feature, this is awesome, thanks :smile: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `_gce_` network when not using discovery gce</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15724</link><project id="" key="" /><description>For now we support `_gce_` only if discovery is set to `gce` and all information about GCE is provided (project_id and zone).
But in some cases, people would like to only bind to `_gce_` on a single node (without any elasticsearch cluster).

They could access the machine then from other machines running inside the same project.

From this thread: https://discuss.elastic.co/t/failed-to-start-elasticsearch-with-gce-cloud-plugin/38176/
</description><key id="124452914">15724</key><summary>Allow `_gce_` network when not using discovery gce</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>:Plugin Discovery GCE</label></labels><created>2015-12-31T09:52:14Z</created><updated>2016-07-28T15:19:37Z</updated><resolved>2016-07-28T15:19:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>search template not rendering correctly in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15723</link><project id="" key="" /><description>When I try the following on ES 2.0, 

```
GET /_render/template
{
  "inline": {
    "query": {
      "terms": {
        "status": [
          "{{#status}}",
          "{{.}}",
          "{{/status}}"
        ]
      }
    }
  },
  "params": {
    "status": [ "pending", "published" ]
  }
}
```

I get 

```
{
   "template_output": {
      "query": {
         "terms": {
            "status": [
               "",
               "pending",
               "",
               "published",
               ""
            ]
         }
      }
   }
}
```

with the extra double quotes and trailing comma after "published" as opposed to the documented one [here](https://github.com/elastic/elasticsearch/blob/2.0/docs/reference/search/search-template.asciidoc#validating-templates)
</description><key id="124439527">15723</key><summary>search template not rendering correctly in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asldevi</reporter><labels /><created>2015-12-31T06:43:30Z</created><updated>2016-01-10T17:58:08Z</updated><resolved>2016-01-10T17:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T17:58:08Z" id="170374616">For conditional templates like these, you need to specify the template as a string, not as JSON
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DOC - field name is changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15722</link><project id="" key="" /><description>`exact_value` is changed to `full_text` at the second (3) example.
</description><key id="124437246">15722</key><summary>DOC - field name is changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jason-heo</reporter><labels><label>docs</label></labels><created>2015-12-31T06:21:15Z</created><updated>2016-01-05T13:34:27Z</updated><resolved>2016-01-05T13:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Requests.putMappingRequest(index).type(index).source(builder);type must use the same value index?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15721</link><project id="" key="" /><description>when i create mapping using java api like this:
PutMappingRequest mapping = Requests.putMappingRequest(index).type(indexType).source(builder);
        PutMappingResponse putMappingResponse = client.admin().indices().putMapping(mapping).actionGet();

if i use "Requests.putMappingRequest(index).type(indexType)" instead of "Requests.putMappingRequest(index).type(index)." ,it reports error like this:

MapperParsingException[Root mapping definition has unsupported parameters:  [mpapercms : {properties={id={type=integer, store=yes}, kw={type=string, store=yes, analyzer=ik, searchAnalyzer=ik}, edate={type=date, store=yes, analyzer=ik, searchAnalyzer=ik}}}]]

so i wonder why i can't use indexType, shouldn't i use indexType instead of index?
</description><key id="124427340">15721</key><summary>Requests.putMappingRequest(index).type(index).source(builder);type must use the same value index?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">selfchanger</reporter><labels /><created>2015-12-31T03:28:43Z</created><updated>2015-12-31T10:18:44Z</updated><resolved>2015-12-31T09:01:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-31T09:01:50Z" id="168153146">Please use the forums to ask questions: http://discuss.elastic.co. I suspect your problem is that you put the name of your index instead of your type at the root of your mapping definition.

We try to only use github for confirmed bugs and feature requests.
</comment><comment author="dadoonet" created="2015-12-31T09:41:11Z" id="168158818">@selfchanger Adding to @jpountz's answer that we just added some documentation in 2.x and master branches with an example: https://www.elastic.co/guide/en/elasticsearch/client/java-api/2.x/java-admin-indices.html#java-admin-indices-put-mapping
</comment><comment author="selfchanger" created="2015-12-31T10:18:44Z" id="168163533">@jpountz @dadoonet sorry for asking in github,i didn't know how to ask at  http://discuss.elastic.co before,so i asked here,now i know,thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices\mpapercms\2\translog\translog-3662425237604305305.tlog java.nio.file.NoSuchFileException: D:\elasticsearch-2.1.0\data\elasticsearch\nodes\0\indices\mpapercms\2\translog\translog-3662425237604305305.tlog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15720</link><project id="" key="" /><description>Hi&#65292;since i installed elasticsearch 2.1.0&#65292;each time i start up es,it reports this exception&#65306;
code:
[2015-12-31 11:02:04,933][WARN ][index.translog           ] [Flash Thompson] [mpapercms][0] failed to delete temp file D:\elasticsearch-2.1.0\data\ela
sticsearch\nodes\0\indices\mpapercms\0\translog\translog-17313653199342993.tlog
java.nio.file.NoSuchFileException: D:\elasticsearch-2.1.0\data\elasticsearch\nodes\0\indices\mpapercms\0\translog\translog-17313653199342993.tlog
        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79)
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102)
        at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:269)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1079)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
</description><key id="124426138">15720</key><summary>indices\mpapercms\2\translog\translog-3662425237604305305.tlog java.nio.file.NoSuchFileException: D:\elasticsearch-2.1.0\data\elasticsearch\nodes\0\indices\mpapercms\2\translog\translog-3662425237604305305.tlog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">selfchanger</reporter><labels /><created>2015-12-31T03:05:51Z</created><updated>2015-12-31T10:04:15Z</updated><resolved>2015-12-31T09:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-31T09:04:21Z" id="168153285">this is fixed in version 2.1.1 https://github.com/elastic/elasticsearch/pull/14872

Also you can safely ignore this warning, it is harmless.
</comment><comment author="selfchanger" created="2015-12-31T10:04:15Z" id="168160463">@jpountz see, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show fetch_count[152] for Fetch Slowlog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15719</link><project id="" key="" /><description>Currently the fetch slowlog and the query slowlog show the same detail, which is generally extremely useful.

However, when debugging slow fetches, it's hard to judge just how many documents were coming from a given shard. As a result, it would be great to know how many documents are actually being fetched from the shard when it's marked as slow.

As a workaround, it is possible to supply the relevant shard as a shard preference in order to get the relevant count, but it may be out of date when a human checks.
</description><key id="124413463">15719</key><summary>Show fetch_count[152] for Fetch Slowlog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-12-30T23:23:53Z</created><updated>2016-01-10T17:57:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Ordering term aggregation based on scripted metric.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15718</link><project id="" key="" /><description>Hi,

Scripted metrics have been introduced to compute complex computations based on a map/reduce approach. Unfortunately the usage of scripted metrics is limited because they can't be used in the 'order' clause of an aggregation. See this discussion for a full description of the issue. 

https://discuss.elastic.co/t/ordering-terms-aggregation-based-on-pipeline-metric/31839

This pull-request is an attempt to fix this issue for scripted metrics returning a number as the result of their reduce method (probably more than 90% of use cases).

That's my first contribution to ES, so there is probably room for improvements and even may be mistakes here and there. Don't hesitate to guide me in the right direction in order to make this pull request a success.

Thanks

Laurent 
</description><key id="124405054">15718</key><summary>Ordering term aggregation based on scripted metric.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lquerel</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label></labels><created>2015-12-30T22:00:18Z</created><updated>2017-06-20T11:02:46Z</updated><resolved>2017-06-20T11:02:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lquerel" created="2015-12-30T22:04:29Z" id="168081455">The CLA is now signed.
</comment><comment author="lquerel" created="2016-01-06T00:07:09Z" id="169174398">For info, this pull request fixes this issue.

https://github.com/elastic/elasticsearch/issues/8486
</comment><comment author="dakrone" created="2016-04-06T17:25:12Z" id="206475266">@colings86 I think this is something you might be able to take a look at?
</comment><comment author="lquerel" created="2016-08-19T01:02:30Z" id="240900889">@dakrone, @colings86 is possible to move forward here? So many people complain that ElasticSearch is not able to order term aggregation based on scripted metric?

It's like a major missing feature IMO and for sure it's inconsistent with the rest of the aggregation framework.
</comment><comment author="colings86" created="2016-08-19T10:25:11Z" id="240983419">@lquerel sorry it's taken so long to get back to you on this PR. 

This is a tricky feature as there are problems with sorting the terms aggregation by any aggregation type which is the focus of https://github.com/elastic/elasticsearch/issues/17588. Given this I am personally hesistant to add more cases where your can sort by a sub-aggregation but have no idea whether you actually have the top N or even what the error in the result could be.

If we do end up implementing this feature I would also be keen for it to not change the aggregation framework in a way that is specific only to this aggregation since this IMO will make the aggregation framework tricky to understand and maintain. We actually want to move the `scripted_metric` aggregation to a plugin since it is an expert feature (https://github.com/elastic/elasticsearch/issues/19821) adding to the case for not having special cases in the core aggregation code to support it.
</comment><comment author="colings86" created="2016-08-19T10:31:02Z" id="240984454">I have also added a comment to the issue asking for use cases where users need to order by a scripted_metric aggregation to better understand where users think the current set of aggregations are lacking and what we can do to fill these gaps https://github.com/elastic/elasticsearch/issues/8486#issuecomment-240984177
</comment><comment author="lquerel" created="2016-08-22T21:29:07Z" id="241556708">@colings86 Thanks for your response. 

I understand the need to remove all special cases from the core aggregation framework. The fact that scripted metrics don't behave as any other metrics is disturbing for the user. 

If scripted metrics are implemented as a plugin, then I hope the "order by" clause will be supported because otherwise ElasticSearch doesn't compete with the aggregation framework of  a solution like crate.io (bases on ElasticSearch core) or even with the basic MongoDB aggregation framework. They support group by, order by on any computation. 

IMO, with a better support of different variants of average functions we could avoid a lot scripted metrics use cases (weighted average + other forms explained in this thread https://github.com/elastic/elasticsearch/issues/8486#issuecomment-241028336).

Finally compute an accurate Top X on a distributed system is not easy and sometimes not really essential. A fast approximation is usually good enough in most case.
</comment><comment author="rjernst" created="2017-06-09T05:10:51Z" id="307296847">@colings86 Where do we stand on this PR? It has been open for 1.5 years, we should make a decision on it.</comment><comment author="elasticmachine" created="2017-06-09T05:10:52Z" id="307296851">Since this is a community submitted pull request, a Jenkins build has not been kicked off automatically. Can an Elastic organization member please verify the contents of this patch and then kick off a build manually?
</comment><comment author="colings86" created="2017-06-20T11:02:45Z" id="309719486">I am going to close this PR as any solution for this would need to work in a way that does not special case the `scripted_metric` aggregation. Also the issue for this PR (#8486) is closed as we want to solve the specific use-cases raised for sorting `terms` aggregations by the `scripted_metric` aggregation in other ways</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation for Java API admin API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15717</link><project id="" key="" /><description>Added:
- refresh API: Closes #10942.
- health API: Closes #10818.
</description><key id="124383676">15717</key><summary>Add documentation for Java API admin API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>review</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-30T18:32:08Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2015-12-31T14:23:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-30T18:32:29Z" id="168050287">@jpountz Could you review as well? Thanks!
</comment><comment author="dadoonet" created="2015-12-31T10:31:42Z" id="168164413">added a commit. Let me know
</comment><comment author="jpountz" created="2015-12-31T14:19:35Z" id="168202745">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 2.x leaving "deleted" dangling files open?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15716</link><project id="" key="" /><description>We've found a few nodes of our ElasticSearch 2.1.1 cluster in a state where there are a ton of "dangling" deleted files that ElasticSearch has not let go of, so the OS cannot let go of them. This is causing some major problems with shard-allocation-by-disk-utilization.

```
[root@ops-flume-es-data-useast1-156-i-30af8181:/mnt:130]# du -sch /mnt
2.5T    /mnt
2.5T    total
[root@ops-flume-es-data-useast1-156-i-30af8181:/mnt]# df -h /mnt
Filesystem      Size  Used Avail Use% Mounted on
/dev/md127      5.0T  4.4T  662G  88% /mnt
[root@ops-flume-es-data-useast1-156-i-30af8181:/mnt]# 
```

```
[root@ops-flume-es-data-useast1-156-i-30af8181:/mnt]# lsof | grep deleted
...
...
java        6130   6151 elasticsearch 2078r      REG              9,127          43     4198403 /mnt/elasticsearch/flume-elasticsearch-production_vpc-useast1/flume-elasticsearch-production_vpc-useast1/nodes/0/indices/events-2015-12-05/5/translog/translog-2.tlog (deleted)
java        6130   6151 elasticsearch 2079u      REG              9,127          43        2060 /mnt/elasticsearch/flume-elasticsearch-production_vpc-useast1/flume-elasticsearch-production_vpc-useast1/nodes/0/indices/events-2015-12-05/5/translog/translog-3.tlog (deleted)
java        6130   6151 elasticsearch 2080r      REG              9,127  2574004451 32227958823 /mnt/elasticsearch/flume-elasticsearch-production_vpc-useast1/flume-elasticsearch-production_vpc-useast1/nodes/0/indices/flume-2015-12-05/4/index/_36jq_Lucene50_0.doc (deleted)
java        6130   6151 elasticsearch 2081r      REG              9,127  9180063556 32227958821 /mnt/elasticsearch/flume-elasticsearch-production_vpc-useast1/flume-elasticsearch-production_vpc-useast1/nodes/0/indices/flume-2015-12-05/4/index/_36jq.fdt (deleted)
java        6130   6151 elasticsearch 2082r      REG              9,127  1083231791 32227958828 /mnt/elasticsearch/flume-elasticsearch-production_vpc-useast1/flume-elasticsearch-production_vpc-useast1/nodes/0/indices/flume-2015-12-05/4/index/_36jq_Lucene50_0.pos (deleted)
...
```

```
[root@ops-flume-es-data-useast1-156-i-30af8181:/mnt]# lsof | grep deleted | wc
  169786 1867204 45795894
```
</description><key id="124375958">15716</key><summary>ElasticSearch 2.x leaving "deleted" dangling files open?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">diranged</reporter><labels><label>:Core</label><label>:Translog</label><label>feedback_needed</label></labels><created>2015-12-30T17:23:28Z</created><updated>2016-05-24T10:33:10Z</updated><resolved>2016-05-24T10:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-31T13:05:38Z" id="168191515">Lucene will hold open file descriptors for deleted index files, when segment merging has happened but flush and/or refresh haven't happened.

Have you customized e.g. `refresh_interval` or any of the translog flush settings?

Do you ever invoke `optimize` (`forceMerge`)?

The still-open-but-deleted translog files are interesting ... @s1monw is that expected?
</comment><comment author="clintongormley" created="2016-03-01T09:09:12Z" id="190624617">Hi @diranged - any further info re @mikemccand 's [questions](https://github.com/elastic/elasticsearch/issues/15716#issuecomment-168191515)

@bleskes any ideas about the still-open-but-deleted translog files?
</comment><comment author="bleskes" created="2016-03-01T10:26:21Z" id="190652160">I looked at the code and the only place I could see where we might leak file references to the translog is if we had an exception while constructing the translog, followed by a successful opening of it. It also turns out that Simon already fixed it with  #15762 :grin: , released with 2.1.2
</comment><comment author="clintongormley" created="2016-05-24T10:33:10Z" id="221230211">Looks like there's nothing left to do here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix dynamic mapping corner case.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15715</link><project id="" key="" /><description>Today we fail if the field exists in another type and multi fields are defined
in a template.
</description><key id="124375546">15715</key><summary>Fix dynamic mapping corner case.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-30T17:21:07Z</created><updated>2015-12-31T08:49:57Z</updated><resolved>2015-12-31T08:44:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-30T17:24:30Z" id="168038893">Tagged as a non issue given that the bug is not released yet.
</comment><comment author="nik9000" created="2015-12-31T03:22:55Z" id="168114866">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disable dynamic mapping on fields with dots instead of rejecting documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15714</link><project id="" key="" /><description>As of ES 2.0; field names with dots are no longer allowed. This is breaking things for a lot of people. I've seen plugins, scripts, etc. being mentioned on twitter and Github that are all intended to hack around this and make this less painful.  Just the amount of stuff like this floating around is indicative of how much of an issue this is. It's a showstopper for us and is the main reason we are still on 1.7.x. We are looking at breaking logging infrastructure, major code changes, and complex data migrations; all of which need lots of testing.

Disabling dynamic mapping for dotted fields is probably what most people would prefer rather than the entire document being rejected with some error about dots. In general, I don't think Elasticsearch should reject any valid json for storing. Whether it adds information from those documents to an index is a different matter. This requires a mapping either explicit or dynamic and I think people can live with adapting their mapping if it has options for dealing with dots. For dynamic mapping you could make the case that it should fail more gracefully than to outright reject the document with a bad request.

The options I had in mind are the following:

```
# controls whether documents with dots in field names are rejected. Setting this to true 
allows them but disables dynamic mappings on these fields
settings.mapper.reject_dotted_fields: false

# automatically maps fields with dots to a field with the dots replaced the value of auto_convert_dots_character
settings.index.mapper.auto_convert_dots:true

# controls how dots are converted, _ is probably a good default
settings.index.mapper.auto_convert_dots_character:_
```

Whether this should be on or off is up for debate, though I'd probably blindly turn it on just about anywhere in my own setups. Unless I'm missing something, none of this should be particularly hard to support. It merely internalizes the hacks people are currently forced to implement outside of elasticsearch.

ps. I left a comment on #15404 with a similar suggestion on how to fix things. I realize the ticket is closed but it seems at least the workaround I suggest might be a viable improvement at least. So, filing it as a separate issue in the hope that we might make some progress.
</description><key id="124375013">15714</key><summary>disable dynamic mapping on fields with dots instead of rejecting documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels /><created>2015-12-30T17:15:39Z</created><updated>2016-01-19T10:21:14Z</updated><resolved>2016-01-05T23:38:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TinLe" created="2015-12-30T17:24:14Z" id="168038793">+1
</comment><comment author="rjernst" created="2015-12-31T19:42:30Z" id="168238283">Not allowing field names with dots has nothing to do with dynamic mappings. It is entirely about disambiguating field mappings in general. See the discussion on #14359 for an explanation of why it is important for ES 2.x.
</comment><comment author="jillesvangurp" created="2016-01-01T16:29:57Z" id="168315005">@rjernst I get that you don't want to index fields with dots in them; so don't by default. That's a different and better solution than rejecting documents entirely because some of their fields have dots. 

This wouldn't be an issue if elasticsearch didn't have dynamic mappings because then there would be simply no way to map field names with dots in them if the mapping didn't support it. One of the workarounds that actually works is actually turning off dynamic mapping on nested fields with dots using a mapping template. So, this is entirely about dynamic mappings and that's also where the fix is going to be. 

I understand the argument why you don't want to map fields with dots because it complicates the query dsl. However, I don't see any argument for rejecting documents with such fields just because you can't map the field dynamically. If it is not mapped, it's not part of the mapping and there is no ambiguity. Problem solved.

Also, I don't see the complexity of having a mapping feature to replace . with _.  This is different then escaping, which would indeed have implications for the query dsl because then you have to worry about unescaping and keeping dotted expressions unambiguous. 
</comment><comment author="rjernst" created="2016-01-05T23:38:46Z" id="169169579">&gt; I get that you don't want to index fields with dots in them

This entirely _not_ the point. The restriction on field names without dots has nothing to do with actual indexing (and nothing to do with dynamic mapping, as I stated before). It is about how we store and lookup mappings by field name internally.  We must be able to distinguish a field called `foo.bar`, and an object field `foo` with a field `bar` under it. This is not necessarily a restriction that will always be around, but we opted to go with simplicity to start (which allowed us to lock down mappings to remove all ambiguity).

&gt; Also, I don't see the complexity of having a mapping feature to replace . with _.

This is trivial enough for a client to do, no need to complicate the ES api with it. (And there is a [logstash plugin](https://www.elastic.co/guide/en/logstash/current/plugins-filters-de_dot.html) to do exactly that).

I'm closing this issue as there are sufficient examples and plugins, as the original description mentions, for how to "de dot" field names.  Again, this is not necessarily something that will always be a restriction, but it is for now, until the work can be scoped and done to eg allow escaping.
</comment><comment author="jillesvangurp" created="2016-01-06T08:58:55Z" id="169270520">If it is not indexed, it is by definition not part of the mapping. So there is no disambiguity whatsoever looking up the field because it is not in it. So, if you don't dynamically map fields with dots in their name, this completely solves the problem. 

Dynamic mapping throwing the error is the entire problem. If I make a static mapping that has a template that disables indexing on affected fields, documents actually index fine. The only time this is a problem is when dynamic mapping attempts to create a lucene index for a field with a dot in it and then throws an error instead of giving up its attempt to map that particular field.

So I disagree with your reason for closing this issue. Please explain how an unmapped field with dots in it causes ambiguity issues in any way. 
</comment><comment author="rjernst" created="2016-01-06T20:24:41Z" id="169450871">&gt; Please explain how an unmapped field with dots in it causes ambiguity issues in any way

I never said this. But again, this does not have to do with dynamic mappings. It has to do with _mapping_ a field name with dots, regardless of whether it is dynamic or not (ie doing a PUT mapping). You are free to do exactly what you said to avoid the issue (don't index the field names that contain dots).
</comment><comment author="jillesvangurp" created="2016-01-06T21:23:05Z" id="169466672">This issue had two parts:
-  the title part: _don't_ map dotted fields dynamically so I can safely index json documents that have some fields with dots in them without having to worry about elasticsearch throwing bad requests. They won't be searchable and will only be available through _source. There's no ambiguity for query dsl or anything else that refers mapped fields because they simply never get created to begin with. This is IMHO much better than rejecting the json content with a bad request. Dynamic mapping is never going to be perfect, it's merely a best effort.
- having a mechanism to escape or convert fields with dots. I get that this is a bit more messy and probably has been debated at length. I'm not arguing otherwise.

So, I can live with doing just part 1 of this issue. This is for me more important than actually getting the fields mapped. That would be extremely helpful for people like me who are planning 1.x to 2.x migration.

I don't get why part 1 is hard/impossible/controversial, all the arguments you provide apply to part 2 of the issue, not part 1. So, should I file a new ticket for this or can we just reopen this ticket or am I really missing something here (this discussion seems very circular so far)?
</comment><comment author="rjernst" created="2016-01-07T20:02:18Z" id="169790297">While I was focusing on what you described as part 2, I still think my original argument applies equally to part 1. It is easy enough for a user to remove fields themselves when serializing, rather than complicating the ES api with additional settings (which may go away in the future when we can support dots).
</comment><comment author="jillesvangurp" created="2016-01-07T21:10:02Z" id="169805993">Sorry, but none of what you say applies to part 1. The only argument I'm hearing now is "we don't want to add complexity". To be clear we are talking about one extra parameter to enable/disable dot tolerance. And, I'd argue that parameter could default to true for the vast majority of users, meaning that it might not actually be needed at all. Why would you actually want Elasticsearch to throw bad requests here?. Just document the behavior that dotted fields won't be dynamically mapped and can't be statically mapped by default until whenever it is that elasticsearch implements escaping. That's actually less complexity because it simplifies using elasticsearch and removes elaborate hacks you need left right and center to de-dot json content. 

We've argued this at length and I can see we are not going to agree. So, I am not going to press this issue any further. But for the sake of other users experiencing the same pain, _please escalate this internally inside Elasticsearch_ to get some broader discussion &amp; consensus on whether it is really that necessary to inconvenience your users this much. 

API purity is a nice goal but you also have a rather large installed user base that you are leaving out in the cold with this one (again this is a major PITA). IMHO using elasticsearch got more complex, not less complex because of this new behavior. I think a rather large portion of the unmigrated 1.x user base (i.e. at this point most of your user base) will run into this as well and fundamentally I don't see a good technical reason for this problem to exist. I probably won't be the last to bring this up. 
</comment><comment author="TinLe" created="2016-01-07T21:58:56Z" id="169819478">Just want to add a metoo.   I have a very large user population and over 1000+ physical nodes spread around the globe that are running 1.x.   No one wants to upgrade because of these issues.   It's very painful.

I've been using tribe nodes as a way to at least federate across the clusters.   Unfotunately for me, that is no longer possible with ES 2.x, not backward compatible protocol wise.   I've already tried it.   That mean I can not do incremental upgrade of cluster by cluster, but have upgrade all of my downstream clusters at once (most of which I do not control :-( ).   Alternative is to setup a separate ELK 2.x infrastructure and migrate clusters over to it.   That's a lot of time, money and people resources.
</comment><comment author="rjernst" created="2016-01-18T22:11:34Z" id="172667870">Please see the discussion in #15951. As I stated before, we want to solve this problem, and we are trying to come up with a solution that works for at least most cases.
</comment><comment author="jillesvangurp" created="2016-01-19T10:21:14Z" id="172806716">Good to see #15951 is moving forward. I think the third option of simply not indexing field names with dots in them as I suggest above could be added there as well. But, I'd be happy with either of the two approaches discussed there as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation for Java API create index and put mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15713</link><project id="" key="" /><description>Starting documentation about the admin client.
- Create index with settings
- put and update mapping. Closes #10816
</description><key id="124372353">15713</key><summary>Add documentation for Java API create index and put mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Java API</label><label>docs</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-30T16:53:22Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2015-12-30T17:41:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-30T17:19:45Z" id="168037561">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian init script exits with success if Elasticsearch binary is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15712</link><project id="" key="" /><description>We've had [an issue filed](https://github.com/elastic/cookbook-elasticsearch/issues/416) on the chef cookbook for Elasticsearch describing a problem [with this line](https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L112) of the Debian init script:

```
# Check DAEMON exists
test -x $DAEMON || exit 0
```

In short, if a user picks the wrong path for the binary, the init script exits 0 / success. I don't see the same problem with the RHEL/CentOS init script.
</description><key id="124364579">15712</key><summary>Debian init script exits with success if Elasticsearch binary is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinb3</reporter><labels><label>:Packaging</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-12-30T15:54:09Z</created><updated>2016-11-04T14:02:31Z</updated><resolved>2016-11-04T09:01:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T17:50:00Z" id="170374153">Hmm so this has been there since Dec 2011.  @spinscale any idea why we do this?
</comment><comment author="spinscale" created="2016-01-10T22:01:23Z" id="170398119">no, looks like indeed like a bug. I think LSB defines what should be returned. There is a "program not running" error code, that should be returned here...
</comment><comment author="martinb3" created="2016-11-04T09:00:03Z" id="258375750">Based on https://refspecs.linuxbase.org/LSB_3.0.0/LSB-PDA/LSB-PDA/iniscrptact.html, it seems like 3 would be reasonable.
</comment><comment author="martinb3" created="2016-11-04T09:01:35Z" id="258376027">It looks like this test was removed in the latest master, FYI. We can probably close this.
</comment><comment author="clintongormley" created="2016-11-04T14:02:31Z" id="258439124">thanks @martinb3 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Simulate api should also work if the document id or type is omitted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15711</link><project id="" key="" /><description>The id and type of sample documents provided in the simulate api should be optional, so that it is easy to just paste in sample logs without having to think about id, type or any other document metadata.
</description><key id="124356735">15711</key><summary>[Ingest] Simulate api should also work if the document id or type is omitted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-12-30T14:42:59Z</created><updated>2016-01-04T19:34:45Z</updated><resolved>2016-01-04T19:34:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Bargs" created="2015-12-30T14:46:22Z" id="168013545">This is critical for allowing first time users to interactively build a pipeline in the new ingest UI using some sample log data before indexing any real data.
</comment><comment author="Bargs" created="2015-12-30T14:53:58Z" id="168014533">@BigFunger you'll need this for the pipeline step you're working on eventually.
</comment><comment author="martijnvg" created="2016-01-04T19:34:45Z" id="168779330">Added via #15747
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for the `multi_field` type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15710</link><project id="" key="" /><description>It is officially unsupported since version 1.0.
</description><key id="124333883">15710</key><summary>Remove support for the `multi_field` type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-30T11:05:05Z</created><updated>2016-03-10T18:53:27Z</updated><resolved>2015-12-30T14:36:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-30T11:21:23Z" id="167981203">LGTM (nice cleanup!)
</comment><comment author="dadoonet" created="2015-12-30T11:24:52Z" id="167981990">Do we need to add it in breaking changes? Though it was deprecated in 1.0?
</comment><comment author="jpountz" created="2015-12-30T13:15:35Z" id="167996571">It is already in the 1.0 breaking changes so I don't think we need to add it again?
</comment><comment author="dadoonet" created="2015-12-30T13:28:42Z" id="167998797">Indeed. https://www.elastic.co/guide/en/elasticsearch/reference/1.3/_multi_fields.html

&gt; Well, the field type `multi_field` has been removed.

Ignore me then! :D 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge FunctionScoreQuery and FiltersFunctionScoreQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15709</link><project id="" key="" /><description>FunctionScoreQuery is a specialization of FiltersFunctionScoreQuery for the case that there are no filters. Instead of specializing at the query level, this should be done at the scorer level. 
</description><key id="124330130">15709</key><summary>Merge FunctionScoreQuery and FiltersFunctionScoreQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-30T10:19:35Z</created><updated>2017-07-28T07:22:21Z</updated><resolved>2017-07-28T07:22:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Extend reroute with an option to force assign stale primary shard copies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15708</link><project id="" key="" /><description>Relates to #14739.
</description><key id="124329872">15708</key><summary>Extend reroute with an option to force assign stale primary shard copies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-30T10:16:30Z</created><updated>2016-01-20T13:37:51Z</updated><resolved>2016-01-19T11:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-07T11:10:34Z" id="169632576">@ywelsch and I discussed this yesterday. Here is the gist of the discussion:

1) Instead of fetching the store data in the command, adapt unassigned info so that the node initializing the shard will expect local data and fail if it's not there.
2) Remove the ignore allocation decider param and always assign if the user asked for it (in the case of the primary assignment)
3) Try to fold the new AllocatePrimaryAllocationCommand in the current AllocateAllocationCommand.

Here's a suggest for how the rest layer will look like for that:

```
POST _cluster/reroute
{
  "commands": [
    {
      "allocate": {
        "index": "index",
        "shard": 0,
        "node": "node",
        "allow_primary": "never|existing_only|force_empty"
      }
    }
  ]
```

Note the `allow_primary` option. The default, `never` refuses to allocate a primary. `existing_only` allocates it but must reuse an existing copy. `force_empty` does what true does now - i.e., force the creation of an empty primary. Also the last two should imply ignoring the allocation deciders.

@ywelsch thoughts?
</comment><comment author="ywelsch" created="2016-01-07T17:03:32Z" id="169730377">Before this PR, users only had the possibility to either allocate a replica or an empty primary. The flag "allow_primary" was misleading in that context. I would very much like to get rid of this flag altogether and introduce something new. What about introducing a new "mode" field:

"mode": "replica" // default, only allocates unassigned replica shard (fails if unassigned primary exists)
"mode": "primary" // only allocates if primary unassigned and if data available on node
"mode": "empty_primary" // only allocates if primary unassigned, creates empty shard

"mode": "replica" is equivalent to the old "allowPrimary" : "false"
"mode": "empty_primary" is equivalent to the old "allowPrimary" : "true"

Also, we should check for consistency with the cancel command that currently has an "allowPrimary" flag as well.
</comment><comment author="ywelsch" created="2016-01-08T09:06:56Z" id="169940336">Also, relates to #14352
</comment><comment author="bleskes" created="2016-01-08T09:18:59Z" id="169943855">I think mode is a bit ambiguous as a flag. I do understand the concerns that `allow_primary` will now do something different than the allow primary in the cancel command.  As far as the values. I like "existing_only" better than just "primary" as it better communicates the restriction (we must use local data). Also I like "force_empty" better than "empty_primary" because it communicates the implications better and the severity better. It also better describes what we do today - always create an empty shard, even if there is local data. That last one is of course subject to change (but that's a bigger discussion)
</comment><comment author="ywelsch" created="2016-01-08T12:27:39Z" id="169985096">&gt;  As far as the values. I like "existing_only" better than just "primary" as it better communicates the restriction (we must use local data). Also I like "force_empty" better than "empty_primary" because it communicates the implications better and the severity better. It also better describes what we do today - always create an empty shard, even if there is local data.

```
$ grep -o "better" | wc -l
6
```

sorry ;-)
</comment><comment author="clintongormley" created="2016-01-09T11:12:51Z" id="170226197">We have a stalled issue (https://github.com/elastic/elasticsearch/issues/14352) about choosing a better name for `allow_primary` which properly indicates the fact that you can lose your data.  The issue is stalled because it is difficult to come up with the right name.

Instead, what about changing the action name from `allocate` to:
- `allocate_replica`
- `allocate_empty_primary` 
- and for this PR: `allocate_stale_primary`

@s1monw is concerned about exposing these last two actions via the API and would prefer to provide a command line tool (perhaps as a plugin) to do this job instead.  The idea is to make it harder for users to make a change that will delete data, and will hopefully force them to read about the consequences of their action.

While I understand this sentiment, I'm not convinced that a command line tool is the way to go here.  ES is API driven so this would be quite a change from what we do today, and there are probably many users who don't have command line access to their hosted clusters, and who need to fix a problem quickly without having to wait for support to respond to their tickets.

Instead, I'd propose adding an `accept_data_loss` flag to these actions which must be set to true for the action to be taken.  I think this would be enough to warn the user that they're doing something dangerous.
</comment><comment author="bleskes" created="2016-01-09T11:42:31Z" id="170228689">+1. My only suggestion would be to use 'force_empty_primary' and 'force_stale_primary' as command names, to make it even clearer that (all) shard content will be lost.

On 9 jan. 2016 12:13 PM +0100, Clinton Gormleynotifications@github.com, wrote:

&gt; We have a stalled issue (#14352(https://github.com/elastic/elasticsearch/issues/14352)) about choosing a better name forallow_primarywhich properly indicates the fact that you can lose your data. The issue is stalled because it is difficult to come up with the right name.
&gt; 
&gt; Instead, what about changing the action name fromallocateto:
&gt; 
&gt; allocate_replica
&gt; allocate_empty_primary
&gt; and for this PR:allocate_stale_primary
&gt; 
&gt; @s1monw(https://github.com/s1monw)is concerned about exposing these last two actions via the API and would prefer to provide a command line tool (perhaps as a plugin) to do this job instead. The idea is to make it harder for users to make a change that will delete data, and will hopefully force them to read about the consequences of their action.
&gt; 
&gt; While I understand this sentiment, I'm not convinced that a command line tool is the way to go here. ES is API driven so this would be quite a change from what we do today, and there are probably many users who don't have command line access to their hosted clusters, and who need to fix a problem quickly without having to wait for support to respond to their tickets.
&gt; 
&gt; Instead, I'd propose adding anaccept_data_lossflag to these actions which must be set to true for the action to be taken. I think this would be enough to warn the user that they're doing something dangerous.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/pull/15708#issuecomment-170226197).
</comment><comment author="ywelsch" created="2016-01-14T10:09:09Z" id="171594201">@bleskes @clintongormley I pushed a new set of changes reflecting the naming suggested by @clintongormley. This new changeset also contains documentation for the new commands. Two open points remain for me:
-  I am still not fully convinced about the command naming. I like @clintongormley's suggestion of command names `allocate_replica`, `allocate_empty_primary` and `allocate_stale_primary`. The `accept_data_loss` flag, however, feels unnatural to me in the rest api and even weirder in the Java API. I am not completely against a safeguard, but would prefer to have it in another form. As an inspiration, we might look at the way it is done for wildcard indices deletion ([`action.destructive_requires_name`](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-delete-index.html)).
- To keep the commands symmetric, we should also split the `cancel` command (which currently has an `allowPrimary` flag) into two commands `cancel_replica` and `cancel_primary`.

@bleskes What I don't like about `force_empty_primary` is that the action verb `allocate` is lost.
</comment><comment author="bleskes" created="2016-01-18T14:13:49Z" id="172536875">Thanks @ywelsch . I really like how it turned out. Left some minor comments here and there. 
</comment><comment author="ywelsch" created="2016-01-18T21:11:24Z" id="172654810">pushed another round of changes addressing the comments. Please have another look @bleskes. (The `NamedWriteableRegistry` refactoring will be for another time)
</comment><comment author="bleskes" created="2016-01-19T10:40:44Z" id="172813441">LGTM. Made one suggestion for simplifying a test (and asked a question). Feel free to adopt the suggestion (or not) and push. Thanks for all the iterations, @ywelsch .
</comment><comment author="ywelsch" created="2016-01-19T11:25:54Z" id="172822489">Thanks for the thorough review @bleskes !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>pipeline agg could return value of a not-exists buckets_path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15707</link><project id="" key="" /><description>Percentiles metric aggregtaion returns a multi-value result. And I know I can write `buckets_path` for multi-value like `bucketsname.metricname`.

But, the key of percentiles metric agg result is named like 50.0, 90.0, those also use DOT!

I try to delete the `.0`, and find something other more interesting: pipeline agg can return values of a not-exists `buckets_path`.

Here is my request:

```
{
  "aggs" : {
    "my_date_histo" : {
      "date_histogram" : {
        "field" : "@timestamp",
        "interval" : "1h"
      },
      "aggs" : {
        "pcttime" : {
        "percentiles" : { "field" : "num_field" }
        },
        "the_movavg45" : {
          "moving_avg" : {
            "buckets_path" : "pcttime.45",
            "window" : 10,
            "model" : "simple"
          }
        },
        "the_movavg50" : {
          "moving_avg" : {
            "buckets_path" : "pcttime.50",
            "window" : 10,
            "model" : "simple"
          }
        }
      }
    }
  }
}
```

And the result:

```
"aggregations" : {
    "my_date_histo" : {
      "buckets" : [ {
        "key_as_string" : "2015-12-24T02:00:00.000Z",
        "key" : 1450922400000,
        "doc_count" : 1462,
        "pcttime" : {
          "values" : {
            "1.0" : 8.61,
            "5.0" : 50.0,
            "25.0" : 259.7801169590643,
            "50.0" : 506.6208791208792,
            "75.0" : 752.2619047619047,
            "95.0" : 946.6333333333332,
            "99.0" : 994.0
          }
        }
      }, {
        "key_as_string" : "2015-12-24T03:00:00.000Z",
        "key" : 1450926000000,
        "doc_count" : 13077,
        "pcttime" : {
          "values" : {
            "1.0" : 10.25333333333333,
            "5.0" : 52.61263736263737,
            "25.0" : 253.4788589832068,
            "50.0" : 501.97747099662007,
            "75.0" : 752.0346378551421,
            "95.0" : 949.8349473684209,
            "99.0" : 988.0
          }
        },
        "the_movavg45" : {
          "value" : 466.2075
        },
        "the_movavg50" : {
          "value" : 506.6208791208792
        }
      }, {
```

There are no 45( nor 45.0) in `pcttime.values` of percentiles metric aggregation. How can the pipeline got the value of `pcttime.45`?
</description><key id="124326319">15707</key><summary>pipeline agg could return value of a not-exists buckets_path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chenryn</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-12-30T09:40:04Z</created><updated>2016-03-01T08:55:44Z</updated><resolved>2016-03-01T08:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:14:59Z" id="170376677">@colings86 could you take a look please?
</comment><comment author="colings86" created="2016-01-11T10:26:46Z" id="170501124">This is a quirk of the percentiles and percentile_ranks aggregations. The underlying algorithms that calculates the percentile values (TDigest and HDRHistogram) do this by storing a histogram which can be queried for any value, not just the values asked for in the request. The `metricname` in the buckets_path is passed directly to the algorithm to return a result so the buckets_path will get a result for any percentile value (between 0 and 100). I guess this is technically a bug but IMO its not a bug that really causes any harm (hence why I called it a quirk at the start of this comment) so I am wary of adding in extra code to protect against something that doesn't really cause a problem (maybe others have a different view on this?). 

Incidently, to access percentile values with a dot in them (e.g. 99.9th percentile) you can use square bracket notation like this:

```
"buckets_path" : "pcttime[99.9]",
```
</comment><comment author="chenryn" created="2016-01-13T02:45:59Z" id="171140364">Thank you, and I think it's a good hidden feature for percentiles and percentile_ranks aggregations.
Maybe more good if describe the percentiles feature and square bracket style of `bucket_path` in doc.
</comment><comment author="clintongormley" created="2016-03-01T08:55:44Z" id="190619028">Documentation about the alternate buckets_path syntax has been added.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multi_fields across types cause MapperParsingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15706</link><project id="" key="" /><description>When using a multi_field across types in the same index, Elasticsearch fails with the following exception during indexing:

```
{
  "took": 482,
  "errors": true,
  "items": [
    {
      "index": {
        "_index": "logstash-2015.09.20",
        "_type": "nginx",
        "_id": "AU_x3-TaGFA8no6QjiSO",
        "_version": 1,
        "_shards": {
          "total": 1,
          "successful": 1,
          "failed": 0
        },
        "created": true,
        "status": 201
      }
    },
    {
      "index": {
        "_index": "logstash-2015.09.20",
        "_type": "apache",
        "_id": "AU_x3-TaGFA8no6QjiSB",
        "status": 400,
        "error": {
          "type": "mapper_parsing_exception",
          "reason": "failed to parse",
          "caused_by": {
            "type": "illegal_state_exception",
            "reason": null
          }
        }
      }
    }
  ]
}
```

And on the server:

```
MapperParsingException[failed to parse]; nested: IllegalStateException;
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:146)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:276)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:483)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:465)
[...]
Caused by: java.lang.IllegalStateException
    at org.elasticsearch.index.mapper.FieldMapper.updateFieldType(FieldMapper.java:365)
    at org.elasticsearch.index.mapper.FieldMapper$MultiFields.updateFieldType(FieldMapper.java:604)
    at org.elasticsearch.index.mapper.FieldMapper.updateFieldType(FieldMapper.java:369)
    at org.elasticsearch.index.mapper.FieldMapper.updateFieldType(FieldMapper.java:50)
    at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:613)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:430)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:251)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:114)
    ... 17 more
```

Steps to reproduce (see also attached files):

```
curl -XPUT localhost:9200/_template/logstash -d @template.txt
curl -s -XPOST localhost:9200/_bulk --data-binary @bulk_data.txt
```

The reason is that ES tries to reuse the multi_field `ip.raw` from the type `nginx` in the type `apache` but somehow fails to "copy" the settings.

The problem was originally found by @LeeDr from the Kibana team. This is a regression that was introduced with commit id 3015eb3.

[template.txt](https://github.com/elastic/elasticsearch/files/74695/template.txt)
[bulk_data.txt](https://github.com/elastic/elasticsearch/files/74696/bulk_data.txt)
</description><key id="124320025">15706</key><summary>multi_fields across types cause MapperParsingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2015-12-30T08:39:36Z</created><updated>2016-01-10T17:26:11Z</updated><resolved>2016-01-04T09:03:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-01-04T09:03:26Z" id="168615959">Fixed via #15715
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>incorrect issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15705</link><project id="" key="" /><description /><key id="124318709">15705</key><summary>incorrect issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oillio</reporter><labels /><created>2015-12-30T08:22:52Z</created><updated>2015-12-30T08:48:43Z</updated><resolved>2015-12-30T08:47:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>When use dynamic scripts or how use dynamic scripts better?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15704</link><project id="" key="" /><description>Hello,

I am using **dynamic scripts** in elasticsearch **1.5.2**&#65292;but I discover the **dynamic scripts** query is so slow, like this:

```
...
scriptParams.put("shopId", 789);
script("doc['shopId'].value == shopId", scriptParams);
...
```

It will costs almost **1000+ms**&#65292;but if i use **filter query**&#65292;only need **40~60ms**&#12290;

What's the mistake? When or How use **dynamic scripts** rightly? 

Thanks &amp;&amp; Regards.
</description><key id="124317524">15704</key><summary>When use dynamic scripts or how use dynamic scripts better?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ihaolin</reporter><labels /><created>2015-12-30T08:06:07Z</created><updated>2016-01-04T14:33:20Z</updated><resolved>2016-01-04T14:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-01-04T14:33:18Z" id="168692485">This is more a question for the forums so I'm going to close it but I'll answer it any way.

A query/filter will use the index. A dynamic script will visit each do and run the check. Even though `doc` uses a column store its going to be much slower than using the index. Dynamic scripts are useful for lots of stuff. Say you wanted to run a one-off report and wanted to get all of the `shopId`s that had a length of 10 or something. Or you wanted to do a scripted aggregation or something. They have their use, but a filter is always going to be faster.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>what is the total score for each query?can it be in a range? or at least i can calculate the range myself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15703</link><project id="" key="" /><description>ive got a problem when im trying to custom the score processing.
im going to use "(_score/TotalScore)CustomTotalScoreweight" to calculate my own score.
i think this can map the _score to a controllable score space.

TotalScore should be the max score the elasticsearch could give in one query.
CustomTotalScore is the max score i set. it could be 100, 50 or any other scores that i give.

the finally score could be the sum of the result of this full formula and the result of other formula.

but now, i cant find the TotalScore the elasticsearch give.

so, what is the totalscore in query?
::: ive analysed the "explain" from elasticsearch to the query, im finding that, the total score is calculated with the score of each term. im wondering, if i can caculate the total score myself of each query when i know the term number and the total score for each term.

so, i think the term number of each query can be easily obtained, but still, what could the total score of each term be in one query?
</description><key id="124306063">15703</key><summary>what is the total score for each query?can it be in a range? or at least i can calculate the range myself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q11112345</reporter><labels /><created>2015-12-30T05:30:02Z</created><updated>2016-01-11T03:04:43Z</updated><resolved>2016-01-10T18:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:13:15Z" id="170376594">You can't rely on the score being in any particular range. it just doesn't work that way.  You could use a function score query to do what you want though
</comment><comment author="q11112345" created="2016-01-11T02:56:00Z" id="170421216">i think i can figure out the total score now.
i disable tf and fieldNorm calculation. the score is only calculated by idf

and idf = (log(1 + total_document/term_documents))^2

if i know total_documents and set term_documents to 1, then i will get the max value of idf.

just like idf=log(1+300/1) ^2, maybe this could be max_idf=log(1+total_document) ^2
and the total idf above will be 2.478...

the question is , how can i get the total_document from es.

it seems that the total_document in the calculation does not equal to the total document stored in  es.

maybe this is not correct. i have another method:

for a specific term, its idf is the same in each document score calculation.
then the total socre is term_idf(1) + term_idf(2) + ... term_idf(N)
1 2 ... N is the term analysed by the es search analyzer,
the max point will be the document which contains all the term.

its easy to do this after disable the tf calculation....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix multi-field support for GeoPoint types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15702</link><project id="" key="" /><description>This PR fixes multi-field support in `BaseGeoPointFieldMapper` by passing an externalValueContext to the multiField parser. Unit testing is added to ensure coverage.

closes #15701 
</description><key id="124275962">15702</key><summary>Fix multi-field support for GeoPoint types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha1</label></labels><created>2015-12-29T22:16:11Z</created><updated>2016-09-08T15:23:52Z</updated><resolved>2015-12-31T17:04:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-30T10:20:57Z" id="167973319">lgtm
</comment><comment author="clintongormley" created="2016-01-10T17:36:33Z" id="170373387">@nknize Could you update the docs as well please: https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-point.html
</comment><comment author="stevewillard" created="2016-09-08T15:23:52Z" id="245634512">I know this was merged several months ago, but is this part of any 2.x release, or is this fix only in 5.x?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-Fields not working for GeoPoint type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15701</link><project id="" key="" /><description>Reproduce with the following:

``` javascript
  "mappings" : {
    "type": {
      "properties": {
        "location": {
          "type": "geo_point",
          "fields": {
            "geohash" : {
              "type" : "geo_point",
              "geohash_precision" : 12,
              "geohash_prefix" : true
            },
            "latlon" : {
              "type" : "geo_point",
              "lat_lon" : true
            }
          }
        }
      }
    }
  }
```

``` javascript
{  "location" : [-0.1485188, 51.5250666] }
```

Throws the following error:

``` javascript
{
   "error": {
      "root_cause": [
         {
            "type": "parse_exception",
            "reason": "geo_point expected"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "failed to parse",
      "caused_by": {
         "type": "parse_exception",
         "reason": "geo_point expected"
      }
   },
   "status": 400
}
```
</description><key id="124275132">15701</key><summary>Multi-Fields not working for GeoPoint type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2015-12-29T22:07:37Z</created><updated>2015-12-31T17:04:29Z</updated><resolved>2015-12-31T17:04:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add test that cluster state update tasks are executed in order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15700</link><project id="" key="" /><description>This commit adds a test that ensures that cluster state update tasks
are executed in order from the perspective of a single thread.

Closes #15483 
</description><key id="124249838">15700</key><summary>Add test that cluster state update tasks are executed in order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-29T18:24:57Z</created><updated>2016-01-06T19:49:18Z</updated><resolved>2016-01-06T19:47:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-01-06T12:51:57Z" id="169317102">looks good in general. Left some comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature to set a ttl at the index level rather than only at the document level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15699</link><project id="" key="" /><description>Hi -- 
According to what I'm hearing in the community, there is a feature request in to allow for setting a ttl at the index level rather than only at the doc level.  See here: https://discuss.elastic.co/t/set-a--ttl-value-at-the-index-level/31668/3 where theuntergeek -- Aaron Mildenstein says "there is no index-level TTL presently. There's a feature request issue out for it (I forget the number), but it has not yet been added."  and https://discuss.elastic.co/t/index-level-ttl-how-do-we-track-this-feature-request/37775 where warkolm (Mark Walkom) responded to my request about how to track such a feature with "It'd be in here - https://github.com/elastic/elasticsearch/issues/1.  If you find it, please link it here so others can benefit :smile: (I couldn't find it!)"

I've checked in issues and features and am not finding anything.  Can someone please point me to where to track this feature request?  I'm surprised there isn't more noise about this one, since it would make index maintenance much simpler.

Thanks,
Casie
</description><key id="124249042">15699</key><summary>Feature to set a ttl at the index level rather than only at the document level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">casieowen</reporter><labels /><created>2015-12-29T18:17:36Z</created><updated>2015-12-29T19:00:03Z</updated><resolved>2015-12-29T18:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-12-29T18:41:42Z" id="167850667">The existing feature request is at https://github.com/elastic/elasticsearch/issues/2114.  It was closed because [curator](https://github.com/elastic/curator) can already do this and it's fairly easy to do so ([delete](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/delete.html) + [index selection](https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index-selection.html)).  I'm going to close this issue as it seems like it's an exact duplicate of https://github.com/elastic/elasticsearch/issues/2114.  If the existing curator functionality doesn't fill your needs, feel free to +1 and/or comment in there and we can always consider reopening it.
</comment><comment author="casieowen" created="2015-12-29T19:00:03Z" id="167853544">Thanks. Yes, I know curator can do it, but setting a ttl at the index level at time of creation is way more efficient.  Will +1 and/or comment in 2114,
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is TransportAddress not Serializable anymore in 2.x ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15698</link><project id="" key="" /><description>Not sure if this is the right place to be posing user@ questions, but I see that org.elasticsearch.common.transport.TransportAddress is not Serializable anymore in ES 2.x.  Is that correct ? What are the alternatives if I need TransportAddress to be Serializable as it used to be in v1.x ?
</description><key id="124239180">15698</key><summary>Is TransportAddress not Serializable anymore in 2.x ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smarthi</reporter><labels /><created>2015-12-29T17:05:37Z</created><updated>2015-12-31T11:40:56Z</updated><resolved>2015-12-31T11:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="smarthi" created="2015-12-31T11:40:56Z" id="168179970">Sorry for the noise, figured it out. Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>clean up test-framework dependencies in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15697</link><project id="" key="" /><description>master is ok, but 2.x really needs some love here:
- elasticearch tests jar depends on `lucene-test-framework` but uses  a _different_ `randomized-testing` version for no good reason, its just outdated. 
- there are complicated maven exclusions that are really unnecessary, and I'm not sure they are complete, possibly dragging in an ant.jar or similar at the moment.

we should upgrade lucene (https://issues.apache.org/jira/browse/LUCENE-6953) and just depend on `lucene-test-framework` and use its version. 
</description><key id="124213920">15697</key><summary>clean up test-framework dependencies in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.3.0</label></labels><created>2015-12-29T13:47:06Z</created><updated>2016-03-01T08:40:33Z</updated><resolved>2016-03-01T08:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-29T14:00:10Z" id="167795440">+1
</comment><comment author="clintongormley" created="2016-03-01T08:40:26Z" id="190613736">this appears to have been done. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es2.1.1 keep throw DocumentAlreadyExistsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15696</link><project id="" key="" /><description>RemoteTransportException[[node1][node1:9300][indices:data/write/index]]; nested: DocumentAlreadyExistsException[[config][4.3.0]: document already exists];
Caused by: [.kibana][[.kibana][0]] DocumentAlreadyExistsException[[config][4.3.0]: document already exists]
        at org.elasticsearch.index.engine.InternalEngine.innerCreateNoLock(InternalEngine.java:432)
        at org.elasticsearch.index.engine.InternalEngine.innerCreate(InternalEngine.java:390)
        at org.elasticsearch.index.engine.InternalEngine.create(InternalEngine.java:362)
        at org.elasticsearch.index.shard.IndexShard.create(IndexShard.java:531)
        at org.elasticsearch.index.engine.Engine$Create.execute(Engine.java:810)
        at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1073)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:170)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</description><key id="124200536">15696</key><summary>es2.1.1 keep throw DocumentAlreadyExistsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-12-29T11:53:02Z</created><updated>2016-01-10T18:28:41Z</updated><resolved>2016-01-10T18:28:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:28:40Z" id="170378467">This looks like a Kibana issue, not an Elasticsearch issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ICU Plugin in reference guide</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15695</link><project id="" key="" /><description>This documentation lives now in plugins documentation at https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu.html.

We don't need a copy in analysis reference guide.
</description><key id="124191755">15695</key><summary>Remove ICU Plugin in reference guide</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Analysis ICU</label><label>docs</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-29T10:24:38Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2015-12-30T10:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-30T10:56:56Z" id="167976646">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>boost parameter in match_all query doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15694</link><project id="" key="" /><description>It said, "The _score can be changed with the boost parameter:", but it's not.

According to MatchAllDocsQuery.sumOfSquaredWeights(), it just returns boost \* boost.

``` java
public float sumOfSquaredWeights() {
      queryWeight = getBoost();
      return queryWeight * queryWeight;
    }
```

hence, queryNorm is just 1.0 / boost,
and score is always 1.0, since boost \* queryNorm is 1.0.
</description><key id="124190752">15694</key><summary>boost parameter in match_all query doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">socurites</reporter><labels /><created>2015-12-29T10:13:57Z</created><updated>2016-01-10T18:27:53Z</updated><resolved>2016-01-10T18:27:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:27:53Z" id="170378207">The queryNorm isn't always 1.0.  For instance, try the following and change the `boost` parameter from 1 to 10:

```
PUT t/t/1
{
  "foo": "one"
}

PUT t/t/2
{
  "foo": "two"
}


GET t/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match_all": {
            "boost": 1
          }
        },
        {
          "match": {
            "foo": "one"
          }
        }
      ]
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when using the delete-by-query plugin from the TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15693</link><project id="" key="" /><description>I installed the delete-by-query plugin on the elasticsearch server and verified that it works fine using the REST interface:

``` JSON
DELETE /index/type/_query?routing=18
{
  "query": { 
    "term": {
      "id": 1558
    }
  }
}
```

However when I try to use the delete-by-query plugin from the `TransportClient`  I get an NPE. 

``` JAVA
DeleteByQueryResponse response = new DeleteByQueryRequestBuilder(client, DeleteByQueryAction.INSTANCE)
  .setIndices(index)
  .setTypes(type)
  .setRouting(18)
  .setQuery(QueryBuilders.termQuery("id", id))
.execute().actionGet();
```

Exception: 

``` JAVA
java.lang.NullPointerException
    at org.elasticsearch.client.transport.support.TransportProxyClient$1.doWithNode(TransportProxyClient.java:58)
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:212)
    at org.elasticsearch.client.transport.support.TransportProxyClient.execute(TransportProxyClient.java:55)
    at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:283)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
    at org.elasticsearch.action.deletebyquery.DeleteByQueryRequestBuilder.execute(DeleteByQueryRequestBuilder.java:172)
```

I also added `plugin.types=org.elasticsearch.plugin.deletebyquery.DeleteByQueryPlugin` to the `TransportClient` settings.

This is with the 2.1.1 version of the Java API and ES Server
</description><key id="124183269">15693</key><summary>NPE when using the delete-by-query plugin from the TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timeu</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2015-12-29T08:57:32Z</created><updated>2016-07-13T13:29:45Z</updated><resolved>2016-01-18T22:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timeu" created="2015-12-29T11:18:18Z" id="167767762">Ok I think this is due to #13055. I will try the `addPlugin`  function and see if it works. 
</comment><comment author="timeu" created="2015-12-29T14:36:35Z" id="167801514">Using the `addPlugin` method solves the NPE. 

``` JAVA
Client client = TransportClient.builder().settings(settings())
                .addPlugin(DeleteByQueryPlugin.class)
                .build()
```

A better error message would be more helpful for the cases when the user forgets to add a plugin. 
Issue can be closed 
</comment><comment author="clintongormley" created="2016-01-10T18:01:49Z" id="170375135">@rjernst would it be possible to throw a more meaningful exception here, rather than an NPE?
</comment><comment author="rjernst" created="2016-01-11T21:34:54Z" id="170697855">We could add an explicit check in 2.x for `plugin.types` in settings when building the transport client, and give an error suggesting to use `addPlugin`? I don't think we should have this indefinitely, but at least for 2.x.
</comment><comment author="clintongormley" created="2016-01-12T10:42:22Z" id="170871169">+1
</comment><comment author="rjernst" created="2016-01-18T22:08:58Z" id="172667444">This was fixed in #15943.
</comment><comment author="NicolasYDDER" created="2016-07-13T13:29:45Z" id="232354972">Hi folks,

I got an NPE with the 2.3 version if `addPlugin` is not define with the `DeleteByQueryPlugin.class`
According this issue it should be fixed, right ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix phrase suggest collate example to use correct script context/syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15692</link><project id="" key="" /><description>Closes #15675
</description><key id="124179496">15692</key><summary>Fix phrase suggest collate example to use correct script context/syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-29T08:23:09Z</created><updated>2016-01-11T13:09:37Z</updated><resolved>2016-01-05T08:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>check if tmpfile exists before delete it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15691</link><project id="" key="" /><description>I think it's better to check if a tmpfile exista before we delete it
</description><key id="124175331">15691</key><summary>check if tmpfile exists before delete it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">baishuo</reporter><labels><label>won't fix</label></labels><created>2015-12-29T07:22:01Z</created><updated>2015-12-29T09:10:13Z</updated><resolved>2015-12-29T09:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-12-29T09:01:43Z" id="167750951">Hi @baishuo!

first of all, thanks for your contribution. But in my opinion, it is better not to check before-hand and leave it as is. Otherwise this might pose a security risk (also known as [file access race condition](https://www.owasp.org/index.php/File_Access_Race_Condition:_TOCTOU)). What do you think?
</comment><comment author="s1monw" created="2015-12-29T09:08:18Z" id="167752954">I don't think we should do this. if this code is executed and the file doesn't exists it's an error condition?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot not failing gracefully when disk quota exceeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15690</link><project id="" key="" /><description>Have a scenario (ES 1.7.1) in the field where the disk quota is exceeded (OpenVZ).  As a result, the snapshot failed shortly after initialization:

```
[2015-12-20 20:00:41,479][WARN ][snapshots                ] [node_name] failed to create snapshot [full_snap:2015_12_20_20_00_02]
org.elasticsearch.snapshots.SnapshotCreationException: [full_snap:2015_12_20_20_00_02] failed to create snapshot
    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:260)
    at org.elasticsearch.snapshots.SnapshotsService.beginSnapshot(SnapshotsService.java:290)
    at org.elasticsearch.snapshots.SnapshotsService.access$600(SnapshotsService.java:92)
    at org.elasticsearch.snapshots.SnapshotsService$1$1.run(SnapshotsService.java:211)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.FileNotFoundException: /es_snapshots/indices/index_name/snapshot-2015_12_20_20_00_02 (Disk quota exceeded)
    at java.io.FileOutputStream.open(Native Method)
    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:221)
    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:171)
    at org.elasticsearch.common.blobstore.fs.FsBlobContainer.createOutput(FsBlobContainer.java:93)
    at org.elasticsearch.repositories.blobstore.BlobStoreRepository.initializeSnapshot(BlobStoreRepository.java:247)
    ... 6 more
```

However, it status remains in_progress status instead of gracefully recovering and reporting an error:

```
{"snapshots":[{"snapshot":"2015_12_20_20_00_02","version_id":1070199,"version":"1.7.1","indices":["list_of_indices],"state":"IN_PROGRESS","start_time":"2015-12-21T04:00:02.952Z","start_time_in_millis":1450670402952,"failures":[],"shards":{"total":0,"failed":0,"successful":0}}]}
```

1450670402952 = 12/20/2015, 8:00:02 PM (local time)
</description><key id="124163761">15690</key><summary>Snapshot not failing gracefully when disk quota exceeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>bug</label></labels><created>2015-12-29T04:58:10Z</created><updated>2016-01-19T08:22:27Z</updated><resolved>2016-01-19T08:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:22:47Z" id="170377115">@ywelsch could you have a look at this please?
</comment><comment author="ywelsch" created="2016-01-14T17:43:57Z" id="171719423">I looked at the code and, indeed, there is a possibility for this situation to happen (also on master). I will open a PR with a fix in the next days.
</comment><comment author="ywelsch" created="2016-01-19T08:22:27Z" id="172773076">I was wrong in my initial analysis. In 2.0 this situation cannot occur anymore. As this is 1.x only, occurring only in very rare circumstances and not a severe bug (next snapshot can properly be made), I'll close this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type name is highlighted when searching in specific type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15689</link><project id="" key="" /><description>I have a `restaurant` index, and a type named `japanese`. So I use the following URL for searching:
`http: //localhost:9200/restaurant/japanese/_search`

And my query body is:

```
{
    "query": {
        "match": {
            "title": {
                "query": "sushi"
            }
        }
    },
    "highlight": {
        "fragment_size": 70,
        "no_match_size": 70,
        "number_of_fragments": 1,
        "require_field_match": false,
        "fields": {
            "title": {},
            "description": {}
        }
    }
}
```

It just search through the title field with 'sushi', but the type name 'japanese' is highlighted as well.
Is this a BUG, or did I do something wrong?

ES version is 2.0.0.
</description><key id="124160672">15689</key><summary>Type name is highlighted when searching in specific type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">kweima</reporter><labels><label>:Highlighting</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-29T04:09:12Z</created><updated>2016-01-12T16:08:31Z</updated><resolved>2016-01-12T16:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="toyama0919" created="2016-01-06T05:00:35Z" id="169222365">me too.
I use Elasticsearch-2.1.1.
</comment><comment author="nik9000" created="2016-01-06T13:46:52Z" id="169326449">I imagine that its a bug. I haven't tried it myself but it looks like it from your description. Can you work around it by specifying a `highlight_query`?
</comment><comment author="nik9000" created="2016-01-06T14:18:34Z" id="169333535">Confirmed as a bug:

```
curl -XDELETE localhost:9200/test?pretty
curl -XPOST 'localhost:9200/test/japanese?pretty&amp;refresh' -d'{"foo": "japanese sushi"}'
curl -XPOST localhost:9200/test/japanese/_search?pretty -d'{
  "query": {
    "match": {
      "foo": "sushi"
    }
  },
  "highlight": {
    "require_field_match": false,
    "fields": {
      "foo": {}
    }
  }
}'
```

Spits out:

```
        "foo" : [ "&lt;em&gt;japanese&lt;/em&gt; &lt;em&gt;sushi&lt;/em&gt;" ]
```

Which it really shouldn't.
</comment><comment author="nik9000" created="2016-01-06T14:20:02Z" id="169333861">I've confirmed you can work around this by setting require_field_match to true or by supplying the original query as a highlight_query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve thirdPartyAudit check, round 3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15688</link><project id="" key="" /><description>- Fix "internal runtime" check to no longer have false positives (e.g. com.sun jersey, ldap, etc)
- remove `missingClasses` boolean. Each missing class is a problem just like any other problem and must be added to `thirdPartyAudit.excludes`.
- scan the `runtime` configuration when possible instead of `test`: this is more valuable because test dependencies can pollute the classpath and hide the fact that e.g. classes are missing at runtime.
- Fail if an exclusion is defined, but "nothing is wrong" with it. This ensures there is no extra bogus excludes, e.g. that `build.gradle`-s accurately document each module's current state of (in)sanity.
- Do some basic documentation of issues, cleanup some low hanging fruit (e.g. remove unsafe-using stuff completely from `lang-groovy`, add missing (tiny) asm-tree to complete classpath for `lang-expressions` and `lang-plan-a`). This reduces the size of the ES distribution by 2MB as well.
- Add basic docs

```
# Checks that we run against bytecode of third-party dependencies
#
# Be judicious about what is denied here: MANY classes will be subject
# to these rules, so please try to keep the false positive rate low!
#
# Each third party .class failing checks will need to be explicitly
# listed in the module's build.gradle file:
#
#   thirdPartyAudit.excludes = [
#    // uses internal java api: sun.misc.Unsafe
#    'org.foo.Bar',
#    // missing class!
#    'com.missing.dependency.WTF',
#    // ...
#   ]
#
# Wildcards are not allowed, excludes must be exact. The build also fails with
# the message "Invalid exclusions, nothing is wrong with these classes" if
# extraneous classes are in the excludes list, this ensures the list is
# up-to-date, and that each module accurately documents the evil things
# that its dependencies do.
#
# For more information, look at ThirdPartyAuditTask.groovy in buildSrc/
```
</description><key id="124159217">15688</key><summary>Improve thirdPartyAudit check, round 3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-29T03:50:27Z</created><updated>2016-01-10T18:00:52Z</updated><resolved>2015-12-29T14:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-29T10:02:01Z" id="167759434">man lots of manual work you did there! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove @Inject from rest handlers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15687</link><project id="" key="" /><description>Do this by adding a single @Inject to NetworkModule. This gives us
a string to pull on: move parameters from injected to constructor.

This is an incremental step to cleaning up our reliance on guice and should
be minimally invasive.

Closes #15685
</description><key id="124142936">15687</key><summary>Remove @Inject from rest handlers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>review</label></labels><created>2015-12-28T23:57:58Z</created><updated>2016-03-08T17:29:01Z</updated><resolved>2016-03-08T13:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-29T00:07:26Z" id="167681908">I'm still running tests now. On a slow machine today....
</comment><comment author="rjernst" created="2015-12-29T01:45:15Z" id="167692584">We just got rid of RestModule! Why can't the rest actions be created in NetworkModule?
</comment><comment author="nik9000" created="2015-12-29T03:42:55Z" id="167711093">&gt; Why can't the rest actions be created in NetworkModule?

I guess it could. I pulled them out initially because I wanted to work on rest stuff in isolation from the transport stuff. I can smoosh it back together but I kind of like rest stuff registered in a RestModule and transport stuff NetworkModule. NetworkModule just looks so much more cohesive now.
</comment><comment author="rjernst" created="2015-12-29T07:35:36Z" id="167739615">&gt; I kind of like rest stuff registered in a RestModule and transport stuff NetworkModule

The entire point of the refactoring that removed RestModule was Rest is really network. Transport and rest requests aren't really any different, they all have to do with communicating with ES over the network (and while the transport actions are not yet registered in NetworkModule, I mentioned in the PR that it was my intention to do that in a follow up).
</comment><comment author="s1monw" created="2015-12-29T09:47:46Z" id="167757687">&gt; The entire point of the refactoring that removed RestModule was Rest is really network. Transport and rest requests aren't really any different, they all have to do with communicating with ES over the network (and while the transport actions are not yet registered in NetworkModule, I mentioned in the PR that it was my intention to do that in a follow up).

while I agree I think we can do this in a followup? This is already awesome and worth a push as it is? :)
</comment><comment author="s1monw" created="2015-12-29T09:49:08Z" id="167757804">oh nevermind, now I see it's adding RestModule back, yeah I think we should never ever again add modules back, can you just fold it into networkModule?
</comment><comment author="nik9000" created="2015-12-29T13:55:43Z" id="167794832">&gt; oh nevermind, now I see it's adding RestModule back, yeah I think we should never ever again add modules back, can you just fold it into networkModule?

I'll do it but I don't get it. NetworkModule without the rest stuff it just sets up the transport implementations and leaves the ticklish bits to something else. But its a separate thing. I'll put it back.
</comment><comment author="nik9000" created="2016-01-04T13:37:18Z" id="168677690">I believe I've addressed all the comments, @s1monw.
</comment><comment author="nik9000" created="2016-01-04T17:06:25Z" id="168737278">Rebased. Sorry for the trouble.
</comment><comment author="nik9000" created="2016-01-12T00:38:48Z" id="170743559">I'm going to rebase again. Any interest in another review?
</comment><comment author="nik9000" created="2016-01-12T14:37:59Z" id="170930789">Rebased!
</comment><comment author="nik9000" created="2016-03-08T13:11:43Z" id="193779712">This is super out of date now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replica shards must be failed before primary shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15686</link><project id="" key="" /><description>As failing primaries also fail associated replicas, we must fail replicas first so that their nodes are properly added to ignore list.

Test failure:

http://build-us-00.elastic.co/job/es_core_master_regression/4241/
</description><key id="124128271">15686</key><summary>Replica shards must be failed before primary shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-28T21:15:25Z</created><updated>2016-01-10T18:20:01Z</updated><resolved>2015-12-28T21:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T21:29:38Z" id="167661592">LGTM - thanks @ywelsch 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove guice from Rest*Action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15685</link><project id="" key="" /><description>If we're going to remove `@Inject` from the node level stuff we have to start somewhere. I will start with `Rest*Action` classes because it looks reasonably easy.
</description><key id="124103362">15685</key><summary>Remove guice from Rest*Action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>v6.0.0</label></labels><created>2015-12-28T18:24:54Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aanchal9" created="2017-04-19T12:07:27Z" id="295242103">Is this still an issue or is it resolved?</comment><comment author="nik9000" created="2017-04-19T12:12:03Z" id="295244052">I believe this is resolved. I'll double check but I believe we can close
it.

On Wed, Apr 19, 2017, 8:08 AM aanchal9 &lt;notifications@github.com&gt; wrote:

&gt; Is this still an issue or is it resolved?
&gt;
&gt; &#8212;
&gt; You are receiving this because you were assigned.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/15685#issuecomment-295242103&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AANLoiFvrgyzVpFECZWzFpLa759oZfNGks5rxfkCgaJpZM4G7-LK&gt;
&gt; .
&gt;
</comment><comment author="aanchal9" created="2017-04-19T12:57:07Z" id="295260253">Ok thanks.

On Wed, Apr 19, 2017 at 5:42 PM, Nik Everett &lt;notifications@github.com&gt;
wrote:

&gt; I believe this is resolved. I'll double check but I believe we can close
&gt; it.
&gt;
&gt; On Wed, Apr 19, 2017, 8:08 AM aanchal9 &lt;notifications@github.com&gt; wrote:
&gt;
&gt; &gt; Is this still an issue or is it resolved?
&gt; &gt;
&gt; &gt; &#8212;
&gt; &gt; You are receiving this because you were assigned.
&gt; &gt; Reply to this email directly, view it on GitHub
&gt; &gt; &lt;https://github.com/elastic/elasticsearch/issues/15685#
&gt; issuecomment-295242103&gt;,
&gt; &gt; or mute the thread
&gt; &gt; &lt;https://github.com/notifications/unsubscribe-auth/
&gt; AANLoiFvrgyzVpFECZWzFpLa759oZfNGks5rxfkCgaJpZM4G7-LK&gt;
&gt; &gt; .
&gt; &gt;
&gt;
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/15685#issuecomment-295244052&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AMn5nSRi5UVGPAtETZTB_yF15-6cfN1Zks5rxfo1gaJpZM4G7-LK&gt;
&gt; .
&gt;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove object notation for core types.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15684</link><project id="" key="" /><description>Closes #15388
</description><key id="124099137">15684</key><summary>Remove object notation for core types.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:CRUD</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-12-28T17:57:30Z</created><updated>2016-01-10T18:13:57Z</updated><resolved>2015-12-30T08:58:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-28T18:14:12Z" id="167618182">LGTM. Want to add an explanation to the commit message before merging so its easier to read in the logs? I didn't know what the object notation was either and I had to read the linked issue. It'd make following git history easier if you just pasted a quick blurb.
</comment><comment author="jpountz" created="2015-12-28T18:32:23Z" id="167622956">Sure. Will do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change in meaning of network.host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15683</link><project id="" key="" /><description>It is common for a setting of 0.0.0.0 to mean bind to all hosts.  The latest release of Elasticsearch 2.1.1 is treating 0.0.0.0 a an error condition and it binds to the first address or some other odd approach in the list of interfaces returned by ifconfig on linux and solaris.

It also doesn't appear to support a non ip name. If I use a hostname i get an error. if I use a dns name I get an error. It only worked with an IP address.
</description><key id="124080172">15683</key><summary>Change in meaning of network.host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">digitalrinaldo</reporter><labels><label>:Network</label><label>feedback_needed</label></labels><created>2015-12-28T15:23:31Z</created><updated>2016-02-29T20:59:29Z</updated><resolved>2016-02-29T20:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-12-29T16:57:07Z" id="167829662">Can you provide a bit more detail?  Post your config (or the relevant networking bits), the system information+versions (OS, java), and what you're seeing vs what you're expecting?  I just successfully tested 2.1.1 using network.host set to 0.0.0.0 and separately as an explicit hostname with no problems.  Note that we don't support Solaris (https://www.elastic.co/support/matrix).  Also, this may be a question of setup better suited for our forums at https://discuss.elastic.co/ .  If that's the case, we can close this one once you open a thread there.
</comment><comment author="clintongormley" created="2016-02-29T20:59:29Z" id="190387401">No further feedback, plus the docs for networking have been greatly improved.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run scripts for solaris are using -E option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15682</link><project id="" key="" /><description>The solaris scripts are using grep with the -E option. This option is not supported in the default distribution.

[ Dec 28 15:11:22 Disabled. ]
[ Dec 28 15:12:11 Enabled. ]
[ Dec 28 15:12:11 Executing start method ("/scratch/elasticsearch/bin/elasticsearch"). ]
grep: illegal option -- E
</description><key id="124079942">15682</key><summary>Run scripts for solaris are using -E option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">digitalrinaldo</reporter><labels /><created>2015-12-28T15:21:15Z</created><updated>2015-12-29T16:59:51Z</updated><resolved>2015-12-29T16:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-12-29T16:59:51Z" id="167830289">This is a duplicate of https://github.com/elastic/elasticsearch/issues/15628 .  I'm closing this in favor of that one.

For the time being, we don't support Elasticsearch on Solaris: https://www.elastic.co/support/matrix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>force reroute of (primary) shards from a specific node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15681</link><project id="" key="" /><description>(perhaps this is possible - I couldn't figure out how.. - for reference, I'm using 1.7.4)

My situation is that I have a large cluster that I want to make smaller.  I would like to be able to route primary shards away from this node so when I take the node out, it minimizes the effect.  I realize I can do this manually with the reroute API but it doesn't support wildcards so I have do it all manually which is really tedious.   The levels of need/impact are:
1) primary shards with no replicas - I know it's not recommended, but we have some logging where the data is very big and we don't want to have to double the disk space.  If a machine goes down, we can reindex.  Let's say I have a 5 machine cluster and I want to take a node, I'd like to be able to move all the shards the other machines.  
2) primary shards with replicas - even though I won't lose any data (theoretically) there is still some "hiccuping" while the cluster determines that this primary shard is not available and a different shard is promoted and another replica is initialized.  Just turning off the node puts the cluster into a "failure mode" which is should be able to recover from.
3) replica shards, this causes less because you just need to create a new replica - I could live without this, but it would be nice.

The basic principle is that we shouldn't force the user to enter a failure mode with a planned node removal.  Failure modes in Elasticsearch can be problematic and there is no reason to go there needlessly.

I had though https://www.elastic.co/guide/en/elasticsearch/reference/current/shard-allocation-filtering.html might help - but it only seems to deal with allocation - not forcing reallocation.
</description><key id="124077683">15681</key><summary>force reroute of (primary) shards from a specific node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yehosef</reporter><labels /><created>2015-12-28T14:59:39Z</created><updated>2015-12-28T23:35:39Z</updated><resolved>2015-12-28T21:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-28T21:36:39Z" id="167662419">It is all explained here:
https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-filtering.html

Please use discuss.elastic.co to ask these kinds of questions ;-)
</comment><comment author="yehosef" created="2015-12-28T23:35:39Z" id="167679346">I did bring it up in discuss.elastic.co - https://discuss.elastic.co/t/force-shard-reallocation/36839, thank you very much ;-)

I tried the advice there and it did not _reallocate_ - but it did prevent allocation.  

But it's still not clear.  I read the docs more carefully (thanks) While the docs says:

&gt; The typical use case for cluster-wide shard allocation filtering is when you want to decommision a node, and you would like to move the shards from that node to other nodes in the cluster before shutting it down.

It then goes on to say:

&gt; Cluster-wide shard allocation filtering works in the same way as index-level shard allocation filtering (see Index Shard Allocation for details).

And the index level shard allocation did _not_ reallocate shards.  

The docs for 1.7 at https://www.elastic.co/guide/en/elasticsearch/reference/1.7/index-modules-allocation.html seems to say this should work:

&gt; The provided settings can also be updated in real time using the update settings API, allowing to "move" indices (shards) around in realtime.

But it didn't work for me.   I change the settings for a particular index to exclude on a node (tried both ip and host exclusion) and waited 15-20 minutes on an idle cluster but nothing happened.  

Any suggestions?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Appending data in Elasticsearch indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15680</link><project id="" key="" /><description>I have 2 servers where Elasticsearch Indexes are stored with same index name but different data on both the indexes, now there is requirement of putting that data into one server, so I want to append the data of both the indexes(present on 2 different servers) into single index, how i can do that.
Currently, I am using this configuration for appending the data.

input {
  elasticsearch {
   hosts =&gt; "server1:9200"
   index =&gt; "logstash-2015.12.16"
  }
}
filter {
 mutate {
  remove_field =&gt; [ "@version", "@timestamp" ]
 }
}
output {
 elasticsearch {
   host =&gt; "localhost"
   port =&gt; 9200
   protocol =&gt; "http"
   manage_template =&gt; false
   index =&gt; "logstash-2015.12.16"
 }
}
</description><key id="124075616">15680</key><summary>Appending data in Elasticsearch indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aakash21</reporter><labels /><created>2015-12-28T14:38:38Z</created><updated>2015-12-28T14:47:08Z</updated><resolved>2015-12-28T14:47:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T14:47:08Z" id="167582777">please use our discuss forum https://discuss.elastic.co/  for questions like this, the issue tracker is only for bugs, improvements or feature tracking 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make text parsing less lenient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15679</link><project id="" key="" /><description>It now requires that the parser is on a value.
</description><key id="124075343">15679</key><summary>Make text parsing less lenient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-28T14:36:01Z</created><updated>2016-01-10T18:17:20Z</updated><resolved>2015-12-28T15:47:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T14:40:09Z" id="167581969">LGTM - thx @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed improvements for BalancedShardsAllocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15678</link><project id="" key="" /><description>This PR contains some speed improvements for `BalancedShardsAllocator` (relates to #6372):
- Removal of the pattern `node.addShard() -&gt; calculate weight -&gt; node.removeShard()` which is expensive as, beside map lookups, it invalidates caching of precomputed values in `ModelNode` and `ModelIndex`. Replaced by adding an additional parameter to the weight function which accounts for the added / removed shard.
- For rebalancing an index, only consider nodes that currently have a shard of that index or where the index can be allocated. This allows to prune a large number of nodes in case of hot/warm setup. Implemented by adding allocation decider rule based on `IndexMetaData`.
- Removed some dead code
- Some caching (avoid reresolving values in map all the time) / simplification of code

I made smaller commits for each improvement to simplify the review process.
</description><key id="124069745">15678</key><summary>Speed improvements for BalancedShardsAllocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-28T13:44:13Z</created><updated>2016-03-16T19:42:27Z</updated><resolved>2015-12-28T18:38:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T14:21:10Z" id="167579908">left one comment - LGTM otherwise
</comment><comment author="ywelsch" created="2015-12-28T18:38:00Z" id="167624301">Addressed comment and added labels. Thanks for the review!
</comment><comment author="clintongormley" created="2016-01-10T18:18:32Z" id="170376874">Nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>License upgrade issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15677</link><project id="" key="" /><description>Running elasticsearch 2.1.1 on ox s 10.11.2 : 
issues&#65306;
[ERROR][license.plugin.core      ] [Baroness Blood] 
# 
# License will expire on [Wednesday, January 27, 2016]. If you have a new license, please update it.
# Otherwise, please reach out to your support contact.
# 
# Commercial plugins operate with reduced functionality on license expiration:
# \- marvel
# \- The agent will stop collecting cluster and indices metrics

Updating license:
curl -XPUT -u myusername 'http://localhost:9200/_license' -d @license.json

{
    &#8221;error":{
        "root_cause"[{
            "type":"parse_exception",
            "reason":"Failed to derive xcontent"
        }],
            "type":"parse_exception",
            "reason":"Failed to derive xcontent"
    },
    "status":400
}
</description><key id="124048905">15677</key><summary>License upgrade issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jhush</reporter><labels><label>feedback_needed</label></labels><created>2015-12-28T10:26:58Z</created><updated>2016-02-13T22:46:57Z</updated><resolved>2016-02-13T22:46:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2016-01-06T21:45:27Z" id="169472613">I've raised this against our internal Marvel issue tracker, I can't provide a link though sorry.
</comment><comment author="GlenRSmith" created="2016-01-07T05:45:46Z" id="169560468">@jhush can you please paste the contents of license.json **with the obviously unique bits redacted**?
</comment><comment author="harictengazel" created="2016-02-03T14:03:43Z" id="179251138">Any update ? 
</comment><comment author="clintongormley" created="2016-02-03T14:08:40Z" id="179255946">@harictengazel given that we haven't received the contents of the license file that we asked for above, no, no update.  Make sure that the json file actually contains the license.  

Try using `curl --trace-ascii - -XPUT...`  to debug
</comment><comment author="clintongormley" created="2016-02-13T22:46:57Z" id="183767144">No further feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix FunctionScore equals/hashCode to include minScore and friends</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15676</link><project id="" key="" /><description /><key id="124047549">15676</key><summary>Fix FunctionScore equals/hashCode to include minScore and friends</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-28T10:15:30Z</created><updated>2016-01-11T13:09:10Z</updated><resolved>2015-12-28T11:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-28T10:17:47Z" id="167537841">Wow this is a very bad bug. LGTM
</comment><comment author="s1monw" created="2015-12-28T10:59:23Z" id="167542408">@jpountz and suddenly there are more problems, can you take another look? i will also port the second commit to master
</comment><comment author="jpountz" created="2015-12-28T11:04:01Z" id="167542777">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase Suggester breaks in 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15675</link><project id="" key="" /><description>_overview_: a phrase suggester query that works in 1.5.1 breaks in 2.0.0-beta2 and 2.1.1 an it shouldn't be related to the new "everything's a phrase" approach to queries and filters.

The following uses the python elasticsearch client.

``` python
body = {
    "mappings": {
      "movie": {
        "properties": {
          "genres": {
            "properties": {
              "name": { 
                "type": "string",
                "index": "not_analyzed"}}},
          "title": {
            "type": "string",
            "analyzer": "english",
            "copy_to":["suggestion"]},
          "suggestion": {
            "type": "string"}}}}}
es.indices.create("tmdb",body=body)
```

Then index some documents. Then:

```
query_body = { 
  "fields": ["title"],
  "query": {
    "match": {"title":"star trec"}},
  "suggest": { "title_completion": {
    "text": "star trec",
    "phrase": {
      "field": "suggestion",
      "max_errors": 2,
      "collate": {
        "query": { 
          "match_phrase": {
            "title" : "{{suggestion}}"
          }
        }}}}}}

es.search(index="tmdb",body=query_body,size=2)
```

The above works in 1.5.1 but not 2.0.0-beta2 or 2.1.1. The 2.1 documentation is roughly the same as 1.5 and indicates that this should work.

The error I get is

```
TransportError: TransportError(500, {u'failed_shards': [{u'node': u'nHzvQPtVR2KW7NMeOuvQLg', u'index': u'tmdb', u'reason': {u'reason': u'unexpected field [match_phrase]', u'type': u'script_parse_exception'}, u'shard': 0}], u'root_cause': [{u'reason': u'unexpected field [match_phrase]', u'type': u'script_parse_exception'}], u'grouped': True, u'reason': u'all shards failed', u'phase': u'query_fetch', u'type': u'search_phase_execution_exception'})
```
</description><key id="124002933">15675</key><summary>Phrase Suggester breaks in 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JnBrymn-EB</reporter><labels><label>docs</label></labels><created>2015-12-28T00:29:45Z</created><updated>2016-01-05T08:27:18Z</updated><resolved>2016-01-05T08:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T09:04:35Z" id="167517722">can you elaborate what works and what doesn't? I wonder if you can try this with an index with only 1 shard just to make sure that the new hashing we use in 2.0 doesn't influence the results since this suggester really relies on per shard frequencies etc.?
</comment><comment author="JnBrymn-EB" created="2015-12-29T03:50:37Z" id="167712254">Sorry @s1monw. I've added the error to the original comment to keep everything together. (I meant to include the error but apparently I got distracted and didn't add it.)

Also I retried the experiment with a single shard as you suggested and I had the same results.
</comment><comment author="s1monw" created="2015-12-29T08:23:40Z" id="167743742">@JnBrymn-EB I see - this is a documentation problem. The syntax you are using is deprecated. I opened a PR to fix this: #15692  can you give it a try?
</comment><comment author="JnBrymn-EB" created="2015-12-29T20:20:02Z" id="167868683">Work's like a charm. Thanks @s1monw.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation and specs for _msearch/template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15674</link><project id="" key="" /><description>Issue: https://github.com/elastic/elasticsearch/issues/10885
PR: https://github.com/elastic/elasticsearch/pull/12414

It was merged with no documentation, no specs and no rest integration tests, it is unclear whether it's a separate API or how the client implementation should look like.
</description><key id="123995545">15674</key><summary>Add documentation and specs for _msearch/template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">HonzaKral</reporter><labels><label>docs</label></labels><created>2015-12-27T20:40:47Z</created><updated>2017-01-20T22:38:08Z</updated><resolved>2017-01-20T22:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:32:52Z" id="170379860">@MaineC could you take care of this please
</comment><comment author="MaineC" created="2016-01-11T09:50:46Z" id="170491496">Makes sense, will do.
</comment><comment author="fedotovalex" created="2016-03-24T21:32:58Z" id="201033300">Bump. Any progress on this one? It would be nice to see this feature available in the clients.
</comment><comment author="MaineC" created="2016-03-29T13:18:48Z" id="202889814">Wrong button (actual intention was to cancel the comment I had started typing)- sorry for the noise.
</comment><comment author="karmi" created="2017-01-16T08:48:55Z" id="272803845">Is there any news here? I have an issue submitted for the Ruby client (https://github.com/elastic/elasticsearch-ruby/issues/328), and would like to use the JSON specs and YAML tests for implementing the feature in the client.</comment><comment author="javanna" created="2017-01-16T09:12:39Z" id="272808478">@karmi as far as I can see spec and docs were committed, I guess this issue should have been closed. @MaineC can you comment on this?</comment><comment author="MaineC" created="2017-01-16T10:35:12Z" id="272826390">@karmi What other than what was merged in #17382 do you need? I'd appreciate feedback from those using the feature.</comment><comment author="javanna" created="2017-01-16T10:40:23Z" id="272827545">one more thing, I wonder if the spec and rest tests should move to the mustache module, which is where the code now is.</comment><comment author="karmi" created="2017-01-20T11:38:00Z" id="274052590">@MaineC, sorry, I have overlooked the #17382 issue, seeing it now!</comment><comment author="karmi" created="2017-01-20T11:38:54Z" id="274052716">@HonzaKral, should we close this?</comment><comment author="HonzaKral" created="2017-01-20T22:38:08Z" id="274198816">Closed via #17382

My apologies for taking so long, it completely slipped. My bad and thanks @MaineC !</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES java very high CPU usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15673</link><project id="" key="" /><description>My CPU usage 120~, it's so high. How to reduce my CPU usage
my server have 2 Gb 
I run elasticsearch 2.1
</description><key id="123967730">15673</key><summary>ES java very high CPU usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tommoholmes10</reporter><labels /><created>2015-12-27T08:37:17Z</created><updated>2015-12-27T13:45:42Z</updated><resolved>2015-12-27T13:45:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-27T13:45:42Z" id="167414250">I recommend taking this question to the [Elastic Discourse forums](https://discuss.elastic.co) and only opening an issue here if the dicussion there reveals an issue in Elasticsearch. When you do post there, you'll want to provide more detail.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fresh install of 2.1.1 only allowing 2 nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15672</link><project id="" key="" /><description>When my 3rd and 4th nodes try to join the cluster, after the Node Started message in the logs, they return No Known Master Node. 
</description><key id="123945703">15672</key><summary>Fresh install of 2.1.1 only allowing 2 nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kawikao</reporter><labels /><created>2015-12-26T19:57:06Z</created><updated>2015-12-26T20:16:37Z</updated><resolved>2015-12-26T20:16:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kawikao" created="2015-12-26T20:16:37Z" id="167357889">My bad. my /etc/default/elasticsearch settings on additional nodes was incorrect.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Specify custom char_filters/tokenizer/token_filters in the analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15671</link><project id="" key="" /><description>Add parser for custom char_filters/tokenizer/token_filters in analyze API request.

You don't need to create temporary index to analyze a text with new char_filter/tokenizer/token_filter settings.
Example: 

```
curl -XGET 'localhost:9200/_analyze' -d '
{
  "tokenizer" : "whitespace",
  "filters" : ["lowercase", {"type": "stop", "stopwords": ["a", "is", "this"]}],
  "text" : "this is a test"
}'
```

Closed #8878
</description><key id="123916746">15671</key><summary>Specify custom char_filters/tokenizer/token_filters in the analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2015-12-26T08:34:18Z</created><updated>2016-07-21T11:18:24Z</updated><resolved>2016-07-21T02:46:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-08T00:15:47Z" id="207146515">@rjernst can you take a look at this?
</comment><comment author="rjernst" created="2016-04-11T20:05:49Z" id="208532832">@johtani I left some comments on char filter that I think apply to tokenizer and token filter as well.
</comment><comment author="johtani" created="2016-04-15T02:09:46Z" id="210251976">@rjernst Thanks for reviewing.
Pushed fixing your comment.
Can you take a look again?
</comment><comment author="johtani" created="2016-06-04T09:36:10Z" id="223746535">Rebase master for fixing code style issue and reviewing
</comment><comment author="jpountz" created="2016-06-17T06:57:43Z" id="226695134">I like the feature but I don't like the hack of using a json string as the analysis component name and then checking with `startsWith("{")` to know whether it is a name or a definition. Could we instead use a wrapper class to make it unambiguous, something like below:

``` java
static class NameOrDefinition {
  // exactly one of these two members is not null
  public final String name;
  public final Map&lt;String, Object&gt; definition;

  NameOrDefinition(String name) {
    this.name = Objects.requireNotNull(name);
    this.definition = null;
  }

  NameOrDefinition(Map&lt;String, Object&gt; definition) {
    this.name = null;
    this.definition = Objects.requireNotNull(definition);
  }
}
```
</comment><comment author="johtani" created="2016-07-11T08:04:49Z" id="231667258">@jpountz Replace checking with `startsWith("{")` to NameOrDefinition.
Using Settings class in NameOrDefinition.

Can you review again?
</comment><comment author="jpountz" created="2016-07-11T09:29:37Z" id="231685319">Thanks @johtani, this looks better to me. I left some more comments.
</comment><comment author="johtani" created="2016-07-15T04:00:55Z" id="232854751">@jpountz Thanks for reviewing. Fixed your comment and added breaking changes documentation.
</comment><comment author="jpountz" created="2016-07-18T08:28:54Z" id="233268584">@johtani I left some comments but I think it is close!
</comment><comment author="johtani" created="2016-07-19T12:07:16Z" id="233612341">@jpountz @clintongormley Thanks for your comments!
Fix your comment and add the note in analyze API documents!
Can you check my English :p ?
</comment><comment author="jpountz" created="2016-07-19T12:53:47Z" id="233622856">LGTM
</comment><comment author="clintongormley" created="2016-07-19T14:06:57Z" id="233643847">One small suggestion otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to set percentage influence of each function in function score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15670</link><project id="" key="" /><description>The functions score query gives a good facility to implement various aspects of the score , but then its not exactly giving control over the influence of each function.
For eg: , for the function below - 

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "decay": {
            "gauss": {
              "date": {
                "origin": "2013-09-17",
                "scale": "10d",
                "offset": "5d",
                "decay": 0.5
              }
            }
          }
        },
        {
          "field_value_factor": {
            "field": "popularity",
            "factor": 1.2,
            "modifier": "sqrt",
            "missing": 1
          }
        },
        {
          "random_score": {}
        },
        {
          "script_score": {
            "script": {
              "lang": "lang",
              "params": {
                "param1": 2,
                "param2": 3
              },
              "inline": "_score * doc['rating'].value / pow(param1, param2)"
            }
          }
        }
      ]
    }
  }
}
```

There are 4 functions and they dictate the end score. Here , either of the function like the script_score function can eat up all the influence of the score. That is the value of the script_score might be in range of 1000 to 2000 and value of the decay would be between 0 and 1. Hence the influence of the decay function is not exactly passed on to the final score , rather its the script_score that eats up all the influence , rest of the functions might have little or no influence on the final score. 

To fix this , it might be useful to have a influenceScore factor per function which tells what percentage of the end score , this function should influence.
For eg: , the above query can be rewriten as

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "influenceScore": "40%",
          "decay": {
            "gauss": {
              "date": {
                "origin": "2013-09-17",
                "scale": "10d",
                "offset": "5d",
                "decay": 0.5
              }
            }
          }
        },
        {
          "influenceScore": "30%",
          "field_value_factor": {
            "field": "popularity",
            "factor": 1.2,
            "modifier": "sqrt",
            "missing": 1
          }
        },
        {
          "influenceScore": "10%",
          "random_score": {}
        },
        {
          "influenceScore": "20%",
          "script_score": {
            "script": {
              "lang": "lang",
              "params": {
                "param1": 2,
                "param2": 3
              },
              "inline": "_score * doc['rating'].value / pow(param1, param2)"
            }
          }
        }
      ]
    }
  }
}
```

Here , we will have a influenceScore per function which dictates the influence of each function. This will help us in further fine tuning the score.
</description><key id="123914128">15670</key><summary>Ability to set percentage influence of each function in function score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2015-12-26T06:57:16Z</created><updated>2017-07-03T20:11:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T09:09:54Z" id="167518285">can't you just use the `weight` attribute of a function? instead of `influenceScore : 20%` you do `weight: 0.2`
</comment><comment author="s1monw" created="2016-01-07T16:46:37Z" id="169723140">@Vineeth-Mohan ping
</comment><comment author="Vineeth-Mohan" created="2016-01-08T02:51:39Z" id="169877447">Hello @s1monw , 

Let me walk through the motivation here.
Lets say , I am running the following query

```
{
  "explain": true,
  "query": {
    "function_score": {
      "functions": [
        {
          "field_value_factor": {
            "field": "dateOfJoining",
            "modifier": "sqrt",
            "missing": 1
          }
        },
        {
          "random_score": {}
        }
      ],
      "score_mode": "sum"
    }
  }
}
```

With this , I am seeing the following results - 

```
{
  "_explanation": {
    "value": 1113172.4,
    "description": "function score, product of:",
    "details": [
      {
        "value": 1,
        "description": "ConstantScore(*:*), product of:",
        "details": [
          {
            "value": 1,
            "description": "boost"
          },
          {
            "value": 1,
            "description": "queryNorm"
          }
        ]
      },
      {
        "value": 1113172.4,
        "description": "Math.min of",
        "details": [
          {
            "value": 1113172.4,
            "description": "function score, score mode [sum]",
            "details": [
              {
                "value": 1113172.2,
                "description": "function score, product of:",
                "details": [
                  {
                    "value": 1,
                    "description": "match filter: *:*"
                  },
                  {
                    "value": 1113172.2,
                    "description": "field value function: sqrt(doc['dateOfJoining'].value?:1.0 * factor=1.0)"
                  }
                ]
              },
              {
                "value": 0.17271471,
                "description": "function score, product of:",
                "details": [
                  {
                    "value": 1,
                    "description": "match filter: *:*"
                  },
                  {
                    "value": 0.17271471,
                    "description": "random score function (seed: 519896482)"
                  }
                ]
              }
            ]
          },
          {
            "value": 3.4028235e+38,
            "description": "maxBoost"
          }
        ]
      },
      {
        "value": 1,
        "description": "queryBoost"
      }
    ]
  }
}
```

As you can see the score by field_value_factor is always shadowing the score given by random_score , as in random_score has no relevance here.

My motivation for this issue came from this problem.
One solution would be to use the weight to  normalize the values , and that is how its currently done.
But then looking into the range of values for each function and deciding the weight score for all the functions and finding them manually seems like a hard case. And these weights that are computed manually might not be applicable across all documents. 

The percentage suggestion was based on this , but I am finding it difficult to pen the maths behind the same. Only solution i found was to find the range of each score given by each function across all document and use that for percentage influence. But as scoring is per document , that wont be feasible. 

Let me know your thoughts on the subject.
</comment><comment author="s1monw" created="2016-01-08T08:19:59Z" id="169929371">@Vineeth-Mohan I can see what you are saying and I admit it can be challenging. I personally don't see a good way to apply a general way of normalization here. I see the function score feature as a toolset of primitives that lets / forces the user to ensure that each element of the equation has it's relevant weight etc. I wonder if other ie. @brwe has some ideas?
</comment><comment author="brwe" created="2016-01-08T14:07:35Z" id="170012318">It seems to me this is a case of "learning to rank". To find proper weights you would need to know what the expected ordering of result for different queries would be and the tune the weights accordingly. Without that the only thing you can do now is guess. 
We currently have no way to scale functions either so they are comparable. This is something you will have to do in advance. Just in case you don't know aggregations help for that, see example below. Other than that we currently have no support to tune the weights automatically. 

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "random_score": {},
          "weight": 1000
        }
      ]
    }
  },
  "aggs": {
    "score_agg": {
      "histogram": {
        "script": "_score",
        "interval": 50
      }
    },
    "score_stats": {
      "extended_stats": {
        "script": "_score"
      }
    }
  }
}
```
</comment><comment author="gkop" created="2016-02-09T19:44:07Z" id="182026878">@brwe another benefit of what's proposed here if I understand correctly is one could use score_mode `avg` which could be weighted by `influenceScore` to generate scores nicely distributed on a range. This can be accounted for now in the client by passing `influenceScore` as a param to our script (which multiplies it by the nicely distributed intermediate score), and keeping a running sum of the influence scores, but it would be quite amazing if the server took care of it for us instead.

In fact, on reading the docs at https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html , I initially interpreted that we could pass `weight` in as an option to any kind of function_score score function to obtain this behavior, that just made sense to me. Alas I misunderstood.
</comment><comment author="clintongormley" created="2016-02-29T20:58:03Z" id="190386362">The only other thing I could suggest is to apply a min/max score to each function, eg you could force `gauss` to be between 0 and 2.  With that, the weights would be easier to adjust.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to check cluster settings is worked after update?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15669</link><project id="" key="" /><description>I execute 

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
            "index.translog.interval" : "10s"
    }
}'
```

it returns`{"acknowledged":true,"persistent":{},"transient":{}}`
It seems no worked, and i use `curl -XGET localhost:9200/_cluster/settings` to check it,but the same things returns as above,How can i to check it?
</description><key id="123885569">15669</key><summary>How to check cluster settings is worked after update?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kewn21</reporter><labels /><created>2015-12-25T15:03:37Z</created><updated>2015-12-25T15:40:45Z</updated><resolved>2015-12-25T15:40:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GlenRSmith" created="2015-12-25T15:40:45Z" id="167246030">@kewn21 [index.translog.interval](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-translog.html) is an [index setting](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html), not a [cluster setting](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html).

[The Elastic Discuss forums](https://discuss.elastic.co/) or the elasticsearch IRC channel are the preferred places to seek this type of assistance. Github issues should be reports of erroneous behavior, preferably those that have already been investigated sufficiently to be certain they aren't user errors.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot perform 'bulk delete' for '_all' types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15668</link><project id="" key="" /><description>Hello!

I try to delete few documents by their IDs. For each document I know ID, index name, but I don't know type name which contains that document. So I try to execute request with "_type" equals "_all":

``` js
POST http://localhost:9200/_bulk
{"delete":{"_index":"index1","_type":"_all","_id":"f031686f-3e88-4447-a726-fa777284efb5"}}
```

but receive:

``` js
{
   "took": 1,
   "errors": true,
   "items": [1],
   "0":  {
      "delete": {
         "_index": "index1",
         "_type": "_all",
         "_id": "f031686f-3e88-4447-a726-fa777284efb5",
         "status": 400,
         "error": "InvalidTypeNameException[mapping type name [_all] can't start with '_']"
      }
   }
}
```

Wildcards doesn't work too ("_type" equals "myType*"):

```
POST http://localhost:9200/_bulk
{"delete":{"_index":"index1","_type":"myType*","_id":"f031686f-3e88-4447-a726-fa777284efb5"}}
```

receive:

``` js
{
   "took": 1,
   "errors": false,
   "items": [
      {
         "delete": {
            "_index": "index1",
            "_type": "myType*",
            "_id": "f031686f-3e88-4447-a726-fa777284efb5",
            "_version": 1,
            "status": 404,
            "found": false
         }
      }
   ]
}
```

How can I delete documents using Balk API if I know only their IDs and index name? Is it possible? Or I have to use another approach?
</description><key id="123876076">15668</key><summary>Cannot perform 'bulk delete' for '_all' types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AlexMAS</reporter><labels /><created>2015-12-25T12:04:00Z</created><updated>2015-12-25T13:03:25Z</updated><resolved>2015-12-25T13:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-25T13:03:24Z" id="167234790">Please ask questions on discuss.elastic.co.

You can look at delete by query plugin. Might help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rolling upgrade from 2.1.0 to 2.1.1 failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15667</link><project id="" key="" /><description>stop one of them, upgrade it and restart it. below is discovery log and it seems the server can connected to all of other nodes.

```
[2015-12-25 10:57:00,313][TRACE][discovery.zen.ping.unicast] [192.168.200.190] [119] received response from {#zen_unicast_3#}{192.168.200.196}{192.168.200.196:9390}: [ping_response{node [{192.168.200.190}{QDVnI6x7RQ2u3T3zBsKQgw}{192.168.200.190}{192.168.200.190:9390}{max_local_storage_nodes=20, master=false}], id[351], master [null], hasJoinedOnce [false], cluster_name[21test]}, ping_response{node [{192.168.200.190}{QDVnI6x7RQ2u3T3zBsKQgw}{192.168.200.190}{192.168.200.190:9390}{max_local_storage_nodes=20, master=false}], id[352], master [null], hasJoinedOnce [false], cluster_name[21test]}, ping_response{node [{192.168.200.190}{QDVnI6x7RQ2u3T3zBsKQgw}{192.168.200.190}{192.168.200.190:9390}{max_local_storage_nodes=20, master=false}], id[353], master [null], hasJoinedOnce [false], cluster_name[21test]}, ping_response{node [{192.168.200.190}{QDVnI6x7RQ2u3T3zBsKQgw}{192.168.200.190}{192.168.200.190:9390}{max_local_storage_nodes=20, master=false}], id[354], master [null], hasJoinedOnce [false], cluster_name[21test]}, ping_response{node [{192.168.200.190}{QDVnI6x7RQ2u3T3zBsKQgw}{192.168.200.190}{192.168.200.190:9390}{max_local_storage_nodes=20, master=false}], id[355], master [null], hasJoinedOnce [false], cluster_name[21test]}, ping_response{node [{192.168.200.190}{QDVnI6x7RQ2u3T3zBsKQgw}{192.168.200.190}{192.168.200.190:9390}{max_local_storage_nodes=20, master=false}], id[356], master [null], hasJoinedOnce [false], cluster_name[21test]}, ping_response{node [{192.168.200.196}{FzxGsQclQ22izuwoi-wTpQ}{192.168.200.196}{192.168.200.196:9390}], id[1081], master [{192.168.200.196}{FzxGsQclQ22izuwoi-wTpQ}{192.168.200.196}{192.168.200.196:9390}], hasJoinedOnce [true], cluster_name[21test]}]
```

but it keep logging: [DEBUG][action.admin.indices.create] [192.168.200.190] no known master node, scheduling a retry

cluster toplogy: 4 nodes, three of them are master&amp;data node, the other is data node.
the config is below:

```
cluster.name: 21test
node.name: 192.168.200.190
node.master: false
node.data: true
path.data: /export/Data/elasticsearch/es219290
path.logs: /export/Logs/elasticsearch/es219290
path.plugins: /export/plugins/elasticsearch/es219290
bootstrap.mlockall: true
network.host: 192.168.200.190
http.port: 9290
transport.tcp.port: 9390
discovery.zen.ping.unicast.hosts: ["192.168.200.191:9390","192.168.200.192:9390","192.168.200.196:9390"]
discovery.zen.minimum_master_nodes: 3
indices.memory.index_buffer_size: 20%
script.engine.groovy.inline.aggs: on
index.translog.sync_interval: 30s
index.translog.durability: async
index.refresh_interval: 180s
index.max_merged_segment: 1g
node.max_local_storage_nodes: 20
```
</description><key id="123849030">15667</key><summary>Rolling upgrade from 2.1.0 to 2.1.1 failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Discovery</label><label>feedback_needed</label></labels><created>2015-12-25T03:27:07Z</created><updated>2016-05-24T10:32:31Z</updated><resolved>2016-05-24T10:32:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T20:54:48Z" id="190384870">Hi @makeyang 

Did you figure out what was happening here?  
</comment><comment author="makeyang" created="2016-03-01T03:26:25Z" id="190519702">not yet. just leave the problem there
</comment><comment author="clintongormley" created="2016-03-02T09:45:30Z" id="191159496">Would help to see more of the logs then
</comment><comment author="clintongormley" created="2016-05-24T10:32:31Z" id="221230092">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting Different Index Mapping when creating Index using Node Client in ES 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15666</link><project id="" key="" /><description>Hi All,
I am trying to create an index using NodeClient/TransportClient in ES 2.1 by calling client.index(request).get() and when the index gets created it is having a different Mapping created and does not match with the source

My Source which i provided to Index api : 
"title":{"type":"string"},"description":{"type":"string"},"hits":{"type":"integer"},"columns":{"type":"string"},"sort":{"type":"string"},"version":{"type":"integer"}}"
:
Mapping Generated by ES :
create_mapping [type] with source [{"search":{"properties":{"columns":{"properties":{"type":{"type":"string"}}},"description":{"properties":{"type":{"type":"string"}}},"hits":{"properties":{"type":{"type":"string"}}},"sort":{"properties":{"type":{"type":"string"}}},"title":{"properties":{"type":{"type":"string"}}},"version":{"properties":{"type":{"type":"string"}}}}}}]

As you see every field is converted to PROPERTIES json object . I want it to remain as is the way i added in request . Can you please let me know if its a bug or i have to set any additional value in the request to that mapping should get generated as per the source I provided.
</description><key id="123836479">15666</key><summary>Getting Different Index Mapping when creating Index using Node Client in ES 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ersushantsood</reporter><labels /><created>2015-12-24T22:07:24Z</created><updated>2016-01-01T17:35:42Z</updated><resolved>2015-12-26T02:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ersushantsood" created="2015-12-25T20:52:56Z" id="167261970">Please respond to the issue as it is blocking my implementation, meanwhile I have done the workaround that I am not storing this mapping for the index and let elastic search dynamically create mapping for me 
</comment><comment author="rjernst" created="2015-12-26T02:02:37Z" id="167271145">It looks like you are indexing the mapping as a document. The reason you see "properties" is because the field names that you want are seen as object fields, which contain a field "type", and each type field is then a string.

See the put mapping api:
https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>logs is full of messages: Field data loading is forbidden on firstReceived</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15665</link><project id="" key="" /><description>```
[2015-12-24 18:22:23,097][DEBUG][action.search.type       ] [Honcho] [296715] Failed to execute fetch phase
RemoteTransportException[[Honcho][127.0.0.1:9300][indices:data/read/search[phase/fetch/id]]]; nested: IllegalStateException[Field data loading is forbidden on firstReceived];
Caused by: java.lang.IllegalStateException: Field data loading is forbidden on firstReceived
        at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.fail(DisabledIndexFieldData.java:68)
        at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.empty(DisabledIndexFieldData.java:59)
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:72)
        at org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase.hitExecute(FieldDataFieldsFetchSubPhase.java:100)
        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:178)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:589)
        at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)
        at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-24 18:22:23,097][DEBUG][action.search.type       ] [Honcho] [296714] Failed to execute fetch phase
RemoteTransportException[[Honcho][127.0.0.1:9300][indices:data/read/search[phase/fetch/id]]]; nested: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on firstReceived]; ne
Caused by: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on firstReceived]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is fo
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:82)
        at org.elasticsearch.search.fetch.fielddata.FieldDataFieldsFetchSubPhase.hitExecute(FieldDataFieldsFetchSubPhase.java:100)
        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:178)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:589)
        at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)
        at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Field data loading is forbidden on firstReceived
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:156)
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:76)
        ... 10 more

```
</description><key id="123827245">15665</key><summary>logs is full of messages: Field data loading is forbidden on firstReceived</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mogadanez</reporter><labels /><created>2015-12-24T18:33:50Z</created><updated>2016-01-10T18:44:46Z</updated><resolved>2016-01-10T18:44:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:44:46Z" id="170381194">This DEBUG level log message is telling you that you (or one of your users) keeps trying to do something which isn't supported.  If you don't want to see this message, then change they log level for `search.action.type`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ParseContext.ignoredValue.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15664</link><project id="" key="" /><description>This API is unused.
</description><key id="123808619">15664</key><summary>Remove ParseContext.ignoredValue.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-24T14:29:24Z</created><updated>2015-12-28T11:05:30Z</updated><resolved>2015-12-28T11:05:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-24T14:46:48Z" id="167119927">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove mapping backward compatibilit with pre-2.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15663</link><project id="" key="" /><description>This removes the backward compatibility layer with pre-2.0 indices, notably
the extraction of _id, _routing or _timestamp from the source document when a
path is defined.
</description><key id="123803123">15663</key><summary>Remove mapping backward compatibilit with pre-2.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-24T13:23:34Z</created><updated>2016-01-04T09:05:40Z</updated><resolved>2016-01-04T09:05:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-31T19:46:10Z" id="168238556">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add GitHub.CHEATSHEET to the project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15662</link><project id="" key="" /><description>Many people have some challenges when using Git/GitHub to contribute to the project.

It would be nice to have a GitHub.CHEATSHEET similar to GRADLE.CHEATSHEET.
</description><key id="123794219">15662</key><summary>Add GitHub.CHEATSHEET to the project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">martinstuga</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-12-24T11:34:54Z</created><updated>2016-02-29T20:37:40Z</updated><resolved>2016-02-29T20:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-24T12:00:21Z" id="167099741">If you make one I'll review it. It might make more sense to put it CONTRIBUTING.md because folks having github problems may have not yet forked the code.

When I started contributing a few years ago I followed whatever tutorial I could find online about making PRs and it seemed to work fine. No none of the project owners said I was doing it wrong. So maybe something like this isn't needed? The advantage of the gradle cheetsheet is that maven is **so** common you can talk about the gradle commands using the shared language of maven so its terse.
</comment><comment author="martinstuga" created="2015-12-24T12:14:08Z" id="167103139">@nik9000 Unfortunately I'm not the right person to do that because I'm still having some difficulties using GitHub with a forked project.

Yeah, I completely agree with you about put it in CONTRIBUTING.md (at least the basic commands).
</comment><comment author="clintongormley" created="2016-01-10T18:40:16Z" id="170380978">Hmmm github has extensive good documentation https://help.github.com/

Not sure it makes sense to duplicate this?
</comment><comment author="martinstuga" created="2016-01-11T15:42:46Z" id="170591270">My idea is to provide a step-by-step that includes stuff like update our remote repository from the upstream and so on.
</comment><comment author="javanna" created="2016-01-11T16:22:02Z" id="170602982">Are there specific issues with the way we use github as a project? In that case we should document things better, otherwise I wonder if it makes sense for every single project on github to include (the same?) instructions about using github.
</comment><comment author="clintongormley" created="2016-02-29T20:37:40Z" id="190376648">Agreed - I think the github docs are sufficient here.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhancements to the mustache script engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15661</link><project id="" key="" /><description>Enhancements:
- Added an option at compile time to decide variable values are json encoded or not encoded at all.
- Added support for variable placeholders to lookup values from specific slots in arrays/lists.
</description><key id="123793115">15661</key><summary>Enhancements to the mustache script engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-24T11:20:29Z</created><updated>2016-01-21T17:06:43Z</updated><resolved>2016-01-21T17:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-29T10:36:13Z" id="167762821">@uboness I've updated the PR with your suggestion. There is only one issue with making plain text the default and that is for file based scripts and this is because compile arguments are used as cache key for file based scripts and the logic for loading file based scripts is generic for all script types, for this reason I had to add a hack that if the script type is mustache then the `content_type` is set to `application/json`. First of all this check is in the wrong place, but also this would be bad for plugins that use file based templating in non json content. In all other places it is easy to use decide where to add content type to the compile arguments, basically on all places where `Template` is being used, so I added a helper method for that on ScriptService.

The temporary if statement I added to ScriptService should be removed. Instead we should add an optional syntax that can be added at the beginning of each mustache template that controls the content type. I also think that in the case this optional syntax is absent we should default to `application/json` in the mustache plugin otherwise this would be a breaking change. Then all the changes I added in the last 2 commits can be reverted.
</comment><comment author="uboness" created="2015-12-29T14:36:57Z" id="167801564">wondering... shouldn't the content type be based on the file extension?
</comment><comment author="martijnvg" created="2015-12-29T15:05:58Z" id="167806018">@uboness That idea crossed my mind too, but this isn't something we do now. Right now, json encoding applies for all files. In the case that someone is relying on json encoding of the parameters in file based mustache script then this would mean that during upgrade files would have to be renamed too. Also it is kind of weird too that then mustache files would need to have the .json extension.
</comment><comment author="uboness" created="2015-12-29T17:34:49Z" id="167837220">I c... :/... this is really broken IMO... template &amp; scripts are really two different things yet we treat them the same... not sure what's the "right" thing to do here in the current context. IMO the right thing to do is to really separate the two... but that's quite a big change for this PR. @s1monw @rmuir wdyt?
</comment><comment author="martijnvg" created="2015-12-29T20:13:28Z" id="167866922">yes, templates and scripts are two different animals each with their use cases. Mustache templates shouldn't be ran in a script query or aggregation and scripts shouldn't be used to template search requests. 

Maybe we should try to get this in? Because this change does clean things up too. We can do what I did initially, but just replace the `escape_json` with `content_type` and then default to json. 

In the followup PR we should make `Template` not extend from `Script` to begin with and separate the template specific infrastructure from the script service. There are overlapping things like location of templates and script, which can both be inline, file or a document in an index.
</comment><comment author="martijnvg" created="2016-01-14T11:06:04Z" id="171614116">I've updated the PR to change the default content type used for escaping back to `application/json` to make this change less invasive. In the future we should look into splitting of templates from scripts. 

@uboness Would you like to take a look?
</comment><comment author="uboness" created="2016-01-21T15:47:52Z" id="173612350">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception in thread "main" java.lang.IllegalStateException: path.home is not configured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15660</link><project id="" key="" /><description>es_version:2.1
code:
   import static org.elasticsearch.node.NodeBuilder.*;

```
Node node = nodeBuilder().node(); 
Client client = node.client();
```

when i run this application,it gets wrong,why?
Exception in thread "main" java.lang.IllegalStateException: path.home is not configured
    at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:99)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:81)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:135)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)
    at com.wap.sohu.mpaper.util.ElasticsearchUtil.main(ElasticsearchUtil.java:237)
Disconnected from the target VM, address: '127.0.0.1:62666', transport: 'socket'
</description><key id="123790098">15660</key><summary>Exception in thread "main" java.lang.IllegalStateException: path.home is not configured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">selfchanger</reporter><labels /><created>2015-12-24T10:49:27Z</created><updated>2015-12-25T08:10:00Z</updated><resolved>2015-12-25T08:10:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="selfchanger" created="2015-12-24T10:50:51Z" id="167091665">elasticsearch.yml:

# Use a descriptive name for your cluster:

cluster.name: cms-elasticsearch

# Use a descriptive name for the node:

node.name: node-1

# Add custom attributes to the node:

# node.rack: r1

# Path to directory where to store the data (separate multiple locations by comma):

path.home: D:/elasticsearch-2.1.0
</comment><comment author="selfchanger" created="2015-12-25T08:09:57Z" id="167208504">solution:
add "-Des.path.home=D:\elasticsearch-2.1.0" in VM options,
see http://blog.csdn.net/jianjun200607/article/details/49821813#reply
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It's a way to create lucene index, and load to elasticsearch directly?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15659</link><project id="" key="" /><description>I have 1 billion rows to write into elasticsearch every day,so i used bulk api to write data,but the performance was unacceptable.
Can i create lucene index, and load to elasticsearch directly,who has idea?
</description><key id="123786273">15659</key><summary>It's a way to create lucene index, and load to elasticsearch directly?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kewn21</reporter><labels><label>feedback_needed</label></labels><created>2015-12-24T10:19:09Z</created><updated>2016-01-18T09:10:42Z</updated><resolved>2016-01-15T12:36:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-28T10:51:34Z" id="167541698">This is a bit surprising that you can load data much faster in a lucene index than in elasticsearch. Did you verify it?

In any case this is not a scenario that we want to support as it prevents us to enforce any structure of the index.
</comment><comment author="kewn21" created="2016-01-15T01:50:41Z" id="171843200">I'm thinking of feasibility,i heard two people say they realized, they use lucene api to create index in mapreduce,but performance is not good.
</comment><comment author="clintongormley" created="2016-01-15T12:36:25Z" id="171949882">Thanks for the feedback @kewn21. This is not a work flow that we intend to support.
</comment><comment author="kewn21" created="2016-01-18T09:10:42Z" id="172470649">Because our hadoop cluster has sufficient resource to create elasticsearch index,especial we would create more than one index in concurrent,so that it can create index by lucene api in mapreduce,then push them to elasticsearch nodes and create elasticsearch metadatas,the throughput may be bigger than using elasticsearch bulk api.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Threshold coloring based on time interval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15658</link><project id="" key="" /><description>Would it be possible to have a threshold list of values using templates and especially $interval ?

Values in the graph are highly based on the time interval used: we do not have the same number of errors during 1second or during 1 hour. It would be nice to be able to use $interval in the threshold values list for coloring tables or single stats.

```
Threshold: 0,100*$interval,500*$interval
```

That would mean always providing the interval in the same unit. So maybe $interval_ms (as I saw in some other issues) might be more relevant.
</description><key id="123785708">15658</key><summary>Threshold coloring based on time interval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">patjlm</reporter><labels /><created>2015-12-24T10:16:57Z</created><updated>2015-12-24T13:40:36Z</updated><resolved>2015-12-24T11:55:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-24T11:55:38Z" id="167099284">I believe you've posted this issue in the wrong github project. Sorry about that!
</comment><comment author="nik9000" created="2015-12-24T11:56:04Z" id="167099312">Did you mean to post this in https://github.com/elastic/kibana ?
</comment><comment author="patjlm" created="2015-12-24T13:40:36Z" id="167113962">sorry, wrong project indeed... need some sleep...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The parameter `char_filters` is not used in analyze POST api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15657</link><project id="" key="" /><description>Test under 2.0.0 and 2.1.0;
A simple custom analyzer test,use POST analyze api

```
curl -XPOST 'localhost:9200/_analyze' -d '
{
  "char_filters" : ["html_strip"],
  "tokenizer" : "keyword",
  "token_filters":["lowercase"],
  "text" : "this is a &lt;b&gt;test&lt;/b&gt;"
}'
```

as you can see the result below,the char_filters doesn't remove neither `&lt;` or `&gt;`,it seems not working:

```
{"tokens":[{"token":"this is a &lt;b&gt;test&lt;/b&gt;","start_offset":0,"end_offset":21,"type":"word","position":0}]}                                                          
```

now try to test the above config via the GET api again,

```
curl -XGET 'localhost:9200/_analyze?tokenizer=keyword&amp;token_filters=lowercase&amp;char_filters=html_strip' -d 'this is a &lt;b&gt;test&lt;/b&gt;'
```

the result is correct:

```
{"tokens":[{"token":"this is a test","start_offset":0,"end_offset":21,"type":"word","position":0}]}
```
</description><key id="123780974">15657</key><summary>The parameter `char_filters` is not used in analyze POST api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels><label>:Analysis</label><label>adoptme</label><label>bug</label></labels><created>2015-12-24T09:29:41Z</created><updated>2015-12-28T01:54:58Z</updated><resolved>2015-12-28T01:54:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2015-12-28T01:54:58Z" id="167455247">Duplicate. already fixed in 2.2
https://github.com/elastic/elasticsearch/issues/15257
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bad indexing performance of elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15656</link><project id="" key="" /><description>Currently, I'm using elastic search to store and query some logs. We set up a five node elastic search cluster. Among them two indexing nodes and three query nodes. In the indexing node, we have redis, logstash and elasticsearch on both two servers. The elasticsearch uses NFS storage as data store. Our requirement is to index 300 log entries/second. But the best performance I can get from elasticsearch is only 25 log entries/second!
Here's the detailed information in StackOverflow: http://stackoverflow.com/questions/34449405/bad-indexing-performance-of-elasticsearch

My question is:
Can anyone tell me what is elastic search doing and why the indexing is so slow? And is it possible to improve it?
</description><key id="123775047">15656</key><summary>Bad indexing performance of elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talentzxf</reporter><labels /><created>2015-12-24T08:26:59Z</created><updated>2015-12-25T02:27:27Z</updated><resolved>2015-12-24T08:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-12-24T08:50:26Z" id="167070477">Please use our [discuss forum](http://discuss.elastic.co/) for questions like these, we try to use github issues for bugs/features only. 

Important side note: Do not use NFS with Elasticsearch, see [this post](https://www.elastic.co/blog/performance-considerations-elasticsearch-indexing) by Mike McCandless or [The definitive guide](https://www.elastic.co/guide/en/elasticsearch/guide/current/indexing-performance.html)
</comment><comment author="talentzxf" created="2015-12-25T02:27:27Z" id="167179643">I have moved this post to https://discuss.elastic.co/t/bad-indexing-performance-of-elasticsearch/37962
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter/Query support inside Top hits Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15655</link><project id="" key="" /><description>Elasticsearch documentation states that **The top_hits aggregation returns regular search hits, because of this many per hit features can be supported** 

https://www.elastic.co/guide/en/elasticsearch/reference/2.1/search-aggregations-metrics-top-hits-aggregation.html

Crucially, the list includes _Named filters and queries_ But trying to add any filter or query throws SearchParseException: Unknown key for a START_OBJECT. So, it looks like the  documentation is wrong.

Stack overflow question: http://stackoverflow.com/questions/34319337/filter-query-support-in-elasticsearch-top-hits-aggregation
</description><key id="123768906">15655</key><summary>Filter/Query support inside Top hits Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sumitjainn</reporter><labels /><created>2015-12-24T07:07:25Z</created><updated>2017-07-26T14:22:31Z</updated><resolved>2015-12-25T12:19:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-12-24T08:46:25Z" id="167070060">Indeed the `TopHitsParser` does not seem able to parse query/filters via the `SubSearchContext`, but maybe I am misreading the source. Maybe @martijnvg can clarify here.

Thinking if you can try a filter aggregation and nest the top_hits inside of that one?
</comment><comment author="martijnvg" created="2015-12-24T09:16:59Z" id="167075586">@sumitjainn This is for filters and queries that have the `_name` option set in the main query. So hits returned by the `top_hits` agg can included reference to named queries/filters used in the main query.  It isn't possible to add a query/filter to the `top_hits` agg.
</comment><comment author="sumitjainn" created="2015-12-25T10:26:01Z" id="167220364">Thanks for clearing it up @martijnvg. @spinscale Yes, indeed, filter aggregations does the trick, was just curious about the correctness of the documentation.
</comment><comment author="spinscale" created="2015-12-25T12:19:31Z" id="167226120">thx for responding. closing then!
</comment><comment author="digitalkaoz" created="2016-10-06T09:34:53Z" id="251913236">@sumitjainn can you post an example here? trying to do some filtering on the docs of a top_hits aggregation, but didnt have any luck here... maybe @spinscale has an opinion too?
</comment><comment author="dukenguyen" created="2017-03-23T00:58:28Z" id="288585778">I'm also unable to do this.  @sumitjainn or @spinscale , do you have an example please?</comment><comment author="itaydvir" created="2017-07-26T14:22:31Z" id="318068338">@digitalkaoz @dukenguyen - i think they meant to wrap it with filter aggregation.
This is what i did:
```json
"aggregations": {
	"last_document_of_specific_doc_types": {
		"filter": {
			"terms": {
				"doc_types": [161, 162]
			}
		},
		"aggregations": {
			"latest": {
				"top_hits": {
					"size": 1,
					"_source": [
						"creationDate"
					],
					"sort": {
						"creationDate": "desc"
					}
				}
			}
		}
	}
}
```</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch index not analyzed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15654</link><project id="" key="" /><description>I have a message, example:
    a    b
this message beginning with multiple spaces and more than one space between this message.
My mapping set no_analyzed,when search this message,This message has only one space, example:
a b
I don't know why this is.
</description><key id="123764755">15654</key><summary>elasticsearch index not analyzed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yqguodataman</reporter><labels /><created>2015-12-24T06:25:38Z</created><updated>2015-12-24T08:48:39Z</updated><resolved>2015-12-24T08:48:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-12-24T08:48:39Z" id="167070289">Please use our [discuss forum](http://discuss.elastic.co/) for questions like these, we try to use github issues for bugs/features only. You may want to use the [analyze API](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/indices-analyze.html) to test things out and provide a full-blown example there, so other people can reproduce your problem. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restoring specific shard(s) from snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15653</link><project id="" key="" /><description>This is 100% a duplicate of #9984, however it's pretty old and nobody will likely notice that a comment was made on it, so I figure I'd recreate it to give it some attention.

I'd be more than happy to implement this feature, as I found myself wishing it existed for the same reason the OP did all those months ago, however, I'm stuck at a bit of a crossroads when it comes the actual implementation.

Here's a rundown of my understanding of the situation and work involved in this feature. I'd love some feedback on whether I'm right or wrong on some points, completely talking out of my ass, etc.; as well as some advice on what the ES team would consider the preferred approach:

When a snapshot of an index is taken, it dumps blobs of the data of each shard, along with the entirety of an index's metadata (mappings, aliases, etc.) to a blobstore (s3, azure storage, etc.)

Later, when restoring, the restoration process effectively:
1. Replaces an existing index's metadata with that which is in the snapshot if the index already exists in the cluster; if not, it creates a new index with the same name and metadata in the snapshot. (or uses other name/metadata/etc. if the operator specified it in the restore request)
2. Allocates new shards, and fills them with the data in the snapshot's blobs, and sets the index's shard routing table to these "new" restored shards. The old shards, if they existed, somehow get reaped.

In essence, if an index already existed and gets restored from a snapshot, it effectively gets completely replaced with a completely different index containing the snapshot's data, just one that happens to have all the same metadata and naming, etc. of the snapshotted one.

This means that fundamentally, to restore specific shard(s), I just need to create "new" shards for the shards I want to restore, and then update the routing for the index to use those new shards and keep the shards that I'm not restoring untouched.

The RestoreService is very intimately coupled with all of conveniences that the snapshot restore feature gives you such as renaming the index, changing the number of replicas, etc. that (IMO) do not apply to the operation of restoring specific shards. Furthermore, some of these conveniences are only possible because of the fact that a restored index is really just a brand new index. 

In the scenarios that I can imagine for restoring specific shards, one would just want a no-bullshit, replacement of (a) corrupt shard(s), ASAP, without affecting the rest of the index properties. 

Inside [o.e.s.RestoreService.restoreSnapshot$ClusterStateUpdateTask.execute (at lines 266-268)](https://github.com/elastic/elasticsearch/blob/4ea19995cff603ce8472e985c902dd2f8fadebee/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java#L266-L268) (see also, [lines 341-347](https://github.com/elastic/elasticsearch/blob/4ea19995cff603ce8472e985c902dd2f8fadebee/core/src/main/java/org/elasticsearch/snapshots/RestoreService.java#L341-L347)) there's an `ignoredShards` Set that gets generated during the restore operation which is used when the `partial` flag is enabled for a snapshot restore. The `partial` flag tells ES, "if you have any trouble restoring a shard from snapshot for this restore operation, just create an empty one instead of failing the restore". These "troublesome" shards are put into `ignoredShards`.

This means that I am left with two choices:
### 1)

 Modify RestoreService, RestoreRequest, RestoreSnapshotRequest, TransportRestoreSnapshotAction, and probably others to take an `"only_restore_shards": [int]` field.
If this new field is given, it prohibits you from performing metadata changes (by giving the operator a stern talking to if they try to do metadata updates in a shard-only restore via an error message), which should hopefully simplify the implementation a little bit.
Furthermore, an `only_restore_shards` implies a `partial` restore, **with the major caveat being**,
the meaning of a `partial` restore gets changed (see the point under 50/50). After this change, in a `partial` restore, if the index already existed and there are shards in `ignoredShards`, they're actually ignored as opposed to replaced with blanks. i.e., shards IDs that are listed in `ignoreShards` and exist in the index get reused. (keep reading and this will all make sense)

Benefits: 
- The `ignoredShards` Set actually works as expected, and you can fill it with the difference of the set of all shards numbers in the index and the set of the shard numbers you want to restore (i.e., in `only_restore_shards`; meaning it ignores [reuses] the existing [unaffected] ones) and the code makes a bit more intuitive sense.
- Compared to option 2, there's a lot less boilerplate code to be written, and minimal modifications to the API.
- If the snapshot you're restoring from has bad shards, and you've opted for a "partial" restore, instead of you potentially losing the data in those shards, at least some data gets kept. This is great for PoLA in the intuitive sense of "partial". Additionally, an `only_restore_shards` operation implies a "partial" restore in the intuitive sense, which ties into the next point:

50/50:
- The semantics of a `partial` restore get inverted, rather than you getting a "partially complete" index restored, you're "partially restoring" the index. (I want to say that this leans a bit towards a drawback, given that ES has had this semantic for a while now, _buuuuuuuuut_ it's certainly more in line with what you would intuitively expect a "partial" restore to mean)

Drawbacks:
- Due to the previously mentioned intimate coupling, this will make the RestoreService even more complicated than it already is.
- It creates the need for a bunch of additional validations and potential edge cases that will need to be considered inside the Requests and their builders.
- This is my first deep foray into patching ES, and I'm more than confident that I'll probably break something that tests might not catch and nobody would have even thought possible :P 
- More room for potentially odd bugs in RestoreService due to its increased complexity.
### 2)

Create a RestoreShardsService/RestoreShardsRequest/etc./etc./etc. along with a new REST endpoint such as /_snapshots/etc/etc/_restore_shards as well as a corresponding TransportAction.

Benefits:
- Cleaner implementation and separation of concerns by starting from a clean slate and not further complicating RestoreService.
- Less likely to break anything as all the logic is separated out into its own module
- No weird short circuits or retrofits around the whole "an `ignoredShard` is really just replaced with an empty shard" logic.
- Full backwards compatibility (assuming someone uses the current `partial` semantics to their advantage somehow?).
- No need for particularly crazy validations, as the _restore_shards action really just takes a snapshot, a single index and a list of shards to restore from it. As long as the snapshot can satisfy the shards to be restored, there's no reason for it to fail.

50/50:
- You don't get to invert the semantics of a `partial` restore to be more intuitive. On the other hand, this might not be necessary since you can now always cherry pick the shards to restore, meaning a full index restore (which is the only way to do it as is), would only get used in the worst case that your index is totally fucked. In which case, if the shards in the snapshot are bad, you're proper fucked anyway.
- An entire `ClusterService` seems kinda heavy for something that should really be an extension of the RestoreService anyway, but on the other hand RestoreService is a pretty damn big chunk of code.

Drawbacks:
- Tons of boilerplate code that bloats up the codebase for request objects, builders, actions, transport, etc.
- I'd be effectively duplicating a lot of logic that's in RestoreService to perform a very specific operation, which is invariably a code smell.

Thanks for taking the time to read this wall of text, and apologies if it's in poor form to self-bump by making a new issue!
</description><key id="123756709">15653</key><summary>Restoring specific shard(s) from snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iostat</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-24T04:24:38Z</created><updated>2017-02-09T21:20:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T18:57:10Z" id="170381950">@imotov could you take a look at this please?
</comment><comment author="imotov" created="2016-01-11T15:20:24Z" id="170585114">@iostat thank you for the analysis of the issue. I agree, that it would be a valuable feature that would help some users who lost a part of their index due to shard corruptions. I also agree that RestoreService is already more complicated than we would like it to be. The good news is that a lot of complexity in RestoreService comes from managing of the restore lifecycle and we are currently working on extracting some of this lifecycle management logic into a separate TaskManagement service. So, my hope is that when this work is complete, it would be possible to split RestoreService into more manageable pieces. Until then I would definitely not recommend to use this task as the first foray into Elasticsearch because it touches some of the most complex parts of Elasticsearch such as shard allocation and recover/restore processes. 

The placement of the new logic that you described is one issue. Even a bigger issue, in my opinion, is that metadata in a snapshot might not be compatible with metadata of an existing index. That means that it might or might not be OK to restore a few shards. When we restore all shards (even in case of a partial restore) we can wipe out old metadata and replace it with metadata from a snapshot without thinking about it because it's guaranteed that the data in the existing index will be completely gone by the end of the process. When we restore only some shards, it's much trickier. What would you do if during restore you discover that settings and/or mappings of the new index are different. In the case of mappings, we have the mapping merging logic that we could reuse to a large degree. In the case of settings - there is no such mechanism. So, we would need to make a determination about what to do. 

Some settings are ok to merge. For example, if an index in a snapshot has refresh_interval set to 10 sec and the corresponding index in the cluster has refresh_interval of 1 sec and everything else is the same, it's perfectly fine to partially restore the index. The same goes for the number_of_replicas setting. However, if the number_of_shards setting is different we definitely shouldn't restore. It gets even more complicated with analysis settings. For example, different settings for the same analyzer can break the restored index, but if we are just adding a new analyzer, it's perfectly fine to merge the settings. 
</comment><comment author="iostat" created="2016-01-12T01:00:14Z" id="170746936">@imotov: Thanks for the reply! I definitely agree that it's a behemoth of a
task especially for someone like me getting their feet wet with the
codebase, but I thought it was least worth starting a discussion for.

On the point of mismatched settings: I think it's fair to say that if the
number of shards in the snapshot doesn't match the number of shards in the
index, you shouldn't be able to restore that snapshot period. As far as I'm
aware there's no way to reshard an index without creating a new one anyway,
so in that case the snapshot being restored from is effectively from a
completely different index and shouldn't be restored from in the first
place. Likewise for settings such as the routing hash function, analyzer
settings, etc. You could apply this to the extreme and even say if the
destination index was created after snapshot was taken, then to reject the
restore request completely.

I don't really see this feature as an absolute replacement for the existing
restore functionality, it should really be available as a convenience if
anything, for instance in the case that 1) regular backups are being taken
and 2) out of nowhere a shard got corrupted for both primary and replica
and 3) no "major" settings changes occurred since the snapshot was taken.

As far as the TaskManagement service, any way to track the progress of
that? I'd love to revisit this if/when that's implemented, but if it's a
long ways off (next major release or something like that) perhaps it's
worth writing this feature as a separate module and then merging its
functionality with the original RestoreService when that's all refactored.

On Monday, January 11, 2016, Igor Motov notifications@github.com wrote:

&gt; @iostat https://github.com/iostat thank you for the analysis of the
&gt; issue. I agree, that it would be a valuable feature that would help some
&gt; users who lost a part of their index due to shard corruptions. I also agree
&gt; that RestoreService is already more complicated than we would like it to
&gt; be. The good news is that a lot of complexity in RestoreService comes from
&gt; managing of the restore lifecycle and we are currently working on
&gt; extracting some of this lifecycle management logic into a separate
&gt; TaskManagement service. So, my hope is that when this work is complete, it
&gt; would be possible to split RestoreService into more manageable pieces.
&gt; Until then I would definitely not recommend to use this task as the first
&gt; foray into Elasticsearch because it touches some of the most complex parts
&gt; of Elasticsearch such as shard allocation and recover/restore processes.
&gt; 
&gt; The placement of the new logic that you described is one issue. Even a
&gt; bigger issue, in my opinion, is that metadata in a snapshot might not be
&gt; compatible with metadata of an existing index. That means that it might or
&gt; might not be OK to restore a few shards. When we restore all shards (even
&gt; in case of a partial restore) we can wipe out old metadata and replace it
&gt; with metadata from a snapshot without thinking about it because it's
&gt; guaranteed that the data in the existing index will be completely gone by
&gt; the end of the process. When we restore only some shards, it's much
&gt; trickier. What would you do if during restore you discover that settings
&gt; and/or mappings of the new index are different. In the case of mappings, we
&gt; have the mapping merging logic that we could reuse to a large degree. In
&gt; the case of settings - there is no such mechanism. So, we would need to
&gt; make a determination about what to do.
&gt; 
&gt; Some settings are ok to merge. For example, if an index in a snapshot has
&gt; refresh_interval set to 10 sec and the corresponding index in the cluster
&gt; has refresh_interval of 1 sec and everything else is the same, it's
&gt; perfectly fine to partially restore the index. The same goes for the
&gt; number_of_replicas setting. However, if the number_of_shards setting is
&gt; different we definitely shouldn't restore. It gets even more complicated
&gt; with analysis settings. For example, different settings for the same
&gt; analyzer can break the restored index, but if we are just adding a new
&gt; analyzer, it's perfectly fine to merge the settings.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/15653#issuecomment-170585114
&gt; .

## 

## 

Sent from a web browser, why is it a trend to tell people what you used to
send an email?

Ilya Ostrovskiy
</comment><comment author="imotov" created="2016-01-12T01:56:22Z" id="170757133">&gt; You could apply this to the extreme and even say if the destination index was created after snapshot was taken, then to reject the restore request completely.  

The requirement of a snapshot being created after creation of the restored index is irrelevant to success or failure of the partial restore. I think what you are trying to say is we could reject the restore request completely if the index that we restore is not the same as the index that was snapshotted. Yes, we could make this a requirement, but even then there is plenty of scenarios for a snapshot and an index to diverge. So, I don't think it's practical to make the determination about possibility of restore by simply looking at historical origins of indices.

&gt; I don't really see this feature as an absolute replacement for the existing restore functionality, it should really be available as a convenience if anything.

Yes, and this is exactly why I wouldn't want to rush this feature in. 

&gt; ... no "major" settings changes occurred since the snapshot was taken.  

Currently, it's hard to determine which settings change is "major" and which one is "minor". 

&gt; As far as the TaskManagement service, any way to track the progress of that?

You can keep track of the task management development progress on the task management meta issue #15117.
</comment><comment author="c4urself" created="2017-02-09T21:20:06Z" id="278777043">FWIW I would love this feature and found this issue while trying to cope with an unexpected outage where two nodes in different racks were lost and 13 out of 128 shards went missing. 

Because we had a snapshot from the day before we took the following steps (note: this may not be the best way to do this) to restore only a small subset of the shards:

* started the snapshot restore process to a new restore index within the same cluster
* disabled shard allocation for the restore index
* used the Cluster Reroute api to manually allocate the 13 shards we wanted to recover (allow_primary: true)
* disabled shard allocation throughout the cluster and stopped Elasticsearch on the node we were trying to recover on
* copied the directory containing the shard from the restore index to the origin index (removed `write.lock`, and ensured that the `_state/state-*.st` file contained the same UUID by copying it from an existing shard
* restarted elasticsearch on that node and re-enabled shard allocation to get replicas of the restored shards
* rinse/repeat for other nodes/shards

Huge caveat, it worked for us on 2.4.x and YMMV but it seemed relatively painless (though time intensive) and much faster than restoring the full snapshot and then reindexing missing writes.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to import a folder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15652</link><project id="" key="" /><description>if i have a folder contains 1000 files which i want to import to elasticsearch ,which api or tools should i use?
right now  i only find 

```
curl -XPOST 'http://192.168.27.236:9200/test/test/1' -d @lane.json
 curl -s -XPOST http://192.168.27.236:9200/_bulk --data-binary @requests
```
</description><key id="123755321">15652</key><summary>how to import a folder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wanghaisheng</reporter><labels /><created>2015-12-24T03:55:14Z</created><updated>2015-12-28T10:56:54Z</updated><resolved>2015-12-28T10:56:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-28T10:56:53Z" id="167542166">Please use the forums for such questions: http://discuss.elastic.co

For only 1000 documents both approaches would probably be fine (assuming docs are small)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Edge NGram: "side" setting was depercated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15651</link><project id="" key="" /><description>Edge NGram: "side" setting was depercated
</description><key id="123754001">15651</key><summary>Edge NGram: "side" setting was depercated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">socurites</reporter><labels /><created>2015-12-24T03:32:00Z</created><updated>2015-12-24T13:25:46Z</updated><resolved>2015-12-24T13:25:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add sample configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15650</link><project id="" key="" /><description>It may be helpful if sample configuration for html_strip is in place.
</description><key id="123751785">15650</key><summary>Add sample configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">socurites</reporter><labels /><created>2015-12-24T03:02:49Z</created><updated>2015-12-29T07:46:42Z</updated><resolved>2015-12-29T07:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-24T13:30:42Z" id="167112206">Actually you don't need to define a char filter, you can directly use "html_strip" as a char filter in your analyzer definition?
</comment><comment author="socurites" created="2015-12-29T07:46:42Z" id="167740408">yes, you're right :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>comma(,) was duplicated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15649</link><project id="" key="" /><description>deleted it.
</description><key id="123748202">15649</key><summary>comma(,) was duplicated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">socurites</reporter><labels /><created>2015-12-24T02:25:39Z</created><updated>2015-12-24T13:27:47Z</updated><resolved>2015-12-24T13:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Nested Query and Filter results in Filter being ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15648</link><project id="" key="" /><description>I ran into this while answering [a question over on StackOverflow](http://stackoverflow.com/questions/34431092/how-to-access-doc-values-of-a-nested-array-in-elasticsearch-script). If you create a `nested` `query` and also supply the `filter`, then the `filter` is silently ignored. If you do not supply the `query`, then the `filter` is used as you would expect.

This issue is present in both ES 1.x and 2.x.

``` http
PUT /test
{
  "mappings": {
    "user": {
      "properties": {
        "name": {
          "type": "string"
        },
        "skills": {
          "type": "nested",
          "properties": {
            "skill_id": {
              "type": "integer"
            },
            "recommendations_count": {
              "type": "integer"
            }
          }
        }
      }
    }
  }
}

PUT /test/user/1
{
   "name": "John",
   "skills": [
      {
         "skill_id": 100,
         "recommendations_count": 5
      },
      {
         "skill_id": 200,
         "recommendations_count": 3
      }
   ]
}

PUT /test/user/2
{
   "name": "Mary",
   "skills": [
      {
         "skill_id": 100,
         "recommendations_count": 9
      },
      {
         "skill_id": 200,
         "recommendations_count": 0
      }
   ]
}
```

Followed by:

``` http
GET /test/_search
{
  "query": {
    "nested": {
      "path": "skills",
      "query": {
        "function_score": {
          "functions": [
            {
              "field_value_factor": {
                "field": "skills.recommendations_count",
                "factor": 1.2,
                "modifier": "sqrt",
                "missing": 1
              }
            }
          ]
        }
      },
      "filter": {
        "term": {
          "skills.skill_id": 100
        }
      }
    }
  }
}
```

Noting the `filter`, the `skill_id` filter works even if the value does not exist (e.g., 500). This is true of both Elasticsearch 1.x and 2.x.

This can be rewritten for each version to work though:

ES 1.x:

``` http
GET /test/_search
{
  "query": {
    "nested": {
      "path": "skills",
      "query": {
        "filtered": {
          "filter": {
            "term": {
              "skills.skill_id": 100
            }
          },
          "query": {
            "function_score": {
              "functions": [
                {
                  "field_value_factor": {
                    "field": "skills.recommendations_count",
                    "factor": 1.2,
                    "modifier": "sqrt",
                    "missing": 0
                  }
                }
              ]
            }
          }
        }
      }
    }
  }
}
```

For ES 2.x:

``` http
GET /test/_search
{
  "query": {
    "nested": {
      "path": "skills",
      "query": {
        "bool": {
          "filter": {
            "term": {
              "skills.skill_id": 100
            }
          },
          "must": {
            "function_score": {
              "functions": [
                {
                  "field_value_factor": {
                    "field": "skills.recommendations_count",
                    "factor": 1.2,
                    "modifier": "sqrt",
                    "missing": 0
                  }
                }
              ]
            }
          }
        }
      }
    }
  }
}
```
</description><key id="123742825">15648</key><summary>Nested Query and Filter results in Filter being ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-12-24T01:14:47Z</created><updated>2016-01-20T15:32:10Z</updated><resolved>2016-01-11T18:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T19:05:14Z" id="170382356">@cbuescher has this been fixed with the search refactoring?
</comment><comment author="cbuescher" created="2016-01-11T11:25:45Z" id="170512986">@clintongormley Looking at the history of NestedQueryParser, it looks like support for the "filter" parameter has been removed on master after deprecating it with 6076ccb. I can take a look at what's causing this problem on 2.x.
</comment><comment author="cbuescher" created="2016-01-11T13:27:58Z" id="170548910">For 2.x, in `NestedQueryParser#build()` the `query` takes precedence over `filter` as observed by @pickypg when constructing the Lucene query. Looking at the docs in 1.7 or 2.x, this seems also like the expected behaviour (`"... the query (or filter) includes the query that will run on the nested ..."`). Not sure if there is anything to fix here @clintongormley? We could throw an exception if both are specified, but not sure if that's worth going into 2.x since the whole parameter is gone on master anyway.
</comment><comment author="clintongormley" created="2016-01-11T18:52:27Z" id="170651137">Makes sense, thanks @cbuescher 

Closing
</comment><comment author="brupm" created="2016-01-19T16:36:18Z" id="172909403">If I understand correctly you're saying that supporting both has been removed from master and that the documentation specified that `the query (or filter)` should be used. As in, either query OR filter is supported, but not both. If that's the case the documentation is not expressive enough to something that can potentially cause hours of headaches. 
</comment><comment author="pickypg" created="2016-01-19T17:02:29Z" id="172918527">I tend to agree with @brupm on this one: it's a documentation bug if the code isn't going to be changed because "or" is overly suggestive.

In `master` (from 6076ccb), [the code literally removed `filter`](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/query/NestedQueryParser.java#L33-L36) as an option. Given the simplification of using filters in 2.0+, I think that that was the right move. It also needs to be listed as a breaking change though for 3.0.

I think that we should:
- Change the docs, starting with 2.0+ to not mention the `filter` parameter -- instead showing the `filter` being used using a `bool` query.
- Add the lack of a filter parameter for the nested query as a breaking change.

What do you think @clintongormley?
</comment><comment author="clintongormley" created="2016-01-20T14:12:13Z" id="173214968">I agree - @cbuescher would you mind adding the removal of the "filter" param from nested to the breaking changes in 3.0, plus anything similar that you can think of that has changed?
</comment><comment author="cbuescher" created="2016-01-20T14:35:49Z" id="173222233">@clintongormley there is already a note in https://github.com/elastic/elasticsearch/blob/master/docs/reference/migration/migrate_3_0.asciidoc#nested-query, should this go to any other place or is this the right location for "breaking changes"? 
</comment><comment author="clintongormley" created="2016-01-20T15:29:51Z" id="173238383">@cbuescher ah i see it now https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_30_rest_api_changes.html#_nested_query

i'll reorganise these breaking changes docs before we release to make them a bit more organised. thanks
</comment><comment author="pickypg" created="2016-01-20T15:32:10Z" id="173239148">Ah, I didn't look under the REST API changes. Honestly, most of those are more of "search changes" than REST API changes from my perspective.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Fail Processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15647</link><project id="" key="" /><description>The fail processor is useful once `on_failure` is introduced to the ingest pipeline.

One may want to enrich a document upon processor failure, and then escape and have a pipeline-level `on_failure` block be executed.

here is an example of a pipeline attempting to parse a timestamp, and utilizing the `fail` processor to relay an error message to the pipeline `on_failure` handler processors.

```
{
  "description" : "...",
  "processors" : [
    {
      "date" : {
        "match_field" : "initial_date",
        "target_field" : "timestamp",
        "match_formats" : ["dd/MM/yyyy hh:mm:ss"],
        "timezone" : "Europe/Amsterdam",
        "on_failure" : [
          {
            "set" : {
              "field" : "timestamp",
              "value" : "unknown"
            }
          },
          {
            "fail" : {
              "message" : "unable to parse timestamp from document"
            }
          }
        ]
      }
    }
  ],
  "on_failure" : [
    {
      "set" : {
         "field" : "error",
         "value" : "{{ _ingest.on_failure_message }}"
      }
    }
  ]
}
```
</description><key id="123739930">15647</key><summary>[Ingest] Fail Processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-24T00:28:14Z</created><updated>2016-01-04T19:35:53Z</updated><resolved>2016-01-04T19:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-24T09:55:41Z" id="167079795">This looks good. Left one question about the type of exception that we should throw.
</comment><comment author="martijnvg" created="2016-01-04T19:32:04Z" id="168778712">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] add on_failure context to ingest metadata during executeOnFailure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15646</link><project id="" key="" /><description>It is useful to pass on information about the failure to the ingest document metadata so that processors can use this information to enrich the actual document with specific information about the exception in the pipeline that has occurred.
</description><key id="123735966">15646</key><summary>[Ingest] add on_failure context to ingest metadata during executeOnFailure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-23T23:22:37Z</created><updated>2016-01-04T19:26:45Z</updated><resolved>2016-01-04T19:26:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-24T09:56:03Z" id="167079836">@talevy Left two comments.
</comment><comment author="martijnvg" created="2016-01-04T19:22:33Z" id="168776485">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Relocating shards is only done from single source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15645</link><project id="" key="" /><description>Hi,

I had 2 servers with 200 indices on them, each index with 5 primaries &amp; 1 replica.
All indices have `"index.routing.allocation.exclude.tag" : "hot"` set on them.

I then added 3 additional servers with tag `hot` to the cluster.
Next i updated the settings on 30 indices to:

```
"index.routing.allocation.include.tag" : "hot",
"index.routing.allocation.exclude.tag" : ""
```

What happened is that all 30 indices started relocating but all from the same source (A single server out of the original two).

Which means that i'm basically using just 50% of my transfer rate.

Any ideas what it might be?

ES version 1.5.2
</description><key id="123735049">15645</key><summary>Relocating shards is only done from single source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaharmor</reporter><labels /><created>2015-12-23T23:11:10Z</created><updated>2015-12-23T23:38:02Z</updated><resolved>2015-12-23T23:38:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-23T23:20:40Z" id="167007810">So my initial assumption is that you are mainly unlucky. We don't throttle outgoing recovery/relocations yet so it's totally possible that they move all from one machine. Do you know if you have any recovery settings set on your cluster? 

https://github.com/elastic/elasticsearch/pull/15372 &lt;== this will help here it won't allow all recoveries to come from the same source
</comment><comment author="shaharmor" created="2015-12-23T23:23:31Z" id="167008050">these are my related settings:

```
indices.recovery.max_bytes_per_sec: "100mb"
indices.recovery.concurrent_streams: 5
cluster.routing.allocation.node_concurrent_recoveries: 5
```

not sure if anything else is related
</comment><comment author="s1monw" created="2015-12-23T23:30:02Z" id="167008605">&gt; cluster.routing.allocation.node_concurrent_recoveries: 5

that limits the target nodes not the source that's why all can come from the same source

&gt; indices.recovery.concurrent_streams: 5

that limits now many concurrent threads we use to send files from the source so all recoveries will shard 5 workers

&gt; indices.recovery.max_bytes_per_sec: "100mb"

that's throttling IO rate of the 5 workers
I don't think here is anything you can do about all this, that's just the way it works :(
</comment><comment author="shaharmor" created="2015-12-23T23:34:52Z" id="167009067">even if i increase the concurrent recoveries and streams to 10, the extra 5 shards after still coming from the same source... looks very "unlucky" to me :(
</comment><comment author="s1monw" created="2015-12-23T23:36:39Z" id="167009233">&gt; even if i increase the concurrent recoveries and streams to 10, the extra 5 shards after still coming from the same source... looks very "unlucky" to me :(

as I said - nothing you can do about this until #15372 is available which will be likely 3.0
</comment><comment author="shaharmor" created="2015-12-23T23:38:02Z" id="167009349">Got it. thanks a lot for the information!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>There is no way to ask ES to insert a timestamp at index time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15644</link><project id="" key="" /><description>Due to the deprecation of _timestamp, there is no way to insert a timestamp at exactly the index time.

In my use case, the _timestamp field is useful for more than just ttl - I use ES as a timeseries storage, and if anyone wants to know how long the event pipeline takes to deliver the document, prior to ES 2.0 it can be calculated as (_timestamp - @timestamp) (provided that @timestamp is obtained from the parsed event time, and _timestamp is enabled)

when such events can be identified, time-sensitive operations that missed these events due to pipeline delays may attempt to recover the events by filtering on the pipeline delay time, i.e. "(_timestamp - @timestamp) &gt; 2 hours"

asking Logstash to insert its own date field does not accurately capture any delay between the time when this new field is created, and the time the document is indexed.

i understand that the user may want to store multiple custom timestamps, e.g. moved, modified, etc. so let's not make one special _timestamp - however, i believe the time at which the document is indexed for the first time, is special and at least important for the purpose of identifying documents that needs some recovery action due to indexing delays.
</description><key id="123733172">15644</key><summary>There is no way to ask ES to insert a timestamp at index time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lam-juice</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-12-23T22:47:47Z</created><updated>2016-03-16T11:48:16Z</updated><resolved>2016-02-29T20:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Ghost93" created="2015-12-26T21:17:39Z" id="167363910">+1

we use it to calculate delay in production - we have a field called `arriveTime` and use aggregation on `arriveTime - _timestamp`
</comment><comment author="israel" created="2016-01-07T07:15:12Z" id="169579348">+1

If there is no _timestamp system meta field, a retry scenario becomes less elegant and less efficient since the client will have to modify the index request source with a new "timestamp" value for its user defined "timestamp" field.
</comment><comment author="l15k4" created="2016-02-16T16:08:20Z" id="184747943">I'm dealing with the same thing, also I'm not sure whether I have to re-index all my data because of the deprecated `_timestamp` field :  https://discuss.elastic.co/t/upgrade-from-1-7-x-to-2-2-0--timestamp-mapping-issue/41894

Documentation says it should be compatible, but it somehow is not...
</comment><comment author="clintongormley" created="2016-02-29T20:36:03Z" id="190376116">In master this can be done using ingest pipelines. Closing in favour of #14049
</comment><comment author="djschny" created="2016-03-14T15:35:55Z" id="196371125">Unfortunately ingest pipelines will not give what is desired though correct? Since it will not account for time an index request spends sitting in a threadpool queue prior to being worked on since the timestamp was added by an ingest node prior to getting to a data node.
</comment><comment author="s1monw" created="2016-03-14T15:43:18Z" id="196374675">&gt; Unfortunately ingest pipelines will not give what is desired though correct? Since it will not account for time an index request spends sitting in a threadpool queue prior to being worked on since the timestamp was added by an ingest node prior to getting to a data node.

there is always a delta between the assignment of a timestamp and the moment when it's actually indexed. The timestamp doesn't guarantee lineralizability in any way, that's a sequence ID. This feature can only be as good as some client app setting the timestamp no matter where you put it. It should not be more neither less.
</comment><comment author="lam-juice" created="2016-03-14T17:11:20Z" id="196416327">However, what is doable then - is to make a guarantee that the ingest pipeline would perform the field insertion at a stage _no earlier than_ where _timestamp insertions are currently performed, correct?

If the guarantee can be made, please ignore everything below.

Even if there is always a delta, it is an issue involving its magnitude - i.e. not a binary issue - I believe in this case an effort should be made to minimize this delta - "delta always exists" - yes it's very well understood - however, it should not be used as a reason to arbitrarily alter this delta, while not providing a mean to accomplish what the small delta used to.

i.e. a small delta is better than a large delta - a timestamp inserted closer to actual indexing is better than a timestamp inserted by the client app.  To say that it "can only as good as some client app setting the timestamp" totally ignores the meaning of the magnitude of the delta.

Do you agree that using _timestamp is a good way to measure pipeline delays?  If not, is an alternative offered that works **at least as well as** _timestamp?  Or, should we ignore Logstash delays altogether?
</comment><comment author="s1monw" created="2016-03-14T19:16:03Z" id="196482419">IMO the timestamp should be assigned as soon as the document enters the system. that is what ingest will do. I think that is a clear property of the timestamp. I don't understand what folks are concerned about here, how much time do you think it takes from sending the doc until it's really indexed? I don't understand your usecase either, do you rely on the actual time the doc is indexed? what does it buy you? 

please be reasonable we can't give you any total ordering guarantees based on timestamps.

if you are concerned about the corner case of the document being stuck in the thread-pool queue? I think  you can just ignore that, unless you totally overload your sever it should be like a second at most or something?
</comment><comment author="lam-juice" created="2016-03-14T20:02:38Z" id="196498558">You can probably find the answer if you ask yourselves, "Why was _timestamp created in the first place"?

What I'm concerned is having a knowledge - the best that a system can muster - the time elapsed between the moment an event is generated and the moment the same event is available for searches.

What does it buy us?  ElasticSearch is only one component in the entire analytics pipeline.  Let's say for example, perioidically, a process queries ElasticSearch and aggregates some results.  When an event is delayed for a long enough time, it may miss such aggregation.  A _timestamp provides a good approximation of the delay of an event, and provides a mean to rerun such aggregations on events that were missed.  It's just one example.

Another example is that it can allow us to profile time spent in Logstash, so its configuration can be optimised - or if not possible - having Logstash replaced with something more efficient.

Judging from the replies I'm not the only person with such concern, so without the tool for measurement, nobody can say it's a "corner case", or "like a second at most or something", we're talking about.
</comment><comment author="s1monw" created="2016-03-14T20:25:44Z" id="196507547">sorry I am not sure `_timestamp` had the properties you are asking for. It was added at some point inside elasticsearch and if you hammering the index you can easily got stuck in a lock on the indexing etc. there is no guarantee of any sort neither in the new way nor in the old way. There can be hours between the doc was indexed and it being visible in search. If you are relying on exact numbers here your system has a huge flaw, IMO. 

@lam-juice @djschny the new way is as good as the old way, it is assigned at a slightly earlier stage but for all usecases of `_timestamp` the neither place should matter. If it does, `_timestamp` is not what you are looking for.
</comment><comment author="lam-juice" created="2016-03-15T15:57:27Z" id="196894650">It depends on the guarantee we want - to us, ElasticSearch and what's feeding ElasticSearch its events are 2 separate entities, thus a facility is better than none in order to optimise and/or evaluate what's between ES and the event source (e.g. Logstash).

Does it provide 100% guarantee?  No - and it's not what I need anyway.  Has _timestamp statistically been providing what I need, which is 99% of the time a good approximation (even if it can have a long tail)?  Definitely.

So you can see I'm not relying on exact numbers @s1monw - if it is assigned at a slightly earlier stage, so be it - deduplications provide the robustness I need anyway if I decide to observe the difference in the fatness of the tail, and reindex more conservatively - I think our fundamental disagreement is over the notion that _"an ES-assigned timestamp does not accomplish more than a client-assigned timestamp"_ - because I believe a client-assigned timestamp does not allow even a rough estimate of time spent before an event crosses the boundary into ES.
</comment><comment author="s1monw" created="2016-03-16T11:48:16Z" id="197280186">&gt; Does it provide 100% guarantee? No - and it's not what I need anyway. Has _timestamp statistically been providing what I need, which is 99% of the time a good approximation? Definitely.

that has not changed if you use ingest

&gt; "an ES-assigned timestamp does not accomplish more than a client-assigned timestamp" 

that is what it basically was all the time no other guarantees given.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mark node as disconnected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15643</link><project id="" key="" /><description>Is it possible for us to list a node as disconnected for a period after it has timed out/left the cluster? It seems that this would be helpful for admins looking to see which node has recently left, due to either intentional or otherwise reasons.

The idea came from [this tweet convo](https://twitter.com/kellabyte/status/679722874039603201).
</description><key id="123726754">15643</key><summary>Mark node as disconnected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-12-23T21:41:03Z</created><updated>2016-02-29T20:31:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T20:31:42Z" id="190372749">@bleskes what do you think about this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documented search_quote_analyzer in mapping types and detailed how to&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15642</link><project id="" key="" /><description>&#8230; disable stemming and stop words as a potential use case
</description><key id="123713202">15642</key><summary>Documented search_quote_analyzer in mapping types and detailed how to&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels><label>docs</label></labels><created>2015-12-23T19:40:04Z</created><updated>2016-01-06T10:11:33Z</updated><resolved>2016-01-06T09:43:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-24T13:35:03Z" id="167113134">Documenting search_quote_analyzer is nice but the example is too hacky to be in the reference documentation in my opinion.
</comment><comment author="imranazad" created="2015-12-24T14:48:07Z" id="167120037">@jpountz Thanks for the feedback. I'm assuming the disable_tokenization analyzer made you feel it's hacky? I guess it is so in that case I can remove that and just point it to a standard analyzer. With this approach you get the same result and it still clearly shows you the purpose of the search_quote_analyzer. Oh and also I forgot to include queries that illustrate how stems can be disabled for phrases so I can include those as well. Coming from a Microsoft FAST background and having experience in search, disabling stop words and stems for phrases is a big plus for Elastic in terms of firstly it's possible to do and secondly how easy it is and I think it's a real shame that the documentation currently lacks this information. The ability to disable stop words and stems can make all the difference to a user when reviewing which search product to adopt since all the major proprietary search engines cater for this. If this information is clearly available in the documentation it can hold Elastic in good stead. Let me know what you think.
</comment><comment author="jpountz" created="2015-12-24T15:08:45Z" id="167122119">@imranazad +1 on using an example with stop words to illustrate use-cases of the search_quote analyzer
</comment><comment author="imranazad" created="2015-12-29T21:18:49Z" id="167879050">@jpountz Hey Adrian, sorry for the delay I caught a nasty cold so have been in bed for the past few days. I've made a few changes to the documentation, I've removed the hacky implementation and also included examples of disabling stems and stop words on phrase queries. Please let me know what you think. Thanks.
</comment><comment author="jpountz" created="2015-12-30T10:47:19Z" id="167975831">No worries! Could we simplify a bit by removing the multi field and doing eg.

```
PUT /my_index
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_stop_analyzer":{
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase",
                  "english_stop"
               ]
            },
            "my_analyzer":{
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase"
               ]
            }
         },
         "filter":{
            "english_stop":{
               "type":"stop",
               "stopwords":"_english_"
            }
         }
      }
   },
   "mappings":{
      "my_type":{
         "properties":{
            "title":
               "type":"string",
               "analyzer":"my_analyzer",
               "search_analyzer":"my_stop_analyzer",
               "search_quote_analyzer":"my_analyzer"
            }
         }
      }
   }
}
```

If I'm not mistaken this way stop words will only be taken into account for phrases.
</comment><comment author="imranazad" created="2015-12-30T14:25:21Z" id="168010769">@jpountz Hey Adrian, I've had a look at your suggestion and that wouldn't ever work with regard to a use case for stop words when dealing with phrases. You are correct stop words will only be taken into account for phrases however since the document is indexed with `my_stop_analyzer` the phrase query will never return any results because the original tokens have been filtered on stop words. So for example a search for `The quick brown fox` won't return any results as the `the` token isn't in the index. I appreciate simplifying it but I don't think it's possible to illustrate a stop words use case without actually having a multi-field as we need to index the content differently in order to get matches for phrase and non-phrase queries in-order to make that distinction.
</comment><comment author="jpountz" created="2015-12-30T14:41:30Z" id="168012757">Woops sorry I had a mistake in my example, the second analyzer should read `search_analyzer` instead of `analyzer`. I just updated the example.
</comment><comment author="imranazad" created="2015-12-30T15:15:03Z" id="168017439">@jpountz No worries. :-) Yes I see now, that would work, I love the simplicity and I remember now I did play around with this idea however that setup wouldn't be possible to replicate for both stop words and stemming would it using a single field? Previously I think I tried to look into this and gave up so resorted to `multi_fields`. Disabling stop words and stemming go together when it comes to phrase searches just like the way Google does it that's why I feel it's important that they be documented together. So for example if we had two documents with the titles `I bought two guard dogs` and `I bought one guard dog`, phrases search would work fine however a non-phrase query for `guard dogs` would never return the document titled  `I bought two guard dogs` since the `search_analyzer` would remove the stem and the original token would be indexed as `dogs`.

What do you think?
</comment><comment author="jpountz" created="2015-12-30T15:41:57Z" id="168021419">&gt; however that setup wouldn't be possible to replicate for both stop words and stemming would it using a single field?

True but I think examples in the reference documentation should be as simple as possible given that the goal is just to illustrate the feature. Additionally I don't really like the multi_field example as it also runs the phrase on the stemmed field, Ideally we would need a way to redirect phrases to a different field.
</comment><comment author="imranazad" created="2015-12-30T15:45:03Z" id="168021846">@jpountz Ah I see, that's fine, not a problem, I'll make those changes you suggested then.
</comment><comment author="imranazad" created="2015-12-31T18:09:04Z" id="168230983">@jpountz Hey Adrian, I finally got round to making the changes you suggested, all done now. Let me know what you think. Thanks.
</comment><comment author="jpountz" created="2016-01-05T13:39:35Z" id="169001098">@imranazad thanks, this looks good. I think there is just a left-over in the search analyser definition which uses a stemmer instead of a stop filter?
</comment><comment author="imranazad" created="2016-01-05T13:57:52Z" id="169008189">@jpountz Argh, my bad! Thanks, I've sorted it now. :-)
</comment><comment author="jpountz" created="2016-01-05T14:04:34Z" id="169009514">:+1: I'll merge soon!
</comment><comment author="imranazad" created="2016-01-05T14:08:12Z" id="169010224">@jpountz You're a star, many thanks Adrien, I must say I'm very excited about my first contribution to Elastic, thanks for the opportunity. :-)
</comment><comment author="jpountz" created="2016-01-06T09:43:01Z" id="169278684">I merged the pr manually.

&gt; thanks for the opportunity

You're welcome, thanks for helping!
</comment><comment author="imranazad" created="2016-01-06T10:11:33Z" id="169283486">@jpountz Thanks pal.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documented search_quote_analyzer in mapping types and detailed how to&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15641</link><project id="" key="" /><description>&#8230; disable stemming and stop words as a potential use case
</description><key id="123712918">15641</key><summary>Documented search_quote_analyzer in mapping types and detailed how to&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels><label>docs</label></labels><created>2015-12-23T19:37:58Z</created><updated>2015-12-23T19:40:54Z</updated><resolved>2015-12-23T19:39:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Documented search_quote_analyzer in mapping types and detailed how to&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15640</link><project id="" key="" /><description>&#8230; disable stemming and stop words as a potential use case
</description><key id="123712347">15640</key><summary>Documented search_quote_analyzer in mapping types and detailed how to&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels /><created>2015-12-23T19:32:49Z</created><updated>2015-12-23T19:37:37Z</updated><resolved>2015-12-23T19:37:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15639</link><project id="" key="" /><description /><key id="123707656">15639</key><summary>2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels /><created>2015-12-23T18:53:38Z</created><updated>2015-12-23T18:54:14Z</updated><resolved>2015-12-23T18:54:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove backwards from .gitignore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15638</link><project id="" key="" /><description>backwards was the canonical name for the directory to which to download
the bwc version. Its no longer needed because we now download it using
maven into a target directory.
</description><key id="123692294">15638</key><summary>Remove backwards from .gitignore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label></labels><created>2015-12-23T17:02:29Z</created><updated>2016-01-10T18:54:35Z</updated><resolved>2015-12-23T19:07:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-23T17:10:09Z" id="166946749">Isn't it also where `dev-tools/get-bwc-version.py` downloads old releases to generate bwc indices?
</comment><comment author="nik9000" created="2015-12-23T17:14:39Z" id="166947396">&gt; Isn't it also where dev-tools/get-bwc-version.py downloads old releases to generate bwc indices?

Yeah I suppose so. We could probably do away with that entirely and rely on the bwc tests to resolve the version but that would mean that the release process has to create the new bwc test projects.
</comment><comment author="rjernst" created="2015-12-23T18:46:02Z" id="166968448">We should not get rid of the static bwc tests. They check every single version of supported indexes can be read, while as I understand it, the current bwc tests you refer to only check wire compatibility with the previous version.

However, I have thought about moving the static bwc tests to its own QA project, and moving the scripts that handle generating those static indexes into gradle as tools that can be used when preparing the version bump after a release.
</comment><comment author="nik9000" created="2015-12-23T18:59:52Z" id="166970816">&gt; However, I have thought about moving the static bwc tests to its own QA project, and moving the scripts that handle generating those static indexes into gradle as tools that can be used when preparing the version bump after a release.

This is pretty much what I was referring to: removing the script that fetches the released version. In 2.x its only used for building the static bwc indices and you could instead build the bwc tests themselves to do that.
</comment><comment author="rjernst" created="2015-12-23T19:03:03Z" id="166971296">I would not want to do this unless we actually move the static bwc tests, not just generate the static indexes from within the wire bwc tests (this seems bad to me to touch source files in a different project). There are however a bunch of random tests right now that rely on some static indexes (eg that is why we have so many copies of the original generation script now).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15637</link><project id="" key="" /><description>Documentation about search_quote_analyzer
</description><key id="123692024">15637</key><summary>2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels /><created>2015-12-23T17:00:14Z</created><updated>2015-12-23T17:05:28Z</updated><resolved>2015-12-23T17:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add sub-fields support to `bool` fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15636</link><project id="" key="" /><description>`bool` is our only core mapper that does not support sub fields.

Close #6587
</description><key id="123691940">15636</key><summary>Add sub-fields support to `bool` fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-23T16:59:34Z</created><updated>2016-03-16T19:45:12Z</updated><resolved>2015-12-24T09:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-23T17:03:32Z" id="166945279">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Configuration failed caused by jar hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15635</link><project id="" key="" /><description>Hi,

Sorry for my english i'm french...

I'm using the java api.

I was using elasticsearch 1.7 and now I'm trying to update my version to elasticsearch 2.1. It works well until i have to set the 'path.home' for my Node client in my application.

So I did something like this:

```
        node   = nodeBuilder().settings(Settings.builder()
                                            .put("path.home", "C:\\path\\to\\elasticsearch-2.1.0\\")
                                            .put("cluster.name", "hotline")
                                            .put("node.data", "false")
                                            .put("node.master", "false")
                                            .put("node.name", "Soldat"))                                        
                                            .node();
```

But when I run the tomcat i have got this error:

```
GRAVE: Critical error during deployment: 
com.sun.faces.config.ConfigurationException: CONFIGURATION FAILED! failed to load bundle [file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/license/license-2.1.0.jar, file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/license/license-core-2.1.0.jar, file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/license/license-plugin-api-2.1.0.jar, file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/marvel-agent/marvel-agent-2.1.0.jar] due to jar hell
    at com.sun.faces.config.ConfigManager.initialize(ConfigManager.java:375)
    at com.sun.faces.config.ConfigureListener.contextInitialized(ConfigureListener.java:225)
    at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4992)
    at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5490)
    at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1575)
    at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1565)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: failed to load bundle [file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/license/license-2.1.0.jar, file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/license/license-core-2.1.0.jar, file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/license/license-plugin-api-2.1.0.jar, file:/C:/Users/fr115484/Desktop/elasticsearch-2.1.0/plugins/marvel-agent/marvel-agent-2.1.0.jar] due to jar hell
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:338)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:109)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)
    at com.foncia.hotlinemyfoncia.service.ClientProvider.prepareClient(ClientProvider.java:44)
    at com.foncia.hotlinemyfoncia.util.ElasticSearchSystemEventListener.processEvent(ElasticSearchSystemEventListener.java:32)
    at javax.faces.event.SystemEvent.processListener(SystemEvent.java:106)
    at com.sun.faces.application.ApplicationImpl.processListeners(ApplicationImpl.java:2168)
    at com.sun.faces.application.ApplicationImpl.invokeListenersFor(ApplicationImpl.java:2144)
    at com.sun.faces.application.ApplicationImpl.publishEvent(ApplicationImpl.java:302)
    at com.sun.faces.config.ConfigManager.publishPostConfigEvent(ConfigManager.java:600)
    at com.sun.faces.config.ConfigManager.initialize(ConfigManager.java:369)
    ... 10 more
Caused by: java.lang.IllegalStateException: jar hell!
class: org.apache.catalina.loader.StandardClassLoader
jar1: C:\dev\apache-tomcat-7.0.55\bin\bootstrap.jar
jar2: C:\dev\bin\bootstrap.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:280)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:186)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:336)
    ... 23 more
```

I think he cannot load the plugins but actually I don't need any plugin for my Node Client...

Please how can I correct that ?

Thanks
</description><key id="123691293">15635</key><summary>Configuration failed caused by jar hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Glecun</reporter><labels /><created>2015-12-23T16:56:09Z</created><updated>2015-12-23T17:12:42Z</updated><resolved>2015-12-23T17:12:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-23T17:12:41Z" id="166947105">You can join us on discuss.elastic.co for questions.
Cherry on the cake: there's a french section :)

I guess that you have to clean the way tomcat is launched. Tomcat seems to add bootstrap.jar twice...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple_query_string ignores search_quote_analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15634</link><project id="" key="" /><description>The simple_query_string seems to ignore the search_quote_analyzer

Here are some CURL requests to replicate the issue:
Mapping:

```
POST test
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "analysis": {
        "analyzer": {
          "test": {
            "type": "custom",
            "tokenizer": "standard",
            "filter": [
              "lowercase",
              "english_stop"
            ]
          },
          "disable_tokenization": {
            "type": "pattern",
            "pattern": "[^.]"
          }
        },
        "filter": {
           "english_stop":{
             "type": "stop",
             "stopwords": "_english_"
            }
        }
      }
    }
  },
  "mappings": {
    "doc": {
      "properties": {
        "full_text": {
          "type": "string",
          "store": true,
          "term_vector": "with_positions_offsets",
          "analyzer": "test",
          "search_quote_analyzer": "disable_tokenization",
          "fields": {
            "plain": {
              "type": "string",
              "analyzer": "standard",
              "store": true,
              "term_vector": "with_positions_offsets"
            }
          }
        }
      },
      "dynamic_templates": [
        {
          "title": {
            "match": "title",
            "mapping": {
              "index": "not_analyzed",
              "type": "string",
              "copy_to": "full_text"
            }
          }
        }
      ]
    }
  }
}
```

Sample docs:

```
post /test/doc
{
  "title": "the mouth"
}

post /test/doc
{
  "title": "mouth"
}
```

simple_query_string query:

```
GET /test/doc/_search
{
    "query": { 
                    "simple_query_string" : {
                        "fields" : ["full_text*"],
                        "default_operator": "AND", 
                        "query" :"\"the mouth\""
                    }
                },
               "highlight" : {
                    "fields" : {
                        "*" : {
                            "matched_fields": ["*", "*"],
                            "fragment_size" : 200,
                            "pre_tags" : ["&lt;mark&gt;"],
                            "post_tags" : ["&lt;/mark&gt;"]
                        }
                    }
                }
}
```

Returns both documents.

query_string query:

```
GET /test/doc/_search
{
    "query": { 
                    "query_string" : {
                        "fields" : ["full_text*"],
                        "default_operator": "AND", 
                        "query" :"\"the mouth\""
                    }
                },
               "highlight" : {
                    "fields" : {
                        "*" : {
                            "matched_fields": ["*", "*"],
                            "fragment_size" : 200,
                            "pre_tags" : ["&lt;mark&gt;"],
                            "post_tags" : ["&lt;/mark&gt;"]
                        }
                    }
                }
}
```

Only returns one document which is correct.
</description><key id="123689938">15634</key><summary>simple_query_string ignores search_quote_analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-23T16:47:38Z</created><updated>2016-01-10T17:54:18Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-30T16:36:38Z" id="168029818">@imranazad the `simple_query_string` query does not support the
`search_quote_analyzer` feature, that is currently an option only allowed on the
`query_string` query.

I will update the documentation accordingly.
</comment><comment author="imranazad" created="2015-12-30T16:44:51Z" id="168031620">@dakrone Thanks for the response, just curious how much work would it be to make it available for simple_query_string?
</comment><comment author="dakrone" created="2015-12-30T18:11:44Z" id="168047535">@imranazad I left a comment about it on #15550, I think it's doable with changes to the SimpleQueryParser
</comment><comment author="imranazad" created="2015-12-30T18:15:32Z" id="168047997">@dakrone Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve cross-type dynamic mapping updates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15633</link><project id="" key="" /><description>Today when dynamically mapping a field that is already defined in another type,
we use the regular dynamic mapping logic and try to copy some settings to avoid
introducing conflicts. However this is quite fragile as we don't deal with every
existing setting. This proposes a different approach that will just borrow a
field mapper from another type (which is safe since they are immutable).

This has a couple downsides, like eg. the fact that sub-fields will be borrowed
as well, but overall I think it goes into the right direction of having more
similar mappings across types.

Close #15568
</description><key id="123688144">15633</key><summary>Improve cross-type dynamic mapping updates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-23T16:37:14Z</created><updated>2016-01-10T19:11:23Z</updated><resolved>2015-12-24T09:24:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-23T16:43:23Z" id="166940954">LGTM. I think its worth putting something in the breaking changes list so this doesn't get lost in the shuffle?
</comment><comment author="jpountz" created="2015-12-23T17:57:21Z" id="166958652">After discussing the problem with @rjernst we came up with a different approach that doesn't have the multi-fields issue.
</comment><comment author="rjernst" created="2015-12-23T18:19:16Z" id="166962590">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to access _score in script_fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15632</link><project id="" key="" /><description>im trying to access _score in script_fields, but got error message:
"reason": "GroovyScriptExecutionException[MissingPropertyException[No such property: _score for class: Script170]]",

when i use doc['_score'] to access the _score, i got  error message:
No field found for [_score] in mapping with types [fulltext]

so, can i access _score in script_fields?
</description><key id="123673854">15632</key><summary>how to access _score in script_fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q11112345</reporter><labels /><created>2015-12-23T15:03:15Z</created><updated>2015-12-23T18:14:57Z</updated><resolved>2015-12-23T18:14:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-23T18:14:56Z" id="166961914">script_fields don't support reading the score. This is only supported on search scripts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Add index template for the '.ingest' index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15631</link><project id="" key="" /><description>Also added logic that ensures that the index template is installed.

An index template for the '.ingest' index is required because:
- We don't want arbitrary fields in pipeline documents, because that can turn into upgrade problems if we add more properties to the pipeline dsl.
- We know what are the usages are of the '.ingest' index, so we can optimize for that and prevent that this index is used for different purposes.
</description><key id="123665743">15631</key><summary>[Ingest] Add index template for the '.ingest' index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-23T14:11:52Z</created><updated>2015-12-24T14:18:49Z</updated><resolved>2015-12-24T14:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-12-23T22:16:57Z" id="167001026">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cloud-azure plugin not working </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15630</link><project id="" key="" /><description>I get the following error which takes down my es service:

```
java.lang.RuntimeException: Service or property not registered:  com.microsoft.windowsazure.management.compute.ComputeManagementClient interface com.microsoft.windowsazure.management.compute.ComputeManagementClient
    at com.microsoft.windowsazure.core.DefaultBuilder.build(DefaultBuilder.java:197)
    at com.microsoft.windowsazure.Configuration.create(Configuration.java:113)
    at com.microsoft.windowsazure.management.compute.ComputeManagementService.create(ComputeManagementService.java:47)
    at org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl.&lt;init&gt;(AzureComputeServiceImpl.java:83)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.lang.reflect.Constructor.newInstance(Unknown Source)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Elasticsearch: 2.0
Java: 1.8.0_66
Windows Server 2012 R2 Datacenter
</description><key id="123651626">15630</key><summary>cloud-azure plugin not working </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">hisuwh</reporter><labels><label>:Plugin Cloud Azure</label><label>bug</label></labels><created>2015-12-23T12:29:38Z</created><updated>2016-07-01T08:21:10Z</updated><resolved>2016-07-01T08:21:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hisuwh" created="2015-12-23T12:46:36Z" id="166888095">Have tried with Java 7 to no avail
</comment><comment author="dadoonet" created="2015-12-23T13:12:41Z" id="166891147">Can you share your settings?
</comment><comment author="hisuwh" created="2015-12-23T13:41:59Z" id="166898561">my elasticsearch settings?

```
cluster.name: empactislive

network.bind_host: _en0_
network.publish_host: _en0_

cloud.azure.management.keystore.path: E:\azurekeystore.pkcs12
cloud.azure.management.keystore.type: pkcs12
cloud.azure.management.keystore.password: ***********
cloud.azure.management.subscription.id: *******-****-****-****-********
cloud.azure.management.cloud.service.name: empctslivees
discovery.azure.deployment.name: empctsesvmlive1
discovery.type: azure
```
</comment><comment author="hisuwh" created="2015-12-23T14:03:31Z" id="166902047">I get the same error with elasticsearch 2.1.1
</comment><comment author="dadoonet" created="2015-12-23T14:24:24Z" id="166906868">Can you try 

```
\\ 
```

instead of 

```
 \
```

Anything else in logs?
</comment><comment author="hisuwh" created="2015-12-23T15:12:32Z" id="166921066">I can't see anything else in the logs. 
I have tried changing the discovery logging level to trace but that does not give any more information
</comment><comment author="hisuwh" created="2015-12-23T16:07:25Z" id="166932597">Tried both 

```
\\
```

and 

```
/
```

to no avail
</comment><comment author="hisuwh" created="2015-12-23T16:08:11Z" id="166932754">I am also seeing a lot of this warning:

```
Java HotSpot(TM) 64-Bit Server VM warning: Using the ParNew young collector with the Serial old collector is deprecated and will likely be removed in a future release
Exception in thread "main" puteManagementClient
    at com.microsoft.windowsazure.core.DefaultBuilder.build(DefaultBuilder.java:197)
    at com.microsoft.windowsazure.Configuration.create(Configuration.java:113)
    at com.microsoft.windowsazure.management.compute.ComputeManagementService.create(ComputeManagementService.java:47)
    at org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl.&lt;init&gt;(AzureComputeServiceImpl.java:83)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
    at java.lang.reflect.Constructor.newInstance(Unknown Source)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:198)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</comment><comment author="david-mills" created="2015-12-23T22:33:37Z" id="167002908">im getting the same issue with 2.1.1, and the same confguration as above, not sure if its to do with the fact i installed the 'cloud-azure' plugin instead of 'discovery-azure' as it says in the latest docs.  discovery-azure gave me a file not found exception as the zip doesnt exist
</comment><comment author="dadoonet" created="2015-12-30T11:35:11Z" id="167983888">@hisuwh Just to make sure about the source of your issue, can you move the pkcs12 file on the same drive as you have elasticsearch instance (I guess it could be for example `C:`) and then change your `elasticsearch.yml` accordingly:

``` yaml
cloud.azure.management.keystore.path: /azurekeystore.pkcs12
```
</comment><comment author="hisuwh" created="2016-01-06T09:56:32Z" id="169281001">I have been away for Christmas as you might expect. I will try this out at some point and let you know if this helps
</comment><comment author="hisuwh" created="2016-01-12T16:50:16Z" id="170971957">I have tried moving my keystore to the c drive but this makes no difference
</comment><comment author="dadoonet" created="2016-01-12T16:55:56Z" id="170973614">@hisuwh And your `elasticsearch.yml` is now?

``` yml
cloud.azure.management.keystore.path: /azurekeystore.pkcs12
```
</comment><comment author="hisuwh" created="2016-01-12T17:06:15Z" id="170977002">yes
</comment><comment author="hisuwh" created="2016-01-14T09:47:13Z" id="171589673">I am sorry for the bump.
But any more ideas on this?
This massively blocking us at the moment.
Has anyone your end tried to reproduce this? I don't know if its a bug with your tools or in my setup.
I have the same setup on 3 other clusters except they are on older versions of elasticsearch and they all work.

Any help would be much appreciated.
Thanks
</comment><comment author="georgievrado" created="2016-01-20T12:00:15Z" id="173183544">I am having the same problem on elasticsearch 2.1.1 (running on ubuntu 15.04 installed from repositories). My config is as follows:

``` yaml
cluster.name: "mycluster"
bootstrap.mlockall: true
path:
 logs: /var/log/elasticsearch
 data: /datadrive/elasticsearch/data
cloud:
 azure:
  management:
   keystore:
    path: /etc/elasticsearch/mykeystorefile.pkcs12
    password: password
   subscription.id: GUID
   cloud.service.name: myservicename
  storage:
   account: mystorageaccount
   key: mystorageaccountkey
discovery:
 type: azure
refresh_interval: 30s
```

This is what i have in the log:

```
[2016-01-20 11:51:36,846][WARN ][bootstrap                ] If you are logged in interactively, you will have to re-login for the new limits to take effect.
[2016-01-20 11:51:37,104][INFO ][node                     ] [Forgotten One] version[2.1.1], pid[17658], build[40e2c53/2015-12-15T13:05:55Z]
[2016-01-20 11:51:37,104][INFO ][node                     ] [Forgotten One] initializing ...
[2016-01-20 11:51:37,555][INFO ][plugins                  ] [Forgotten One] loaded [cloud-azure], sites [head]
[2016-01-20 11:51:37,585][INFO ][env                      ] [Forgotten One] using [1] data paths, mounts [[/datadrive (/dev/sdc1)]], net usable_space [121.2gb], net total_space [127.8gb], spins? [possibly], types [ext4]
[2016-01-20 11:51:41,140][ERROR][bootstrap                ] Guice Exception: java.lang.RuntimeException: Service or property not registered:  com.microsoft.windowsazure.management.compute.ComputeManagementClient interface com.microsoft.windowsazure.management.compute.ComputeManagementClient
        at com.microsoft.windowsazure.core.DefaultBuilder.build(DefaultBuilder.java:197)
        at com.microsoft.windowsazure.Configuration.create(Configuration.java:113)
        at com.microsoft.windowsazure.management.compute.ComputeManagementService.create(ComputeManagementService.java:47)
        at org.elasticsearch.cloud.azure.management.AzureComputeServiceImpl.&lt;init&gt;(AzureComputeServiceImpl.java:83)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Any pointers will be greatly appreciated
[Update] I've isolated the code for creating the ComputeManagementClient in a small application to verify that my settings are correct. It is ran on the same machine where the elasticsearch is installed and uses the same values as from my configuration. The client is successfully instantiated without throwing exceptions:

``` java
import com.microsoft.windowsazure.Configuration;
import com.microsoft.windowsazure.core.utils.KeyStoreType;
import com.microsoft.windowsazure.management.compute.ComputeManagementClient;
import com.microsoft.windowsazure.management.compute.ComputeManagementService;
import com.microsoft.windowsazure.management.compute.models.HostedServiceGetDetailedResponse;
import com.microsoft.windowsazure.management.configuration.ManagementConfiguration;
import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

class azuretest {
    static final class Azure {
        private static final String ENDPOINT = "https://management.core.windows.net/";
    }
    public static void main(String[] args) {
        String subscriptionId = "GUID";
        String serviceName = "mysubscriptionname";
        String keystorePath = "/etc/elasticsearch/mykeystorefile.pkcs12";
        String keystorePassword = "password";
        Configuration configuration;
        try{
                configuration = ManagementConfiguration.configure(new URI(Azure.ENDPOINT),
                    subscriptionId, keystorePath, keystorePassword, KeyStoreType.pkcs12);
        } catch (IOException|URISyntaxException e) {
            System.out.println(e.getMessage());
            return;
        }
        System.out.println(configuration.toString());
        ComputeManagementClient computeManagementClient = ComputeManagementService.create(configuration);
        System.out.println(computeManagementClient.getApiVersion());

    }
}
```

There is one difference though - here I am using Azure SDK version 0.9.0
</comment><comment author="hisuwh" created="2016-01-28T09:55:58Z" id="176094276">@dadoonet - any more thoughts? We are currently running a 1 node cluster in live because of this which presents a pretty huge risk
</comment><comment author="georgievrado" created="2016-01-28T13:02:58Z" id="176173268">@hisuwh I've worked around by disabling azure discovery and relying on unicast, by hardcoding the nodes.
</comment><comment author="hisuwh" created="2016-01-28T13:10:02Z" id="176176223">Of course, that is a good point.  Thanks
Good workaround though by no means a solution
</comment><comment author="clintongormley" created="2016-02-29T20:24:19Z" id="190369690">@gmarz are you familiar with the azure plugin? Is this something you could investigate?
</comment><comment author="dadoonet" created="2016-03-05T11:25:17Z" id="192624265">@hisuwh @georgievrado Any chance you could turn on `TRACE` level? 

In your `logging.yml`, add:

``` yml
  cloud.azure: TRACE
  discovery.azure: TRACE
```

It might give us some ideas about what is happening. Thanks!
</comment><comment author="tlrx" created="2016-07-01T08:21:10Z" id="229886255">This issue is closed in c557663b90dc3ac3f0ae7391cb6eac543812a946

Please note that since ES 2.0, the keystore used by the Azure plugin must be placed in a directory accessible by the elasticsearch process. I suggest to place it in the `config` directory.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15629</link><project id="" key="" /><description /><key id="123640230">15629</key><summary>2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tangjiaxu</reporter><labels /><created>2015-12-23T11:05:20Z</created><updated>2015-12-23T11:16:08Z</updated><resolved>2015-12-23T11:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>grep -E used in elasticsearch startup script causes error on Solaris</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15628</link><project id="" key="" /><description>Since Elasticsearch 2.1.1 the elasticsearch startup script has contained this line:

```
daemonized=`echo $* | grep -E -- '(^-d |-d$| -d |--daemonize$|--daemonize )'`
```

This means Elasticsearch 2.1 doesn't start on Solaris.  Changing grep -E to egrep makes it work on all of Solaris, Linux and Mac OS X:

```
daemonized=`echo $* | egrep -- '(^-d |-d$| -d |--daemonize$|--daemonize )'`
```

On Solaris:

```
$ echo -d -x -g --hello | grep -E -- '(^-d |-d$| -d |--daemonize$|--daemonize )'
grep: illegal option -- E
Usage: grep [-c|-l|-q] -bhinsvw pattern file . . .
$ echo -d -x -g --hello | egrep -- '(^-d |-d$| -d |--daemonize$|--daemonize )'
-d -x -g --hello
```

On Linux and Mac OS X:

```
$ echo -d -x -g --hello | grep -E -- '(^-d |-d$| -d |--daemonize$|--daemonize )'
-d -x -g --hello
$ echo -d -x -g --hello | egrep -- '(^-d |-d$| -d |--daemonize$|--daemonize )'
-d -x -g --hello
```

However, I am not sure if there are other platforms where egrep wouldn't work, so if there are any others you want to support you might want to check them too.
</description><key id="123639382">15628</key><summary>grep -E used in elasticsearch startup script causes error on Solaris</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">droberts195</reporter><labels /><created>2015-12-23T10:57:34Z</created><updated>2016-01-06T21:18:52Z</updated><resolved>2016-01-06T21:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-29T17:13:43Z" id="167832962">The linux man page for grep states that egrep is deprecated, but provided for
backwards compatibility.

Since, however, I know of no GNU/Linux distribution where they are _not_
provided, I think it's reasonable to switch to using them in the script.

@droberts195 would you be interested in submitting a pull request for this?
</comment><comment author="droberts195" created="2016-01-04T09:41:02Z" id="168623833">Sure, I can submit a pull request.

One way to fix it without using the deprecated egrep invocation on Linux would be to do something like this:

```
if [ -x /usr/xpg4/bin/grep ]; then
    GREP=/usr/xpg4/bin/grep
else
    GREP=`which grep`
fi
daemonized=`echo $* | "$GREP" -E -- '(^-d |-d$| -d |--daemonize$|--daemonize )'`
```

That might avoid the chance of somebody who knows that egrep is deprecated on Linux from changing back to grep -E in the future, but obviously the downside is that it's quite a bit more verbose than changing grep to egrep.

Do you have a preference for which way to fix it?
</comment><comment author="dakrone" created="2016-01-04T15:32:56Z" id="168707779">I think I'd prefer the `egrep` method rather than hardcoding paths into the script.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Querying nested objects with query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15627</link><project id="" key="" /><description>It would be useful to be able to query nested objects with the query_string.

For example consider the following document structure:

@graph
------------- @drug
 ------------------- @value

The query could look like this:
         "query": { 
                    "query_string" : {
                        "fields" : ["@graph*"]
                        "query" :"*@value:paracetamol"
                    }
         }
It can be laborious to explicitly specify nested fields when the document is heavily nested.
What do you think?
</description><key id="123638875">15627</key><summary>Querying nested objects with query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels><label>:Query DSL</label><label>feedback_needed</label></labels><created>2015-12-23T10:53:09Z</created><updated>2016-02-29T20:22:27Z</updated><resolved>2016-02-29T20:22:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T19:17:36Z" id="170383014">Do you mean fields of type `nested` or fields of type `object`?  Either way, `@` is a legal character in a field name, so you couldn't use it to mean something else.
</comment><comment author="clintongormley" created="2016-02-29T20:22:27Z" id="190369166">Duplicate of #16551
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unresponsive cluster: weird fluctuating behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15626</link><project id="" key="" /><description>Hello
Since a few days ago, my ES cluster is unresponsive and I noticed a weird fluctuating behavior.

If a periodically check the status, I see `number_of_pending_tasks` increasing (3 millions and more) and at the same time `unassigned_shards` decreasing (5k). And this is what I expected, but the weird thing is that, at a certain point it falls down and `unassigned_shards` goes back to 10k and `number_of_pending_tasks` back to 4k... and so on again and again... 

15 minutes ago my cluster status was 

```
{
  "cluster_name" : "sods",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 5,
  "number_of_data_nodes" : 5,
  "active_primary_shards" : 17969,
  "active_shards" : 30613,
  "relocating_shards" : 0,
  "initializing_shards" : 2,
  "unassigned_shards" : 5333,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 3018887,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 19152497,
  "active_shards_percent_as_number" : 85.15911872705018
}
```

10 minutes ago

```
{
  "cluster_name" : "sods",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 5,
  "number_of_data_nodes" : 5,
  "active_primary_shards" : 17969,
  "active_shards" : 30857,
  "relocating_shards" : 0,
  "initializing_shards" : 2,
  "unassigned_shards" : 5089,               &lt;==== decreasing, ok
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 3385989,               &lt;==== increasing, ok 
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 20548203,
  "active_shards_percent_as_number" : 85.83787693334817
}
```

And now 

```
{
  "cluster_name" : "sods",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 4,
  "number_of_data_nodes" : 4,
  "active_primary_shards" : 17969,
  "active_shards" : 24204,
  "relocating_shards" : 0,
  "initializing_shards" : 8,
  "unassigned_shards" : 11736,                     &lt;======== up again!
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 4660,                     &lt;======== fallen down!
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 96240,
  "active_shards_percent_as_number" : 67.33058862801825
}
```

The same thing occurred several times in the last days. Is it a right behavior? 

I don't know what is going on. In the log files I see lots of `ProcessClusterEventTimeoutException`. I'm using Elasticsearch 2.0

Thanks for any advice
</description><key id="123627689">15626</key><summary>Unresponsive cluster: weird fluctuating behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vicvega</reporter><labels /><created>2015-12-23T09:31:17Z</created><updated>2015-12-24T09:01:16Z</updated><resolved>2015-12-24T09:01:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-12-23T21:33:01Z" id="166993278">Please join us at https://discuss.elastic.co/ or at #elasticsearch on Freenode for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment><comment author="vicvega" created="2015-12-24T09:01:16Z" id="167072132">ok
sorry
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log warning if minimum_master_nodes set to less than quorum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15625</link><project id="" key="" /><description>The setting `minimum_master_nodes` is crucial to avoid split brains in a cluster. In order to avoid data loss, it should always be configured to at least a quorum (majority) of master-eligible nodes.

I suggest adding a warning to the logs on the master node if the value is set to less than quorum of master-eligible nodes.
</description><key id="123627086">15625</key><summary>Log warning if minimum_master_nodes set to less than quorum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Discovery</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2015-12-23T09:27:02Z</created><updated>2016-05-26T09:19:21Z</updated><resolved>2016-05-25T14:49:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T17:27:52Z" id="206476613">I think this looks good but you added the "discuss" label @ywelsch, is this something contentious? :)
</comment><comment author="ywelsch" created="2016-04-11T08:05:23Z" id="208215283">@dakrone We discussed this PR during Fixit Friday 3+ months ago and agreed that it would be good change to make. @bleskes said he had some concerns w.r.t. the implementation though but did not follow-up with any specifics. I'm removing the discuss label and letting it be picked up for review (@bleskes input welcome).
</comment><comment author="bleskes" created="2016-04-11T08:57:26Z" id="208238734">I think we're trying to be overly smart here (doing things on publish). I think the logging (and in the future protection, see below) should be done around node join time (i.e., when we process a join that will bring us above the threshold set by the current min master node) and when we process the results of pinging and elect a master using a too low min master node setting.

Long term I think we should prevent a node from joining and fail elections if it will bring min master node to an illegal value (and we're in production mode).

Also - I think we should make the setting validation stronger and fail the update. Now we just fail setting min_master_node to  a number that will make the master step down immediately (&gt; # nodes).

I suggest we do logging first and get this in. We should then a do a quick followup with the setting validation part and discuss the possible for being even stricter.
</comment><comment author="ywelsch" created="2016-05-24T16:18:10Z" id="221324490">@bleskes I've updated the PR. Please check if it is more to your taste.
</comment><comment author="jasontedor" created="2016-05-25T00:15:51Z" id="221439442">I like a lot; it's a great first step towards stricter enforcement. LGTM.
</comment><comment author="bleskes" created="2016-05-25T09:52:51Z" id="221526561">LGTM2 - agreed it's a great first step. @ywelsch can you open follow up issues for the rest of things discussed here?
</comment><comment author="ywelsch" created="2016-05-25T15:10:45Z" id="221607419">Thanks for the reviews. I've opened #18573 for further discussion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>One server SERVER_UNAVAILBLE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15624</link><project id="" key="" /><description>```
[2015-12-23 07:28:50,395][INFO ][cluster.service          ] [elasticsearch5eu] detected_master {elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1}, added {{elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1},}, reason: zen-disco-receive(from master [{elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1}])
[2015-12-23 07:28:50,397][INFO ][discovery.zen            ] [elasticsearch5eu] master_left [{elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1}], reason [transport disconnected]
[2015-12-23 07:28:50,397][INFO ][discovery.zen            ] [elasticsearch5eu] failed to send join request to master [{elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1}], reason [NodeDisconnectedException[[elasticsearch4eu][10.240.0.27:9300][internal:discovery/zen/join] disconnected]]
[2015-12-23 07:28:50,397][WARN ][discovery.zen            ] [elasticsearch5eu] master left (reason = transport disconnected), current nodes: {{elasticsearch3eu}{p5GtQctYTUmBxt9ICB8NrA}{10.240.0.22}{10.240.0.22:9300}{max_local_storage_nodes=1},{elasticsearch5eu}{-PXOfPkwQRSPYpwsLpapag}{10.240.0.29}{10.240.0.29:9300}{max_local_storage_nodes=1},}
[2015-12-23 07:28:50,397][INFO ][cluster.service          ] [elasticsearch5eu] removed {{elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1},}, reason: zen-disco-master_failed ({elasticsearch4eu}{mwj3SPCCTxC6Hs-vVUi7DA}{10.240.0.27}{10.240.0.27:9300}{max_local_storage_nodes=1})
[2015-12-23 07:28:50,397][WARN ][cluster.service          ] [elasticsearch5eu] failed to notify ClusterStateListener
java.lang.IllegalStateException: master not available when registering auto-generated license
    at org.elasticsearch.license.plugin.core.LicensesService.requestTrialLicense(LicensesService.java:750)
    at org.elasticsearch.license.plugin.core.LicensesService.clusterChanged(LicensesService.java:484)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-12-23 07:28:51,422][INFO ][rest.suppressed          ] /_bulk Params: {}
ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/2/no master];]
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:154)
    at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedRaiseException(ClusterBlocks.java:144)
    at org.elasticsearch.action.bulk.TransportBulkAction.executeBulk(TransportBulkAction.java:212)
    at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:159)
    at org.elasticsearch.action.bulk.TransportBulkAction.doExecute(TransportBulkAction.java:71)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:424)
    at org.elasticsearch.rest.action.bulk.RestBulkAction.handleRequest(RestBulkAction.java:90)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:348)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:194)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:135)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:452)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="123614309">15624</key><summary>One server SERVER_UNAVAILBLE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaharmor</reporter><labels /><created>2015-12-23T07:30:40Z</created><updated>2015-12-23T08:20:44Z</updated><resolved>2015-12-23T08:20:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-23T08:20:44Z" id="166832173">apparently your master left and the node can't find another one.... please use our discuss forum to ask questions
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extra quotes in template query rendering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15623</link><project id="" key="" /><description>I used a template query like below.

``` json
GET /_search/template
{
"inline":{
      "size":20,
      "explain":false,
      "query":{
         "bool":{
            "should":[
               "{{#my_field}}",
               {
                  "dis_max":{
                     "tie_breaker":0,
                     "queries":[
                        {
                           "multi_match":{
                              "query":"{{my_value}}",
                              "fields":"{{.}}_*_value^100",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":0
                           }
                        },
                        {
                           "multi_match":{
                              "query":"{{my_value}}",
                              "fields":"{{.}}_*_value^3",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":2
                           }
                        },
                        {
                           "multi_match":{
                              "query":"{{my_value}}",
                              "fields":"{{.}}_*_nGramValue^3",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":0
                           }
                        }
                     ]
                  }
               },
               "{{/my_field}}"
            ]
         }
      }
   },
   "params":{
      "my_field":[
         "1_1",
         "1_2"
      ],
      "my_value":"book"
   }
}
```

when I execute this template query, I see an error in parsing query from Elastic Search Engine. I rendered it to see what query is generated, and I found out that above template query is rendered as:

``` json
{
   "template_output":{
      "size":20,
      "explain":false,
      "query":{
         "bool":{
            "should":[
               "",
               {
                  "dis_max":{
                     "tie_breaker":0,
                     "queries":[
                        {
                           "multi_match":{
                              "query":"book",
                              "fields":"1_1_*_value^100",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":0
                           }
                        },
                        {
                           "multi_match":{
                              "query":"book",
                              "fields":"1_1_*_value^3",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":2
                           }
                        },
                        {
                           "multi_match":{
                              "query":"book",
                              "fields":"1_1_*_nGramValue^3",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":0
                           }
                        }
                     ]
                  }
               },
               "",
               {
                  "dis_max":{
                     "tie_breaker":0,
                     "queries":[
                        {
                           "multi_match":{
                              "query":"book",
                              "fields":"1_2_*_value^100",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":0
                           }
                        },
                        {
                           "multi_match":{
                              "query":"book",
                              "fields":"1_2_*_value^3",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":2
                           }
                        },
                        {
                           "multi_match":{
                              "query":"book",
                              "fields":"1_2_*_nGramValue^3",
                              "type":"best_fields",
                              "tie_breaker":0,
                              "fuzziness":0
                           }
                        }
                     ]
                  }
               },
               ""
            ]
         }
      }
   }
}
```

The result has some extra quote ("") and my query does not execute. why these quotes are added?
</description><key id="123614307">15623</key><summary>Extra quotes in template query rendering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">ghasem1992</reporter><labels><label>bug</label></labels><created>2015-12-23T07:30:38Z</created><updated>2016-01-10T11:08:19Z</updated><resolved>2016-01-10T11:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-23T08:23:01Z" id="166832391">It seems like a problem in that example, where did you get that from? @MaineC can you take a look at this?
</comment><comment author="MaineC" created="2016-01-07T11:06:43Z" id="169631854">@ghasem1992 after re-reading the docs myself I think you ran into the following issue: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html#_conditional_clauses
</comment><comment author="clintongormley" created="2016-01-10T11:08:19Z" id="170334800">Agreed - this template needs to be passed as a string, not as JSON
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Primary shard stuck initialising</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15622</link><project id="" key="" /><description>Hi,

This is related to https://github.com/elastic/elasticsearch/issues/15431 however now I have some more info. I am processing over 200GB of PA firewall logs per day. The cluster is working really well until I make any changes. I just started rolling in 2.1.1 into the 2.1.0 cluster and when the shards started relocating one of the primary shards got stuck initialising. I couldn't get it working so I wound up trashing the whole cluster and deploying the whole thing again with just 2.1.1. Not a massive task as we have automated deployment so no problem there. The logs started streaming in again and then I started restoring my data which was working well until again, one of the primary shards got stuck initialising again. It's also an active shard, lots of data trying to go in there. I then tried deleting the index and have it dynamically create again however the primary shard is stuck again and I have this error in the logs...

[2015-12-23 15:47:18,442][DEBUG][action.admin.indices.recovery] [elkrp14] [indices:monitor/recovery] failed to execute operation for shard [[pa_traffic-2015.12.23][4], node[ijdqvmZHTi66urgpuvCyUQ], [P], v[1], s[INITIALIZING], a[id=3A8c4qmfSU6cggJqOAu9bA], unassigned_info[[reason=INDEX_CREATED], at[2015-12-23T05:38:42.740Z]]]
[pa_traffic-2015.12.23][[pa_traffic-2015.12.23][4]] BroadcastShardOperationFailedException[operation indices:monitor/recovery failed]; nested: IndexNotFoundException[no such index];
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
    at org.elasticsearch.shield.transport.ShieldServerTransportService$ProfileSecuredRequestHandler.messageReceived(ShieldServerTransportService.java:165)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [pa_traffic-2015.12.23] IndexNotFoundException[no such index]
    at org.elasticsearch.indices.IndicesService.indexServiceSafe(IndicesService.java:310)
    at org.elasticsearch.action.admin.indices.recovery.TransportRecoveryAction.shardOperation(TransportRecoveryAction.java:102)
    at org.elasticsearch.action.admin.indices.recovery.TransportRecoveryAction.shardOperation(TransportRecoveryAction.java:52)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
    ... 8 more

Any ideas?

Cheers,

Marty
</description><key id="123606895">15622</key><summary>Primary shard stuck initialising</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels /><created>2015-12-23T06:01:55Z</created><updated>2016-01-07T16:45:57Z</updated><resolved>2016-01-06T04:07:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-23T08:27:00Z" id="166832748">lemme ask some questions:
- when you say _"until I make any changes"_ what do you mean exactly?
- _"when the shards started relocating one of the primary shards got stuck initialising."_ what does that mean?
  - how long is it in that state until you do something
  - are you actively indexing into that shard / index
- do you have any throtteling enabled?
- do you have any allocaiton decider settings in your cluster?
- can you share your index settings / cluster settings?
- is your cluster RED while this all happens?
</comment><comment author="robertsmarty" created="2015-12-23T11:06:35Z" id="166863973">Sorry for being so general with my comments. The changes which caused the issue in the past have been when cycling in and out new nodes, restoring data and on dynamic index creation when the clock ticks over to the next day.

In all circumstances I am actively indexing about 120-180k of events per minute into elasticsearch with no throttling and no allocation decider settings. I have 5x r3.xlarge nodes behind an ELB hosted in AWS and the nodes do not appear to be under a large amount of load. There is also loads of disk space with more than 400GB free on each node. I am only using this cluster to process these logs and therefore I have not changed the default settings. The index has all non analysed fields and is using doc values. There are also 5 shards and 1 replicas.

I have also been monitoring rejected bulk inserts and it all looks ok. This issue seems to occur very suddenly. Usually it ends up with 4 out of the 5 primary shards started and one shard that stays in an initialising state. I've waited for up to 30 minutes before taking action.

I understand that the cluster can be in a red state while restoring as the shards can be initialising for some time however it is the active shards receiving PA firewall logs from logstash that have the issue.

I have the cluster in a good state again now but I can reproduce the issue easily. This issue has occurred under different conditions however today it was initiated by rolling in 2.1.1 into the cluster and rolling out one of the 2.1.0 nodes. This caused the shards today's index to be relocated and one of them got stuck in the initialising state.

I'll attach the config shortly.
</comment><comment author="robertsmarty" created="2015-12-23T11:21:06Z" id="166866017">[_nodes.txt](https://github.com/elastic/elasticsearch/files/70665/_nodes.txt)
</comment><comment author="s1monw" created="2015-12-23T13:14:32Z" id="166891380">&gt; I have the cluster in a good state again now but I can reproduce the issue easily. This issue has occurred under different conditions however today it was initiated by rolling in 2.1.1 into the cluster and rolling out one of the 2.1.0 nodes. This caused the shards today's index to be relocated and one of them got stuck in the initialising state.

I still don't fully understand, your index is green (all shards are allocated) then you do a rolling restart and shards start to initialize on a different node. Then one of the shards are stuck in initialization state? Do you have replicas for your index? 
</comment><comment author="robertsmarty" created="2015-12-23T23:33:27Z" id="167008929">I've just removed a node and added a new one. The cluster stayed in a yellow state until the new node was available which makes sense as some of the replicas would have become primary and new replicas would be in the process of being initialised or unassigned. Once the new node was available the cluster started moving things around as it does however this is the current state...

pa_traffic-2015.12.23        4 r STARTED      21353381  11.6gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.23        4 p STARTED      21353630  11.6gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.23        1 p STARTED      21356538  11.4gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.23        1 r STARTED      21356538  11.6gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.23        2 p STARTED      21351154  11.4gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.23        2 r STARTED      21351154  11.6gb 10.10.60.146 elkrp14 
pa_traffic-2015.12.23        3 p STARTED      21351399  11.4gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.23        3 r INITIALIZING                  10.10.60.146 elkrp14 
pa_traffic-2015.12.23        0 p STARTED      21354418  11.6gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.23        0 r STARTED      21354414  11.4gb 10.10.60.32  elkrp11 
.kibana                      0 p STARTED             3  28.1kb 10.10.60.46  elkrp9  
.kibana                      0 r UNASSIGNED  
.shield_audit_log-2015.12.23 1 p STARTED       3354547  10.8gb 10.10.60.121 elkrp7  
.shield_audit_log-2015.12.23 1 r STARTED       3354547    12gb 10.10.60.58  elkrp13 
.shield_audit_log-2015.12.23 2 p INITIALIZING                  10.10.60.146 elkrp14 
.shield_audit_log-2015.12.23 2 r UNASSIGNED  
.shield_audit_log-2015.12.23 0 p STARTED       3245269  12.7gb 10.10.60.121 elkrp7  
.shield_audit_log-2015.12.23 0 r STARTED       3245269    11gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.20        4 p STARTED      23091307  12.1gb 10.10.60.146 elkrp14 
pa_traffic-2015.12.20        4 r STARTED      23091307  12.1gb 10.10.60.32  elkrp11 
pa_traffic-2015.12.20        1 r STARTED      23090686    12gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.20        1 p STARTED      23090686    12gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.20        2 p STARTED      23085383    12gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.20        2 r UNASSIGNED  
pa_traffic-2015.12.20        3 p STARTED      23085349    12gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.20        3 r STARTED      23085349    12gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.20        0 p STARTED      23084229    12gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.20        0 r STARTED      23084229    12gb 10.10.60.58  elkrp13 
.marvel-es-data              0 p STARTED             9 255.8kb 10.10.60.58  elkrp13 
.marvel-es-data              0 r STARTED             9    17kb 10.10.60.46  elkrp9  
pa_traffic-2015.12.21        4 p STARTED      31254221  16.7gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.21        4 r STARTED      31254221  16.7gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.21        1 p STARTED      31252701  16.8gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.21        1 r UNASSIGNED  
pa_traffic-2015.12.21        2 p STARTED      31246488  16.8gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.21        2 r UNASSIGNED  
pa_traffic-2015.12.21        3 r STARTED      31238741  16.8gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.21        3 p STARTED      31238741  16.8gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.21        0 p STARTED      31249916  16.8gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.21        0 r STARTED      31249916  16.8gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.22        4 p STARTED      13337219   7.1gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.22        4 r UNASSIGNED  
pa_traffic-2015.12.22        1 r STARTED      13721140   7.4gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.22        1 p STARTED      13721140   7.4gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.22        2 r STARTED      13582002   7.3gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.22        2 p STARTED      13582002   7.3gb 10.10.60.46  elkrp9  
pa_traffic-2015.12.22        3 p STARTED      13643624   7.3gb 10.10.60.58  elkrp13 
pa_traffic-2015.12.22        3 r UNASSIGNED  
pa_traffic-2015.12.22        0 p STARTED      13571877   7.3gb 10.10.60.121 elkrp7  
pa_traffic-2015.12.22        0 r UNASSIGNED  
.marvel-es-2015.12.23        0 p STARTED        159014  67.6mb 10.10.60.121 elkrp7  
.marvel-es-2015.12.23        0 r STARTED        159015  66.5mb 10.10.60.58  elkrp13 

You can see the problem here...

.shield_audit_log-2015.12.23 2 p INITIALIZING                  10.10.60.146 elkrp14 
.shield_audit_log-2015.12.23 2 r UNASSIGNED              

This is not a shield issue as it also occurs with the pa_traffic. I'm assuming that the replica is unassigned from when I removed the node. I'm not sure why the primary would be initialising. Perhaps this may provide some insight?

[2015-12-24 09:18:24,681][DEBUG][action.admin.indices.stats] [elkrp14] [indices:monitor/stats] failed to execute operation for shard [[.shield_audit_log-2015.12.23][2], node[WubrhkLySOecswSxuP1Oyg], [P], v[15], s[INITIALIZING], a[id=MoCPzrqVSU6rsraDA1yIYQ], unassigned_info[[reason=NODE_LEFT], at[2015-12-23T23:03:37.884Z], details[node_left[WubrhkLySOecswSxuP1Oyg]]]]
[.shield_audit_log-2015.12.23][[.shield_audit_log-2015.12.23][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
    at org.elasticsearch.shield.transport.ShieldServerTransportService$ProfileSecuredRequestHandler.messageReceived(ShieldServerTransportService.java:165)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [.shield_audit_log-2015.12.23][[.shield_audit_log-2015.12.23][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
    at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
    at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
    ... 8 more
</comment><comment author="robertsmarty" created="2015-12-29T03:36:21Z" id="167710776">Just come back after the break (4 days) and the cluster is looking good. Decided to kill a node and see what happens. After the new node joins I can see replicas initialising and some unassigned replicas. I'm assuming the unassigned replicas are due to a maximum number of initialising shards that are permitted due to current settings. After about 30 mins or so this happens...

pa_traffic-2015.12.29        0 p INITIALIZING                  10.10.60.58  elkrp13 
pa_traffic-2015.12.29        0 r UNASSIGNED 

And the cluster went RED. After about another 30 mins it changes to STARTED!

pa_traffic-2015.12.28        0 p STARTED    33679290  17.8gb 10.10.60.58  elkrp13  
pa_traffic-2015.12.28        0 r STARTED    33679290  17.8gb 10.10.60.46  elkrp9 

However, I lost 30 mins of pa_traffic logs.
</comment><comment author="robertsmarty" created="2015-12-29T22:34:01Z" id="167892915">Some more info. To improve resilience I thought that adding an additional replica might help however after removing a node and adding another I still ended up in this state after the new node joined the cluster...

pa_traffic-2015.12.29        3 p INITIALIZING                  10.10.60.46  elkrp9  
pa_traffic-2015.12.29        3 r UNASSIGNED  
pa_traffic-2015.12.29        3 r UNASSIGNED 

I'll go back to my original state of 1 replica so I can do more testing.
</comment><comment author="robertsmarty" created="2015-12-29T22:35:08Z" id="167893052">Re last comment, the primary shard is stuck initialising after 11 hours.
</comment><comment author="robertsmarty" created="2015-12-29T22:52:04Z" id="167895262">Here is the error in the log...

[2015-12-30 08:42:02,661][DEBUG][action.admin.indices.stats] [elkrp9] [indices:monitor/stats] failed to execute operation for shard [[pa_traffic-2015.12.29][3], node[4a3qk_8bQgeZ90ii33WwOw], [P], v[12], s[INITIALIZING], a[id=TdUeaEvyRKKfaqiui4BBwA], unassigned_info[[reason=NODE_LEFT], at[2015-12-29T11:26:49.382Z], details[node_left[zia7--P2Rs6jkxjquILX2w]]]]
[pa_traffic-2015.12.29][[pa_traffic-2015.12.29][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
    at org.elasticsearch.shield.transport.ShieldServerTransportService$ProfileSecuredRequestHandler.messageReceived(ShieldServerTransportService.java:165)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [pa_traffic-2015.12.29][[pa_traffic-2015.12.29][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
    at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
    at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
    ... 8 more

Also many of these errors...

[2015-12-30 08:42:02,681][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [access_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:02,688][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [access_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:02,712][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [access_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:02,712][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [access_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:02,716][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [access_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:02,910][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [connection_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:02,958][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [connection_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:03,109][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [connection_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
[2015-12-30 08:42:03,111][WARN ][shield.audit.index       ] [elkrp9] failed to index audit event: [connection_granted]. queue is full; bulk processor may not be able to keep up or has stopped indexing.
</comment><comment author="jasontedor" created="2015-12-29T23:51:07Z" id="167901395">The logging from `TransportBroadcastByNodeAction` is harmless and is slated to be removed in future patch releases of Elasticsearch from #14950. It just means that management actions (from Marvel?) are being executed against shards that aren't ready for such actions (from the shard that you say is stuck initializing?).
</comment><comment author="robertsmarty" created="2015-12-30T13:00:06Z" id="167992415">I've been looking further into the logs and I can see events showing the master not being able to communicate with all node and offending nodes reporting that they can't ping the master...

From the master...

[2015-12-30 21:55:15,665][WARN ][discovery.zen.publish    ] [elkrp4] timed out waiting for all nodes to process published state [1403](timeout [30s], pending nodes: [{elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}, {elkrp11}{I7U0iJwYQH6emidwdosnCg}{10.10.60.32}{10.10.60.32:9300}{master=true}])
[2015-12-30 21:55:15,669][WARN ][cluster.service          ] [elkrp4] cluster state update task [shard-failed ([.marvel-es-data][0], node[oNTMquLMS_mK1B0jnf-Log], [R], v[42], s[INITIALIZING], a[id=LUCMR3VrQ5S8rG4V9khyHA], unassigned_info[[reason=NODE_LEFT], at[2015-12-30T11:04:07.286Z], details[node_left[9nL6sGpGRC6YxPdWSrSYRA]]]), message [failed to perform indices:data/write/bulk[s] on replica on node {elkrp7}{oNTMquLMS_mK1B0jnf-Log}{10.10.60.250}{10.10.60.250:9300}{master=true}]] took 30s above the warn threshold of 30s
[2015-12-30 21:57:28,886][WARN ][discovery.zen.publish    ] [elkrp4] timed out waiting for all nodes to process published state [1404](timeout [30s], pending nodes: [{elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}, {elkrp11}{I7U0iJwYQH6emidwdosnCg}{10.10.60.32}{10.10.60.32:9300}{master=true}])
[2015-12-30 21:57:28,890][WARN ][cluster.service          ] [elkrp4] cluster state update task [shard-started ([pa_traffic-2015.12.30][4], node[vhYUtKacQrKqBv_NDH-kTg], [R], v[13], s[INITIALIZING], a[id=qotxyLhuQRS68r1B0d2BuA], unassigned_info[[reason=NODE_LEFT], at[2015-12-30T11:04:07.286Z], details[node_left[9nL6sGpGRC6YxPdWSrSYRA]]]), reason [after recovery (replica) from node [{elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}]]] took 30s above the warn threshold of 30s
[2015-12-30 22:00:49,471][DEBUG][discovery.zen.publish    ] [elkrp4] failed to send cluster state to {elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}
NodeDisconnectedException[[elkrp13][10.10.60.58:9300][internal:discovery/zen/publish] disconnected]
[2015-12-30 22:00:49,471][TRACE][discovery.zen.fd         ] [elkrp4] [node  ] [{elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}] transport disconnected
[2015-12-30 22:00:49,471][DEBUG][discovery.zen.publish    ] [elkrp4] failed to send cluster state to {elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}
NodeDisconnectedException[[elkrp13][10.10.60.58:9300][internal:discovery/zen/publish] disconnected]
[2015-12-30 22:00:49,473][DEBUG][discovery.zen.publish    ] [elkrp4] failed to send cluster state to {elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}
NodeDisconnectedException[[elkrp13][10.10.60.58:9300][internal:discovery/zen/publish] disconnected]
[2015-12-30 22:00:49,496][DEBUG][gateway                  ] [elkrp4] [pa_traffic-2015.12.28][2]: delaying allocation of [[pa_traffic-2015.12.28][2], node[null], [R], v[36], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-12-30T12:00:49.471Z], details[node_left[AyRjNl8dSbiYAUFW_c96-Q]]]] for [1m]
[2015-12-30 22:00:49,496][DEBUG][gateway                  ] [elkrp4] [pa_traffic-2015.12.26][1]: delaying allocation of [[pa_traffic-2015.12.26][1], node[null], [R], v[43], s[UNASSIGNED], unassigned_info[[reason=NODE_LEFT], at[2015-12-30T12:00:49.471Z], details[node_left[AyRjNl8dSbiYAUFW_c96-Q]]]] for [1m]
[2015-12-30 22:00:49,497][INFO ][cluster.service          ] [elkrp4] removed {{elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true},}, reason: zen-disco-node_failed({elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true}), reason transport disconnected
[2015-12-30 22:00:49,715][DEBUG][action.admin.cluster.node.info] [elkrp4] failed to execute on node [AyRjNl8dSbiYAUFW_c96-Q]
SendRequestTransportException[[elkrp13][10.10.60.58:9300][cluster:monitor/nodes/info[n]]]; nested: NodeNotConnectedException[[elkrp13][10.10.60.58:9300]

From the offending node...

[2015-12-30 22:00:54,549][TRACE][discovery.zen.fd         ] [elkrp13] [master] failed to ping [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], retry [1] out of [3]
RemoteTransportException[[elkrp4][10.10.60.236:9300][internal:discovery/zen/fd/master_ping]]; nested: IllegalStateException;
Caused by: java.lang.IllegalStateException
[2015-12-30 22:00:54,549][TRACE][discovery.zen.fd         ] [elkrp13] [master] failed to ping [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], retry [2] out of [3]
RemoteTransportException[[elkrp4][10.10.60.236:9300][internal:discovery/zen/fd/master_ping]]; nested: IllegalStateException;
Caused by: java.lang.IllegalStateException
[2015-12-30 22:00:54,550][TRACE][discovery.zen.fd         ] [elkrp13] [master] failed to ping [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], retry [3] out of [3]
RemoteTransportException[[elkrp4][10.10.60.236:9300][internal:discovery/zen/fd/master_ping]]; nested: IllegalStateException;
Caused by: java.lang.IllegalStateException
[2015-12-30 22:00:54,550][DEBUG][discovery.zen.fd         ] [elkrp13] [master] failed to ping [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], tried [3] times, each with maximum [30s] timeout
[2015-12-30 22:00:54,550][DEBUG][discovery.zen.fd         ] [elkrp13] [master] stopping fault detection against master [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], reason [master failure, failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2015-12-30 22:00:54,551][INFO ][discovery.ec2            ] [elkrp13] master_left [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2015-12-30 22:00:54,551][TRACE][discovery.ec2            ] [elkrp13] removed [0] pending cluster states
[2015-12-30 22:00:54,551][WARN ][discovery.ec2            ] [elkrp13] master left (reason = failed to ping, tried [3] times, each with  maximum [30s] timeout), current nodes: {{elkrp7}{oNTMquLMS_mK1B0jnf-Log}{10.10.60.250}{10.10.60.250:9300}{master=true},{elkrp13}{AyRjNl8dSbiYAUFW_c96-Q}{10.10.60.58}{10.10.60.58:9300}{master=true},{elkrp11}{I7U0iJwYQH6emidwdosnCg}{10.10.60.32}{10.10.60.32:9300}{master=true},{elkrp12}{vhYUtKacQrKqBv_NDH-kTg}{10.10.60.169}{10.10.60.169:9300}{master=true},}
[2015-12-30 22:00:54,551][INFO ][cluster.service          ] [elkrp13] removed {{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true},}, reason: zen-disco-master_failed ({elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true})
[2015-12-30 22:00:54,551][TRACE][discovery.zen            ] [elkrp13] starting to accumulate joins

Then about 30 seconds later...

[2015-12-30 22:01:21,804][DEBUG][discovery.zen.publish    ] [elkrp13] received full cluster state version 1414 with size 13087
[2015-12-30 22:01:21,804][DEBUG][discovery.zen.fd         ] [elkrp13] [master] restarting fault detection against master [{elkrp4}{0ZqKG13FSJCwe9VNtql8hw}{10.10.60.236}{10.10.60.236:9300}{master=true}], reason [new cluster state received and we are monitoring the wrong master [null]]
[2015-12-30 22:01:21,804][DEBUG][discovery.ec2            ] [elkrp13] got first state from fresh master [0ZqKG13FSJCwe9VNtql8hw]

So it looks like some communication issue is the catalyst. This only happens under certain circumstances such as doing a restore, when a dynamic index is created or when cycling in and out nodes. Perhaps caused by load? CPU, heap and disk all look fine

You can see node elkrp13 takes 1ms to decide that the master is no longer available and then goes through the process of finding another which takes about 30 seconds. After that, it discovers the same master again however it's active primary shards remain in an INITIALIZING state.

There is about 120-180k of events coming in per minute. Is it possible that this 30 second blip combined with this volume of events creates a situation where the cluster can't recover?
</comment><comment author="s1monw" created="2016-01-04T08:38:15Z" id="168611688">you might have answered it already but when you stop/start a node or take one out is your cluster green? 
I mean the primary is unassigned and starts initializing and I wonder how this could happen if a replica is active? If there is no replica active for this primary then this is expected that the primary initializes. Why it's not becoming active is a different story...
</comment><comment author="robertsmarty" created="2016-01-04T09:12:18Z" id="168617449">Hi Simon, thanks for your reply. I just purchased a license and now going through support via my gold subscription. We are using host based firewall which is dropping idle connections after 5 minutes. I'm assuming the issue has some thing to do with that although I'm currently waiting on a response regarding why the connection is idle and not kept alive via default elasticsearch settings. I have a meeting with support in 14 hours. We haven't got to the bottom of it yet however I'll update this issue in case someone else experiences the same problem.
</comment><comment author="s1monw" created="2016-01-04T09:51:02Z" id="168626780">thanks @robertsmarty 
</comment><comment author="robertsmarty" created="2016-01-05T22:52:14Z" id="169159873">It appears that this issue may be caused by the Trend Deep Security agent that we have installed. Profile updates are resetting the connection table and then dropping network connections...

http://esupport.trendmicro.com/solution/en-US/1096766.aspx

This issue has been fixed in Deep Security 9.5 Service Pack (SP) 1 Patch 3.

I'll test and post an update. This could also affect other customers that use this agent due to very strict security requirements.
</comment><comment author="robertsmarty" created="2016-01-06T04:07:22Z" id="169215264">Confirmed this is caused by Trend Deep Security agent &lt; 9.5 Service Pack (SP) 1 Patch 3 when stateful inspection is enabled.
</comment><comment author="s1monw" created="2016-01-07T16:45:57Z" id="169722959">@robertsmarty glad we sorted it out &amp; thanks for reporting back!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How can I know which files from the log information is collected&#65311;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15621</link><project id="" key="" /><description>I used the fluentd-elasticsearch-plugin to collect log, 
now I want to know which files from the log is collected by ES api.
Thanks.
</description><key id="123585888">15621</key><summary>How can I know which files from the log information is collected&#65311;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kakalo24</reporter><labels /><created>2015-12-23T01:34:50Z</created><updated>2015-12-23T21:33:55Z</updated><resolved>2015-12-23T21:33:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-12-23T21:33:55Z" id="166993649">Please join us at https://discuss.elastic.co/ or at #elasticsearch on Freenode for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index creation via tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15620</link><project id="" key="" /><description>Currently the tribe node does not support things like index and mapping creation. This presents a problem for Kibana users that utilize a tribe node, requiring them to first point kibana at a single cluster, then stop it and point it at the tribe node. 

Ideally the tribe node could be configured to route requests matching a given pattern to a particular cluster. It could be configured much the way we do the `tribe.blocks` section today, but say, `tribe.route`. This would/could also work for users that need to write to indices that exist on several clusters that a tribe node points to

Something like:

```
tribe:
  routes:
    indices.create: 
      .kibana: cluster1
    indices.write:
      myIndex-*: cluster2
```
</description><key id="123564582">15620</key><summary>Index creation via tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels><label>:Tribe Node</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-22T22:03:50Z</created><updated>2016-02-29T20:21:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T22:56:57Z" id="166756613">Do we need action-level granularity or would it be enough in most cases to just be able to specify which cluster to use when there is ambiguity?
</comment><comment author="rashidkpc" created="2015-12-22T23:50:12Z" id="166764044">Probably enough to just specify which cluster to use when its ambiguous 
</comment><comment author="salimane" created="2016-01-23T13:48:24Z" id="174187761">@jpountz @rashidkpc this would be awesome. 

An addition to this could be, somehow if the route feature could support some sort of "tribe sharding". Let's say you have clusters based on a field (date, country or ....) and you have a tribe node in front of them. If you query with a date field, the tribe node should just route that query to the cluster that only has that date or country.

What I'm really looking for is a sharding feature with the tribe node, is it already possible ? how could it be possible ?

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiple types in one index caused to ES collapsed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15619</link><project id="" key="" /><description>Hi

A few days ago I started to use the _type meta as part of my data modeling conventions.
The convention defined that I have one index and under this index many accounts + product name.
The mappings that I created in a /_template is using only one default without any exceptions between the types since I expect that all the types will have the same fields but the _type meta.

I started to ran some tests and create tons of accounts and my jvm throw out of memory, which means the ES instance collapsed.  
When I GET the template of this index, I realized that it creates nested object for each account and the file reached the 1M lines!

My question is how i can avoid this behaviour? that the mapping file is so big? What I need to do so the file will remain short as I build it from the beginning, because I want to keep using the _type but not with so many resources from my machine.

Thanks
</description><key id="123552073">15619</key><summary>Multiple types in one index caused to ES collapsed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">panda87</reporter><labels /><created>2015-12-22T20:37:31Z</created><updated>2015-12-22T21:33:18Z</updated><resolved>2015-12-22T21:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T21:29:09Z" id="166734381">Right, each type holds its own mapping. So having many types can make the cluster state really large. I would advise to put all your accounts into the same index and only use a field to differentiate them (which is what types do as well) and filter at search time.

Please use https://discuss.elastic.co in the future for such questions and use github issues for bugs / feature requests. Thanks!
</comment><comment author="panda87" created="2015-12-22T21:33:18Z" id="166735154">Thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rename "pipeline_id" param to "pipeline"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15618</link><project id="" key="" /><description /><key id="123547725">15618</key><summary>rename "pipeline_id" param to "pipeline"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-22T20:08:19Z</created><updated>2015-12-22T20:33:44Z</updated><resolved>2015-12-22T20:33:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-22T20:24:12Z" id="166721591">The rename also needs to be done in the ingest-with-mustache qa module. OTT LGTM
</comment><comment author="talevy" created="2015-12-22T20:30:34Z" id="166722800">updated the `qa/ingest-with-metadata` rest test with the change
</comment><comment author="martijnvg" created="2015-12-22T20:31:12Z" id="166722917">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix build to run correctly on FreeBSD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15617</link><project id="" key="" /><description>This adds the required changes/checks so that the build can run on
FreeBSD.

There are a few things that differ between FreeBSD and Linux:
- CPU probes return -1 for CPU usage
- `hot_threads` cannot be supported on FreeBSD

From OpenJDK's `os_bsd.cpp`:

``` c++
bool os::is_thread_cpu_time_supported() {
  #ifdef __APPLE__
  return true;
  #else
  return false;
  #endif
}
```

So this API now returns (for each FreeBSD node):

```
curl -s localhost:9200/_nodes/hot_threads
::: {Devil Hunter Gabriel}{q8OJnKCcQS6EB9fygU4R4g}{127.0.0.1}{127.0.0.1:9300}
   hot_threads is not supported on FreeBSD
```
- multicast fails in native `join` method - known bug:
  https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=193246

Which causes:

```
1&gt; Caused by: java.net.SocketException: Invalid argument
1&gt;    at java.net.PlainDatagramSocketImpl.join(Native Method)
1&gt;    at java.net.AbstractPlainDatagramSocketImpl.join(AbstractPlainDatagramSocketImpl.java:179)
1&gt;    at java.net.MulticastSocket.joinGroup(MulticastSocket.java:323)
1&gt;    at org.elasticsearch.plugin.discovery.multicast.MulticastChannel$Plain.buildMulticastSocket(MulticastChannel.java:309)
```

So these tests are skipped on FreeBSD.

Resolves #15562
</description><key id="123543304">15617</key><summary>Fix build to run correctly on FreeBSD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T19:37:48Z</created><updated>2016-02-29T16:37:27Z</updated><resolved>2015-12-22T23:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-22T19:38:13Z" id="166711723">I am planning on working on a followup PR for cleaning up some of the hot threads detection logic.
</comment><comment author="nik9000" created="2015-12-22T19:41:25Z" id="166712346">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the PLANE and ARC distance types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15616</link><project id="" key="" /><description>Elasticsearch has 3 ways to compute geo distances:
- PLANE,
- ARC,
- SLOPPY_ARC.

But this is sometimes confusing to our users, see eg #15606. Do we really need to have 3 options or could we just use SLOPPY_ARC all the time? It seems to be like a good trade-off between accuracy and speed?
</description><key id="123538064">15616</key><summary>Remove the PLANE and ARC distance types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Geo</label><label>deprecation</label></labels><created>2015-12-22T19:02:04Z</created><updated>2016-09-30T05:20:45Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T21:29:44Z" id="166734494">cc @nknize 
</comment><comment author="eskibars" created="2015-12-29T18:23:04Z" id="167847879">I think a significant set of users do need the accuracy of ARC.  I'm less certain about usage of PLANE in the current user base, though there are a variety of use cases for rectilinear calculations on even non-geographic data which could be valid.  Would love to hear if/where/how people are using PLANE.

I do think the documentation is a bit confusing in a few ways.  First, we use some different terminology on our [scripting](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html) documentation (in terms of function names and a little bit in the description) vs the [geo distance query](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/query-dsl-geo-distance-query.html) documentation.  Second, we don't have any document (that I'm aware of) to identify what the tradeoffs are in terms of speed and accuracy for the 3 in any quantitative manner, so it's difficult for somebody to make an informed decision.
</comment><comment author="dakrone" created="2015-12-29T18:38:25Z" id="167850223">I definitely think people need all 3. I have had users in training talk about using the geo-distance calculation for "earth-line" planes in games (where the virtual earth really is flat). What's the motivation behind removing it?
</comment><comment author="rmuir" created="2015-12-29T19:10:33Z" id="167855618">&gt; I think a significant set of users do need the accuracy of ARC.

For what exactly? Its not accurate either, we can assume up to 0.3% error (http://www.movable-type.co.uk/scripts/latlong.html)

Personally I think we should just have `ARC` (the explicit `SLOPPY_ARC` selection should be deprecated and alias to it), and we decide how to implement that with the best tradeoffs given what we have. Having "within 1 ulp" of an inaccurate math formula does not necessarily translate into better results. Stuff like https://issues.apache.org/jira/browse/LUCENE-5271 is a great example of how it can be practically improved over time. btw, since https://github.com/elastic/elasticsearch/issues/5192 "accurate" arc is now using methods from SloppyMath.java to be more accurate :)

we can imagine still improving these functions going forward, for example maybe it won't make sense to approximate `cos` at all after https://bugs.openjdk.java.net/browse/JDK-8143353, maybe `asin` gets improvements too, etc etc.
</comment><comment author="nknize" created="2015-12-29T21:42:12Z" id="167883662">I'm +1 for removing `ARC`, `PLANE`, and `FACTOR`

As @rmuir points out the `GeoDistance.ARC` is just a copy of Lucene's `SloppyMath.haversin` implementation which, when using `GeoUtils.earthDiameter` to approximate the ellipsoid, actually gives ~0.65% error, or ~720m error at the equator for 1 degree longitude.

For the most accurate distance calculation (~0.05mm) used in photogrammetry and surveying use-cases, Vincenty or Karney should be used. I committed the vincenty implementation to lucene a while ago and posted some performance and accuracy results [here](https://www.google.com/url?q=https%3A%2F%2Fissues.apache.org%2Fjira%2Fbrowse%2FLUCENE-6908%3FfocusedCommentId%3D15036890%26page%3Dcom.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel%23comment-15036890&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNFOFVDihA0rt_Fw3DmnDo9okZnOrg)

From those benchmarks you can see why vincenty and karney is not used in GeoPoint queries. Instead I've cut over GeoPoint queries from using `SloppyMath.haversin` to an implementation of Sinnott's original haversine publication which gives slightly better accuracy and performance.  For now, if someone wants vincenty level of accuracy they can post filter results in their application using `GeoDistanceUtils.vincentyDistance`.
</comment><comment author="nknize" created="2015-12-29T21:57:50Z" id="167886499">&gt; I have had users in training talk about using the geo-distance calculation for "earth-line" planes in games (where the virtual earth really is flat). What's the motivation behind removing it?

Currently using geo types for flat game maps? or is this something they've expressed interest in trying?

Use-cases like these strengthen the argument for a new `dimensional` field mapper in 3.0 that uses the Lucene 6.0 `Dimensional*Field` and flat map cartesian linear algebra. 
</comment><comment author="dakrone" created="2015-12-29T22:13:25Z" id="167888803">&gt; &gt; I have had users in training talk about using the geo-distance
&gt; &gt; calculation for "earth-line" planes in games (where the virtual earth
&gt; &gt; really is flat). What's the motivation behind removing it?
&gt; 
&gt; Currently using geo types for flat game maps? or is this something
&gt; they've expressed interest in trying?

Yeah, currently using geo for game maps with flat worlds.
</comment><comment author="jpountz" created="2015-12-30T10:55:47Z" id="167976554">&gt; I think we should just have ARC (the explicit SLOPPY_ARC selection should be deprecated and alias to it), and we decide how to implement that with the best tradeoffs given what we have.

+1

&gt; Use-cases like these strengthen the argument for a new dimensional field mapper in 3.0 that uses the Lucene 6.0 Dimensional*Field and flat map cartesian linear algebra. 

I'm unsure whether we will want to have a `dimensional` field directly or rather expose higher-level fields that leverage Lucene's Dimensional*Field but I'm +1 on restricting geo points to actual earth points and exposing different features to deal with the other uses-cases.
</comment><comment author="nknize" created="2015-12-30T16:08:12Z" id="168025135">&gt; I think we should just have ARC (the explicit SLOPPY_ARC selection should be deprecated and alias to it)

+1 With the new GeoPointField in 2.2, the `distance_type` option can be removed. For 2.x I can add a `precision` option that trades performance for accuracy at query time and update the documentation to include expected performance.

&gt; I'm unsure whether we will want to have a dimensional field directly or rather expose higher-level fields that leverage Lucene's Dimensional*Field

+1.
</comment><comment author="clintongormley" created="2016-02-29T20:20:20Z" id="190367811">@nknize can this issue be closed yet?
</comment><comment author="esen" created="2016-09-30T05:20:45Z" id="250661054">I think these distance types should remain. 
- One argument is that you may need to use `plane` for filtering (since it's the fastest but less accurate), and `sloppy_arc` for sorting. Or use both of them for filtering, like first using `plane` filter to get documents in several kilometers radius, then using more accurate `sloppy_arc` for better filtering.
- One more argument is in scripting. If you use lucene expression lang you can only use `haversin`  function(which is lucene sloppy_arc), but there's no way to get latitude and longitude parameters from your location field. You can only get `Single valued document fields`. If you use Groovy lang, then you can get latitude and longitude, but you can't use `haversin`. You can use `plane` or `arc` distances only. I may be wrong, but this is what I understood from the docs.

Btw, great job guys!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>YUM Repository for 1.7 version times out, can we get the yum repo fixed for 1.x versions?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15615</link><project id="" key="" /><description>View the following page:

https://www.elastic.co/guide/en/elasticsearch/reference/1.7/setup-repositories.html#_yum

I tried setting up the yum repository but I get a timeout error. The 2.0 version works (I can add the repo to my box and install via `yum install elasticsearch`), but when I try and add the 1.7 version (which I want to use as FOSElasticaBundle does not support 2.0 yet), I get a timeout error.
</description><key id="123534061">15615</key><summary>YUM Repository for 1.7 version times out, can we get the yum repo fixed for 1.x versions?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lakhman</reporter><labels /><created>2015-12-22T18:39:24Z</created><updated>2015-12-22T18:44:01Z</updated><resolved>2015-12-22T18:43:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lakhman" created="2015-12-22T18:44:01Z" id="166700498">ok my bad, this seemed like a local intermittent issue, sorry.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove query warmers and the warmer API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15614</link><project id="" key="" /><description>Warmers are now barely useful and will be removed in 3.0. Note that this only
removes the warmer API and query-based warmers. We still have warmers internally
for eg. global ordinals.

Close #15607
</description><key id="123525121">15614</key><summary>Remove query warmers and the warmer API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Warmers</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T17:39:39Z</created><updated>2016-01-10T11:00:39Z</updated><resolved>2016-01-07T08:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-22T22:06:35Z" id="166742821">LGTM if we're truly ok with warmers going away.

I didn't see a test that making sure that an old index with warmers will just be ignored. Did I miss it? I think it might be as simple as trying to build an index using xcontent, specifying warmers, and asserting that the index was created without exception.
</comment><comment author="s1monw" created="2015-12-28T14:36:34Z" id="167581552">LGTM - I left one comment about BWC testing
</comment><comment author="jpountz" created="2016-01-05T18:24:26Z" id="169088590">@s1monw @nik9000 I added bw testing so hopefully this should be good to go now.
</comment><comment author="nik9000" created="2016-01-05T20:11:43Z" id="169118131">LGTM
</comment><comment author="jpountz" created="2016-01-07T09:53:15Z" id="169613373">Note: if you are on 2.x and using warmers, the upgrade to 3.0 will be fine, warmers will be ignored during the upgrade.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for types?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15613</link><project id="" key="" /><description>The ability to have several types on the same index is causing problems:
- the mappings APIs need to maintain one mapping per type, yet those mappings can't be independent and keeping them synchronized is complicated (see eg. discussions on #15539)
- it gives the feeling that the system can easily deal with documents that have very different mappings in the same index, which is not true. This is why in 2.0 we added more restrictions on mappings across types. In addition types encourage sparsity and sparse fields cause Lucene to either be slow when there is a special impl for the sparse case (eg. doc values) or use tremendous memory and disk space in spite of the fact that few documents have a value (eg. norms, because a fixed amount of memory is used for every doc, regardless of whether they have a value for this field or not).

Migrating existing users is certainly going to be complicated but this would also make the system more honest to new users about the fact that we can't do index-level multi-tenancy efficiently. Also I suspect that the restrictions that we added in 2.0 (that eg. two fields that have the same name in different types) already made lots of users migrate to a single index per data type instead of folding them into different types of the same index.

See also https://www.elastic.co/blog/index-vs-type.
</description><key id="123521347">15613</key><summary>Remove support for types?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>Meta</label></labels><created>2015-12-22T17:16:20Z</created><updated>2017-06-02T09:02:04Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zygfryd" created="2015-12-22T21:51:36Z" id="166740063">Seems like removing support for types would be blocked on https://github.com/elastic/elasticsearch/issues/11432.

It'd be lovely if this accelerated parent-child relations across indexes though, that'd get rid of a lot of the aforementioned sparsity.
</comment><comment author="nik9000" created="2015-12-22T21:56:20Z" id="166740928">What about divorcing mappings from types? Make mappings an index level feature and types just kind of like part of the id? Would that make them light enough to not get in the way?
</comment><comment author="jpountz" created="2015-12-22T22:38:30Z" id="166752890">@zygfryd indeed

@nik9000 That is an option too. At least it would make clear that there is a single mapping per index and we could stimplify the internals. With this option, I guess types would remain as first-class filters only (eg. we could do index sorting on _type so that filtering on them would be faster)?
</comment><comment author="nik9000" created="2015-12-22T22:40:31Z" id="166753207">&gt; types would remain as first-class filters only

I'd be fine with that.
</comment><comment author="Mogztter" created="2015-12-31T15:38:44Z" id="168212042">I think divorcing mappings from types is a good idea. In my opinion, removing types is too radical.

In our use case (logs management) we have 80+ types of logs and we use a (daily, weekly or monthly) index per project/tenant to be able to handle the load.
We do have a different mappings for each type of logs but mappings are identical across all indices.

&gt; In addition types encourage sparsity and sparse fields cause Lucene to either be slow when there is a special impl for the sparse case (eg. doc values) or use tremendous memory and disk space in spite of the fact that few documents have a value (eg. norms, because a fixed amount of memory is used for every doc, regardless of whether they have a value for this field or not).

If I read you correctly, we should instead use one index per log type ? 
In our use case we have different needs for each project/tenant. Some projects can log as much as 5K logs/sec while others projects only log 10 logs/sec.
So we need to be able to configure replicas/shards and index creation frequency (daily, weekly, monthly...) per project/tenant.

Could you please explain a little bit more your proposal with this use case and with types removed ?

&gt; Make mappings an index level feature 

:+1:
</comment><comment author="jpountz" created="2016-05-30T20:49:14Z" id="222552862">&gt; If I read you correctly, we should instead use one index per log type ? 

Yes. Types are trappy: at first sight they look like an efficient way to have multiple tenants in a single index, but in practice this usually makes things _worse_ than having multiple indices due to the fact that Lucene likes dense data better than sparse data, especially for norms and doc values.

If some tenants have lower indexing rates, they would get fewer shards and/or longer time frames (eg. weekly indices instead of daily).
</comment><comment author="jpountz" created="2016-05-30T20:54:09Z" id="222553336">I first thought divorcing types from mappings would be a good compromise, but types have another issue that they force us to fold the type into the uid, which typically either makes the _uid less efficient (slower indexing and slower gets) if we prepend the type (like today) or more space-intensive if we append the type. So I think we should think about getting rid of types entirely. For instance, maybe we could consider enforcing a single type per index in version X, with APIs still working with either `index/type` or just `index`, and then removing types entirely in version X+1?
</comment><comment author="s1monw" created="2016-05-31T07:18:02Z" id="222610422">I think we should deprecate type in 5.0 and start moving towards index level mappings, uuid per index not per type etc. If somebody really needs the type in the UUID they can still do that I guess. Types can be build on top of es without native support, there is nothing today that prevents you from doing this. It rather complicates things on all end internally without real benefit to the outsite except of the first level API support that someone might find useful but is only syntactic sugar with a potential high price to pay. I am also +1 to remove this in 6.0 entirely and guide folks how to do it correctly.  
</comment><comment author="dadoonet" created="2016-06-02T08:29:01Z" id="223228121">The main concern I have for now is the support of the parent/child feature.
Today we need to have colocated data within the same shard for parent and children in order to perform joins in memory.

Removing types will only allow to do parent/child using the same "kind" of document.

Not super terrible as at the end of the day this is what is happening behind the scene.

So if we had:

```
PUT index/company/1
{
  "name": "elastic"
}
PUT index/employee/11?parent=1
{
  "first_name": "david"
}
```

We will basically have to rewrite this as for example:

```
PUT index/1
{
  "company": {
    "name": "elastic"
  }
}
PUT index/11?parent=1
{
  "employee": {
    "name": "david"
  }
}
```

It means that parent/child will be able to do self referencing as proposed here in #11432.
So as soon as we have support for #11432, I'm totally +1 to remove types.

BTW, may be we should already start to educate people to use only one type per index and use data structures similar to what I proposed in my example?
</comment><comment author="jpountz" created="2016-08-05T15:24:36Z" id="237880437">What about the following plan:
- require 5.0+ indices to have at most one type (which means parent/child does not work)
- add APIs without a type parameter, eg. `POST index/` to index with an auto-generated id or `PUT index/id` to index with an explicit id, remove the requirement to have the type name as a top-level key in mappings, etc. (all this does not need to done in 5.0, this could come in 5.x)
- remove types from APIs in 6.0+
- separately work on another way to expose joins in a reasonable way, as a replacement for parent/child (assuming we want a replacement)

If we are not ready to drop parent/child right now, one trade-off I could consent would be to have a setting that allows indices to have multiple types so that parent/child can be used, but these indices could not be upgraded to 6.0.

For the record, we have some evidence that removing types could help indexing speed quite significantly since we would not have to fold the type name into the uid: https://github.com/elastic/elasticsearch/issues/18154#issuecomment-237851085

Thoughts?
</comment><comment author="rjernst" created="2016-08-11T16:24:40Z" id="239213496">@jpountz I think we should do this, but it seems your proposal has gone unnoticed given the lack of reaction (positive or negative). Can we get some other thoughts on this?

/cc @s1monw @clintongormley 
</comment><comment author="clintongormley" created="2016-08-11T16:26:01Z" id="239213884">@rjernst i'm staring at it as you type :)
</comment><comment author="clintongormley" created="2016-08-12T12:42:15Z" id="239435920">We discussed this in Fix it Friday.  

## Where we want to get to:

We want to remove the concept of types from Elasticsearch, while still supporting parent/child. 
- Field mappings will be at the top level, instead of under type names
- The `_uid` will consist only of the `_id`, not `type#id`.
- Creating a document with ID : `PUT index/_doc/id`
- Creating a document with autogenerated ID : `POST index/_doc`

It's very important to me that we don't leave users behind - we need to give them a smooth upgrade and transition path.

## Proposed path:

**In 5.0:**
- [ ] ~~In new indices, enforce that fields with the same name in different types must have identical mappings.~~
- [x] `POST index` should no longer create an index. #20001
- [x] Check other REST endpoints for similar clashes. #20055

**In 5.x:**
- [x] Add support for setting `enabled:false` to the `_type` field, which will exclude type from the UID and use a fixed type name for (eg) mapping requests/responses. These indices will not support parent/child. #24317
- [ ] ~~Add support for `PUT index/_doc/id`, `POST index/_doc`, and `PUT|POST index/_mapping`~~
- [ ] Add new mechanism for specifying and supporting parent/child.

**In 6.0:**
- [ ] Prevent new indices from being created with more than one type.

**In 7.0:**

- [ ] Remove support for old parent/child
- [&#160;] `type` becomes an optional parameter in URLs, and is completely ignored.  
- [ ] for `GET|PUT _mapping` we add a query string parameter (eg `include_type_name`) which indicates whether the body should include a layer for the type name.  this defaults to `true`.  7.x indices which don't have an explicit type will use the dummy type name `_doc`

**In 8.0**

* [ ] `type` can no longer be specified in URLs
* [ ] the `include_type_name` parameter now defaults to `false`

**In 9.0**

* [ ] remove the `include_type_name` parameter

In 6.0, all existing types from 5.x indices will have identical mappings. We will still have indices with old parent/child implementation.  If we can migrate existing parent/child settings to the new settings, then we could move the "return fields at top level" issue into 6.0.

Alternatively, we could return fields at the top level in 6.0 regardless, and still show types (for old indices with types enabled, or with old parent-child) as a separate section in GET mapping.

**UPDATED TO REFLECT CHANGES IN https://github.com/elastic/elasticsearch/issues/15613#issuecomment-303727982**</comment><comment author="jpountz" created="2016-08-18T07:31:37Z" id="240645902">&gt; Check other REST endpoints for similar clashes.

I think the only other endpoint we need to check is `PUT/POST {index}/_mapping`, which is free today. Am I missing another one?
</comment><comment author="clintongormley" created="2016-08-18T07:47:44Z" id="240648998">`indices.exists_type` (`HEAD {index}/{type}`)  clashes with a typeless `exists` (`HEAD {index}/{id}`)

Perhaps the existing form should be deprecated in favour of `HEAD {index}/_mapping/{type}`

I think that's the lot
</comment><comment author="jpountz" created="2016-08-18T09:59:58Z" id="240679004">&gt; In new indices, enforce that fields with the same name in different types must have identical mappings.

Having started to work on it, this is more challenging than I initially thought. However I think this might not be needed: since we will require at most one type in 6.0+ anyway, we will not have to merge mappings across types in 7.0, so this step is not required for the type removal?
</comment><comment author="kimchy" created="2016-08-22T09:41:02Z" id="241362573">@clintongormley I like the purposed plan! _If_ we can find a way to support the new parent/child format in 5.x with the enabled set to false on _type, it would mean simpler migration down the road. We could start to push for setting this setting for new users.
</comment><comment author="dakrone" created="2016-09-14T21:28:49Z" id="247160644">@clintongormley I believe this:

&gt; In new indices, enforce that fields with the same name in different types must have identical mappings.

Is resolved (from the 5.0 checkboxes):

```
PUT /i?pretty
{
  "mappings": {
    "typea": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "standard"
        }
      }
    },
    "typeb": {
      "properties": {
        "title": {
          "type": "text",
          "analyzer": "english"
        }
      }
    }
  }
}

{
  "error" : {
    "root_cause" : [
      {
        "type" : "mapper_parsing_exception",
        "reason" : "Failed to parse mapping [typea]: Mapper for [title] conflicts with existing mapping in other types:\n[mapper [title] has different [analyzer], mapper [title] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [title] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]"
      }
    ],
    "type" : "mapper_parsing_exception",
    "reason" : "Failed to parse mapping [typea]: Mapper for [title] conflicts with existing mapping in other types:\n[mapper [title] has different [analyzer], mapper [title] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [title] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "Mapper for [title] conflicts with existing mapping in other types:\n[mapper [title] has different [analyzer], mapper [title] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [title] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]"
    }
  },
  "status" : 400
}
```

So I _think_ this is no longer blocking for 5.0, is that correct? (I didn't want to check the box if there was something else missing)
</comment><comment author="KodrAus" created="2016-09-14T22:38:25Z" id="247178703">As an Elasticsearch user and someone who's spent the last few months working on a type mapping client implementation this sounds good to me. From what I can tell from the conversation so far: types aren't _really_ going away, they're just being shifted up to the index level, so the index determines the shape of its one document type, right?
</comment><comment author="dakrone" created="2016-09-14T23:06:13Z" id="247183932">&gt; From what I can tell from the conversation so far: types aren't really going away, they're just being shifted up to the index level, so the index determines the shape of its one document type, right?

That's pretty much correct, once we eventually add support for `_type: {enabled: false}` (and a user uses it) it will be one type per index. The only trickiness will be parent/child which @martijnvg has been working on in https://github.com/elastic/elasticsearch/issues/20257
</comment><comment author="clintongormley" created="2016-09-15T13:02:03Z" id="247321473">&gt; So I think this is no longer blocking for 5.0, is that correct? (I didn't want to check the box if there was something else missing)

Not quite. There are certain properties today which can differ, eg:

```
PUT t
{
  "mappings": {
    "one": {
      "properties": {
        "text": {
          "type": "text",
          "copy_to": "foo"
        }
      }
    },
    "two": {
      "properties": {
        "text": {
          "type": "text",
          "copy_to": "bar"
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-09-19T07:34:18Z" id="247927465">&gt; Having started to work on it, this is more challenging than I initially thought. However I think this might not be needed: since we will require at most one type in 6.0+ anyway, we will not have to merge mappings across types in 7.0, so this step is not required for the type removal?

@jpountz makes sense to me.  OK, I'll remove that task and remove the blocker label.
</comment><comment author="colings86" created="2016-10-07T10:14:08Z" id="252206747">I like this change in general, my only concern is the fact that the create index and index document apis will be on the same rest endpoint with only a different method. I am worried this is confusing and error-prone for users.

As discussed above the create index will be only accessible with `PUT index` and indexing a document with an auto-generated id will be `POST index`. This seems weird to me as we are affecting different resources purely based on the method used (adding an index to the cluster vs adding a document to the index). We could instead have the auto-generated id endpoint as something like `POST index/_auto_id`. This should not clash with actual ids as we already have a convention of reserving `index/_X` for endpoints (e.g. `index/_mapping`).
</comment><comment author="Danack" created="2017-03-13T11:49:21Z" id="286085890">Hi,

Please could it be documented in the ES documentation that the long term plan is to move away from multiple types per index, so that people starting new projects now know to avoid them, and for people who have projects that already use multiple mappings, to start going through the seven stages of loss sooner rather than later. 

I only stumbled across this issue by chance when reading up on indexes vs types via google.

Obviously, apologies in advance if this has already been documented and I've just missed it.

cheers
Dan
</comment><comment author="stephane-bastian" created="2017-03-16T10:22:49Z" id="287015195">Hi All,

I already posted a comment related to typeless parent/child #20257 
Now I would like to post some sort of summary about the Pros and Cons of types. The reason is that the more I think about the prospect of removing types, the more I feel that I would really miss what it buys me: considering the data store in ES as ONE entity (or sort of).

Pros:
- Types allows me to use an index as some sort of database, each 'table' being a different type. It might be seen as some sort of edge case but a lot of people as using it this way and I would like to say that this is a really nice feature! Over the year, I've been involved in several project leveraging ES where ES became the only source of data. In all these projects ES is the database. There is no external SQL db at all and every single piece of data is stored in an index containing different types. Which brings me to the next point

- In term of operations, in order to backup all the data spread across several types I simply need to snapshot ONE index. When something goes wrong I simply restore ONE snapshot and I'm back in business. To me this single feature is very important because data stored in ES are related. It's not a bunch of indexes but rather a group of related things. And Types buys me this. 

- We sometimes use Alias to perform operations on several types. For instance it could be upgrading all our data and switching to the upgraded data by simply updating the alias. I know aliases can point to several indices already but once again when I *use* ES, the notions of types allows me to thing about all this inter-related data as one (ahem...) database.

Cons:
- Several issues mainly (only?) related to mappings, inconsistencies and such.

Don't get me wrong, I do agree that types can be confusing until you understand their shortcomings. Once you are aware of them, it almost becomes a no-brainer. 

To wrap up, we all agree that types have issues. One way of getting rid of these issues is to ged rid of types. Would it be possible to 'fix' these issues and keep types? For instance, would it be possible to prefix all field with the types they belong to at the lucene level. To me it would solve the mapping issues. WDYT ? 

Sorry for the long post...
All the best,

St&#233;phane
</comment><comment author="rjernst" created="2017-03-20T20:23:09Z" id="287885896">@stephane-bastian 

&gt; Types allows me to use an index as some sort of database, each 'table' being a different type. It might be seen as some sort of edge case but a lot of people as using it this way and I would like to say that this is a really nice feature!

This is a common misconception. Types are *not* tables. The underlying data structures are the same for the entire index, not per type. This means using multiple types causes gaps (ie sparsity) in the data structure each time consecutive docs have different fields.  Lucene is getter better at handling sparse data, but that will never fully solve this problem.

&gt; In term of operations, in order to backup all the data spread across several types I simply need to snapshot ONE index. When something goes wrong I simply restore ONE snapshot and I'm back in business.

You can snapshot multiple indices in one snapshot.

&gt; We sometimes use Alias to perform operations on several types. For instance it could be upgrading all our data and switching to the upgraded data by simply updating the alias. I know aliases can point to several indices already but once again when I use ES, the notions of types allows me to thing about all this inter-related data as one (ahem...) database.

As you say, you aliases can be used with multiple indices. There is nothing inherent about types that make this better, it is only an additional filter added to each query (ie a term filter on the type essentially).

&gt; Cons:
&gt; Several issues mainly (only?) related to mappings, inconsistencies and such.

This is a drastic oversimplification.  Types have caused issues for users for a very long time. We have made improvements to fix inconsistencies (eg #8870), and have discussed what types really give users since then. The remaining issues with types are well laid out by @jpountz in the original issue description here, as well as the linked blog post.

&gt;  For instance, would it be possible to prefix all field with the types they belong to at the lucene level.

We discussed this idea long ago (around the same time as #8870). It does not solve the code complexities of types, and removes one of the few benefits of types (shared lexicon for similar fields).

The key to all of this is that types are not actually special. Essentially they are just an additional field, with an element from the URL path translated into an additional query filter. This is simple for a user to do themselves if they really want to, without causing new users to fall into the trap of "types are like tables".</comment><comment author="synhershko" created="2017-04-04T16:16:03Z" id="291551866">Historically, types were useful for separating documents with different structure within indexes. However, in recent versions this practically no longer holds - different document types cannot share the same field name if their mappings don't match.

In addition the only manageable units in Elasticsearch are documents and indexes. You can add and delete them. You cannot manage types. As such, the usual recommendation I give is to maintain an index per type - since usually when there are several types involved their sizes and SLAs are different, and thus different configurations for replication, retention etc are required anyway. If not now then in the future.

With Elasticsearch slowly moving to index-level and even cluster-level sharding (with cross-cluster search), and with index sizes only growing in real-world systems - I think the chances of types actually being useful anywhere are now close to zero, and different types of documents are better separated on an index level.

At this point, IMO types are just an unnecessary noise for the majority of systems. We can certainly live without them, type less (pun intended) and manage different types of data using separate indexes. This will also avoid various not-uncommon pitfalls (e.g. large number of fields in mapping).

Only feature affected will be parent/child, and I'm sure there could be an easy solution there (I'd probably go with an "invisible" types for parents and children).

So I'm all in for this, hopefully will make it to v6!</comment><comment author="brusic" created="2017-04-05T02:30:11Z" id="291722874">Many years ago, I created an issue asking for inclusion of custom index-level metadata. The work-around given was to use a custom type within the same index with a single document: https://github.com/elastic/elasticsearch/issues/1649

I always found multiple types in the same index to be kludgy (as highlighted by this issue), so I never implemented the suggested solution, but used aliases (had the added benefit if being visible in UIs such as head). It would be great to have custom index-level metadata, especially since the suggested workaround will no longer be supported. Please excuse me if such a feature has been created since the original issue.</comment><comment author="bleskes" created="2017-04-05T08:19:00Z" id="291788687">&gt;  It would be great to have custom index-level metadata,

@brusic I presume the [mapping meta field](https://www.elastic.co/guide/en/elasticsearch/reference/5.3/mapping-meta-field.html) will cover this, as the mappings will now be an index level thing. Is this correct?</comment><comment author="brusic" created="2017-04-06T16:47:14Z" id="292234657">@bleskes If the mapping meta field will be promoted to an index level setting, it would be perfect.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for "quarter" ranges in date math for filtering by date range. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15612</link><project id="" key="" /><description>It would be nice if we could add "quarter" date math for date range filters.

Some examples: now/q (this quarter), 2015-01-01||-1M/q (previous quarter from 2014)

``` javascript
{
    "from": 0,
    "size": 10,
    "query": {
        "bool": {
            "must": {
                "range": {
                    "my_date": {
                        "gte": "now-1q/q",
                        "lte": "now-1q/q"
                    }
                }
            }
        }
    }
}
```
</description><key id="123516817">15612</key><summary>Add support for "quarter" ranges in date math for filtering by date range. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javadevmtl</reporter><labels><label>:Dates</label><label>discuss</label><label>enhancement</label></labels><created>2015-12-22T16:49:18Z</created><updated>2016-09-15T18:15:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="monkeyboy64" created="2016-09-15T18:15:25Z" id="247407486">We are running into this to. We can programmatically figure out the dates, but that isn't as easy has just have a quarter time unit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster state parsing is too lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15611</link><project id="" key="" /><description>See https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/metadata/MetaData.java#L1150. If an element is not recognized in a cluster state, it is simply ignored.
</description><key id="123513005">15611</key><summary>Cluster state parsing is too lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Cluster</label><label>bug</label></labels><created>2015-12-22T16:28:05Z</created><updated>2016-02-29T20:17:33Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-22T16:45:16Z" id="166670182">This is on purpose, to allow plugins to add custom metadata. If the plugin that added custom metadata is removed, the metadata of that plugin cannot be read anymore. If we were to fail the parsing here, cluster state could not be recovered on startup after uninstalling plugin.

I agree that improvements can be made in this area, just wanted to give an explanation for the current implementation.
</comment><comment author="s1monw" created="2015-12-22T16:47:26Z" id="166670705">&gt; I agree that improvements can be made in this area, just wanted to give an explanation for the current implementation.

++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] add `node.ingest` setting that controls whether ingest is active</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15610</link><project id="" key="" /><description>Defaults to `false`.

If `node.ingest` isn't active then ingest related API calls fail and if the `pipeline_id` parameter is set then index and bulk requests fail.
</description><key id="123506210">15610</key><summary>[Ingest] add `node.ingest` setting that controls whether ingest is active</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-22T15:52:53Z</created><updated>2015-12-22T21:39:35Z</updated><resolved>2015-12-22T21:39:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-22T17:24:32Z" id="166679665">I reviewed this and my comments were only about disabling put, delete and get template when `node.ingest` is set to `false`. I had doubts on this given that it's going to affect which nodes clients can send those requests to, but clients will need to be aware of the ingest plugin anyway if we disable data ingestion. I am ok with getting this in as-is. I would implement a redirect internally rather sooner than later though instead of pushing this to clients.
</comment><comment author="javanna" created="2015-12-22T17:24:43Z" id="166679696">that was a LGTM then :)
</comment><comment author="martijnvg" created="2015-12-22T19:58:03Z" id="166715752">The idea is that for now clients have to be aware to what nodes they send ingest requests too and enabling indexing with a pipeline enabled. So I felt it is okay to disable both index/bulk requests with a pipeline enabled and the ingest APIs if `node.ingest=false`. As a follow up issue we can add a redirect feature, that redirects  index/bulk requests with a pipeline enabled and ingest API requests if `node.ingest=false`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClassCastException DoubleTerms$Bucket cannot be cast to LongTerms$Bucket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15609</link><project id="" key="" /><description>I see very strange behavior of ES. I use ES 1.7.3. When I execute a query which you can see below I get 

`ClassCastException[org.elasticsearch.search.aggregations.bucket.terms.DoubleTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket` 

`curl 'localhost:9200/index1_2015-12-19,index1_2015-12-20,index1_2015-12-21,index1_2015-12-22,index1_2015-12-23/_search?ignore_unavailable=true' -d '{"query":{"filtered":{"query":{"query_string":{"query":"*","default_operator":"AND"}},"filter":{"range":{"@timestamp":{"gte":"2015-12-20T14:45:36","lte":"2015-12-22T14:45:36"}}}}},"aggs":{"values":{"terms":{"field":"measure","size":10}}}}'`

This doesn't happen when I run the same query on each index separately. This made me think that I have a problem with _mapping. I checked the mapping for each of indices and for all fields I have 

```
          "measure" : {
            "type" : "long",
            "doc_values" : true
          },
```

Am I doing something wrong or it is a bug?
</description><key id="123502426">15609</key><summary>ClassCastException DoubleTerms$Bucket cannot be cast to LongTerms$Bucket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2015-12-22T15:34:27Z</created><updated>2015-12-22T23:24:29Z</updated><resolved>2015-12-22T22:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T18:44:56Z" id="166700675">Can you check how `measure` is mapped on all types of the following indices:
- index1_2015-12-19
- index1_2015-12-20
- index1_2015-12-21
- index1_2015-12-22
- index1_2015-12-23

If it is mapped as a double on any of these types, then it is the cause of the issue that you are seeing.

Otherwise it could mean that you are hit by this bug: #8688.
</comment><comment author="prog8" created="2015-12-22T19:12:56Z" id="166706522">On all types this field is mapped to long 

```
"measure" : {
            "type" : "long",
            "doc_values" : true
          }
```

I realized that now it fails also when I query single index (index1_2015-12-22 and index1_2015-12-21). Maybe you are right that I hit the bug #8688
</comment><comment author="jpountz" created="2015-12-22T22:01:04Z" id="166741949">Thanks for checking. Then I confirm it is very likely caused by #8688. Unfortunately it is required to reindex the data in order to fix the problem. The only way to prevent it from happening is to either map fields explicitly or use dynamic templates to that elasticsearch never has to decide whether to use a long or a double by itself.

The issue is fixed in 2.0, but you could still have the issue when querying multiple indices. For instance if the first document that is indexed looks like the following:

```
{
  "measure": 3.0
}
```

then the field will be mapped as a double, so if you run an aggregation against several indices including this one and another index that has the field mapped as a long, then you could still get this exception.
</comment><comment author="prog8" created="2015-12-22T22:24:05Z" id="166749964">I'm considering adding a raw "sub-field" to measure field which would be string. This will prevent from terms aggregation problems. I checked and a few other aggregations work (avg, min, max) so raw field should resolve all problems, including the case you mentioned
</comment><comment author="jpountz" created="2015-12-22T23:24:29Z" id="166761206">@prog8 if you control the mappings, then let's map the field explicitly when creating the index (possibly via index templates) as a long instead of using sub fields?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle's run task should pick up extra stuff used in integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15608</link><project id="" key="" /><description>`lang-groovy` uses

```
integTest {
  cluster {
    systemProperty 'es.script.inline', 'on'
    systemProperty 'es.script.indexed', 'on'
  }
}
```

to setup super permissive configuration for testing but when you run it with `gradle plugins:lang-groovy:run` you get the standard configuration which makes testing one-offs more complicated. It'd be more friendly to pick up the integTest config and just use it for run. Or maybe to have a separate config for run so it could define stuff just in case it wanted to be different.

```
run {
  cluster {
    systemProperty 'es.script.inline', 'on'
    systemProperty 'es.script.indexed', 'off'
  }
}
```

This came up while I was working on #15125, reindex and update-by-query. update-by-query is very difficult to test without a scripting language so I enable `lang-groovy` to both tasks.
</description><key id="123502231">15608</key><summary>Gradle's run task should pick up extra stuff used in integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>build</label></labels><created>2015-12-22T15:33:14Z</created><updated>2017-01-12T10:45:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T20:17:13Z" id="190366209">@rjernst can this be closed yet?
</comment><comment author="rjernst" created="2016-02-29T20:22:59Z" id="190369324">Nothing directly has been done for it. We do support setting run settings, just like with integ tests, but they must be set separately. So to fix the issue originally mentioned here, you would add:

```
run {
  systemProperty 'es.script.inline', 'on'
  systemProperty 'es.script.indexed', 'off'
}
```
</comment><comment author="clintongormley" created="2016-03-02T09:41:49Z" id="191157856">Actually:

```
run {
  systemProperty 'es.script.inline', 'true'
  systemProperty 'es.script.indexed', 'false'
}
```

`on` and `off` are no longer supported
</comment><comment author="clintongormley" created="2016-10-18T08:10:59Z" id="254437352">Is this still an issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the warmer API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15607</link><project id="" key="" /><description>With time, warmers have become less and less useful: we got doc values by default in 2.0 and disk-based norms in 2.1. Instead of fixing the warmer-related bugs (#5155, #15590 or #15194) that we have, I'd like to suggest that we remove the warmer API entirely.

The migration strategy would be the following:
- deprecate the warmer API in 2.x,
- remove warmers and the warmer API entirely in 3.0, and ignore warmer definitions when upgrading from 2.x.
</description><key id="123498298">15607</key><summary>Remove the warmer API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Warmers</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T15:12:11Z</created><updated>2016-04-28T20:26:57Z</updated><resolved>2016-01-07T08:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-12-22T15:14:16Z" id="166642879">++ for this plan, I love the fact that we don't need warmers anymore
</comment><comment author="Mpdreamz" created="2015-12-24T12:57:19Z" id="167106534">I was just looking at taking a stab at #5155 but this is even better :+1: 
</comment><comment author="bpicolo" created="2016-04-28T18:38:03Z" id="215522339">@jpountz  How are we expected to take care of the use case of pre-building filter caches in upcoming versions? I can't find any docs suggesting that's not a useful case still (it seems very relevant). I also can't find any actual documentation for filter-caches that claims to still be relevant.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distance sorting and calculation incorrect and inaccurate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15606</link><project id="" key="" /><description>I'm using elasticsearch for a search functionality on a website. The search must provide a search by distance from a location in an - for example - 25km radius.

That's no problem, I'm using the following elasticsearch query array:

```
{
    "index": "kasd9i9021profiles",
    "type": "profile",
    "size": 30,
    "from": 0,
    "body": {
    "query": {
        "bool": {
            "filter": {
                "geo_distance": {
                    "distance": "25km",
                    "distance_type": "sloppy_arc",
                    "_cache": true,
                    "location": {
                        "lat": 48.136833417046,
                        "lon": 11.570900696682
                    },
                    "unit": "km"
                }
            },
            "must": [
                {
                    "term": {
                        "published": "1"
                    }
                },
                {
                    "match": {
                        "country": "DE"
                    }
                },
                {
                    "match": {
                        "category_id": "1"
                    }
                }
            ]
        }
    },
    "script_fields": {
        "distance": {
            "lang": "groovy",
            "params": {
                "lat": 48.136833417046,
                "lon": 11.570900696682
            },
            "script": "doc['location'].distanceInKm(lat,lon)"
        }
    },
    "sort": [
        {
            "upgrade_sort": {
                "order": "desc"
            }
        },
        {
            "has_siegel": {
                "order": "desc"
            }
        },
        {
            "_geo_distance": {
                "location": {
                    "lat": 48.136833417046,
                    "lon": 11.570900696682
                },
                "order": "asc",
                "unit": "km"
            }
        }
    ]
    },
    "fields": [
    "_source",
    "distance"
    ]
}
```

The value of "upgrade_sort" can be between 0 and 3. The value of "has_siegel" can be true or false.

The problems are:

```
The results are not sorted by distance
The distance in the result is between 0 and ~30km, not between 0 and 25km.
```

Is that a bug or a wrong query?

Also postet on stackoverflow.com: http://stackoverflow.com/questions/34372194/elasticsearch-distance-sorting-and-calculation
</description><key id="123492101">15606</key><summary>Distance sorting and calculation incorrect and inaccurate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2015-12-22T14:39:52Z</created><updated>2015-12-22T19:11:42Z</updated><resolved>2015-12-22T19:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2015-12-22T15:45:57Z" id="166651695">I've published more details about my mapping and the results on stackoverflow: http://stackoverflow.com/questions/34372194/elasticsearch-distance-sorting-and-calculation-incorrect-and-inaccurate
</comment><comment author="jpountz" created="2015-12-22T18:50:30Z" id="166701833">The groovy script and the geo filter don't use the same method to compute the distance (the groovy script assumes a plane). Can you try to run `doc['location'].arcDistanceInKm(lat,lon)` instead of your current script to see if it gives different results?
</comment><comment author="ghost" created="2015-12-22T18:57:19Z" id="166703174">It works, thanks! I could'nt find this solution or any hint in the documentation and so I tried for two weeks... :(
</comment><comment author="jpountz" created="2015-12-22T19:03:08Z" id="166704365">I agree it's confusing that there are several ways to compute geo distances (and I had to look up the code to see which one distanceInKm is using). I opened an issue for it: #15616.
</comment><comment author="ghost" created="2015-12-22T19:11:42Z" id="166706277">Thanks! :)

Am 22.12.2015 um 20:04 schrieb Adrien Grand:

&gt; I agree it's confusing that there are several ways to compute geo 
&gt; distances (and I had to look up the code to see which one distanceInKm 
&gt; is using). I opened an issue for it: #15616 
&gt; https://github.com/elastic/elasticsearch/issues/15616.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elastic/elasticsearch/issues/15606#issuecomment-166704365.

## 

Anton Dachauer
web developer | freelancer

sicher kommunizieren: https://www.antondachauer.de/kommunikation/

t: 0911 - 927 85 805
m: mail@antondachauer.de
w: http://www.antondachauer.de
t: https://twitter.com/antondachauer
x: https://www.xing.com/profile/Anton_Dachauer
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up CodecTests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15605</link><project id="" key="" /><description>Some tests, but in particular CodecTests, are slow because they test all
versions that ever existed even though they should only test supported
versions.
</description><key id="123489422">15605</key><summary>Speed up CodecTests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T14:22:51Z</created><updated>2015-12-22T14:45:59Z</updated><resolved>2015-12-22T14:45:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-22T14:24:15Z" id="166628269">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Turn off stemming and stop words for phrase searches only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15604</link><project id="" key="" /><description>I am using the query_string so that my users may be able to run boolean queries mixed with phrase queries such as:

"the mouth" OR the cancerous cells AND drugs AND "heart attacks"

The stop words and stemming should not be removed from the phrases but it should be removed from the non-phrase searches so:

drugs = &gt; drug
cells =&gt; cell

The key issue I have discovered when trying to work around this is with the highlighting. So for example take the following query which queries two fields, one with stems and one without:

"the mouth"

it returns a document with two highlighted fields:

"the mouth"
"mouth"

"I think" what I really need is the ability to merge the two so that there is only one highlight field that gets returned with all the keywords and phrases highlighted if they exist within those fields.

Could this feature be implemented?
</description><key id="123472182">15604</key><summary>Turn off stemming and stop words for phrase searches only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imranazad</reporter><labels><label>discuss</label></labels><created>2015-12-22T12:17:19Z</created><updated>2016-01-10T18:55:25Z</updated><resolved>2016-01-10T18:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T14:33:49Z" id="166630067">I believe this could be implemented by having two fields: one that has stemming and stop words enabled, another one that has them disabled, and then redirecting to one field or the other from the query parser depending on whether a phrase is being parsed.

I'll leave this issue open to let others chime in, but in my opinion, this is a too specific feature for inclusion into elasticsearch.
</comment><comment author="nik9000" created="2015-12-22T14:37:23Z" id="166630885">&gt; I believe this could be implemented by having two fields: one that has stemming and stop words enabled, another one that has them disabled, and then redirecting to one field or the other from the query parser depending on whether a phrase is being parsed.

That is how I've done it in the [past](https://en.wikipedia.org/w/index.php?title=Special:Search&amp;search=%22i+like+cats%22&amp;cirrusDumpQuery). The query_string usage there was a mistake but the analysis was a good idea, I believe.
</comment><comment author="nik9000" created="2015-12-22T14:39:43Z" id="166631308">&gt; Could this feature be implemented?

Its called `matched_fields` and I implemented it to solve exactly the problem you describe above. Play around with that link in my last comment so how its used on wikipedia. Its certainly not perfect but it works pretty well.
</comment><comment author="imranazad" created="2015-12-22T15:36:17Z" id="166649033">@jpountz 

&gt; and then redirecting to one field or the other from the query parser depending on whether a phrase is being parsed.

This sounds like an excellent idea but how would I parse the phrases only to redirect them to fields when I'm using the query_string and my query is mixed with terms and phrases including booleans?
</comment><comment author="imranazad" created="2015-12-22T16:19:22Z" id="166664372">@nik9000 matched_fields looks very promising and I have it working now where the results are merged however I'm still obviously getting documents being returned with highlights just for "mouth" how can I filter these out since it's a phrase search and I should only be getting documents back with "the mouth".
</comment><comment author="nik9000" created="2015-12-22T16:48:02Z" id="166670864">Right. I think the only way to do that is to issue the "right" query. See if you can define a `search_quote_analyzer` on the field and see if that works. It might. Otherwise its up to you to change the query, sadly.
</comment><comment author="imranazad" created="2015-12-22T17:20:48Z" id="166678967">@nik9000 You are a legend man! That works! I'm quite amazed I've managed to get this to work, I really do think Elastic shines in this regard, this is a major issue with full-text search and the documentation doesn't really do a good job of selling the features already available in Elastic to make this happen.
</comment><comment author="nik9000" created="2015-12-22T17:27:30Z" id="166680198">Hurrray!

I don't remember seeing _any_ documentation on search_quote_analyzer. Do you happen to be interested in sending a pull request? The documentation is right there in the source tree. I should have done it when I found it but I didn't. You can be better than I am and save the next person the trouble you went through!
</comment><comment author="imranazad" created="2015-12-22T18:37:05Z" id="166698775">@nik9000  Sure thing, I'd love to, I can make my first contribution to Elastic! So what do I do exactly? You'll have to bear with me I've not done this before. Shall I create a new pull request? What do I do after that?
</comment><comment author="nik9000" created="2015-12-22T18:47:00Z" id="166701180">&gt; So what do I do exactly? You'll have to bear with me I've not done this before. Shall I create a new pull request? What do I do after that?

I believe the simplest way is to go to the page you wan to edit and click the "edit" pencil that is to the right of every heading. That should let you make a pull request without too much fuss. I don't know what page to edit. Maybe [this](https://www.elastic.co/guide/en/elasticsearch/reference/current/string.html?q=string#string-params) one?

The other thing is that you'll have to sign a CLA - there are links to it on the top of the page about making a pull request. We need everyone who's sent a pull request, no matter how small to fill it out. I don't fully understand that reasoning behind it but I'm not a lawyer.

I'm just excited that you are happy and excited about making a contribution. Its how I got started and its fun and worthwhile!
</comment><comment author="imranazad" created="2015-12-23T18:59:57Z" id="166970829">@nik9000 I'm very excited and happy to be contributing so much so I've documented the `search_quote_analyzer` on the page you suggested but I've also gone one step further and created a new page on how to actually use it to stop stemming and stop words for phrases as a potential use case. However I'm unsure how to get these changes committed. Since I needed to add a new page I thought the easiest option for me would be to clone master, create a branch, push and then issue a pull request but when I try to push my branch I'm getting a 403. I'm keen to get this in and see my first contribution go live! :-)
</comment><comment author="nik9000" created="2015-12-23T19:10:16Z" id="166972950">Yeah, if you want to make a new page the best thing to do is to fork and clone like you did. Have a look at https://help.github.com/articles/fork-a-repo/ and see if that makes it make more sense.
</comment><comment author="clintongormley" created="2016-01-10T18:55:25Z" id="170381836">It seems there is nothing more to do here, so I'm clsoing this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix documentation of Java API for indexed scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15603</link><project id="" key="" /><description>Relates to #15583.
</description><key id="123466643">15603</key><summary>Fix documentation of Java API for indexed scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T11:43:46Z</created><updated>2015-12-22T19:37:03Z</updated><resolved>2015-12-22T15:59:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T14:29:17Z" id="166629232">LGTM
</comment><comment author="sokrahta" created="2015-12-22T19:37:03Z" id="166711497">:+1:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FunctionScoreQuery should implement two-phase iteration.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15602</link><project id="" key="" /><description>FunctionScoreQuery should do two things that it doesn't do today:
- propagate the two-phase iterator from the wrapped scorer so that things are
  still executed efficiently eg. if a phrase or geo-distance query is wrapped
- filter out docs that don't have a high enough score using two-phase
  iteration: this way the score is only checked when everything else matches

While doing these changes, I noticed that minScore was ignored when scores were
not needed and that explain did not take it into account, so I fixed these
issues as well.
</description><key id="123464790">15602</key><summary>FunctionScoreQuery should implement two-phase iteration.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T11:31:27Z</created><updated>2015-12-23T13:54:29Z</updated><resolved>2015-12-23T13:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-22T16:26:15Z" id="166665827">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>WIP: Move ingest under modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15601</link><project id="" key="" /><description>Ingest plugin will be installed by default, thus it needs to be moved under modules.

This PR is a WIP given that the build currently breaks because it doesn't expect a module to have its own packaging files. Ingest has in fact grok pattern files under packaging and we need to find a way to package them.
</description><key id="123458984">15601</key><summary>WIP: Move ingest under modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>WIP</label></labels><created>2015-12-22T10:57:51Z</created><updated>2015-12-22T16:00:10Z</updated><resolved>2015-12-22T16:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-22T14:11:00Z" id="166625993">&gt; This PR is a WIP given that the build currently breaks because it doesn't expect a module to have its own packaging files. Ingest has in fact grok pattern files under packaging and we need to find a way to package them.

Sounds like maybe its not tall enough to be a module yet.
</comment><comment author="rmuir" created="2015-12-22T14:12:19Z" id="166626222">Yeah, modules don't have configuration files. Lets keep this as a plugin.
</comment><comment author="javanna" created="2015-12-22T16:00:10Z" id="166657090">Closing, this will be done differently and when ingest is more mature.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Data inconsistency with different queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15600</link><project id="" key="" /><description>Hi guys,

I was doing some work with Kibana and I got some inconsistent results in my data, after that I did some more digging around with queries using REST and I discovered some problems with ES.

So I have some millions of events in my index and I get completely different results from different queries,  I think the problem is when I use aggregations inside of another aggregation. Let me show you the queries and the results:

Aggs 1:

```
"aggs" : {
  "3" : {
    "filter" : { "term" : { "type" : "type1" } },
    "aggs" : {
      "1" : { 
        "filter" : { "term" : { "domain" : "X" } },   
        "aggs" : {
          "distinct" : { "cardinality" : { "field" : "remote_addr" } }
        }
      }
    }
  }
}
```

Result 1:

```
"hits" : {
  "total" : 13785806,
  "max_score" : 0.0,
  "hits" : [ ]
},
"aggregations" : {
  "3" : {
    "doc_count" : 617159,
    "1" : {
      "doc_count" : 617159,
      "distinct" : {
        "value" : 293193,
        "value_as_string" : "0.4.121.73"
      }
    }
  }
}
```

Aggs 2:

```
"aggs" : {
  "3" : {
    "filter" : { "term" : { "type" : "type1" } },
    "aggs" : {
      "distinct" : { "cardinality" : { "field" : "remote_addr" } }
    }
  }
}
```

Result 2:

```
"aggregations" : {
  "3" : {
    "doc_count" : 617159,
    "distinct" : {
      "value" : 293193,
      "value_as_string" : "0.4.121.73"
    }
  }
}
```

Aggs 3:

```
"aggs": {
 "2": {
   "terms": {
     "field": "domain",
     "size": 5,
     "order": {
       "2-orderAgg": "desc"
     }
   },
   "aggs": {
     "3": {
       "terms": {
         "field": "type",
         "size": 1,
         "order": {
           "1": "desc"
         }
       },
       "aggs": {
         "1": {
           "cardinality": {
             "field": "remote_addr"
           }
         }
       }
     },
     "2-orderAgg": {
       "cardinality": {
         "field": "remote_addr"
       }
     }
   }
 }
}
```

Result 3:

```
"aggregations" : {
  "2" : {
    "doc_count_error_upper_bound" : -1,
    "sum_other_doc_count" : 6767717,
    "buckets" : [ {
      "key" : "X",
      "doc_count" : 617159,
      "3" : {
        "doc_count_error_upper_bound" : 0,
        "sum_other_doc_count" : 0,
        "buckets" : [ {
          "key" : "type1",
          "doc_count" : 617159,
          "1" : {
            "value" : 368688,
            "value_as_string" : "0.5.160.48"
          }
        } ]
      },
      "2-orderAgg" : {
        "value" : 283687,
        "value_as_string" : "0.4.84.39"
      }
    }
    ...
    } ]
  }
}
```

So something is going wrong inside ES by this results.
We can conclude:
- The index has 13785806 documents
- 617159 documents have domain with value "X"
- 617159 documents have the type with value "type1"
  ? We can see with aggs3 that the document with domain="X" and type="type1" are the same, so how can we have different cardinalities for this sets in aggs3? Notice that the cardinality in the inner set is even bigger that the one on the outside set...

Do you have any ideas about this? I think this really is an ES bug, but could be any kind of error from my part? (I know comparing filters and eggs could be misleading, but the results are too different...)

Other info:
- I experienced the error with ES 2.1.0 and after I upgraded to 2.1.1 the error persists
- About the data types
  -  Domain is a string, not analyzed, doc values active
  -  Type is a string, not analyzed, doc values active
  -  Remote_addr  is an ip, not analyzed, doc values active
</description><key id="123455197">15600</key><summary>Data inconsistency with different queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">pseverino</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-12-22T10:29:58Z</created><updated>2016-02-29T22:20:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T20:16:27Z" id="190365730">Besides the issues you see, the `value_as_string` should not be returned as an IPv4 address.
</comment><comment author="pseverino" created="2016-02-29T22:19:40Z" id="190424046">@clintongormley that behavior happens in other queries if I'm not mistaken, if you need more info about that just ask. I will see if that behavior really happens in other queries
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Upgraded geoip2 and its dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15599</link><project id="" key="" /><description /><key id="123454758">15599</key><summary>[Ingest] Upgraded geoip2 and its dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-22T10:26:33Z</created><updated>2015-12-22T10:40:52Z</updated><resolved>2015-12-22T10:40:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-22T10:27:58Z" id="166576392">great, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put space into last comment line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15598</link><project id="" key="" /><description>This is a very small CL, but it'll be helpful.
</description><key id="123449049">15598</key><summary>Put space into last comment line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lks21c</reporter><labels><label>non-issue</label></labels><created>2015-12-22T09:47:07Z</created><updated>2015-12-28T14:41:15Z</updated><resolved>2015-12-28T14:41:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-28T14:41:15Z" id="167582095">thanks 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: analysis custom file paths migration notes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15597</link><project id="" key="" /><description>Closes https://github.com/elastic/elasticsearch/issues/15556
</description><key id="123448919">15597</key><summary>Docs: analysis custom file paths migration notes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aeryaguzov</reporter><labels /><created>2015-12-22T09:46:15Z</created><updated>2016-02-29T19:56:57Z</updated><resolved>2016-02-29T19:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-29T19:56:30Z" id="190355516">thanks @aeryguzov - merged with some edits.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Put space into last comment line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15596</link><project id="" key="" /><description>This is a very small CL but, I hope it'll be helpful.
</description><key id="123448313">15596</key><summary>Put space into last comment line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lks21c</reporter><labels /><created>2015-12-22T09:41:37Z</created><updated>2015-12-22T09:44:00Z</updated><resolved>2015-12-22T09:44:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Put space into last comment line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15595</link><project id="" key="" /><description>This is a very small CL.
I put a space into last comment line which seems like fixing typo.
</description><key id="123446603">15595</key><summary>Put space into last comment line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lks21c</reporter><labels /><created>2015-12-22T09:29:25Z</created><updated>2015-12-22T09:40:33Z</updated><resolved>2015-12-22T09:40:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove `index.compound_on_flush` setting and default to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15594</link><project id="" key="" /><description>We added this undocumented realtime setting as backup plan long ago
but to date we haven't had a situation where it was a problem. It's reducing
the number of filehandles in the NRT case dramatically and should always be enabled.

Closes #10778
</description><key id="123441964">15594</key><summary>Remove `index.compound_on_flush` setting and default to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T08:59:13Z</created><updated>2016-03-18T13:22:15Z</updated><resolved>2015-12-22T09:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-22T09:23:20Z" id="166564494">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Change ExecutionService to process multiple request at a time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15593</link><project id="" key="" /><description>ExecutionService should be able to process multiple index requests at a time. This change is important for the when ingest is enabled for the bulk api, so that we don't use a head per bulk item, but instead use a threed per bulk request.
</description><key id="123441513">15593</key><summary>[Ingest] Change ExecutionService to process multiple request at a time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-22T08:56:16Z</created><updated>2015-12-22T12:24:44Z</updated><resolved>2015-12-22T12:24:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-22T10:07:08Z" id="166572272">I left a couple of minor comments, LGTM otherwise. Was wondering, this changes things only for bulk requests doesn't it? It would be good to clarify that in the description of the PR.
</comment><comment author="martijnvg" created="2015-12-22T11:07:42Z" id="166583174">@javanna yes, this change only applies for bulk request. I'll update the
 description.
</comment><comment author="martijnvg" created="2015-12-22T11:20:45Z" id="166587155">@javanna I've update this PR.
</comment><comment author="javanna" created="2015-12-22T12:01:44Z" id="166595377">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated query cache settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15592</link><project id="" key="" /><description /><key id="123439152">15592</key><summary>Remove deprecated query cache settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cache</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T08:40:04Z</created><updated>2016-01-10T19:42:32Z</updated><resolved>2015-12-22T09:34:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-22T08:40:13Z" id="166554097">@jpountz can you take a look?
</comment><comment author="jpountz" created="2015-12-22T09:01:12Z" id="166558837">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>change S3 repository link to elastic/elasticsearch@2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15591</link><project id="" key="" /><description>The previous link went to the outdated repo for ES &lt; 2.0. Two things:
- Not sure if it's smart to link current-docs with something like tree/2.0, or if tree/2.x would be better
- Not sure if it wouldn't be better to link to the plugin doc page anyway, which would be https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-aws.html. Would have to change the other 2 links (HDFS and Azure plugins) too for consistency.
</description><key id="123435886">15591</key><summary>change S3 repository link to elastic/elasticsearch@2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tkaiser</reporter><labels /><created>2015-12-22T08:08:25Z</created><updated>2016-04-06T17:28:36Z</updated><resolved>2016-04-06T17:28:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tkaiser" created="2015-12-22T08:38:31Z" id="166553698">ok 1. sorry i messed up the branch name and 2. i did sign the CLA just before submitting the PR, not sure if this check auto-refreshes or something.
</comment><comment author="clintongormley" created="2016-01-10T19:41:33Z" id="170384442">Hi @tkaiser 

Thanks for the PR.  I think it makes more sense to link to the docs for each plugin, eg https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository.html

Note: These plugins changed names between 2.1 and 2.2
</comment><comment author="dadoonet" created="2016-01-10T19:55:16Z" id="170385300">They changed names in master.
</comment><comment author="dakrone" created="2016-04-06T17:28:36Z" id="206476972">Closing as there has been no feedback, feel free to comment if this was closed erroneously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Global ordinals crash warmers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15590</link><project id="" key="" /><description>This has been happening in ES 2.0.x and 2.1.x, the last release where warmers worked for me was 2.0rc1. Affects newly created indexes.

Stacktrace for a simple terms aggregation warmer for a string field (ES 2.1.1):

```
[2015-12-21 19:28:06,323][WARN ][index.warmer             ] [lxc] [works9][0] warmer [warm_contrib_aggs] failed
java.lang.ClassCastException: org.apache.lucene.index.MultiReader cannot be cast to org.apache.lucene.index.DirectoryReader
    at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals$FieldData.globalOrdinalsValues(ValuesSource.java:144)
    at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals.globalMaxOrd(ValuesSource.java:117)
    at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.doCreateInternal(TermsAggregatorFactory.java:214)
    at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:64)
    at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:102)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:87)
    at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:78)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:104)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
    at org.elasticsearch.search.SearchService.access$600(SearchService.java:119)
    at org.elasticsearch.search.SearchService$SearchWarmer$1.run(SearchService.java:1160)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="123430457">15590</key><summary>Global ordinals crash warmers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zygfryd</reporter><labels><label>bug</label></labels><created>2015-12-22T07:23:01Z</created><updated>2016-03-16T02:25:27Z</updated><resolved>2016-01-07T09:50:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T15:02:19Z" id="166638132">Thanks for the report, this problem is due to #14084.
</comment><comment author="jpountz" created="2016-01-07T09:50:17Z" id="169612820">Closing: warmers will go away in 3.0 #15614
</comment><comment author="ppf2" created="2016-03-16T02:25:27Z" id="197117293">I can still reproduce the issue on 2.2.0 so it doesn't look like it is addressed by https://github.com/elastic/elasticsearch/pull/14084.  

```
[2016-03-15 19:04:28,764][WARN ][index.warmer             ] [cluster1_node1] [testwarmer][3] warmer [testwarmer] failed
java.lang.ClassCastException: org.apache.lucene.index.MultiReader cannot be cast to org.apache.lucene.index.DirectoryReader
    at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals$FieldData.globalOrdinalsValues(ValuesSource.java:144)
    at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$WithOrdinals.globalMaxOrd(ValuesSource.java:117)
    at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.doCreateInternal(TermsAggregatorFactory.java:217)
    at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:64)
    at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:102)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:87)
    at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:85)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:111)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364)
    at org.elasticsearch.search.SearchService.access$600(SearchService.java:120)
    at org.elasticsearch.search.SearchService$SearchWarmer$1.run(SearchService.java:1165)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Simple warmer with a terms agg:

```
PUT testwarmer/type/1
{
  "test":"test"
}

PUT /testwarmer/_warmer/testwarmer
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "test":{
      "terms": {
        "field": "test",
        "size": 10
      }
    }
  }
}

PUT testwarmer/type/1
{
  "test":"test"
}
```

However, as noted above, warmers will be removed from the product in 5.0 (previously known as 3.0) so this is just a note in case  users run into it on 2.2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkRequest estimated size in bytes does not include script param size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15589</link><project id="" key="" /><description>HI,
When executing an update as follows:

```
            final ArrayList&lt;Integer&gt; test_list = new ArrayList&lt;Integer&gt;();
    Random r = new Random();
    for(int i=0; i &lt; 50000; i++){
        test_list.add(r.nextInt(3000));
    }
    Collections.sort(test_list);
            //ElasticClientHolder is a singleton that holds the transport client.
    BulkRequestBuilder brb = new BulkRequestBuilder(ElaticClientHolder.getInstance());

    UpdateRequest ureq = new UpdateRequest("test_index", "test_type", "test");
    ureq.script("update_test_script", ScriptType.FILE,new HashMap&lt;String,Object&gt;(){{
        put("test_list",test_list);
    }});
    ureq.scriptedUpsert(true);
            ureq.upsert(new HashMap&lt;String,Object&gt;());

    brb.add(ureq);

           System.out.println(brb.request().estimatedSizeInBytes());
```

=&gt; this returns 34

If instead we simply add the  following:

```
          ureq.upsert(new HashMap&lt;String,Object&gt;(){{
            put("test_list",test_list);
         }});

        System.out.println(brb.request().estimatedSizeInBytes());
```

=&gt; this returns something like 231511

The script params aren't being included in the size calculation, Is this intentional and if yes, how do we determine the size of the bulk request while using script_params.

Thanks, 
Bux
</description><key id="123429461">15589</key><summary>BulkRequest estimated size in bytes does not include script param size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rrphotosoft</reporter><labels><label>:Bulk</label><label>:Java API</label><label>discuss</label></labels><created>2015-12-22T07:17:07Z</created><updated>2016-01-11T13:20:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T20:01:19Z" id="170386266">@danielmitterdorfer any ideas?
</comment><comment author="danielmitterdorfer" created="2016-01-11T13:19:13Z" id="170545925">The reason is straightforward: The [size calculation for scripts](https://github.com/elastic/elasticsearch/blob/ace1b33c3c5ddcd933045cb88c04475799fa96ea/core/src/main/java/org/elasticsearch/action/bulk/BulkRequest.java#L163) in `BulkRequest` does only take into account the script size (i.e. the actual source code of the script) but not its parameters. That's why in the first case, only 34 is returned.

The second case does something different: Internally, we create a new update request and treat the provided `Map` as its source. The source of a request is always considered in total in the size calculation. That's why we get a size of 231511 in this case.

We _could_ do something similar in the script case but this will add some overhead both in terms of time and code complexity as we'd have to prematurely have to serialize it, just to determine the size.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>merge current hdfs improvements to master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15588</link><project id="" key="" /><description>We have made a lot of progress on this plugin, I think we should merge back the changes to master and let them run in jenkins.

It has a real hdfs integration test fixture and tests pass on all operating systems, no longer has jar hell, filesystem, configuration, security logic around hadoop are cleaned up, and so on.

Lets get it going in jenkins builds and take it from there...
</description><key id="123417720">15588</key><summary>merge current hdfs improvements to master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Repository HDFS</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T05:48:03Z</created><updated>2016-01-10T19:01:55Z</updated><resolved>2015-12-23T23:17:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-22T06:10:35Z" id="166522323">related issue: https://github.com/elastic/elasticsearch/issues/15438
</comment><comment author="s1monw" created="2015-12-23T22:56:00Z" id="167005209">I left some really minor comments as I went through the code. Having the fixture for HDFS is a huge improvement and getting the stuff actually working is awesome. I am surprised you got that far with all these hacks? it's pretty crazy what's happening in HDFS!!!

LGTM - please take one more look at the minor comments and push once you are done
</comment><comment author="rmuir" created="2015-12-23T23:17:04Z" id="167007334">ok lets destroy jenkins!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plain highlighter does not work with span_near queries in 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15587</link><project id="" key="" /><description>Highlighting a document based on a `span_near` query in 2.1.1 causes ES to incorrectly highlight a term that is not actually a hit because it is outside the span in question. The same steps work correctly in 2.0.2.

The steps I'm executing are:

```
# delete the index
curl -XDELETE localhost:9200/twitter

# create the index
curl -XPUT 'http://localhost:9200/twitter/' -d '{
    "mappings": {
        "tweet": {
            "properties": {
                "message": {
                    "type": "string", 
                    "term_vector": "with_positions_offsets", 
                    "store": true
                }
            }
        }
    }
}'

# insert a document
curl -XPUT 'localhost:9200/twitter/tweet/1?refresh=true' -d '{
    "message" : "A new bonsai tree in the office. Bonsai!"
}'

# search with highlighting
curl -XGET 'http://localhost:9200/twitter/tweet/_search?pretty' -d '{
    "query" : {
        "span_near" : {
            "clauses" : [
                {"span_term": {"message": "new"}}, 
                {"span_term": {"message": "bonsai"}}
            ], 
            "slop": 1, 
            "in_order": false
        }
    }, 
    "highlight": {"fields": {"message": {"type": "plain"}}}
}'
```

The search above is returning:

```
{
  "took" : 7,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.13561106,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : 0.13561106,
      "_source":{"message" : "A new bonsai tree in the office. Bonsai!"},
      "highlight" : {
        "message" : [ "A &lt;em&gt;new&lt;/em&gt; &lt;em&gt;bonsai&lt;/em&gt; tree in the office. &lt;em&gt;Bonsai&lt;/em&gt;!" ]
      }
    } ]
  }
}
```

As you can see, it is incorrectly highlighting the occurrence of "Bonsai" at the end of the field that is not within 1 word of "new". A couple of things to note:
1. This same exact set of steps produces correct highlighting results against Elasticsearch 2.0.2.
2. There is an open bug for `span_near` queries not working using the Fast Vector Highlighter (FVH) -- https://github.com/elastic/elasticsearch/issues/5496 -- which is why I'm attempting to use the plain highlighter above
</description><key id="123409537">15587</key><summary>Plain highlighter does not work with span_near queries in 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dposada</reporter><labels /><created>2015-12-22T04:17:29Z</created><updated>2015-12-22T18:54:04Z</updated><resolved>2015-12-22T18:54:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T18:54:04Z" id="166702534">This is fixed by #15516.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The practicalities of forcing interactive mode for "${prompt.*}"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15586</link><project id="" key="" /><description>We currently allow users to specify `${prompt.text}` and `${prompt.secret}` in their `elasticsearch.yml` which will cause Elasticsearch to prompt on start-up for the value to use.  However, as part of this, we also require Elasticsearch to be run in an interactive terminal, i.e., one that a human is controlling.  

How is this meant to work in practice?  Are we expecting users to log into every node in the cluster and type the same values in?  This doesn't seem to scale.  Many users have strict security requirements around not storing passwords on disk (i.e., PCI compliance) and need to make use of this feature in an automated fashion.  In some cases they have a app that can provide the password automatically, but we're not allowing them to utilise that easily.

What's the reasoning around forcing the use of `${prompt.*}` only on interactive terminals?  If we want to enforce this, maybe we need to show an example in our docs on how to use this without human input?
</description><key id="123396245">15586</key><summary>The practicalities of forcing interactive mode for "${prompt.*}"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2015-12-22T02:05:38Z</created><updated>2016-11-17T21:07:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-12-30T02:33:41Z" id="167920878">It seems you'd have only 2 options:
1. Don't prompt for a password.  The only way this works is if you store the password somewhere.  That "somewhere" could be a config file, an environment variable, or a command line parameter.  The password would either have to be in plaintext in that location or encrypted using something that would be trivial for anybody to decrypt.  That is, if we decided to encrypt a password here, we'd have to pick either an arbitrary key (which somebody could easily look at in the source code) or make the key be passed in a config file, environment variable, or command line parameter, which would all suffer the same security issue as storing the plaintext password.  So we've chosen plaintext here because the alternative doesn't really give you any more security.
2. Prompt for a password, requiring an interactive terminal.

ES supports prompt, environment variable, command-line parameters, and config files for passing it data.  Is there a missing option?
</comment><comment author="joshuar" created="2015-12-30T02:45:43Z" id="167922150">Hi @eskibars,

I believe there is an option 3: fetch the secret from an external application and pass this to ES (either programmatically if possible or via stdin). This appears to be a relatively common approach used where there are strict requirements of not storing passwords in plain-text on disk.

The following is a shell script that will call out to an external program to provide the secrets and pass them to ES when prompted:

```
#!/usr/bin/env bash

{
    script -qec /path/to/bin/elasticsearch &lt;&lt;EOF /dev/null
$(/some/program that gets the keystore pass)
$(/some/program that gets the truststore pass)
EOF
} &gt; /dev/null
```

This fools ES into thinking its running in an interactive terminal via the `script` program.  It would be much nicer to be able to just echo or pass those inputs in directly with, for example, shell piping. It's more pain than it should be.  If I can get around the requirement to have an interactive terminal enforced by ES, why have it at all?
</comment><comment author="clintongormley" created="2016-01-10T18:11:22Z" id="170376343">Your password would then be available to anybody who can run that command?  if that's the case, then why not just put it in the config file.
</comment><comment author="joshuar" created="2016-01-10T22:03:39Z" id="170398263">To adhere to security guidelines (regardless of whether those security guidelines are meaningful) for compliance in a particular industry.  Some guidelines (_cough_ PCI _cough_) explicitly forbid storing passwords in plain-text in any file.  It also means you can have a centralised source of truth for passwords rather than have them littered through a bunch of config files on multiple servers.  And the program itself may have an audit log that allows you to see when the secret is being requested and by whom and potentially other features that could restrict the access beyond file permissions.  See emerging software like [Vault](https://vaultproject.io/).

Besides the security standpoint, having a central source of the passwords could be useful. It seems pointless to require an interactive terminal if I can just get around it.  Do we expect people with a 100 node cluster to enter shell into those 100 servers and  enter a password each time?
</comment><comment author="eskibars" created="2016-01-14T00:07:26Z" id="171477957">Do you happen to know which requirement imposes ban on "plain-text in any file?"  I had a brief look at [PCI-DSS](https://www.pcisecuritystandards.org/documents/PCI_DSS_v3.pdf) provides not only requirements, but also guidance and testing procedures for each requirement, so I want to make sure that any implementation we do would not just "not be what we have now," but also pass whatever testing/guidance is provided.  I think we'd need to read over the requirement/testing/guidance to make any proper decision here.
</comment><comment author="joshuar" created="2016-01-14T00:28:43Z" id="171483228">When I looked at the guidelines, it looked like this one was applicable:

```
8.2.1 Using strong cryptography,
render all authentication credentials
(such as passwords/phrases)
unreadable during transmission and
storage on all system components.
```

It possibly comes down to an interpretation here, but that reads that passwords should not be plain-text in files. "Unreadable" in that requirement could also be interpreted as "unreadable to everything except the program that needs it".

Excluding encryption, what's the reason for requiring a human here? Wouldn't it be beneficial to allow users to not store these credentials in the config _and_ also not need to interactively enter them on node start-up?  Why can't it be a non-human process?
</comment><comment author="eskibars" created="2016-01-14T01:01:43Z" id="171488836">It's not the "what is the current status/why is it this way now" but "what's the concrete, required alternative" that we should be focusing on in a GitHub ticket.

There are a number of ways to pass variables to ES, which I'm sure you're aware of.  You can do elasticsearch -Des.foo=bar, set the password in an environmental variable, use the config with plaintext, or use the config with prompt.\* variables.  There are valuable cases for each of these, across various use cases.  If you wanted to avoid a prompt and not store the something on disk, you could use an environmental variable or command line parameters if those suit a particular use case better.

I'm just not seeing/understanding any alternative that actually provides an additional tangible security benefit to these options.  If there's a requirement that actually provides real benefit/requirement that we can't meet, I do want to understand what that exact requirement is.
</comment><comment author="joshuar" created="2016-01-14T04:16:25Z" id="171525626">Passing environment variables or command-line arguments violates the "don't exchange passwords in plain-text in transport" requirements of these security guidelines.  So we're stuck with the prompting option.  But if I have 200 nodes, I have to enter a password 200 times.  What if the person who manages Elasticsearch shouldn't have access to that password.  Why is a human considered better security than a program? 

Security plugins like Shield are implementing solutions to handle this that will most likely mean you don't need this prompt option but you'll still store the password in plain-text briefly.  That may still violate some security guidelines and can still be exploited.   What about community/third-party plugins that need sensitive data in the config? By only allowing humans to provide this information, aren't we missing out on making it easier for users to automate their deployments without comprising flexibility and security?  

Solutions like Vault do provide a tangible security benefit here.  You abstract a secret away from operations.  You get a central, controlled secret store.  You get auditing of who/what/when/where of passwords in that store.  You get policies on password aging/changing enforced more easily. There may be a whole chain of steps that unlock that vet the request and unlock that one secret. You eliminate the often faulty wetware. 

The biggest direct benefit today would be allowing users who have strict security requirements around secrets another option for presenting them to Elasticsearch automatically.

So my original intention for this discussion was, why make humans required for entering passwords when a program like Vault can handle this as well?  However, this topic touches issues like PCI/security guideline compliance.  It is also beneficial to allowing an external source to provide non-secret config options to Elasticsearch without needing to modify the config file on every node when these change.  

So do we really need to **only** require humans to use the `${prompt.*}` options, especially when you can get around it anyway? 
</comment><comment author="jordansissel" created="2016-11-17T21:07:38Z" id="261369769">I forget what link brought me here, but I have a suggestion.

The Java KeyStore has facilities for storing all sorts of data and protecting it with encryption. Can we reuse this for the storage unit and allow references (possibly implied based on the setting name) to data in the KeyStore?

Idea is that this would prompt the user on startup for the keystore passphrase and then Elasticsearch could read the secrets it wants from this store.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit the max size of bulk and index thread pools to bounded number of processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15585</link><project id="" key="" /><description>I only limit `bulk` and `index` thread pools.

Closes #15582 
</description><key id="123386664">15585</key><summary>Limit the max size of bulk and index thread pools to bounded number of processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-22T00:20:15Z</created><updated>2016-01-08T19:36:21Z</updated><resolved>2016-01-08T19:36:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-22T21:54:00Z" id="166740526">I'm also curious if there is a way to give feedback to users without breaking existing installations. Maybe @s1monw has a clue?
</comment><comment author="s1monw" created="2016-01-05T15:08:58Z" id="169028272">@mikemccand to fix this the right way I agree we should be loud. I think we have to upgrade the settings in the clusterstate first and that can on work well if we do it only in a major version? On the other hand what if you have machines with 8 CPUs and others with 80 CPU? I mean this is really a per-node setting and I think we should maybe not allow to set this globally? I think you patch is good as a first step but we should really think about how to expose these settings?
</comment><comment author="mikemccand" created="2016-01-05T15:34:26Z" id="169036264">I talked to @s1monw ... we decided to add a `logger.warn` here if you tried to set to a too-large value, but otherwise be silent for now, and to open a separate issue that this setting should not be global and dynamically updatable.
</comment><comment author="mikemccand" created="2016-01-05T15:59:59Z" id="169043690">OK I pushed another commit w/ the feedback; I think it's ready.
</comment><comment author="s1monw" created="2016-01-08T19:27:36Z" id="170099793">LGTM
</comment><comment author="mikemccand" created="2016-01-08T19:33:35Z" id="170101156">Thanks @s1monw, I'll port to 2.2.0 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move async translog sync logic into IndexService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15584</link><project id="" key="" /><description>Today the logic to async - commit the translog is in every translog instance
itself. While the setting is a per index setting we manageing it per shard. This
polluts the translog code and can more easily be managed in IndexService.
</description><key id="123353092">15584</key><summary>Move async translog sync logic into IndexService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T20:39:33Z</created><updated>2015-12-22T08:28:13Z</updated><resolved>2015-12-22T08:28:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-21T20:55:17Z" id="166415638">LGTM, it was unexpected that something as low level as Translog.java would be submitting its own background tasks in a thread pool... it's great to move this higher up.
</comment><comment author="jpountz" created="2015-12-21T21:34:33Z" id="166430942">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java Indexed Scripts API - prepare put - Failed to derive XContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15583</link><project id="" key="" /><description>Trying to use the indexed scripts api from the java client

have latest versions
- elasticsearch 2.1.1
- groovy 2.4.4

attempting to follow the guide
https://github.com/elastic/elasticsearch/blob/master/docs/java-api/indexed-scripts.asciidoc

```
    client.client.preparePutIndexedScript
      .setScriptLang("groovy")
      .setId("script1")
      .setSource("_score * doc['my_numeric_field'].value")
      .execute.actionGet
```

fails with this exception

```
org.elasticsearch.ElasticsearchParseException: Failed to derive xcontent
 at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:293)
 at org.elasticsearch.script.ScriptService.validate(ScriptService.java:359)
 at org.elasticsearch.script.ScriptService.putScriptToIndex(ScriptService.java:394)
 at org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction.doExecute(TransportPutIndexedScriptAction.java:51)
 at org.elasticsearch.action.indexedscripts.put.TransportPutIndexedScriptAction.doExecute(TransportPutIndexedScriptAction.java:37)
 at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
 at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
 at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:347)
 at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)
 at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)
 at com.socrata.cetera.search.TestESData$class.bootstrapSettings(TestESData.scala:292)
...
```
</description><key id="123348152">15583</key><summary>Java Indexed Scripts API - prepare put - Failed to derive XContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sokrahta</reporter><labels /><created>2015-12-21T20:05:17Z</created><updated>2015-12-22T19:37:18Z</updated><resolved>2015-12-22T11:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-22T11:38:36Z" id="166591816">The documentation is wrong. Instead of

```
.setSource("_score * doc['my_numeric_field'].value")
```

use

```
.setSource("{ \"script\": \"_score * doc['my_numeric_field'].value\" }")
```

or simpler:

```
.setSource("script", "_score * doc['my_numeric_field'].value")
```
</comment><comment author="sokrahta" created="2015-12-22T19:37:18Z" id="166711543">thanks @ywelsch :+1:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow `bulk` and `indexing` thread pools size larger than number of CPU cores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15582</link><project id="" key="" /><description>I've seen a couple cases recently where users "tuned" up the number of threads in the thread pools for indexing, and then didn't realize this leads to crazy issues like Lucene writing an insane number of segments and doing an insane number of merges, etc.

I don't see any reason to allow this value to exceed the number of CPU cores?
</description><key id="123345218">15582</key><summary>Don't allow `bulk` and `indexing` thread pools size larger than number of CPU cores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T19:46:24Z</created><updated>2016-01-08T19:36:21Z</updated><resolved>2016-01-08T19:36:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T21:36:11Z" id="166431217">+1
</comment><comment author="s1monw" created="2015-12-21T22:02:10Z" id="166436095">I guess that's a good limit to have
</comment><comment author="makeyang" created="2015-12-22T07:49:01Z" id="166544482">then how to deal with high concurrency indexing situation?
I "tuned" up the number of threads just in order to handle it although I know well its side effort.
</comment><comment author="mikemccand" created="2015-12-22T09:26:40Z" id="166565178">@makeyang increasing the bulk and index thread pool sizes beyond the number of cores your hardware does not help indexing throughput ... it will hurt it, as you're seeing in #15538.  See https://www.elastic.co/blog/performance-indexing-2-0 for ways to optimize for indexing.
</comment><comment author="makeyang" created="2015-12-22T09:40:26Z" id="166567922">@mikemccand 
it's not about indexing performance. 
my situation is here: I have a ES cluster and many other teams will write their data into this cluster, the data is very samll though the clients are many, so sometime when all bulk/index thread is occupied, the client will get rejecte exception. the write is not heavy at all. cpu/memory is all used little. but reject exception happened, how to deal with this situation? 
</comment><comment author="mikemccand" created="2015-12-22T09:42:45Z" id="166568280">@makeyang in that case you should increase your bulk `queue_size`, not the `size`.
</comment><comment author="makeyang" created="2015-12-22T09:48:01Z" id="166569113">@mikemccand  got it, thanks.
I have another question to ask: I have done load test(especially for indexing performance) and found out that single node can handle 40000 docs/second, over this, ES will throw reject exception to client. but at the max throughput, the node's resouce is not completely occupied, 
cpu usage is 80%
iops is 170(spin disk, I have done iops test with other tools, iops can reacheve 500).
so my question is why it can't write more while resouce is not fully occupied?
what is bottleneck and how to identify it?
</comment><comment author="makeyang" created="2015-12-22T10:44:35Z" id="166578873">@mikemccand please help to share any thoughts, insights, anything
</comment><comment author="mikemccand" created="2015-12-22T11:03:20Z" id="166581917">@makeyang did you try the suggestions from the above blog post?
</comment><comment author="makeyang" created="2015-12-23T01:38:59Z" id="166774425">yeah. try all of them.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Could not load hunspell dictionary through a symlink</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15581</link><project id="" key="" /><description>Hi,

using ES 2.1.0 I can't use a system hunspell dictionary, as follow:

```
mkdir -p /etc/elasticsearch/hunspell/pl_PL
cd /etc/elasticsearch/hunspell/pl_PL
ln -s /usr/share/hunspell/pl_PL.* .
ls -l /etc/elasticsearch/hunspell/pl_PL
lrwxrwxrwx 1 root root 29 Dec 21 15:48 pl_PL.aff -&gt; /usr/share/hunspell/pl_PL.aff
lrwxrwxrwx 1 root root 29 Dec 21 15:48 pl_PL.dic -&gt; /usr/share/hunspell/pl_PL.dic
```

```
[2015-12-21 15:54:27,417][ERROR][indices.analysis         ] [Hammer and Anvil] Could not load hunspell dictionary [pl_PL]
java.security.AccessControlException: access denied ("java.io.FilePermission" "/etc/elasticsearch/hunspell/pl_PL/pl_PL.dic" "read")
        at java.security.AccessControlContext.checkPermission(Unknown Source)
        at java.security.AccessController.checkPermission(Unknown Source)
        at java.lang.SecurityManager.checkPermission(Unknown Source)
        at java.lang.SecurityManager.checkRead(Unknown Source)
        at sun.nio.fs.UnixChannelFactory.open(Unknown Source)
        at sun.nio.fs.UnixChannelFactory.newFileChannel(Unknown Source)
        at sun.nio.fs.UnixChannelFactory.newFileChannel(Unknown Source)
        at sun.nio.fs.UnixFileSystemProvider.newByteChannel(Unknown Source)
        at java.nio.file.Files.newByteChannel(Unknown Source)
        at java.nio.file.Files.newByteChannel(Unknown Source)
        at java.nio.file.spi.FileSystemProvider.newInputStream(Unknown Source)
        at java.nio.file.Files.newInputStream(Unknown Source)
        at org.elasticsearch.indices.analysis.HunspellService.loadDictionary(HunspellService.java:185)
        at org.elasticsearch.indices.analysis.HunspellService.access$000(HunspellService.java:70)
        at org.elasticsearch.indices.analysis.HunspellService$1.load(HunspellService.java:96)
        at org.elasticsearch.indices.analysis.HunspellService$1.load(HunspellService.java:91)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824)
        at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4830)
        at org.elasticsearch.indices.analysis.HunspellService.scanAndLoadDictionaries(HunspellService.java:134)
        at org.elasticsearch.indices.analysis.HunspellService.&lt;init&gt;(HunspellService.java:102)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
        at java.lang.reflect.Constructor.newInstance(Unknown Source)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:56)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
        at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
        at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:46)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2015-12-21 15:54:27,420][ERROR][indices.analysis         ] [Hammer and Anvil] exception while loading dictionary pl_PL
```

However, coping files or using hard-links (`ln` without -s) works fine. Also version ES 1.7 works with symlinks.

ES user has proper perms for those files:

```
su - elasticsearch -c "wc -l /usr/share/hunspell/pl_PL.*"
   7232 /usr/share/hunspell/pl_PL.aff
 297671 /usr/share/hunspell/pl_PL.dic
 304903 total
```
</description><key id="123345201">15581</key><summary>Could not load hunspell dictionary through a symlink</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thlawiczka</reporter><labels /><created>2015-12-21T19:46:13Z</created><updated>2016-01-10T19:52:38Z</updated><resolved>2016-01-10T19:52:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thlawiczka" created="2015-12-21T19:58:37Z" id="166403538">It seem to be the same as this one: https://github.com/elastic/elasticsearch/issues/14733 .
</comment><comment author="clintongormley" created="2016-01-10T19:52:38Z" id="170385122">Correct. This is no longer supported. https://www.elastic.co/guide/en/elasticsearch/reference/2.1/breaking_20_plugin_and_packaging_changes.html#_symbolic_links_and_paths
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BlobContainer needs to better document its contract</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15580</link><project id="" key="" /><description>This is like another Directory api in that it abstracts the filesystem.

So it needs to be very specific regarding exactly what happens, e.g. which exceptions you should get, what the behavior is when things do or do not already exist, and so on.

Otherwise different `repository` implementations could be lenient, buggy, or even cause data loss.

First we need to figure out https://github.com/elastic/elasticsearch/issues/15579
</description><key id="123344167">15580</key><summary>BlobContainer needs to better document its contract</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>adoptme</label><label>docs</label></labels><created>2015-12-21T19:39:40Z</created><updated>2016-05-31T23:25:45Z</updated><resolved>2016-05-31T23:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>FSBlobContainer leniently truncates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15579</link><project id="" key="" /><description>The two write methods are defined like this:

```
    public void writeBlob(String blobName, InputStream inputStream, long blobSize) throws IOException {
        final Path file = path.resolve(blobName);
        try (OutputStream outputStream = Files.newOutputStream(file)) {
            ...
```

Passing nothing to Files.newOutputStream will truncate the output file if its already there:

```
If no options are present then this method works as if the CREATE, TRUNCATE_EXISTING, and WRITE options are present. In other words, it opens the file for writing, creating the file if it doesn't exist, or initially truncating an existing regular-file to a size of 0 if it exists. 
```

Can we please pass `StandardOpenOptions.CREATE_NEW` so that silent data truncation never happens? 
</description><key id="123342402">15579</key><summary>FSBlobContainer leniently truncates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-12-21T19:28:17Z</created><updated>2016-08-02T13:48:21Z</updated><resolved>2016-08-02T13:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-21T20:13:24Z" id="166406354">&gt;  Can we please pass StandardOpenOptions.CREATE_NEW so that silent data truncation never happens?

+100
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index alias on another alias does not retain the initial filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15578</link><project id="" key="" /><description>If we create an alias on an index with a filter and then subsequently create another alias on the first alias, the second alias does not retain the original filter.

The steps to reproduce.

```
// create index
curl -XPOST localhost:9200/test -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "properties" : {
                "field1" : { "type" : "string", "index" : "not_analyzed" }
            }
        }
    }
}'

// create first alias
curl -XPOST 'http://localhost:9200/_aliases' -d '{
    "actions" : [
        {
            "add" : {
                 "index" : "test",
                 "alias" : "alias1",
                 "filter" : { "term" : { "field1" : "kimchy" } }
            }
        }
    ]
}'

// create secons alias
curl -XPOST 'http://localhost:9200/_aliases' -d '{
    "actions" : [
        {
            "add" : {
                 "index" : "alias1",
                 "alias" : "alias2"
            }
        }
    ]
}'

// Retrieve the first alias definition
curl -XGET http://localhost:9200/_alias/alias1?pretty

{
  "test" : {
    "aliases" : {
      "alias1" : {
        "filter" : {
          "term" : {
            "field1" : "kimchy"
          }
        }
      }
    }
  }
}

// Retrieve the second alias definition
curl -XGET http://localhost:9200/_alias/alias2
{
  "test" : {
    "aliases" : {
      "alias2" : { }
    }
  }
}
```

Please see that there is no filter associated with the second alias.
</description><key id="123336070">15578</key><summary>Index alias on another alias does not retain the initial filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">athanikkal</reporter><labels /><created>2015-12-21T18:49:46Z</created><updated>2015-12-22T09:08:31Z</updated><resolved>2015-12-22T09:08:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-22T09:08:31Z" id="166560423">I think this is more of a documentation issue, which we already have an issue for: #10106 . Alias to alias is not supported and will just create an alias that points to the indices that the target alias points to at creation time. We should document this better so people are not too surprised when this happens. Closing in favor of #10106 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add append processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15577</link><project id="" key="" /><description>The append processor allows to append one or more values to an existing list; add a new list with the provided values if the field doesn't exist yet, or convert an existing scalar into a list and add the provided values to the newly created  list.

This required adapting of IngestDocument#appendFieldValue behaviour, also added support for templating to it.

Closes #14324
</description><key id="123319955">15577</key><summary>Add append processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-12-21T17:15:44Z</created><updated>2015-12-22T15:18:04Z</updated><resolved>2015-12-22T15:18:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-21T21:46:04Z" id="166432975">LGTM
</comment><comment author="javanna" created="2015-12-22T14:21:19Z" id="166627709">heya @martijnvg I updated the PR.
</comment><comment author="martijnvg" created="2015-12-22T15:03:32Z" id="166638692">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException for query with index time boost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15576</link><project id="" key="" /><description>The the following index, mapping, and data,

```
curl -XPUT localhost:9200/rolodex

curl -XPUT localhost:9200/rolodex/_mapping/people -d '{
  "properties": {
    "first_name": {
      "type": "string",
      "boost": 2
    },
    "last_name": {
      "type": "string"
    }
  }
}'

curl -XPOST localhost:9200/rolodex/people/_bulk
-d '{"index":{}}
{"first_name":"Luke","last_name":"Skywalker"}
{"index":{}}
{"first_name":"Darth","last_name":"Vader"}
{"index":{}}
{"first_name":"Hans","last_name":"Solo"}
'
```

the following query reports a null pointer exception.

```
curl -XGET localhost:9200/rolodex/people/_search -d '{
  "query": {
    "query_string": {
      "query": "Luke Skywalker",
      "default_operator": "AND"
    }
  }
}'

RESPONSE:
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 4,
    "failed": 1,
    "failures": [
      {
        "shard": 0,
        "index": "rolodex",
        "node": "L_In19tcTLSNSzmsmgil-A",
        "reason": {
          "type": "null_pointer_exception",
          "reason": null
        }
      }
    ]
  },
  "hits": {
    "total": 1,
    "max_score": 1.3258252,
    "hits": [
      {
        "_index": "rolodex",
        "_type": "people",
        "_id": "AVHFemEFCCiMRlO7V7ZY",
        "_score": 1.3258252,
        "_source": {
          "first_name": "Luke",
          "last_name": "Skywalker"
        }
      }
    ]
  }
}
```

With only 3 documents, sometimes I have to delete and recreate the index a couple of times before the failure is reported. Sometimes no nodes fail, and sometimes all nodes fail.

I am running the elasticsearch 2.1.0 image from
https://hub.docker.com/_/elasticsearch/ on boot2docker 1.9.0.

Here is the stack trace from the elasticsearch logs.

```
2015-12-21T17:05:46.136403752Z [2015-12-21 17:05:46,136][DEBUG][action.search.type       ] [Morg] [rolodex][0], node[L_In19tcTLSNSzmsmgil-A], [P], v[2], s[STARTED], a[id=LF1cKPjDTsiQf5nQP6xCAA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@adc647e] lastShard [true]
2015-12-21T17:05:46.136437040Z RemoteTransportException[[Morg][172.17.0.2:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
2015-12-21T17:05:46.136442165Z Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: NullPointerException;
2015-12-21T17:05:46.136445843Z  at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
2015-12-21T17:05:46.136449638Z  at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
2015-12-21T17:05:46.136461835Z  at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
2015-12-21T17:05:46.136466054Z  at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
2015-12-21T17:05:46.136469858Z  at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
2015-12-21T17:05:46.136473454Z  at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
2015-12-21T17:05:46.136477012Z  at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
2015-12-21T17:05:46.136480479Z  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
2015-12-21T17:05:46.136483856Z  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
2015-12-21T17:05:46.136487285Z  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
2015-12-21T17:05:46.136491130Z  at java.lang.Thread.run(Thread.java:745)
2015-12-21T17:05:46.136494768Z Caused by: java.lang.NullPointerException
2015-12-21T17:05:46.136498057Z  at org.apache.lucene.index.OrdTermState.copyFrom(OrdTermState.java:37)
2015-12-21T17:05:46.136501479Z  at org.apache.lucene.codecs.BlockTermState.copyFrom(BlockTermState.java:56)
2015-12-21T17:05:46.136506167Z  at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat$IntBlockTermState.copyFrom(Lucene50PostingsFormat.java:475)
2015-12-21T17:05:46.136509715Z  at org.apache.lucene.codecs.blocktree.SegmentTermsEnum.seekExact(SegmentTermsEnum.java:1014)
2015-12-21T17:05:46.136513150Z  at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:152)
2015-12-21T17:05:46.136516449Z  at org.elasticsearch.common.lucene.all.AllTermQuery$1.scorer(AllTermQuery.java:102)
2015-12-21T17:05:46.136520126Z  at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
2015-12-21T17:05:46.136523487Z  at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
2015-12-21T17:05:46.136526914Z  at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
2015-12-21T17:05:46.136530305Z  at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:201)
2015-12-21T17:05:46.136533711Z  at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:233)
2015-12-21T17:05:46.136537120Z  at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:769)
2015-12-21T17:05:46.136540486Z  at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
2015-12-21T17:05:46.136543839Z  at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
2015-12-21T17:05:46.136547271Z  ... 10 more
```
</description><key id="123319847">15576</key><summary>NullPointerException for query with index time boost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbgo</reporter><labels /><created>2015-12-21T17:15:05Z</created><updated>2015-12-21T17:35:42Z</updated><resolved>2015-12-21T17:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2015-12-21T17:20:50Z" id="166364298">You can find the fix for 2.1.2 here: https://github.com/elastic/elasticsearch/pull/15506
</comment><comment author="jimczi" created="2015-12-21T17:21:06Z" id="166364352">I am closing it since the fix is pushed.
</comment><comment author="jbgo" created="2015-12-21T17:35:42Z" id="166368832">@jimferenczi thanks! :smile: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.5.0-snapshot-1721183.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15575</link><project id="" key="" /><description>Some files that implement or use the Scorer API had to be changed because of
https://issues.apache.org/jira/browse/LUCENE-6919.
</description><key id="123306582">15575</key><summary>Upgrade to lucene-5.5.0-snapshot-1721183.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T16:06:46Z</created><updated>2016-01-22T18:42:35Z</updated><resolved>2015-12-21T16:17:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-21T16:11:29Z" id="166344648">looks good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop support for simple translog and hard-wire buffer to 8kb</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15574</link><project id="" key="" /><description>Today we have two variants of translogs for indexing. We only recommend the buffered
one which also has a 20% advantage in indexing speed. This commit removes the option and defaults
to the buffered case. It also hard-wires the translog buffer to 8kb instead of 64kb. We used to
adjust that buffer based on if the shard is active or not, this code has also been removed and
instead we just keep an 8kb buffer arround.
</description><key id="123303068">15574</key><summary>Drop support for simple translog and hard-wire buffer to 8kb</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T15:51:29Z</created><updated>2015-12-21T16:28:58Z</updated><resolved>2015-12-21T16:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-21T15:55:00Z" id="166338792">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify translog-based flush settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15573</link><project id="" key="" /><description>This commit removes `index.translog.flush_threshold_ops` and `index.translog.disable_flush`
in favor of `index.translog.flush_threshold_size`. The number of operations is meaningless by itself and
can easily be turned into a size value with knowledge of the data. Disabling the flush is only useful in
tests and we can set the size value to a really high value. If users really need to do this they can
also apply a very high value like `1PB`.
</description><key id="123287263">15573</key><summary>Simplify translog-based flush settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T14:18:17Z</created><updated>2016-01-10T20:09:41Z</updated><resolved>2015-12-21T14:45:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-21T14:24:23Z" id="166316482">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `index.merge.scheduler.notify_on_failure` and default to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15572</link><project id="" key="" /><description>This setting was undocumented and should not be set by any user. We should
fail the shard instead.

Closes  #15570
</description><key id="123281540">15572</key><summary>Remove `index.merge.scheduler.notify_on_failure` and default to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T13:41:27Z</created><updated>2016-01-10T20:07:33Z</updated><resolved>2015-12-21T14:13:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-21T13:43:24Z" id="166308546">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Min should match greater than the number of optional clauses should return no result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15571</link><project id="" key="" /><description>Changes Queries.calculateMinShouldMatch to return the number of "min should match" clauses that the user wanted even if the number of optional clauses is smaller than the provided number.
In such case the query now returns no result.
Closes #15521
</description><key id="123280553">15571</key><summary>Min should match greater than the number of optional clauses should return no result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Query DSL</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T13:34:49Z</created><updated>2015-12-30T19:17:20Z</updated><resolved>2015-12-21T15:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T14:54:43Z" id="166323262">LGTM
</comment><comment author="javanna" created="2015-12-21T17:18:21Z" id="166363828">Hi @jimferenczi I think this one related to #14167 too, which we may close now. Can you have a look and confirm please? ;)
</comment><comment author="jimczi" created="2015-12-30T19:17:20Z" id="168058731">@javanna sorry for the late answer. I answered directly on #14167 but I think that this one resolves your issue only partially, min should match is still ignored when there is no optional clause in the query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `index.merge.scheduler.notify_on_failure` and default to `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15570</link><project id="" key="" /><description>I don't think we should allow folks to swallow merge failures. If this happens all bets are off and we have to fail the shard.
</description><key id="123275813">15570</key><summary>Remove `index.merge.scheduler.notify_on_failure` and default to `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>adoptme</label><label>v5.0.0-alpha1</label></labels><created>2015-12-21T13:04:38Z</created><updated>2015-12-21T14:13:49Z</updated><resolved>2015-12-21T14:13:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Packages in the APT repository do not install any JRE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15569</link><project id="" key="" /><description>When installing the elasticsearch DEB package from the http://packages.elastic.co/ apt repository, the system doesn't automatically bring in any JRE package. This means the package is unusable until this is manually resolved (which causes issues with automated config systems such as Puppet).

The easiest way to get a JRE installed on Debian is to add `default-jre-headless | java6-runtime-headless` to the "Depends" line in the package control file. This tells APT to install Debian's default JRE  if no JRE is currently installed, and the second part tells APT that the dependency can be satisfied by any JRE package that provides at least a version 6-level Java (both the 7.x and 8.x JRE packages provide this).
</description><key id="123275667">15569</key><summary>Packages in the APT repository do not install any JRE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">crispygoth</reporter><labels /><created>2015-12-21T13:03:52Z</created><updated>2016-01-10T20:09:16Z</updated><resolved>2016-01-10T20:09:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-21T14:14:24Z" id="166314798">I remember a similar issue to this one but don't recall the resolution. @tlrx, do you remember?
</comment><comment author="jpountz" created="2015-12-21T14:18:44Z" id="166315550">@nik9000 Maybe it was this one? https://github.com/elastic/elasticsearch/pull/3105
</comment><comment author="tlrx" created="2015-12-21T14:20:02Z" id="166315759">And #6163
</comment><comment author="clintongormley" created="2016-01-10T20:09:16Z" id="170387760">Yes - in #6163 we decided not to require a specific version of java, and leave it up to the user.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cross-type dynamic mappings are fragile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15568</link><project id="" key="" /><description>If you have a dynamic mapping update for a field that already exists on another field, we have a hack in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L615 to try to not introduce a conflict with an existing type. However this is very fragile as it doesn't copy all parameters. Instead we should try to directly reuse the existing field type.
</description><key id="123271347">15568</key><summary>Cross-type dynamic mappings are fragile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.3.0</label></labels><created>2015-12-21T12:32:02Z</created><updated>2015-12-24T09:24:08Z</updated><resolved>2015-12-24T09:24:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[Suggestion] Major version upgrades should be rolling upgrade </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15567</link><project id="" key="" /><description>Hi,

For us, here, elasticsearch is very critical component, and is very painful to torn off the cluster for upgrade to use new features.

So for us is very important rolling upgrades.

Thanks
</description><key id="123271258">15567</key><summary>[Suggestion] Major version upgrades should be rolling upgrade </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felipegs</reporter><labels /><created>2015-12-21T12:31:36Z</created><updated>2016-02-29T20:02:41Z</updated><resolved>2016-02-29T20:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-21T14:10:44Z" id="166313277">For what its worth I agree that all upgrades _should_ be rolling but I don't think that is in the cards, at least for a while. I don't know how long "a while" is though. Sorry!

Also for what its worth, if you have a disaster recovery site you could probably get away with failing over to it for the upgrade. Its less than idea but its better than a total outage of the things that depend on Elasticsearch.
</comment><comment author="felipegs" created="2015-12-21T18:09:24Z" id="166377470">Yes, failling over for the upgrade may be a reasonable option.

But it will be very nice if the new major version is designed with this goal.
</comment><comment author="felipegs" created="2015-12-21T18:19:44Z" id="166381128">I think that are other people that use elasticsearch which has same problem..
</comment><comment author="makeyang" created="2015-12-22T07:54:25Z" id="166545169">better add this
</comment><comment author="eskibars" created="2015-12-29T19:14:28Z" id="167856169">We'll have to treat this on a case-by-case basis.  I think everybody would love to keep rolling upgrades as often as possible, but sometimes it's just not going to be possible.
</comment><comment author="clintongormley" created="2016-02-29T20:02:41Z" id="190358805">Exactly what @eskibars has said. We'd love to make major version upgrades were rolling, but it's just not something we can provide at this point in time. It is something we aspire to.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Top Hits Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15566</link><project id="" key="" /><description /><key id="123255170">15566</key><summary>Aggregations Refactor: Refactor Top Hits Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-21T10:57:49Z</created><updated>2015-12-21T13:42:21Z</updated><resolved>2015-12-21T13:42:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T13:35:46Z" id="166307455">This looks good to me. The FetchPhase wiring is a bit invasive but I have no idea how to make it better. :(
</comment><comment author="jpountz" created="2015-12-21T13:39:15Z" id="166307997">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] ingest on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15565</link><project id="" key="" /><description>After some discussion, a sort of consensus around an `on_failure` block for all processors to have was decided on, instead of a `continue_on_failure` flag described by this issue.

An example of this is as follows:

```
{
  description: "my pipeline with on_failure",
  processors: [
    {
      "grok" : {
          ...
          on_failure : [
            { "set" : { "grok.failed" : true } },
            { "meta": { "_index" : "failed-index" } }
          ]
      }
    },
    {
       "date" : {
         ...
       }
    }
  ]
}
```

an `on_failure` will also be added to a pipeline as a whole:

```
{
   "description" : "...",
   "processors" : [ ... ],
   "on_failure" : [ ... ]
}
```

processors without an `on_failure` parameter defined will throw an exception and exit the pipeline immediately. processors with `on_failure` defined will catch the exception and allow for further processors to run. Exceptions within the `on_failure` block will be treated the same as the top-level.

If a user wishes to handle a failure, and still exit the pipeline immediately, a `fail` processor will be introduced. This processor will do nothing but throw an exception so that the pipeline exits.

Closes: #14548
</description><key id="123222093">15565</key><summary>[Ingest] ingest on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-12-21T07:07:49Z</created><updated>2015-12-22T18:34:28Z</updated><resolved>2015-12-22T18:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-12-21T07:11:43Z" id="166219532">@martijnvg @javanna . I am convinced I was thinking of the on_failure all wrong previously. We spoke of it as something that is a feature of a Processor. I have opened a PR for on_failure where I assume it is a feature of a pipeline definition. This way Processors do not need to have knowledge of their failure processors. That knowledge was a rather confusing self-referencing dependency. so I wrap processors that are created within pipelines within HandledProcessor objects. I am not in love with that name, but I can't think of a better one at the moment. Please, let me know what you think.

currently, I am missing an `on_failure` for a whole pipeline.
</comment><comment author="martijnvg" created="2015-12-21T11:41:05Z" id="166279108">@talevy I left an idea on how we should get `HandledProcessor` to work on both processor and pipeline level.
</comment><comment author="javanna" created="2015-12-22T17:41:04Z" id="166684955">I left a few minor comments, I like this very much though, the CompoundProcessor looks good ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] on_failure block for processors in pipeline</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15564</link><project id="" key="" /><description>After some discussion, a sort of consensus around an `on_failure` block for all processors to have was decided on, instead of a `continue_on_failure` flag described by this issue.

An example of this is as follows:

```
{
  description: "my pipeline with on_failure",
  processors: [
    {
      "grok" : {
          ...
          on_failure : [
            { "set" : { "grok.failed" : true } },
            { "meta": { "_index" : "failed-index" } }
          ]
      }
    },
    {
       "date" : {
         ...
       }
    }
  ]
}
```

an `on_failure` will also be added to a pipeline as a whole:

```
{
   "description" : "...",
   "processors" : [ ... ],
   "on_failure" : [ ... ]
}
```

processors without an `on_failure` parameter defined will throw an exception and exit the pipeline immediately. processors with `on_failure` defined will catch the exception and allow for further processors to run. Exceptions within the `on_failure` block will be treated the same as the top-level.

If a user wishes to handle a failure, and still exit the pipeline immediately, a `fail` processor will be introduced. This processor will do nothing but throw an exception so that the pipeline exits.
</description><key id="123221963">15564</key><summary>[Ingest] on_failure block for processors in pipeline</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-12-21T07:06:23Z</created><updated>2015-12-21T07:06:37Z</updated><resolved>2015-12-21T07:06:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>mvel  Thread Safe question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15563</link><project id="" key="" /><description>We encounter a problem.When concurrent is large&#65292;it occurs stack  as follow&#65306;

"rateplanDateExecutor-499" - Thread t@38815
   java.lang.Thread.State: RUNNABLE
    at java.util.WeakHashMap.get(WeakHashMap.java:403)
    at org.mvel2.util.ParseTools.getBestCandidate(ParseTools.java:243)
    at org.mvel2.util.ParseTools.getBestCandidate(ParseTools.java:224)
    at org.mvel2.PropertyAccessor.getMethod(PropertyAccessor.java:864)
    at org.mvel2.PropertyAccessor.get(PropertyAccessor.java:164)
    at org.mvel2.PropertyAccessor.get(PropertyAccessor.java:135)
    at org.mvel2.ast.ASTNode.getReducedValue(ASTNode.java:167)
    at org.mvel2.MVELInterpretedRuntime.parseAndExecuteInterpreted(MVELInterpretedRuntime.java:93)
    at org.mvel2.MVELInterpretedRuntime.parse(MVELInterpretedRuntime.java:44)
    at org.mvel2.MVEL.eval(MVEL.java:155)
    at org.mvel2.PropertyAccessor.getMethod(PropertyAccessor.java:802)
    at org.mvel2.PropertyAccessor.get(PropertyAccessor.java:164)
    at org.mvel2.PropertyAccessor.get(PropertyAccessor.java:135)
    at org.mvel2.ast.Union.getReducedValue(Union.java:50)
    at org.mvel2.MVELInterpretedRuntime.parseAndExecuteInterpreted(MVELInterpretedRuntime.java:93)
    at org.mvel2.MVELInterpretedRuntime.parse(MVELInterpretedRuntime.java:44)
    at org.mvel2.MVEL.eval(MVEL.java:262)
    at org.mvel2.MVEL.evalToBoolean(MVEL.java:691)
    at cobar.client.router.rules.ibatis.IBatisNamespaceShardingRule.isDefinedAt(IBatisNamespaceShardingRule.java:54)
    at cobar.client.router.rules.ibatis.IBatisNamespaceShardingRule.isDefinedAt(IBatisNamespaceShardingRule.java:33)
    at cobar.client.router.DefaultCobarClientInternalRouter.searchMatchedRuleAgainst(DefaultCobarClientInternalRouter.java:109)
    at cobar.client.router.DefaultCobarClientInternalRouter.doRoute(DefaultCobarClientInternalRouter.java:72)
    at cobar.client.router.DefaultCobarClientInternalRouter.doRoute(DefaultCobarClientInternalRouter.java:52)
    at cobar.client.CobarSqlSessionTemplate.lookupDataSourcesByRouter(CobarSqlSessionTemplate.java:849)
    at cobar.client.CobarSqlSessionTemplate.selectList(CobarSqlSessionTemplate.java:258)
    at cobar.client.CobarSqlSessionTemplate.selectList(CobarSqlSessionTemplate.java:249)
    at org.apache.ibatis.binding.MapperMethod.executeForMany(MapperMethod.java:119)
    at org.apache.ibatis.binding.MapperMethod.execute(MapperMethod.java:63)
    at org.apache.ibatis.binding.MapperProxy.invoke(MapperProxy.java:52)
    at com.sun.proxy.$Proxy33.queryListByParams(Unknown Source)
    at wolverine.ris.index.rateplan.service.RatePlanService.sendRatePlanDate(RatePlanService.java:192)
    at wolverine.ris.index.rateplan.service.RatePlanService.queryRatePlanDate(RatePlanService.java:159)
    at wolverine.ris.index.rateplan.service.RatePlanService$$FastClassByCGLIB$$2e73933b.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:701)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:150)
    at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:95)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
    - locked &lt;26c6fd14&gt; (a java.util.concurrent.ThreadPoolExecutor$Worker)

code&#65306;
if(bestCandidate != null){
    if(methCathe == null){
           RESOLVED_METH_CATHE.put(method,methCathe=new WeakHashMap&lt;Integer,WeakReference&lt;Method&gt;&gt;());
    }
</description><key id="123221563">15563</key><summary>mvel  Thread Safe question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhangrong4</reporter><labels /><created>2015-12-21T07:01:26Z</created><updated>2015-12-21T09:21:55Z</updated><resolved>2015-12-21T09:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T08:54:03Z" id="166239417">Can you give more context, I'm confused to not even see 'elasticsearch' in the stack trace. Also how is RESOLVED_METH_CATHE defined, is it  shared by several scripts?
</comment><comment author="zhangrong4" created="2015-12-21T09:04:14Z" id="166241291">it's just mvel question&#65292;not ES &#65292;
the defination is  as follow:
private Static Map&lt;String,Map&lt;Integer,WeakReference&lt;Method&gt;&gt;&gt;  RESOLVE_METH_CATHE = new  WeakHashMap&lt;String,Map&lt;Integer,WeakReference&lt;Method&gt;&gt;&gt;
when  use get method,it is deadlocked,the cpu usage reach 100%
</comment><comment author="jpountz" created="2015-12-21T09:21:54Z" id="166243980">Since you are not using elasticsearch from within elasticsearch, it is probably a better idea to ask the question at the mvel project directly https://github.com/mvel/mvel. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`/_nodes/hot_threads` fails entirely with no message on FreeBSD</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15562</link><project id="" key="" /><description>When running on FreeBSD, using the hot_threads API causes this exception is the logs:

```
[2015-12-20 22:21:42,473][DEBUG][action.admin.cluster.node.hotthreads] [Noh-Varr] failed to execute on node [fQuWKkdWRauzZNKz21sskA]
RemoteTransportException[[Noh-Varr][127.0.0.1:9300][cluster:monitor/nodes/hot_threads[n]]]; nested: ElasticsearchException[failed to detect hot threads]; nested: IllegalStateException[MBean doesn't support thread CPU Time];
Caused by: ElasticsearchException[failed to detect hot threads]; nested: IllegalStateException[MBean doesn't support thread CPU Time];
        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:88)
        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:45)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:218)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:214)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:335)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: MBean doesn't support thread CPU Time
        at org.elasticsearch.monitor.jvm.HotThreads.innerDetect(HotThreads.java:153)
        at org.elasticsearch.monitor.jvm.HotThreads.detect(HotThreads.java:89)
        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:86)
        ... 8 more
```

The HTTP response however, is an empty 200 response:

```
&#187; get -v 127.0.0.1:9200/_nodes/hot_threads
*   Trying 127.0.0.1...
* Connected to 127.0.0.1 (127.0.0.1) port 9200 (#0)
&gt; GET /_nodes/hot_threads HTTP/1.1
&gt; Host: 127.0.0.1:9200
&gt; User-Agent: curl/7.46.0
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; Content-Type: text/plain; charset=UTF-8
&lt; Content-Length: 0
&lt; 
* Connection #0 to host 127.0.0.1 left intact
```

(Note this is running from the master branch of Elasticsearch)
</description><key id="123213108">15562</key><summary>`/_nodes/hot_threads` fails entirely with no message on FreeBSD</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>bug</label></labels><created>2015-12-21T05:23:20Z</created><updated>2015-12-22T23:01:57Z</updated><resolved>2015-12-22T23:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add fixture capabilities to integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15561</link><project id="" key="" /><description>This change adds a Fixture class for use by gradle. A Fixture is an external process that integration tests will use. It can be added as a dependsOn for integTest, and will automatically be shutdown upon success or failure, as well as relevant information dumped on failure. There is also an example fixture in this change.
</description><key id="123115869">15561</key><summary>Add fixture capabilities to integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-19T23:14:27Z</created><updated>2015-12-19T23:46:46Z</updated><resolved>2015-12-19T23:46:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-19T23:25:00Z" id="166033325">looks good, lets make this little step and then iterate and refactor as we add more. This is really needed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight on more_like_this query not working in 2.0+, it was in 1.7.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15560</link><project id="" key="" /><description>I'll give some examples with `curl` to prove my point, I have both `elasticsearch-2.1.1` and `elasticsearch-1.7.2` on my local to test with. I've also tested with all releases after 2.0 and all behave the same.
### Index setup

``` bash
curl -XDELETE 'http://localhost:9200/test'
curl -XPOST localhost:9200/test -d '{
    "mappings" : {
        "test" : {
            "properties" : {
                "text" : { "type" : "string", "index_options": "offsets" }
            }
        }
    }
}'

curl -XPOST localhost:9200/test/test/1 -d '{
 "text": "Once upon a time there was a pretty little princess. And then the time changed."
}'
```
### Query ES 1.7.2

```
curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "highlight": {
        "fields": {
            "text": {}
        }
    },
    "query": {
        "more_like_this" : {
            "fields" : ["text"],
            "like_text" : "time",
            "min_term_freq" : 1,
            "max_query_terms" : 12,
            "min_doc_freq": 1
        }
    }
}'
```

gives 

```
{
  "took" : 14,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.10848885,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.10848885,
      "_source":{
 "text": "Once upon a time there was a pretty little princess. And then the time changed."
},
      "highlight" : {
        "text" : [ "Once upon a &lt;em&gt;time&lt;/em&gt; there was a pretty little princess.", "And then the &lt;em&gt;time&lt;/em&gt; changed." ]
      }
    } ]
  }
}
```
### Query ES 2.0.x, 2.1.0 and 2.1.1 give same result

```
curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "highlight": {
        "fields": {
            "text": {}
        }
    },
    "query": {
        "more_like_this" : {
            "fields" : ["text"],
            "like" : "time",
            "min_term_freq" : 1,
            "max_query_terms" : 12,
            "min_doc_freq": 1
        }
    }
}'
```

gives

```
{
  "took" : 48,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.10848885,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.10848885,
      "_source":{
 "text": "Once upon a time there was a pretty little princess. And then the time changed."
}
    } ]
  }
}
```

notice above doesn't include `highlight` in response. Is this intended? I saw [this PR](https://github.com/elastic/elasticsearch/pull/11077), thinking it might be related?

Is there another way to achieve the highlighting in newer versions? I've tried indexing with term vectors for `text` and that works for all ES versions, but I don't want to do that, since I'd rather keep the returned results in highlight on a sentence level to be easier to understand the context, so `postings` highlighting is the best choice for me. Plus it takes less space. Any recommendations to achieve highlighting, or am I doing sth wrong? Thank you!
</description><key id="123105827">15560</key><summary>Highlight on more_like_this query not working in 2.0+, it was in 1.7.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">andreip</reporter><labels><label>:Highlighting</label><label>:More Like This</label></labels><created>2015-12-19T19:20:10Z</created><updated>2016-02-09T16:41:33Z</updated><resolved>2016-02-09T16:41:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dtran320" created="2015-12-19T19:25:53Z" id="166017470">+1 Is this related to the change to Lucene's PostingsHighlighter in #11077?
</comment><comment author="andreip" created="2016-01-07T07:59:38Z" id="169590038">can someone check this out? I did provide easy curl setup for the issues, just get two ES versions and try it out. Thanks!
</comment><comment author="clintongormley" created="2016-01-10T10:31:16Z" id="170332765">@javanna could  you take a look at this please?
</comment><comment author="andreip" created="2016-01-22T08:47:31Z" id="173848304">ping
</comment><comment author="javanna" created="2016-01-28T11:46:06Z" id="176139649">I checked this out (sorry it took me a while!) and I can confirm this is due to the move to the lucene postings highlighter. Our own postings highlighter used to rewrite the query against the real index reader, which is a no-go performance wise. The lucene postings highlighter rewrites against an empty reader, the downside being that any query relying on the content of the reader it is rewritten against will not be highlighted. A more like this query does depend on what's in the index as the corresponding boolean query that is executed depends on frequencies of indexed terms. I am not sure the lucene postings highlighter can support the more like this query to be honest, maybe @rmuir can comment on whether that is technically possible without doing crazy rewrites. For now I would suggest to either provide a different query to highlight, if possible at all, or move to a different highlighter, for instance the plain one.
</comment><comment author="rmuir" created="2016-01-28T11:55:18Z" id="176144020">I'm gonna be honest: I do not think highlighting even makes sense for this query.

I also don't know why MoreLikeThisQuery is being used (vs MoreLikeThis directly), as its an odd thing to expose via a query. MoreLikeThisQuery documents it mainly exists as a way to wedge MoreLikeThis into certain circumstances like custom queryparsers. It does strange things like set a 30% minShouldMatch by default (this is random, and does not make sense to me, probably a hack around the shitty query expansion happening here, e.g. no term frequency normalization).
</comment><comment author="andreip" created="2016-02-05T13:56:57Z" id="180369524">&gt; I also don't know why MoreLikeThisQuery is being used (vs MoreLikeThis directly),

@rmuir I don't think I understood that. What's the alternative of MLTQuery? There's another thing called [MLT API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-more-like-this.html) but it's been removed and it's said to use MLTQuery instead. Can you detail a bit? Thanks
</comment><comment author="rmuir" created="2016-02-05T15:25:50Z" id="180399934">I am talking in lucene terms. I think ES exposes MLT in the wrong way.
</comment><comment author="andreip" created="2016-02-05T15:38:23Z" id="180405314">@javanna Thanks for looking into this and for the thorough explanation! Indeed, using `plain` works for highlight, I'm not sure how fast this will be for our case, will try. Just as a fallback, is there a way to bypass MLT and still get highlighting? e.g. getting top K tf-idf terms extracted from text provided via a different query? since after that, I'd use those top-K and do an or-based search query with highlight on.
</comment><comment author="javanna" created="2016-02-09T15:27:30Z" id="181913765">If the problem is speed, I would recommend to test it before declaring it a problem, it might very well be that it's ok. There may be alternatives using the terms vectors api to extract the top K terms and create the boolean query yourself based on those terms, but it would make things slower than using the plain highlighter I suspect.
</comment><comment author="andreip" created="2016-02-09T16:09:39Z" id="181932658">We tested with plain and it seems fine. Issue is solved, thanks for looking
into this and providing an alternative!
On 9 Feb 2016 5:28 p.m., "Luca Cavanna" notifications@github.com wrote:

&gt; If the problem is speed, I would recommend to test it before declaring it
&gt; a problem, it might very well be that it's ok. There may be alternatives
&gt; using the terms vectors api to extract the top K terms and create the
&gt; boolean query yourself based on those terms, but it would make things
&gt; slower than using the plain highlighter I suspect.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/15560#issuecomment-181913765
&gt; .
</comment><comment author="javanna" created="2016-02-09T16:41:32Z" id="181949429">thank you for getting back to us @andreip . Good to hear the plain highlighter works well for you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make RescoreBuilder implement Writable &amp; fromXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15559</link><project id="" key="" /><description>The RescoreBuilders in SearchSourceBuilder also need to be refactored to be serializable as java objects and be able to parse from xContent. The necessary steps are similar to the recent HighlightBuilder refactoring in #15044.
</description><key id="123096373">15559</key><summary>Make RescoreBuilder implement Writable &amp; fromXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-19T16:33:44Z</created><updated>2016-01-26T14:45:11Z</updated><resolved>2016-01-26T14:45:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Safe cluster state task notifications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15558</link><project id="" key="" /><description>This commit addresses an issue where a cluster state task listener
throwning an exception could prevent other listeners from being
notified, and could prevent the executor from receiving notifications
that a new cluster state was published. Additionally, this commit also
addresses a similar issue for executors handling cluster state
publication notifications.
</description><key id="123096148">15558</key><summary>Safe cluster state task notifications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>blocker</label><label>enhancement</label><label>review</label></labels><created>2015-12-19T16:29:44Z</created><updated>2016-01-10T12:33:45Z</updated><resolved>2016-01-05T20:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-01-05T15:58:25Z" id="169043279">Adding back the [v2.2.0 label](https://github.com/elastic/elasticsearch/labels/v2.2.0) and marking this as a [blocker](https://github.com/elastic/elasticsearch/labels/blocker) for the 2.2.0 release.
</comment><comment author="jasontedor" created="2016-01-05T20:35:19Z" id="169125372">Closing in favor of #15777.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: define a default mapping for all indexes and all types using templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15557</link><project id="" key="" /><description>Documented how to define a default mapping to match all indexes and all types using custom template.
Useful when you have the name of the index and the type generated dynamically. With this approach you  can avoid sending a new PUT request for each new type/index you have created.
</description><key id="123082770">15557</key><summary>Docs: define a default mapping for all indexes and all types using templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">fforbeck</reporter><labels><label>docs</label><label>review</label></labels><created>2015-12-19T12:14:14Z</created><updated>2016-01-14T00:39:11Z</updated><resolved>2016-01-12T12:36:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T10:24:35Z" id="170332414">Hi @fforbeck 

Thanks for the PR.  In general i like it, the only thing I don't like is the use of the `attachment` plugin.  We will be removing this plugin in the future (to be replaced by an attachment processor for the new ingest node functionality) so i'd prefer to use some other example, eg just disabling _all is sufficient to demonstrate this technique.
</comment><comment author="fforbeck" created="2016-01-11T11:48:45Z" id="170517871">Hi @clintongormley 
Yep, I got your point. 
I will update the docs and remove the `attachment` stuff.

Thanks for the review. :+1: 
</comment><comment author="fforbeck" created="2016-01-12T02:48:46Z" id="170766720">Hi @clintongormley 
I have removed the `attachment` plugin details and added an example using the `_all` property, as you had recommended.

Please, take a look if possible.

Thank you!!
</comment><comment author="clintongormley" created="2016-01-12T12:36:44Z" id="170898766">Thanks @fforbeck - there were a couple of issues with your example which I've fixed, and i've simplified the text to just demonstrate the basics.

thanks for the PR
</comment><comment author="fforbeck" created="2016-01-14T00:39:11Z" id="171484880">@clintongormley 
Alright, no problem! Much better now :+1: 

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security manager and file permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15556</link><project id="" key="" /><description>Current version: 2.1.1

```
IndexCreationException[failed to create index]; nested: AccessControlException[access denied ("java.io.FilePermission" "/data/search/ext/exceptions.txt" "read")];
        at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:360)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:307)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:176)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:494)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.security.AccessControlException: access denied ("java.io.FilePermission" "/data/search/ext/exceptions.txt" "read")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)
        at java.security.AccessController.checkPermission(AccessController.java:559)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
        at java.io.File.isDirectory(File.java:838)
```

```
$ ls -la /data/search/ext/exceptions.txt
-rwxrwxrwx 1 root root 816 &#1076;&#1077;&#1082;.  19 10:05 /data/search/ext/exceptions.txt
```

File has right permissions and can be read by elasticsearch user.

I found a [workaround](https://discuss.elastic.co/t/accesscontrolexception-when-accessing-local-file-from-plugin/33658) and disable security manager, everything is working fine now.

Can you explain this behavior or fix it if it is wrong ?
</description><key id="123071075">15556</key><summary>Security manager and file permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">aeryaguzov</reporter><labels><label>docs</label></labels><created>2015-12-19T08:17:41Z</created><updated>2016-02-29T19:56:57Z</updated><resolved>2016-02-29T19:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-19T09:32:11Z" id="165963729">Is it a config file or so? If so, you should place it in the config folder.

My 2 cents
</comment><comment author="aeryaguzov" created="2015-12-21T04:48:37Z" id="166202249">It is a file with mapping for char filter, specified in **mapping_path**.

Can you explain why this file and, I think, any other must be in config folder? Is there any docs about that?
</comment><comment author="dadoonet" created="2015-12-21T05:25:44Z" id="166207441">I agree that it should be documented in char filter doc. Also may be added to breaking changes in 2.0.

Do you want to contribute this doc change?
</comment><comment author="aeryaguzov" created="2015-12-21T11:36:58Z" id="166278634">Yeah, of course.

Just need time to test all "paths" options like **synonyms_path** and **stopwords_path** to ensure they are all must be in the config folder.

I'll provide a PR in a couple of days.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow plugins to "upgrade" provided deps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15555</link><project id="" key="" /><description>This only really applies to slf4j, since the others are in the
distribution, but it is necessary if a plugin wants to depend on slf4j.
</description><key id="123070819">15555</key><summary>Allow plugins to "upgrade" provided deps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-19T08:08:14Z</created><updated>2015-12-21T02:00:45Z</updated><resolved>2015-12-19T08:10:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-19T08:09:57Z" id="165959895">looks good, thanks.
</comment><comment author="rjernst" created="2015-12-21T02:00:44Z" id="166173474">I had to revert this, but made a follow up commit that fixed the issue just for slf4j-api:
a518599
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed to run inline script | No such property: for class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15554</link><project id="" key="" /><description>I'm trying to use function_score-&gt;script_score and calculate with doc value i got error

&lt;pre class="source"&gt;
curl -XGET "localhost:9200/my_ipod/song_properties/_search?pretty=true&amp;size=3" -d'
{
  "query": {
    "function_score": {
      "query": {"simple_query_string": 
            {
                "query": "dewa satu"
            }
        },
      "script_score": {
        "script": "_score * doc['score_boost']"
      }
    }
  }
}'

{
  "error" : {
    "root_cause" : [ {
      "type" : "groovy_script_execution_exception",
      "reason" : "failed to run inline script [_score * doc[score_boost]] using lang [groovy]"
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "my_ipod",
      "node" : "BFIXhyPyR46cyzRES7E0Dw",
      "reason" : {
        "type" : "groovy_script_execution_exception",
        "reason" : "failed to run inline script [_score * doc[score_boost]] using lang [groovy]",
        "caused_by" : {
          "type" : "missing_property_exception",
          "reason" : "No such property: score_boost for class: 383d17d583550b6f5b92342d7059da6add45f3aa"
        }
      }
    } ]
  },
  "status" : 400
}

&lt;/pre&gt;


But if i change doc['score_boost'] to 1 it work, ip pretty sure if 'score_boost' is there and all doc have that value. bellow is sample result of my doc type item

&lt;pre class="source"&gt;
curl -XGET "localhost:9200/my_ipod/song_properties/_search?pretty=true&amp;size=3" -d '{
  "query": {
    "function_score": {
      "query": {"simple_query_string": 
            {
                "query": "dewa satu"
            }
        },
      "script_score": {
        "script": "_score * 1"
      }
    }
  }
}
&lt;/pre&gt;

result is fine

&lt;pre class="source"&gt;
{
  "took" : 310,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1124,
    "max_score" : 25.883312,
    "hits" : [ {
      "_index" : "my_ipod",
      "_type" : "song_properties",
      "_id" : "100000086",
      "_score" : 25.883312,
      "_source":{"doc": {"artist_name": "Dewa 19", "song_name_origin": "Satu Sisi", "album_name": "Pandawa Lima", "artist_name_origin": "Dewa 19", "song_id": "100000086", "album_name_origin": "Pandawa Lima", "genre_name": "Rock", "song_name": "Satu Sisi", "score_boost": 1.002098104639327}}
    }, {
      "_index" : "my_ipod",
      "_type" : "song_properties",
      "_id" : "100000116",
      "_score" : 25.883312,
      "_source":{"doc": {"artist_name": "Dewa", "song_name_origin": "Satu", "album_name": "Laskar Cinta", "artist_name_origin": "Dewa", "song_id": "100000116", "album_name_origin": "Laskar Cinta", "genre_name": "Rock", "song_name": "Satu", "score_boost": 1.0129150221711472}}
    }, {
      "_index" : "my_ipod",
      "_type" : "song_properties",
      "_id" : "100000066",
      "_score" : 25.718107,
      "_source":{"doc": {"artist_name": "Dewa 19", "song_name_origin": "Satu Hati", "album_name": "Terbaik - Terbaik", "artist_name_origin": "Dewa 19", "song_id": "100000066", "album_name_origin": "Terbaik - Terbaik", "genre_name": "Rock", "song_name": "Satu Hati", "score_boost": 1.004947400036952}}
    } ]
  }
}
&lt;/pre&gt;


and i try to enable all setting about scripting it does't work too :( pls help

&lt;pre class="source"&gt;
script.inline: on
script.indexed: on
script.engine.groovy.inline.aggs: on
script.engine.groovy.inline.mapping: on
script.engine.groovy.inline.search: on
script.engine.groovy.inline.update: on
script.engine.groovy.inline.plugin: on
&lt;/pre&gt;
</description><key id="123057994">15554</key><summary>failed to run inline script | No such property: for class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oktarahadian</reporter><labels /><created>2015-12-19T03:18:16Z</created><updated>2016-01-10T20:14:35Z</updated><resolved>2016-01-10T20:14:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="oktarahadian" created="2015-12-20T04:23:56Z" id="166062235">http://stackoverflow.com/questions/28551634/elasticsearch-sort-error-using-script

I think this issue with escaping in shell, by changing to doc['"'"'score_boost'"'"'].value, not error but still, not calculate score correction (all score is 0.0) pls help

&lt;pre class="code"&gt;
curl -XGET "localhost:9200/my_ipod/song_properties/_search?pretty=true&amp;size=3" -d'
{
  "query": {
    "function_score": {
      "query": {"simple_query_string":
            {
                "query": "dewa satu"
            }
        },
      "script_score": {
        "script": {
            "lang": "groovy",
            "inline": "_score * doc['"'"'score_boost'"'"'].value"
            }
        }
    }
  }
}'
{
  "took" : 365,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1124,
    "max_score" : 0.0,
    "hits" : [ {
      "_index" : "my_ipod",
      "_type" : "song_properties",
      "_id" : "110004712",
      "_score" : 0.0,
      "_source":{"doc": {"song_name_origin": "Lelaki Yang Kumau (Jazz Up Your Life)", "artist_name": "Ratu", "album_name": "No. Satu", "score_boost": 1.0004258090311626, "song_name": "Lelaki Yang Kumau (Jazz Up Your Life)", "album_name_origin": "No. Satu", "genre_name": "Pop", "song_id": "110004712", "artist_name_origin": "Ratu"}}
    }, {
      "_index" : "my_ipod",
      "_type" : "song_properties",
      "_id" : "110004717",
      "_score" : 0.0,
      "_source":{"doc": {"song_name_origin": "Teman Tapi Mesra", "artist_name": "Ratu", "album_name": "No. Satu", "score_boost": 1.001438722822439, "song_name": "Teman Tapi Mesra", "album_name_origin": "No. Satu", "genre_name": "Pop", "song_id": "110004717", "artist_name_origin": "Ratu"}}
    }, {
      "_index" : "my_ipod",
      "_type" : "song_properties",
      "_id" : "110004827",
      "_score" : 0.0,
      "_source":{"doc": {"song_name_origin": "Hanya Satu", "artist_name": "Shitta Devi", "album_name": "Berikan Dia Cinta", "score_boost": 1, "song_name": "Hanya Satu", "album_name_origin": "Berikan Dia Cinta", "genre_name": "Pop", "song_id": "110004827", "artist_name_origin": "Shitta Devi"}}
    } ]
  }
}
&lt;/pre&gt;
</comment><comment author="clintongormley" created="2016-01-10T20:14:35Z" id="170388091">Hi @oktarahadian 

This sounds like a question for the forums http://discuss.elastic.co/ rather than a bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restrict index names to ASCII</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15553</link><project id="" key="" /><description>Index names are currently used as folder names to store data. On some systems this can lead to issues when index names contain characters outside the ASCII range. We currently already forbid special characters in index names such as '\', '/', '*', '?', '"', '&lt;', '&gt;', '|', ' ', ','.

Relates to #15552 
</description><key id="123035539">15553</key><summary>Restrict index names to ASCII</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Index APIs</label><label>bug</label><label>discuss</label></labels><created>2015-12-18T22:16:27Z</created><updated>2016-02-14T15:27:55Z</updated><resolved>2016-02-14T15:27:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-19T14:07:12Z" id="165987208">+1
</comment><comment author="nik9000" created="2015-12-19T14:08:11Z" id="165987260">&gt; Index names are currently used as folder names to store data.

I'd be just as happy to break that relationship.
</comment><comment author="jpountz" created="2015-12-19T14:09:59Z" id="165987332">Relates to #9059 as well.
</comment><comment author="nik9000" created="2015-12-19T14:14:37Z" id="165987507">&gt; I'd be just as happy to break that relationship.

Actually the more I think about the more I'd prefer to break that relationship. I'd like us to be able to atomically rename an index and leave behind an alias pointing to the new index one day. And that'd require breaking the relationship between index name and directory name.
</comment><comment author="ywelsch" created="2015-12-19T14:53:53Z" id="165992748">@nik9000 I'm very much in favour of breaking that relationship as well (Index UUIDs play a key role in this). I added the discussion label as we should probably come up with a plan when and how to tackle this and related issues (e.g. #13265 , #14932). I also wonder whether we should strive for a short term solution (restrict index names) and, at the same time, start thinking about more concrete plans for changing the representation on disk.
</comment><comment author="drewr" created="2015-12-29T18:04:08Z" id="167845005">/me presses :+1: button
</comment><comment author="clintongormley" created="2016-02-14T15:27:55Z" id="183905427">Closing in favour of #16442
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set -Dsun.jnu.encoding=UTF-8 in elasticsearch.in.sh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15552</link><project id="" key="" /><description>Currently we set `-Dfile.encoding=UTF-8` in elasticsearch.in.sh (which affects the content of a file) but not the dual `sun.jnu.encoding` which affects the creation of file names. The value of `sun.jnu.encoding` is usually determined on Unix systems by the `LANG` environment variable.

With UTF-8 index names, this can lead to interesting issues. I have observed this first when setting up my build server as the REST test `rest-api-spec/src/main/resources/rest-api-spec/test/index/10_with_id.yaml` failed. This test creates an index with name `test-weird-index-&#20013;&#25991;`.

To reproduce:
- Change the following line in elasticsearch.in.sh:
  `JAVA_OPTS="$JAVA_OPTS -Dfile.encoding=UTF-8"` to `JAVA_OPTS="$JAVA_OPTS -Dfile.encoding=UTF-8 -Dsun.jnu.encoding=ANSI_X3.4-1968"`
- start elasticsearch
- Execute `curl -XPOST localhost:9200/test-weird-index-&#20013;&#25991;`
- see an endless stream of exceptions fly by

```
[2015-12-18 22:04:27,637][WARN ][gateway                  ] [One Above All] [test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;][4]: failed to list shard for shard_started on node [0SZxDwUvSoSdMLkVhBRsUQ]
FailedNodeException[Failed node [0SZxDwUvSoSdMLkVhBRsUQ]]; nested: RemoteTransportException[[One Above All][127.0.0.1:9300][internal:gateway/local/started_shards[n]]]; nested: ElasticsearchException[failed to load started shards]; nested: InvalidPathException[Malformed input or input contains unmappable characters: test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;];
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.onFailure(TransportNodesAction.java:187)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.access$700(TransportNodesAction.java:94)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction$2.handleException(TransportNodesAction.java:160)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: RemoteTransportException[[One Above All][127.0.0.1:9300][internal:gateway/local/started_shards[n]]]; nested: ElasticsearchException[failed to load started shards]; nested: InvalidPathException[Malformed input or input contains unmappable characters: test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;];
Caused by: ElasticsearchException[failed to load started shards]; nested: InvalidPathException[Malformed input or input contains unmappable characters: test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;];
    at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:154)
    at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:59)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
    at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.file.InvalidPathException: Malformed input or input contains unmappable characters: test-weird-index-&#65508;&#65464;&#65453;&#65510;&#65430;&#65415;
    at sun.nio.fs.UnixPath.encode(UnixPath.java:147)
    at sun.nio.fs.UnixPath.&lt;init&gt;(UnixPath.java:71)
    at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
    at sun.nio.fs.AbstractPath.resolve(AbstractPath.java:53)
    at org.elasticsearch.env.NodeEnvironment$NodePath.resolve(NodeEnvironment.java:91)
    at org.elasticsearch.env.NodeEnvironment$NodePath.resolve(NodeEnvironment.java:84)
    at org.elasticsearch.env.NodeEnvironment.availableShardPaths(NodeEnvironment.java:634)
    at org.elasticsearch.gateway.TransportNodesListGatewayStartedShards.nodeOperation(TransportNodesListGatewayStartedShards.java:125)
    ... 8 more
``
```
</description><key id="123029685">15552</key><summary>Set -Dsun.jnu.encoding=UTF-8 in elasticsearch.in.sh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-12-18T21:30:57Z</created><updated>2015-12-18T22:05:31Z</updated><resolved>2015-12-18T22:05:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-18T21:57:12Z" id="165906409">We can't set this: it will break systems like windows. Its something we shouldn't touch.

Things like index names (which are like identifiers) cannot safely be unicode filenames without serious effort: windows is case sensitive, OS X is even worse (doing unicode normalization and other things). some windows filesystems like FAT32 simply aren't encoded in unicode but instead use platform encoding. 

Even if we went to crazy extremes to get all this stuff right, some filesystems just can't encode unicode names, and the JDK still struggles (e.g. https://bugs.openjdk.java.net/browse/JDK-7130915): it won't be reliable.

restrict to basic ascii...
</comment><comment author="ywelsch" created="2015-12-18T22:05:31Z" id="165907824">I'm ok with restricting index names to ASCII. I'll close this PR and open an issue for ASCII index names.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Add validation of shapes to ShapeBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15551</link><project id="" key="" /><description>Currently the validation of geo shapes is taking place in the various parse() methods in ShapeBuilder. With the search refactoring, we can no longer rely on shapes being parsed from json, so the shapes also need to be validated when just using the java api.

A lot of validation concerns the number of points a shape needs to have in order to be valid (e.g. LineString &gt;= 2 points). This is not possible with current the builders where points can be added one by one, so some no-arg constructors are changed to require mandatory parameters and to validate those at construction time. To help with constructing longer lists of points, a new utility PointsListBuilder is instroduces which can produce list of coordinates accepted by most of the other shape builder constructors. 

Also adding tests for invalid shape exceptions to the already existing shape builder tests.

Relates to #14416
</description><key id="123021543">15551</key><summary>Geo: Add validation of shapes to ShapeBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T20:33:00Z</created><updated>2017-06-10T15:48:04Z</updated><resolved>2016-01-12T18:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2016-01-11T23:40:02Z" id="170733324">Logic LGTM. Left a few minor comments.
</comment><comment author="cbuescher" created="2016-01-12T12:33:00Z" id="170897993">Thanks for the review @nknize, I addressed your comments by renaming the new helper for chaining points to CoordinatesBuilder (also changing the method names). I also changed the name of the common supertype of LineStringBuilder and MultiPointBuilder from PointCollection to CoordinateCollection to reflect that it wraps a List&lt;Coordinate&gt;. I left the MultiPointBuilder name as is since that is the name of the GeoShape it creates.

I have a minor question left concerning the "coerce" option for PolygonBuilder. So far polygon shells and holes could be coerced from un-closed coordinate lists in ShapeBuilder#parseLinearRing(). I'm not sure if we need this option for the PolygonBuilder as well, that's why I added an aditional constructor and a hole() method with the "coerce" option, but both are basically unused in other than in new tests that I introduced, so I'm wondering if those options are needed when using the Java API or if we rather force users to close shells/holes themselves explicitely. What do you think?
</comment><comment author="nknize" created="2016-01-12T14:38:43Z" id="170931052">&gt; I'm wondering if those options are needed when using the Java API

Without them the Java API would be inconsistent with the REST parser, correct? If so then I think we should keep them for consistency? 
</comment><comment author="cbuescher" created="2016-01-12T15:06:45Z" id="170941232">As far as I can see the REST parsers don't really parse a dedicated "coerce" parameter, that flag is retrieved from the FieldMapper if one is present, and ShapeBuilder#parseLinearRing() should add the missing point before creating the PolygonBuilder, so I'm not sure if that option is needed in the PolygonBuilder.
</comment><comment author="nknize" created="2016-01-12T17:49:45Z" id="170990961">Ah yes, I was thinking of the QueryBuilders. You're right, the mapper should contain the coerce option. I don't think its necessary to carry it in the PolygonBuilder, but that can be done in a separate PR? I'm good w/ this as is.
</comment><comment author="cbuescher" created="2016-01-12T17:52:31Z" id="170992084">Okay, thanks, will merge then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"simple_query_string" vs. "query_string" field analysis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15550</link><project id="" key="" /><description>Recently we discovered that, since we aren't sanitizing search terms as they come into our system, we would get occasional parsing exceptions in ES when special characters such as / (forward slash) , etc. were used w/ "query_string".  So, we decided to switch to "simple_query_string".  However, we discovered that the same analyzers are not being used for each.  I reviewed "When Analyzers Are Used" (https://www.elastic.co/guide/en/elasticsearch/guide/current/analysis-intro.html?q=analy#_when_analyzers_are_used) to see if it indicated there would be a difference between simple and regular query string but it did not, so I'm wondering if this is a bug.  For example:

`"query_string": {
   "query": "sales",
   "fields": [
      "title"
   ]
}`

will use the analyzer for the "title" field which is the en_analyzer and properly stem "sales" to "sale" and find the matching documents.  Simply changing "query_string" to "simple_query_string" will not.  We have to search for "sale" or add an analyzer to the query, like so:

`"simple_query_string": {
   "query": "sales",
   "fields": [
      "title"
   ],
   "analyzer": "en_analyzer"
}`

Of course, not all our fields are analyzed the same way and the default behavior described in the documentation reference above makes perfect sense.  Is this a bug or does "simple_query_string" not behave the same way w/ respect to field analysis during query?  We are using ES 1.7.2.
</description><key id="123020338">15550</key><summary>"simple_query_string" vs. "query_string" field analysis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">tdoman</reporter><labels><label>:Query DSL</label><label>feedback_needed</label></labels><created>2015-12-18T20:24:08Z</created><updated>2016-02-09T21:15:09Z</updated><resolved>2016-02-09T17:14:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tdoman" created="2015-12-21T19:41:56Z" id="166400357">Sorry, forgot to include our definition for "en_analyzer":
`"en_analyzer": {
   "type": "custom",
   "tokenizer": "icu_tokenizer",
   "filter": [
      "icu_normalizer",
      "en_stop_filter",
      "en_stem_filter",
      "icu_folding",
      "shingle_filter"
     ],
    "char_filter": [
       "html_strip"
     ]
}`

`"en_stop_filter": {
          "type": "stop",
          "stopwords": [
            "_english_"
          ]
        },
        "en_stem_filter": {
          "type": "stemmer",
          "name": "minimal_english"
        }`
</comment><comment author="dakrone" created="2015-12-30T18:11:04Z" id="168047452">So I looked into this, the way that the `SimpleQueryParser` from Lucene works is
that it takes a single `Analyzer` to analyze the terms.

I think from the ES side we might be able to expand a `SimpleQueryParser` parser
into multiple queries (passing a different analyzer for each field) that can be
combined into a single boolean query. It does sound like it would be worthwhile
however, instead of tying the query to a single analyzer.
</comment><comment author="rmuir" created="2015-12-30T18:29:44Z" id="168049934">&gt; So I looked into this, the way that the SimpleQueryParser from Lucene works is
&gt; that it takes a single Analyzer to analyze the terms.

Because no method should ever take anything else: if you want per-field behavior, pass e.g. PerFieldAnalyzerWrapper! It is really just that simple
</comment><comment author="dakrone" created="2015-12-30T19:23:20Z" id="168059531">&gt; if you want per-field behavior, pass e.g. PerFieldAnalyzerWrapper! It is
&gt; really just that simple

That's great, thanks for the pointer @rmuir!
</comment><comment author="tdoman" created="2016-01-03T16:09:33Z" id="168511342">Thanks guys, I just read issue #15634, we need that also.  Which versions of ES might we be able to get a fix for this?  As I stated above, we're using 1.7.2 currently.
</comment><comment author="dakrone" created="2016-01-05T23:04:22Z" id="169163012">@tdoman I've been looking more into this to try and fix it, however, I can't
reproduce the issue you are seeing, here is my full reproduction:

``` json
DELETE /myindex
{}

POST /myindex
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index": {
      "analysis": {
        "analyzer": {
          "en_analyzer": {
            "type": "custom",
            "tokenizer": "icu_tokenizer",
            "filter": ["icu_normalizer",
                       "en_stop_filter",
                       "en_stem_filter",
                       "icu_folding",
                       "shingle" ],
            "char_filter": [ "html_strip" ]
          }
        },
        "filter": {
          "en_stop_filter": {
            "type": "stop",
            "stopwords": [ "_english_" ]
          },
          "en_stem_filter": {
            "type": "stemmer",
            "name": "minimal_english"
          }
        }
      }
    }
  },
  "mappings": {
    "doc": {
      "properties": {
        "title": {
          "type": "string",
          "analyzer": "en_analyzer"
        }
      }
    }
  }
}

POST /myindex/doc/1?refresh
{
  "title": "sale"
}

POST /myindex/doc/_search?pretty
{
  "query": {
    "query_string": {
      "query": "sales",
      "fields": ["title"]
    }
  }
}

POST /myindex/doc/_search?pretty
{
  "query": {
    "simple_query_string": {
      "query": "sales",
      "fields": ["title"]
    }
  }
}
```

Both the `query_string` and `simple_query_string` versions of the query return
the single hit.

Is there a difference I am missing for reproducing this?
</comment><comment author="dakrone" created="2016-02-09T17:14:54Z" id="181962604">Closing this as I haven't heard back, feel free to comment if it's still an issue.
</comment><comment author="tdoman" created="2016-02-09T21:15:08Z" id="182070329">Yes thanks Lee, I still need to get back to this one and figure out why you can't reproduce w/ the mapping you gave.  I must be missing some detail.  I'll definitely ping you if I can get to the bottom of it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix Windows service installation failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15549</link><project id="" key="" /><description>When ES_HOME contains parentheses, parsing of the if statement around ES_CLASSPATH is thrown off.  This fix enables delayed expansion in service.bat (already enabled in elasticsearch.bat).

Closes #15349
</description><key id="123019326">15549</key><summary>Fix Windows service installation failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T20:17:19Z</created><updated>2016-01-21T15:30:13Z</updated><resolved>2016-01-21T14:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="russcam" created="2015-12-23T08:29:39Z" id="166832998">Looks good to me @gmarz :+1:  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add AntTask to simplify controlling logging when running ant from gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15548</link><project id="" key="" /><description>This new task allows setting code, similar to a doLast or doFirst,
except it is specifically geared at running ant (and thus called doAnt).
It adjusts the ant logging while running the ant so that the log
level/behavior can be tweaked, and automatically buffers based on gradle
logging level, and dumps the ant output upon failure.
</description><key id="123014732">15548</key><summary>Add AntTask to simplify controlling logging when running ant from gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T19:48:48Z</created><updated>2015-12-18T20:13:33Z</updated><resolved>2015-12-18T20:13:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-18T19:51:58Z" id="165883426">This has the same problem of mindlessly trying to avoid creating a new antbuilder. The thirdparty task does this **on purpose** because forbidden apis might load tons of classes.
</comment><comment author="rjernst" created="2015-12-18T20:02:27Z" id="165885619">I pushed a new commit.
</comment><comment author="rjernst" created="2015-12-18T20:09:14Z" id="165887520">And one more commit. The access change was not intentional; changed back.
</comment><comment author="rmuir" created="2015-12-18T20:13:10Z" id="165888288">looks good. i will see if i can make forbidden behave with it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary license categories/matchers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15547</link><project id="" key="" /><description>I just re-used the lucene config in #15545, but we can clean up a bit.

We only really need 3 matchers for Apache License, and 1 matcher for Antlr generated code.

I kept the matcher explicitly putting BSD with advertising clause in a different category just to be safe.
</description><key id="123005581">15547</key><summary>Remove unnecessary license categories/matchers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-12-18T18:49:16Z</created><updated>2015-12-18T18:56:14Z</updated><resolved>2015-12-18T18:56:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-18T18:55:19Z" id="165870349">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make mappings tests more realistic.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15546</link><project id="" key="" /><description>DocumentMapperParser has both parse and parseCompressed methods. Except that the
parse methods are ONLY used from the unit tests. This commit removes the parse
method and moves all tests to parseCompressed so that they test more
realistically how mappings are managed.

Then I renamed parseCompressed to parse given that this is the only alternative
anyway.
</description><key id="122993766">15546</key><summary>Make mappings tests more realistic.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T17:37:54Z</created><updated>2015-12-21T09:47:15Z</updated><resolved>2015-12-21T09:47:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-18T21:55:30Z" id="165906161">Can we make the caller of parse do the decompression, so it just takes a String? There are only a handful of non test code places calling parse, and I think its ok to make the burden on them, rather than complicating every test?
</comment><comment author="jpountz" created="2015-12-18T22:05:18Z" id="165907785">Do you mind if we tackle it in a follow-up PR? Here I didn't want to change what happens in core to avoid controversy. But I agree this feels wrong. Like the fact that the type needs to be provided while it should also exist in the mapping source.
</comment><comment author="rjernst" created="2015-12-18T22:06:59Z" id="165908045">Well, it just means a lot of back and forth because this change makes all the tests created compressed strings, but then a follow up would just change them back? I'm fine either way though, if you want to do this first. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add gradle licenseHeaders to precommit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15545</link><project id="" key="" /><description>This is a port of the logic from apache lucene that uses Rat. I just brought in all the lucene matchers and category logic, we can refine it later.

Differences:
- adds ES license header to the apache 2.0 matchers
- only works on java sources (vs. also resources). 
- simpler output (vs dumping whole report), since we are in gradle...

There are problems with some source files, i did not fix them. Please help me vet these files so we can add the correct licenses to them. I don't think we should add `exclusions` or any of that capability when we can just have license headers on all of our source code.

```
:core:licenseHeaders

Unapproved licenses:

  /home/rmuir/workspace/elasticsearch/core/src/main/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandler.java
  /home/rmuir/workspace/elasticsearch/core/src/main/java/org/elasticsearch/http/netty/pipelining/OrderedDownstreamChannelEvent.java
  /home/rmuir/workspace/elasticsearch/core/src/main/java/org/elasticsearch/http/netty/pipelining/OrderedUpstreamMessageEvent.java
  /home/rmuir/workspace/elasticsearch/core/src/main/java/org/elasticsearch/search/profile/InternalProfileShardResults.java
  /home/rmuir/workspace/elasticsearch/core/src/test/java/org/elasticsearch/action/support/master/IndexingMasterFailoverIT.java
  /home/rmuir/workspace/elasticsearch/core/src/test/java/org/elasticsearch/cluster/routing/PrimaryAllocationIT.java
  /home/rmuir/workspace/elasticsearch/core/src/test/java/org/elasticsearch/cluster/routing/allocation/ActiveAllocationIdTests.java
```

```
:plugins:mapper-attachments:licenseHeaders

Unapproved licenses:

  /home/rmuir/workspace/elasticsearch/plugins/mapper-attachments/src/test/java/org/elasticsearch/mapper/attachments/TikaImplTests.java
```

```
:plugins:repository-hdfs:licenseHeaders

Unapproved licenses:

  /home/rmuir/workspace/elasticsearch/plugins/repository-hdfs/src/main/java/org/elasticsearch/plugin/hadoop/hdfs/Utils.java
  /home/rmuir/workspace/elasticsearch/plugins/repository-hdfs/src/test/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsRepositoryRestIT.java
  /home/rmuir/workspace/elasticsearch/plugins/repository-hdfs/src/test/java/org/elasticsearch/plugin/hadoop/hdfs/HdfsTestPlugin.java
  /home/rmuir/workspace/elasticsearch/plugins/repository-hdfs/src/test/java/org/elasticsearch/plugin/hadoop/hdfs/UtilsTests.java
```
</description><key id="122987083">15545</key><summary>add gradle licenseHeaders to precommit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label></labels><created>2015-12-18T16:58:58Z</created><updated>2015-12-18T18:27:43Z</updated><resolved>2015-12-18T18:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-18T17:28:28Z" id="165846940">Gradle work makes sense to me. I believe all the unapproves licenses in :core are just mistakes. I know the pipelining stuff came in via a PR from a community member but I believe they'd signed the CLA. The I only scanned the package names but they don't feel like they should have another license.
</comment><comment author="rmuir" created="2015-12-18T17:34:08Z" id="165848417">I just want to trace each file one by one and make sure they are fine. The mapper attachers one i might have added (my ide is in a perpetual state of disrepair), i will look into it.
</comment><comment author="polyfractal" created="2015-12-18T17:43:27Z" id="165850173">`InternalProfileShardResults` is mine, and indeed just a mistake.  I'll fix it in a few.
</comment><comment author="rmuir" created="2015-12-18T17:57:24Z" id="165856501">I found the easy solution for tracking these (https://chrome.google.com/webstore/detail/github-follow/agalokjhnhheienloigiaoohgmjdpned/). It lets you follow history across renames in the github UI, that was my problem in tracing these back...

mapper attachments one was introduced by me.
</comment><comment author="rmuir" created="2015-12-18T18:27:43Z" id="165864230">I traced back all of these files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin Upgrade Instructions in  Rolling Upgrade Doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15544</link><project id="" key="" /><description>Added plugin upgrade instructions per #15389
</description><key id="122985100">15544</key><summary>Plugin Upgrade Instructions in  Rolling Upgrade Doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tylerfontaine</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-12-18T16:49:27Z</created><updated>2016-04-07T08:43:23Z</updated><resolved>2016-04-07T08:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T17:30:40Z" id="206478188">@tylerfontaine I think this needs to be rebased on master to get rid of the conflicts, additionally, `bin/plugin` is now `bin/elasticsearch-plugin` I believe
</comment><comment author="clintongormley" created="2016-04-07T08:43:23Z" id="206762226">I added these steps in https://github.com/elastic/elasticsearch/commit/06604708d4bfc56eb397c6003567886bb1ce0114
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTML Strip Char Filter documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15543</link><project id="" key="" /><description>There are at least two configuration examples of using the HTML Strip Char Filter (char filter type `html_strip`) found in Elasticsearch documentation.
- [Analysis &gt;&gt; Analyzers &gt;&gt; Custom Analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html)
- [Analysis &gt;&gt; Token Filters &gt;&gt; Lowercase Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenfilter.html)

```
char_filter :
    my_html :
        type : html_strip
        escaped_tags : [xxx, yyy]
        read_ahead : 1024
```

[HTML Strip Char Filter documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-htmlstrip-charfilter.html) should indicate what `escaped_tags` and `read_ahead` parameters do. According to `org.apache.lucene.analysis.charfilter.HTMLStripCharFilter` documentation:

&gt; escapedTags - Tags in this set (both start and end tags) will not be filtered out.

For example, `escaped_tags` could be configured to ignore `a` and `title` tags:

```
escaped_tags : [a, title]
```

Incidentally, I am not seeing any support for the `read_ahead` parameter. How does it work?
</description><key id="122980104">15543</key><summary>HTML Strip Char Filter documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">inqueue</reporter><labels><label>:Analysis</label><label>adoptme</label><label>docs</label></labels><created>2015-12-18T16:27:27Z</created><updated>2016-02-29T19:30:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add non-negative option to derivative aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15542</link><project id="" key="" /><description>The derivative aggregation is great for getting the changes in a variable such as a packet counter. Some variables, however, wrap around or reset to zero (such as after a server reboot).

I would find it very useful to have non negative derivative option, where a negative change is regarded as a zero change.
</description><key id="122952775">15542</key><summary>Add non-negative option to derivative aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felixbarny</reporter><labels><label>:Aggregations</label><label>enhancement</label></labels><created>2015-12-18T13:55:29Z</created><updated>2016-01-11T13:47:01Z</updated><resolved>2016-01-11T13:47:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T20:39:34Z" id="170391866">@colings86 what do you think?
</comment><comment author="bleskes" created="2016-01-11T09:05:09Z" id="170475569">I wonder if we should treat a negative derivative as a non-value/null and let the gap policy decide what to do.

&gt; On 10 Jan 2016, at 21:39, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; @colings86 what do you think?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="colings86" created="2016-01-11T10:05:17Z" id="170496154">Personally I would like to keep the logic in the derivative simple and have it working out the derivative in the standard way (where the result can be positive or negative). This makes it very easy for users to know what the derivative aggregation does. For cases such as this we already have a couple of options which could be used to :
- Use the `bucket_selector` aggregation to only keep the buckets which have a non-negative value for the derivative
- Use the `bucket_script` aggregation to run a script on each bucket which outputs the value of the aggregation if the derivative value is non-negative or a default value (could be zero) for negative values
- Use the `bucket_script` aggregation to detect the wrap_around/reset and calculate the actual change in the counter. By this I mean, if you know your values go up to 1000 before they wrap around and you have a value of the derivative of -900 and a value for the counter value of 50 then you know the previous buckets value was 950 and the corrected derivative value (accounting for the wrap_around) should be 100.

IMO they are many things that you might want to do in this (and other use-cases) with the output of the derivative and it would be better to support the various possibilities using other aggregations to process the derivatives output then to try to support everything in the derivative aggregation
</comment><comment author="clintongormley" created="2016-01-11T13:47:01Z" id="170553287">@colings86 ++  makes sense, thanks.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix ConcurrentModificationException from nodes info and nodes stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15541</link><project id="" key="" /><description>A ConcurrentModificationException can arise on an Elasticsearch cluster
running OpenJDK 8 based JVMs from concurrent requests to the nodes info,
nodes stats and cat plugins endpoints. The issue can arise because
PlugsinInfo#getInfos currently performs a sort of its backing list. This
method is used in each of those endpoints and two concurrent requests
will cause this backing list to be sorted concurrently. Since sorting
the backing list modifies it, and concurrent modifications are bad, the
sort implementation in OpenJDK 8 added protection against concurrent
modifications.

This commit addresses this issue by removing the sort
from the Plugins#getInfos method, but still preserves the sorted result
from this method.

Closes #15537
</description><key id="122948969">15541</key><summary>Fix ConcurrentModificationException from nodes info and nodes stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label></labels><created>2015-12-18T13:28:38Z</created><updated>2015-12-18T13:46:59Z</updated><resolved>2015-12-18T13:46:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-18T13:30:57Z" id="165779241">During to refactoring that has occurred in master and backported to 2.x, this bug can not manifest there.
</comment><comment author="bleskes" created="2015-12-18T13:32:26Z" id="165779482">LGTM. good catch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Having a type and a field in a different type with the same name seems to break analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15540</link><project id="" key="" /><description>In tracking down a bug in an application, I found this behaviour in elasticsearch which is not what I would expect. 

It seems that adding a new mapping type somehow breaks the analyzers in another type. I see the behaviour in elasticsearch 1.4.4 which we are using and in 1.7.4.

Here is a script to to reproduce the behaviour:

```
ELASTICSEARCH=localhost:9200
INDEX=text-index

# Clear the index
curl -XDELETE $ELASTICSEARCH/$INDEX/
echo 

# Create and index
curl -XPUT -H "Content-Type: json" $ELASTICSEARCH/$INDEX/ -d '
    {
        "settings": {
            "index": {
                "number_of_shards": 1,
                "analysis": {
                    "filter": {
                        "stemmer": {
                            "type": "snowball",
                            "language": "English"
                        }
                    },
                    "analyzer": {
                        "custom": {
                            "type": "custom",
                            "tokenizer": "standard",
                            "filter": "stemmer"
                        }
                    }
                },
                "number_of_replicas": 0
            }
        },
        "mappings": {
            "user": {
                "dynamic": "strict",
                "properties": {
                    "address": {
                        "properties": {
                            "description": {
                                "type": "string",
                                "analyzer": "custom",
                                "store": "yes"
                            }
                        }
                    }
                }
            }

        }
    }
}'
echo 

# Index a document
curl -XPOST -H "Content-Type: json" $ELASTICSEARCH/$INDEX/user -d '
    {
        "address": 
            {
                "description": "Walked"
            }

    }'
echo 


sleep 1
# Search, should find something
curl -XPOST -H "Content-Type: json" $ELASTICSEARCH/$INDEX/user/_search -d '
{
    "query": {
        "filtered": {
            "query": {
                "simple_query_string": {
                    "default_operator": "AND",
                    "fields": [
                        "user.address.description"
                    ],
                    "query": "Walk"
                }
            }
        }
    }
}'
echo 


# Add a mapping called address, which happens to be the name of field a nested datatype
curl -XPUT -H "Content-Type: json" $ELASTICSEARCH/$INDEX/_mapping/address -d '
{ 

    "dynamic": "strict"
}'
echo


sleep 1
# Search, now returns no results!
curl -XPOST -H "Content-Type: json" $ELASTICSEARCH/$INDEX/user/_search -d '
{
    "query": {
        "filtered": {
            "query": {
                "simple_query_string": {
                    "default_operator": "AND",
                    "fields": [
                        "user.address.description"
                    ],
                    "query": "Walk"
                }
            }
        }
    }
}'
echo
```

Which produces this:

```
{"acknowledged":true}
{"acknowledged":true}
{"_index":"text-index","_type":"user","_id":"AVG0hasF-Z9PaJz0rP2W","_version":1,"created":true}
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"text-index","_type":"user","_id":"AVG0hasF-Z9PaJz0rP2W","_score":0.30685282,"_source":
    {
        "address": 
            {
                "description": "Walked"
            }

    }}]}}
{"acknowledged":true}
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

I would expect the last search request to return the same results as the first one:

```
{"acknowledged":true}
{"acknowledged":true}
{"_index":"text-index","_type":"user","_id":"AVG0hasF-Z9PaJz0rP2W","_version":1,"created":true}
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"text-index","_type":"user","_id":"AVG0hasF-Z9PaJz0rP2W","_score":0.30685282,"_source":
    {
        "address": 
            {
                "description": "Walked"
            }

    }}]}}
{"acknowledged":true}
{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"text-index","_type":"user","_id":"AVG0hasF-Z9PaJz0rP2W","_score":0.30685282,"_source":
    {
        "address": 
            {
                "description": "Walked"
            }

    }}]}}
```

I guess I can work around this by renaming some things, but I would like to know if I am doing something wrong in my script or if this is something that is a bit broken in elasticsearch.

Thanks
</description><key id="122946271">15540</key><summary>Having a type and a field in a different type with the same name seems to break analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tarnacious</reporter><labels /><created>2015-12-18T13:13:04Z</created><updated>2015-12-18T13:36:42Z</updated><resolved>2015-12-18T13:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-18T13:15:42Z" id="165774158">I think you are talking about this: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_mapping_changes.html#_conflicting_field_mappings

Which won't be possible anymore in 2.0.
</comment><comment author="tarnacious" created="2015-12-18T13:32:06Z" id="165779433">That certainly looks related, thanks for the link. It's not exactly that same as this is a conflict between a type and a field with the same name, but hopefully 2.0 also won't start on this condition and until we start using 2.0 I'll just be more careful about naming conflicts.

Thanks for looking into my issue!
</comment><comment author="dadoonet" created="2015-12-18T13:36:42Z" id="165780226">Ok thanks. I guess we can close your issue right? If not, feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make mapping updates more robust.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15539</link><project id="" key="" /><description>This changes a couple of things:

Mappings are truly immutable. Before, each field mapper stored a
MappedFieldTypeReference that was shared across fields that have the same name
across types. This means that a mapping update could have the side-effect of
changing the field type in other types when updateAllTypes is true. This works
differently now: after a mapping update, a new copy of the mappings is created
in such a way that fields across different types have the same MappedFieldType.
See the new Mapper.updateFieldType API which replaces MappedFieldTypeReference.

DocumentMapper is now immutable and MapperService.merge has been refactored in
such a way that if an exception is thrown while eg. lookup structures are being
updated, then the whole mapping update will be aborted. As a consequence,
FieldTypeLookup's checkCompatibility has been folded into copyAndAddAll.

Synchronization was simplified: given that mappings are truly immutable, we
don't need the read/write lock so that no documents can be parsed while a
mapping update is being processed. Document parsing is not performed under a
lock anymore, and mapping merging uses a simple synchronized block.
</description><key id="122943851">15539</key><summary>Make mapping updates more robust.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T12:53:41Z</created><updated>2015-12-23T08:56:03Z</updated><resolved>2015-12-23T08:55:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-22T16:49:12Z" id="166671265">I discussed with @jpountz and I agree now making mappings completely immutable is more important here. We should separately drive the simplification of types (ie type removal/turning them into just a simple set-of-fields-selector). +1 to this PR.
</comment><comment author="jpountz" created="2015-12-22T18:34:12Z" id="166698276">@rjernst I rebased with master and pushed a new commit to address the problem you found with index analyzers.
</comment><comment author="rjernst" created="2015-12-23T02:42:32Z" id="166791790">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>merge thread are blocked each other</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15538</link><project id="" key="" /><description>my server runs very slow and I did a thread dump and found out that many lucene merge threads are blocked.
my question is:
1. why are there so many merge threds?
2. why are thy blocked each other?
3. is this a ES 2.1.0 bug?

Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.60-b23 mixed mode):

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655955]" #668132 daemon prio=5 os_prio=0 tid=0x00007f24880dd800 nid=0x17978 waiting for monitor entry [0x00007f221c9b6000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.beforeMerge(InternalEngine.java:1200)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:93)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655954]" #668131 daemon prio=5 os_prio=0 tid=0x00007f236912a000 nid=0x17977 waiting for monitor entry [0x00007f21fb618000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655953]" #668130 daemon prio=5 os_prio=0 tid=0x00007f2368d59800 nid=0x17976 waiting for monitor entry [0x00007f22051e7000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655952]" #668129 daemon prio=5 os_prio=0 tid=0x00007f2368d59000 nid=0x17975 waiting for monitor entry [0x00007f221e161000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655951]" #668128 daemon prio=5 os_prio=0 tid=0x00007f236913e800 nid=0x17974 waiting for monitor entry [0x00007f22515ce000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4237)
    - waiting to lock &lt;0x0000000125da7ac0&gt; (a org.apache.lucene.index.IndexWriter)
    at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)
    at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655950]" #668127 daemon prio=5 os_prio=0 tid=0x00007f243408c800 nid=0x17973 waiting for monitor entry [0x00007f2251a83000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.beforeMerge(InternalEngine.java:1200)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:93)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655949]" #668126 daemon prio=5 os_prio=0 tid=0x00007f253c0a1000 nid=0x17972 waiting for monitor entry [0x00007f21bf650000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655948]" #668125 daemon prio=5 os_prio=0 tid=0x00007f2368d5f000 nid=0x17971 waiting for monitor entry [0x00007f21bf04a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655947]" #668124 daemon prio=5 os_prio=0 tid=0x00007f2369156000 nid=0x17970 waiting for monitor entry [0x00007f22052e8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

"elasticsearch[node1][[jsf-monitor-data-performance-2015-12-18][9]: Lucene Merge Thread #655946]" #668123 daemon prio=5 os_prio=0 tid=0x00007f2369144800 nid=0x1796f waiting for monitor entry [0x00007f221dd5d000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler.afterMerge(InternalEngine.java:1212)
    - waiting to lock &lt;0x000000019b76d350&gt; (a org.elasticsearch.index.engine.InternalEngine$EngineMergeScheduler)
    at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:99)
    at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)
</description><key id="122924621">15538</key><summary>merge thread are blocked each other</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-12-18T10:38:37Z</created><updated>2016-01-10T19:55:40Z</updated><resolved>2016-01-10T19:55:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-18T14:59:31Z" id="165798147">Can you include all threads in the thread dump?

Somehow your shard is scheduled a truly insane number of merges (655K+) over time ... have you changed any settings from defaults?  How are you using ES, e.g. are you asking ES to refresh or flush frequently?
</comment><comment author="makeyang" created="2015-12-21T03:27:21Z" id="166188487">@mikemccand  sorry for late replay. it was the weekend last 2 days and I happened can't touch internet.
the full thread dump is below link:
https://github.com/makeyang/grocery/blob/master/td.txt
Yes, I changed some settings from default:
index.translog.sync_interval: 30s
index.translog.durability: async
indices.memory.index_buffer_size: 40%
</comment><comment author="mikemccand" created="2015-12-21T19:35:32Z" id="166398703">I see way too many bulk indexing threads (stalled, because merging can't keep up).

Did you increase your bulk thread pool size to &gt;= 200?
</comment><comment author="makeyang" created="2015-12-22T02:04:15Z" id="166478716">@mikemccand 
yes, bulk thread pool size to = 200

our situation is below:
many client try to write to ES and concurrency is high, so I try to handle this high concurrency  and modified the thread pool size and queue size.
</comment><comment author="clintongormley" created="2016-01-10T19:55:40Z" id="170385319">@makeyang don't increase the number of bulk threads like that - you're overloading the system on one side and paying for it somewhere else.  You won't be able to set such high values going forward. See https://github.com/elastic/elasticsearch/pull/15585

If your hardware isn't keeping up with your requirements, you need more hardware
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES2.1.0 keep throw NotSerializableExceptionWrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15537</link><project id="" key="" /><description>same ES2.1.0 server and client. 

[DEBUG][action.admin.cluster.node.info] [node1] failed to execute on node [yQYHZ3hxTWOwrQPq8w4rrQ]
RemoteTransportException[[node2][node2:9305][cluster:monitor/nodes/info[n]]]; nested: NotSerializableExceptionWrapper;
Caused by: NotSerializableExceptionWrapper[null]
at java.util.ArrayList.sort(ArrayList.java:1456)
at java.util.Collections.sort(Collections.java:175)
at org.elasticsearch.action.admin.cluster.node.info.PluginsInfo.getInfos(PluginsInfo.java:55)
at org.elasticsearch.action.admin.cluster.node.info.PluginsInfo.writeTo(PluginsInfo.java:86)
at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:284)
at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:97)
at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:75)
at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
</description><key id="122919225">15537</key><summary>ES2.1.0 keep throw NotSerializableExceptionWrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">makeyang</reporter><labels><label>bug</label><label>v2.0.2</label><label>v2.1.1</label></labels><created>2015-12-18T10:06:39Z</created><updated>2016-09-09T14:23:58Z</updated><resolved>2015-12-29T01:13:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2015-12-18T10:31:55Z" id="165742565">no, the cluster noeds are all 2.1.0
</comment><comment author="jasontedor" created="2015-12-18T11:25:14Z" id="165752472">The exception that is being thrown here is very likely a `ConcurrentModificationException` which means something rather nefarious is going on. What plugins do you have installed? Can you provide the steps that led to you seeing this so that we have a reproduction?
</comment><comment author="jasontedor" created="2015-12-18T12:31:14Z" id="165767772">I can reproduce this locally. This can be caused by concurrent requests to any of the nodes info, cluster stats and the cat plugins APIs. I will open a pull request to address shortly.
</comment><comment author="robertsmarty" created="2015-12-29T00:48:17Z" id="167687052">I'm also receiving these errors intermittently. I left my 5 node 2.1.0 cluster running over the Christmas break collecting about 120k events per minute from firewalls. Have the following plugins installed...

```
- license
- marvel-agent
- cloud-aws
- shield
- watcher
```

Receiving the following error on all cluster nodes...

[2015-12-29 07:26:26,735][DEBUG][action.admin.cluster.node.info] [elkrp11] failed to execute on node [m_uC_oh1TAmasv2k1n3zJg]
RemoteTransportException[[elkrp7][10.10.60.121:9300][cluster:monitor/nodes/info[n]]]; nested: NotSerializableExceptionWrapper;
Caused by: NotSerializableExceptionWrapper[null]
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:901)
        at java.util.ArrayList$Itr.next(ArrayList.java:851)
        at org.elasticsearch.action.admin.cluster.node.info.PluginsInfo.writeTo(PluginsInfo.java:86)
        at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:284)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:97)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:75)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
        at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
        at org.elasticsearch.shield.transport.ShieldServerTransportService$ProfileSecuredRequestHandler.messageReceived(ShieldServerTransportService.java:165)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</comment><comment author="jasontedor" created="2015-12-29T01:13:17Z" id="167689452">Closed in the 2.1 branch by 0cd08afd53b90523ed9cb052869653bc2184cf25 and in the 2.0 branch by 916218d1f87ea95785e659f514fae5f9a524005d.
</comment><comment author="robertsmarty" created="2015-12-29T05:10:22Z" id="167719826">I'm running a 2.1.1 cluster and this issue still persists
</comment><comment author="jasontedor" created="2015-12-29T12:20:01Z" id="167778020">&gt; I'm running a 2.1.1 cluster and this issue still persists

The fix for this was put into the 2.1 branch for inclusion in an eventual 2.1.2 patch release by commit 0cd08afd53b90523ed9cb052869653bc2184cf25, and the into the 2.0 branch for inclusion in a potential (but not guaranteed) 2.0.3 release by commit 916218d1f87ea95785e659f514fae5f9a524005d. The version labels on #15541 are v2.0.3 and v2.1.2 showing the potential releases that will contain the fix.
</comment><comment author="oleg-andreyev" created="2016-09-09T13:40:24Z" id="245915777">Issue seems still persists on v2.4.0:

```
Caused by: NotSerializableExceptionWrapper[too_complex_to_determinize_exception: Determinizing automaton with 77 states and 317 transitions would result in more than 10000 states.]
        at org.apache.lucene.util.automaton.Operations.determinize(Operations.java:743)
        at org.apache.lucene.util.automaton.MinimizationOperations.minimize(MinimizationOperations.java:58)
        at org.apache.lucene.util.automaton.RegExp.toAutomatonInternal(RegExp.java:515)
        at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:495)
        at org.apache.lucene.util.automaton.RegExp.toAutomaton(RegExp.java:426)
        at org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude.toAutomaton(IncludeExclude.java:384)
        at org.elasticsearch.search.aggregations.bucket.terms.support.IncludeExclude.convertToOrdinalsFilter(IncludeExclude.java:408)
        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory$ExecutionMode$2.create(TermsAggregatorFactory.java:71)
        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.doCreateInternal(TermsAggregatorFactory.java:246)
        at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.createInternal(ValuesSourceAggregatorFactory.java:64)
        at org.elasticsearch.search.aggregations.AggregatorFactory.create(AggregatorFactory.java:102)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:87)
        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:85)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:111)
        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:372)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:480)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
        at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:77)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:293)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)

```

```
{
"name": "opcart-es-data-1",
"cluster_name": "opcart-prod-v2",
"version": {
"number": "2.4.0",
"build_hash": "ce9f0c7394dee074091dd1bc4e9469251181fc55",
"build_timestamp": "2016-08-29T09:14:17Z",
"build_snapshot": false,
"lucene_version": "5.5.2"
},
"tagline": "You Know, for Search"
}
```
</comment><comment author="jasontedor" created="2016-09-09T14:23:46Z" id="245927766">@oleg-andreyev This is a completely different issue. You're hitting a `TooComplexToDeterminizeException` inside Lucene, which we do not serialize, hence the wrapper, hence the message:

&gt; `too_complex_to_determinize_exception: Determinizing automaton with 77 states and 317 transitions would result in more than 10000 states.`

It looks like you have a regular expression that is too complex? I suggest opening a discuss thread on the [Elastic Discourse forums](https://discuss.elastic.co) if you have additional questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support pull-based bulk processing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15536</link><project id="" key="" /><description>### Terminology

The term "client" refers to any code using the `BulkProcessor`, not the Elasticsearch Java client.
### Rationale

This originates from a discussion in #15125. Currently, `BulkProcessor` provides a push-based API, i.e. clients actively feed it individual requests (e.g. index, delete, ..). `BulkProcessor` buffers them and executes a bulk requests synchronously when a certain threshold is reached (blocking the client thread).

For some use cases such as reindexing (see #15125) we'd like to have a pull-based API, i.e. the bulk processor implementation requests items from the client instead of the client feeding the bulk processor. Advantages of this approach:
- Bulk processing can be completely asynchronous, there is no more blocking in the client thread
- Backpressure is still applied, so there is no need to buffer lots of data.
### API sketch

As pull-based bulk processing requires a completely different API than the push-based model it makes no sense to force both of them into the same class. Therefore, there will be a new class which we'll assign the preliminary name `AsyncBulkProcessor` for the sake of this discussion.

If we take the [Reactive Streams API](http://www.reactive-streams.org/) as guidance for the API, we can sketch the following API (note that this is a bit simplified):

Let's start with the client which wants to provide data to the `AsyncBulkProcessor`. Clients are called publishers, the bulk processor is called a subscriber in this terminology.

Clients implement the interface `Publisher`:

``` java
interface Publisher&lt;ActionRequest&gt; {
  void subscribe(Subscriber&lt;? super ActionRequest&gt; s);
}
```

In `#subscribe()` we just create a `Subscription` which does the actual work and pass it to to the subscriber. The subscription would need to be implemented as follows:

``` java
class SampleSubscription {
  // this is the AsyncBulkProcessor
  private final Subscriber&lt;ActionRequest&gt; subscriber;

  public SampleSubscription(Subscriber&lt;ActionRequest&gt; subscriber) {
    this.subscriber = subscriber;
  }

  public void request(long numberOfItems) {
    // processor has requested more items, now hand them over
    if (hasMoreItems(numberOfItems)) {
      for (ActionRequest item : requestItems(numberOfItems)) {
        subscriber.onNext(item);
      }
    } else {
      subscriber.onComplete();
    }
  }
}

```

Finally, `AsyncBulkProcessor` implements the `Subscriber` interface:

``` java
class AsyncBulkProcessor implements Subscriber&lt;ActionRequest&gt; {
  private Subscription subscription;

  public void onSubscribe(Subscription subscription) {
    // a Subscription "connects" the processor with the client
    this.subscription = subscription;
  }

  // called by the client after we have requested more items
  public void onNext(ActionRequest request) {
    // Note: All of this does NOT happen on the caller thread but on a dedicated thread

    // (1) add to internal buffer and create a new bulk request if threshold is reached.

    // (2) Request up to bulkSize more items from the client after we're 
    // done with a bulk request
    subscription.request(bulkSize);
  }

  public void onComplete() {
    // there are no more data, flush internal buffers and probably
    // issue one final bulk request
  }
}
```

Note: This does not mean that we have to implement the reactive streams API. It just serves as an example of how a pull-based API might look like.

An issue we also need to address is error handling. As everything is asynchronous we have to use some kind of backchannel. We can use the same mechanisms as for regular items, the bulk processor would then be a publisher of errors and some other component (a client or a dedicated error handler) would be the subscriber.
</description><key id="122912208">15536</key><summary>Support pull-based bulk processing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Bulk</label><label>:Java API</label><label>enhancement</label><label>stalled</label></labels><created>2015-12-18T09:29:58Z</created><updated>2016-10-05T10:05:01Z</updated><resolved>2016-10-05T10:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-18T09:35:05Z" id="165726581">When I look at this API I wonder why we can't just pass in an Iterator to the bulk processor that the bulk processor can pull from. It seems to me that this would be much simpler that a callback based API and we can hide the logic in a single place? But I might miss something?
</comment><comment author="danielmitterdorfer" created="2015-12-18T09:51:49Z" id="165733366">You're right. These are related concepts. The reactive streams API just offers more flexibility (but admittedly at the cost of more complexity). One thing that we need to consider is whether the bulk processor is "reused", e.g. consider this API:

``` java
class AsyncBulkProcessor {
  void sendItems(Iterator&lt;ActionRequest&gt; items) (
    // ...
  }
}
```

As we perform this operation asynchronously, I think the client needs some kind of handle to this operation (similar to a Future) in order to e.g. cancel it and also to have some kind of "context" if it calls `#sendItems()` multiple times. This is - to some extent - the role of `Subscription` in the API sketch above (`Subscription` really has two roles, providing the items and also being a handle for the subscriber).

So I sense we might end up with a similar API anyway, that's why I have chosen the reactive streams API as an example for the sketch. But nothing is stopping us from starting small...

Btw, one benefit of the callback-based subscription model is that also the provider can be asynchronous (which can obviously not work with an iterator).
</comment><comment author="danielmitterdorfer" created="2015-12-28T08:51:25Z" id="167516560">To get a better idea for a suitable API sketch, I'll first review #15125 (reindex API) which could benefit from it.
</comment><comment author="danielmitterdorfer" created="2016-10-05T10:04:56Z" id="251634117">No time to work on this for the foreseeable future and interest seems also limited. Therefore I close it for the time being. Feel free to reopen if necessary.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for tragic event on all kinds of exceptions not only ACE and IOException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15535</link><project id="" key="" /><description>It's important to close not matter what exception caused a tragic event. Today
we only check on IOException and AlreadyClosedExceptions. The test had a bug and
threw an IAE instead causing the translog not to be closed.
</description><key id="122910311">15535</key><summary>Check for tragic event on all kinds of exceptions not only ACE and IOException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>v2.0.2</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T09:17:42Z</created><updated>2015-12-18T15:00:27Z</updated><resolved>2015-12-18T09:53:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-18T09:45:46Z" id="165732009">LGTM
</comment><comment author="mikemccand" created="2015-12-18T10:02:12Z" id="165735732">LGTM, good catch: since `TranslogWriter.add` will declare tragedy on any `Throwable` coming when writing to the channel, all callers of this must do the same.

Maybe in `TranslogWriter.ensureOpen` we could assert `tragedy != null`, to ensure nobody every tries to continue adding to an open translog that experienced tragedy?
</comment><comment author="s1monw" created="2015-12-18T10:23:36Z" id="165740796">&gt; Maybe in TranslogWriter.ensureOpen we could assert tragedy != null, to ensure nobody every tries to continue adding to an open translog that experienced tragedy?

this becomes very tricky, we close the writer on a tragedy but the `Translog#closeOnTragicEvent(ex)` is executed in a catch block (outside of the lock) that can slip in a concurrent env. Anyway if that happens we get a ACE from `TranslogWriter#ensureOpen` since that is closed under lock in the case of a tragedy
</comment><comment author="mikemccand" created="2015-12-18T15:00:27Z" id="165798334">&gt; this becomes very tricky, 

Ahh OK nevermind!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor typos in inner hits document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15534</link><project id="" key="" /><description /><key id="122907156">15534</key><summary>Fix minor typos in inner hits document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmluy</reporter><labels><label>docs</label></labels><created>2015-12-18T08:54:43Z</created><updated>2016-01-10T20:52:44Z</updated><resolved>2016-01-10T20:51:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T20:52:44Z" id="170393062">thanks @jmluy 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not getting All data in case of multiple types. Getting data for one specific type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15533</link><project id="" key="" /><description>Hi. I have a problem with multiple types.
If I specify multiple type with comma, It's getting data from just one specific type.

Let's assume that we have an index which name is "account_test"
"account" has 3 types in it which are "Campaign", "Advertiser", "Agency"

"Campaign" has one data as below

```
{
    "_index": "account_test",
    "_type": "Campaign",
    "_id": "102399",
    "_score": 1,
    "_source": {
    "ID": "CM_12022015",
    "NAME": "CM_12022015",
    "Advertiser": {
        "ID": "Adv_12022015",
        "NAME": "unknown_advertiser"
    }
}
```

"Advertiser" has one data as below

```
{
    "_index": "account_test",
    "_type": "Advertiser",
    "_id": "52957",
    "_score": 1,
    "_source": {
        "ID": "Adv_12022015",
        "NAME": "unknown_advertiser",
    }
}
```

Now I want to find these with the keyword "Adv_12022015".
- **No type specified in URL**

```
http://eshost:9200/account_test/_search?q=Campaign.Advertiser.ID:*adv_12022015* OR Advertiser.ID:*adv_12022015*
// Total : 2
```

It is getting both 2 data correctly.
However If I specify types in it, It only gets "Campaign" data.
- **Single type**

```
http://eshost:9200/account_test/Campaign/_search?q=Campaign.Advertiser.ID:*adv_12022015* OR Advertiser.ID:*adv_12022015*
// Total : 1
```

and

```
http://eshost:9200/account_test/Advertiser/_search?q=Campaign.Advertiser.ID:*adv_12022015* OR Advertiser.ID:*adv_12022015*
// Total : 1
```

It works correctly. Both query gets data from each specified type.
- **Multiple types with comma**

```
http://eshost:9200/account_test/Advertiser,Campaign/_search?q=Campaign.Advertiser.ID:*adv_12022015* OR Advertiser.ID:*adv_12022015*
// Total : 1
```

However If I specify multiple types with comma in url, it gets only data from "Campaign".

Single type is fine, It is only a problem when I use multiple type with comma.
I think it is similar to the issue [#2218](https://github.com/elastic/elasticsearch/issues/2218)

Please check it out.
Thank you.

Installed Elastic Search Info
{
  "status": 200,
  "name": "node1",
  "cluster_name": "es.oas.cluster",
  "version": {
    "number": "1.4.2",
    "build_hash": "927caff6f05403e936c20bf4529f144f0c89fd8c",
    "build_timestamp": "2014-12-16T14:11:12Z",
    "build_snapshot": false,
    "lucene_version": "4.10.2"
  },
  "tagline": "You Know, for Search"
}
</description><key id="122905318">15533</key><summary>Not getting All data in case of multiple types. Getting data for one specific type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cheonsoo</reporter><labels /><created>2015-12-18T08:38:43Z</created><updated>2016-01-10T20:50:48Z</updated><resolved>2016-01-10T20:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T20:50:48Z" id="170392960">Hi @cheonsoo 

The problem is that you have object fields that are the same as your type names, which is producing ambiguity.  2.0 had a major mapping rewrite to fix these kinds of issues (see #8870)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement proper handoff between primary copies during relocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15532</link><project id="" key="" /><description>This implements a clean handoff between shard copies during primary relocation. 
After replaying the translog to target copy during relocation, the source copy is 
marked as `relocated`. Further writes are blocked on `relocated` shard copies 
and are retried until relocation completes (waits for the cluster state to point to 
the new copy). 
The recovery process blocks until all pending writes complete on the source copy. 
In case of failure/cancellation of recoveries after the source copy has been marked 
as `relocated`, the source state is marked back to `started` and resumes to accept
writes.

relates #8706
</description><key id="122904171">15532</key><summary>Implement proper handoff between primary copies during relocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>review</label></labels><created>2015-12-18T08:28:58Z</created><updated>2016-01-12T10:32:14Z</updated><resolved>2016-01-12T10:32:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-01-02T20:47:51Z" id="168427098">@bleskes I have simplified this PR based on our discussion. Updated the description with the implemented approach. 
</comment><comment author="clintongormley" created="2016-01-12T10:32:09Z" id="170868514">Closed in favour of #15900
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>what's the meaning for 'pretty' in {curl -XPUT 'localhost:9200/customer/external/2?pretty' -d ' {   "name": "Jane Doe" }'}</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15531</link><project id="" key="" /><description>hi,I am learning elasticsearch,i don't understand what's the meaning for 'pretty',can someone tell me?thanks,the curl cmd is :
curl -XPUT 'localhost:9200/customer/external/2?pretty' -d ' {   "name": "Jane Doe" }'
</description><key id="122902649">15531</key><summary>what's the meaning for 'pretty' in {curl -XPUT 'localhost:9200/customer/external/2?pretty' -d ' {   "name": "Jane Doe" }'}</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">selfchanger</reporter><labels /><created>2015-12-18T08:16:01Z</created><updated>2015-12-18T08:47:02Z</updated><resolved>2015-12-18T08:42:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-18T08:42:27Z" id="165715131">Please ask your questions on discuss.elastic.co.

And read https://www.elastic.co/guide/en/elasticsearch/reference/current/common-options.html#_pretty_results?q=pretty
</comment><comment author="selfchanger" created="2015-12-18T08:47:02Z" id="165715947">I am sorry,i am new,thank you very much
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide item-based listener API in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15530</link><project id="" key="" /><description>This ticket originates from a discussion in #14829. Currently, `BulkProcessor` provides a bulk-based listener API. A listener gets notified before a bulk request is issued and after a response came back. But clients should not need to care about the internal handling of bulk requests and can just work on individual items. Hence we want to introduce a new listener interface `ItemListener` sketched below:

``` java
interface ItemListener {
    void beforeExecute(ActionRequest request);
    void afterExecute(ActionRequest request, ActionWriteResponse response);
    void afterExecute(ActionRequest request, Throwable failure);
}
```

This interface is simpler: There is no need to know about bulk requests and how bulk processor handles them internally. Also request / response correlation is straightforward (the current API uses an execution id for that). Consequently, in case a bulk request fails completely, the callback method will be invoked for each `ActionRequest`.

Bulk-based listeners are considered to be the exception rather than the rule. Bulk-based listening will still be supported but the idea is to subclass `BulkProcessor` and implement the callback methods in a subclass (i.e. the base class `BulkProcessor` should implement a template method).
</description><key id="122901135">15530</key><summary>Provide item-based listener API in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-18T08:02:51Z</created><updated>2015-12-18T11:59:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Snapshot repository disappears during upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15529</link><project id="" key="" /><description>On my home/test cluster with one node and snapshot to local filesystem, the backup repository definition disappeared when upgrading from 2.0.0 -&gt; 2.1.0, and again when upgrading from 2.1.0 -&gt; 2.1.1

The snapshot is created like this:

```
curl -XPUT 'http://rcl-nas:9200/_snapshot/backup' -d '{
    "type": "fs",
    "settings":{
        "compression": true,
        "location": "/mnt/raid/elasticsearch-snapshots"
    }
}'
```

No errors referencing repositories are in the logs.

I was able to re-create the repository again with exactly the same configuration, and the previous snapshots still exist.

I have tried to reproduce this on other test cluster, but have not been able to provoke it...
</description><key id="122898976">15529</key><summary>Snapshot repository disappears during upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels /><created>2015-12-18T07:45:12Z</created><updated>2016-01-17T15:46:20Z</updated><resolved>2015-12-22T16:08:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-18T13:41:12Z" id="165780897">I cannot reproduce this (repo is preserved).

Can you share full configs (elasticsearch.yml)?

What does your upgrade process look like?
</comment><comment author="robin13" created="2015-12-18T15:13:38Z" id="165801016">I haven't been able to reproduce either, other than on the one cluster.  I have heard of others with the same experience though.
Upgrade process:
- update apt repository settings, apt-get update, apt-get upgrade
- stop elasticsearch
- uninstall and re-install plugins (watcher, license)
- start elasticsearch

Config: mostly defaults accept for memory and number of shards/replicas.
</comment><comment author="imotov" created="2015-12-18T17:19:19Z" id="165845048">@robin13 did you have any templates or licenses registered in cluster? If yes, have they survived the upgrade in which repository definition disappeared? You can verify this by looking at the log file and checking if the new trail license was generated. 
</comment><comment author="robin13" created="2015-12-21T09:06:51Z" id="166241667">Yes - templates (logstash and beats), and license (using watcher).  License survived the upgrade.
</comment><comment author="robin13" created="2015-12-21T10:46:31Z" id="166268554">Found root cause / method to reproduce:
If disk is spun down (`hdparm -Y /dev/sdX`) and Elasticsearch is restarted, the repository will be (silently - no entry in the log) removed.
</comment><comment author="robin13" created="2015-12-21T15:59:25Z" id="166340625">To elaborate ^^ : The disk being spun down is the disk of the repository, not elasticsearch data.  The repository definition is removed, but the data remains (and is usable when the repository is re-created).
</comment><comment author="ywelsch" created="2015-12-21T16:03:31Z" id="166341964">I think I found the issue. Seems to be the caused by the custom metadata parsing of the license plugin (still need to investigate). My observations so far:
- repositories were sometimes removed after restart of cluster
- the issue occured only when repository metadata was written after license metadata. As the order of writing custom metadata is undefined, this led to the bug occurring only sometimes (on my machine very rarely, especially NOT when I tried to reproduce it 3 days ago).

I'll have a closer look tomorrow at this.
</comment><comment author="ywelsch" created="2015-12-22T16:08:24Z" id="166661333">Issue fixed in plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sharing the searched log with a person outside organization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15528</link><project id="" key="" /><description>What facility elastic gives if we need to share a section of log(or a searched log) to a person who does not have any of elastic product or does not have access to it?

A common usecase - sharing the log with a product support team for a PMR.

I did some thought over this on how to enable this, but would like to hear if there is an inbuilt/available way to do this.
</description><key id="122897698">15528</key><summary>Sharing the searched log with a person outside organization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prraina1</reporter><labels /><created>2015-12-18T07:34:38Z</created><updated>2016-01-10T20:45:26Z</updated><resolved>2016-01-10T20:45:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T20:45:26Z" id="170392224">I don't know if you are referring to the application logs from elasticsearch, or log documents indexed in elasticsearch.  If the latter, then maybe you want to use Kibana for this.  Application logs are just text files, so share them as you would any other text files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Align handling of interrupts in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15527</link><project id="" key="" /><description>With this commit we implement a cancellation policy in
BulkProcessor which is aligned for the sync and the async case
and also document it.

Closes #14833.
</description><key id="122895879">15527</key><summary>Align handling of interrupts in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T07:21:38Z</created><updated>2016-01-18T07:46:49Z</updated><resolved>2015-12-18T18:35:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-12-18T18:35:44Z" id="165865838">Thanks! Merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document usage of backoff policy in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15526</link><project id="" key="" /><description>With this commit we update the documentation to explain the
new backoff feature in BulkProcessor.

Relates to #14620.
</description><key id="122891925">15526</key><summary>Document usage of backoff policy in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>docs</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T06:54:08Z</created><updated>2016-01-18T07:46:49Z</updated><resolved>2015-12-18T18:41:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-18T16:21:05Z" id="165823529">LGTM! I didn't realize this was documented in the asciidoc!
</comment><comment author="danielmitterdorfer" created="2015-12-18T18:42:01Z" id="165867609">Thanks! Merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc() array size error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15525</link><project id="" key="" /><description>Hello

I tried to index an double array {1, 2, 3, 4, 5} and let ES automatically detect the type. Then I wrote a plugin to get the value as follows

ScriptDocValues docHashValues = (ScriptDocValues) doc().get("array");
final List&lt;Double&gt; tmp = ((ScriptDocValues.Doubles) docHashValues).getValues();

However I found the tmp size is always &lt;= 4. Is this a bug or there are some limitations for array size in ES? Thanks!
</description><key id="122882067">15525</key><summary>doc() array size error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YunchaoGongSC</reporter><labels /><created>2015-12-18T05:29:51Z</created><updated>2015-12-18T13:07:54Z</updated><resolved>2015-12-18T13:07:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="YunchaoGongSC" created="2015-12-18T06:25:46Z" id="165687831">The data is indexed as 

int[] tmp = {1, 3, 5, 7, 5};
XContentBuilder doc = XContentFactory.jsonBuilder().startObject()
                   .field("feature", tmp)
.endObject();

And in head, checking the content, I do find 1 3 5 7 5. But in the code, I can only get first 4 values.
</comment><comment author="YunchaoGongSC" created="2015-12-18T06:36:42Z" id="165689780">To be more specific, I found in the source() method, it did contain the right number of values in each array, but in doc(), it only contains &lt;= 4 values.
</comment><comment author="jpountz" created="2015-12-18T13:07:54Z" id="165773171">This is expected if your field does not have doc values enabled as fielddata does not retain duplicates. To fix this problem you would have to reindex your data in an index that has doc values enabled on this field (default on and after 2.0).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In elasticsearch , How to modify mapping, and  influence the existing data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15524</link><project id="" key="" /><description>Now,  I say the method I use now.
1. delete elasticsearch' type , then delete all data
2. put  my mapping, this mapping config is my own definition.
3. push data again .

Can you better idea?  or  elasticsearch  support dynamic to modify mapping and existing data online?

Waiting for your answer.  Thanks You  ! 
</description><key id="122873322">15524</key><summary>In elasticsearch , How to modify mapping, and  influence the existing data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rfyiamcool</reporter><labels /><created>2015-12-18T03:45:55Z</created><updated>2015-12-18T06:02:05Z</updated><resolved>2015-12-18T06:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-18T06:02:05Z" id="165680959">Please join us on discuss.elastic.co.

We can help you there.

Not an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>install head plugin error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15523</link><project id="" key="" /><description>[root@localhost bin]# ./plugin -install mobz/elasticsearch-head -v
-&gt; Installing mobz/elasticsearch-head...
Trying https://github.com/mobz/elasticsearch-head/archive/master.zip...
Failed: SSLException[java.security.ProviderException: java.security.KeyException]; nested: ProviderException[java.security.KeyException]; nested: KeyException; 
Failed to install mobz/elasticsearch-head, reason: failed to download out of all possible locations..., use --verbose to get detailed information
</description><key id="122868965">15523</key><summary>install head plugin error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kys1230</reporter><labels /><created>2015-12-18T02:58:02Z</created><updated>2015-12-29T19:42:39Z</updated><resolved>2015-12-29T19:42:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-12-29T19:42:39Z" id="167862293">I recommend you open this up for discussion over at https://discuss.elastic.co/ .  I just successfully tested installing the head plugin myself and I suspect you have an issue specific to your system.  I'm going to close this here, but if a discussion there reveals a real bug, feel free to open an issue back here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move test framework under a "test" top level dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15522</link><project id="" key="" /><description>This allows adding more test projects, eg integ test fixtures that will
be coming soon.
</description><key id="122855419">15522</key><summary>Move test framework under a "test" top level dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-18T01:00:21Z</created><updated>2015-12-18T15:50:15Z</updated><resolved>2015-12-18T05:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-18T02:45:22Z" id="165653952">LGTM
</comment><comment author="dadoonet" created="2015-12-18T09:23:05Z" id="165722137">@rjernst Does it mean that this test framework is now published as an artifact so anyone (even if using maven) would be able to use it?
</comment><comment author="s1monw" created="2015-12-18T09:26:18Z" id="165722728">I think so! @dadoonet see https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/test-framework/3.0.0-SNAPSHOT/
</comment><comment author="dadoonet" created="2015-12-18T09:27:20Z" id="165723115">Fantastic! I missed it! Thanks @s1monw 
</comment><comment author="rjernst" created="2015-12-18T15:50:15Z" id="165812395">@dadoonet That was already the case. With this change the coordinates are slightly different:
https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/test/framework/3.0.0-SNAPSHOT/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minimum_should_match ignored when number of `should` clauses smaller </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15521</link><project id="" key="" /><description>For programmatically-generated queries, you sometimes want to apply min-match constraints on your bool should clauses. Is there thinking behind why minimum_should_match is ignored when the number of should clauses is lesser than it?

To be clear, I'm looking for the behavior where the following query returns 0 results:

```
{
    "query": {
        "bool": {
            "should": [{
                "match": {
                    "_all": "elizabeth"
                }
            }, {
                "match": {
                    "_all": "smith"
                }
            }],
            "minimum_should_match": 3
        }
    }
}
```

We're using a query-understanding pipeline which should get 0 results from one of the programmatically generated queries if there isn't enough support in the query to make the match. However, that is not currently happening right now.

Any way I can resolve this in the current set up? Or would this be a feature/bug request?
</description><key id="122825752">15521</key><summary>minimum_should_match ignored when number of `should` clauses smaller </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">navneet-lyra</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2015-12-17T21:22:28Z</created><updated>2015-12-21T15:06:51Z</updated><resolved>2015-12-21T15:06:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-18T13:23:44Z" id="165777211">I agree this is a bug and this query should return no hits. The issue seems to stem from Queries.calculateMinShouldMatch which returns the number of optional clauses in case minimum_should_match gets higher.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/etc/init.d/elasticsearch stop sleeps for 86400 seconds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15520</link><project id="" key="" /><description>elasticsearch wasn't restarting correctly. I investigated and found:

```
# ps auxwwwf
[...]
root      3034  0.0  0.0 106364  1652 pts/4    S+   11:43   0:00      \_ /bin/sh /etc/init.d/elasticsearch stop
root      3049  0.0  0.0 100908   568 pts/4    S+   11:43   0:00          \_ sleep 86400
```

Line 129 of /etc/init.d/elasticsearch appears to be the culprit.

This machine is running RHEL 6.4.
</description><key id="122809781">15520</key><summary>/etc/init.d/elasticsearch stop sleeps for 86400 seconds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paulv-quest</reporter><labels /><created>2015-12-17T19:48:18Z</created><updated>2016-01-22T01:40:04Z</updated><resolved>2015-12-17T20:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-17T20:02:37Z" id="165567206">This is intentional so that shards can be closed; see #10680 and #11248 for details and #12298 is the pull request that ultimately closed that issue. If Elasticsearch is not shutting down, there is _something_ else going on, but the sleep is not the issue. That's just waiting before issuing sending a `SIGKILL` in case Elasticsearch really is dead but we don't want to signal prematurely.
</comment><comment author="satoshi-kimura" created="2016-01-22T01:40:04Z" id="173772418">I think so elasticsearch have to confirm os version.
Because elasticsearch can not restart on RHEL6.x , it sleep 86400 in stop.
RHEL7.x is no probrem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run pipeline aggregations for empty buckets added in the Range Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15519</link><project id="" key="" /><description>Closes #15471
</description><key id="122786393">15519</key><summary>Run pipeline aggregations for empty buckets added in the Range Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T17:46:53Z</created><updated>2016-01-19T10:52:36Z</updated><resolved>2015-12-18T14:43:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-18T10:16:23Z" id="165739641">The fix looks good to me, it's surprisingly simple! I'm more concerned about the test however: can we make it not use an actual script?
</comment><comment author="colings86" created="2015-12-18T13:12:26Z" id="165773731">@jpountz I pushed a new commit which changes the test so it doesn't use scripts
</comment><comment author="jpountz" created="2015-12-18T13:50:40Z" id="165782774">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log message suggesting setting network.host if using EC2 discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15518</link><project id="" key="" /><description>@dadoonet looked into the feasibility of setting more helpful defaults for network.host if EC2 discovery is being used with the AWS plugin in #13970. He found good reasons not to do so though.

I still wonder if we can improve the user experience by logging a message suggesting they may want to consider setting network.host to _ec2_ or one of the other EC2 specific options if discovery.type: ec2 is set. @dedemorton 's idea.
</description><key id="122771241">15518</key><summary>Log message suggesting setting network.host if using EC2 discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peterskim12</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-17T16:34:02Z</created><updated>2017-04-24T00:31:30Z</updated><resolved>2017-04-24T00:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-17T16:42:27Z" id="165505143">The problem is not specific to ec2. Its that ES was so promiscuous before, bound to every interface, and then screamed via multicast to any computers it possibly could.

Now defaults are for localhost only, but people don't read the documentation.

So maybe we should just loudly scream something like:

```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@ WARNING: DEFAULTING TO LOCALHOST ONLY
@ Please set network.host....
```

Basically impl-wise this would mean we default to some internal value like `_local_warn_` or something instead of `_local_`
</comment><comment author="rmuir" created="2015-12-17T17:02:20Z" id="165513649">I would even think about something like 'network is not configured' and pointing to a docs page specific to how to configure it for most use cases (worst case: https://www.elastic.co/guide/en/elasticsearch/reference/2.x/modules-network.html#common-network-settings)

The problem is: typically I think at a minimum, you need to adjust two things, regardless of what plugins you have:
1. what addresses you listen on: (network.host)
2. what addresses you talk to: (discovery settings)

For most users, this means setting up the unicast list as well as setting network.host.

This issue as worded assumes the user correctly installed the plugin, and correctly did 2 but not 1. There are more ways that things can go wrong.
</comment><comment author="dedemorton" created="2015-12-18T18:07:02Z" id="165858918">I'd avoid providing links to specific doc pages, though, because content moves around, and the links could get stale.
</comment><comment author="rmuir" created="2015-12-18T18:14:58Z" id="165860783">docs are versioned.
</comment><comment author="debadair" created="2015-12-18T18:48:53Z" id="165869060">But do we want to have to update error messages for each version? Otherwise, we'd have error messages pointing to a specific version that isn't the current version. If we just pointed to 2.x or current, then we're back to the issue of links potentially getting stale. 

While it's not a big deal for one error message, it could turn into a real headache if we start doing this more generally without having a way to update/validate the links for each release. 
</comment><comment author="rmuir" created="2015-12-18T18:51:46Z" id="165869656">if we want to link to docs, we just need a constant in Version like DOCROOT or whatever.
</comment><comment author="debadair" created="2015-12-18T19:02:34Z" id="165871852">But then when the version is bumped, there's no guarantee that the links are still valid in the new version--we'd need to validate them, which can't really be done until the docs for the new version go live. 

Totally agree that linking to the docs would be really helpful--I could totally see wanting to do this in a number of cases, as long as they can be tested &amp; easily updated. 
</comment><comment author="rmuir" created="2015-12-18T19:44:07Z" id="165880476">There are no guarantees on anything, i mean if i look at the docs for a random page (https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-transport.html) I can see that `transport.tcp.compress` is now wrong, since we no longer use the LZF algorithm.

Does this mean we should just do nothing for this issue at all? Its somehow quickly turned into 'but our error messages and docs must be unbreakable!' when they are far, far from that right now... when IMO all that is needed is some improved messaging "Hey, you didn't configure your network, we defaulted to localhost only". It could reduce a lot of confusion.
</comment><comment author="debadair" created="2015-12-18T20:00:18Z" id="165885189">Not saying we should do nothing--absolutely agree we should add the log message. My personal preference would be for a more-verbose message over embedding a link to the docs, but if we can add tests that extract the anchor from the URL and verify that the anchor still exists in the doc repo, I think the links would be great, too.
</comment><comment author="rmuir" created="2015-12-18T20:02:11Z" id="165885572">I guess my problem is we already spent half a day figuring out the docs for https://www.elastic.co/guide/en/elasticsearch/reference/2.x/modules-network.html#common-network-settings

So i wanted to reuse that, rather than re-enter a bikeshed about what the exact log messages should be and all that nonsense. This is why these kinds of things do not get done...
</comment><comment author="debadair" created="2015-12-18T20:27:50Z" id="165891210">Understood. I think we can all agree that any message is better than no message, it doesn't have to be perfect. I would suggest the following:

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@ WARNING: NO NETWORK CONFIGURED, DEFAULTING TO LOCALHOST
@ Set network.host to specify the host's listening address. To use Zen Discovery to 
@ join a cluster, set discovery.zen.ping.unicast.hosts. 
</comment><comment author="dedemorton" created="2015-12-18T21:26:58Z" id="165901266">+1 for Deb's suggestion. Deep linking is great when you can validate that the links work, but difficult to maintain and potentially very frustrating to users when the links are broken (been there, done that). You need an infrastructure to support deep linking, or you're opening up a can of worms (and the worms always end up in someone's belly). Of course, if someone wants to step forward and create an infrastructure of validating the links, I'm all for that....
</comment><comment author="asettouf" created="2017-04-23T23:49:41Z" id="296497865">I made my first ever pull request to contribute to this enhancement based on what was suggested in that topic. Please do let me know if there is anything wrong with it (pretty likely I guess) as it is my first time making one. </comment><comment author="jasontedor" created="2017-04-24T00:31:30Z" id="296500364">I do not think we should log a message here, we already make it clear in the logs the bound addresses. I think the particular issue here is best left to the documentation.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>post_filer and script_file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15517</link><project id="" key="" /><description>Using this syntax:

```
"post_filter": { 
    "script": {
        "lang": "groovy",
        "file": "min_distance_comparison",
        "params": {
            "lat": 29.201168,
            "lon": -81.037071
        }
    }
}
```

throws [script] query does not support [file]

Replacing 'file' with 'script_file' works but the docs say to use 'file'
https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_scripting_changes.html
</description><key id="122768380">15517</key><summary>post_filer and script_file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coldlamper</reporter><labels /><created>2015-12-17T16:19:25Z</created><updated>2015-12-18T13:30:36Z</updated><resolved>2015-12-18T13:30:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-18T13:30:36Z" id="165779186">@coldlamper I think you are missing one level of script and it should be (not tested):

```
"post_filter": { 
    "script": {
        "script": {
            "lang": "groovy",
            "file": "min_distance_comparison",
            "params": {
                "lat": 29.201168,
                "lon": -81.037071
            }
        }
    }
}
```

The outer `script` object tells elasticsearch to use the `script` query while the inner `script` object gives the script definition. See https://www.elastic.co/guide/en/elasticsearch/reference/2.1/query-dsl-script-query.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix spans extraction to not also include individual terms.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15516</link><project id="" key="" /><description>This is a bug that I introduced in #13239 while thinking that the differences
were due to changes in Lucene: extractUnknownQuery is also called when span
extraction already succeeded, so we should only fall back to Weight.extractTerms
if no spans have been extracted yet.

Close #15291
</description><key id="122765388">15516</key><summary>Fix spans extraction to not also include individual terms.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Highlighting</label><label>bug</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T16:05:12Z</created><updated>2015-12-17T16:12:22Z</updated><resolved>2015-12-17T16:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-17T16:07:12Z" id="165495660">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch tests use deprecated API when being very slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15515</link><project id="" key="" /><description>```
16:58:57.609 java[45366:25547260] 16:58:57.609 WARNING:  140: This application, or a library it uses, is using the deprecated Carbon Component Manager for hosting Audio Units. Support for this will be removed in a future release. Also, this makes the host incompatible with version 3 audio units. Please transition to the API's in AudioComponent.h.
```

Really this is not acceptable, what's the justification for calling such an API here. This spams my console, it's almost like guice going down hard in loops causing out of disk errors. Who is responsible for calling such an API. We have to be more carful here really :8ball: 
</description><key id="122765153">15515</key><summary>Elasticsearch tests use deprecated API when being very slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>won't fix</label></labels><created>2015-12-17T16:03:55Z</created><updated>2016-02-24T17:40:11Z</updated><resolved>2016-02-24T17:40:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-17T16:08:48Z" id="165496043">What is your version of java? Maybe its not the latest? If the latest java still reports this, we should open a bug at openjdk!
</comment><comment author="s1monw" created="2015-12-17T16:09:44Z" id="165496295">```
java version "1.8.0_40"
Java(TM) SE Runtime Environment (build 1.8.0_40-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)
```
</comment><comment author="rmuir" created="2015-12-17T16:12:07Z" id="165496912">What is your mac os? Maybe it only happens on El Capitan? 

Personally I am not running this one, because I am not ready to do battle with my operating system about who should or should not be `root` (short answer: me not them). It nags me to upgrade almost every day and i am waiting for my older version to really annoy me before considering any upgrade, as usual.
</comment><comment author="s1monw" created="2015-12-17T16:15:24Z" id="165497870">@jasontedor runs it on el captoROOTio and it apparently works fine :bomb:  
</comment><comment author="jasontedor" created="2015-12-30T00:03:39Z" id="167902673">This is an OpenJDK issue binding against API calls that were deprecated in El Capitan; it looks like there is a fix slated for Java 9.
</comment><comment author="clintongormley" created="2016-01-10T18:09:30Z" id="170375771">Is there anything for us to do here?
</comment><comment author="s1monw" created="2016-02-24T17:40:11Z" id="188372102">closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename bwc test to 2.0, update versions to latest minor and add 2.1 b&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15514</link><project id="" key="" /><description>&#8230;wc test
</description><key id="122751774">15514</key><summary>Rename bwc test to 2.0, update versions to latest minor and add 2.1 b&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-12-17T14:58:28Z</created><updated>2015-12-17T15:52:29Z</updated><resolved>2015-12-17T15:52:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-17T15:37:46Z" id="165486182">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkProcessor backs off exponentially by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15513</link><project id="" key="" /><description>With this commit we change the default behavior of
BulkProcessor from not backing off when getting
EsRejectedExecutionException to backing off exponentially.

Related to #14829.
</description><key id="122749857">15513</key><summary>BulkProcessor backs off exponentially by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T14:51:32Z</created><updated>2016-01-18T07:46:49Z</updated><resolved>2015-12-17T15:52:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-17T15:04:17Z" id="165477597">LGTM. Thanks!
</comment><comment author="nik9000" created="2015-12-17T15:38:19Z" id="165486354">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the unused mergeScheduleFuture from IndexShard.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15512</link><project id="" key="" /><description>we never set it
</description><key id="122741989">15512</key><summary>Remove the unused mergeScheduleFuture from IndexShard.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T14:12:45Z</created><updated>2015-12-17T14:47:04Z</updated><resolved>2015-12-17T14:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-17T14:18:42Z" id="165463311">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify the Text API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15511</link><project id="" key="" /><description>We have the Text API, which is essentially a wrapper around a String and a
BytesReference and then we have 3 implementations depending on whether the
String view should be cached, the BytesReference view should be cached, or both
should be cached.

This commit merges everything into a single Text that is essentially the old
StringAndBytesText impl.

Long term we should look into whether this API has any performance benefit or
if we could just use plain strings. This would greatly simplify all our other
APIs that currently use Text.
</description><key id="122741709">15511</key><summary>Simplify the Text API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T14:10:53Z</created><updated>2016-01-10T20:58:10Z</updated><resolved>2015-12-17T17:07:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-17T15:59:16Z" id="165493613">LGTM. Left small comment. Having two versions might have been useful if they were immutable but it looks like none were. So this doesn't change the immutable-ness anywhere so its safe and simpler.
</comment><comment author="jpountz" created="2015-12-17T16:14:17Z" id="165497601">Thanks @nik9000 for having a look. I just pushed a new commit.
</comment><comment author="nik9000" created="2015-12-17T16:20:34Z" id="165499288">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reroute once per batch of shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15510</link><project id="" key="" /><description>This commit modifies the behavior after publication of a new cluster
state to only invoke the reroute logic once per batch of shard failures
rather than once per shard failure.
</description><key id="122740645">15510</key><summary>Reroute once per batch of shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T14:04:25Z</created><updated>2015-12-17T14:08:58Z</updated><resolved>2015-12-17T14:08:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-17T14:05:52Z" id="165460739">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simpler using compressed oops flag representation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15509</link><project id="" key="" /><description>This commit modifies the internal representation of the JVM flag
UseCompressedOops to just be a String. This means we can just store the
value of the flag or "unknown" directly so that we do not have to engage
in shenanigans with three-valued logic around a boxed boolean.

Relates #15489
</description><key id="122734670">15509</key><summary>Simpler using compressed oops flag representation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T13:29:02Z</created><updated>2015-12-17T18:06:41Z</updated><resolved>2015-12-17T17:21:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-17T13:30:23Z" id="165453534">The ultimate endgame here is simpler handling in the backwards compatibility code in the 2.x line so that we can distinguish between "true", "false" and "unknown" from a &gt;= 2.2.0 node, and null from a &lt; 2.2.0 node.
</comment><comment author="nik9000" created="2015-12-17T15:26:03Z" id="165483274">LGTM - I left a comment asking about the return value from the jvm's info API. Maybe if it returns null we should set it to "unknown".
</comment><comment author="jasontedor" created="2015-12-17T17:20:46Z" id="165519813">&gt; LGTM - I left a comment asking about the return value from the jvm's info API. Maybe if it returns null we should set it to "unknown".

The return from the API can not be null.
</comment><comment author="jasontedor" created="2015-12-17T18:06:36Z" id="165534190">Thank you for reviewing @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MetaDataMappingService should call MapperService.merge with the original mapping update.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15508</link><project id="" key="" /><description>Currently MetaDataMappingService parses the mapping updates, reserializes it and
finally calls MapperService.merge with the serialized mapping. Given that mapping
serialization only writes differences from the default, this is a bit unfair to
parsers since they can't know whether some option has been explicitly set or not.

Furthermore this can cause bugs with metadata fields given that these fields use
existing field types as defaults.

This commit changes MetaDataMappingService to call MapperService.merge with the
original mapping update.
</description><key id="122723364">15508</key><summary>MetaDataMappingService should call MapperService.merge with the original mapping update.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T12:11:50Z</created><updated>2015-12-17T17:04:50Z</updated><resolved>2015-12-17T17:04:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-17T16:10:33Z" id="165496502">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_recovery API better add reason why it is recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15507</link><project id="" key="" /><description>GET _recovery?active_only
{
  "2015-12-17": {
    "shards": [
      {
        "id": 3,
        "type": "REPLICA",
        "stage": "TRANSLOG",
        "primary": false,
        "start_time_in_millis": 1450347822349,
        "total_time_in_millis": 1555568,
        "source": {
          "id": "sMq8Q2xxSVK32ra6BGMPHg",
          },
        "target": {
          "id": "rRFazB3qSbKaRdMU-JbmOA",
        },
        "index": {
          "size": {
            "total_in_bytes": 9902349586,
            "reused_in_bytes": 0,
            "recovered_in_bytes": 9902349586,
            "percent": "100.0%"
          }
        }
      }
    ]
  }
}

it is hardly to know why there is a recovery. better add a key called "reason"
</description><key id="122710220">15507</key><summary>_recovery API better add reason why it is recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Recovery</label><label>discuss</label></labels><created>2015-12-17T10:49:14Z</created><updated>2016-01-10T21:07:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>AllTermQuery's scorer should skip segments that never saw the requested term</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15506</link><project id="" key="" /><description>Spinoff from this user's issue: https://discuss.elastic.co/t/2-1-0-nullpointerexception-not-sure-where-to-start/37419

When a segment never indexed the requested term, it has a null `TermState`, which means we should skip scoring that segment, and not cause an NPE.
</description><key id="122706863">15506</key><summary>AllTermQuery's scorer should skip segments that never saw the requested term</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T10:31:47Z</created><updated>2015-12-17T16:01:15Z</updated><resolved>2015-12-17T16:01:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-17T12:10:23Z" id="165436974">LGTM
</comment><comment author="nik9000" created="2015-12-17T14:17:07Z" id="165462997">LGTM
</comment><comment author="rmuir" created="2015-12-17T14:57:01Z" id="165475697">+1, thanks mike!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>close inactive index in certain times and become active again when there is operation on it.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15505</link><project id="" key="" /><description>config:
index.close.after_time_inactive: 30m

action:
when an index is inactive for 30m, then close it and reclam its occupied memory for active index to use.
when there is a operation on it later, open it first and then operate it. 
</description><key id="122699034">15505</key><summary>close inactive index in certain times and become active again when there is operation on it.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-12-17T09:48:13Z</created><updated>2016-01-10T19:57:39Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2015-12-22T03:44:09Z" id="166497708">any update?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix cyclic dependency issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15504</link><project id="" key="" /><description>In certain places we inject the `Client` or `ScriptService` as a dependency to plugin services/components. There is a cyclic dependency issue when one of these classes is used in a plugin node service/component and this service/component is also used as a dependency in either a transport action or transport action filter. In this case were then unable to start the node and a big guice cyclic dependency error is printed.

The workaround is to provide `Client` or `ScriptService` via setter instead of constructor. This isn't ideal. I think there is just a mismatch on we think services/components depend on each other and how things are really wired up together with Guice. Specially the only Client in a node is `NodeClient` and that indirectly requires all services/components, but if another services is then having a dependency on `Client` things get tricky and it creates a dependency mess. I'm not sure how to fix this.

In certain cases the setter workaround doesn't suffice and in order to avoid cyclic dependencies a Guice `Provider` or directly using the Guice injector is used to fetch the `Client` or `ScriptService`. This is hack is just dirty and event worse then the setter workaround.
</description><key id="122698703">15504</key><summary>Fix cyclic dependency issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2015-12-17T09:46:25Z</created><updated>2016-07-15T12:15:36Z</updated><resolved>2016-07-15T12:15:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-18T07:09:23Z" id="165698026">@martijnvg Can you list out the cycle in initialization ordering?
</comment><comment author="martijnvg" created="2015-12-20T19:54:51Z" id="166150142">@rjernst There are many cyclic dependency issues. Basically no transport action can't be constructed. 

Here is one problematic initialisation order when `Client` is added as constructor parameter to `Ingestbootstrapzper` in the ingest plugin:
- IngestBootstrapper
- Client
- NodeClient
- java.util.Map&lt;org.elasticsearch.action.GenericAction, org.elasticsearch.action.support.TransportAction&gt;
- TransportAction 
- TransportOpenIndexAction
- MetaDataIndexStateService
- AllocationService
- AllocationDeciders
- AllocationDecider
- InternalClusterInfoService
- DiskThresholdDecider
- ClusterInfoService
- ActionFilters
- TransportNodesStatsAction
- ActionFilter
- IngestActionFilter
- IngestBootstrapper

Other problematic initialisation orderings are similar. If `ScriptService` is added as constructor parameter then the only difference is that between the first and second component initialisation is `ScriptsService` since that relies on `Client` too.

What currently is done in ingest and other plugins as workaround is to supply `Client` and `ScriptsService` lazily via a setter or use Guice's `Provider` mechanism.
</comment><comment author="martijnvg" created="2016-05-18T09:46:40Z" id="219977703">Update: If `ScriptService` is used as a dependency, it no longer causes cyclic dependency issues, because it no longer depends on `Client` since indexed scripts have been replaced with stored scripts.
</comment><comment author="javanna" created="2016-07-14T15:02:14Z" id="232691801">I think this has been recently solved in master. Can we close it already or is there work left to do?
</comment><comment author="nik9000" created="2016-07-15T12:15:34Z" id="232936621">I think we'll get to this as we de-guice the code base and work on the plugin interface. I think it is safe to close this because of that. So I'm closing it.

We've broken one of the larger cycles lately by making `NodeClient` not depend on the actions, instead they are set on it after it is constructed. We did that because guice was unable to do whatever synthetic magic it does to break the dependency in some case so we _had_ to to move forward.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mutliple nested types in the same query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15503</link><project id="" key="" /><description>Hi

I have a question to which I can't seem to find an answer anywhere. 

Let's say I have an Index type with a mapping that contains two or more nested types (parallel, not deeply nested). 

&lt;pre&gt;
...
"field1": {  
  "type": "nested", 
  "properties": {
    "subfield1": { "type": "string" },
    "subfield2": { "type": "integer" }
  }
},
"field2": {  
  "type": "nested", 
  "properties": {
    "subfield1": { "type": "string" },
    "subfield2": { "type": "integer" }
  }
}                 
...
&lt;/pre&gt;


Is it possible to use those two fields in the same query?
</description><key id="122684496">15503</key><summary>Mutliple nested types in the same query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lengoyvaerts</reporter><labels /><created>2015-12-17T08:13:33Z</created><updated>2015-12-21T09:00:24Z</updated><resolved>2015-12-21T09:00:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T09:00:24Z" id="166240728">It depends how you want to query these fields, can you be more specific about what you are trying to do?

Also we try to use this bug tracker for confirmed bugs and features requests, please ask such questions as http://discuss.elastic.co instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Searching range not work, why?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15502</link><project id="" key="" /><description>I can't query data by date range after I upgrade ES version from 1.7 to 2.1.
Searching range json: {"range":{"CreateTime":{"gt":"2015-12-17 00:21:20","lte":"2015-12-17 12:21:10"}}.
It seems range is not work and the data exist in the range.

Thank you!
</description><key id="122682241">15502</key><summary>Searching range not work, why?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">117111302</reporter><labels /><created>2015-12-17T07:55:41Z</created><updated>2015-12-21T05:44:26Z</updated><resolved>2015-12-21T05:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-18T13:31:46Z" id="165779371">How is your `CreateTime` field mapped? I suspect it doesn't work because it got indexed as a string instead of a date?
</comment><comment author="117111302" created="2015-12-21T05:44:25Z" id="166209889">Yep! I forgot field mapping. It's my mistake. Thank you very much for your help!
Close it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>thirdPartyAudit round 2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15501</link><project id="" key="" /><description>This fixes the `lenient` parameter to be `missingClasses`. I will remove this boolean in a followup and we can handle them via the normal `exclusions` like any other problem.

It also adds a check for sheisty classes (jar hell with the jdk).
This is inspired by the lucene "sheisty" classes check, but it has false positives. This check is more evil, it validates every class file against the extension classloader as a resource, to see if it exists there. If so: jar hell.

This jar hell is a problem for several reasons:
1. causes insanely-hard-to-debug problems (like bugs in forbidden-apis)
2. hides problems (like internal api access)
3. the code you think is executing, is not really executing
4. security permissions are not what you think they are
5. brings in unnecessary dependencies
6. its jar hell

The more difficult problems are stuff like jython, where these classes are simply 'uberjared' directly in, so you cant just fix them by removing a bogus dependency. And there is a legit reason for them to do that, they want to support java 1.4.
</description><key id="122680566">15501</key><summary>thirdPartyAudit round 2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T07:42:27Z</created><updated>2015-12-17T08:44:27Z</updated><resolved>2015-12-17T08:44:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-17T08:35:21Z" id="165383725">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor typo fix in Update suggesters.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15500</link><project id="" key="" /><description /><key id="122667579">15500</key><summary>Minor typo fix in Update suggesters.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">priyanshuch</reporter><labels /><created>2015-12-17T06:05:07Z</created><updated>2015-12-18T16:18:18Z</updated><resolved>2015-12-18T04:48:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-17T15:22:25Z" id="165482058">I'm pretty sure this is intentionally misspelled to demonstrate the suggester.
</comment><comment author="priyanshuch" created="2015-12-18T04:48:41Z" id="165672724">Sorry, don't know what I was thinking.
</comment><comment author="nik9000" created="2015-12-18T16:18:18Z" id="165822138">Thanks for helping! Its always nice to get doc corrections and _usually_ misspelled words are a good catch!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery loop never ends or comes back shortly </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15499</link><project id="" key="" /><description>I have a small two machines cluster and starting with ES 2.1 it got barely usable. One of the two machines (random) falls out of the cluster, then rejoins, runs many hours of recovery, then goes green for just a little and then scenario repeats with one machine or another. I suspected overload and gradually reduced the load, but it does not seem relevant. It pretty much feels like 2.1 issue. 

A few symptoms, which may be relevant or may be not:
1.  A lot of these: failed to create shard, failure ElasticsearchException[failed to create shard]; nested: LockObtainFailedException[Can't lock shard [&#8230;][4], timed out after 5000ms]; I was looking for a way to increase this 5000ms timeout, but for the life of me could not find how;
2.  Lots of these: BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
3.   It is not Out of Memory &#8211; I had it in the past and now pretty sure it is not the case;
4.  Some of these: RemoteTransportException[[Acorn6][10.150.0.111:9300][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[Phase[1] phase1 failed]; nested: RecoverFilesRecoveryException[Failed to transfer [25] files with total size of [45gb]]; nested: ReceiveTimeoutTransportException[[Acorn5][10.150.0.110:9300][internal:index/shard/recovery/prepare_translog] request_id [32655383] timed out after [900386ms]];

Any troubleshooting tips? 
Thank you!
Konstantin
</description><key id="122659250">15499</key><summary>Recovery loop never ends or comes back shortly </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konste</reporter><labels /><created>2015-12-17T04:34:12Z</created><updated>2015-12-17T10:52:10Z</updated><resolved>2015-12-17T10:52:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-17T10:52:10Z" id="165420911">The trick is indeed finding out why the nodes are falling of the cluster. Here is a bit of help answering your questions (I'm going to close this for now - feel free to reopen or continue discussing on discuss.elastic.co ).

1) When we create a shard on a folder we acquire a lock to make sure the folder is not used by a background process (like a recovery). This is to protect against overriding folders in the case of quick index deletion and creation (among others). We give this a best effort of 5s which is more then enough for things to unravel things. The assumption is that if this fails we better try another node (and if that fails come back to this node). This is indeed not configurable. 
2) The BroadcastShardOperation is too verbose. These errors are correct and expected. This was fixed with #14950 . You can ignore those.
3) OK :)
4) This indicate that a recovery doesn't have any activity for more then 15 minutes and therefore the recovery is cancelled. This is probably the source of your problems. The question is why does it time out. I would check the logs for network issues or engine related things.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added an example of how inner objects don't match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15498</link><project id="" key="" /><description>The page didn't mention the fact if the value for the field is an inner object the field won't match so I added an example.
</description><key id="122653817">15498</key><summary>Added an example of how inner objects don't match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryangrimm</reporter><labels /><created>2015-12-17T03:32:04Z</created><updated>2016-01-10T21:29:00Z</updated><resolved>2016-01-10T21:28:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T21:29:00Z" id="170395993">thanks @ryangrimm 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CentOS 6.5 can't start elasticsearch 2.1 with root. And no permission run as non-root. Why?   </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15497</link><project id="" key="" /><description>bin/elasticsearch
Exception in thread "main" java.lang.RuntimeException: don't run elasticsearch as root.
    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:93)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:144)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
</description><key id="122650356">15497</key><summary>CentOS 6.5 can't start elasticsearch 2.1 with root. And no permission run as non-root. Why?   </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rentianhua</reporter><labels /><created>2015-12-17T03:00:58Z</created><updated>2015-12-23T02:45:57Z</updated><resolved>2015-12-21T09:02:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-21T09:02:01Z" id="166240950">elasticsearch doesn't allow to be run as root so that if your elasticsearch instance ever gets compromised, an attacker would have limited permissions.
</comment><comment author="spinscale" created="2015-12-21T09:04:36Z" id="166241339">in case you installed the RPM: If you run as non-root, make sure you run as the elasticsearch user, otherwise you will have permission problems when starting up - the easiest way is to use the service/systemctl tools to start/stop elasticsearch, as those use all the things configured in the package.

If you want to run as your own user, make sure you configure the data/log directories to directories you can write into.

If you cannot run elasticsearch as a package with the default user, please reopen this issue and provide more details, so we can reproduce the issue. Thanks!
</comment><comment author="rentianhua" created="2015-12-23T02:45:57Z" id="166792365">thanks all of your comments :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs explanation for unassigned shard reason codes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15496</link><project id="" key="" /><description>Added the reason codes with the respective explanation for unassigned shard state. 

Closes #14001
</description><key id="122647167">15496</key><summary>Docs explanation for unassigned shard reason codes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fforbeck</reporter><labels><label>docs</label></labels><created>2015-12-17T02:24:31Z</created><updated>2016-01-12T02:32:51Z</updated><resolved>2016-01-12T02:32:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T21:27:32Z" id="170395914">Hi @fforbeck and thanks for the PR

I've added one formatting suggestion - please could you make the change and push again, and I'll be happy to merge it
</comment><comment author="fforbeck" created="2016-01-11T11:53:08Z" id="170519154">Hi @clintongormley,

Sure, I will reformat and send it again.

Thanks for the review :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow integ tests to exclude mock plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15495</link><project id="" key="" /><description>We currently randomly add a set of mock plugins to integ tests.
Sometimes it is necessary to omit this mock plugins, but other times you
may just want to suppress a particular mock plugin. For example, if you
have your own transport, you want to omit the asserting local transport
mock, since they would both try to set the transport.type.
</description><key id="122646423">15495</key><summary>Allow integ tests to exclude mock plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-17T02:16:33Z</created><updated>2015-12-17T02:22:05Z</updated><resolved>2015-12-17T02:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-12-17T02:18:01Z" id="165316071">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add callback for publication of new cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15494</link><project id="" key="" /><description>This commit adds a callback for a cluster state task executor that will
be invoked if the execution of a batch of cluster state update tasks
led to a new cluster state and that new cluster state was successfully
published.

Closes #15482
</description><key id="122630259">15494</key><summary>Add callback for publication of new cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T23:49:31Z</created><updated>2015-12-17T13:31:05Z</updated><resolved>2015-12-17T13:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-17T11:03:07Z" id="165423324">Left one comment. Do we also want to immediately use it for the shard state action?
</comment><comment author="jasontedor" created="2015-12-17T11:07:28Z" id="165424278">&gt; Left one comment. Do we also want to immediately use it for the shard state action?

@bleskes Yes. I've a commit sitting locally pending the outcome of this pull request which I'll push to another pull request.
</comment><comment author="bleskes" created="2015-12-17T11:10:11Z" id="165424796">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo in scroll.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15493</link><project id="" key="" /><description>Fix scroll request with sort. Replace } with ].
</description><key id="122621254">15493</key><summary>fix typo in scroll.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">inqueue</reporter><labels><label>docs</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T22:47:48Z</created><updated>2015-12-17T19:45:30Z</updated><resolved>2015-12-17T01:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-16T23:51:35Z" id="165292339">LGTM. Thank you! I'll integrate this shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error message failure when analyzer created without tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15492</link><project id="" key="" /><description>```
POST /analyzertest
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "filter": ["lowercase","stop", "shingl"],
          "char_filter": ["html_strip"]
        }
      }
    }
  }
}
```

Returns error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "index_creation_exception",
            "reason": "failed to create index"
         }
      ],
      "type": "illegal_argument_exception",
      "reason": "Analyzer [my_analyzer] must have a type associated with it"
   },
   "status": 400
}
```

reason should probably be: "Analyzer [my_analyzer] must have a declared tokenizer associated with it"
</description><key id="122610501">15492</key><summary>Error message failure when analyzer created without tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">willf</reporter><labels><label>:Analysis</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-16T21:49:48Z</created><updated>2016-05-19T13:47:22Z</updated><resolved>2016-05-19T13:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T21:22:36Z" id="170395638">Analyzers are configurable, so you can specify a `type` and a number of parameters to configure that base analyzer.  Alternatively, if you specify a `tokenizer`, then `type` defaults to `custom`.  Perhaps we could change the error to `... must specify either an analyzer type, or a tokenizer`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add gradle thirdPartyAudit to precommit tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15491</link><project id="" key="" /><description>Currently, we do all kinds of checks on our own code, but dependencies are no different. The computer does not care if you wrote it, or dragged it in via a dependency. 

This task fails for two situations (implemented with forbidden apis checks):
1. dependency classpath is incomplete: fails the build if third party dependencies refer to other classes that do not exist. Really, when classes are missing, this is just asking for trouble e.g. `NoClassDefFoundError`, but in some cases its actually intentional (e.g. optional dependency of a dependency). In those cases, this check can be disabled with `thirdPartyAudit.lenient = true`
2. use of internal JDK classes: fails the build, unless exact third party class doing this is excluded in `thirdPartyAudit.excludes`. Wildcards are explicitly not permitted in exclusion lists.

We can expand in the future, e.g. add additional forbidden apis beyond just internal JDK apis for 3rd party jars if we want. Probably better to worry about sorting out the messes discovered here first (I don't fix em, i just document what they are).
</description><key id="122609987">15491</key><summary>Add gradle thirdPartyAudit to precommit tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T21:47:29Z</created><updated>2015-12-17T06:29:19Z</updated><resolved>2015-12-16T23:56:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-16T22:13:56Z" id="165269699">&gt; thirdPartyAudit.lenient = true

`thirdParyAudit.ignoreClassNotFound = true`? `lenient` leaves a lot to the imagination.
</comment><comment author="rmuir" created="2015-12-16T22:20:57Z" id="165271947">That is intentional, because of this problem, we have to hide all forbidden APIs warnings (otherwise the thousands of missing classes would overflow your screen).

So yes, it should be leaving stuff to the imagination, like "what else could be wrong".
</comment><comment author="rjernst" created="2015-12-16T22:21:22Z" id="165272134">LGTM
</comment><comment author="nik9000" created="2015-12-16T22:21:58Z" id="165272369">&gt; So yes, it should be leaving stuff to the imagination, like "what else could be wrong".

Got it.
</comment><comment author="uschindler" created="2015-12-16T22:26:32Z" id="165273490">The unzipping is not nice, but looks fine. The alternative would be to not use forbiddenapis ANT task, but instead instantiate the forbiddenapis Checker class directly, pass custom logger with INFO and ERROR, but no WARN, and finally feed all classes from a ZipInputStream or similar to prevent the "Ant O(n^2) zipfileset" bug. Backside: You need a bit more code to fix the Checker initialization (classloader, load classes from ZIP stream)

+1
</comment><comment author="nik9000" created="2015-12-16T22:33:21Z" id="165274859">Tried it locally as well. LGTM.
</comment><comment author="rmuir" created="2015-12-16T22:35:58Z" id="165275426">Instead of `lenient` i would also be fine with `CLASSES_ARE_MISSING`

Basically if we have this situation, anything goes. I thought lenient would make it sound appropriately negative, but something in all-upper-case like that would be fine too. :)
</comment><comment author="rmuir" created="2015-12-17T06:29:19Z" id="165355959">I am working on improving it further, to eventually remove `lenient`. I think there should only be `excludes`, that is where you list classes, if classes are wrong in any way, including if they are missing. I also have some new checks to add that reveal additional problems. 

This is better as it always "keeps tabs", which is what we need. It does require doing a little hack to parse the forbidden apis warnings :)

The me, the point of these checks right now is to have nice documentation of issues for every module, not necessarily even to try to fix them. We have to at least know what is happening and what code is or is not running!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>shingle filter apparently requires a declared tokenizer, but probably shouldn't</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15490</link><project id="" key="" /><description>The following should, I think, produce equivalent results, since the `standard` tokenizer is the default. But they do not:

```
GET _analyze
{
  "text": "i like search",
  "filters" : ["shingle"]
}
GET _analyze
{
  "text": "i like search",
  "tokenizer": "standard",
   "filters" : ["shingle"]
}
```

The second call does the right thing, but the first only produces 'shingles' of length 1.
</description><key id="122607919">15490</key><summary>shingle filter apparently requires a declared tokenizer, but probably shouldn't</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">willf</reporter><labels><label>:Analysis</label><label>discuss</label></labels><created>2015-12-16T21:35:49Z</created><updated>2016-09-28T16:04:47Z</updated><resolved>2016-09-28T16:04:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-10T21:20:40Z" id="170395515">The default behaviour is to use the default analyzer, if no tokenizer is specified, so in your first example, the `filters` are just being ignored.

@johtani Perhaps we can throw an exception if `filter`/`char_filter` is specified without `tokenizer`?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Info on compressed ordinary object pointers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15489</link><project id="" key="" /><description>This commit adds to JvmInfo the status of whether or not compressed
ordinary object pointers are enabled. Additionally, logging of the max
heap size and the status of the compressed ordinary object pointers flag
are provided on startup.

Relates #13187, relates elastic/elasticsearch-definitive-guide#455
</description><key id="122578336">15489</key><summary>Info on compressed ordinary object pointers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T19:03:33Z</created><updated>2015-12-17T18:06:24Z</updated><resolved>2015-12-16T22:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-16T19:08:09Z" id="165214425">This provides log messages on startup that look like

```
[2015-12-16 13:53:33,417][INFO ][env                      ] [Illyana Rasputin] heap size [989.8mb], compressed ordinary object pointers [true]
```

and

```
[2015-12-16 13:54:25,051][INFO ][env                      ] [Everyman] heap size [31.9gb], compressed ordinary object pointers [false]
```

and provides a `using_compressed_ordinary_object_pointers` field in the `jvm` field of the nodes info API response.
</comment><comment author="polyfractal" created="2015-12-16T19:12:57Z" id="165215570">Woo!  The log message LGTM, but I don't think I'm qualified to comment on the JvmInfo code itself :)

Should this be added to the `/_cat/nodes` output too as an optional column?  Is it possible to backport this to the 2.x series, or something technical which disallows it?
</comment><comment author="jasontedor" created="2015-12-16T19:22:34Z" id="165218945">&gt; Should this be added to the /_cat/nodes output too as an optional column?

@polyfractal I considered that and I ultimately decided against; the reason that I decided against is because nearly all of the information that is in `JvmInfo` is not provided in the cat nodes API. That's just my thinking; I'm certainly open to other thoughts.

&gt; Is it possible to backport this to the 2.x series, or something technical which disallows it?

It's possible.
</comment><comment author="polyfractal" created="2015-12-16T19:26:20Z" id="165220552">&gt; I considered that and I ultimately decided against; the reason that I decided against is because nearly all of the information that is in JvmInfo is not provided in the cat nodes API.

Makes sense.  Someone would only need to check this once, and you'd never need to monitor it dynamically... no need to clutter up the cat endpoint with it :)
</comment><comment author="rmuir" created="2015-12-16T20:10:19Z" id="165231473">Is reflection on this API really allowed in java 9? We should not add this if not.
</comment><comment author="jasontedor" created="2015-12-16T20:19:10Z" id="165233405">&gt; Is reflection on this API really allowed in java 9? We should not add this if not.

@rmuir Compiled with the `9-ea+96-2015-12-10-032020.javare.4030.nc` compiler, and run on that JVM, it's still available and works.

``` json
                "using_compressed_ordinary_object_pointers": true,
                "version": "9-ea",
                "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
                "vm_vendor": "Oracle Corporation",
                "vm_version": "9-ea+96-2015-12-10-032020.javare.4030.nc"
```
</comment><comment author="nik9000" created="2015-12-16T20:32:50Z" id="165236338">LGTM if @rmuir is ok with it.
</comment><comment author="rmuir" created="2015-12-16T21:08:28Z" id="165245920">that is not a jigsaw build.
</comment><comment author="jasontedor" created="2015-12-16T21:21:26Z" id="165251611">&gt; that is not a jigsaw build.

@rmuir Oops, I had `JAVA_HOME` set to the wrong build; thanks for catching that. That said, it works against the latest Jigsaw build `9-ea+96-jigsaw-nightly-h4080-20151215`:

``` json
                "using_compressed_ordinary_object_pointers": true,
                "version": "9-ea",
                "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
                "vm_vendor": "Oracle Corporation",
                "vm_version": "9-ea+96-jigsaw-nightly-h4080-20151215"
```
</comment><comment author="rmuir" created="2015-12-16T21:55:50Z" id="165263911">OK, looks good, thanks for checking, like the other management apis we do this trick for, it is a public api (just not guaranteed in all jvms) and the reflection is correct.
</comment><comment author="jasontedor" created="2015-12-16T22:04:58Z" id="165267569">Thanks for reviewing @nik9000, @rmuir, and @polyfractal.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `index_name` back compat.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15488</link><project id="" key="" /><description>Since 2.0 we enforce that fields have the same full and index names. So in 3.x
we can remove the ability to have different names on the same field.
</description><key id="122575265">15488</key><summary>Remove `index_name` back compat.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T18:45:46Z</created><updated>2016-01-10T19:21:45Z</updated><resolved>2015-12-23T13:55:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-18T08:30:37Z" id="165713491">LGTM, this is great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>-XDELETE wouldn't delete all the indexes specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15487</link><project id="" key="" /><description>I have 550 indexes, all in the format of `ashok_dev*`, that I wanted to flush out of my system. I issued this:

```
$ curl -XDELETE http://localhost:9200/ashok_dev_*
{"acknowledged":true}
```

But I still have 459 indexes. I had to run the `-XDELETE` command about 14 times to delete all the indexes. 
</description><key id="122567897">15487</key><summary>-XDELETE wouldn't delete all the indexes specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bashok001</reporter><labels /><created>2015-12-16T18:04:41Z</created><updated>2015-12-16T20:00:46Z</updated><resolved>2015-12-16T20:00:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bashok001" created="2015-12-16T18:05:43Z" id="165196655">The number of indexes deleted in each run were varying too.
</comment><comment author="javanna" created="2015-12-16T19:35:39Z" id="165223140">Hi @bashok001 I think you are hitting #7295 which was recently fixed. The fix will be included in the upcoming 2.2 version.
</comment><comment author="bashok001" created="2015-12-16T20:00:45Z" id="165229247">@javanna That's right. Thank you for pointing it out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor issues in delimited payload token filter docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15486</link><project id="" key="" /><description>This commit addresses a few minor issues in the delimited payload token
filter docs:
- the provided example reversed the payloads associated with the
  tokens "the" and "fox"
- two additional typos in the same sentence
  - "per default" -&gt; "by default"
  - "default int to" -&gt; "default into"
- adds two serial commas
</description><key id="122567286">15486</key><summary>Fix minor issues in delimited payload token filter docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T18:00:56Z</created><updated>2015-12-16T22:05:27Z</updated><resolved>2015-12-16T20:26:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-16T18:05:39Z" id="165196641">Many thanks to @dkwasny for noticing and reporting!
</comment><comment author="nik9000" created="2015-12-16T18:17:59Z" id="165201170">LGTM
</comment><comment author="jasontedor" created="2015-12-16T22:05:27Z" id="165267690">Thanks for the quick review @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced Global checkpoints for Sequence Numbers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15485</link><project id="" key="" /><description>This is a work in progress PR for introducing Global Checkpoints into the feature/seq_no branch.  I made it to get some feedback before things are finalized / cycles are spent on docs etc. It is fairly solid, but a couple of things miss:

1) Lucene level doc rejections are not yet dealt with (I need to do some work on master).
2) Nothing is persisted yet. This will be a future PR.
3) No tests - I'll add those if we're good on the setup.
</description><key id="122558465">15485</key><summary>Introduced Global checkpoints for Sequence Numbers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label></labels><created>2015-12-16T17:17:51Z</created><updated>2016-06-06T10:53:04Z</updated><resolved>2016-06-06T10:53:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-17T10:28:21Z" id="165411927">@jasontedor  I fixed the issue we ran into yesterday where the a primary relocation target started updating the global checkpoint too early
</comment><comment author="jasontedor" created="2016-01-02T20:26:47Z" id="168426316">Conceptually this is very sound, I only have a bunch of nits and minor comments and otherwise LGTM.
</comment><comment author="bleskes" created="2016-05-30T13:08:45Z" id="222489195">@jasontedor I rebased this and added a very basic integration test. It's hard to write heavier IT tests with all the rest of the system being wired up (recoveries, engine persistency etc.). I want to got through it myself again - it's been a long time since this was written - but I think it's ready to start the review process again.
</comment><comment author="bleskes" created="2016-05-31T10:32:29Z" id="222651597">@jasontedor I went through the code again and cleaned up some minor things. It's ready for review, when you have time.
</comment><comment author="jasontedor" created="2016-06-06T03:36:56Z" id="223862247">Since this has gone through a few rounds of review before, I largely have the same thoughts I had then. I gave it a thorough re-read and left some comments.
</comment><comment author="bleskes" created="2016-06-06T10:30:46Z" id="223922804">@jasontedor thx for the review. I pushed another commit addressing it.
</comment><comment author="jasontedor" created="2016-06-06T10:45:54Z" id="223925609">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Keyed response: document explicitly for all aggs that support it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15484</link><project id="" key="" /><description>In docs the possibility of using keyed responses is mentioned here and there

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-range-aggregation.html#_keyed_response
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html#_response_format

by running a quick test on percentile aggs

```
GET my-index/_search
{
  "size": 0,
  "aggs": {

    "percentiles": {
      "percentiles": {
        "keyed":false,
        "field": "a_numeric_field",
        "percents": [
          1,
          5,
          25,
          50,
          75,
          95,
          99
        ]
      }
    }
  }
}
```

it is observable that `keyed` is also enforced in there 

```
{
  "took": 47,
  "timed_out": false,
  "_shards": {
    "total": 15,
    "successful": 15,
    "failed": 0
  },
  "hits": {
    "total": 133952,
    "max_score": 0,
    "hits": []
  },
  "aggregations": {
    "percentiles": {
      "values": [
        {
          "key": 1,
          "value": -122.41699999999999
        },
        {
          "key": 5,
          "value": -122.05740000000003
        },
        {
          "key": 25,
          "value": -122.0574
        },
        {
          "key": 50,
          "value": -122.0574
        },
        {
          "key": 75,
          "value": -78.34338576181548
        },
        {
          "key": 95,
          "value": 2.17410000000001
        },
        {
          "key": 99,
          "value": 12.649400000000016
        }
      ]
    }
  }
}
```

however no mention of keyed is made in https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-percentile-aggregation.html?q=percentiles#search-aggregations-metrics-percentile-aggregation

unless I'm missing something, is there an opportunity to have this documented clearly, perhaps just mentioning for each bucket agg if this is supported or not, not to make this a massive task?

CC @clintongormley @colings86 @markharwood @martijnvg @debadair @palecur 
</description><key id="122557818">15484</key><summary>Keyed response: document explicitly for all aggs that support it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-12-16T17:15:32Z</created><updated>2017-05-11T09:24:44Z</updated><resolved>2017-05-11T09:24:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-17T09:44:05Z" id="165400312">It is a bug that we don't have this documented for Percentiles but I am not keen in having a section for keyed response in the documentation listed which aggregation support it and which don't since a vast majority of them don't support it. 

I also am inclined to think its a bit of a useless feature anyway. In range and percentile aggregations you ask for specific buckets and so I would think you would always want to get the buckets by key (implying `keyed: true`) rather than iterating through them. For the few cases where you would want to iterate through them, most (probably all) languages allow you to iterate through maps anyway. In the histogram aggregation you actually don't know what buckets are going to be returned so getting a bucket by key doesn't make too much sense to me (if you wanted a specific bucket surely you would have used the range aggregation?) so I would say that `keyed: false` is probably the only viable option here too.

This is just my opinion though.
</comment><comment author="uboness" created="2015-12-17T12:54:14Z" id="165445628">@colings86 I don't share your opinion here. I think there are two things to take into account here:
- There should be a consistent response structure across all aggs. This enables more generic tools to build UIs on top of the aggs and treat all the aggs the same when parsing the response. IMO, Having an array of buckets as a default structure is a good choice for that.
- sometimes you do know what you're looking for and you use the aggs to get a few specific aggs. In those cases you want to have random access to them instead of writing boilerplate code that iterates over an array to find those aggs.

This is where the current support comes from - default to arrays, but you can customize the response to be keyed instead (and along with that, customize the keys for every bucket).
</comment><comment author="clintongormley" created="2015-12-18T10:28:27Z" id="165741960">I'm of the opinion that we should remove the keyed response for three reasons:
- you lose order
- you produce key names that potentially look like paths eg `99.9` and can't be indexed into elasticsearch and won't work with path syntax
- converting an array into a map is a one liner on the application side

@rashidkpc does kibana use keyed responses?
</comment><comment author="rashidkpc" created="2015-12-18T15:47:41Z" id="165811790">@clintongormley yes it does. If you choose to remove them please file a ticket on the kibana repo.
</comment><comment author="clintongormley" created="2016-01-10T20:29:21Z" id="170391144">@rashidkpc may I ask what for? ie would it be annoying to kibana to have keyed responses removed, or are you on board with this?
</comment><comment author="rashidkpc" created="2016-01-10T20:52:38Z" id="170393054">I'm fine with it, we can change how we handle the agg. I'd much rather have things be consistent. Just file a ticket if/when you do it.
</comment><comment author="uboness" created="2016-01-10T21:54:56Z" id="170397702">if everyone is onboard for removing keyed responses, I'm +1 as well. will certainly clean up some code. Hopefully not many will complain about it... lets see how it goes.
</comment><comment author="javanna" created="2017-05-05T14:58:33Z" id="299487996">I wasn't aware that we planned on removing keyed responses, and I recently got in #23758 that documented it where it was undocumented.

@clintongormley if we still plan on removing keyed responses do we want to create a new issue or relabel this one?</comment><comment author="clintongormley" created="2017-05-08T13:39:13Z" id="299869409">@colings86 what do you think?</comment><comment author="colings86" created="2017-05-11T09:24:40Z" id="300734318">The documentation for this is fixed in https://github.com/elastic/elasticsearch/pull/23758. There is stil;l a discussion to be had about whether we should have the `keyed` option but I have created a new issue to discuss this: https://github.com/elastic/elasticsearch/issues/24611</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test that cluster state task updates are batched in order of insertion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15483</link><project id="" key="" /><description>It's important that cluster state task updates are batched in order of insertion. We rely on this explicitly when a tribe node client is updating its cluster state because it only applies the last cluster state update in the batch. A test should be added to test that the tasks are actually batched in order of insertion.
</description><key id="122538591">15483</key><summary>Add test that cluster state task updates are batched in order of insertion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T16:01:27Z</created><updated>2016-01-06T19:47:59Z</updated><resolved>2016-01-06T19:47:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add post-batch method to cluster state task executors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15482</link><project id="" key="" /><description>Today, when executing cluster state update tasks, we have per-task hooks that can be used to execute logic after a cluster state update batch is processed. What is missing is a single per-batch hook that can be used to execute logic after a cluster state update batch is processed.

One place where this is a concern is in [`o.e.c.a.s.ShardStateAction`](https://github.com/elastic/elasticsearch/blob/44467df35301cb4f43e34a326c2f1fa101b50343/core/src/main/java/org/elasticsearch/cluster/action/shard/ShardStateAction.java#L151-L157) where, after a batch of shard failures are processed, there is the possibility that a reroute will be required. Without the post-batch hook, this logic is placed in the post-task hook causing reroutes when needed to be executed many times.

``` java
@Override
public void clusterStateProcessed(String source, ClusterState oldState, ClusterState newState) {
    if (oldState != newState &amp;&amp; newState.getRoutingNodes().unassigned().size() &gt; 0) {
        logger.trace("unassigned shards after shard failures. scheduling a reroute.");
        routingService.reroute("unassigned shards after shard failures, scheduling a reroute");
    }
}
```

With a post-batch hook, this logic could be executed exactly once after successful publication of the new cluster state.
</description><key id="122527549">15482</key><summary>Add post-batch method to cluster state task executors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label></labels><created>2015-12-16T15:23:47Z</created><updated>2015-12-17T13:31:03Z</updated><resolved>2015-12-17T13:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Segment merges should try to avoid exhausting all free disk space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15481</link><project id="" key="" /><description>When a Lucene segment merge runs, it needs sizable free temporary disk space to do its work.  If it runs out of disk space part way through, the OS rejects the `write` call, and we'll hit a "tragic" exception that will fail that shard.

This behavior is particularly evil because on filling up the disk, the merge will then go and remove all temp files, thus freeing up (sizable) disk space and provoking issues like #15420 where "at first it failed and now it succeeded" leads to corruption.

Instead, we should avoid kicking off the merge, up front, if there isn't enough space?  I think we need some simple "reservation" API where the merge can claim how much space it expects to use, and fail at that point if there isn't enough space.

Probably we could do this with a `Directory` wrapper that 1) uses Java's APIs to monitor free space periodically, and 2) adds this new reservation API.  This wrapper should then throw the "disk full" `IOException` long before the underlying disk actually fills up.  We would need to wrap `IndexOutput` as well, because the up front estimate could be off, and of course other things (merges from other shards, users copying files around, etc.) can concurrently fill up the disk.

I think much of this improvement could be done in Lucene, with ES customizing the "reserve" method to be aware of N shards that are all reserving space for their merges on one node?  Lucene's default `TieredMergePolicy` already estimates the size of the merged segment to enforce its `maxMergedSegmentMB`...

Or maybe there's a simpler approach?
</description><key id="122525016">15481</key><summary>Segment merges should try to avoid exhausting all free disk space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>PITA</label></labels><created>2015-12-16T15:13:41Z</created><updated>2016-01-15T16:39:42Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-16T15:55:26Z" id="165153587">&gt; Instead, we should avoid kicking off the merge, up front, if there isn't enough space

This is a pretty good start, I think. Its also probably worth loudly complaining about how we can't start the merge because of free space. I don't know how to prevent spamming the logs with it, maybe eating more free space.

I don't know what we can do about not having enough space though.

&gt; "reserve" method

Elasticsearch already does some reservation math when it copies shards from one node to another. I suspect having a common abstraction that we can plug in a few places would allow restores to play well with merges.
</comment><comment author="jpountz" created="2015-12-18T13:44:55Z" id="165781462">Do we need to do anything here? Regarding #15420 the bug was in the translog, not due to merges? And it's actually a good thing that this behaviour exposed #15420, otherwise we might have never caught the bug.
</comment><comment author="mikemccand" created="2015-12-21T19:37:48Z" id="166399505">It is indeed nice that merges will go and fill up disk space for us, provoking nasty disk-full bugs that otherwise wouldn't be found, but it would be best if our tests could properly simulate random disk full situations, using the mock filesystems from Lucene.

Maybe, we shouldn't fix this until we have better disk-full testing in place?
</comment><comment author="rmuir" created="2015-12-21T20:04:47Z" id="166404767">I am -1 to stopping merging because space runs out. Failing the merge is the right thing to do.

I do not think we should add "reserve space" in lucene unless we back it by a method in the JDK that actually reserves space (http://pubs.opengroup.org/onlinepubs/009695399/functions/posix_fallocate.html or similar), and lucene does not need this since we only rely on atomic rename always. The problem is not lucene corrupting anything here.

As far as testing, I still have a disk full branch if anyone wants to help: https://github.com/apache/lucene-solr/compare/trunk...rmuir:diskfull

Testing this well is easy with something like lucene that uses a very contained set of the java filesystem api and is only write-once, it needs lots of improvements to implement the other apis. For example, lucene does not even use FileChannel for write, just for locking. But this is more complex than an outputstream for testing this.
</comment><comment author="rmuir" created="2015-12-21T20:14:56Z" id="166406595">By the way, if we want a best effort check to fail earlier, lets put it somewhere in e.g. ElasticsearchMergePolicy to check estimated size against filestore.

To me this is very different from adding i/o methods to lucene's Directory API that do not work.
</comment><comment author="mikemccand" created="2015-12-22T00:32:56Z" id="166466690">&gt; Failing the merge is the right thing to do.

+1: I think failing the shard (that's pointing to a data path that is too full to do the requested merge) is the right thing to do.  In Lucene, I think closing the `IndexWriter` with a tragic exception...

&gt; I do not think we should add "reserve space" in lucene unless we back it by a method in the JDK that actually reserves space 

`RAF.setLength` doesn't `fallocate` right (it will create sparse files)?  Lucene used to call this API when building compound files I think :)

Is there any way to `fallocate` from Java now?

&gt; lets put it somewhere in e.g. ElasticsearchMergePolicy to check estimated size against filestore.

OK, I'll explore doing this only in Elasticsearch: I agree it's the easy way out, but I don't think it's the correct approach.  I.e., other Lucene users also don't want large merges to use up all free disk space.
</comment><comment author="rmuir" created="2015-12-22T06:06:02Z" id="166520733">&gt; RAF.setLength doesn't fallocate right (it will create sparse files)? Lucene used to call this API when building compound files I think :)
&gt; 
&gt; Is there any way to fallocate from Java now?

Nothing in java calls it.

&gt; OK, I'll explore doing this only in Elasticsearch: I agree it's the easy way out, but I don't think it's the correct approach. I.e., other Lucene users also don't want large merges to use up all free disk space.

Why? because their code is broken on disk-full and they would rather hide from that? That's not lucene's job, its a search engine library. We should just deliver the exception!
</comment><comment author="mikemccand" created="2016-01-15T16:39:42Z" id="172010224">Both @jpountz and @rmuir seem against making any change to ES's or Lucene's behavior when disk full is approaching ... I'm going to close this as "won't fix" for now.

We can revisit it when we have better test coverage of disk full situations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make mapping serialization more robust.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15480</link><project id="" key="" /><description>When creating a metadata mapper for a new type, we reuse an existing
configuration from an existing type (if any) in order to avoid introducing
conflicts. However this field type that is provided is considered as both an
initial configuration and the default configuration. So at serialization time,
we might only serialize the difference between the current configuration and
this default configuration, which might be different to what is actually
considered the default configuration.

This does not cause bugs today because metadata mappers usually override the
toXContent method and compare the current field type with Defaults.FIELD_TYPE
instead of defaultFieldType() but I would still like to do this change to
avoid future bugs.
</description><key id="122524356">15480</key><summary>Make mapping serialization more robust.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T15:10:12Z</created><updated>2016-01-10T21:05:30Z</updated><resolved>2015-12-17T10:23:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-16T17:06:08Z" id="165176860">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw exception when trying to write map with null keys</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15479</link><project id="" key="" /><description>Throw exception when trying to write map with null keys in XContentBuilder, otherwise jackson will break and return a cryptic error message.

Closes #14346
</description><key id="122523772">15479</key><summary>Throw exception when trying to write map with null keys</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T15:07:17Z</created><updated>2015-12-16T21:06:10Z</updated><resolved>2015-12-16T16:04:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-16T15:14:59Z" id="165137391">LGTM
</comment><comment author="andrewvc" created="2015-12-16T21:06:10Z" id="165244895">Thanks @javanna !!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch logs full of errors since 2.1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15478</link><project id="" key="" /><description>Hi,

Since I've upgraded my cluster to Elasticsearch 2.1.0, the Elasticsearch logs are full of those lines:

```
[2015-12-14 14:04:46,433][WARN ][indices.cluster          ] [ELK-211] [[syslog-2015.12.14][2]] marking and sending shard failed due to [engine failure, reason [refresh failed]]
java.lang.NullPointerException
[2015-12-14 14:04:48,309][DEBUG][action.admin.indices.stats] [ELK-211] [indices:monitor/stats] failed to execute operation for shard [[syslog-2015.12.14][2], node[1HVMXYCeTVSehoM7jTEu-g], [R], v[5], s[INITIALIZING], a[id=A1kOQfL9Timtcw9k5Y3loQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-14T19:04:46.474Z], details[engine failure, reason [refresh failed], failure NullPointerException[null]]], expected_shard_size[16416442009]]
[syslog-2015.12.14][[syslog-2015.12.14][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: ShardNotFoundException[no such shard];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [syslog-2015.12.14][[syslog-2015.12.14][2]] ShardNotFoundException[no such shard]
        at org.elasticsearch.index.IndexService.shardSafe(IndexService.java:198)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:98)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-14 14:04:53,271][DEBUG][action.admin.indices.stats] [ELK-211] [indices:monitor/stats] failed to execute operation for shard [[syslog-2015.12.14][2], node[1HVMXYCeTVSehoM7jTEu-g], [R], v[5], s[INITIALIZING], a[id=A1kOQfL9Timtcw9k5Y3loQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-14T19:04:46.474Z], details[engine failure, reason [refresh failed], failure NullPointerException[null]]], expected_shard_size[16416442009]]
[syslog-2015.12.14][[syslog-2015.12.14][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [syslog-2015.12.14][[syslog-2015.12.14][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
```

It looks like there's a problem in Elasticsearch's code...

Regards,

David
</description><key id="122507667">15478</key><summary>Elasticsearch logs full of errors since 2.1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davmrtl</reporter><labels /><created>2015-12-16T13:54:37Z</created><updated>2015-12-18T09:43:07Z</updated><resolved>2015-12-16T14:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T14:03:09Z" id="165114213">Fixed by https://github.com/elastic/elasticsearch/pull/15202
</comment><comment author="davmrtl" created="2015-12-16T14:14:41Z" id="165119519">When is the ETA for v2.1.1?
</comment><comment author="jasontedor" created="2015-12-18T09:43:07Z" id="165730889">It's harmless debug logging, nothing is wrong. This has been addressed in #14950 to remedy the excess logging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.4.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15477</link><project id="" key="" /><description /><key id="122498066">15477</key><summary>Upgrade to lucene-5.4.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>review</label><label>upgrade</label><label>v2.2.0</label></labels><created>2015-12-16T12:57:13Z</created><updated>2015-12-17T19:54:47Z</updated><resolved>2015-12-16T13:36:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-16T13:07:00Z" id="165099087">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Fix ScriptServiceTests.testFineGrainedSettings that can loop indefinitely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15476</link><project id="" key="" /><description>The test tries to generate 10 distinct settings out of 9 possible combinations and thus goes into endless loop.

Build failure on Jenkins:

http://build-us-00.elastic.co/job/es_core_master_centos/9045/testReport/junit/org.elasticsearch.script/ScriptServiceTests/testFineGrainedSettings/

Seed to reproduce: `-Dtests.seed=BA6A5AFDEC50C44F`
</description><key id="122473429">15476</key><summary>[TEST] Fix ScriptServiceTests.testFineGrainedSettings that can loop indefinitely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>test</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T10:34:26Z</created><updated>2015-12-16T17:11:29Z</updated><resolved>2015-12-16T17:05:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-16T12:54:01Z" id="165096490">LGTM, left one minor comment
</comment><comment author="jpountz" created="2015-12-16T15:12:21Z" id="165136751">LGTM I just ran into it locally
</comment><comment author="ywelsch" created="2015-12-16T16:40:16Z" id="165169637">@javanna I pushed another commit that makes explicit that the test only uses a single ScriptEngineService. As the change from 2 script engine services to 1 was only done on master, I think the fix here should go only to master as well.
</comment><comment author="javanna" created="2015-12-16T16:43:19Z" id="165170467">LGTM @ywelsch if we have two script engines in 2.x seems like the randomIntBetween will have to take that into account too right?
</comment><comment author="ywelsch" created="2015-12-16T17:05:38Z" id="165176711">Yes, I will fix 2.x and below separately.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Beef up TranslogTests with concurrent fatal exceptions test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15475</link><project id="" key="" /><description>Today we only test this when writing sequentially. Yet, in practice we mainly
write concurrently, this commit adds a test that tests that concurrent writes with
sudden fatal failure will not corrupt our translog.

Relates to #15420
</description><key id="122462122">15475</key><summary>Beef up TranslogTests with concurrent fatal exceptions test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label><label>v2.0.1</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T09:28:49Z</created><updated>2015-12-16T11:43:56Z</updated><resolved>2015-12-16T11:43:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-16T09:29:19Z" id="165045521">@bleskes can you quickly look at the small change in `Translog.java`
</comment><comment author="bleskes" created="2015-12-16T10:01:49Z" id="165053864">LGTM. Great test. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use configured GPG key as local user / default key when signing packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15474</link><project id="" key="" /><description>This commit passes the configured GPG_KEY_ID to the sign command otherwise the default key will be used which might be different to the configured key.

@spinscale can you take a look? -- this is only relevant against 1.7
</description><key id="122458727">15474</key><summary>Use configured GPG key as local user / default key when signing packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label></labels><created>2015-12-16T09:09:07Z</created><updated>2015-12-17T16:24:08Z</updated><resolved>2015-12-17T16:24:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-12-16T14:51:47Z" id="165131750">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update sample in sort for consistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15473</link><project id="" key="" /><description>Match latitude and longitude based on previous samples.
</description><key id="122456966">15473</key><summary>Update sample in sort for consistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmluy</reporter><labels><label>docs</label></labels><created>2015-12-16T08:59:44Z</created><updated>2015-12-16T11:52:36Z</updated><resolved>2015-12-16T11:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T11:52:36Z" id="165082033">thanks @jmluy - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Evaluate and trim down the thread pools of a TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15472</link><project id="" key="" /><description>At the moment, for simplicity, the ThreadPool used by the TransportClient is the same as a full blown node. I.e., we have pools for things like BULK , SEARCH  which are not relevant.  Since we create threads lazily it's typically not a big deal as we will not spawn too many threads. However, it is error prone and conveys the wrong message to people extending functionality around the client like a BulkProcessor.  IMO we should have a trimmed down set.
</description><key id="122456754">15472</key><summary>Evaluate and trim down the thread pools of a TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Java API</label><label>discuss</label><label>enhancement</label></labels><created>2015-12-16T08:58:47Z</created><updated>2016-02-15T16:55:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-16T09:11:33Z" id="165041466">+1 - we might go even further and move all the threadpool stuff into a module and allow only this module to spawn threads etc? I know this is not what this issue is for but I wanted to get the idea out as @rmuir mentioned it to me a couple of times and I think it makes sense
</comment><comment author="bleskes" created="2015-12-16T09:43:43Z" id="165049732">Sounds good to me

On 16 dec. 2015 10:12 AM +0100, Simon Willnauernotifications@github.com, wrote:

&gt; +1 - we might go even further and move all the threadpool stuff into a module and allow only this module to spawn threads etc? I know this is not what this issue is for but I wanted to get the idea out as@rmuir(https://github.com/rmuir)mentioned it to me a couple of times and I think it makes sense
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/issues/15472#issuecomment-165041466).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bucket selector script is not applied to empty histogram buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15471</link><project id="" key="" /><description>Consider this dataset:

```
POST _bulk
{"index": { "_index": "aggtest", "_type": "default" }}
{"a": 1, "b": 1}
{"index": { "_index": "aggtest", "_type": "default" }}
{"a": 1, "b": 2}
{"index": { "_index": "aggtest", "_type": "default" }}
{"a": 3, "b": 1}
{"index": { "_index": "aggtest", "_type": "default" }}
{"a": 3, "b": 3}
```

Run this aggregation on it:

```
POST aggtest/default/_search?size=0
{
  "aggs": {
    "by_a": {
      "histogram": {
        "field": "a",
        "interval": 1
      },
      "aggs": {
        "by_b": {
          "range": {
            "field": "b",
            "ranges": [
              { "from": 1, "to": 2 },
              { "from": 2, "to": 3 },
              { "from": 3, "to": 4 }
            ]
          },
          "aggs": {
            "the_filter": {
              "bucket_selector": {
                "buckets_path": {
                  "the_doc_count": "_count"
                },
                "script": {
                  "inline": "the_doc_count &gt; 0",
                  "lang": "expression"
                }
              }
            }
          }
        }
      }
    }
  }
}
```

This is the result. Note how the empty buckets of `by_b` are not removed by the script when the parent bucket of `by_a` is empty.

![Result](https://cloud.githubusercontent.com/assets/1188538/11832268/fb045e06-a3b6-11e5-8623-677fe47f79c3.png)
</description><key id="122427526">15471</key><summary>Bucket selector script is not applied to empty histogram buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">AndreKR</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-12-16T04:38:36Z</created><updated>2015-12-18T14:50:52Z</updated><resolved>2015-12-18T14:43:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T11:47:29Z" id="165081017">@colings86 could you take a look please?
</comment><comment author="colings86" created="2015-12-18T14:50:52Z" id="165795481">@AndreKR thanks for raising this, I have merged a PR with the fix and it should be available from version 2.2. In the meantime a workaround could be to set min_doc_count on the histogram to 1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose query/fetch timing to IndexEventListener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15470</link><project id="" key="" /><description>Elasticsearch collects a limited number of stats that can be collected in a very
performant manner. Other stats, such as latency percentiles, are perhaps out of
scope of the main elasticsearch project. This allows plugins to add their own
stat collection routines to the existing shard stats collection.

Closes #15412
</description><key id="122423862">15470</key><summary>Expose query/fetch timing to IndexEventListener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ebernhardson</reporter><labels><label>:Plugins</label><label>:Stats</label><label>discuss</label></labels><created>2015-12-16T04:00:57Z</created><updated>2016-06-24T10:06:41Z</updated><resolved>2016-06-24T10:06:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ebernhardson" created="2015-12-16T04:01:45Z" id="164985603">I wasn't sure if it would be better to attach to the existing IndexEventListener, or to create a new ShardSearchStatsEventListener. This first method is a much less involved patch so I started here, but I can adjust if desired.
</comment><comment author="ebernhardson" created="2015-12-16T04:07:14Z" id="164986717">I've now reviewed and signed the provided CLA
</comment><comment author="clintongormley" created="2015-12-16T11:42:23Z" id="165079552">thanks for the PR @ebernhardson.  This may be useful in itself, but I'd also like to get feedback on #15412 from others to figure out whether we should be providing percentiles out of the box.
</comment><comment author="bleskes" created="2016-06-24T10:06:41Z" id="228307948">@ebernhardson thx for submitting this again. In the meantime, it looks like it was superseded by https://github.com/elastic/elasticsearch/pull/17398 . I'm therefore going to close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update scripting documentation to provide examples using true/false vs. on/off</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15469</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_setting_changes.html#migration-script-settings

https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html#enable-dynamic-scripting

```
script.inline: on
script.indexed: on
```

It may be nicer to provide examples using true/false vs. on/off for the new script options to be consistent with the other boolean settings that we have.
</description><key id="122421853">15469</key><summary>Update scripting documentation to provide examples using true/false vs. on/off</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Scripting</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-16T03:48:59Z</created><updated>2016-02-15T16:55:13Z</updated><resolved>2016-02-15T16:55:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T11:39:33Z" id="165078141">@ppf2 It is not a boolean setting, it is a ternary: `on`, `off`, `sandbox`.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html#enable-dynamic-scripting

That said, we no longer have any sandboxed scripting engines.  Wondering if we should change this to boolean?
</comment><comment author="rmuir" created="2015-12-16T12:00:08Z" id="165085186">I would love it if we defaulted to 'off', and 'on' acted like 'sandbox'.
</comment><comment author="clintongormley" created="2015-12-18T10:34:45Z" id="165743433">Discussed in FixItFriday:  switch to `true` / `false`, where `true` means sandbox if there is one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master should wait on cluster state publication when failing a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15468</link><project id="" key="" /><description>When a client sends a request to fail a shard to the master, the current
behavior is that the master will submit the cluster state update task
and then immediately send a successful response back to the client;
additionally, if there are any failures while processing the cluster
state update task to fail the shard, then the client will never be
notified of these failures.

This commit modifies the master behavior when handling requests to fail
a shard. In particular, the master will now wait until successful
publication of the cluster state update before notifying the request
client that the shard is marked as failed; additionally, the client is
now notified of any failures during the execution of the cluster state
update task.

Relates #14252 
</description><key id="122417135">15468</key><summary>Master should wait on cluster state publication when failing a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T03:05:45Z</created><updated>2015-12-16T15:39:43Z</updated><resolved>2015-12-16T15:39:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-16T12:45:17Z" id="165095034">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Organize and annotate gitignores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15467</link><project id="" key="" /><description /><key id="122415753">15467</key><summary>Organize and annotate gitignores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label><label>v2.0.2</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T02:53:18Z</created><updated>2015-12-16T19:57:34Z</updated><resolved>2015-12-16T19:57:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-16T09:33:55Z" id="165046356">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>integration tests doc example out of date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15466</link><project id="" key="" /><description>[Documentation about changing node settings](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/integration-tests.html#changing-node-settings) in integration tests need some update.
the `settingsBuilder()` method is not accessible from ESIntegTestCase child classes.

Here is a workaround to add some custom node settings that work with 2.1 sourcecode

```
public class Mytests extends ESIntegTestCase {
  @Override
  protected Settings nodeSettings(int nodeOrdinal) {
      return Settings.settingsBuilder().put(super.nodeSettings(nodeOrdinal))
             .put("node.mode", "network")
             .build();
  }

}
```
</description><key id="122410397">15466</key><summary>integration tests doc example out of date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martin-a</reporter><labels><label>discuss</label><label>docs</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T02:00:09Z</created><updated>2015-12-18T07:24:33Z</updated><resolved>2015-12-18T07:23:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T11:43:49Z" id="165080243">@rjernst could you take a look at this please?
</comment><comment author="rjernst" created="2015-12-18T07:23:22Z" id="165700129">I pushed a fix, the docs were assuming `Settings.setttingsBuilder` was statically imported. We should also really just get rid of `settingsBuilder()`, as it is the exact same as `builder()`...

Master: 2093ea5
2.x: f545b6d
2.1: c9d765f
2.0: 1c094d3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apparent degradation in handling of large concurrent index job on update from 1.3.4 to 2.1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15465</link><project id="" key="" /><description>Scoping out potential issues from upgrading from 1.3.4 to 2.1.0 since 1.3.4 is swiftly approaching end of life.

I am standing up identical spec 1.3.4 and 2.1.0 single node clusters, and attempting to reindex an existing index from the current staging environment into them(doing this to both to see how the versions compare when being overloaded as well as to get some test data into them). I am launching a spark job which uses elasticsearch-hadoop-2.2.0-m1 and is indexing into it using 16 executors.

To my surprise, the 1.3.4 index got about 200 times more data into it before the index job failed, compared to the 2.1.0 index. While indexing into both seemed to face error conditions along the way(will post errors below), the 2.1.0 index job seemed to fail almost immediately while the 1.3.4 seems to attempt recovery and continue indexing for quite some time before failing.

Recurring 1.3.4 error(seen repeatedly, but indexing continues):

```
15/12/15 16:33:51 ERROR Executor: Exception in task 8.0 in stage 0.0 (TID 8)
org.elasticsearch.hadoop.EsHadoopException: Could not write all entries [6/1047872] (maybe ES was overloaded?). Bailing out...
    at org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:239)
    at org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:176)
    at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:152)
    at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:49)
    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)
    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```

Error in 2.1.0(fails almost immediately after encountering):

```
15/12/15 16:58:29 ERROR Executor: Exception in task 8.0 in stage 0.0 (TID 8)
org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: Found unrecoverable error [Too Many Requests(429) - [rejected execution of org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1@250a2a1c on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@75bb8775[Running, pool size = 2, active threads = 2, queued tasks = 50, completed tasks = 0]]]]; Bailing out..
    at org.elasticsearch.hadoop.rest.RestClient.retryFailedEntries(RestClient.java:215)
    at org.elasticsearch.hadoop.rest.RestClient.bulk(RestClient.java:169)
    at org.elasticsearch.hadoop.rest.RestRepository.tryFlush(RestRepository.java:214)
    at org.elasticsearch.hadoop.rest.RestRepository.flush(RestRepository.java:237)
    at org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:176)
    at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:152)
    at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:49)
    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)
    at org.elasticsearch.spark.rdd.EsSpark$$anonfun$saveToEsWithMeta$1.apply(EsSpark.scala:86)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
```
</description><key id="122406273">15465</key><summary>Apparent degradation in handling of large concurrent index job on update from 1.3.4 to 2.1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hronik1</reporter><labels /><created>2015-12-16T01:25:46Z</created><updated>2016-12-31T05:06:05Z</updated><resolved>2015-12-16T02:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-16T02:15:17Z" id="164964616">&gt; single node clusters

From the bulk thread pool having `pool size = 2` (in your second exception message) we can deduce that this a very small node, a system presenting itself as having two cores it appears?

&gt; indexing into it using 16 executors

From your description, I'm assuming that these are all operating concurrently. This is basically the perfect setup for an overwhelmed node.

On 1.3.4:

&gt; `maybe ES was overloaded?`

On 2.1.0:

&gt; `Too Many Requests(429) - [rejected execution... queue capacity = 50...queued tasks = 50...]`

In both cases, the exception messages are telling you that Elasticsearch can't keep up, and this doesn't strike me as surprising at all given that you have sixteen workers and two consumers. Elasticsearch is trying to apply backpressure to the job because its bulk queue is stuffed.

Given this, I don't see an issue to be addressed.
</comment><comment author="hronik1" created="2015-12-16T07:06:43Z" id="165018485">Jason yes, as I pointed out earlier in the post, the purpose is to test how
the cluster handles when it is overwhelmed.

I am aware that those messages are pointing out the fact the node is being
overwhelmed, that is not my point of confusion. The point I am trying to
prove is that this appears to be a regression, since 1.3.4 consistently
appears to be doing a much better job at successfully applying back
pressure and indexing data into a node when in an overwhelmed stated, while
2.1.0 appears to immediately just fall over.

On Tue, Dec 15, 2015 at 6:16 PM, Jason Tedor notifications@github.com
wrote:

&gt; Closed #15465
&gt; https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_elastic_elasticsearch_issues_15465&amp;d=BQMCaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=MXoI1K0dgBQpc4jGuqgr9Y3z8gHgOxW8KepkbfZ_AUQ&amp;m=c72KgSl49JqX8kvbhBn_uRhB-H40_UuYIUd3BLYX564&amp;s=s9vXNlje2trC2po-JWNS6rlMASznP6b1bVimaOLkPT8&amp;e=
&gt; .
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_elastic_elasticsearch_issues_15465-23event-2D492985248&amp;d=BQMCaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=MXoI1K0dgBQpc4jGuqgr9Y3z8gHgOxW8KepkbfZ_AUQ&amp;m=c72KgSl49JqX8kvbhBn_uRhB-H40_UuYIUd3BLYX564&amp;s=nVmA2fUSBd8Ppbo10FMgBFehs4FQUIDw72mKikHbTYk&amp;e=
&gt; .
</comment><comment author="clintongormley" created="2015-12-16T11:50:12Z" id="165081429">It's not falling over. It is rejecting requests (telling the client to back off) so that it doesn't fall over and consume all resources, depriving other tasks like eg search from working.
</comment><comment author="hronik1" created="2015-12-16T18:28:56Z" id="165204294">Clinton, yes my apologies for the misuse of fallover.

I am aware of why the reject requests are happening, as well as their
purpose. My confusion, and underlying issue I am trying to get at is why
the older version appears to be able to index substantially more(200x, 15
million vs. 85 thousand) documents before reaching a state where they
completely stop indexing altogether. If this was a trivial difference, I
would not be so concerned/confused, however a 200x difference is not
trivial.

On Wed, Dec 16, 2015 at 3:51 AM, Clinton Gormley notifications@github.com
wrote:

&gt; It's not falling over. It is rejecting requests (telling the client to
&gt; back off) so that it doesn't fall over and consume all resources, depriving
&gt; other tasks like eg search from working.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_elastic_elasticsearch_issues_15465-23issuecomment-2D165081429&amp;d=BQMCaQ&amp;c=8hUWFZcy2Z-Za5rBPlktOQ&amp;r=MXoI1K0dgBQpc4jGuqgr9Y3z8gHgOxW8KepkbfZ_AUQ&amp;m=PQQfiVwHMkmZM6gM2z7Q0PaPnrEGjxnIYjsc3nrYx0A&amp;s=SqYeWYuzRQ_5OgEZ1ZG_kr0zlf1Da8WgkQnm5AkYdaE&amp;e=
&gt; .
</comment><comment author="jasontedor" created="2015-12-16T22:08:49Z" id="165268568">@hronik1 I understand what you're saying, but sixteen producers against two consumers is so far beyond the intended usage of Elasticsearch and the bulk API (which sits behind the elasticsearch-hadoop implementation) that it is not a situation to engineer for.
</comment><comment author="AshisPF" created="2016-12-31T05:05:04Z" id="269849777">increase the ulimit on the consumer .This problem is due to too may files opening which by default is limited to 1048 number of files

Use a text editor and open the /etc/initscript file and add the following lines to the file:
ulimit -Hn 65535
ulimit -Sn 65535
eval exec "$4"

Save the file.
If you start the server as a regular process
Use a text editor and open the /etc/security/limits.conf file.
Increase the upper limit on the number of file descriptors by adding the following lines to the file:

* soft    nofile          65535
* hard    nofile          65535

Save the file.
Restart the Linux server for the operating system change to take effect for all processes.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add plumbing for script compile-time parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15464</link><project id="" key="" /><description>Supports parameters necessary for Plan A scripts at compile time.
</description><key id="122403755">15464</key><summary>Add plumbing for script compile-time parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Scripting</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-16T01:04:05Z</created><updated>2016-01-10T21:10:22Z</updated><resolved>2015-12-17T04:02:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-16T02:10:36Z" id="164964014">Just scanned it but lgtm. I thought we had some nice (simple) Boolean
parser somewhere. Something strict.
On Dec 15, 2015 8:04 PM, "Jack Conradson" notifications@github.com wrote:

&gt; ## Supports parameters necessary for Plan A scripts at compile time.
&gt; 
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;   https://github.com/elastic/elasticsearch/pull/15464
&gt; Commit Summary
&gt; - Add plumbing for script compile-time parameters.
&gt; 
&gt; File Changes
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/action/admin/cluster/validate/template/TransportRenderSearchTemplateAction.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-0 (4)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/action/update/UpdateHelper.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-1 (3)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/index/query/QueryShardContext.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-2 (3)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/index/query/ScriptQueryBuilder.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-3 (5)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/index/query/functionscore/script/ScriptScoreFunctionBuilder.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-4 (5)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/script/NativeScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-5 (2)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/script/ScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-6 (2)
&gt; - _M_ core/src/main/java/org/elasticsearch/script/ScriptService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-7
&gt;   (78)
&gt; - _M_ core/src/main/java/org/elasticsearch/search/SearchService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-8 (5)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/aggregations/bucket/significant/heuristics/ScriptHeuristic.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-9 (5)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/InternalScriptedMetric.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-10
&gt;   (3)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/aggregations/metrics/scripted/ScriptedMetricAggregator.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-11
&gt;   (7)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketscript/BucketScriptPipelineAggregator.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-12
&gt;   (2)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/aggregations/pipeline/having/BucketSelectorPipelineAggregator.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-13
&gt;   (3)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/aggregations/support/ValuesSourceParser.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-14
&gt;   (3)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/fetch/script/ScriptFieldsParseElement.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-15
&gt;   (5)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-16
&gt;   (3)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/search/suggest/phrase/PhraseSuggestParser.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-17
&gt;   (3)
&gt; - _M_ core/src/test/java/org/elasticsearch/script/FileScriptTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-18
&gt;   (4)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/script/NativeScriptTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-19
&gt;   (7)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/script/ScriptContextTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-20
&gt;   (14)
&gt; - _M_ core/src/test/java/org/elasticsearch/script/ScriptModesTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-21
&gt;   (2)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/script/ScriptServiceTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-22
&gt;   (45)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/search/aggregations/bucket/DateHistogramIT.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-23
&gt;   (4)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/search/aggregations/metrics/AvgIT.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-24
&gt;   (6)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/search/aggregations/metrics/SumIT.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-25
&gt;   (6)
&gt; - _M_
&gt;   core/src/test/java/org/elasticsearch/search/aggregations/metrics/ValueCountIT.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-26
&gt;   (4)
&gt; - _M_ core/src/test/java/org/elasticsearch/update/UpdateIT.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-27
&gt;   (8)
&gt; - _M_
&gt;   modules/lang-expression/src/main/java/org/elasticsearch/script/expression/ExpressionScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-28
&gt;   (2)
&gt; - _M_
&gt;   modules/lang-expression/src/test/java/org/elasticsearch/script/expression/ExpressionTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-29
&gt;   (10)
&gt; - _M_
&gt;   modules/lang-groovy/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-30
&gt;   (2)
&gt; - _M_
&gt;   modules/lang-groovy/src/test/java/org/elasticsearch/script/groovy/GroovySecurityTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-31
&gt;   (6)
&gt; - _M_
&gt;   modules/lang-mustache/src/main/java/org/elasticsearch/script/mustache/MustacheScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-32
&gt;   (2)
&gt; - _M_
&gt;   modules/lang-mustache/src/test/java/org/elasticsearch/script/mustache/MustacheScriptEngineTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-33
&gt;   (5)
&gt; - _M_
&gt;   plugins/lang-javascript/src/main/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-34
&gt;   (6)
&gt; - _M_
&gt;   plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptEngineTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-35
&gt;   (25)
&gt; - _M_
&gt;   plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptScriptMultiThreadedTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-36
&gt;   (7)
&gt; - _M_
&gt;   plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/JavaScriptSecurityTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-37
&gt;   (13)
&gt; - _M_
&gt;   plugins/lang-javascript/src/test/java/org/elasticsearch/script/javascript/SimpleBench.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-38
&gt;   (3)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/main/java/org/elasticsearch/plan/a/PlanAScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-39
&gt;   (29)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowDisabledTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-40
&gt;   (11)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/FloatOverflowEnabledTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-41
&gt;   (11)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowDisabledTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-42
&gt;   (11)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/IntegerOverflowEnabledTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-43
&gt;   (11)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptEngineTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-44
&gt;   (5)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/ScriptTestCase.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-45
&gt;   (17)
&gt; - _M_
&gt;   plugins/lang-plan-a/src/test/java/org/elasticsearch/plan/a/WhenThingsGoWrongTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-46
&gt;   (11)
&gt; - _M_
&gt;   plugins/lang-python/src/main/java/org/elasticsearch/script/python/PythonScriptEngineService.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-47
&gt;   (6)
&gt; - _M_
&gt;   plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptEngineTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-48
&gt;   (19)
&gt; - _M_
&gt;   plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonScriptMultiThreadedTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-49
&gt;   (5)
&gt; - _M_
&gt;   plugins/lang-python/src/test/java/org/elasticsearch/script/python/PythonSecurityTests.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-50
&gt;   (15)
&gt; - _M_
&gt;   plugins/lang-python/src/test/java/org/elasticsearch/script/python/SimpleBench.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-51
&gt;   (3)
&gt; - _M_
&gt;   test-framework/src/main/java/org/elasticsearch/script/MockScriptEngine.java
&gt;   https://github.com/elastic/elasticsearch/pull/15464/files#diff-52
&gt;   (2)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elastic/elasticsearch/pull/15464.patch
&gt; - https://github.com/elastic/elasticsearch/pull/15464.diff
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/15464.
</comment><comment author="jdconrad" created="2015-12-17T04:03:07Z" id="165331634">@nik9000 Thanks for the review.  I know @rmuir did create an actual github project for a strict boolean parser, but I wasn't aware of one in ES.  If there is I can follow up in another PR.
</comment><comment author="uboness" created="2015-12-17T13:21:29Z" id="165451858">@jdconrad we should backport this to 2.x as well (/cc @rmuir)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rename minimum_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15463</link><project id="" key="" /><description>Currently when explaining and talking about `minimum_master_nodes` setting, we always clarify that it is really minimum number of **eligible** master nodes. In the spirit of self-documenting settings, this is a suggestion to rename the setting to `minimum_eligible_master_nodes` and deprecate the current setting name.
</description><key id="122401762">15463</key><summary>rename minimum_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Settings</label><label>breaking</label><label>discuss</label></labels><created>2015-12-16T00:47:01Z</created><updated>2016-03-16T00:07:36Z</updated><resolved>2016-03-15T23:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-16T01:15:35Z" id="164952304">perhaps `minimum_electable_nodes` would be a consideration also?
</comment><comment author="ywelsch" created="2015-12-16T10:47:00Z" id="165063735">I think this should go hand in hand with renaming the setting `node.master` to `node.master_eligible`. Also the `zen` prefix should be removed from `discovery.zen.minimum_master_nodes`.
</comment><comment author="markharwood" created="2015-12-18T10:37:28Z" id="165743875">Also the comments in the elasticsearch.yml talks about "nodes" not "master-eligible nodes"

```
# Prevent the "split brain" by configuring the majority of nodes (total number of nodes / 2 + 1):
```
</comment><comment author="jasontedor" created="2016-02-26T06:20:32Z" id="189128903">I discussed this in person with a few people. My thoughts are that we should _not_ make any changes here. Yes, the name could be better but the reason is because we lose search engine history if we change the name. This is an _extremely_ important setting with a long history and if the name is changed then the past literature on the setting will not appear in searches. For a setting like this, I think that is far more valuable than a semantically perfect name.
</comment><comment author="djschny" created="2016-02-26T11:56:11Z" id="189242921">That's a good point that I had not thought of. Based upon that how about a hybrid approach?
- update comments in the default 
- instead of renaming and making it a breaking change, accept both properties
- update `elasticsearch.yml`to have the new property name in by default
- on the document page, make sure the old value stays on the page so that google search history is maintained

Just brainstorming ways to have the cake and eat it too.
</comment><comment author="jasontedor" created="2016-03-15T23:59:38Z" id="197073135">&gt; Based upon that how about a hybrid approach?

I'm afraid that it still has the risk of fracturing the knowledge base. People have to be aware to search for both settings, and if you only search for the new setting than you don't see the accumulated knowledge for the old setting.

&gt; instead of renaming and making it a breaking change, accept both properties

Unfortunately I think this has the potential to cause more harm then good. What if both settings are present, and the value is different? Well, of course we should probably throw an exception, but why are we are adding all this extra code?

&gt; Just brainstorming ways to have the cake and eat it too.

I admire the effort, but the fractured knowledge base concerns me, and I've seen too many times when trouble arises when we try to support more than one way to do thing. See #17124 for a recent example.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>URL param parsing should not be lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15462</link><project id="" key="" /><description>First commit to allow the team to check if this approach is OK.
Now with the right files.

Closes #14719
</description><key id="122394237">15462</key><summary>URL param parsing should not be lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinstuga</reporter><labels><label>:REST</label><label>enhancement</label><label>review</label></labels><created>2015-12-15T23:43:01Z</created><updated>2016-04-06T17:33:43Z</updated><resolved>2016-04-06T17:33:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-18T07:54:10Z" id="165706386">@martinstuga Thanks for the PR! I like the idea here, but I have a couple concerns:
- I don't like that this is only set to work for GET requests. What is the problem with making it work for all request methods?
- I'm not happy with it being inside netty code...
- There are other methods that this does not address: `hasParam(String key)` and `params()`. The first is questionable as to whether it should be considered "consuming" a param by checking if it exists, but searching the codebase shows we do use it like this, for example with `pretty`. The latter method looks like it would not be possible to use, but I also question whether we need that method altogether...some of the usages are odd (we are _setting_ params?)
</comment><comment author="martinstuga" created="2015-12-18T09:43:56Z" id="165731242">@rjernst Thanks for your feedback.
- This approach only works with GET requests because it only throws the Exception after the method is invoked on the node. So, for write operations we would need to rollback the write, so for the write operations I'm going to implement it in all single methods (before invoke the node).
- We can move the `consumedParams` field and some methods to the `HttpRequest` class, but I'll still need some code inside `NettyHttpRequest`.
- We can add `hasParams(String key)` as consumer method. About `params()`, I've searched the code, and it's used to set params, but not the get params (that's what we want), so in that case, we could do some easy factoring to hide this method.

My ideia was to go ahead with write operations (doing this check in every single method), but if you don't feel comfortable with this approach, I can implement that check also in the GET requests.
</comment><comment author="martinstuga" created="2016-01-28T14:57:23Z" id="176224721">#14719 url params parsing should not be lenient

I've added the parameter check to some methods in the commit a04d3d9. Please, validate if it's OK.

Thanks.
</comment><comment author="rjernst" created="2016-02-29T22:25:39Z" id="190427101">Thanks for the update @martinstuga, sorry for the long delay in looking at this again.

Unfortunately there are still a number of issues I have with this PR. While I think it is important to have this validation, I also think doing it cleanly, in a way that will not be error prone for future rest handlers is important, is extremely important.

Here are some issues I have:
- Each handler must add a call to checkParameters: this is error prone
- It still involves touching netty code, which should not have anything to do with how rest requests are validated.
- The new exception added is not useful. Not only do we not need a new exception (IllegalArgumentException should work great), but the singleton instance of the new exception gives a user zero insight into _which_ extra parameters were found.

I think a better approach might be as follows:
- Change the BaseRestHandler abstract method to something like "parseRequest", which returns a runnable. This way the action can be returned and the base class call it, once parameters have been validated (and thus will work for post/put requests).
- Change the RestRequest parameter to separate out the request info, from a separate map/class for the parameters. Each rest handler should remove a parameter as it checks it. Thus any parameters still in the params map after parsing the request are extra parameters for which the base implementation can throw an IAE.

While this is a larger refactoring to the rest handler api, I think it is cleaner for future rest handlers as the api forces rest handler implementations to consume all parameters, regardless of the http method. If you feel this is too complex a task to take on, I completely understand, and in either case, I appreciate the time you've spent working on this issue.
</comment><comment author="martinstuga" created="2016-03-07T00:14:04Z" id="193020663">Hi @rjernst, thanks for your reply.

I'm new to this project and I've adopted this issue because I thought it would be a good entry point. Unfortunately, it's not as straight forward as I thought, and I don't want to be a blocker but the opposite. 
For this reason, I think it would be better to leave this issue, leaving it free for another person.

I'm still eager to collaborate to the project, so I'll be looking for other issues I can help. If you find some, please, feel free to suggest.

Thank you.
</comment><comment author="dakrone" created="2016-04-06T17:33:43Z" id="206479846">Thanks for the explanation @martinstuga, I'm going to close this for now then, but I do hope you find some issues to collaborate on! Check the "adopt-me" label on github or the "low-hanging fruit" label if you need some ideas!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup o/e/c/r/RoutingNodes.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15461</link><project id="" key="" /><description>This commit applies a few trivial cleanups to o/e/c/r/RoutingNodes.java.
</description><key id="122384234">15461</key><summary>Cleanup o/e/c/r/RoutingNodes.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T22:42:39Z</created><updated>2015-12-16T11:21:04Z</updated><resolved>2015-12-16T11:21:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-16T11:10:11Z" id="165068995">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`--debug-jvm` should wait forever</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15460</link><project id="" key="" /><description>Right now gradle takes a long time to start and I typically alt-tab away rather than stare intently at the screen for 30 seconds while everything gets ready. Then I get distracted and do email or something. Anyway, I typically come back to gradle having given up waiting for me, terminated the build, and left a JVM sitting around. It should just wait forever.
</description><key id="122376360">15460</key><summary>`--debug-jvm` should wait forever</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-rc1</label></labels><created>2015-12-15T21:58:07Z</created><updated>2016-10-07T10:44:35Z</updated><resolved>2016-09-27T16:28:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-15T21:58:17Z" id="164910192">If we're going to do #15459 this would really make life easier.
</comment><comment author="clintongormley" created="2016-02-29T19:22:56Z" id="190344043">@nik9000 can this be closed now?
</comment><comment author="nik9000" created="2016-02-29T19:24:43Z" id="190344834">I mean, we aren't doing it and it would be good to do it.
</comment><comment author="dakrone" created="2016-09-27T16:28:41Z" id="249919314">@nik9000 doing `gradle run --debug-jvm` waits forever`*` currently, so I'm going to close this.

`*`: I did not actually wait for "forever" to test this manually.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require rest tests to run via gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15459</link><project id="" key="" /><description>Some time ago, we added "real integration tests" that run in a separate phase of the build (mvn verify, gradle integTest) after the unit test phase. These tests work differently in that we setup and teardown external test fixtures before it runs: allows for real integration tests with a bin/elasticsearch, and use the rest test framework to perform actions and checks against that server.

In order to properly test many of our plugins and qa scenarios, we need the ability to have external test fixtures other than bin/elasticsearch. For example, testing repository-s3 should be using an external mock s3 server which responds to the s3 requests the plugin would make, so that the ES server can do a real s3 snapshot and restore.  But in order for this to work, we must shed the fantasyland requirement: rest integ tests will no longer work from the IDE, since they need this external process to be managed (which is done by gradle). Other examples could be hdfs minicluster, email server, http proxy server, ec2 service, ldap/directory service, etc.

This issue is to remove the hack that was added to make rest integ tests "work" from the IDE, in preparation for more external test fixtures beyond bin/elasticsearch.  Unit tests will continue to work from the IDE, and should definitely be improved for all of these plugins to use mocks. Rest integ tests will not be runnable from the IDE, but they should only be testing small parts (ie the integration of ES and s3), not that the code deep inside a plugin works at all.

See #15439 for the issue to add these external test fixtures.
</description><key id="122373011">15459</key><summary>Require rest tests to run via gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T21:39:51Z</created><updated>2016-01-19T01:38:23Z</updated><resolved>2016-01-19T01:38:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-15T21:46:07Z" id="164907183">Note that for debugging purposes, with gradle we have "gradle run --debug-jvm" which allows you to attach an external debugger.
</comment><comment author="nik9000" created="2015-12-15T21:59:06Z" id="164910560">&gt; `gradle run --debug-jvm`

You can run REST tests with `--debug-jvm` too. Its pretty classy.
</comment><comment author="rjernst" created="2015-12-15T22:16:09Z" id="164914855">@nik9000 indeed, that is what I meant to write!
</comment><comment author="rjernst" created="2015-12-15T22:16:23Z" id="164914911">`gradle integTest --debug-jvm`
</comment><comment author="nik9000" created="2015-12-15T22:25:49Z" id="164916747">You can just stick --debug-jvm on the end of the reproduction lines and it'll pause the jvm that you need to debug. Which is better than it did in maven-times iirc.
</comment><comment author="jpountz" created="2015-12-16T19:10:55Z" id="165215079">+1 Debugging rest tests is something that should be needed very rarely as problems should be caught earlier in the unit/integ tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused import.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15458</link><project id="" key="" /><description>We don't need to import ByteSizeUnit anymore after removing the confusing private constant.
</description><key id="122368117">15458</key><summary>Remove unused import.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">soinlv</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T21:16:08Z</created><updated>2015-12-15T21:33:03Z</updated><resolved>2015-12-15T21:32:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-15T21:32:51Z" id="164903940">LGTM. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Children agg] fix bug that prevented all child docs from being evaluated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15457</link><project id="" key="" /><description>Before we only evaluated segments that yielded matches in parent aggs, which caused us to miss to evaluate child docs in segments we didn't have parent matches for.

The fix for this is stop remember in what segments we have matches for and simply evaluate all segments. This makes the code simpler and we can still quickly see if a segment doesn't hold child docs like we did before.
</description><key id="122358699">15457</key><summary>[Children agg] fix bug that prevented all child docs from being evaluated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T20:24:16Z</created><updated>2015-12-17T09:50:42Z</updated><resolved>2015-12-16T09:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-15T21:41:43Z" id="164906098">LGTM

this is much simpler now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reorganize o/e/c/a/s/ShardStateAction.java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15456</link><project id="" key="" /><description>This commit is a trivial reorganization of
o/e/c/a/s/ShardStateAction.java. The primary motive is have all of the
shard failure handling grouped together, and all of the shard started
handling grouped together.
</description><key id="122357044">15456</key><summary>Reorganize o/e/c/a/s/ShardStateAction.java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T20:15:30Z</created><updated>2015-12-16T12:37:14Z</updated><resolved>2015-12-16T12:37:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-16T12:33:58Z" id="165093132">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin failure returns 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15455</link><project id="" key="" /><description>I was working on what I thought was a clean machine, but it turns out some strange dependency pulled in and set default java-1.6.0.  bin/plugin barfs (won't even run), but for some reason exits 0, which threw me way off course in debugging.

[root@ca6-06:/usr/share/elasticsearch]# bin/plugin  http://nexus/nexus/content/repositories/releases/boilerplate/1.0.0/boilerplate-1.0.0.zip
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/plugins/PluginManagerCliParser : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:268)

Could not find the main class: org.elasticsearch.plugins.PluginManagerCliParser. Program will exit.
</description><key id="122340843">15455</key><summary>bin/plugin failure returns 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AshtonDavis</reporter><labels /><created>2015-12-15T19:00:14Z</created><updated>2015-12-15T19:55:24Z</updated><resolved>2015-12-15T19:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="AshtonDavis" created="2015-12-15T19:00:43Z" id="164856923">The resolution for this issue was obviously to kill 1.6.0 with extreme prejudice.  The issue is that bin/plugin returns 0 on this failure.
</comment><comment author="jasontedor" created="2015-12-15T19:08:08Z" id="164859030">I can not reproduce this on 2.x nor on master. What version of Elasticsearch are you seeing this on? Are there any other exact reproduction steps that you can provide?

```
14:04:45 [jason:~/elasticsearch/elasticsearch-2.1.0/bin] $ \
&gt; JAVA_HOME=`/usr/libexec/java_home -v 1.6` \
&gt; ./plugin \
&gt; http://nexus/nexus/content/repositories/releases/boilerplate/1.0.0/boilerplate-1.0.0.zip
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/plugins/PluginManagerCliParser : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
14:05:05 [jason:~/elasticsearch/elasticsearch-2.1.0/bin] 1 $ echo $?
1
```
</comment><comment author="AshtonDavis" created="2015-12-15T19:50:28Z" id="164876687">You know what, I can't reproduce it - also I think it's more related to bad handling of this error in the puppet module anyway.  My mistake. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>serialize os name, arch and version too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15454</link><project id="" key="" /><description>These three properties are build in the JSON response but were not
transported when a node sends the response.

closes #15422
</description><key id="122328218">15454</key><summary>serialize os name, arch and version too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Stats</label><label>bug</label><label>v2.2.0</label></labels><created>2015-12-15T17:50:55Z</created><updated>2015-12-16T12:06:29Z</updated><resolved>2015-12-16T09:42:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-15T18:10:50Z" id="164844755">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix SnapshotBackwardsCompatibilityIT to not use wildcards with 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15453</link><project id="" key="" /><description>This was only introduced in 2.2 with #15151 so we must not use
wildcards when running with 2.0 nodes.
</description><key id="122324288">15453</key><summary>fix SnapshotBackwardsCompatibilityIT to not use wildcards with 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-12-15T17:29:59Z</created><updated>2015-12-16T07:52:49Z</updated><resolved>2015-12-16T07:52:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-15T18:46:48Z" id="164853093">LGTM. Thanks for this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle idea plugin does not properly mark resources directories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15452</link><project id="" key="" /><description>Using `gradle cleanIdea idea` I have the issue that resource directories (`src/main/resources` and `src/test/resources`) are not properly marked as resource directories (but only as normal source directories). When I then build the project in IDEA, I get exceptions from the Groovy compiler (as it finds Groovy sources under our `src/test/resources`. 

The issue seems to be an well-known one.
https://discuss.gradle.org/t/idea-plugin-resource-folders-support/5692/2
https://discuss.gradle.org/t/the-idea-plugin-breaks-the-new-intellij-13-iml-configuration/2456
https://issues.gradle.org/browse/GRADLE-2975

Note that by applying the [Intellij Gradle integration](https://www.jetbrains.com/idea/help/gradle.html) (import as Gradle project), resource directories are properly marked and the issue disappears.
</description><key id="122314686">15452</key><summary>Gradle idea plugin does not properly mark resources directories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>build</label></labels><created>2015-12-15T16:44:36Z</created><updated>2016-01-14T05:02:58Z</updated><resolved>2015-12-18T16:27:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-15T18:58:37Z" id="164856384">+1
</comment><comment author="diwayou" created="2016-01-14T03:18:38Z" id="171517939">i hate gradle and love maven, why use gradle instead of maven from 2.X
</comment><comment author="nik9000" created="2016-01-14T04:03:04Z" id="171523116">&gt; i hate gradle and love maven, why use gradle instead of maven from 2.X

This probably isn't the appropriate place to discuss it but I don't know what is so I'll just answer.

All build systems have their problems but the two biggest problems I personally had with maven while working on this project is that its so wordy to extend the build and that its very hard to add new things that don't fit into the maven lifecycle. `gradle run` is pretty idiomatic but to do the same thing in maven requires some amazing backflips. In maven we were always having to drop to ant to get anything done and any time you wanted to invoke a task it was minimally 6 lines of xml and you had no ability to share code beyond copy and paste and, maybe, parent poms. At least in gradle you are just writing more groovy and you can set things up using groovy.

My complaint about gradle is that it start so slow and that some of the plugins try to be too cute for their own good, trying to expose abstractions that make sense but failing to fully implement the abstractions and silently dropping the whole request on the floor. That isn't so much a gradle problem, but still.

The nicest thing about gradle is that it figures out what has to be done. I can say `gradle plugins:lang-groovy:run` and gradle will figure out what needs to be compiled and built so we can spin up a proper elasticsearch instance with that plugin. In maven you had to go through the whole lifecycle and manually skip steps.
</comment><comment author="rmuir" created="2016-01-14T05:02:58Z" id="171530433">We can always open an issue to improve our "generated poms" from gradle, so that someone who likes maven can still use maven easier (for e.g. a plugin). I think we should avoid a "shadow build", but maybe it can just be enough simple stuff to ensure that plugin is easily packaged properly and tested? Even if it has to do something sneaky like invoke some "maven-gradle" plugin for logic. Of course we would want to test that it works, but we can already say that about the POMs we are currently generating from the gradle build. Just ideas.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only create index on document deletion if external versioning is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15451</link><project id="" key="" /><description>Closes #15425
</description><key id="122307475">15451</key><summary>Only create index on document deletion if external versioning is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:CRUD</label><label>enhancement</label><label>review</label></labels><created>2015-12-15T16:14:21Z</created><updated>2016-10-07T13:57:49Z</updated><resolved>2016-10-06T12:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-15T16:17:23Z" id="164814433">I like it.
</comment><comment author="bleskes" created="2015-12-15T16:24:09Z" id="164816728">we should also deal with the bulk request. Also, I'm fine with not pushing it all the way to 1.7.5 if mergine gets hairy.
</comment><comment author="ywelsch" created="2015-12-15T17:52:27Z" id="164840351">good catch @bleskes, added support for bulk.
</comment><comment author="ywelsch" created="2015-12-18T12:59:52Z" id="165772026">@bleskes pointed out that creating the index on delete can be useful in the case of external versioning. With concurrent delivery from an external source, operations might arrive out of order. A delete operation might thus arrive earlier than its preceding index operation. If the delete operation does not create an index and marks the document as deleted (with higher version), the index operation will recreate the index and document (with lower version).

I adapted the PR accordingly to only create index on document deletion if external versioning is used.

Also I changed the labels. As this is a breaking change, I don't think it should be backported.
</comment><comment author="nik9000" created="2015-12-18T15:23:47Z" id="165803192">&gt; If the delete operation does not create an index and marks the document as deleted (with higher version), the index operation will recreate the index and document (with lower version).

My understanding is that those deletes don't stick around forever anyway so you can't rely on this behavior. So its just a bonus. Is it worth the complexity in this case?

&gt; Also I changed the labels. As this is a breaking change, I don't think it should be backported.

I don't think its worth backporting.
</comment><comment author="dakrone" created="2016-04-06T17:34:53Z" id="206480410">@ywelsch is this still an issue with @abeyad's index tombstone PR in #17265 ? Is it still applicable?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleting snapshot causes NPE with no useful error information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15450</link><project id="" key="" /><description>I create nightly snapshots of '_all' indices. They are created in a file-system repository on a Windows Server 2012 shared folder. The snapshots are created successfully and I can successfully restore data from them, and _some_ can be successfully deleted.

However, when I try to some particular snapshots, ES return a 500 status code and only this info:
`{"error":"NullPointerException[null]","status":500}`
Since there is no stacktrace and no other info (I checked ES node logs of the cluster), I have no idea how to fix it.

This is on ES 1.7.3.

I don't think it is permissions issues since only some of the snapshots fail to be deleted. Others have been deleted successfully from the same repo.

The bottom line is that it would be nice to get more detailed error info on the failure.
</description><key id="122307295">15450</key><summary>Deleting snapshot causes NPE with no useful error information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bradvido</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>feedback_needed</label></labels><created>2015-12-15T16:13:43Z</created><updated>2016-05-24T12:51:27Z</updated><resolved>2016-05-24T12:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T10:38:43Z" id="165061595">@imotov @ywelsch any ideas how to figure this out?
</comment><comment author="s1monw" created="2015-12-16T11:58:28Z" id="165084413">this is very very likely fixed in 2.0 and above - there are tons of places where we just don't do the right thing with exceptions in 1.7.3. @bradvido is there any chance you can try reproducing this with 2.1?
</comment><comment author="bradvido" created="2016-02-11T22:01:27Z" id="183077530">Unfortunately, we don't have any plans to move to 2.0+ soon.  This is still occurring though. 
Is there any way to force ES to print the stacktrace?
</comment><comment author="imotov" created="2016-02-16T16:44:15Z" id="184762122">@bradvido when you said 

&gt; However, when I try to some particular snapshots

what did you mean? What are you trying to do with these snapshots?
</comment><comment author="bradvido" created="2016-02-16T17:05:22Z" id="184774077">@imotov I am trying to delete them. Some snapshots can be deleted successfully, others can not
</comment><comment author="imotov" created="2016-02-16T17:39:57Z" id="184792720">@bradvido could you email me `snapshot-XXXXXX` and `metadata-XXXXXX` files from the root directory of your repository, where `XXXXXXX` is the name of the snapshot that you cannot delete?
</comment><comment author="clintongormley" created="2016-05-24T12:51:27Z" id="221258726">This issue is fairly old and there hasn't been much activity on it. Closing, but please re-open if it still occurs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trace log REST test headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15449</link><project id="" key="" /><description>We already log the request, logging the headers is useful for debugging also.
</description><key id="122301790">15449</key><summary>Trace log REST test headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T15:49:59Z</created><updated>2016-02-29T16:37:37Z</updated><resolved>2015-12-15T15:53:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-15T15:51:52Z" id="164805249">LGTM. This is test code. If it's important, we can also do debug.
</comment><comment author="jasontedor" created="2015-12-15T15:52:24Z" id="164805417">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Source filtering is not permissive enough</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15448</link><project id="" key="" /><description>When working with source filtering (e.g. setFetchSource ), the excludes are interpreted first, and then the includes are interpreted.  I would assume it would be the other way around on conflicts, as includes is directly stating what is desired.  Ultimately, this causes problems when using wildcards.  

If I have the following structure

``` json
{
   "person_name" : "Charles",
   "person_age" : 67,
   "person_dob" : "1950-10-01",
   "person_ssn" : "xxx-xx-xxxx",
   "person_location" : {
        "name" : "My House",
        "address" : "xxx",
        "city" : "Orlando",
        "state" : "Florida"
    }
}
```

And I want to get all the immediate person fields plus the location name, but not including all the other location fields. I would expect  to be able to ask for the following:  

``` java
builder.setFetchSource(
    /* include */ new String[]{ "person_*", "person.location_name"}, 
    /* exclude */ new String[]{"person.location_*"}
)
```

This results in all person.location_\* fields being excluded even person.location_name.  Even though this example seems to be a more complicated usage of the source filtering,  I would still assume that includes should always win over excludes
</description><key id="122301693">15448</key><summary>Source filtering is not permissive enough</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">arciisine</reporter><labels><label>:Search</label><label>adoptme</label><label>docs</label><label>enhancement</label></labels><created>2015-12-15T15:49:30Z</created><updated>2017-04-18T16:11:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T10:31:17Z" id="165059644">Your example mixes up dots and underscores, but the description of the problem is correct:

```
PUT t/t/1
{
   "person_name" : "Charles",
   "person_age" : 67,
   "person_dob" : "1950-10-01",
   "person_ssn" : "xxx-xx-xxxx",
   "person_location" : {
        "name" : "My House",
        "address" : "xxx",
        "city" : "Orlando",
        "state" : "Florida"
    }
}

GET t/_search 
{
  "_source": {
    "include": ["person_*", "person_location.name"],
    "exclude": "person_location.*"
  }
}
```

I think I agree with your reasoning.  @bleskes what do you think?
</comment><comment author="colings86" created="2015-12-18T10:42:55Z" id="165744729">Discussed in FixItFriday: We have to choose a precedence order one way or the other and there are going to be use cases which are hard either way (Include all X except X.Y or exclude all X expect X.Y). Elsewhere (e..g term agg) the includes take precedence over excludes so we should keep it this way for consistency. The behaviour should be documented properly though so the fix here is to add an explanation of the precedence order in the documentation.
</comment><comment author="marshall007" created="2017-04-18T16:11:23Z" id="294895229">@colings86 I think you're saying that the intended behavior is for `includes` to take precedence over `excludes`, but this does not appear to be the case (at least in `v2.4.3`).

The following query, for example, returns an empty `_source` object in all cases:

```json
{
  "_source": {
    "includes": "data.name",
    "excludes": "data.*"
  }
}
```

I think there are a few issues here:

1. It appears that by specifying a value for `includes`, the default behavior becomes "omit all fields". This doesn't really make sense in the presence of an explicit `excludes`.
1. `excludes` seems to take priority over `includes` in all cases.
1. More specific paths do not take precedence over glob paths (i.e. `* &lt; *.name &lt; data.* &lt; data.name`). If the path specs were evaluated in order based on specificity it would make all use cases easier to implement.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis : Fix no response from Analyze API without specified index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15447</link><project id="" key="" /><description>Fix error handling in TransportSingleShardAction without shardIt

Closes #15148
</description><key id="122297926">15447</key><summary>Analysis : Fix no response from Analyze API without specified index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>bug</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T15:32:49Z</created><updated>2016-03-16T14:05:22Z</updated><resolved>2015-12-22T08:37:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-15T15:46:44Z" id="164803828">Left minor comments. LGTM though.
</comment><comment author="johtani" created="2015-12-18T00:08:44Z" id="165620890">@rjernst @nik9000 Thanks for your comment. I added the assertion of message and deleted the prepareCreate in the first line.
</comment><comment author="nik9000" created="2015-12-21T16:46:53Z" id="166356046">&gt; @rjernst @nik9000 Thanks for your comment. I added the assertion of message and deleted the prepareCreate in the first line.

LGTM
</comment><comment author="clintongormley" created="2016-01-10T19:36:27Z" id="170384160">Closed in 267cd65506ab22ceb7606243ef58979bde5dbfe5
</comment><comment author="dadoonet" created="2016-03-02T18:07:46Z" id="191353904">@clintongormley I wonder if this is should be backported in 2.2.x as well as it's most likely a bug (not critical though)?
</comment><comment author="clintongormley" created="2016-03-03T10:37:21Z" id="191699835">I'm ok with that - it's a small fix
</comment><comment author="johtani" created="2016-03-04T06:04:36Z" id="192121869">backported 586af1452b65265b511a7439a6e2b8f769073cad
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Renames `default` similarity into `classic`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15446</link><project id="" key="" /><description>Replaces deprecated DefaultSimilarity with ClassicSimilarity.

Closes #15102
</description><key id="122295588">15446</key><summary>Renames `default` similarity into `classic`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Similarities</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T15:21:40Z</created><updated>2016-02-13T13:23:41Z</updated><resolved>2015-12-30T16:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-15T16:07:00Z" id="164809652">@jimferenczi looks good but I miss some kind of BWC layer for existing 2.0 indices that may have set this? at the very least add an index to our bwc tests with a similarity set in it's mapping.
</comment><comment author="jimczi" created="2015-12-15T16:15:50Z" id="164813559">@bleskes yep that part is missing. I don't know what's the process for upgrades, do we have some sort of mappings/settings rewriter somewhere or should I keep the "default" compatibility in the similarity service ?
</comment><comment author="bleskes" created="2015-12-15T16:21:58Z" id="164816135">@jimferenczi there are a couple approaches. You can clean up the mapping when recovering  (just remove explicit `default`). You can also map classic to  default if the index was created before 3.0.  I prefer the first one. Not sure how difficult it is code wise.
</comment><comment author="jimczi" created="2015-12-15T16:34:41Z" id="164820026">@bleskes ok thanks I'll try the first approach and see where it goes ;).
</comment><comment author="jpountz" created="2015-12-24T00:07:42Z" id="167011835">@jimferenczi Please ping us here when you push new commits, otherwise we might not even know there is more stuff to review. The change looks good to me now, I just left a question about the bw layer because of something that is not clear to me.
</comment><comment author="jimczi" created="2015-12-30T02:56:34Z" id="167922843">@jpountz I changed the bw layer, it was a leftover that I forgot to remove in my last commit.
</comment><comment author="jpountz" created="2015-12-30T08:59:59Z" id="167960280">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Floating heap size causes weird issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15445</link><project id="" key="" /><description>From https://github.com/elastic/puppet-elasticsearch/issues/522 it seems that having a float number as heap size it causes weird startup problems.

After talking with @clintongormley this might be worth investigating.
</description><key id="122291507">15445</key><summary>Floating heap size causes weird issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">electrical</reporter><labels><label>:Packaging</label><label>adoptme</label><label>docs</label></labels><created>2015-12-15T15:03:55Z</created><updated>2016-01-15T12:38:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-15T17:05:38Z" id="164828754">This isn't specific to Elasticsearch; for the HotSpot JVM, if a maximum heap size is set, it _must_ be all digits unless the final character is `k`, `K`, `m`, `M`, `g`, `G`, `t`, or `T` (which will just serve as the obvious multipliers for the preceding digits). This also applies to the minimum heap size and the thread stack size settings. The JVM will fail to startup if this is violated.

In my mind, the root issue here (from the linked issue elastic/puppet-elasticsearch#522) is how poorly startup failures are reported. In general, there is a lot of room for improvement for the handling of startup failures related to how the Elasticsearch process is daemonized. With improved daemonization and reporting of startup failures, the failure here could be more readily apparent.

I do not think that we should get in the business of trying to validate JVM parameters before passing to the JVM; that would just be a maintenance headache.
</comment><comment author="electrical" created="2015-12-15T17:31:04Z" id="164835340">The user saw [here](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html) refering 30.5G so he tried that, which failed.
Perhaps we should modify that to 30G to avoid confusion as well?
</comment><comment author="TinLe" created="2015-12-15T18:00:52Z" id="164842355">Or convert it to M.  E.g. 30.5G = 31232M

:-)
</comment><comment author="jasontedor" created="2015-12-15T18:37:35Z" id="164850904">&gt; The user saw [here](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html) refering 30.5G so he tried that, which failed.
&gt; Perhaps we should modify that to 30G to avoid confusion as well?

I can see the confusion and agree that we should provide different guidance here, but do note that there is no example where `30.5g` is actually passed a command-line argument.

Even though there are no explicit command-line examples there, I'm not sure why we even say `30.5g` as using `31g` will _usually_ get `UseCompressedOops` to be true. However, I think we should avoid giving explicit examples because the values that permit `UseCompressedOops` vary from JVM to JVM. For example:

```
13:43:32 [jason:~] $ uname -mprsv
Darwin 15.2.0 Darwin Kernel Version 15.2.0: Fri Nov 13 19:56:56 PST 2015; root:xnu-3248.20.55~2/RELEASE_X86_64 x86_64 i386
13:43:33 [jason:~] $ JAVA_HOME=`/usr/libexec/java_home -v 1.7` java -Xmx32600m -XX:+PrintFlagsFinal 2&gt; /dev/null | grep UseCompressedOops
     bool UseCompressedOops                        := true            {lp64_product}      
13:43:35 [jason:~] $ JAVA_HOME=`/usr/libexec/java_home -v 1.7` java -Xmx32766m -XX:+PrintFlagsFinal 2&gt; /dev/null | grep UseCompressedOops
     bool UseCompressedOops                         = false           {lp64_product}      
13:43:38 [jason:~] $ JAVA_HOME=`/usr/libexec/java_home -v 1.8` java -Xmx32766m -XX:+PrintFlagsFinal 2&gt; /dev/null | grep UseCompressedOops
     bool UseCompressedOops                        := true                                {lp64_product}
13:43:40 [jason:~] $ JAVA_HOME=`/usr/libexec/java_home -v 1.8` java -Xmx32767m -XX:+PrintFlagsFinal 2&gt; /dev/null | grep UseCompressedOops
     bool UseCompressedOops                         = false                               {lp64_product}
```

In particular, note that `-Xmx32766m` gives `UseCompressedOops` being `false` on one JVM, but `true` on another.

I opened elastic/elasticsearch-definitive-guide#455.
</comment><comment author="clintongormley" created="2015-12-18T10:45:45Z" id="165745167">We should improve our documentation to show using integers for heap sizing, and add a section explaining what happens when the heap is &lt; 4gb, &lt; ~26gb, etc, and how to figure out what the boundary is for compressed oops
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Init script -b option hides actual error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15444</link><project id="" key="" /><description>It seems in some cases the -b option in the init scripts hides important error messages.

See https://github.com/elastic/puppet-elasticsearch/issues/522 for more details.
</description><key id="122291239">15444</key><summary>Init script -b option hides actual error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electrical</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-12-15T15:02:47Z</created><updated>2015-12-16T10:25:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T10:25:00Z" id="165058460">Related to https://github.com/elastic/elasticsearch/issues/8796
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexService: remove unneed inject annotation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15443</link><project id="" key="" /><description /><key id="122283768">15443</key><summary>IndexService: remove unneed inject annotation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T14:27:11Z</created><updated>2015-12-15T14:33:50Z</updated><resolved>2015-12-15T14:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-15T14:32:44Z" id="164781406">Confirmed unneeded. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove back compat for the `path` option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15442</link><project id="" key="" /><description>The `path` option allowed to index/store a field `a.b.c` under just `c` when
set to `just_name`. This "feature" has been removed in 2.0 in favor of `copy_to`
so we can remove the back compat in 3.x.
</description><key id="122278423">15442</key><summary>Remove back compat for the `path` option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T13:58:23Z</created><updated>2015-12-15T14:04:12Z</updated><resolved>2015-12-15T14:04:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-15T14:00:55Z" id="164773459">LGTM trash it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail fast when executing master level write operations via a tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15441</link><project id="" key="" /><description>Currently, executing master level write operations against a tribe node leads to MasterNotDiscoveredException after a 1 minute timeout. This PR adds a new mode to TransportMasterNodeAction to fail fast if the action does not specify to always execute locally.

Closes #13290
</description><key id="122267765">15441</key><summary>Fail fast when executing master level write operations via a tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Tribe Node</label><label>enhancement</label><label>review</label></labels><created>2015-12-15T13:03:32Z</created><updated>2016-10-07T18:09:00Z</updated><resolved>2016-10-06T14:12:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-15T13:03:50Z" id="164757210">@martijnvg can you have a look?
</comment><comment author="bleskes" created="2015-12-16T15:16:27Z" id="165137756">I looked at this and I like the change from a technical perspective but the error message we will give the user is very cryptic. Another potential change is to check block on the coordinating node as well as on the master  (like we do in replication action). This will cause non-retryable blocks to fail immediately (like the blocks the tribe node adds). 
</comment><comment author="dakrone" created="2016-04-06T17:35:35Z" id="206480928">ping @ywelsch, this is a little stale, is it still applicable?
</comment><comment author="ywelsch" created="2016-10-06T14:12:44Z" id="251973710">Opened #20779 instead
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow a different bundlePlugin to be specified in integTest task</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15440</link><project id="" key="" /><description>Currently, the `bundlePlugin` configuration for `integTest` is locked down and cannot be changed since, the [task](https://github.com/elastic/elasticsearch/blob/4e80a5e0994f25caa19c3e78573bdc9fa6bcc4c6/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy#L40) relies on `project.afterEvaluate` which means whatever configuration is added, the plugin comes in last and overrides it.

This is not a common case however for plugins (like hdfs) which publish multiple artifacts (one for hadoop1, one for hadoop2), this would allow testing out all published zips not just the `bundlePlugin` which might not be representative.
</description><key id="122267310">15440</key><summary>Allow a different bundlePlugin to be specified in integTest task</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">costin</reporter><labels><label>:Plugin Repository HDFS</label><label>build</label></labels><created>2015-12-15T13:00:23Z</created><updated>2016-05-03T13:52:53Z</updated><resolved>2016-05-03T13:52:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-15T13:04:34Z" id="164757336">Do we really need multiple artifacts? I think we should just drop the hadoop1 support, for snapshot-restore, we really need synchronization, so people do not lose data.
</comment><comment author="rjernst" created="2015-12-15T16:32:12Z" id="164819364">Even if we do have multiple artifacts, they should be separate plugins (ie gradle projects). This means each would have its own integration tests. But I agree we should simplify here and just do "hadoop2".
</comment><comment author="clintongormley" created="2016-02-14T19:17:02Z" id="183957851">@costin I believe this is no longer needed?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow build system to start a separate process for during integration testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15439</link><project id="" key="" /><description>When doing integration testing for third party libraries, it is fairly common to have to deal with an external service/process/port which could be spawned during integration testing, _separately_ from ES.
This has the benefit of being more close to real world and it doesn't clash with the security manager for ES (giving it a more real feel).
It would be great to have this available in 2.x as well.
</description><key id="122266699">15439</key><summary>Allow build system to start a separate process for during integration testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">costin</reporter><labels><label>:Plugin Repository HDFS</label><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T12:56:27Z</created><updated>2016-02-14T19:25:14Z</updated><resolved>2016-02-14T19:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2015-12-15T12:56:58Z" id="164756062">relates #15192 
</comment><comment author="clintongormley" created="2016-02-14T19:16:24Z" id="183957813">@costin @rjernst has this issue been dealt with already?
</comment><comment author="rjernst" created="2016-02-14T19:25:14Z" id="183959051">Yes, this was addressed when we added fixtures. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simplify and improve repository-hdfs plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15438</link><project id="" key="" /><description>As-is, the repository-hdfs plugin isn't going to work at all from what i see.

I'm just plotting what I think we need to do to fix it, before releasing it:
- Four plugin zip versions are created, none are really tested, none are working (except on windows). For example the `-lite.zip` has no hadoop jars and expects the users to drop hadoop jars in their ES_CLASSPATH. We don't allow this, this is not going to work at all.
- we need to drop the `-hadoop1` and `-lite` stuff and just one have one jar, we need to get the basics working, walk before we can run. from what i see, things aren't working at all.
- documentation recommends adding configuration files and similar to the `CLASSPATH` or ES `lib/` folder, we can't do things this way anymore! Its not going to work!
- security permissions are not correct: it only adds `jaas_nt`. This means that this plugin is only going to work on windows NT platforms: anything else = instant security exception.
- The use of HDFS should be made reliable (means hdfs2 only), since its a snapshot restore plugin. To me this means two things, fsync and atomic rename. I don't see any synchronization, and the rename it uses for committing does not look like the atomic one (rename2).
- no tests are failing!
</description><key id="122264590">15438</key><summary>simplify and improve repository-hdfs plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Repository HDFS</label><label>blocker</label><label>enhancement</label></labels><created>2015-12-15T12:44:48Z</created><updated>2016-03-16T19:33:10Z</updated><resolved>2015-12-23T23:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2015-12-16T00:58:32Z" id="164949417">Pushed fixes [here](https://github.com/costin/elasticsearch/tree/hdfs-simplifications).
1. the plugin has been updated to be hadoop2 only (dropped `-hadoop1` and `lite`)
2. the plugin documentation does not mention `ES_CLASSPATH` or `lib/` - if this is mentioned anywhere, let me know
3. I have removed `hadoop-yarn-common` to prevent jar hell from occuring in the IDE
4. the MiniHDFS starts and stops a mini HDFS cluster. Its classpath is `testCompile` - I've tried creating a separate configuration however for some reason, it ends up with several dependencies with a different version that need to be specified in the resolution strategy and further more adds in jars that trigger jar hell (commons-beanutils vs commons-collection).`testCompile` has this already resolved so the configuration is smaller.
5. The permissions have been double-checked. I have tested this locally on windows only hence the `jaas_nt` permission. `nix is next
6. Updated the plugin code to use the `FileContext` class.
</comment><comment author="costin" created="2015-12-18T23:46:09Z" id="165922059">I have updated the branch and rebased against master.
1. Since the branch is Hadoop2 only, I have replaced the aging `FileSystem` interface with `FileContext`. The underlying code proved to be separate and in many cases, the new features in `FileContext` were ignored by `FileSystem`. The new class in use should be more reliable in terms of fsync semantics (through `hsync).
2. the code explicitly writes blobs (creates new files) using `hsync`. Further more in case of a rename, it tries to `append` the file to trigger the `hsync` yet again.
3. The number of Hadoop2 jars required has been removed - turns out some were not really used.
4. Last but not least, the build system was improved a bit to start MiniHDFS before integration testing and to stop it afterwards. While it works when done manually, I couldn't get it so far to work properly in gradle:
   a. Gradle starts the process in foreground which means the whole build is stopped. It needs to be wrapped to run in the background which is somewhat tricky because we don't want to compute the classpath ourselves but rather get it from gradle. ant.exec might help here instead of Gradle `JavaExec`
   b. killing a process is tricky. I have the basic portable code in place (similar to @rjernst 's code) however I need to find a way to pass the pid info around and against, due to classpath issues (gradle vs the project) this might require some code duplication.

Basically 4 is the last item completely addressing the issue. @rjernst  do you have some free time to help? If not, I'll push further over the weekend to get Gradle to play nice.
</comment><comment author="rmuir" created="2015-12-19T03:25:39Z" id="165940364">Thanks costin, ryan and I continued with your branch, he did some cleanup on dependencies, I am basically trying to help with the integration test and getting this working on unix systems.

The things resolved so far are:
1. Hadoop jars were added one by one, only the most minimal set with no transitive dependencies. this way we can get this thing working without jar hell etc and minimize the surface area. It is now up to ConnectException so I think its fairly close. 
2. Hadoop will not work at all on non-windows systems without full execute permission across the entire filesystem (https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java#L381-L402). This is obviously unacceptable, I had to add a brutal hack to deliver a custom IOException from our security code (instead of SecurityException) when it tries to do this. This needs to ultimately be fixed in hadoop.
3. Hadoop needs `jaas_unix` loadLibrary permission or it fails with security exception (because only `jaas_nt` is added right now.

See in-progress changes here: https://github.com/elastic/elasticsearch/compare/master...rjernst:hdfs_without_transitive

One thing we have right now is a bug in our handling of slf4j which affects hadoop. We don't actually ship it in any ES distributions, yet the build treats it as `provided`, so its also not in any plugin that wants it. I hacked around this temporarily, but that needs to be sorted out.

I will try your minihdfs cluster now (manually) and try to get the integ  connecting / snapshotting / restoring. Ryan is working on hooking it into the integTest part of the build.
</comment><comment author="rmuir" created="2015-12-19T04:18:54Z" id="165945836">@costin i converted core's rest tests (take snapshots and restores) and they all pass against the minihdfs now. As there is no longer jar hell, and we can do a real integration test, I will see if i can do further simplifications.
</comment><comment author="costin" created="2015-12-20T00:12:20Z" id="166035094">Wow, thanks for the improvements - this must have been quite an effort. Tested it locally and it worked great (note that under Windows one needs `winuntils.exe` and the proper permissions for it but since Windows is really a second class citizen under Hadoop, I haven't added them to the policy file since as you mentioned, it's simply too dangerous (and unacceptable).
Glad to see the CL hack is no longer needed - makes things a lot easier.

I have updated (simplified) the docs [here](https://github.com/costin/elasticsearch/tree/hdfs2-only).
Is there anything else to push this into 2.2?

Thanks again for everything!
</comment><comment author="rmuir" created="2015-12-20T00:49:07Z" id="166045076">Well there are a few things we should do for merging back to master: 
1. fully automate the new rest tests, so they can run like normal tests in jenkins on all platforms, etc. this means launching the hdfs minicluster as a proper test fixture. Right now they have hardcoded port numbers from the last time i ran it on my machine :) There is some new stuff around this here: #15561. I will look into this.
2. Ensure we only support `hdfs://`. We don't test anything else, and I know for a fact `file://` will not work, users should instead use the proper repo plugin which will be tested for that, and we should not expose hadoop VFS here in this plugin.  We should think about the configuration API here too, especially considering the warning in the docs about not putting paths in the URI. Instead of a URI at all, maybe we should just ask for the hdfs server. We can have an undocumented Setting that lets our hacky test filesystem still work for unit testing.
3. open bug report at hadoop so that their classes can be initialized under unix without blanket process execution permissions across the entire machine. Our hack is ok only as a temporary one.
4. fix the handling of `slf4j-api.jar` in our gradle build. the root cause is our gradle struggling on this `optional` dependency, really all are bogus and caused by the server having to perform double-duty as a client. currently the build is marking this as `provided` but it is in fact not provided, its excluded explicitly from all distributions. So it does not make its way into the HDFS plugin zip and then its missing. in this branch my hack just adds that jar to all distributions, I doubt that is what we really want to do.

as far as the docs go, I strongly dislike having docs that recommend the user modify plugin jars. What is the protocol compatibility issue and how often does it break in hadoop?

I have no clue about 2.2, I think we still have plenty of work to get this correct in master. Also 2.x would require additional maven/ant work to startup/shutdown hdfs during integration tests phase, etc.
</comment><comment author="rmuir" created="2015-12-20T21:06:01Z" id="166155009">The miniHDFS is working now for integration tests. I forked your branch here: https://github.com/elastic/elasticsearch/compare/master...rmuir:hdfs2-only?expand=1

I will try to improve it now, but at least it is working and rest tests are passing with it.
</comment><comment author="rmuir" created="2015-12-20T22:17:15Z" id="166158902">`gradle check` now works on both linux and os X and with reasonable speed. Windows is totally broken: unit tests do not work nor does the hdfs fixture work... I am working through those issues. 
</comment><comment author="rmuir" created="2015-12-21T00:36:17Z" id="166168260">On windows we can't start `hdfsFixture` correctly without having native libraries available. They aren't exactly packaged in any clean/efficient way and I don't think we want windows `dll` files flying around in our builds anyway.

So windows `integTest` skips the fixture and the snapshot/restore rest tests against it. Unit tests do work now though.
</comment><comment author="rmuir" created="2015-12-21T07:38:21Z" id="166223846">`gradle hdfs` (the simple javaexec test) works under `test/fixtures/hdfs-fixture` if you setup hadoop native libraries. You must set HADOOP_HOME and put HADOOP_HOME/bin in the PATH. I used https://github.com/karthikj1/Hadoop-2.7.1-Windows-64-binaries/releases as a quick test in a VM. 

So this gives the possibility to test windows in jenkins, if some things are setup. It is optional and these things are still skipped if its not setup (only unit tests and simple "plugin is installed" rest test will run).

But the test fixture stuff is still totally broken on windows, it tries to do something crazy with a .bat file..., @rjernst needs to look.
</comment><comment author="rmuir" created="2015-12-21T19:32:17Z" id="166398085">I reviewed the hdfs code and ensured it matches the same semantics as our FS repository and removed any kind of leniency/exists/etc, with the exception of https://github.com/elastic/elasticsearch/issues/15579 (we fail with FileAlreadyExistsException on purpose)
</comment><comment author="rmuir" created="2015-12-23T23:21:11Z" id="167007848">I'm closing this. We are in fairly good shape in master... not perfect but the basics are there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Filters Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15437</link><project id="" key="" /><description /><key id="122256668">15437</key><summary>Aggregations Refactor: Refactor Filters Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-15T11:56:44Z</created><updated>2015-12-15T13:34:04Z</updated><resolved>2015-12-15T13:33:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-15T13:26:42Z" id="164764204">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix inaccurate docs for nested datatype</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15436</link><project id="" key="" /><description /><key id="122255463">15436</key><summary>Fix inaccurate docs for nested datatype</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">umeku</reporter><labels><label>docs</label></labels><created>2015-12-15T11:48:29Z</created><updated>2015-12-15T14:15:50Z</updated><resolved>2015-12-15T14:15:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T14:15:50Z" id="164777091">thanks @umeku - it wasn't quite right because the second query wouldn't show the inner hits, but i've fixed and pushed. thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.1.0 does not bind to multiple interfaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15435</link><project id="" key="" /><description>Hello.

I have just upgraded from Elasticsearch 1.7.3 to Elasticsearch 2.1.0. I have a interface on my nodes, bond1, which relates to an ip 192.168.112.3. It seems I am unable to bind to multiple interfaces in 2.1.0.

(part of) elasticsearch.yaml

```
network:
  bind_host:
           - _bond1:ipv4_
           - localhost
  publish_host: _bond1:ipv4_
```

(Also, transport.tcp.port is 9301 and http.port is 9201)

In the boot, I get the following logline;

```
publish_address {192.168.112.3:9301}, bound_addresses {127.0.0.1:9301}, {[::1]:9301}
```

**Where is my bond1 in the bound_addresses?**

Netstat;

```
tcp6       0      0 ::1:9301                :::*                    LISTEN      30793/java       off (0.00/0/0)
tcp6       0      0 127.0.0.1:9301          :::*                    LISTEN      30793/java       off (0.00/0/0)
tcp6       0      0 ::1:9201                :::*                    LISTEN      30793/java       off (0.00/0/0)
tcp6       0      0 127.0.0.1:9201          :::*                    LISTEN      30793/java       off (0.00/0/0)
```

If I change my network.bind_host to only '_bond1:ipv4_' it all works perfectly;

```
publish_address {192.168.112.3:9301}, bound_addresses {192.168.112.3:9301}
```

Netstat;

```
tcp6       0      0 192.168.112.3:9301      :::*                    LISTEN      31194/java       off (0.00/0/0)
tcp6       0      0 192.168.112.3:9201      :::*                    LISTEN      31194/java       off (0.00/0/0
```

The Manual (https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html) spesifically states you can have multiple bind_host, and one publish_host (as we did in elasticsearch 1.7.3).
</description><key id="122224241">15435</key><summary>Elasticsearch 2.1.0 does not bind to multiple interfaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kjelle</reporter><labels /><created>2015-12-15T09:05:01Z</created><updated>2015-12-15T09:08:19Z</updated><resolved>2015-12-15T09:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-15T09:08:19Z" id="164693769">Fixed in 2.2.0 by #13954
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose http.type setting, and collapse al(most all) modules relating to transport/http</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15434</link><project id="" key="" /><description>This change adds back the http.type setting. It also cleans up all the
transport related guice code to be consolidated within the
NetworkModule (as transport and http related stuff is what and how ES
exposes over the network). The setter methods previously used by some
plugins to override eg the TransportService or HttpServerTransport are
removed, and those plugins should now register a custom implementation
of the class with a name and set that using the appropriate config
setting. Note that I think ActionModule should also be moved into here,
to sit along side the rest actions, but I left that for a followup.

closes #14148
</description><key id="122204249">15434</key><summary>Expose http.type setting, and collapse al(most all) modules relating to transport/http</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T06:37:07Z</created><updated>2016-01-10T21:15:01Z</updated><resolved>2015-12-16T19:54:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-15T15:24:44Z" id="164797308">This is awesome. Left some minor suggestions
</comment><comment author="s1monw" created="2015-12-16T11:04:48Z" id="165067957">LGTM - awesome
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>smoke test plugins does not test any plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15433</link><project id="" key="" /><description>Currently the build has a bug and it loads 0 plugins.
</description><key id="122194839">15433</key><summary>smoke test plugins does not test any plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T05:06:02Z</created><updated>2015-12-15T06:11:34Z</updated><resolved>2015-12-15T06:11:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-15T05:54:29Z" id="164651341">LGTM, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>restoration from a snapshot is failing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15432</link><project id="" key="" /><description>I am getting the following exception while doing a restoration

curl -XPOST "http://hostname1:9200/_snapshot/snapshot_restore_repo/snapshot_07_12_15_17_08_56/_restore"
{"error":"RemoteTransportException[[hostname2][inet[/ipaddress:9300]][cluster:admin/snapshot/restore]]; nested: ConcurrentSnapshotExecutionException[[snapshot_restore_repo:snapshot_07_12_15_17_08_56] Restore process is already running in this cluster]; ","status":503}-bash-4.1$

Cluster state is shown below

GET _cluster/state

"restore": {
         "snapshots": [
            {
               "snapshot": "snapshot_07_12_15_17_08_56",
               "repository": "snapshot_restore_repo",
               "state": "STARTED",
               "indices": [
                  "indexname"
               ],
               "shards": [
                  {
                     "index": "indexname",
                     "shard": 0,
                     "state": "SUCCESS"
                  },
                  {
                     "index": "indexname",
                     "shard": 1,
                     "state": "SUCCESS"
                  },
                  {
                     "index": "indexname",
                     "shard": 2,
                     "state": "INIT"
                  },
                  {
                     "index": "indexname",
                     "shard": 3,
                     "state": "SUCCESS"
                  },
                  {
                     "index": "indexname",
                     "shard": 4,
                     "state": "SUCCESS"
                  }
               ]
            }
         ]
      }

How to come out of "INIT" and complete restoration
</description><key id="122190563">15432</key><summary>restoration from a snapshot is failing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">binishabraham</reporter><labels><label>:Snapshot/Restore</label><label>feedback_needed</label></labels><created>2015-12-15T04:18:39Z</created><updated>2016-02-02T13:53:01Z</updated><resolved>2016-02-02T13:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T13:53:49Z" id="164771497">What version of Elasticsearch are you using?
</comment><comment author="binishabraham" created="2015-12-15T16:06:22Z" id="164809463">I am using 1.7.0
</comment><comment author="clintongormley" created="2015-12-16T10:32:34Z" id="165059897">@imotov @ywelsch any ideas?
</comment><comment author="imotov" created="2015-12-16T11:19:30Z" id="165070629">@binishabraham anything in the log file on master?
</comment><comment author="binishabraham" created="2015-12-16T20:37:17Z" id="165237493">@clintongormley ,
@imotov 

Please find more details

I used to restore the snapshot from one cluster to three other clusters.

Saw the following error during restoration from that particulat snapshot to last cluster

{"error":"MasterNotDiscoveredException[]","status":503}

But even after this exception I was able to find all the documents in the last cluster also. Count of documents equal to other three clusters.

From the next day snapshot onwards, I am getting the following exception for the fourth cluster

 {"error":"RemoteTransportException[[app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]][cluster:admin/repository/put]]; nested: ElasticsearchIllegalStateException[trying to modify or unregister repository that is currently used ]; ","status":500}  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
372                                  Dload  Upload   Total   Spent    Left  Speed
373 ^M  0     0    0     0    0    60      0  49059 --:--:-- --:--:-- --:--:-- 49059^M  0     0    0     0    0    60      0         59 --:--:--  0:00:01 --:--:--     0^M  0     0    0     0    0    60      0     29 --:--:--  0:00:02 --:--:--     0^M  0         0    0     0    0    60      0     19 --:--:--  0:00:03 --:--:--     0^M  0     0    0     0    0    60      0     14 -    -:--:--  0:00:04 --:--:--     0^M176   293  146   293    0    60     68     14  0:00:04  0:00:04 --:--:--    54
374 {"error":"RemoteTransportException[[app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]][cluster:admin/snapshot/restore]]; nested: ConcurrentSnapshotExecutionException[[snapshot_restore_repo:snapshot_08_12_15_15_32_36] Restore process is already running in this cluster]; ","status":503}

But cluster-2 and cluster-3 restores it successfully from same snapshot.

Exception from same day's ES log.

-bash-4.1$ grep Exception es-prd-b-cluster.log.2015-12-07
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[/172.28.74.67:9300]][internal:discovery/zen/unicast_gte_1_4] request_id [2412434] timed out after [3751ms]
[2015-12-07 23:06:19,862][INFO ][discovery.zen] [app-wc-b1p.sys.company.net] failed to send join request to master [[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}], reason [RemoteTransportException[[app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]][internal:discovery/zen/join]]; nested: ElasticsearchIllegalStateException[Node [[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}] not master for join request from [[app-wc-b1p.sys.company.net][wVA3GQwHTW6MM4jHlubQqA][app-wc-b1p.sys.company.net][inet[/172.28.74.7:9300]]{master=true}]]; ], tried [3] times
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[/172.28.74.67:9300]][internal:discovery/zen/unicast_gte_1_4] request_id [2412470] timed out after [3750ms]
org.elasticsearch.ElasticsearchIllegalStateException: cluster state from a different master then the current one, rejecting (received [app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}, current [app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true})

I have the following questions as part of immediate fix
1. Any fix to this instead of recreating the index and reloading the data 
2. Is there any provision to restore all the snapshots present in my share path
   These are created on daily basis and present in a NAS mount point as /NAS/esbackup and has the names as follows
   snapshot-snapshot_08_12_15_15_32_36, snapshot-snapshot_10_12_15_17_13_05 etc.
</comment><comment author="imotov" created="2015-12-17T10:15:10Z" id="165407931">@binishabraham could you post here or send me by email the output of the following command as well as complete log from the master node?

```
curl "localhost:9200/_cluster/state/routing_table,customs"
```
</comment><comment author="binishabraham" created="2015-12-17T17:43:19Z" id="165525160">@imotov 
Please find details

I am using 7 node cluster. 5 - data nodes eligible to become master. 2 - client nodes

Log from master
[2015-12-07 23:01:45,541][WARN ][transport                ] [app-wc-b1p.sys.company.net] Received response for a request that has timed out, sent [72441ms] ago, timed out [42440ms] ago, action [internal:discovery/zen/fd/master_ping], node [[app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}], id [2412225]
[2015-12-07 23:01:45,543][WARN ][transport                ] [app-wc-b1p.sys.company.net] Received response for a request that has timed out, sent [42442ms] ago, timed out [12442ms] ago, action [internal:discovery/zen/fd/master_ping], node [[app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}], id [2412226]
[2015-12-07 23:02:23,013][WARN ][monitor.jvm              ] [app-wc-b1p.sys.company.net] [gc][young][2250943][49420] duration [1.4s], collections [1]/[1.7s], total [1.4s]/[32m], memory [7gb]-&gt;[6.8gb]/[9.9gb], all_pools {[young] [248.8mb]-&gt;[878.5kb]/[266.2mb]}{[survivor] [20.6mb]-&gt;[21.9mb]/[33.2mb]}{[old] [6.7gb]-&gt;[6.7gb]/[9.6gb]}
[2015-12-07 23:06:15,118][INFO ][discovery.zen            ] [app-wc-b1p.sys.company.net] master_left [[app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2015-12-07 23:06:15,119][WARN ][discovery.zen            ] [app-wc-b1p.sys.company.net] master left (reason = failed to ping, tried [3] times, each with  maximum [30s] timeout), current nodes: {[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true},[app-wc-b5p.sys.company.net][fCe-HjVqSdSj8Opr0A0amQ][app-wc-b5p.sys.company.net][inet[/172.28.74.69:9300]]{master=true},[app-wc-b4p.sys.company.net][ctuotCZqRjGbsZ1LfOIa0A][app-wc-b4p.sys.company.net][inet[/172.28.74.68:9300]]{master=true},[app-wc-b1p.sys.company.net][wVA3GQwHTW6MM4jHlubQqA][app-wc-b1p.sys.company.net][inet[/172.28.74.7:9300]]{master=true},[cspesweb-wc-b2p.sys.company.net][MNtdFAsqRI6SryWUEawdqA][cspesweb-wc-b2p.sys.company.net][inet[/76.96.55.242:9300]]{data=false, master=false},[cspesweb-wc-b1p.sys.company.net][vcU4GzyDTMCrYPfN_dbYkw][cspesweb-wc-b1p.sys.company.net][inet[/76.96.55.241:9300]]{data=false, master=false},}
[2015-12-07 23:06:15,119][INFO ][cluster.service          ] [app-wc-b1p.sys.company.net] removed {[app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true},}, reason: zen-disco-master_failed ([app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true})
[2015-12-07 23:06:18,876][WARN ][discovery.zen.ping.unicast] [app-wc-b1p.sys.company.net] failed to send ping to [[#zen_unicast_3#][app-wc-b1p.sys.company.net][inet[/172.28.74.67:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[/172.28.74.67:9300]][internal:discovery/zen/unicast_gte_1_4] request_id [2412434] timed out after [3751ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-07 23:06:19,862][INFO ][discovery.zen            ] [app-wc-b1p.sys.company.net] failed to send join request to master [[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}], reason [RemoteTransportException[[app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]][internal:discovery/zen/join]]; nested: ElasticsearchIllegalStateException[Node [[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}] not master for join request from [[app-wc-b1p.sys.company.net][wVA3GQwHTW6MM4jHlubQqA][app-wc-b1p.sys.company.net][inet[/172.28.74.7:9300]]{master=true}]]; ], tried [3] times
[2015-12-07 23:06:21,235][INFO ][cluster.service          ] [app-wc-b1p.sys.company.net] detected_master [app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}, reason: zen-disco-receive(from master [[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}])
[2015-12-07 23:06:23,631][WARN ][discovery.zen.ping.unicast] [app-wc-b1p.sys.company.net] failed to send ping to [[#zen_unicast_3#][app-wc-b1p.sys.company.net][inet[/172.28.74.67:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[/172.28.74.67:9300]][internal:discovery/zen/unicast_gte_1_4] request_id [2412470] timed out after [3750ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-07 23:08:21,515][WARN ][discovery.zen            ] [app-wc-b1p.sys.company.net] received a cluster state from a different master then the current one, rejecting (received [app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}, current [app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true})
[2015-12-07 23:08:21,516][ERROR][discovery.zen            ] [app-wc-b1p.sys.company.net] unexpected failure during [zen-disco-receive(from master [[app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}])]
org.elasticsearch.ElasticsearchIllegalStateException: cluster state from a different master then the current one, rejecting (received [app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true}, current [app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true})
        at org.elasticsearch.discovery.zen.ZenDiscovery.shouldIgnoreOrRejectNewClusterState(ZenDiscovery.java:898)
        at org.elasticsearch.discovery.zen.ZenDiscovery$10.execute(ZenDiscovery.java:778)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:374)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-07 23:08:24,929][INFO ][cluster.service          ] [app-wc-b1p.sys.company.net] added {[app-wc-b3p.sys.company.net][QNbUU32-T3yYaVBGBf6z9A][app-wc-b3p.sys.company.net][inet[/172.28.74.67:9300]]{master=true},}, reason: zen-disco-receive(from master [[app-wc-b2p.sys.company.net][FJ3lMq59TKmnSfdPlJNm5g][app-wc-b2p.sys.company.net][inet[/172.28.74.70:9300]]{master=true}])

GET _cluster/state/routing_table,customs
{
   "cluster_name": "es-prd-b-cluster",
   "routing_table": {
      "indices": {
         "company_prd": {
            "shards": {
               "0": [
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "R3BcxJx1SfiLhdvGr8A-5Q",
                     "relocating_node": null,
                     "shard": 0,
                     "index": "company_prd"
                  },
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "bufngu7lS9igVfz0G1xVsg",
                     "relocating_node": null,
                     "shard": 0,
                     "index": "company_prd"
                  }
               ],
               "1": [
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "T6DUea7CTmm8VnjEDXCvMg",
                     "relocating_node": null,
                     "shard": 1,
                     "index": "company_prd"
                  },
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "o32Diim1T6qQGKrZynjsXw",
                     "relocating_node": null,
                     "shard": 1,
                     "index": "company_prd"
                  }
               ],
               "2": [
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "o32Diim1T6qQGKrZynjsXw",
                     "relocating_node": null,
                     "shard": 2,
                     "index": "company_prd"
                  },
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "pWSBCl57Qqu3b5R5EF1-vg",
                     "relocating_node": null,
                     "shard": 2,
                     "index": "company_prd"
                  }
               ],
               "3": [
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "bufngu7lS9igVfz0G1xVsg",
                     "relocating_node": null,
                     "shard": 3,
                     "index": "company_prd"
                  },
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "pWSBCl57Qqu3b5R5EF1-vg",
                     "relocating_node": null,
                     "shard": 3,
                     "index": "company_prd"
                  }
               ],
               "4": [
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "T6DUea7CTmm8VnjEDXCvMg",
                     "relocating_node": null,
                     "shard": 4,
                     "index": "company_prd"
                  },
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "R3BcxJx1SfiLhdvGr8A-5Q",
                     "relocating_node": null,
                     "shard": 4,
                     "index": "company_prd"
                  }
               ]
            }
         },
         "company": {
            "shards": {
               "0": [
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "bufngu7lS9igVfz0G1xVsg",
                     "relocating_node": null,
                     "shard": 0,
                     "index": "company"
                  },
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "o32Diim1T6qQGKrZynjsXw",
                     "relocating_node": null,
                     "shard": 0,
                     "index": "company"
                  }
               ],
               "1": [
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "bufngu7lS9igVfz0G1xVsg",
                     "relocating_node": null,
                     "shard": 1,
                     "index": "company"
                  },
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "o32Diim1T6qQGKrZynjsXw",
                     "relocating_node": null,
                     "shard": 1,
                     "index": "company"
                  }
               ],
               "2": [
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "T6DUea7CTmm8VnjEDXCvMg",
                     "relocating_node": null,
                     "shard": 2,
                     "index": "company"
                  },
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "R3BcxJx1SfiLhdvGr8A-5Q",
                     "relocating_node": null,
                     "shard": 2,
                     "index": "company"
                  }
               ],
               "3": [
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "R3BcxJx1SfiLhdvGr8A-5Q",
                     "relocating_node": null,
                     "shard": 3,
                     "index": "company"
                  },
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "pWSBCl57Qqu3b5R5EF1-vg",
                     "relocating_node": null,
                     "shard": 3,
                     "index": "company"
                  }
               ],
               "4": [
                  {
                     "state": "STARTED",
                     "primary": true,
                     "node": "T6DUea7CTmm8VnjEDXCvMg",
                     "relocating_node": null,
                     "shard": 4,
                     "index": "company"
                  },
                  {
                     "state": "STARTED",
                     "primary": false,
                     "node": "pWSBCl57Qqu3b5R5EF1-vg",
                     "relocating_node": null,
                     "shard": 4,
                     "index": "company"
                  }
               ]
            }
         }
      }
   },
   "routing_nodes": {
      "unassigned": [],
      "nodes": {
         "T6DUea7CTmm8VnjEDXCvMg": [
            {
               "state": "STARTED",
               "primary": false,
               "node": "T6DUea7CTmm8VnjEDXCvMg",
               "relocating_node": null,
               "shard": 1,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "T6DUea7CTmm8VnjEDXCvMg",
               "relocating_node": null,
               "shard": 4,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "T6DUea7CTmm8VnjEDXCvMg",
               "relocating_node": null,
               "shard": 2,
               "index": "company"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "T6DUea7CTmm8VnjEDXCvMg",
               "relocating_node": null,
               "shard": 4,
               "index": "company"
            }
         ],
         "R3BcxJx1SfiLhdvGr8A-5Q": [
            {
               "state": "STARTED",
               "primary": false,
               "node": "R3BcxJx1SfiLhdvGr8A-5Q",
               "relocating_node": null,
               "shard": 0,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "R3BcxJx1SfiLhdvGr8A-5Q",
               "relocating_node": null,
               "shard": 4,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "R3BcxJx1SfiLhdvGr8A-5Q",
               "relocating_node": null,
               "shard": 2,
               "index": "company"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "R3BcxJx1SfiLhdvGr8A-5Q",
               "relocating_node": null,
               "shard": 3,
               "index": "company"
            }
         ],
         "bufngu7lS9igVfz0G1xVsg": [
            {
               "state": "STARTED",
               "primary": true,
               "node": "bufngu7lS9igVfz0G1xVsg",
               "relocating_node": null,
               "shard": 0,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "bufngu7lS9igVfz0G1xVsg",
               "relocating_node": null,
               "shard": 3,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "bufngu7lS9igVfz0G1xVsg",
               "relocating_node": null,
               "shard": 0,
               "index": "company"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "bufngu7lS9igVfz0G1xVsg",
               "relocating_node": null,
               "shard": 1,
               "index": "company"
            }
         ],
         "o32Diim1T6qQGKrZynjsXw": [
            {
               "state": "STARTED",
               "primary": false,
               "node": "o32Diim1T6qQGKrZynjsXw",
               "relocating_node": null,
               "shard": 2,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "o32Diim1T6qQGKrZynjsXw",
               "relocating_node": null,
               "shard": 1,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "o32Diim1T6qQGKrZynjsXw",
               "relocating_node": null,
               "shard": 0,
               "index": "company"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "o32Diim1T6qQGKrZynjsXw",
               "relocating_node": null,
               "shard": 1,
               "index": "company"
            }
         ],
         "pWSBCl57Qqu3b5R5EF1-vg": [
            {
               "state": "STARTED",
               "primary": true,
               "node": "pWSBCl57Qqu3b5R5EF1-vg",
               "relocating_node": null,
               "shard": 2,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": true,
               "node": "pWSBCl57Qqu3b5R5EF1-vg",
               "relocating_node": null,
               "shard": 3,
               "index": "company_prd"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "pWSBCl57Qqu3b5R5EF1-vg",
               "relocating_node": null,
               "shard": 3,
               "index": "company"
            },
            {
               "state": "STARTED",
               "primary": false,
               "node": "pWSBCl57Qqu3b5R5EF1-vg",
               "relocating_node": null,
               "shard": 4,
               "index": "company"
            }
         ]
      }
   },
   "allocations": []
}
</comment><comment author="imotov" created="2015-12-17T23:36:44Z" id="165616064">@binishabraham What happened to the `indexname` index? Did you delete it?
</comment><comment author="binishabraham" created="2015-12-18T19:26:56Z" id="165876753">@imotov It was a mistake in my replacement
Here indexname (previous post) = company (last post)
</comment><comment author="imotov" created="2015-12-18T19:41:22Z" id="165879886">@binishabraham could you try closing and opening the company index to see if it will get recovery process unstuck?
</comment><comment author="imotov" created="2016-01-04T19:51:37Z" id="168784685">@binishabraham any updates?
</comment><comment author="binishabraham" created="2016-01-05T20:13:39Z" id="169118544">@imotov I tried, but didn't workout. So deleted the index 'indexname' and restored it using the snapshot from similar ES cluster from another Farm.
</comment><comment author="clintongormley" created="2016-02-02T13:53:01Z" id="178582684">No further info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Primary shard stuck initialising on dynamic index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15431</link><project id="" key="" /><description>Hi,

I have a 5 node 2.1.0 elasticsearch cluster receiving about 150k of firewall logs per minute from 3 load balanced logstash servers. The index template is configured for 5 shards and 1 replica and each shard contains about 20GB of logs per day. It all works great until the clock ticks over to the next day. The new index is dynamically created however at least one of the primary shards is stuck initiallising and doesn't recovery. At this point I have to stop the logs coming in. Delete the index and start the logs again.

Any ideas?

Cheers,

Marty
</description><key id="122174303">15431</key><summary>Primary shard stuck initialising on dynamic index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels><label>:Recovery</label><label>bug</label><label>feedback_needed</label></labels><created>2015-12-15T01:53:29Z</created><updated>2016-01-22T18:31:08Z</updated><resolved>2015-12-17T00:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T13:49:01Z" id="164769318">Hi @robertsmarty 

Please can you upload your logs around midnight.  Also, do you have any shard allocation settings?
</comment><comment author="robertsmarty" created="2015-12-17T00:03:09Z" id="165294988">Sorry to waste your time with this. I'll investigate this further myself as I think it's a load issue and I need to do some tuning.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch logging problems are unfriendly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15430</link><project id="" key="" /><description>Our logging abstraction is weird.

We can override our default logging configuration by adding extra files in the configuration directory with a name starting with `logging.` and ending with `.yml`. 

So consider when I add the following extra logging configuration file, `logging.whoops.yml`:

```
logger.index.search.slowlog : WARN
logger.index.indexing.slowlog : WARN
```

Whoops, I just broke slow logging.  When a slow log entry is attempted to be logged I get the following unhelpful errors:

```
log4j:WARN No appenders could be found for logger (index.search.slowlog.query).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
log4j:WARN No appenders could be found for logger (index.search.slowlog.query).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```

 For our handling of slow logs, the above lines don't really make sense, but it appears to trip users up, see #7461.  

It doesn't look like we do any syntax checking or warn when a user adds logging configuration that could break our abstraction.   Should we be doing that?
</description><key id="122167471">15430</key><summary>Elasticsearch logging problems are unfriendly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>:Logging</label><label>adoptme</label><label>bug</label></labels><created>2015-12-15T00:56:34Z</created><updated>2016-02-14T17:39:28Z</updated><resolved>2016-02-14T17:39:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-12-15T02:31:44Z" id="164624075">Similar issue here (https://github.com/elastic/elasticsearch/issues/14953).  The "log4j:WARN No appenders could be found for logger" warnings can be improved.
</comment><comment author="colings86" created="2015-12-18T10:56:53Z" id="165748214">Discussed in FixItFriday: We should fix this by adding validation in our code that picks up the logging file that only allows the file to be called 'logging.yml' (as in errors if there is an extra logging file) and also errors if the logging file is missing.
</comment><comment author="joshuar" created="2015-12-20T21:58:10Z" id="166157772">@colings86 so we're taking away the functionality of overriding the base `logging.yml` with your own custom `logging.something.yml`?  That's a nice feature, almost equivalent to the `*.d` config directory used by many *nix applications.  
</comment><comment author="clintongormley" created="2016-02-14T17:39:28Z" id="183935194">Closing in favour of #16585
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix IntelliJ query builder type inference issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15429</link><project id="" key="" /><description>This commit addresses two type inference issues that the IntelliJ source
editor struggles with when registering query builder prototypes in
o/e/i/q/IndicesQueriesRegistry.java and
o/e/i/q/f/ScoreFunctionParserMapper.java.
</description><key id="122160780">15429</key><summary>Fix IntelliJ query builder type inference issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-15T00:00:12Z</created><updated>2015-12-15T00:35:32Z</updated><resolved>2015-12-15T00:35:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-15T00:05:40Z" id="164599854">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explicitly log cluster state update failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15428</link><project id="" key="" /><description>This commit adds explicit logging at the DEBUG level for cluster state
update failures. Currently this responsibility is left to the cluster
state task listener, but we should expliclty log these with a generic
message to address cases where the listener might not.

Relates #14899, relates #15016, relates #15023
</description><key id="122147472">15428</key><summary>Explicitly log cluster state update failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-14T22:34:57Z</created><updated>2015-12-16T17:30:47Z</updated><resolved>2015-12-16T15:55:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-16T14:42:10Z" id="165129211">LGTM. Left a minor suggestion about the assertions.
</comment><comment author="jasontedor" created="2015-12-16T17:30:47Z" id="165185210">Integrated to master in 89c960b00aa8dcdbeb94866ea6a426092c8c7fd9 and backported to 2.x in 8632918d8600a054242a58fda526dc552dc81870.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document 1.6 breaking change for snapshot "path.repo" requirement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15427</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-1.6.html?q=breaking

Starting in ES 1.6, for shared file repositories, we require the setting of "path.repo" as part of a security fix.  If users have defined their snapshot repositories in versions &lt; 1.6 and perform an upgrade to 1.6+, they will encounter this breaking change.

This change is currently documented [here](https://www.elastic.co/blog/elasticsearch-1-6-0-released#fs-config), but it is missing from the breaking changes documentation
</description><key id="122139772">15427</key><summary>Document 1.6 breaking change for snapshot "path.repo" requirement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-12-14T21:52:42Z</created><updated>2016-01-15T12:38:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Provide option to also delete files when deleting repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15426</link><project id="" key="" /><description>Currently, when deleting a snapshot repository (by design), we leave the actual files in place so that one can recreate the repository at a later time and it will automatically pick up existing snapshots.  It will be nice to provide an option for those who would like the delete snapshot repository to also delete the underlying snapshot files (eg. the "start over" scenario).  This is helpful for the case where there are many snapshots in the repository and it will take a while to run individual delete snapshot api calls.
</description><key id="122132122">15426</key><summary>Provide option to also delete files when deleting repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>enhancement</label></labels><created>2015-12-14T21:10:57Z</created><updated>2015-12-15T12:52:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-14T21:19:51Z" id="164563013">++

Or may be a purge API? So you can remove all files in a given repository without having to delete the repo itself?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleting a document from a non-existing index creates the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15425</link><project id="" key="" /><description>Using the default settings, if a delete request is issued for a single document against a non-existing index, Elasticsearch will create the index. Steps to reproduce:

```
DELETE /foo
```

```
DELETE /foo/bar/1
```

Then:

```
GET /foo
```

responds with HTTP 200 OK and response body

```
{
  "foo" : {
    "aliases" : { },
    "mappings" : { },
    "settings" : {
      "index" : {
        "creation_date" : "1450124918393",
        "number_of_shards" : "5",
        "number_of_replicas" : "1",
        "uuid" : "Ny3PruSCTIG5TlYecU0_XA",
        "version" : {
          "created" : "3000099"
        }
      }
    },
    "warmers" : { }
  }
}
```

showing that it created the index.
</description><key id="122126143">15425</key><summary>Deleting a document from a non-existing index creates the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:CRUD</label><label>adoptme</label><label>bug</label></labels><created>2015-12-14T20:39:32Z</created><updated>2017-05-22T07:00:23Z</updated><resolved>2017-05-22T07:00:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="niemyjski" created="2016-08-08T17:27:03Z" id="238309047">+1 this just cost me a day of work.
</comment><comment author="bleskes" created="2016-12-02T10:29:20Z" id="264423046">Carrying the discussion over from #21926 

&gt; I think we should have a dedicated setting for this which defaults to false.

I would prefer to make deletes with an external version auto create indices rather than have a settings that controls deletes in general. It seems that's the only use case that needs it so we can have it contained. </comment><comment author="s1monw" created="2016-12-02T10:54:31Z" id="264428038">++ @bleskes </comment><comment author="nikkelj" created="2017-04-25T16:06:47Z" id="297080329">+1, I was using an index management scheme that was open loop deleting potentially existent indices based on time range.  This then creates thousands of ghost indices that cannot be deleted, and completely kills the cluster performance.  Poor behaviour.  I am on v5.20</comment><comment author="olcbean" created="2017-04-28T12:52:03Z" id="297989805">Hey guys, as this appears to be still an issue in master, I would like to give it a try iff nobody else is working on it.

As the discussion was held over time in multiple threads, let me sum up what the expected behavior should be : 
* if an external version is used : create the index ( the same behavior as of now )
* otherwise : throw an `index_not_found`
(the change should not introduce an additional `IndexOption`)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperService: check index.mapper.dynamic during index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15424</link><project id="" key="" /><description>The MapperService doesn't currently check the
index.mapper.dynamic setting during index creation,
so indices can be created with dynamic mappings even
if this setting is false. Add a check that throws an
exception in this case. Fixes #15381
</description><key id="122090782">15424</key><summary>MapperService: check index.mapper.dynamic during index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">davidvgalbraith</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-14T17:30:25Z</created><updated>2015-12-30T22:17:36Z</updated><resolved>2015-12-30T22:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2015-12-17T14:32:59Z" id="165466563">@davidvgalbraith thank you for your PR.
I think you mixed the index.mapper.dynamic at the index level and at the root level. The bug is when you set index.mapper.dynamic.false for all indices (not for a single index). You should look at TransportIndexAction.java where the auto create is launched. I think it's here that you need to check if the index.mapper.dynamic should be honored or not (you can access the settings through the cluster state).
</comment><comment author="davidvgalbraith" created="2015-12-17T22:53:13Z" id="165607409">Thanks for the tips! I moved the check into `TransportIndexAction.java` as suggested. I couldn't get to the settings through the cluster state but conveniently they were already present in the `settings` variable.
</comment><comment author="rjernst" created="2015-12-17T23:39:30Z" id="165616466">I'm not sure we should be doing this change in general. We need to try to move away from defaults like this in the node configuration. It is error prone because one node missing the setting can cause weird behavior (eg some indexes not getting the setting depending on which nodes the index is created on).

Instead, the better path is to use index templates.
</comment><comment author="davidvgalbraith" created="2015-12-18T00:51:45Z" id="165626869">I replaced all occurrences of  `"index.mapper.dynamic"` with `MapperService.DYNAMIC_MAPPING_ENABLED_SETTING` and fixed the licensing. I kept the test as an integration test as I need to run a `TransportIndexAction` which requires a whole lot of infrastructure -- https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java#L73 that I don't have in a unit test scenario to my knowledge.

@rjernst I don't understand how index templates factor into this issue? The problem is that `TransportIndexAction` doesn't check its configured setting for `index.mapper.dynamic` before creating an index, so it'll still ignore it if it's in an index template without this change.
</comment><comment author="rjernst" created="2015-12-18T06:57:53Z" id="165695250">@davidvgalbraith `TransportIndexAction` is looking at the node settings, which means it is looking for `index.mapper.dynamic` being set in `elasticsearch.yml` for that specific node the action runs on.
</comment><comment author="davidvgalbraith" created="2015-12-18T14:25:25Z" id="165789775">@rjernst Yes! I agree that looking at the node settings is janky. However, it is the documented API, so I submit that we should make it work. We can improve the API in a subsequent version of Elasticsearch.

@jimferenczi I made it throw a `TypeMissingException` with a message including the index and type.
</comment><comment author="davidvgalbraith" created="2015-12-18T14:34:37Z" id="165791479">@jasontedor I changed the variable names as suggested and used `INDEX_MAPPER_DYNAMIC_DEFAULT` where I had that `true`.
</comment><comment author="jasontedor" created="2015-12-18T14:37:58Z" id="165792134">&gt; I kept the test as an integration test as I need to run a TransportIndexAction which requires a whole lot of infrastructure -- https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/index/TransportIndexAction.java#L73 that I don't have in a unit test scenario to my knowledge.

@davidvgalbraith Yes, `TransportIndexAction` requires a lot of infrastructure, but we have other `Transport.*Action` classes that have supporting unit tests (instead of integration tests) so it might be possible here too and that would be preferred over an integration test. For example, see `TransportBroadcastByNodeAction` and its accompanying unit tests.
</comment><comment author="davidvgalbraith" created="2015-12-18T16:13:12Z" id="165819973">I made `DynamicMappingDisabledTests` a unit test.
</comment><comment author="davidvgalbraith" created="2015-12-28T18:33:38Z" id="167623284">Hey! Can we get a little more review on this / get it in? 
</comment><comment author="jasontedor" created="2015-12-28T19:07:32Z" id="167630100">&gt; Hey! Can we get a little more review on this / get it in?

@davidvgalbraith Please do note that activity is slow at this time of year and probably will be for another week due to seasonal holidays.
</comment><comment author="jasontedor" created="2015-12-28T19:37:49Z" id="167636258">@davidvgalbraith I left some more review comments.
</comment><comment author="davidvgalbraith" created="2015-12-29T02:49:09Z" id="167703890">Thank you for the review today @jasontedor and @jpountz! I made all the suggested changes. Rolling the logic into `AutoCreateIndex` means it now throws an `IndexNotFoundException` when you try to autocreate an index with dynamic mappings disabled. That's the same thing it throws if automatic index creation is disabled, which seems like an admirable consistency. 
</comment><comment author="davidvgalbraith" created="2015-12-29T22:59:32Z" id="167896521">Thank you for the suggestions @jasontedor, I've implemented them all. Think we're getting close on this one!
</comment><comment author="davidvgalbraith" created="2015-12-29T23:45:53Z" id="167900953">Ok, I changed the assertion to `assertThat(e, instanceOf(IndexNotFoundException.class));`, commented the nullification of `THREAD_POOL`, and used `new AtomicBoolean()` instead of `new AtomicBoolean(false)`.
</comment><comment author="jasontedor" created="2015-12-29T23:47:06Z" id="167901044">&gt; Ok, I changed the assertion to `assertThat(e, instanceOf(IndexNotFoundException.class));`, commented the nullification of `THREAD_POOL`, and used `new AtomicBoolean()` instead of `new AtomicBoolean(false)`.

Excellent. The Gradle check task passes locally for me so this is looking good. I'm going to defer to @jpountz for final sign-off and we can integrate after that. Please do note it is still a slow time of the year. :)

Thanks a ton for all of your hard work here and your contribution to Elasticsearch!
</comment><comment author="davidvgalbraith" created="2015-12-29T23:57:09Z" id="167902100">Hey thank you for the patience and guidance, @jasontedor! 
</comment><comment author="jpountz" created="2015-12-30T09:21:38Z" id="167964581">This looks good to me too. I will merge later today.
</comment><comment author="jpountz" created="2015-12-30T22:17:36Z" id="168082911">I squashed and merged manually. Thanks @davidvgalbraith!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix bwc for NodeInfo, plugins and modules streaming was changed only &#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15423</link><project id="" key="" /><description>&#8230;in 2.2

see #15303 

While at it, I found two more things: 
- #15422, see comment in the test
- the bwc tests still don't all pass (edit: now made a pr here: #15453). I get a failure when I run `mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.qa.backwards:2.0.0 -Dtests.seed=96FBCE4CB2B7B1EC -Dtests.class=org.elasticsearch.bwcompat.SnapshotBackwardsCompatibilityIT -Dtests.method="testBasicWorkflow" -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=hi_IN -Dtests.timezone=SystemV/CST6CDT` but it is a different failure and seems unrelated.

I'd like to fix these two things after this is pushed.
</description><key id="122090139">15423</key><summary>fix bwc for NodeInfo, plugins and modules streaming was changed only &#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-12-14T17:26:48Z</created><updated>2015-12-15T17:32:06Z</updated><resolved>2015-12-15T10:45:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-14T18:01:59Z" id="164511635">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>node info does not contain os arch and name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15422</link><project id="" key="" /><description>These two parameters are not serialized (https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/monitor/os/OsInfo.java#L107), although they are build in the json response (https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/monitor/os/OsInfo.java#L82). Consequently, when I start two nodes on one machine I get for one node

```
"os": {
        "refresh_interval_in_millis": 1000,
        "name": "Linux",
        "arch": "amd64",
        "version": "3.13.0-39-generic",
        "available_processors": 12,
        "allocated_processors": 12
      },
```

and

```
"os": {
        "refresh_interval_in_millis": 1000,
        "available_processors": 12,
        "allocated_processors": 12
      },
```

for the one that did not handle the request.
Is there any reason for it or should I fix it?
</description><key id="122073437">15422</key><summary>node info does not contain os arch and name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Stats</label><label>bug</label><label>v2.2.0</label></labels><created>2015-12-14T16:07:15Z</created><updated>2015-12-16T10:24:59Z</updated><resolved>2015-12-16T10:24:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-14T16:35:04Z" id="164487450">+1 to fix
</comment><comment author="nik9000" created="2015-12-14T16:36:48Z" id="164487946">Seems worth fixing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch index in red state after open</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15421</link><project id="" key="" /><description>I'm using elasticsearch 1.5.0 (to be upgraded to 2.0.0 soon) in a 4 node cluster with two serving as master/data nodes. I've run this configuration and version many times but never faced this problem. A lot of my logstash indexes are closed and I only keep recent ones open. But when I open one of the indexes, the index status remains at red. It is supposed to balance and change to green. There is absolutely no error in logs.  It is not even trying to rebalance. Any ideas as to why this would happen?

This is cluster state after _open logstash-2015.10.16

```
curl http://localhost:9200/_cluster/health?pretty
{
  "cluster_name" : "_Logstash.ElasticSearch_",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 4,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 81,
  "active_shards" : 162,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 10,
  "number_of_pending_tasks" : 0
}

```

Shards remain unassigned at 10. 

```
 curl http://localhost:9200/_cat/indices?pretty
      close logstash-2015.10.23
      close logstash-2015.11.29
green open  logstash-2015.12.09 5 1 3410927 0   3.2gb   1.6gb
      close logstash-2015.11.10
      close logstash-2015.11.15
      close logstash-2015.10.31
      close logstash-2015.10.24
      close logstash-2015.11.01
green open  .kibana             1 1      82 5 423.9kb 198.9kb
      close logstash-2015.11.08
      close logstash-2015.11.19
      close logstash-2015.11.09
      close logstash-2015.10.21
      close logstash-2015.11.05
red   open  logstash-2015.10.16 5 1
      close logstash-2015.11.16
green open  logstash-2015.11.30 5 1 2609254 0   2.7gb   1.3gb
      close logstash-2015.10.19
green open  logstash-2015.12.13 5 1 3400366 0   3.7gb   1.8gb
      close logstash-2015.11.11
      close logstash-2015.11.04
      close logstash-2015.10.17
      close logstash-2015.11.06
      close logstash-2015.11.20
      close logstash-2015.10.25
      close logstash-2015.11.22
      close logstash-2015.10.18
green open  logstash-2015.12.08 5 1 2853164 0   2.9gb   1.4gb
green open  logstash-2015.12.12 5 1 2628382 0   2.6gb   1.3gb
green open  logstash-2015.12.03 5 1 2828414 0     3gb   1.5gb
      close logstash-2015.10.30
      close logstash-2015.11.21
      close logstash-2015.11.26
green open  logstash-2015.12.04 5 1 3078653 0   3.3gb   1.6gb
      close logstash-2015.11.03
      close logstash-2015.11.12
      close logstash-2015.10.29
green open  logstash-2015.12.14 5 1 2605670 0   2.9gb   1.4gb
      close logstash-2015.11.13
green open  logstash-2015.12.07 5 1 3082722 0   3.3gb   1.6gb
      close logstash-2015.11.18
      close logstash-2015.11.14
      close logstash-2015.11.28
      close logstash-2015.11.07
      close logstash-2015.11.17
      close logstash-2015.11.25
      close logstash-2015.11.27
green open  logstash-2015.12.10 5 1 2985242 0   3.3gb   1.6gb
green open  logstash-2015.12.02 5 1 2705853 0   2.9gb   1.4gb
green open  logstash-2015.12.11 5 1 2936190 0   3.1gb   1.5gb
      close logstash-2015.11.23
      close logstash-2015.10.22
      close logstash-2015.10.27
green open  kibana-int          5 1      15 0 394.3kb 197.2kb
      close logstash-2015.10.26
green open  logstash-2015.12.01 5 1 2670255 0   2.9gb   1.4gb
      close logstash-2015.10.28
      close logstash-2015.11.02
green open  logstash-2015.12.06 5 1 2238398 0     2gb     1gb
green open  logstash-2015.12.05 5 1 2725170 0   2.8gb   1.4gb
      close logstash-2015.11.24
      close logstash-2015.10.20

```

```
curl http://localhost:9200/_cat/shards?pretty
logstash-2015.12.08 4 r STARTED    570527 305.5mb 127.0.0.1 Paibo
logstash-2015.12.08 4 p STARTED    570527 305.5mb 127.0.0.1 Temugin
logstash-2015.12.08 0 r STARTED    570517 305.4mb 127.0.0.1 Paibo
logstash-2015.12.08 0 p STARTED    570517 305.4mb 127.0.0.1 Temugin
logstash-2015.12.08 3 r STARTED    570700   304mb 127.0.0.1 Paibo
logstash-2015.12.08 3 p STARTED    570700   304mb 127.0.0.1 Temugin
logstash-2015.12.08 1 r STARTED    570761   304mb 127.0.0.1 Paibo
logstash-2015.12.08 1 p STARTED    570761   304mb 127.0.0.1 Temugin
logstash-2015.12.08 2 r STARTED    570659 304.7mb 127.0.0.1 Paibo
logstash-2015.12.08 2 p STARTED    570659 304.7mb 127.0.0.1 Temugin
logstash-2015.12.07 2 r STARTED    616275 338.3mb 127.0.0.1 Paibo
logstash-2015.12.07 2 p STARTED    616275 338.3mb 127.0.0.1 Temugin
logstash-2015.12.07 0 r STARTED    616398 339.2mb 127.0.0.1 Paibo
logstash-2015.12.07 0 p STARTED    616398 339.2mb 127.0.0.1 Temugin
logstash-2015.12.07 3 r STARTED    616512 338.1mb 127.0.0.1 Paibo
logstash-2015.12.07 3 p STARTED    616512 338.1mb 127.0.0.1 Temugin
logstash-2015.12.07 1 r STARTED    616673 338.5mb 127.0.0.1 Paibo
logstash-2015.12.07 1 p STARTED    616673 338.5mb 127.0.0.1 Temugin
logstash-2015.12.07 4 r STARTED    616864 338.9mb 127.0.0.1 Paibo
logstash-2015.12.07 4 p STARTED    616864 338.9mb 127.0.0.1 Temugin
logstash-2015.12.09 4 r STARTED    682098 338.9mb 127.0.0.1 Paibo
logstash-2015.12.09 4 p STARTED    682098 338.9mb 127.0.0.1 Temugin
logstash-2015.12.09 0 r STARTED    682158   336mb 127.0.0.1 Paibo
logstash-2015.12.09 0 p STARTED    682158   336mb 127.0.0.1 Temugin
logstash-2015.12.09 3 r STARTED    681864   336mb 127.0.0.1 Paibo
logstash-2015.12.09 3 p STARTED    681864   336mb 127.0.0.1 Temugin
logstash-2015.12.09 1 r STARTED    682583 336.7mb 127.0.0.1 Paibo
logstash-2015.12.09 1 p STARTED    682583 336.7mb 127.0.0.1 Temugin
logstash-2015.12.09 2 r STARTED    682224 336.4mb 127.0.0.1 Paibo
logstash-2015.12.09 2 p STARTED    682224 336.4mb 127.0.0.1 Temugin
logstash-2015.12.04 2 r STARTED    615649 340.6mb 127.0.0.1 Paibo
logstash-2015.12.04 2 p STARTED    615649 340.6mb 127.0.0.1 Temugin
logstash-2015.12.04 0 r STARTED    615846 340.9mb 127.0.0.1 Paibo
logstash-2015.12.04 0 p STARTED    615846 340.9mb 127.0.0.1 Temugin
logstash-2015.12.04 3 r STARTED    615920 341.6mb 127.0.0.1 Paibo
logstash-2015.12.04 3 p STARTED    615920 341.6mb 127.0.0.1 Temugin
logstash-2015.12.04 1 r STARTED    615627 341.8mb 127.0.0.1 Paibo
logstash-2015.12.04 1 p STARTED    615627 341.8mb 127.0.0.1 Temugin
logstash-2015.12.04 4 r STARTED    615611 341.2mb 127.0.0.1 Paibo
logstash-2015.12.04 4 p STARTED    615611 341.2mb 127.0.0.1 Temugin
logstash-2015.12.03 2 r STARTED    565975 315.1mb 127.0.0.1 Paibo
logstash-2015.12.03 2 p STARTED    565975 315.1mb 127.0.0.1 Temugin
logstash-2015.12.03 0 r STARTED    565497 314.7mb 127.0.0.1 Paibo
logstash-2015.12.03 0 p STARTED    565497 314.7mb 127.0.0.1 Temugin
logstash-2015.12.03 3 r STARTED    565772 314.5mb 127.0.0.1 Paibo
logstash-2015.12.03 3 p STARTED    565772 314.5mb 127.0.0.1 Temugin
logstash-2015.12.03 1 r STARTED    565257 314.7mb 127.0.0.1 Paibo
logstash-2015.12.03 1 p STARTED    565257 314.7mb 127.0.0.1 Temugin
logstash-2015.12.03 4 r STARTED    565913 313.5mb 127.0.0.1 Paibo
logstash-2015.12.03 4 p STARTED    565913 313.5mb 127.0.0.1 Temugin
logstash-2015.10.16 4 p UNASSIGNED
logstash-2015.10.16 4 r UNASSIGNED
logstash-2015.10.16 0 p UNASSIGNED
logstash-2015.10.16 0 r UNASSIGNED
logstash-2015.10.16 3 p UNASSIGNED
logstash-2015.10.16 3 r UNASSIGNED
logstash-2015.10.16 1 p UNASSIGNED
logstash-2015.10.16 1 r UNASSIGNED
logstash-2015.10.16 2 p UNASSIGNED
logstash-2015.10.16 2 r UNASSIGNED
logstash-2015.12.06 4 r STARTED    447840   208mb 127.0.0.1 Paibo
logstash-2015.12.06 4 p STARTED    447840   208mb 127.0.0.1 Temugin
logstash-2015.12.06 0 r STARTED    447607 207.3mb 127.0.0.1 Paibo
logstash-2015.12.06 0 p STARTED    447607 207.3mb 127.0.0.1 Temugin
logstash-2015.12.06 3 r STARTED    447830 207.3mb 127.0.0.1 Paibo
logstash-2015.12.06 3 p STARTED    447830 207.3mb 127.0.0.1 Temugin
logstash-2015.12.06 1 r STARTED    447669 207.2mb 127.0.0.1 Paibo
logstash-2015.12.06 1 p STARTED    447669 207.2mb 127.0.0.1 Temugin
logstash-2015.12.06 2 r STARTED    447452 206.3mb 127.0.0.1 Paibo
logstash-2015.12.06 2 p STARTED    447452 206.3mb 127.0.0.1 Temugin
logstash-2015.12.14 4 p STARTED    521729 301.5mb 127.0.0.1 Paibo
logstash-2015.12.14 4 r STARTED    521719 300.8mb 127.0.0.1 Temugin
logstash-2015.12.14 0 p STARTED    521579 301.6mb 127.0.0.1 Paibo
logstash-2015.12.14 0 r STARTED    521570 306.6mb 127.0.0.1 Temugin
logstash-2015.12.14 3 r STARTED    521396 306.4mb 127.0.0.1 Paibo
logstash-2015.12.14 3 p STARTED    521400 298.9mb 127.0.0.1 Temugin
logstash-2015.12.14 1 r STARTED    521720 307.5mb 127.0.0.1 Paibo
logstash-2015.12.14 1 p STARTED    521723 297.1mb 127.0.0.1 Temugin
logstash-2015.12.14 2 p STARTED    521725 296.7mb 127.0.0.1 Paibo
logstash-2015.12.14 2 r STARTED    521724 297.8mb 127.0.0.1 Temugin
logstash-2015.12.05 2 r STARTED    545282 290.1mb 127.0.0.1 Paibo
logstash-2015.12.05 2 p STARTED    545282 290.1mb 127.0.0.1 Temugin
logstash-2015.12.05 0 r STARTED    545255 290.3mb 127.0.0.1 Paibo
logstash-2015.12.05 0 p STARTED    545255 290.3mb 127.0.0.1 Temugin
logstash-2015.12.05 3 r STARTED    544818 287.5mb 127.0.0.1 Paibo
logstash-2015.12.05 3 p STARTED    544818 287.5mb 127.0.0.1 Temugin
logstash-2015.12.05 1 r STARTED    545034 289.1mb 127.0.0.1 Paibo
logstash-2015.12.05 1 p STARTED    545034 289.1mb 127.0.0.1 Temugin
logstash-2015.12.05 4 r STARTED    544781 288.9mb 127.0.0.1 Paibo
logstash-2015.12.05 4 p STARTED    544781 288.9mb 127.0.0.1 Temugin
logstash-2015.12.10 2 r STARTED    597014 344.8mb 127.0.0.1 Paibo
logstash-2015.12.10 2 p STARTED    597014 343.9mb 127.0.0.1 Temugin
logstash-2015.12.10 0 r STARTED    597379 346.2mb 127.0.0.1 Paibo
logstash-2015.12.10 0 p STARTED    597379 345.6mb 127.0.0.1 Temugin
logstash-2015.12.10 3 r STARTED    597284 342.6mb 127.0.0.1 Paibo
logstash-2015.12.10 3 p STARTED    597284 340.6mb 127.0.0.1 Temugin
logstash-2015.12.10 1 r STARTED    597010 340.5mb 127.0.0.1 Paibo
logstash-2015.12.10 1 p STARTED    597010 340.5mb 127.0.0.1 Temugin
logstash-2015.12.10 4 r STARTED    596555 347.9mb 127.0.0.1 Paibo
logstash-2015.12.10 4 p STARTED    596555   348mb 127.0.0.1 Temugin
logstash-2015.12.11 4 p STARTED    587446 326.7mb 127.0.0.1 Paibo
logstash-2015.12.11 4 r STARTED    587446 322.9mb 127.0.0.1 Temugin
logstash-2015.12.11 0 p STARTED    587202   329mb 127.0.0.1 Paibo
logstash-2015.12.11 0 r STARTED    587202 329.9mb 127.0.0.1 Temugin
logstash-2015.12.11 3 r STARTED    587393 322.9mb 127.0.0.1 Paibo
logstash-2015.12.11 3 p STARTED    587393 323.3mb 127.0.0.1 Temugin
logstash-2015.12.11 1 r STARTED    586813 327.3mb 127.0.0.1 Paibo
logstash-2015.12.11 1 p STARTED    586813 327.1mb 127.0.0.1 Temugin
logstash-2015.12.11 2 p STARTED    587336 324.5mb 127.0.0.1 Paibo
logstash-2015.12.11 2 r STARTED    587336 325.3mb 127.0.0.1 Temugin
logstash-2015.12.12 4 p STARTED    525500 266.9mb 127.0.0.1 Paibo
logstash-2015.12.12 4 r STARTED    525500 268.5mb 127.0.0.1 Temugin
logstash-2015.12.12 0 p STARTED    525833   267mb 127.0.0.1 Paibo
logstash-2015.12.12 0 r STARTED    525833 268.1mb 127.0.0.1 Temugin
logstash-2015.12.12 3 r STARTED    525606   276mb 127.0.0.1 Paibo
logstash-2015.12.12 3 p STARTED    525606   274mb 127.0.0.1 Temugin
logstash-2015.12.12 1 r STARTED    525562 277.2mb 127.0.0.1 Paibo
logstash-2015.12.12 1 p STARTED    525562 278.3mb 127.0.0.1 Temugin
logstash-2015.12.12 2 p STARTED    525881 272.6mb 127.0.0.1 Paibo
logstash-2015.12.12 2 r STARTED    525881 272.2mb 127.0.0.1 Temugin
logstash-2015.12.13 2 p STARTED    679879 381.5mb 127.0.0.1 Paibo
logstash-2015.12.13 2 r STARTED    679879 381.4mb 127.0.0.1 Temugin
logstash-2015.12.13 0 p STARTED    680322 382.1mb 127.0.0.1 Paibo
logstash-2015.12.13 0 r STARTED    680322 381.5mb 127.0.0.1 Temugin
logstash-2015.12.13 3 r STARTED    680303 382.8mb 127.0.0.1 Paibo
logstash-2015.12.13 3 p STARTED    680303   383mb 127.0.0.1 Temugin
logstash-2015.12.13 1 r STARTED    679774 380.2mb 127.0.0.1 Paibo
logstash-2015.12.13 1 p STARTED    679774 378.4mb 127.0.0.1 Temugin
logstash-2015.12.13 4 p STARTED    680088 379.2mb 127.0.0.1 Paibo
logstash-2015.12.13 4 r STARTED    680088 381.1mb 127.0.0.1 Temugin
.kibana             0 r STARTED        82 224.9kb 127.0.0.1 Paibo
.kibana             0 p STARTED        82 198.9kb 127.0.0.1 Temugin
logstash-2015.11.30 4 r STARTED    521751 276.4mb 127.0.0.1 Paibo
logstash-2015.11.30 4 p STARTED    521751 276.4mb 127.0.0.1 Temugin
logstash-2015.11.30 0 r STARTED    521748 277.4mb 127.0.0.1 Paibo
logstash-2015.11.30 0 p STARTED    521748 277.4mb 127.0.0.1 Temugin
logstash-2015.11.30 3 r STARTED    522057 277.1mb 127.0.0.1 Paibo
logstash-2015.11.30 3 p STARTED    522057 277.1mb 127.0.0.1 Temugin
logstash-2015.11.30 1 r STARTED    521885   278mb 127.0.0.1 Paibo
logstash-2015.11.30 1 p STARTED    521885   278mb 127.0.0.1 Temugin
logstash-2015.11.30 2 r STARTED    521813 278.2mb 127.0.0.1 Paibo
logstash-2015.11.30 2 p STARTED    521813 278.2mb 127.0.0.1 Temugin
kibana-int          2 r STARTED         1  11.4kb 127.0.0.1 Paibo
kibana-int          2 p STARTED         1  11.4kb 127.0.0.1 Temugin
kibana-int          0 r STARTED         3  36.3kb 127.0.0.1 Paibo
kibana-int          0 p STARTED         3  36.3kb 127.0.0.1 Temugin
kibana-int          3 r STARTED         3    40kb 127.0.0.1 Paibo
kibana-int          3 p STARTED         3    40kb 127.0.0.1 Temugin
kibana-int          1 r STARTED         3  38.9kb 127.0.0.1 Paibo
kibana-int          1 p STARTED         3    39kb 127.0.0.1 Temugin
kibana-int          4 r STARTED         5  70.2kb 127.0.0.1 Paibo
kibana-int          4 p STARTED         5  70.3kb 127.0.0.1 Temugin
logstash-2015.12.02 2 r STARTED    541104 298.7mb 127.0.0.1 Paibo
logstash-2015.12.02 2 p STARTED    541104 298.7mb 127.0.0.1 Temugin
logstash-2015.12.02 0 r STARTED    540806   297mb 127.0.0.1 Paibo
logstash-2015.12.02 0 p STARTED    540806   297mb 127.0.0.1 Temugin
logstash-2015.12.02 3 r STARTED    541018 296.1mb 127.0.0.1 Paibo
logstash-2015.12.02 3 p STARTED    541018 296.1mb 127.0.0.1 Temugin
logstash-2015.12.02 1 r STARTED    541461 298.9mb 127.0.0.1 Paibo
logstash-2015.12.02 1 p STARTED    541461 298.9mb 127.0.0.1 Temugin
logstash-2015.12.02 4 r STARTED    541464 295.8mb 127.0.0.1 Paibo
logstash-2015.12.02 4 p STARTED    541464 295.8mb 127.0.0.1 Temugin
logstash-2015.12.01 4 r STARTED    534341 300.2mb 127.0.0.1 Paibo
logstash-2015.12.01 4 p STARTED    534341 300.2mb 127.0.0.1 Temugin
logstash-2015.12.01 0 r STARTED    534279 298.5mb 127.0.0.1 Paibo
logstash-2015.12.01 0 p STARTED    534279 298.5mb 127.0.0.1 Temugin
logstash-2015.12.01 3 r STARTED    534227 299.2mb 127.0.0.1 Paibo
logstash-2015.12.01 3 p STARTED    534227 299.2mb 127.0.0.1 Temugin
logstash-2015.12.01 1 r STARTED    533581 299.7mb 127.0.0.1 Paibo
logstash-2015.12.01 1 p STARTED    533581 299.7mb 127.0.0.1 Temugin
logstash-2015.12.01 2 r STARTED    533827 299.6mb 127.0.0.1 Paibo
logstash-2015.12.01 2 p STARTED    533827 299.6mb 127.0.0.1 Temugin

```

Now I can try and use reroute API but I want to understand why this is happening on my cluster. I have run the same config on test and other environments and never faced this.
</description><key id="122064500">15421</key><summary>Elasticsearch index in red state after open</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pkr1234</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2015-12-14T15:24:48Z</created><updated>2016-02-14T19:15:07Z</updated><resolved>2016-02-14T19:15:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pkr1234" created="2015-12-14T15:27:52Z" id="164466904">My transient cluster settings as of now.

```
 curl -XGET localhost:9200/_cluster/settings?pretty
{
  "persistent" : { },
  "transient" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "disable_allocation" : "false"
        }
      }
    }
  }
}

```

I have even tried to get rid of transient settings by doing a full cluster restart but the shards do not rebalance after open. 

Log on Data/Master 1:

```
[2015-12-14 15:01:35,008][INFO ][cluster.metadata         ] [Temugin] opening indices [[logstash-2015.10.16]]

```

Log on Data/Master2:

No recent logs.
</comment><comment author="clintongormley" created="2015-12-14T19:41:15Z" id="164537996">Hi @pkr1234 

You've restarted some nodes since you closed those indices, no?  I think you've lost the copies of those shards because closed indices aren't replicated.  See https://github.com/elastic/elasticsearch/issues/12963
</comment><comment author="pkr1234" created="2015-12-16T12:34:46Z" id="165093261">@clintongormley  I believe you are correct, Clinton. I did not even know. That's is quite a dangerous behaviour. Any plans to fix that? 

I close indexes all the time, add new instances and take em' out. This means I have to make sure the indexes are open before I terminate any instance.
</comment><comment author="clintongormley" created="2016-02-14T19:15:07Z" id="183957742">Closing in favour of #12963
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail and close translog hard if writing to disk fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15420</link><project id="" key="" /><description>Today we are super lenient (how could I missed that for f**k sake) with failing
/ closing the translog writer when we hit an exception. It's actually worse, we allow
to further write to it and don't care what has been already written to disk and what hasn't.
We keep the buffer in memory and try to write it again on the next operation.

When we hit a disk-full expcetion due to for instance a big merge we are likely adding document to the
translog but fail to write them to disk. Once the merge failed and freed up it's diskspace (note this is
a small window when concurrently indexing and failing the shard due to out of space exceptions) we will
allow in-flight operations to add to the translog and then once we fail the shard fsync it. These operations
are written to disk and fsynced which is fine but the previous buffer flush might have written some bytes
to disk which are not corrupting the translog. That wouldn't be an issue if we prevented the fsync.

Closes #15333
</description><key id="122032285">15420</key><summary>Fail and close translog hard if writing to disk fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>blocker</label><label>bug</label><label>critical</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-14T12:21:47Z</created><updated>2015-12-15T10:49:22Z</updated><resolved>2015-12-14T17:30:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-14T14:27:31Z" id="164450863">LGTM. Great catch.
</comment><comment author="s1monw" created="2015-12-14T15:10:16Z" id="164461673">@mikemccand @bleskes pushed a new commit
</comment><comment author="bleskes" created="2015-12-14T15:12:49Z" id="164462280">last change LGTM
</comment><comment author="mikemccand" created="2015-12-14T15:21:22Z" id="164464718">LGTM, thanks @s1monw!
</comment><comment author="s1monw" created="2015-12-14T17:30:18Z" id="164503038">I let CI run on this before backporting
</comment><comment author="s1monw" created="2015-12-14T18:26:57Z" id="164517911">pushed to 
- 2.x in https://github.com/elastic/elasticsearch/commit/b3e5518c4b8920e3223a9a6f714ce395b2c20ca2
- 2.1 in https://github.com/elastic/elasticsearch/commit/d51ecc9f0702552eeabe97a5faea65b9a46296c6
- 2.0 in https://github.com/elastic/elasticsearch/commit/aeee579a752ce50eb7ef8dd925dabd619e328dee
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replica stuck at initializing (CorruptedIndexException)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15419</link><project id="" key="" /><description>Hey,

We are using elasticsearch 1.4.2. We have a cluster with 4 nodes and two indices, each split into 5 shards and 1 replica per shard. After performing a necessary reboot on both servers, the cluster is stuck at yellow, because one replica in the first index is stuck at initializing (second index is fine). It looks as follows:

logs          2 p STARTED       9187625   1.7gb 127.0.1.1 node21_1 
logs           2 r STARTED       9187625   1.7gb 127.0.1.1 node22_2 
logs           0 r STARTED      56009065    11gb 127.0.1.1 node21_2 
logs           0 p STARTED      56009065    11gb 127.0.1.1 node21_1 
logs           3 p STARTED      56013686    11gb 127.0.1.1 node22_1 
logs           3 r INITIALIZING                                127.0.1.1 node21_2 
logs           1 r STARTED      56012034    11gb 127.0.1.1 node21_2 
logs           1 p STARTED      56012036    11gb 127.0.1.1 node21_1 
logs           4 p STARTED      56018956    11gb 127.0.1.1 node22_1 
logs           4 r STARTED      56018956    11gb 127.0.1.1 node22_2 

I checked the logs and I see the following exception keeps poping up:

[2015-12-14 12:34:18,689][DEBUG][action.admin.indices.stats] [node22_1] [logs][3], node[KfPAznKQRqu3QzTPRcQOTA], [R], s[INITIALIZING]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@644c3543]
org.elasticsearch.transport.RemoteTransportException: [node21_2][inet[/10.11.11.21:9301]][indices:monitor/stats[s]]
Caused by: org.elasticsearch.ElasticsearchException: io exception while building 'store stats'

I'd appreciate any tips on this. Thanks!
</description><key id="122028672">15419</key><summary>Replica stuck at initializing (CorruptedIndexException)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rok89</reporter><labels /><created>2015-12-14T11:56:59Z</created><updated>2015-12-14T19:22:46Z</updated><resolved>2015-12-14T19:22:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T19:22:46Z" id="164532455">Hiya @Rok89 

Whatever is causing that exception, it's coming from a stats request, not from recovery.  You need to look elsewhere in your logs.  I'd suggest taking this to the forums instead: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Sampler Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15418</link><project id="" key="" /><description /><key id="122028627">15418</key><summary>Aggregations Refactor: Refactor Sampler Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-14T11:56:41Z</created><updated>2015-12-21T09:32:44Z</updated><resolved>2015-12-21T09:32:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-14T11:57:36Z" id="164419429">Note that this PR includes splitting the sampler aggregation into "sampler" and "diversified_sampler" aggregations for the API to make validation and parsing easier
</comment><comment author="jpountz" created="2015-12-18T09:52:29Z" id="165733492">I actually like the split. I'm wondering that we might rename `sampler` to eg. `quality_sampler` to make it clearer how it works (as opposed to eg. a random sampler?).
</comment><comment author="colings86" created="2015-12-18T14:57:36Z" id="165797742">@jpountz I pushed a commit with your suggestions implemented. I'm not a fan of `quality_sampler` as the `diversified_sampler` can also be thought of as sampling for quality purposes. Personally I think `sampler` is fine as its the simple no frills version of sampling
/cc @markharwood 
</comment><comment author="jpountz" created="2015-12-18T15:56:09Z" id="165813825">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ATLEAST  query </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15417</link><project id="" key="" /><description>Customer will like a new type of query 

We can call it "ATLEAST" query it will look for all documents which contain at least N times a specific term 

like find all the document that Atleast contain the term Paris 5 times .
</description><key id="122024774">15417</key><summary>ATLEAST  query </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ESamir</reporter><labels><label>:Query DSL</label><label>feature</label></labels><created>2015-12-14T11:29:26Z</created><updated>2015-12-18T15:50:27Z</updated><resolved>2015-12-14T11:42:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T11:42:03Z" id="164416827">This can be done very easily today. 

```
PUT t/t/1
{
  "text":"Paris is Paris and so is Paris"
}
```

 The first method is a bit of a hack using the phrase query:

```
GET t/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "match_phrase": {
          "text": {
            "query": "Paris Paris Paris",
            "slop": 1000
          }
        }
      }
    }
  }
}
```

The second method using scripting will probably perform better:

```
GET t/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "script": {
          "script": {
            "inline": "_index[field][term].tf() &gt;= required",
            "params": {
              "field": "text",
              "term": "paris",
              "required": 3
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="ESamir" created="2015-12-18T12:28:53Z" id="165767448">@clintongormley 

When testing with phrase query   was faster then using the script 
- phrase query: run in around 1.5s with 78 947 documents matching
- the script  run in around 40s with 82 documents matching
</comment><comment author="jpountz" created="2015-12-18T15:50:27Z" id="165812442">Note that the script query example will examine all documents, including those that don't have the term. This can be made faster by intersecting it with a term query:

```
GET t/_search
{
  "query": {
    "bool": {
      "filter": [
        {
          "term": {
            "text": "paris"
          }
        },
        {
          "script": {
            "script": {
              "inline": "_index[field][term].tf() &gt;= required",
              "params": {
                "field": "text",
                "term": "paris",
                "required": 3
              }
            }
          }
        }
      ]
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cluster.routing.allocation.node_concurrent_recoveries doesn't work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15416</link><project id="" key="" /><description>GET _cluster/settings
{
  "transient": {
    "cluster": {
      "routing": {
        "allocation": {
          "node_concurrent_recoveries": "20"
        }
      }
    }
  }
}

but each time 
GET _recovery?active_only
{
  "2015-12-14": {
    "shards": [
      {
        "id": 14,
        "type": "REPLICA",
        "stage": "TRANSLOG",
        "primary": false,
      },
      {
        "id": 9,
        "type": "RELOCATION",
        "stage": "TRANSLOG",
        "primary": true,
      }
    ]
  }
}

only 2 shards are recoverying
</description><key id="122008210">15416</key><summary>cluster.routing.allocation.node_concurrent_recoveries doesn't work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-12-14T09:50:53Z</created><updated>2015-12-14T19:19:14Z</updated><resolved>2015-12-14T19:19:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2015-12-14T09:51:07Z" id="164393599">version: 2.1
</comment><comment author="clintongormley" created="2015-12-14T19:19:13Z" id="164531515">There are a number of different settings.  one which may be hitting you is `cluster.routing.allocation.cluster_concurrent_rebalance`.  See https://www.elastic.co/guide/en/elasticsearch/reference/current/shards-allocation.html for more
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Remove meta processor and expose templates in other processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15415</link><project id="" key="" /><description>- Added template infrastructure via FieldPathTemplate (adds template support for field like settings) and ValueSource (adds template support for value like settings.
- Adjusted the `set` and `remove` processor to be able to use templates via the new infrastructure. The `field` and `value` settings are now templateble.
- Removed the `meta` processor, since any processor can now access and modify meta fields.

PR for #14990
</description><key id="121997506">15415</key><summary>[Ingest] Remove meta processor and expose templates in other processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-14T08:40:00Z</created><updated>2015-12-18T16:39:58Z</updated><resolved>2015-12-18T16:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-14T09:19:58Z" id="164386215">why can't this plugin use mustache via the scriptengine service? Sorry: I don't think we should have secret embedded script engines inside other plugins. I have found this before and it seriously pisses me off, we can't do this.
</comment><comment author="martijnvg" created="2015-12-14T10:00:38Z" id="164395502">@rmuir That is the plan. The ingest framework has been directly using mustache, because it can't directly depend on the script service. It needs to be ES agnostic. In a follow up PR, I plan to add a `TemplateService` (or something like that) that the ingest framework will use directly. The implementation in the ingest plugin will then use `lang-mustache` module via the script service.
</comment><comment author="rmuir" created="2015-12-14T12:18:25Z" id="164423073">&gt; The ingest framework has been directly using mustache, because it can't directly depend on the script service. It needs to be ES agnostic

This makes no sense. 

&gt; In a follow up PR, I plan to add a TemplateService (or something like that) that the ingest framework will use directly.

This makes no sense.

Please, lets just use the scriptengineservice, and lookup "mustache", it is really just that simple.
</comment><comment author="uboness" created="2015-12-14T16:49:46Z" id="164491665">&gt; This makes no sense.

why? The processors themselves cannot depend on ES specific codebase. The reason for this is that we want to reuse the processors in LS. So in order to have processors use templates, we need to have some sort of a Template abstraction (I think that's what @martijnvg refers to with `TemplateService`) that is part of the ingest code (at least the code that will be shared with LS). 
</comment><comment author="rmuir" created="2015-12-14T16:51:52Z" id="164492264">This does not justify forking scripting engines, sorry. It needs to go thru the script engine service.
</comment><comment author="nik9000" created="2015-12-14T16:53:44Z" id="164492789">&gt; The processors themselves cannot depend on ES specific codebase.

I wonder if that is a good excuse to yank the some of the ES code base into a library you can share.
</comment><comment author="rmuir" created="2015-12-14T16:56:46Z" id="164493646">our script engine support is not something that needs to become a library, its out of the question. We do everything possible to contain their craziness.
</comment><comment author="martijnvg" created="2015-12-14T17:04:34Z" id="164495944">@rmuir @uboness I've updated the PR. Ingest no longer has a mustache compile time dependency and indirectly depends on the lang-mustache module. I've added a `TemplateService` interface to avoid that the ingest framework gets a compile dependency on ES. The ingest plugin provides an implementation that uses ES' script service.

&gt; I wonder if that is a good excuse to yank the some of the ES code base into a library you can share.

@nik9000 Indirectly using the script service in ingest avoids granting permissions in ingest that a template engine may need. Also it has already been granted in the lang-moustache module, so no need to grant twice.
</comment><comment author="martijnvg" created="2015-12-18T11:17:44Z" id="165751428">@javanna I've updated this PR.
</comment><comment author="javanna" created="2015-12-18T13:03:34Z" id="165772572">I left a few minor comments but it's really close now. I think that we will have to see whether the templating logic needs to be moved to IngestDocument, I think it will be at some point, but for now it's ok as-is. Also, we will need to make adjustments and better document how other runtimes can use ingest-core, given that we use ScriptService and each runtime has to provide its own template service, but again for now it's ok I think.
</comment><comment author="martijnvg" created="2015-12-18T15:16:59Z" id="165801738">@javanna I've updated the PR based on your comments.

&gt; I think that we will have to see whether the templating logic needs to be moved to IngestDocument

Agreed, it is trivial now the cut other processor settings over to use templates.

&gt; Also, we will need to make adjustments and better document how other runtimes can use ingest-core, given that we use ScriptService and each runtime has to provide its own template service, but again for now it's ok I think.

Yes, we are likely going to makes changes, once the ingest framework is going to be ran in other runtimes. The main achieved goal here is that the templating in ingest isn't tied to ES.
</comment><comment author="javanna" created="2015-12-18T16:08:01Z" id="165817901">LGTM go @martijnvg go
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Grouping - getting error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15414</link><project id="" key="" /><description>POST logstash-_\Win32-EventLog-__search
{
  "aggregations": {
    "group_by_type": {
      "terms": {
        "field": "EventType"
      }
    }
  }
}

Got the response as 
{
   "error": {
      "root_cause": [
         {
            "type": "exception",
            "reason": "java.lang.IllegalStateException: Field data loading is forbidden on EventType"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "logstash-2015.12.09",
            "node": "E1fNPBJVSi6mg09orkNsJg",
            "reason": {
               "type": "exception",
               "reason": "java.lang.IllegalStateException: Field data loading is forbidden on EventType",
               "caused_by": {
                  "type": "unchecked_execution_exception",
                  "reason": "java.lang.IllegalStateException: Field data loading is forbidden on EventType",
                  "caused_by": {
                     "type": "illegal_state_exception",
                     "reason": "Field data loading is forbidden on EventType"
                  }
               }
            }
         }
      ]

Sample available data
{
   "took": 63,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 70,
      "max_score": 1,
      "hits": [
         {
            "_index": "logstash-2015.12.09",
            "_type": "Win32-EventLog-Application",
            "_id": "AVGF_qw84BUvtVRjtnfx",
            "_score": 1,
            "_source": {
               "host": "Sriraman",
               "Logfile": "Application",
               "message": "CoId={4462F43E-1262-4AD1-99CD-9BC92C6D914A}: The user SRIRAMAN\Toshiba has started dialing a Dial-up connection using a all-user connection profile named airtel. The connection settings are: \nDial-in User = \nVpnStrategy =Not Applicable\nDataEncryption = Requested\nPrerequisiteEntry = \nAutoLogon = No\nUseRasCredentials = Yes\nAuthentication Type = CHAP \nIpv4DefaultGateway = Yes\nIpv4AddressAssignment = By Server\nIpv4DNSServerAssignment = By Server\nIpv6DefaultGateway = No\nIpv6AddressAssignment = By Server\nIpv6DNSServerAssignment = By Server\nIpDnsFlags = \nIpNBTEnabled = Yes\nUseFlags = Private Connection\nConnectOnWinlogon = No.",
               "Category": 0,
               "ComputerName": "Sriraman",
               "EventIdentifier": 20221,
               "EventType": "information",
               "RecordNumber": 31962,
               "SourceName": "RasClient",
               "TimeGenerated": "2015-12-09 14:37:14 +0530",
               "TimeWritten": "2015-12-09 14:37:14 +0530",
               "Type": "information",
               "User": null,
               "InsertionStrings": [
                  "{4462F43E-1262-4AD1-99CD-9BC92C6D914A}",
                  "SRIRAMAN\Toshiba",
                  "Dial-up",
                  "all-user",
                  "airtel",
                  "\nDial-in User = \nVpnStrategy =Not Applicable\nDataEncryption = Requested\nPrerequisiteEntry = \nAutoLogon = No\nUseRasCredentials = Yes\nAuthentication Type = CHAP \nIpv4DefaultGateway = Yes\nIpv4AddressAssignment = By Server\nIpv4DNSServerAssignment = By Server\nIpv6DefaultGateway = No\nIpv6AddressAssignment = By Server\nIpv6DNSServerAssignment = By Server\nIpDnsFlags = \nIpNBTEnabled = Yes\nUseFlags = Private Connection\nConnectOnWinlogon = No"
               ],
               "@version": "1",
               "@timestamp": "2015-12-09T09:07:14.991Z",
               "type": "Win32-EventLog-Application",
               "user_id": "100",
               "guid": "83e65e43-3ddd-454f-a9c1-c27be7ac36b9"
            }
         },

Elasticsearch Version - 2.1.0
Java Version jdk1.7.0_65
</description><key id="121973426">15414</key><summary>Grouping - getting error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SriramanK</reporter><labels /><created>2015-12-14T04:37:17Z</created><updated>2015-12-14T07:30:28Z</updated><resolved>2015-12-14T06:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-14T06:09:06Z" id="164352130">Please use discuss.elastic.co

I guess you are using logstash so probably you should use EventType.raw

Open a thread on discuss and also provide your mapping.
</comment><comment author="SriramanK" created="2015-12-14T07:09:00Z" id="164364714">I am using logstash. 
GET logstash-2015.12.09\Win32-EventLog-Application_mapping
Enclosed Mapping
   "logstash-2015.12.09": {
      "mappings": {
         "Win32-EventLog-Application": {
            "_all": {
               "enabled": true,
               "omit_norms": true
            },
            "dynamic_templates": [
               {
                  "message_field": {
                     "mapping": {
                        "index": "analyzed",
                        "omit_norms": true,
                        "fielddata": {
                           "format": "disabled"
                        },
                        "type": "string"
                     },
                     "match": "message",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "string_fields": {
                     "mapping": {
                        "index": "analyzed",
                        "omit_norms": true,
                        "fielddata": {
                           "format": "disabled"
                        },
                        "type": "string",
                        "fields": {
                           "raw": {
                              "index": "not_analyzed",
                              "ignore_above": 256,
                              "doc_values": true,
                              "type": "string"
                           }
                        }
                     },
                     "match": "_",
                     "match_mapping_type": "string"
                  }
               },
               {
                  "float_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "float"
                     },
                     "match": "_",
                     "match_mapping_type": "float"
                  }
               },
               {
                  "double_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "double"
                     },
                     "match": "_",
                     "match_mapping_type": "double"
                  }
               },
               {
                  "byte_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "byte"
                     },
                     "match": "_",
                     "match_mapping_type": "byte"
                  }
               },
               {
                  "short_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "short"
                     },
                     "match": "_",
                     "match_mapping_type": "short"
                  }
               },
               {
                  "integer_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "integer"
                     },
                     "match": "_",
                     "match_mapping_type": "integer"
                  }
               },
               {
                  "long_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "long"
                     },
                     "match": "_",
                     "match_mapping_type": "long"
                  }
               },
               {
                  "date_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "date"
                     },
                     "match": "_",
                     "match_mapping_type": "date"
                  }
               },
               {
                  "geo_point_fields": {
                     "mapping": {
                        "doc_values": true,
                        "type": "geo_point"
                     },
                     "match": "*",
                     "match_mapping_type": "geo_point"
                  }
               }
            ],
            "properties": {
               "@timestamp": {
                  "type": "date",
                  "format": "strict_date_optional_time||epoch_millis"
               },
               "@version": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "Category": {
                  "type": "long"
               },
               "ComputerName": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "EventIdentifier": {
                  "type": "long"
               },
               "EventType": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "InsertionStrings": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "Logfile": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "RecordNumber": {
                  "type": "long"
               },
               "SourceName": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "TimeGenerated": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "TimeWritten": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "Type": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "geoip": {
                  "dynamic": "true",
                  "properties": {
                     "city_name": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "continent_code": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "coordinates": {
                        "type": "double"
                     },
                     "country_code2": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "country_code3": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "country_name": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "ip": {
                        "type": "ip"
                     },
                     "latitude": {
                        "type": "float"
                     },
                     "location": {
                        "type": "geo_point"
                     },
                     "longitude": {
                        "type": "float"
                     },
                     "real_region_name": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "region_name": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "timezone": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fielddata": {
                           "format": "disabled"
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     }
                  }
               },
               "guid": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "host": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "hostip": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "message": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  }
               },
               "query": {
                  "properties": {
                     "term": {
                        "properties": {
                           "EventType": {
                              "type": "string",
                              "norms": {
                                 "enabled": false
                              },
                              "fielddata": {
                                 "format": "disabled"
                              },
                              "fields": {
                                 "raw": {
                                    "type": "string",
                                    "index": "not_analyzed",
                                    "ignore_above": 256
                                 }
                              }
                           }
                        }
                     }
                  }
               },
               "tags": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "type": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "user_id": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fielddata": {
                     "format": "disabled"
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               }
            }
         }
      }
   }
}

On using EventType.raw
POST logstash-_\Win32-EventLog-__search
{
  "aggregations": {
    "group_by_type": {
      "terms": {
        "field": "EventType.raw"
      }
    }
  }
}
Output is 
{
   "took": 81,
   "timed_out": false,
   "_shards": {
      "total": 10,
      "successful": 10,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   },
   "aggregations": {
      "group_by_type": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": []
      }
   }
}

But this is not expected
</comment><comment author="dadoonet" created="2015-12-14T07:10:25Z" id="164364862">fielddata is disabled on analyzed String fields. So your error message is expected.
Please follow up on discuss.
</comment><comment author="SriramanK" created="2015-12-14T07:30:28Z" id="164368462">Can you help me with any link to solve this issue. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>grandchildren aggregation broken in ES 2.0?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15413</link><project id="" key="" /><description>I am looking into this [SO question](http://stackoverflow.com/questions/34222753/elasticsearch-deep-children-aggregation-with-sum-metric-returning-empty-re), I am also facing something similar in my own project after I upgraded from 1.7 to 2.0. **children aggregation** gives weird results.

I created test index like this

```
POST companies
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "category": {
      "properties": {
        "id": {
          "type": "string"
        },
        "title": {
          "type": "string"
        }
      }
    },
    "subcategory": {
      "_parent": {
        "type": "category"
      },
      "properties": {
        "id": {
          "type": "string"
        },
        "title": {
          "type": "string"
        }
      }
    },
    "item": {
      "_parent": {
        "type": "subcategory"
      },
      "properties": {
        "id": {
          "type": "string"
        },
        "price": {
          "type": "integer"
        }
      }
    }
  }
}
```

I indexed some documents

```
PUT companies/category/cat1
{
  "id" : "cat1",
  "title" : "t1"
}

PUT companies/category/cat2
{
  "id" : "cat2",
  "title" : "t2"
}

PUT companies/category/cat3
{
  "id" : "cat3",
  "title" : "t3"
}

PUT companies/subcategory/sub1?parent=cat1
{
  "id" : "sub1",
  "title" : "st1"
}

PUT companies/subcategory/sub2?parent=cat1
{
  "id" : "sub2",
  "title" : "st2"
}

PUT companies/subcategory/sub3?parent=cat2
{
  "id" : "sub3",
  "title" : "st2"
}

PUT companies/subcategory/sub4?parent=cat2
{
  "id" : "sub4",
  "title" : "st4"
}

PUT companies/subcategory/sub5?parent=cat3
{
  "id" : "sub5",
  "title" : "st4"
}

PUT companies/subcategory/sub6?parent=cat3
{
  "id" : "sub6",
  "title" : "st6"
}

PUT companies/subcategory/sub7?parent=cat3
{
  "id" : "sub7",
  "title" : "st7"
}

PUT companies/item/i1?parent=sub1&amp;routing=cat1
{
  "id" : "i1",
  "price" : 100
}

PUT companies/item/i2?parent=sub1&amp;routing=cat1
{
  "id" : "i2",
  "price" : 200
}

PUT companies/item/i3?parent=sub2&amp;routing=cat1
{
  "id" : "i3",
  "price" : 200
}

PUT companies/item/i4?parent=sub2&amp;routing=cat1
{
  "id" : "i4",
  "price" : 150
}

PUT companies/item/i5?parent=sub3&amp;routing=cat2
{
  "id" : "i5",
  "price" : 150
}

PUT companies/item/i6?parent=sub3&amp;routing=cat2
{
  "id" : "i6",
  "price" : 180
}

PUT companies/item/i7?parent=sub4&amp;routing=cat2
{
  "id" : "i7",
  "price" : 180
}

PUT companies/item/i8?parent=sub5&amp;routing=cat3
{
  "id" : "i8",
  "price" : 1180
}

PUT companies/item/i9?parent=sub6&amp;routing=cat3
{
  "id" : "i9",
  "price" : 11180
}

PUT companies/item/i10?parent=sub7&amp;routing=cat3
{
  "id" : "i10",
  "price" : 180
}

PUT companies/item/i11?parent=sub7&amp;routing=cat3
{
  "id" : "i11",
  "price" : 150
}

PUT companies/item/i12?parent=sub7&amp;routing=cat3
{
  "id" : "i12",
  "price" : 210
}

```

This is the aggregation query

```
GET companies/_search
{
  "query": {
    "has_child": {
      "type": "subcategory",
      "query": {
        "has_child": {
          "type": "item",
          "query": {
            "range": {
              "price": {
                "gte": 100,
                "lte": 250
              }
            }
          }
        }
      }
    }
  },
  "aggs": {
    "categories": {
      "terms": {
        "field": "id"
      },
      "aggs": {
        "sub_categories": {
          "children": {
            "type": "subcategory"
          },
          "aggs": {
            "sub_category_ids": {
              "terms": {
                "field": "id"
              },
              "aggs": {
                "items": {
                  "children": {
                    "type": "item"
                  },
                  "aggs": {
                    "price": {
                      "sum": {
                        "field": "price"
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}
```

Results for ES 1.7

```
{
   "took": 189,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 3,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "categories": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "cat1",
               "doc_count": 1,
               "sub_categories": {
                  "doc_count": 2,
                  "sub_category_ids": {
                     "doc_count_error_upper_bound": 0,
                     "sum_other_doc_count": 0,
                     "buckets": [
                        {
                           "key": "sub1",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 2,
                              "price": {
                                 "value": 300
                              }
                           }
                        },
                        {
                           "key": "sub2",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 2,
                              "price": {
                                 "value": 350
                              }
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "cat2",
               "doc_count": 1,
               "sub_categories": {
                  "doc_count": 2,
                  "sub_category_ids": {
                     "doc_count_error_upper_bound": 0,
                     "sum_other_doc_count": 0,
                     "buckets": [
                        {
                           "key": "sub3",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 2,
                              "price": {
                                 "value": 330
                              }
                           }
                        },
                        {
                           "key": "sub4",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 1,
                              "price": {
                                 "value": 180
                              }
                           }
                        }
                     ]
                  }
               }
            },
            {
               "key": "cat3",
               "doc_count": 1,
               "sub_categories": {
                  "doc_count": 3,
                  "sub_category_ids": {
                     "doc_count_error_upper_bound": 0,
                     "sum_other_doc_count": 0,
                     "buckets": [
                        {
                           "key": "sub5",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 1,
                              "price": {
                                 "value": 1180
                              }
                           }
                        },
                        {
                           "key": "sub6",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 1,
                              "price": {
                                 "value": 11180
                              }
                           }
                        },
                        {
                           "key": "sub7",
                           "doc_count": 1,
                           "items": {
                              "doc_count": 3,
                              "price": {
                                 "value": 540
                              }
                           }
                        }
                     ]
                  }
               }
            }
         ]
      }
   }
}
```

Result for ES 2.0

```
{
  "took": 24,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "hits": {
    "total": 3,
    "max_score": 1,
    "hits": [
      {
        "_index": "companies",
        "_type": "category",
        "_id": "cat1",
        "_score": 1,
        "_source": {
          "id": "cat1",
          "title": "t1"
        }
      },
      {
        "_index": "companies",
        "_type": "category",
        "_id": "cat2",
        "_score": 1,
        "_source": {
          "id": "cat2",
          "title": "t2"
        }
      },
      {
        "_index": "companies",
        "_type": "category",
        "_id": "cat3",
        "_score": 1,
        "_source": {
          "id": "cat3",
          "title": "t3"
        }
      }
    ]
  },
  "aggregations": {
    "categories": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "cat1",
          "doc_count": 1,
          "sub_categories": {
            "doc_count": 2,
            "sub_category_ids": {
              "doc_count_error_upper_bound": 0,
              "sum_other_doc_count": 0,
              "buckets": [
                {
                  "key": "sub1",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 0,
                    "price": {
                      "value": 0
                    }
                  }
                },
                {
                  "key": "sub2",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 0,
                    "price": {
                      "value": 0
                    }
                  }
                }
              ]
            }
          }
        },
        {
          "key": "cat2",
          "doc_count": 1,
          "sub_categories": {
            "doc_count": 2,
            "sub_category_ids": {
              "doc_count_error_upper_bound": 0,
              "sum_other_doc_count": 0,
              "buckets": [
                {
                  "key": "sub3",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 0,
                    "price": {
                      "value": 0
                    }
                  }
                },
                {
                  "key": "sub4",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 0,
                    "price": {
                      "value": 0
                    }
                  }
                }
              ]
            }
          }
        },
        {
          "key": "cat3",
          "doc_count": 1,
          "sub_categories": {
            "doc_count": 3,
            "sub_category_ids": {
              "doc_count_error_upper_bound": 0,
              "sum_other_doc_count": 0,
              "buckets": [
                {
                  "key": "sub5",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 1,
                    "price": {
                      "value": 1180
                    }
                  }
                },
                {
                  "key": "sub6",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 0,
                    "price": {
                      "value": 0
                    }
                  }
                },
                {
                  "key": "sub7",
                  "doc_count": 1,
                  "items": {
                    "doc_count": 0,
                    "price": {
                      "value": 0
                    }
                  }
                }
              ]
            }
          }
        }
      ]
    }
  }
}
```

Did I miss something?
</description><key id="121961262">15413</key><summary>grandchildren aggregation broken in ES 2.0?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">chintan-shah-25</reporter><labels><label>:Aggregations</label><label>:Parent/Child</label><label>bug</label></labels><created>2015-12-14T02:06:24Z</created><updated>2015-12-17T14:27:52Z</updated><resolved>2015-12-16T09:30:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T19:10:03Z" id="164529092">Hmm I've tried this on 2.0.0 and 2.1.0, and with different numbers of shards.  I'm unable to replicate this.  Could you confirm that this recreation is correct?
</comment><comment author="clintongormley" created="2015-12-14T20:33:08Z" id="164550953">Related to https://github.com/elastic/elasticsearch/issues/15005#issuecomment-163090273 ?
</comment><comment author="chintan-shah-25" created="2015-12-15T03:06:12Z" id="164630229">I recreated index after deleting  and It is not working with ES 2.0, This is another [SO question](http://stackoverflow.com/questions/34072977/children-aggregation-broken-in-elasticsearch-2-1) and I also tried that too on both ES 1.7 and 2.0 and it is not working on 2.0.
</comment><comment author="clintongormley" created="2015-12-15T13:53:13Z" id="164771218">@chintan-shah-25 by "not working" do you mean that the recreation isn't working? In other words, Elasticsearch is working correctly?
</comment><comment author="chintan-shah-25" created="2015-12-15T14:09:07Z" id="164775308">I meant query is not working, it is still giving wrong results in 2.0, also could you please try the other question in the link, it is also giving wrong results in 2.0 but working perfectly in 1.7
</comment><comment author="clintongormley" created="2015-12-15T14:27:40Z" id="164780175">@chintan-shah-25 For some reason, your recreation works fine for me, but the one you linked to on SO shows the error on 2.0 and 2.1:

```
PUT /test_index
{
  "mappings": {
    "parent_doc": {
      "properties": {
        "name": {
          "type": "string",
          "index": "not_analyzed"
        },
        "town": {
          "type": "string",
          "index": "not_analyzed"
        }
      }
    },
    "child_doc": {
      "_parent": {
        "type": "parent_doc"
      },
      "properties": {
        "name": {
          "type": "string",
          "index": "not_analyzed"
        },
        "age": {
          "type": "integer"
        }
      }
    }
  }
}

POST /test_index/parent_doc/_bulk
{"index":{"_id":1}}
{"name":"Bob","town":"Memphis"}
{"index":{"_id":2}}
{"name":"Alice","town":"Chicago"}
{"index":{"_id":3}}
{"name":"Bill","town":"Chicago"}

POST /test_index/child_doc/_bulk
{"index":{"_id":3,"_parent":1}}
{"name":"Jill","age":5}
{"index":{"_id":4,"_parent":1}}
{"name":"Joey","age":3}
{"index":{"_id":5,"_parent":2}}
{"name":"John","age":2}
{"index":{"_id":6,"_parent":3}}
{"name":"Betty","age":6}
{"index":{"_id":7,"_parent":3}}
{"name":"Dan","age":1}


POST /test_index/parent_doc/_search
{
  "size": 0,
  "aggs": {
    "towns": {
      "terms": {
        "field": "town"
      },
      "aggs": {
        "parent_names": {
          "terms": {
            "field": "name"
          },
          "aggs": {
            "child_docs": {
              "children": {
                "type": "child_doc"
              }
            }
          }
        }
      }
    }
  }
}
```

Returns:

```
  "aggregations": {
    "towns": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "Chicago",
          "doc_count": 2,
          "parent_names": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
              {
                "key": "Alice",
                "doc_count": 1,
                "child_docs": {
                  "doc_count": 0
                }
              },
              {
                "key": "Bill",
                "doc_count": 1,
                "child_docs": {
                  "doc_count": 0
                }
              }
            ]
          }
        },
        {
          "key": "Memphis",
          "doc_count": 1,
          "parent_names": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
              {
                "key": "Bob",
                "doc_count": 1,
                "child_docs": {
                  "doc_count": 0
                }
              }
            ]
          }
        }
      ]
    }
```
</comment><comment author="chintan-shah-25" created="2015-12-15T14:31:29Z" id="164781129">Is it throwing error or giving wrong results?
</comment><comment author="clintongormley" created="2015-12-15T14:33:47Z" id="164781671">giving wrong results
</comment><comment author="martijnvg" created="2015-12-15T20:28:22Z" id="164887301">@chintan-shah-25 @clintongormley The children agg makes an assumption (that all segments are being seen by children agg) that was true in 1.x but not in 2.x. I opened #15457 which fixes that.
</comment><comment author="martijnvg" created="2015-12-16T09:30:51Z" id="165045796">This bug has been fixed via #15457 and back ported to the 2.x, 2.1 and 2.0 branches.
</comment><comment author="chintan-shah-25" created="2015-12-17T14:27:51Z" id="165465168">Thanks a lot @martijnvg @clintongormley :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide per-node query latency percentiles, or the ability for a plugin to collect them.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15412</link><project id="" key="" /><description>From the existing stats i can derive per node average query latency, but the average isn't quite as interesting. Recording the 'took' time from queries I find that our percentiles come out to, roughly: 50th: 25ms, 75th: 70ms, 95th: 180ms, 99th: 8000ms. We run a non-homogeneous cluster so i'm really interested what the impact of some of the less performant hardware is on these percentiles. It would be quite interesting to learn that 99th percentile for one set of hardware is 3s, while the older hardware is up near 8s (completely guessing).

I've looked over the existing code base and it does not look like a plugin can currently collect these things, I would probably need a way to access the timing information that currently flows through org.elasticsearch.index.search.stats.ShardSearchStats.

Collecting percentiles within ES proper, providing a configuration mechanism for a plugin to replace the ShardSearchStats, or some kind of listener on ShardSearchStats would all be acceptable solutions to the problem.
</description><key id="121956608">15412</key><summary>Provide per-node query latency percentiles, or the ability for a plugin to collect them.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ebernhardson</reporter><labels><label>:Stats</label><label>discuss</label></labels><created>2015-12-14T00:53:19Z</created><updated>2016-06-24T11:02:32Z</updated><resolved>2016-06-24T11:02:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-16T11:42:40Z" id="165079728">@polyfractal any thoughts?
</comment><comment author="polyfractal" created="2015-12-16T15:19:10Z" id="165138457">I don't know enough about the plugin architecture to comment on how easy/hard it'd be to expose this stuff for pluggability.  But I do like the idea of natively recording percentiles.

Now that we have an implementation of HDRHistogram, that'd probably be the sketch to use here.  Relatively small memory footprint, large dynamic range.  Some open questions/problems though:
- Like all other ES stats, the percentiles would be cumulative and probably lose value over time as they saturate (e.g. thousands of fast queries will, over time, move the 90th percentile up and mask real problems)
- Memory usage: iirc a 2 sig-fig HDRHistogram uses around 30kb of memory in the worst case.  So 30kb \* a dozen stats is only ~360kb.  But we'd want to put a maximum dynamic range so it doesn't grow unbounded (but I don't think that would be a problem).  3 sig-figs is a lot heavier, ~200kb per sketch
- If we wanted smaller memory footprint, we could use much less accurate Frugal Streams (&lt;1kb)
- HDRHistogram is not thread-safe, and I think multiple threads can update these stats?  Might introduce some non-negligible contention locking the histograms, or have to maintain a histogram-per-thread and increase memory usage.
- It would add a small constant time latency to requests, since any percentile recording/sketching will necessarily be slower than just bumping a cumulative time counter.  May be negligible though?
- Practically speaking, adding percentiles will make the Stats output a lot more verbose

All said, it _would_ be very useful to see distributions in the stats.  All stats that expose a cumulative `time_in_millis` would benefit.

/cc @colings86 who added HDRHistogram to the aggs...any gotchas that might trip this up?
</comment><comment author="ebernhardson" created="2015-12-17T00:19:02Z" id="165297528">- I'm uncertain that cumulative percentiles will be as useful as something that provides recency. Most of the servers in my cluster have been up since mid September. Any change to query patterns made now will not show up in percentiles that take into account several months and several hundred million queries.
- The search stats at least are updated by multiple threads.
- If lock contention is an issue sampling of the query stats for use in the histogram could be viable. Getting the sampling level right could be difficult though. I might err on the side of histogram-per-thread, but i'm not sure how many threads are involved so I can't estimate the impact of that.

I understand that providing recency is a much more difficult problem with stream data, but in my opinion it is paramount to these percentiles being useful. I'm not sure how it would be implemented, but some sort of decay may be sufficient to provide reasonable recency.
</comment><comment author="polyfractal" created="2015-12-17T16:52:39Z" id="165507877">Yep, I agree that recency is important.  The main problem is that compact/fast streaming sketches tend to be unable to deal with recency, since the relevant time information is lost as soon as the data is added to the sketch.

We could use something like windowing or resevoir sampling, which will naturally drift with the data over time.  Downside is that they require saving some amount of the data points and are therefore a fair amount heavier memory footprint.

Some sketches naturally age data, like frugal streams.  But they aren't super accurate (in part because of their ability to drift over time).

Or we could implement something like A&lt;sup&gt;2&lt;/sup&gt; Bloom Filters, where you have two (or more) active copies of the sketch and rotate between them.  I haven't seen any literature for this, but I think you could actually rig something up with a triplet-ring buffer:

In pseudocode:

``` rust
histos[] = new HdrHisto[3];
current = 0;

fn addData(data) {
  // Add data point to current and current + 1
  histos[current % 2].add(data);
  histos[(current + 1) % 2].add(data);

  // After a predfined interval, rotate the histos
  if (elapsed_time &gt; interval) {
    current += 1;
    histos[(current - 1) % 2].clear();
  }
}

// only return percentile estimates from the current sketch
fn getPercentiles() {
  return histos[current].percentiles();
}
```

Basically, by rotating the sketch, you guarantee that `current` holds `2 * interval` worth of data.  The triplet is preferred over the A&lt;sup&gt;2&lt;/sup&gt; setup because it avoids a "cold" sketch when it rolls over.

Alternatively, the truly simplest way to solve this is an ability to reset all the stats, so you could just manually reset at the start of testing.
</comment><comment author="jasontedor" created="2016-06-24T09:54:27Z" id="228305796">&gt; Alternatively, the truly simplest way to solve this is an ability to reset all the stats, so you could just manually reset at the start of testing.

We've already decided in #9693 that this will not be added.
</comment><comment author="jasontedor" created="2016-06-24T11:02:32Z" id="228317709">Closed by #17398
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot create stubs of CreateIndexResponse and PutMappingResponse for testing with dummy data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15411</link><project id="" key="" /><description>Since the constructors for CreateIndexResponse.java and PutMappingResponse.java have no access modifiers defined, they are "package-only" and cannot be extended or instantiated outside the package.

The files:

https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/action/admin/indices/create/CreateIndexResponse.java

https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/PutMappingResponse.java

That means that in code like this:

```
final ActionFuture&lt;CreateIndexResponse&gt; creatureFuture = client.admin().indices().create(new CreateIndexRequest("index_name"));

boolean acknowledged = createFuture.actionGet().isAcknowledged();
```

The actionGet() method cannot be overriden, because you can't extend or override CreateIndexResponse.

You could create a class that extends `ActionFuture&lt;CreateIndexResponse&gt;`, but the moment you do

```
@Override
public CreateIndexResponse actionGet() throws ElasticsearchException {
    return new CreateIndexResponse() {
        // override code
    };
}
```

You'll get exceptions because the class cannot be instantiated outside the package.

Meaning you can't write a stub for unit testing with dummy data, without actually running ElasticSearch.

Is there a way to overcome this? Or is this by design?

I'm working with ES 1.7.3.
</description><key id="121941308">15411</key><summary>Cannot create stubs of CreateIndexResponse and PutMappingResponse for testing with dummy data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KoenDG</reporter><labels><label>:Java API</label></labels><created>2015-12-13T20:21:45Z</created><updated>2017-05-05T17:04:07Z</updated><resolved>2017-05-05T14:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-05-05T14:29:10Z" id="299480006">hi @KoenDG sorry it has taken ages to get back to you. This is kind of by design, we usually increase visibility of classes and methods only when we really need to. These two classes can be subclassed now though and they have protected constructors, so this specific issue is solved.</comment><comment author="KoenDG" created="2017-05-05T16:52:17Z" id="299517496">Oh man, I had forgotten all about this. I got around this issue by using an in-memory cluster and setting that as the Client in dev and test mode of my program at the time.

I should have closed this ticket when I had a working solution, my bad.</comment><comment author="javanna" created="2017-05-05T17:04:07Z" id="299520378">no worries @KoenDG !</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Exact Match option to Suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15410</link><project id="" key="" /><description>Solves #11579.

Added option to term suggest that returns the input term if it exists. Turned off by default so it doesn't break normal behaviour.

The example query given in #11579 would now be:

```
"Suggest": {
      "term": {
        "field": "word",
        "suggest_mode": "popular",
        "size": 2,
        "prefix_len": 1,
        "analyzer": "default",
        "exact_matching": true
      },
      "text": "Software"
    }
```

And the result:

```
"Suggest": [
      {
         "text": "software",
         "offset": 0,
         "length": 8,
         "options": [
            {
               "text": "software",
               "score": 1,
               "freq": 1
            }
         ]
      }
   ]
```

The output now clearly indicates if the term is present. If it is, its score is always 1, representing an exact match, which differentiates this result from other suggestions.
</description><key id="121940634">15410</key><summary>Add Exact Match option to Suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ricardocerq</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>feedback_needed</label><label>review</label></labels><created>2015-12-13T20:08:58Z</created><updated>2017-02-15T08:47:23Z</updated><resolved>2017-02-15T08:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T18:35:47Z" id="164520167">@areek could you have a look at this please.
@ricardocerq you'll also need a test in the PR before it will be merged.
</comment><comment author="areek" created="2015-12-14T20:38:38Z" id="164552213">Thanks @ricardocerq for the PR :). I left some comments. Would be awesome if we could have a test for the new setting too. You could have a look at `SuggestSearchTests.java` for some inspiration.
</comment><comment author="ricardocerq" created="2015-12-15T12:58:42Z" id="164756346">Thanks for the attention and the feedback! I'm making the necessary changes. :)
</comment><comment author="clintongormley" created="2016-03-10T13:37:46Z" id="194841090">@areek could you review this again please?
</comment><comment author="dakrone" created="2016-04-06T17:35:55Z" id="206481159">pinging @areek again for review :)
</comment><comment author="areek" created="2016-04-07T19:55:33Z" id="207065848">@ricardocerq sorry for the delay, this LGTM, would you mind updating the PR with master?
</comment><comment author="dakrone" created="2016-09-12T21:22:30Z" id="246498201">@ricardocerq are you able to update this by rebasing or merging master in so it can be merged?
</comment><comment author="ricardocerq" created="2016-10-02T23:00:10Z" id="251002228">Sorry about the delay :(. I was unable to retest the changes. If more changes are necessary, please let me know.
</comment><comment author="worldsayshi" created="2016-10-13T09:58:49Z" id="253469643">@areek @clintongormley Can this be merged?
</comment><comment author="clintongormley" created="2016-11-06T10:24:42Z" id="258672029">@ricardocerq are you still interested in completing this PR?
</comment><comment author="clintongormley" created="2017-02-15T08:47:22Z" id="279950928">Closing in favour of #22902</comment></comments><attachments /><subtasks /><customfields /></item><item><title>docker &gt;= 1.8.3 + marvel plugin renders a host machine network dead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15409</link><project id="" key="" /><description>We've isolated our tests to indicate that when running ES 2.1.0, on a docker 1.8.3+ utilizing kubernetes 1.1.1 a weird networking problem occurs when installing the marvel plugin.  

The problem renders most networking activity dead on the host machine and subsequently on all containers running on that machine, but mostly and 100% it renders the machine unable to resolve any DNS names.

Is marvel dead locking each node internally with a flood of DNS requests to itself?
</description><key id="121936124">15409</key><summary>docker &gt;= 1.8.3 + marvel plugin renders a host machine network dead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielschonfeld</reporter><labels><label>:Network</label><label>discuss</label></labels><created>2015-12-13T18:45:14Z</created><updated>2016-02-14T19:13:32Z</updated><resolved>2016-02-14T19:13:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielschonfeld" created="2015-12-14T15:43:21Z" id="164471132">Update: this issue is not related to kubernetes in any way.  Also not specific to my company image, this problem occurs with the docker hub elasticsearch:2.1.0 image + CoreOS &gt;= 835.9.0.

Steps to reproduce:
1) Need at least 2 machines of CoreOS, 1 should be release &gt;= 835.9.0 and the other a release before that. IE `CoreOS 766.5.0`.

Notes: 
- You will need an ES cluster &gt; 1 node.  2 Nodes is fine as long as they join into a cluster together.
- Machine number 1 will be the one with CoreOS &gt;= 835.9.0
- Machine number 2 will be the one with CoreOS = 766.5.0

2) We happen to use flanneld so for the sake of the experiment please utilize flannel
3) On machine number 1 start ES with the following: `docker run --name es -i -t elasticsearch:2.1.0 /bin/bash`
4) Do the same on machine number 2.
5) On both machines, inside the shell install the license and marvel-agent plugins:

```
/usr/share/elasticsearch/bin/plugin install license
/usr/share/elasticsearch/bin/plugin install marvel-agent
```

6) On machine number 2 edit the `elasticsearch.yaml` and add the discovery line with the IP of machine number 1, i.e.: `discovery.zen.ping.unicast.hosts: [10.244.61.148]`

7) Start ES on machine number 1 `/docker-entry.sh elasticsearch`
8) Do the same on machine number 2
9) Allow both machines to agree they've connected and joined into a cluster

10) Go back to machine number 1 and try to `ping google.com`... You will see the name can't even be resolved. And generally the whole network on the host machine is trashed and so is ES.
</comment><comment author="clintongormley" created="2015-12-14T19:50:06Z" id="164540287">I doubt Marvel is making lots of DNS request (@tlrx could you confirm?)

What about looking at what network sockets are in use?  Also, it seems very weird that this requires two different versions of CoreOs... sounds like it might be an issue with CoreOs then, no?
</comment><comment author="danielschonfeld" created="2015-12-14T20:08:58Z" id="164544996">@clintongormley it requires two versions because in my reproduction steps, I want you to be left with one machine that actually works while the other is trashed.

using an older version archives just that...
</comment><comment author="danielschonfeld" created="2015-12-14T20:09:51Z" id="164545208">@clintongormley also see the linked docker bug... this might not be directly the fault of ES but rather my steps just find a proven way to reproduce a docker bug.  So it might not be ES's fault per say.
</comment><comment author="clintongormley" created="2016-02-14T19:13:31Z" id="183957653">This sounds like a docker problem rather than an ES problem, so I'm going to close this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>discovery-ec2 plugin unavailable?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15408</link><project id="" key="" /><description>```
bin/plugin install --verbose cloud-aws
-&gt; Installing cloud-aws...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/cloud-aws/2.1.0/cloud-aws-2.1.0.zip ...
Downloading ............................................................................................................................................................................................................................................................................................................................................DONE

bin/plugin install --verbose discovery-ec2
-&gt; Installing discovery-ec2...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/discovery-ec2/2.1.0/discovery-ec2-2.1.0.zip ...
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/discovery-ec2/2.1.0/discovery-ec2-2.1.0.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/discovery-ec2/2.1.0/discovery-ec2-2.1.0.zip];
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```
</description><key id="121906311">15408</key><summary>discovery-ec2 plugin unavailable?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devoncrouse</reporter><labels /><created>2015-12-13T08:05:42Z</created><updated>2015-12-13T08:25:57Z</updated><resolved>2015-12-13T08:20:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="devoncrouse" created="2015-12-13T08:20:55Z" id="164237310">https://discuss.elastic.co/t/unable-to-install-discovery-ec2-plugin/33234/2
</comment><comment author="xuzha" created="2015-12-13T08:25:57Z" id="164237480">See readme [here](https://github.com/elastic/elasticsearch-cloud-aws)

The `cloud-aws` is what you need. The code split is in master branch, you cannot install discovery-ec2 and repository-s3 plugins by using the `bin/plugin`. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possible regression in highlighting - highlight_query ignored if is different than highlight field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15407</link><project id="" key="" /><description>I'm indexing simple document with three fields:
`text_content_0: "Skrzyd&#322;a samolotu"`
`text_content_1: "Wyposa&#380;enie samolotu"`
`text_content: "Skrzyd&#322;a samolotu. Wyposa&#380;enie samolotu".``

text_content\* fields have `"index_options": "offsets"`.

Then i'm querying:

```
{
  "highlight": {
    "fields": {
      "text_content_*": {
        "type": "postings",
        "highlight_query": {
          "match": {
            "text_content": "Skrzyd&#322;a"
          }
        }
      }
    }
  }, 
  "query": {
   "filtered": {
      "filter": {
        "ids": {
          "values": [1]
        }
      }
    }
  }
}
```

In ES &lt; 2.0 (1.7.3) i'm getting proper results: one document is returned, field _text_content_0 is highlighted with word "skrzyd&#322;o". In ES &gt; 2.0 (i've tried 2.0.1, 2.1.0) highlight is not returned. I've found  #10627 but adding `require_field_match: false` does not working. Changing `highlight_query.match` to 

```
"text_content_0": "Skrzyd&#322;a"
```

or
to muliti_match:

```
"fields": ["text_content_*"],
 "query": "Skrzyd&#322;a"
```

works fine - highlights are returned.
</description><key id="121893009">15407</key><summary>Possible regression in highlighting - highlight_query ignored if is different than highlight field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sicarrots</reporter><labels /><created>2015-12-13T02:12:27Z</created><updated>2015-12-14T13:51:53Z</updated><resolved>2015-12-14T13:51:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T13:51:53Z" id="164443193">Correct: Require-field-match is no longer supported by the postings highlighter: https://github.com/elastic/elasticsearch/pull/11077
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Systemd - Template, This process can't be executed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15406</link><project id="" key="" /><description>I, 

I haved a work to do in my class with systemd templates. 

I need to make a service that will erase log fill in a log directory at boot time. For exemple /var/log/squid/.

I need to erase logs of 3 differents directory. And I need one systemd file.

i have create my "rmlog@.service" file in "/etc/systemd/system". 

my "rmlog@.service" look like that : 

```
            [Unit]
            Description=Suppressions des fichiers contenus dans /var/log/X

            [Service]
            Type=oneshot
            ExecStart=/etc/TP/%i.sh

            [Install]
            WantedBy=default.target
```

My logs says "this process /etc/TP/%i.sh can't be executed"

Anyone can help me ? 

Thanks
</description><key id="121872838">15406</key><summary>Systemd - Template, This process can't be executed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clems09</reporter><labels /><created>2015-12-12T19:18:50Z</created><updated>2015-12-12T19:50:25Z</updated><resolved>2015-12-12T19:50:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-12T19:50:25Z" id="164182666">Closing as this does not seem to be specific to Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More detailed explanation of some similarity types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15405</link><project id="" key="" /><description>While configuring my Elasticsearch instance, I faced the problem of picking most suitable similarity algorithm. The explanation of similarity types is very poor, it seems a good idea to provide some small description of how each of this algorithms works.
</description><key id="121865974">15405</key><summary>More detailed explanation of some similarity types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexg-dev</reporter><labels><label>docs</label></labels><created>2015-12-12T16:53:59Z</created><updated>2015-12-14T13:27:47Z</updated><resolved>2015-12-14T13:27:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexg-dev" created="2015-12-12T17:01:16Z" id="164168193">signed
</comment><comment author="clintongormley" created="2015-12-14T13:27:47Z" id="164436166">thanks @alexg-dev - merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.x does not appear to fully support JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15404</link><project id="" key="" /><description>Throwing exceptions on JSON field names that contain characters ( such as "." ) that are valid in JSON means that by definition Elasticsearch no longer fully supports JSON.  I am finding this especially problematic.  A large number of programming languages use the period to denote calling a method / function on an object / struct.  In cases where logging is being used to pinpoint issues in code execution, this results in confusion when having to substitute these characters so that elasticsearch will accept them.

Consider the usefulness of a case such as:

``` json
{ "type":"net.Conn", "function":"Dial", "error":"could not connect"}
```

The technical solution for an end user is of course simple - replace the characters that Elasticsearch no longer supports with another character. However, the side effect of this is that the logs I have to change in order to accomodate Elasticsearch 2.x no longer supporting valid characters become more distanced from what I am trying to communicate with them.

Am I missing something about this change?  I'm hoping that I am - but in my tests it does seem as simple as "Elasticsearch no longer supports periods in field names, period" - pardon the pun! ;)
</description><key id="121863781">15404</key><summary>Elasticsearch 2.x does not appear to fully support JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">taotetek</reporter><labels /><created>2015-12-12T16:15:38Z</created><updated>2016-01-26T21:24:39Z</updated><resolved>2015-12-13T13:34:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-12T21:31:02Z" id="164195271">&gt; Am I missing something about this change?

The change is not made arbitrarily or lightly, the problem is actually to
_support_ various JSON nesting methods, for example:

For example, given two simple JSON documents:

Document A:

``` json
{
  "foo": {
    "bar.baz": 5
  }
}
```

Document B:

``` json
{
  "foo": {
    "bar": {
      "baz": 7
    }
  }
}
```

If these were in the same document, what does `foo.bar.baz` refer to? 5 or 7?
While useful to be able to have fields with periods in them, it leads to even
more confusion than not allowing them.

It's not just Elasticsearch that "doesn't like" ambiguity like this, take for
example the [jq](https://stedolan.github.io/jq/) tool:

```
&#187; echo '{"foo": {"bar.baz": 5}}' | jq .foo
{
  "bar.baz": 5
}
&#187; echo '{"foo": {"bar.baz": 5}}' | jq .foo.bar
null
&#187; echo '{"foo": {"bar.baz": 5}}' | jq .foo.bar.baz
null
```

Or accessing JSON members as objects in Javascript:

```
&#187; node
&gt; s = JSON.parse('{"foo": {"bar.baz": 5}}')
{ foo: { 'bar.baz': 5 } }
&gt; s.foo
{ 'bar.baz': 5 }
&gt; s.foo.bar.baz
TypeError: Cannot read property 'baz' of undefined
    at repl:1:11
    at REPLServer.self.eval (repl.js:110:21)
    at Interface.&lt;anonymous&gt; (repl.js:239:12)
    at Interface.emit (events.js:95:17)
    at Interface._onLine (readline.js:203:10)
    at Interface._line (readline.js:532:8)
    at Interface._ttyWrite (readline.js:761:14)
    at ReadStream.onkeypress (readline.js:100:10)
    at ReadStream.emit (events.js:98:17)
    at emitKey (readline.js:1096:12)
&gt; s.foo."bar.baz"
...
... (node expects more input instead of resolving "bar.baz" as a key)
```

Would you say neither of those tools support JSON fully?

The complexity of having to remove the "." from field names is better than
ambiguous field resolving.
</comment><comment author="taotetek" created="2015-12-12T23:49:31Z" id="164202774">@dakrone - thank you for the response!

You compare this limitation to  both javascript and jq - but your assertion is incorrect.  Both javascript and jq fully support JSON with periods in field names, and provide syntax for working with them, as follows:

```
echo "{\"first.name\":\"brian\"}" | jq .'["first.name"]'
"brian"
```

You can use this associative array syntax for accessing fields with periods in them in javascript as well.

If there now exists a subset of possible JSON documents that are in compliance with the JSON standard, will validate properly with tools that implement the standard, but cannot be inserted into Elasticsearch - then Elasticsearch no longer supports the JSON standard.

While I understand that the decision was made in order avoid complexity in the query parser and to avoid some queries possibly resolving in unexpected ways, I find the decision regrettable. 

I'll add to this that in general, I'm very happy with Elasticsearch - I don't want my opinion on this matter to come across as a general opinion about ES or the engineers who contribute to it.  

Cheers,
Brian
</comment><comment author="jasontedor" created="2015-12-13T13:34:50Z" id="164258875">&gt; If there now exists a subset of possible JSON documents that are in compliance with the JSON standard, will validate properly with tools that implement the standard, but cannot be inserted into Elasticsearch - then Elasticsearch no longer supports the JSON standard.

This appears to be the crux of your argument. Namely, you're asserting that since Elasticsearch (intentionally) returns an error on some conforming JSON texts, Elasticsearch does not support the JSON standard.

JSON is a _data interchange format_, and the [JSON standard](https://tools.ietf.org/html/rfc7159) specifies the JSON grammar; the JSON standard does not a put a requirement on applications beyond specifying what a conforming JSON text is and requirements for parsing conforming JSON text.

Most importantly, there is no standard nor practical requirement that applications that accept JSON must handle without error every conforming JSON text.

Consider an application that uses the JSON format for application configuration; some configurations can be conforming JSON text but will be invalid for the application; the application can reject those but still be in conformance with how it _parses_ the configuration file.

Similarly, a web server that accepts a request with media type `application/json` can reject requests from clients that contain conforming JSON text in the request body, but are not valid requests for that web server. By way of a specific example, Twitter uses JSON to represent tweets and the associated metadata when it is communicating with a client; this does not mean that it must accept as a valid tweet every JSON document sent to one of its endpoints by a Twitter client.

And that is what Elasticsearch does. You can send it conforming JSON text via HTTP, but if it does not meet the requirements that Elasticsearch puts on JSON documents, Elasticsearch will give you an [HTTP Bad Request](https://tools.ietf.org/html/rfc7231#section-6.5.1). Elasticsearch will have parsed this document according to the JSON standard, and then after that it tells the client this is a bad request for Elasticsearch.

If, however, you were to find a conforming JSON document that Elasticsearch does not parse and represent correctly internally according to the JSON standard, or a JSON response from Elasticsearch that is not conforming JSON text, then there would be a legitimate issue and it would be addressed appropriately.
</comment><comment author="taotetek" created="2015-12-13T14:36:50Z" id="164263779">Elasticsearch no longer allows storage of key names that are in compliance with ECMA 404 definition of key names.  "It parses the full standard in order to reject a subset of it" is a poor argument for Elasticsearch's support of JSON.  

`Consider an application that uses the JSON format for application configuration; some configurations can be conforming JSON text but will be invalid for the application`

This is an apples to oranges comparison.  Elasticsearch is not rejecting conforming JSON text that describes a configuration or API call that elasticsearch does not accept.  Elasticsearch is rejecting the storage of JSON keys  that are conforming JSON text within JSON objects that should be perfectly valid to store.  In this case, the structure of the JSON object is perfectly valid, and what you are in fact rejecting is the JSON data exchange format as described by the standard.  

From what I'm hearing, it sounds like this decision is permanent.  Since I have no control over it, I'll work around it as best as I can.
</comment><comment author="jasontedor" created="2015-12-13T16:16:19Z" id="164271546">&gt; Elasticsearch no longer allows storage of key names that are in compliance with ECMA 404 definition of key names.

The word "key" nor the term "key name" never appear in [ECMA 404](http://www.ecma-international.org/publications/standards/Ecma-404.htm). The word "key" does [appear](https://tools.ietf.org/html/rfc7159#section-1.1) in [RFC 7159](https://tools.ietf.org/html/rfc7159), but only in the context of referring to the [RFC requirement levels](http://www.ecma-international.org/publications/standards/Ecma-404.htm); the term "key name" never appears in [RFC 7159](https://tools.ietf.org/html/rfc7159). Both documents do refer to "name/value" pairs. However, there is no requirement by either specification that an application must accept all names as being valid for that application.

The concerns that you're discussing are concerns at the application layer on which the JSON standard places no restrictions.

&gt;  "It parses the full standard in order to reject a subset of it" is a poor argument for Elasticsearch's support of JSON.

The _only_ valid argument that Elasticsearch does not support JSON is to provide an example of conforming JSON text that is not parsed correctly by Elasticsearch, or to provide an example of a response with media type `application/json` that is not conforming JSON text.

Elasticsearch, like any other application that consumes JSON, places application logic on top of JSON. This is valid, and in conformance with the intended uses of JSON.

&gt; &gt; Consider an application that uses the JSON format for application configuration; some configurations can be conforming JSON text but will be invalid for the application
&gt; 
&gt; This is an apples to oranges comparison.

It is not, because the analogy is for a concern at the application layer, just as Elasticsearch rejecting fields with dots in their name is a concern at the application layer. The JSON standard places no requirements on the application layer.

&gt; Elasticsearch is rejecting the storage of JSON keys that are conforming JSON text within JSON objects that should be perfectly valid to store.

Again, this is a concern at the application layer. The JSON standard does not concern itself with concepts at the application layer such as "storage". It merely specifies what conforming JSON text is, and how it is to be parsed. It does not place requirements on the application layer and rejecting fields with dots in their name is a concern at the application layer.

&gt; In this case, the structure of the JSON object is perfectly valid, and what you are in fact rejecting is the JSON data exchange format as described by the standard.

Such JSON is valid, and Elasticsearch correctly parses that JSON and then rejects it at the application layer. This does not violate [ECMA 404](http://www.ecma-international.org/publications/standards/Ecma-404.htm) nor [RFC 7159](https://tools.ietf.org/html/rfc7159).

&gt; From what I'm hearing, it sounds like this decision is permanent.

I'm hesitant to use a word like "permanent" but it is highly unlikely that this will change. Relates #12068.
</comment><comment author="bryanl" created="2015-12-14T13:58:42Z" id="164444468">Hello, I've been following along, and I'd like to point out two things:

```
&#10095;&#10095; node                                                                                                                                                                                                                                                                                                                                   &#9166; setup-install
&gt; s = JSON.parse('{"foo": {"bar.baz": 5}}')
{ foo: { 'bar.baz': 5 } }
&gt; s.foo
{ 'bar.baz': 5 }
&gt; s.foo["bar.baz"]
5
```

and 

```
echo '{"foo": {"bar.baz": 5}}' | jq '.foo["bar.baz"]'
5
```

Both node and jq handle keys with periods in them.
</comment><comment author="jasontedor" created="2015-12-14T14:15:46Z" id="164448179">&gt; Both node and jq handle keys with periods in them.

`JSON.parse` from ECMAScript and `jq` are _general purpose_ in their handling of JSON.

Not allowing dots in field names is a logical policy decision enforced in the _application layer_ of Elasticsearch. Elasticsearch correctly _parses_ the field names with dots in them, and then makes a logical policy decision to reject those at the application layer. This is not a violation of the JSON standard, and is consistent with the intended uses of JSON as a _data interchange format_. The JSON standard places no restrictions on the application layer of an application that consumes conforming JSON text.
</comment><comment author="doot0" created="2015-12-16T14:20:19Z" id="165122895">@jasontedor It seems your argument for not supporting these features is because you proactively choose not to "at the application layer". Your support for JSON is [explicitly implied](https://www.elastic.co/products/elasticsearch) on the elasticsearch product page under the "Schema-Free" heading. 

If one cannot actually index a valid JSON file (with periods in key names) into an elasticsearch DB, surely you should not be claiming that you can?
</comment><comment author="jasontedor" created="2015-12-16T15:10:40Z" id="165136368">&gt; If one cannot actually index a valid JSON file (with periods in key names) into an elasticsearch DB, surely you should not be claiming that you can?

The claim that Elasticsearch supports JSON does not translate into Elasticsearch has to accept without any restrictions whatsoever every conforming JSON text that is handed to it. There are rules to using the system, they must be understood and followed, and having them doesn't violate any claims that Elasticsearch supports JSON.

For example, if you specify a field as having type [`long`](https://www.elastic.co/guide/en/elasticsearch/reference/current/number.html) in a mapping, and then pass Elasticsearch a document for which that field can not be parsed as a valid [`long`](https://www.elastic.co/guide/en/elasticsearch/reference/current/number.html), then Elasticsearch _can_ make a logical policy decision at the application layer to reject that document. This is but one of many examples of reasons that conforming JSON text can be rejected at the application layer.

The _only_ valid argument that Elasticsearch does not fully support JSON is if there exists conforming JSON text that Elasticsearch does not parse correctly, or if there exists a JSON response body from Elasticsearch that is not valid JSON. If either of those possibilities occur, then we have a legitimate issue and it will be addressed appropriately.

But there is _still_ no standard nor practical requirement that every application that consumes conforming JSON text can not make logical policy decisions based on the contents of the JSON text at the application layer. This is perfectly within the use cases of JSON as a _data interchange format_.
</comment><comment author="jillesvangurp" created="2015-12-30T11:37:30Z" id="167984049">Respectfully, I strongly disagree with the notion that Elasticsearch can break its promise to index any valid json documents and reject content based on rather arbitrary limitations on field names that are not part of the json standard (as defined in https://tools.ietf.org/html/rfc7159). The standard defines what is a legal field name and elasticsearch doesn't support all legal fieldnames anymore.

This is a major breaking change that is deeply affecting us in multiple points in our architecture. Essentially, json compatibility was sacrificed in favor of syntactic sugar in the query language to be able to refer nested objects in an unambiguous way. That is valid of course but I'm now confronted with multiple external sources of perfectly valid json that used to index just fine that I can no longer index as is in elasticsearch as well as gigabytes of indices that I have to worry about migrating and testing. Migration to ES2 is a nightmare so far because of this. I'm months into planning the migration and still have a gazillion open issues; all revolving around finding and working around stupid dots in field names. This will likely continue to block us for some time and it is not like I haven't got more important stuff to worry about than field interpunction. Also even after I actually migrate this thing successfully I fully expect frequent regressions of dotted json slipping through and causing errors in the future as well. 

So, I respect that this decision was taken. Also I respect the fact that it wasn't taken lightly. But I do hope that Elasticsearch finds a way back to being a general purpose JSON document store, which it currently isn't anymore. IMHO more can be and should have been done to make this less painful.

One fix that comes to mind is to simply disable dynamic index creation for fields with dots; which is probably what people would prefer rather than the entire document being rejected with some error about dots. We are talking about unmapped fields here that are being dynamically mapped. If you then want the field indexed anyway, all you need to do is rename it or copy it to some field with the dots replaced with underscores (this could even be a mapping feature: auto_convert_dots:true). A couple of new mapping features to enable/disable this behavior would probably fix things for most users and unbreak json compatibility. Any indexed fields would be guaranteed to be dot free this way and instead of fixing the data or the intake pipeline all you need to fix is your mappings.
</comment><comment author="taotetek" created="2015-12-30T13:17:01Z" id="167997585">@jillesvangurp thank you - I was certain I couldn't be the only person this breaking change caused issues for.  For what it's worth, I've written a small daemon I'm now using that changes "."'s found in field names in my syslog traffic.  It was aggravating to burn engineering hours on turning JSON into "elasticsearch JSON" but the service is working for us.  The library includes a golang RFC3164 compliant syslog parser and a mutator that can scan keys and change them - in case it might be useful for your current pain it's available at https://github.com/digitalocean/captainslog
</comment><comment author="svincent" created="2015-12-31T07:42:25Z" id="168144083">There seems to be a fundamental difference in what the commenters here think the term "support" means. One camps sees it as end-to-end support, the other sees it as an input format.

I would contend that &#8220;JSON support&#8221; means end-to-end support. That is, saying Elasticsearch supports JSON implies to me that I can provide Elasticsearch with a valid, arbitrarily structured JSON document and Elasticsearch will index its contents. As long as the JSON supplied is (ECMA 404) valid, Elasticsearch&#8217;s application layer should handle it. Even more so since disallowing periods in the name of a collection's name/value pair is new to ES 2.x.

Saying that Elasticsearch supports JSON is, in my mind, tantamount to saying that JSON objects should flow through the system without limitation. If that&#8217;s not the intent of the Elasticsearch dev team and/or Elasticsearch BV, then that should be clearly indicated in the project&#8217;s documentation. I&#8217;d also suggest that the team avoid describing Elasticsearch supporting JSON because of the obvious confusion associated with that phrase. Rather, the documentation should clearly state (where appropriate) that JSON is only used as a data transfer format or that Elasticsearch supports a subset of JSON.

---

As a small addendum, I&#8217;ve been looking for Elasticsearch documentation on the character/format restrictions for field names. All I managed to find where this issue and a couple other issues on GitHub.

I did find that ES 2.x ues on Lucene 5.x and as far as I can tell Lucene 5.x only requires that [field names are strings](http://lucene.apache.org/core/5_4_0/core/org/apache/lucene/document/Field.html#name). I also found some docs for Solr that clearly specifies [the format of a valid field name](https://cwiki.apache.org/confluence/display/solr/Defining+Fields). 
</comment><comment author="jasontedor" created="2016-01-01T15:48:56Z" id="168313719">&gt; One camps sees it as end-to-end support, the other sees it as an input format.

@svincent The appeal to a formal standard was implied in the following from the [first comment](https://github.com/elastic/elasticsearch/issues/15404#issue-121863781) in this issue (emphasis on "by definition" added here):

&gt; Throwing exceptions on JSON field names that contain characters ( such as "." ) that are valid in JSON means that _by definition_ Elasticsearch no longer fully supports JSON.

and made explicit in the [third comment](https://github.com/elastic/elasticsearch/issues/15404#issuecomment-164202774) in this issue (the second by the OP):

&gt; Elasticsearch no longer supports the JSON standard.

Saying that an implementation does not support a formal standard has a well-established understanding: there are requirements in the formal standard placed on all implementations that the implementation does not meet.

An appeal to an actual formal standard was made in the [fifth comment](https://github.com/elastic/elasticsearch/issues/15404#issuecomment-164263779) in this issue (the third by the OP):

&gt; Elasticsearch no longer allows storage of key names that are in compliance with ECMA 404 definition of key names. "It parses the full standard in order to reject a subset of it" is a poor argument for Elasticsearch's support of JSON.

As the comments in this issue continued along these lines with additional appeals to ECMA 404 and RFC 7159, the meaning of "fully supports JSON" was solidified. I think it is fair to claim that we have been talking about the same thing: whether or not Elasticsearch is in compliance with the JSON standard.

&gt; I would contend that &#8220;JSON support&#8221; means end-to-end support.

In this issue, it does not.

&gt; Saying that Elasticsearch supports JSON is, in my mind, tantamount to saying that JSON objects should flow through the system without limitation.

This is a requirement that Elasticsearch has never provided.

&gt; As a small addendum, I&#8217;ve been looking for Elasticsearch documentation on the character/format restrictions for field names.

It's in the [breaking changes for 2.0](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots).

&gt; I did find that ES 2.x ues on Lucene 5.x and as far as I can tell Lucene 5.x only requires that [field names are strings](http://lucene.apache.org/core/5_4_0/core/org/apache/lucene/document/Field.html#name).

The requirement is not from Lucene, it's a requirement from the logic that Elasticsearch builds on top of Lucene.
</comment><comment author="jasontedor" created="2016-01-01T16:56:26Z" id="168317023">&gt; Respectfully, I strongly disagree with the notion that Elasticsearch can break its promise to index any valid json documents and reject content based on rather arbitrary limitations on field names

@jillesvangurp It is not arbitrary and it was, as you note, not taken lightly. The ultimate reason was to avoid ambiguity, a dangerous problem. This is covered throughly in the [breaking changes for 2.0](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots), #5972, #7112, #11337, #12068, and #14359.

&gt; that are not part of the json standard (as defined in https://tools.ietf.org/html/rfc7159). The standard defines what is a legal field name and elasticsearch doesn't support all legal fieldnames anymore.

Please note these key clauses from [RFC 7159](https://tools.ietf.org/html/rfc7159):

&gt; `An object is an unordered collection of zero or more name/value`
&gt; `pairs, where a name is a string and a value is a string, number,`
&gt; `boolean, null, object, or array.`

and

&gt; `An implementation may set limits on the length and`
&gt; `character contents of strings.`

I maintain that even without these clauses, Elasticsearch can make a logical policy decision at the application layer to reject certain conforming JSON texts, but these clauses leave no doubt.
</comment><comment author="davidelang" created="2016-01-26T21:24:39Z" id="175236751">During the Project Lumberjack discussions, we were talking about ways to 'flatten' references to multi-tier JSON structure elements and during that discussion we picked ! as the level delimiter, because it's a reserved character in so many languages, people are very unlikely to use it in a name. As this discussion demonstrates, using a period as the delimiter is problematic as it's a common character to use in variable names.

Perhapse the easiest path forward is to tweak Elasticsearch so that the delimiter character is configurable. For ES2.0, leave the default as '.' (as currently defined and documented), and consider migrating to '!' going forward. 

Rsyslog uses ! as the delimiter, and it is the default on linux distros right now.
syslog-ng uses . as the delimiter, so it would have the same problem that ES currently has.
logstash accesses multi tier data as [level1][level2]
nxlog doesn't implement any support for multi-level variables
sumologic uses . as the delimiter, so it would have the same problem

so in spite of people discussing he issue and the problems using dot as the separator and agreeing on a 'standard', it seems that the different logging systems have gone in different directions (the nice thing about standards is that there are so many to choose from )-:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added description for parameter &lt;local&gt;, FIX #14125</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15403</link><project id="" key="" /><description>Updated cluster-health docs with the `local` parameter description. 

Closs #14125
</description><key id="121825665">15403</key><summary>Added description for parameter &lt;local&gt;, FIX #14125</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fforbeck</reporter><labels><label>docs</label></labels><created>2015-12-12T02:01:47Z</created><updated>2015-12-14T13:24:56Z</updated><resolved>2015-12-14T13:24:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T13:24:56Z" id="164435649">thanks @fforbeck - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change rest integ tests to not have hardcoded ports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15402</link><project id="" key="" /><description>This change removes hardcoded ports from cluster formation. It passes
port 0 for http and transport, and then uses a special property to have
the node log the ports used for http and transport (just for tests).
This does not yet work for multi node tests. This brings us one step
closer to working with --parallel.
</description><key id="121824423">15402</key><summary>Change rest integ tests to not have hardcoded ports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-12T01:41:33Z</created><updated>2015-12-12T02:20:57Z</updated><resolved>2015-12-12T02:20:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-12T01:47:13Z" id="164095115">+1
</comment><comment author="rjernst" created="2015-12-12T02:10:45Z" id="164097164">@nik9000 I pushed a new commit.
</comment><comment author="nik9000" created="2015-12-12T02:19:50Z" id="164098432">LGTM
</comment><comment author="rjernst" created="2015-12-12T02:20:51Z" id="164098729">FYI, I pushed another commit that removes the supressions altogether.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow nodes with missing custom meta data to join cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15401</link><project id="" key="" /><description>Currently, when some nodes in a cluster are missing plugins with custom metadata, the nodes silently fail while trying to deserialize the unknown custom metadata on the first cluster state update. Ideally, the node should fail hard and make it obvious why it failed. 

This PR treats node with missing custom metadata same as a node with version incompatibility. When a node with missing custom metadata tries to join a cluster, the master detects it and the node fails to join the cluster. 

closes #13445 
</description><key id="121820813">15401</key><summary>Don't allow nodes with missing custom meta data to join cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Discovery</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-12T00:47:21Z</created><updated>2016-01-10T19:48:58Z</updated><resolved>2015-12-18T16:34:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2015-12-12T00:48:45Z" id="164090747">would like to know your thoughts on this, @imotov &amp; @bleskes. 
</comment><comment author="imotov" created="2015-12-15T05:35:08Z" id="164649245">Custom data types can be added on three levels - cluster state level, metadata level and index metadata level. This PR only handles metadata level. I think we should handle all three level
</comment><comment author="areek" created="2015-12-16T03:46:02Z" id="164981055">Thanks for having a look, @imotov! I updated the PR to use the node join validation request (thanks @bleskes for the tip!) to attempt to  deserialize master's cluster state. Now, if the joining node has missing custom data types on any level, the deserialization will fail. The join failure is logged as an ISE hinting missing plugins on the joining node.
</comment><comment author="bleskes" created="2015-12-18T15:19:20Z" id="165802259">LGTM. Left one minor request but no need for another review. Also, I think this can go into 2.2, no?
</comment><comment author="areek" created="2015-12-19T00:22:30Z" id="165925999">Thanks @bleskes for the review, back ported to 2.x in https://github.com/elastic/elasticsearch/commit/8ab3ca8a87310ca0e60da16f80680748d273ee5a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix docs with `match_pattern` in dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15400</link><project id="" key="" /><description /><key id="121785094">15400</key><summary>Fix docs with `match_pattern` in dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">williamsandrew</reporter><labels><label>docs</label></labels><created>2015-12-11T20:23:29Z</created><updated>2015-12-14T13:19:36Z</updated><resolved>2015-12-14T13:18:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T13:19:36Z" id="164434187">thanks @TheDude05 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Range Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15399</link><project id="" key="" /><description /><key id="121754879">15399</key><summary>Aggregations Refactor: Refactor Range Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-11T17:24:15Z</created><updated>2015-12-14T13:24:22Z</updated><resolved>2015-12-14T13:24:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-14T09:50:02Z" id="164393394">I left two minor comments but it looks good to me otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the `format` option of the `_source` field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15398</link><project id="" key="" /><description>This option allows to force the xcontent type to use to store the `_source`
document. The default is to use the same format as the input format.

This commit makes this option ignored for 2.x indices and rejected for 3.0
indices.
</description><key id="121752039">15398</key><summary>Remove the `format` option of the `_source` field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-12-11T17:08:47Z</created><updated>2015-12-11T17:27:29Z</updated><resolved>2015-12-11T17:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-11T17:11:34Z" id="163993476">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>LM Similarity returns all zero scores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15397</link><project id="" key="" /><description>When I use the LMDirichlet similarity I get back are all zero scores. Here is the steps to reproduce the problem:
1) Created the index:

{ "settings": { "similarity": { "LMSimilarity": { "type": "LMDirichlet", "mu": 2500 } } }, "mappings": { "item": { "properties": { "title": { "type": "string", "similarity": "LMSimilarity" } } } } }

2) Indexed two documents:

{"title":"This is a test for search similarity when we search by other search options."}
{"title&#8221;:&#8221;Search looks weird when use other search possibilities. Numbers are not clear. Just adding new stuff to make the document longer. Document norm looks weird."}

3) Run a simple query:

{ "explain": "true", "query": { "match": { "title": "search" } } }

If you look at the returned scores, there are multiple weird numbers:
1) The score for all documents is zero
2) The collection probability, a term property that is independent from individual documents, is different for each document. I expect this number to be the same for all documents for a given term.
3) Document norm has a negative value, probably it's the log of another number, but I can't match these numbers to the LM formula.
</description><key id="121742000">15397</key><summary>LM Similarity returns all zero scores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keikha</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-12-11T16:26:29Z</created><updated>2016-02-14T19:12:23Z</updated><resolved>2016-02-14T19:12:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T19:12:23Z" id="183957299">Closing in favour of https://github.com/elastic/elasticsearch/issues/15345
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add java API for synced flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15396</link><project id="" key="" /><description>We initially did not add this because we thought it was not needed
but it is, see #12812.

closes #12812
</description><key id="121740188">15396</key><summary>Add java API for synced flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label><label>v2.2.0</label></labels><created>2015-12-11T16:17:32Z</created><updated>2015-12-18T10:18:39Z</updated><resolved>2015-12-16T15:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-12-14T13:31:48Z" id="164437733">Thanks for the review! Addressed all comments.
</comment><comment author="jaymode" created="2015-12-15T18:04:03Z" id="164843151">LGTM. Would be good to let @nik9000 review again or have someone else take a look. Thanks @brwe 
</comment><comment author="nik9000" created="2015-12-15T22:10:36Z" id="164913599">LGTM
</comment><comment author="nik9000" created="2015-12-16T13:19:40Z" id="165101670">Lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fail build on wildcard imports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15395</link><project id="" key="" /><description>Wildcard imports are terrible, they cause ambiguity in the code,
make it not compile with the future versions of java in many cases.

We should simply fail the build on this, it is messiness, caused by
messy Intellij configuration
</description><key id="121737553">15395</key><summary>fail build on wildcard imports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-11T16:04:58Z</created><updated>2015-12-18T21:53:07Z</updated><resolved>2015-12-18T21:45:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-11T16:06:34Z" id="163975767">+1
</comment><comment author="jpountz" created="2015-12-11T16:11:02Z" id="163977000">+1
</comment><comment author="jasontedor" created="2015-12-11T16:14:18Z" id="163977890">I am very much in favor in this change, but this will also require a change to our [CONTRIBUTING.md](https://github.com/elastic/elasticsearch/blob/ed53edd4d0b0a2e08a13571603cd642c5910ee67/CONTRIBUTING.md) where the [lines](https://github.com/elastic/elasticsearch/blob/ed53edd4d0b0a2e08a13571603cd642c5910ee67/CONTRIBUTING.md#contributing-to-the-elasticsearch-codebase)

&gt; Don't worry too much about imports. Try not to change the order but don't worry about fighting your IDE to stop it from switching from \* imports to specific imports or from specific to \* imports.

currently appear.

Additionally, this very topic came up a few months ago so I think we should ensure we have consensus here.
</comment><comment author="rmuir" created="2015-12-11T16:16:12Z" id="163978381">I expect Intellij users to defend their shitty IDE, but they have no ground to stand on.

Bottom line: wildcard imports are bad.
</comment><comment author="jasontedor" created="2015-12-11T16:24:09Z" id="163980495">&gt; I expect Intellij users to defend their shitty IDE, but they have no ground to stand on.

The issue is independent of the IDE (and it's a simple configuration change in IntelliJ).
</comment><comment author="rjernst" created="2015-12-11T16:32:02Z" id="163983506">+1 to the change as is.

I think we can followup separately with a change to the contributing docs. This is for master only.
</comment><comment author="rmuir" created="2015-12-11T16:32:36Z" id="163983702">I see wildcard imports being created by intellij ides, not folks using emacs or eclipse.

Its fine to keep the settings in intellij for wildcard imports if you like that. just clean up your mess before pushing, before pushing it onto others to deal with.
</comment><comment author="jpountz" created="2015-12-11T16:49:42Z" id="163988057">&gt; I think we can followup separately with a change to the contributing docs. This is for master only.

I don't understand the push back. I think @jasontedor 's note makes perfect sense: we should be consistent about what we tell contributors and what we enforce. If this was a burden that was in the way of a simple incremental change, I would have a different opinion, but here it's nothing like that?
</comment><comment author="jasontedor" created="2015-12-11T16:54:10Z" id="163989067">&gt; +1 to the change as is.

In addition to @jpountz's reply, it can not go in as is because the build will fail. It will require a wholesale change to imports in all source files.
</comment><comment author="bleskes" created="2015-12-11T16:58:24Z" id="163990092">+1 to every one on this thread - both getting this in and doing it properly, with a doc change and preventing the build from immediately failing.

&gt; On 11 Dec 2015, at 17:54, Jason Tedor notifications@github.com wrote:
&gt; 
&gt; +1 to the change as is.
&gt; 
&gt; In addition to @jpountz's reply, it can not go in as is because the build will fail. It will require a wholesale change to imports in all source files.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="nik9000" created="2015-12-11T16:58:54Z" id="163990195">&gt; In addition to @jpountz's reply, it can not go in as is because the build will fail. It will require a wholesale change to imports in all source files.

I'm up for taking 1/2 of the classes this evening and just opening them in Eclipse, organizing imports, and saving them again. Its like what I did with Map only it won't take 123187123 hours.
</comment><comment author="jpountz" created="2015-12-11T16:59:47Z" id="163990398">@nik9000 I suspect some ides have a way to do it automatically for a whole project, maybe it's not such a painful task?
</comment><comment author="nik9000" created="2015-12-11T16:59:47Z" id="163990399">It'll make merge conflicts. But having Eclipse do no \* imports and intellij do them after 5 imports or something has been like a merge conflict machine for years.
</comment><comment author="jasontedor" created="2015-12-11T16:59:57Z" id="163990434">&gt; I'm up for taking 1/2 of the classes this evening and just opening them in Eclipse, organizing imports, and saving them again. Its like what I did with Map only it won't take 123187123 hours.

IntelliJ can do it in two clicks (do not even need to open the files, can wholesale optimize the entire project).
</comment><comment author="nik9000" created="2015-12-11T17:00:32Z" id="163990570">&gt; IntelliJ can do it in two clicks (do not even need to open the files, can wholesale optimize the entire project).

Go nuts. I dunno if I have such a button but I'd certainly look for it.
</comment><comment author="nik9000" created="2015-12-11T17:01:59Z" id="163990887">&gt; Go nuts. I dunno if I have such a button but I'd certainly look for it.

Looks like Eclipse has it in 4 steps.
1. Click in file list.
2. Ctrl-A
3. Click Source
4. Click Organize Imports
</comment><comment author="nik9000" created="2015-12-11T17:05:48Z" id="163992086">Ok - I just finished doing it on whatever I had checked out. The diff is an unreviewable 1.4MB.

Proposal: We make two PRs. One with all the \* imports removed which we won't review at all and just cram in if the tests pass. The other we'll actually review. This one can be the first one. We can get fancy and merge this one into the big one and then merge them at the same time or something so its atomic.
</comment><comment author="dakrone" created="2015-12-11T17:12:11Z" id="163993621">+1
</comment><comment author="rjernst" created="2015-12-11T17:57:39Z" id="164003793">&gt; &gt;  I think we can followup separately with a change to the contributing docs. This is for master only.
&gt; 
&gt; I don't understand the push back. 

My push back is to making changes that are good sit and wait on bikeshedding. And I am confident any change to contributing docs about IDE setup would be bikeshedded (wording, particular path to change settings, particular value to this setting in intellij because it is a number of classes before using wildcard). But this is a general problem: master has to be more fluid. Should we have a followup issue to update contributing documentation? Absolutely. But should this change sit for 2 days while devs from 2 continents argue about wording that doesn't affect the _actual_ change?

&gt; it can not go in as is because the build will fail

I had assumed this would be done just before merge, with the couple of clicks mentioned here already.
</comment><comment author="nik9000" created="2015-12-11T18:09:44Z" id="164006408">&gt; I had assumed this would be done just before merge, with the couple of clicks mentioned here already.

That'd be fine too. Just for reference - the project didn't build after I ran the cleanup. I'm sure it was minor but I just reverted and went back to what I was working on. So it'll take a little bit of by hand cleanup. Probably not much. Its fine.

&gt; But this is a general problem: master has to be more fluid.

I'm super forgetful and, as someone who relatively recently made this my full time job I remember very clearly what it was like when it wasn't. Its alienating when the contributing docs are wrong. So I'm in the "just change the wording now and bike shed about the wording in another PR". So I sent @rmuir a PR against his branch with an amended CONTRIBUTING.md.
</comment><comment author="nik9000" created="2015-12-11T18:44:02Z" id="164014587">LGTM now. Thanks for fixing the typo.
</comment><comment author="jpountz" created="2015-12-18T21:53:07Z" id="165905800">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RangeQueryParser should accept `_name` in inner field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15394</link><project id="" key="" /><description>This re-introduces support for the "_name" parameter in range queries on the field level that was droped while transitioning to 2.x but was allowed up to 1.7 and is the prefered location on master.
Also keeps parsing "_name" on the top level but deprecating it via use of ParseField.

Closes #15306 
</description><key id="121724195">15394</key><summary>RangeQueryParser should accept `_name` in inner field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>regression</label><label>review</label><label>v2.1.1</label><label>v2.2.0</label></labels><created>2015-12-11T15:01:12Z</created><updated>2015-12-15T10:49:22Z</updated><resolved>2015-12-11T16:31:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-11T15:17:57Z" id="163961261">LGTM
</comment><comment author="jpountz" created="2015-12-11T16:11:51Z" id="163977205">LGTM
</comment><comment author="cbuescher" created="2015-12-11T16:31:05Z" id="163983192">Thanks, will merge. @jpountz do you also think this should go to the 2.1 branch?
</comment><comment author="jpountz" created="2015-12-14T09:11:28Z" id="164384426">The fix is low-risk, so +1 to backport to 2.1.2
</comment><comment author="cbuescher" created="2015-12-14T09:23:12Z" id="164386756">Backported to 2.1. branch with 4a49d8b4e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>getKeyAsString and key_as_string should be the same for terms aggregation on boolean field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15393</link><project id="" key="" /><description>LongTerms.Bucket.getKeyAsString now returns "false" or "true" for boolean fields
in order to be consistent with the json response field `key_as_string` of the terms aggregation.
The downside is that getBucketByKey("0") needs to be changed with getBucketByKey("false") when dealing with booleans.
</description><key id="121723957">15393</key><summary>getKeyAsString and key_as_string should be the same for terms aggregation on boolean field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Aggregations</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-11T15:00:06Z</created><updated>2016-07-29T12:08:58Z</updated><resolved>2015-12-15T11:11:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-15T09:19:22Z" id="164696342">LGTM
</comment><comment author="jimczi" created="2015-12-15T11:25:10Z" id="164734695">@jpountz @colings86 should this be backported to 2.x ?
</comment><comment author="colings86" created="2015-12-15T11:28:13Z" id="164735447">I would personally say no as this is a breaking change in the Java API. Which brings up something we missed, could you update the braking changes in 3.0 document to detail this braking change?
</comment><comment author="jimczi" created="2015-12-15T11:42:13Z" id="164739083">@colings86 sure I'll do, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Recover the old indices after restarting Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15392</link><project id="" key="" /><description>I have an elasticsearch cluster with three nodes and with this configuration:

```
cluster.name: my_cluster_name

bootstrap.mlockall: true

indices.recovery.max_bytes_per_sec: 100mb
discovery.zen.minimum_master_nodes: 2
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: ["ip_node_1", "ip_node_2"]

path.logs: elasticsearch/logs
path.data: elasticsearch/data
path.work: elasticsearch/work
path.plugins: elasticsearch/plugins

http.cors.enabled: true
http.cors.allow-origin: /.*/
http.cors.allow-credentials: true 

index.routing.allocation.disable_allocation: false
```

This cluster was restarted and in the path

```
/elasticsearch/data/my_cluster_name/nodes/0/indices
```

Where elasticsearch save the indices I can see:

```
ls -las

total 24
4 drwxr-xr-x 6 jdiazg group 4096 Dec 11 09:01 .
4 drwxr-xr-x 4 jdiazg group 4096 Dec 10 14:19 ..
4 drwxr-xr-x 7 jdiazg group 4096 Dec  8 09:29 index_1
4 drwxr-xr-x 5 jdiazg group 4096 Dec  1 03:15 index_2
4 drwxr-xr-x 5 jdiazg group 4096 Nov 20 15:34 index_3
4 drwxr-xr-x 6 jdiazg group 4096 Dec  3 03:42 index_4
```

and with

```
du -sh  * 

514M    index_1
48M     index_2
420K    index_3
332K    index_4
```

in the three nodes, but if I execute:

```
curl localhost:9200/_cat/indices?v
```

I am not able to see this old indices and elastic_search don&#180;t recognice when the name of the cluster after restarting is the same.
</description><key id="121719508">15392</key><summary> Recover the old indices after restarting Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juandasgandaras</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2015-12-11T14:38:32Z</created><updated>2016-04-08T20:30:15Z</updated><resolved>2016-02-14T19:11:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T17:13:41Z" id="164498622">What's in the logs?
</comment><comment author="juandasgandaras" created="2015-12-15T07:58:39Z" id="164678284">In the logs from the cluster node I can see that the the module gateway try to recover the one index but it is not able to  recognize the oldest indices in the directory although they appear in the directory 

```
/elasticsearch/data/my_cluster_name/nodes/0/indices
```

The logs are:

```
[2015-12-14 16:28:38,305][INFO ][http                     ] [Jacqueline Falsworth] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.8.5.15:9200]}
[2015-12-14 16:28:38,305][INFO ][node                     ] [Jacqueline Falsworth] started
[2015-12-14 16:28:38,764][INFO ][cluster.service          ] [Jacqueline Falsworth] added {[Mary Jane Parker][sJRJICsiQcO2T3YL3m7bQw][bc10-07][inet[/10.8.5.17:9300]],}, reason: zen-disco-receive(join from node[[Mary Jane Parker][sJRJICsiQcO2T3YL3m7bQw][bc10-07][inet[/10.8.5.17:9300]]])
[2015-12-14 16:28:38,780][INFO ][gateway                  ] [Jacqueline Falsworth] delaying initial state recovery for [5m]. expecting [3] nodes, but only have [2]
[2015-12-14 16:30:19,035][INFO ][cluster.service          ] [Jacqueline Falsworth] added {[Hindsight Lad][ONuKxehCQz-ty-L5_2Mn6Q][bc10-03][inet[/10.8.5.13:9300]],}, reason: zen-disco-receive(join from node[[Hindsight Lad][ONuKxehCQz-ty-L5_2Mn6Q][bc10-03][inet[/10.8.5.13:9300]]])
[2015-12-14 16:30:19,886][INFO ][gateway                  ] [Jacqueline Falsworth] recovered [1] indices into cluster_state
[2015-12-14 16:52:22,694][INFO ][cluster.service          ] [Jacqueline Falsworth] removed {[Mary Jane Parker][sJRJICsiQcO2T3YL3m7bQw][bc10-07][inet[/10.8.5.17:9300]],}, reason: zen-disco-node_left([Mary Jane Parker][sJRJICsiQcO2T3YL3m7bQw][bc10-07][inet[/10.8.5.17:9300]])
[2015-12-14 16:55:26,961][INFO ][cluster.service          ] [Jacqueline Falsworth] added {[Ringmaster][rfnlKtTiQa6yEuwgkg6hag][bc10-07][inet[/10.8.5.17:9300]],}, reason: zen-disco-receive(join from node[[Ringmaster][rfnlKtTiQa6yEuwgkg6hag][bc10-07][inet[/10.8.5.17:9300]]])
```

And in the directory we can see:

```
pwd

/tmp/elasticsearch/data/my_cluster_name/nodes/0/indices

ls -las

total 24
4 drwxr-xr-x 6 jdiazg group 4096 Dec 11 09:01 .
4 drwxr-xr-x 4 jdiazg group 4096 Dec 10 14:19 ..
4 drwxr-xr-x 7 jdiazg group 4096 Dec  8 09:29 index_1
4 drwxr-xr-x 5 jdiazg group 4096 Dec  1 03:15 index_2
4 drwxr-xr-x 5 jdiazg group 4096 Nov 20 15:34 index_3
4 drwxr-xr-x 6 jdiazg group 4096 Dec  3 03:42 index_4
```

I am not able to know why.

And the day my service fail and my three nodes stop to work in the logs from two nodes appear this:

one node:

```
[2015-12-08 09:29:47,871][INFO ][cluster.service          ] [Rhiannon] removed {[Susan Richards][S9SqbKMqQ4idmBSYUW8g7Q][bc10-05][inet[/10.8.5.15:9300]],}, reason: zen-disco-node_failed([Susan Richards][S9SqbKMqQ4idmBSYUW8g7Q][bc10-05][i
net[/10.8.5.15:9300]]), reason transport disconnected
```

and in the other node:

```
[2015-12-08 09:29:47,863][WARN ][index.store              ] [Scaleface] [index_3][3] failed to build store metadata. checking segment info integrity (with commit [no])
java.nio.file.NoSuchFileException: /tmp/elasticsearch/data/my_cluster_name/nodes/0/indices/index_3/3/index/_b_es090_0.tip
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
        at java.nio.channels.FileChannel.open(FileChannel.java:287)
        at java.nio.channels.FileChannel.open(FileChannel.java:334)
        at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:81)
        at org.apache.lucene.store.FileSwitchDirectory.openInput(FileSwitchDirectory.java:172)
        at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
        at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.checksumFromLuceneFile(Store.java:708)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:613)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:596)
        at org.elasticsearch.index.store.Store.getMetadata(Store.java:186)
        at org.elasticsearch.index.store.Store.getMetadataOrEmpty(Store.java:150)
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:160)
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:141)
        at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:62)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:278)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:269)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
[2015-12-08 09:29:47,865][INFO ][index.store              ] [Scaleface] [index_3][3] Failed to open / find files while reading metadata snapshot
[2015-12-08 09:29:47,873][INFO ][cluster.service          ] [Scaleface] removed {[Susan Richards][S9SqbKMqQ4idmBSYUW8g7Q][bc10-05][inet[/10.8.5.15:9300]],}, reason: zen-disco-receive(from master [[Rhiannon][Exa-bPALTeiamy-MdqWCyA][bc10-0
7][inet[/10.8.5.17:9300]]])
[2015-12-08 09:29:50,956][INFO ][discovery.zen            ] [Scaleface] master_left [[Rhiannon][Exa-bPALTeiamy-MdqWCyA][bc10-07][inet[/10.8.5.17:9300]]], reason [transport disconnected]
[2015-12-08 09:29:50,957][WARN ][discovery.zen            ] [Scaleface] master left (reason = transport disconnected), current nodes: {[Scaleface][anTWSyPMRPSwy4cZqs_I2w][bc10-03][inet[/10.8.5.13:9300]],}
[2015-12-08 09:29:50,960][INFO ][cluster.service          ] [Scaleface] removed {[Rhiannon][Exa-bPALTeiamy-MdqWCyA][bc10-07][inet[/10.8.5.17:9300]],}, reason: zen-disco-master_failed ([Rhiannon][Exa-bPALTeiamy-MdqWCyA][bc10-07][inet[/10.
8.5.17:9300]])
```

In the master node does not appear logs from this day.
</comment><comment author="bleskes" created="2015-12-15T13:59:44Z" id="164773208">&gt; ```
&gt; 4 drwxr-xr-x 7 jdiazg group 4096 Dec  8 09:29 index_1
&gt; 4 drwxr-xr-x 5 jdiazg group 4096 Dec  1 03:15 index_2
&gt; 4 drwxr-xr-x 5 jdiazg group 4096 Nov 20 15:34 index_3
&gt; 4 drwxr-xr-x 6 jdiazg group 4096 Dec  3 03:42 index_4
&gt; ```

Thanks. Can you also list the directories bellow the index directory? I'm looking to see if there is a valid `_state` folder with a state file.
</comment><comment author="juandasgandaras" created="2015-12-15T14:42:57Z" id="164784183">In one of the nodes of the elasticsearch cluster we can see the next list:

```
tree index_1/

index_1/
&#9500;&#9472;&#9472; 0
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-8
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9500;&#9472;&#9472; 2
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-5
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9500;&#9472;&#9472; 3
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-6
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9492;&#9472;&#9472; 4
    &#9500;&#9472;&#9472; index
    &#9500;&#9472;&#9472; _state
    &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-6
    &#9492;&#9472;&#9472; translog

tree index_2/
index_2/
&#9500;&#9472;&#9472; 1
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-8
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9500;&#9472;&#9472; 3
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-6
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9492;&#9472;&#9472; 4
    &#9500;&#9472;&#9472; index
    &#9500;&#9472;&#9472; _state
    &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-6
    &#9492;&#9472;&#9472; translog

 tree index_3/
new_gompute_history_ldap/
&#9500;&#9472;&#9472; 1
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9500;&#9472;&#9472; 2
&#9474;&#160;&#160; &#9492;&#9472;&#9472; index
&#9500;&#9472;&#9472; 3
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9492;&#9472;&#9472; 4
    &#9492;&#9472;&#9472; index

tree index_4/

tree index_4/
index_4/
&#9500;&#9472;&#9472; 0
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_5
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; state-21
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-3
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9474;&#160;&#160;     &#9492;&#9472;&#9472; translog-1449847248888
&#9500;&#9472;&#9472; 2
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_5
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-16
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9474;&#160;&#160;     &#9492;&#9472;&#9472; translog-1449847248515
&#9500;&#9472;&#9472; 3
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_5
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-13
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9474;&#160;&#160;     &#9492;&#9472;&#9472; translog-1449847248888
&#9500;&#9472;&#9472; 4
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_6
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-18
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9474;&#160;&#160;     &#9492;&#9472;&#9472; translog-1449847249266
&#9492;&#9472;&#9472; _state
    &#9492;&#9472;&#9472; state-3

```

The last index the index_4 I can see it but I after I inserted new data over it ot test and I try to do do a  snapshot to restore all the data without success.

Do you want to see more information?
</comment><comment author="bleskes" created="2015-12-16T14:55:44Z" id="165132719">Thanks @juandasgandaras this makes it clear. You'll notice only index_4 has a `_state` folder, which is why it's the only one that's imported. The question is what happened to the _state folder of the other indices. Were the indices delete at some point in time? also what ES version are we talking about?
</comment><comment author="juandasgandaras" created="2015-12-16T15:58:03Z" id="165154373">Hi,

This is because I modified the index_4 to test and after I tried to save all the indices but only one (the index_4) was saved.

I have never deleted the indices in Elasticsearch. The problem was that the diferent host over elasticsearch was running were shotdowned.

After this I tried to restart the Elasticsearch cluster with the same cluster name and the same parameters, but when I restart elasticsearch, this cluster doesn&#180;t find this folders. 

The elasticsearch version that I am using is 1.4.4
</comment><comment author="clintongormley" created="2016-02-14T19:11:15Z" id="183956860">@juandasgandaras So much has changed since 1.4.4, I highly recommend upgrading.
</comment><comment author="shortdudey123" created="2016-04-01T22:04:46Z" id="204585932">@clintongormley @bleskes any update on this? having this issue on 1.7.2

```
/var/lib/elasticsearch/cluster01/nodes/0/indices/index1-2016.03.01/
&#9500;&#9472;&#9472; 1
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu.fdt
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu.fdx
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu.fnm
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu_Lucene410_0.dvd
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu_Lucene410_0.dvm
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu_Lucene41_0.doc
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu_Lucene41_0.pos
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu_Lucene41_0.tim
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu_Lucene41_0.tip
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu.nvd
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu.nvm
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _20fu.si
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_73
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-0.st
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9474;&#160;&#160;     &#9492;&#9472;&#9472; translog-1456783294865
&#9500;&#9472;&#9472; 2
&#9474;&#160;&#160; &#9500;&#9472;&#9472; index
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu.fdt
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu.fdx
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu.fnm
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu_Lucene410_0.dvd
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu_Lucene410_0.dvm
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu_Lucene41_0.doc
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu_Lucene41_0.pos
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu_Lucene41_0.tim
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu_Lucene41_0.tip
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu.nvd
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu.nvm
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1xzu.si
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_76
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
&#9474;&#160;&#160; &#9500;&#9472;&#9472; _state
&#9474;&#160;&#160; &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-1.st
&#9474;&#160;&#160; &#9492;&#9472;&#9472; translog
&#9474;&#160;&#160;     &#9492;&#9472;&#9472; translog-1456783294918
&#9492;&#9472;&#9472; 4
    &#9500;&#9472;&#9472; index
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx.fdt
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx.fdx
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx.fnm
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx_Lucene410_0.dvd
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx_Lucene410_0.dvm
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx_Lucene41_0.doc
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx_Lucene41_0.pos
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx_Lucene41_0.tim
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx_Lucene41_0.tip
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx.nvd
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx.nvm
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; _1ydx.si
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments_73
    &#9474;&#160;&#160; &#9500;&#9472;&#9472; segments.gen
    &#9474;&#160;&#160; &#9492;&#9472;&#9472; write.lock
    &#9500;&#9472;&#9472; _state
    &#9474;&#160;&#160; &#9492;&#9472;&#9472; state-2.st
    &#9492;&#9472;&#9472; translog
        &#9492;&#9472;&#9472; translog-1456783294935

12 directories, 51 files
```
</comment><comment author="bleskes" created="2016-04-07T10:16:38Z" id="206798783">@ShirlinYiu not sure what kind of update you are expecting. The index folders you mention also do not contain the index state file, which means you were probably running with dedicated master nodes. I presume you had some problem and lost the master nodes' data folders. With 1.7 that would mean the index data can not be used as we miss all the meta data associated with it. With 2.x this is no longer the case as we store a redundant copy (it's not used) of the meta data on all data nodes as well. 
</comment><comment author="shortdudey123" created="2016-04-07T16:15:38Z" id="206976218">@bleskes assuming you meant me

I was expecting there to be a resolution to this issue since you had said `So much has changed since 1.4.4`.  I ended up having to blow the cluster away with complete data loss.
</comment><comment author="bleskes" created="2016-04-08T09:58:16Z" id="207355943">@shortdudey123 yeah, sorry I meant you indeed...  I suspect the problem is cause by the loss of all master nodes which in 1.x will indeed cause indices not to be re-imported (as designed, ...) . In 2.x we add a safety net for this kind of cases by adding a backup file into each index folder. In that sense, this issue is solved, but you need to upgrade.
</comment><comment author="shortdudey123" created="2016-04-08T20:30:15Z" id="207591266">gotcha, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add test for parsing "_name" field in RangeQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15391</link><project id="" key="" /><description>The issue in #15306 brought up the question where the query parser accepts the "_name" parameter for range queries. This adds a test for the current situation on master which expects "_name" in the inner field but also allows but deprecates it on the top level.
</description><key id="121710751">15391</key><summary>Tests: Add test for parsing "_name" field in RangeQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-11T13:47:17Z</created><updated>2015-12-14T09:13:08Z</updated><resolved>2015-12-14T09:13:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-12-14T09:04:53Z" id="164382974">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce Local checkpoints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15390</link><project id="" key="" /><description>This PR introduces the notion of a local checkpoint on the shard level. A local check point is defined as a the highest sequence number for which all previous operations (i.e. with a lower seq#) have been processed.

The current implementation is based on a fixed in memory bit array which is used in a round robin fashion. This introduces a limit to the spread between inflight indexing operation. We are still discussing options to work around this, but I think we should move forward toward a working system and optimize from there (and either remove this limitation or better understand it's implications).

relates to #10708

replaces #15111 
</description><key id="121706097">15390</key><summary>Introduce Local checkpoints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label></labels><created>2015-12-11T13:15:24Z</created><updated>2015-12-15T14:26:15Z</updated><resolved>2015-12-15T14:26:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-14T16:22:49Z" id="164483423">This is a much simpler implementation than the first. I've minor comments on LocalCheckpointService and the associated tests but otherwise LGTM.
</comment><comment author="bleskes" created="2015-12-15T08:32:28Z" id="164683371">Thanks @jasontedor . I pushed another commit, addressing all your feedback
</comment><comment author="jasontedor" created="2015-12-15T13:06:43Z" id="164757709">I have one more small nit, but LGTM.
</comment><comment author="bleskes" created="2015-12-15T14:26:09Z" id="164779804">pushed to `feature/seq_no`. Thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document need to upgrade plugins during rolling/ full cluster restarts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15389</link><project id="" key="" /><description /><key id="121686421">15389</key><summary>Document need to upgrade plugins during rolling/ full cluster restarts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-12-11T11:03:52Z</created><updated>2016-01-15T12:38:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jakommo" created="2015-12-18T16:07:12Z" id="165817456">:+1: 
</comment><comment author="tylerfontaine" created="2015-12-18T16:08:56Z" id="165818506">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the object notation for string fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15388</link><project id="" key="" /><description>When specifying a string field, you can either do:

```
{
  "foo": "bar"
}
```

or

```
{
  "foo": {
    "value": "bar",
    "boost": 42
  }
}
```

The latter option is not documented anymore but I think it used to. However I have some doubts that many users are using it, and we tend to not recommend index-time boosting anyway so maybe it should go away?
</description><key id="121684718">15388</key><summary>Remove the object notation for string fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>discuss</label></labels><created>2015-12-11T10:52:56Z</created><updated>2015-12-30T08:58:01Z</updated><resolved>2015-12-30T08:58:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T12:57:18Z" id="163931006">+1
</comment><comment author="jimczi" created="2015-12-11T13:29:25Z" id="163936346">The object definition with boost and value is not working with multi fields (see #4320) so +1 on this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation about translog fsync is a bit confusing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15387</link><project id="" key="" /><description>From https://www.elastic.co/guide/en/elasticsearch/reference/2.x/index-modules-translog.html#_translog_settings

```
By default, Elasticsearch fsyncs and commits the translog every 5 seconds and at the end of every index, delete, update, or bulk request. 
```

I think the interval-based fsync is only useful in case you disable the per-write fsync, so it's a bit confusing to say that elasticsearch does both as it implies that the interval-based fsync is still useful when you already fsync after every write operation?
</description><key id="121678430">15387</key><summary>Documentation about translog fsync is a bit confusing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>docs</label></labels><created>2015-12-11T10:12:45Z</created><updated>2016-04-29T16:11:59Z</updated><resolved>2016-04-29T16:11:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-11T11:23:53Z" id="163914579">Can we just remove the "every 5 seconds and"?  Also, maybe remove the "fsync" ... this is an impl detail?  So just say:

```
By default, Elasticsearch commits the translog after every index, delete, update or bulk request.
```
</comment><comment author="s1monw" created="2015-12-11T13:54:06Z" id="163941234">++ to what mike said @jpountz do you just wanna make that change and push it?
</comment><comment author="jpountz" created="2015-12-11T13:55:00Z" id="163941474">+1
</comment><comment author="nik9000" created="2015-12-11T18:25:51Z" id="164009752">&gt; ++ to what mike said @jpountz do you just wanna make that change and push it?

+1
</comment><comment author="clintongormley" created="2016-04-29T16:11:59Z" id="215784784">Fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Terms Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15386</link><project id="" key="" /><description /><key id="121676786">15386</key><summary>Aggregations Refactor: Refactor Terms Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-11T10:05:46Z</created><updated>2015-12-18T14:12:32Z</updated><resolved>2015-12-18T14:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-15T09:45:56Z" id="164704601">@jpountz thanks for the review, I added a commit to address your comments
</comment><comment author="jpountz" created="2015-12-18T09:35:54Z" id="165727119">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix copy_to when the target is a dynamic object field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15385</link><project id="" key="" /><description>Backport from master: Fix copy_to when the target is a dynamic object field.
Closes #11237
</description><key id="121673570">15385</key><summary>Fix copy_to when the target is a dynamic object field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label></labels><created>2015-12-11T09:48:17Z</created><updated>2015-12-11T13:13:39Z</updated><resolved>2015-12-11T10:03:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-11T09:59:44Z" id="163894921">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error on ElasticSearch 1.0.3 EndWithValue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15384</link><project id="" key="" /><description>This has been driving me crazy for 2 weeks now. I have an elastic search on my dev machine that is working great but when I shove it up to LIVE I get this error whenever I sort by relevancy i.e. use script_score:

{
"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[Gy8DKMvoQKO5rousUfhQsw][modernrugs_3][1]: QueryPhaseExecutionException[[modernrugs_3][1]: query[function score (filtered(ConstantScore(_:_))-&gt;+cache(_type:models.Product) ++cache(all_sizes.lt:[85 TO 100]) ++cache(all_sizes.price:[0.0 TO 1000000.0]) +cache(BooleanFilter(relevancy_scores.162:[\* TO _])) +cache(BooleanFilter(relevancy_scores.176:[_ TO _])),function=script[if (!relevancyIds.isEmpty()) {cnt=0;x = relevancyIds.split(',');if (x.length&gt;0) {for (i=0;i&lt;x.length;i++) {p=x[i];if (!doc['relevancy_scores.'+p].isEmpty()) {relsProd=doc['relevancy_scores.'+p];if ((relsProd != null) &amp;&amp; (!relsProd.empty&amp;&amp;relsProd.values.length&gt;0)) {for (y=0;y&lt;relsProd.values.length;y++) {cnt+=(int)relsProd.values[y];}}}}return cnt;} else {return _score}} else {return _score};], params [{_PAYLOADS=4, _FREQUENCIES=8, _source=org.elasticsearch.search.lookup.SourceLookup@5c7519d9, _fields=org.elasticsearch.search.lookup.FieldsLookup@bf424fb, _doc=org.elasticsearch.search.lookup.DocLookup@49a7e0de, _index=org.elasticsearch.search.lookup.IndexLookup@2c9c3499, doc=org.elasticsearch.search.lookup.DocLookup@49a7e0de, _CACHE=32, _score=1.0, _OFFSETS=2, relevancyIds=162, _POSITIONS=16}])],from[0],size[40]: Query Failed [Failed to execute main query]]; nested: EndWithValue; }{[Gy8DKMvoQKO5rousUfhQsw][modernrugs_3][0]: QueryPhaseExecutionException[[modernrugs_3][0]: query[function score (filtered(ConstantScore(_:_))-&gt;+cache(_type:models.Product) ++cache(all_sizes.lt:[85 TO 100]) ++cache(all_sizes.price:[0.0 TO 1000000.0]) +cache(BooleanFilter(relevancy_scores.162:[_ TO _])) +cache(BooleanFilter(relevancy_scores.176:[_ TO *])),function=script[if (!relevancyIds.isEmpty()) {cnt=0;x = relevancyIds.split(',');if (x.length&gt;0) {for (i=0;i&lt;x.length;i++) {p=x[i];if (!doc['relevancy_scores.'+p].isEmpty()) {relsProd=doc['relevancy_scores.'+p];if ((relsProd != null) &amp;&amp; (!relsProd.empty&amp;&amp;relsProd.values.length&gt;0)) {for (y=0;y&lt;relsProd.values.length;y++) {cnt+=(int)relsProd.values[y];}}}}return cnt;} else {return _score}} else {return _score};], params [{_PAYLOADS=4, _FREQUENCIES=8, _source=org.elasticsearch.search.lookup.SourceLookup@12baae0f, _fields=org.elasticsearch.search.lookup.FieldsLookup@1df96464, _doc=org.elasticsearch.search.lookup.DocLookup@602df09b, _index=org.elasticsearch.search.lookup.IndexLookup@680f8171, doc=org.elasticsearch.search.lookup.DocLookup@602df09b, _CACHE=32, _score=1.0, _OFFSETS=2, relevancyIds=162, _POSITIONS=16}])],from[0],size[40]: Query Failed [Failed to execute main query]]; nested: EndWithValue; }]",
"status": 500
}

My query is as follows:

{
  "from": 0,
  "size": 40,
  "query": {
    "function_score": {
      "query": {
        "filtered": {
          "query": {
            "bool": {
              "should": {
                "match_all": {}
              }
            }
          },
          "filter": {
            "and": {
              "filters": [
                {
                  "or": {
                    "filters": [
                      {
                        "type": {
                          "value": "models.Product"
                        }
                      }
                    ]
                  }
                },
                {
                  "and": {
                    "filters": [
                      {
                        "range": {
                          "all_sizes.lt": {
                            "from": 85,
                            "to": 100,
                            "include_lower": true,
                            "include_upper": true
                          }
                        }
                      }
                    ]
                  }
                },
                {
                  "and": {
                    "filters": [
                      {
                        "range": {
                          "all_sizes.price": {
                            "from": 0,
                            "to": 1000000,
                            "include_lower": true,
                            "include_upper": true
                          }
                        }
                      }
                    ]
                  }
                },
                {
                  "or": {
                    "filters": []
                  }
                },
                {
                  "or": {
                    "filters": [
                      {
                        "or": {
                          "filters": [
                            {
                              "exists": {
                                "field": "relevancy_scores.162"
                              }
                            }
                          ]
                        }
                      }
                    ]
                  }
                },
                {
                  "or": {
                    "filters": [
                      {
                        "or": {
                          "filters": [
                            {
                              "exists": {
                                "field": "relevancy_scores.176"
                              }
                            }
                          ]
                        }
                      }
                    ]
                  }
                },
                {
                  "or": {
                    "filters": []
                  }
                },
                {
                  "or": {
                    "filters": []
                  }
                },
                {
                  "or": {
                    "filters": []
                  }
                },
                {
                  "or": {
                    "filters": []
                  }
                }
              ]
            }
          }
        }
      },
      "script_score": {
        "script": "if (!relevancyIds.isEmpty()) {cnt=0;x = relevancyIds.split(',');if (x.length&gt;0) {for (i=0;i&lt;x.length;i++) {p=x[i];if (!doc['relevancy_scores.'+p].isEmpty()) {relsProd=doc['relevancy_scores.'+p];if ((relsProd != null) &amp;&amp; (!relsProd.empty&amp;&amp;relsProd.values.length&gt;0)) {for (y=0;y&lt;relsProd.values.length;y++) {cnt+=(int)relsProd.values[y];}}}}return cnt;} else {return _score}} else {return _score};",
        "params": {
          "relevancyIds": "162"
        }
      },
      "boost_mode": "replace"
    }
  },
  "fields": "_id",
  "sort": [
    {
      "_score": {
        "order": "desc"
      }
    }
  ],
  "facets": {
    "modelType": {
      "terms": {
        "field": "_type",
        "size": 10
      }
    },
    "relevancyId": {
      "terms": {
        "field": "all_relevancies.id",
        "size": 10
      }
    }
  }
}

Like I say it works fine on DEV but not on LIVE with the same code version. At the moment I am thinking its down to the actual data having something missing but I did a compare (using es mashup) and the object are identical :( :( :(

I am quickly losing hair so if anyone can help that would be awesome
</description><key id="121670736">15384</key><summary>Error on ElasticSearch 1.0.3 EndWithValue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">elliswood</reporter><labels /><created>2015-12-11T09:32:43Z</created><updated>2015-12-11T12:52:01Z</updated><resolved>2015-12-11T12:48:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T12:48:53Z" id="163929802">Hi @elliswood 

This appears to be coming from MVEL https://github.com/mvel/mvel/blob/master/src/main/java/org/mvel2/compiler/EndWithValue.java

MVEL has had problems with recent JVMs and we no longer support it.  I can't offer much more advice than: it's time to upgrade... 
</comment><comment author="elliswood" created="2015-12-11T12:52:01Z" id="163930296">Cheers ClintonGormley..good to know but we are kinda stuck with what we got :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change docs on "node client" to not use an in-memory node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15383</link><project id="" key="" /><description>Currently we suggesting users create a Node (using NodeBuilder in 2.x) to have a client that is capable of keeping up-to-date information. This is generally a bad idea as it means elasticsearch has no control over eg max heap size or gc settings, and is also problematic for users because they must deal with dependency collisions (and in 2.x+ dependencies of elasticsearch itself).

A better alternative, and what we should document, is to run a local elasticsearch server using bin/elasticsearch, and then use the transport client to connect to that local node. This local connection is virtually free, and allows the client code to be completely isolated from the elasticsearch process. Plugins are then also easy to deal with: just install them in elasticsearch as usual.
</description><key id="121628117">15383</key><summary>Change docs on "node client" to not use an in-memory node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>docs</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-11T03:18:56Z</created><updated>2016-02-29T15:17:19Z</updated><resolved>2016-02-29T15:17:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-02-12T11:13:07Z" id="183280701">+1
</comment><comment author="clintongormley" created="2016-02-14T19:02:18Z" id="183951370">The node docs have been significantly rewritten. https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html Closing.
</comment><comment author="clintongormley" created="2016-02-14T19:03:26Z" id="183952102">Actually, the Java docs need updating: https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/node-client.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix module/integ-test cluster shutdown task ordering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15382</link><project id="" key="" /><description>Both modules and integ-test-zip have integration tests (the latter being
the base rest tests). We can currently get odd behavior where
integ-test-zip's integ test does not shutdown its cluster before running
mdoule integ tests (and it then tries to shutdown all those clusters at
once after modules integ tests have run).

The underlying issue can be attributed to a bug in gradle with how cross project
mustRunAfter work with finalizers. This change works around this bug by
setting up mustRunAfter on the shutdown task itself.
</description><key id="121608195">15382</key><summary>Fix module/integ-test cluster shutdown task ordering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-11T00:22:28Z</created><updated>2015-12-11T01:20:13Z</updated><resolved>2015-12-11T01:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-11T01:18:39Z" id="163802267">looks ok
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Config `index.mapper.dynamic: false` is not honored.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15381</link><project id="" key="" /><description>Even with `index.mapper.dynamic: false` in the config file, automatic mapping is being created.
[Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html#index-creation) states that:

&gt; Automatic mapping creation can be disabled by setting index.mapper.dynamic to false in the config files
# steps to reproduce

step 1.  Make sure setting exists:

```
cat config/elasticsearch.yml | grep mapper
index.mapper.dynamic: false
```

step 2.  Make sure no mapping or template exists:

```
curl http://localhost:9200/_mapping
{}
curl http://localhost:9200/_template
{}
```

step 3. Create an index:

```
curl -XPUT http://localhost:9200/myindex/mytype/1 -d '{
   "myfield": 1
}'
{"_index":"myindex","_type":"mytype","_id":"1","_version":1,"_shards":{"total":1,"successful":1,"failed":0},"created":true}
```

step 4. Notice, that a mapping was automatically created:

```
curl http://localhost:9200/_mapping
{"myindex":{"mappings":{"mytype":{"properties":{"myfield":{"type":"long"}}}}}}
```
# expected

No mapping should be automatically created at step 4.
# other info

```
curl http://localhost:9200
{
  "name" : "master",
  "cluster_name" : "traveltime",
  "version" : {
    "number" : "2.1.0",
    "build_hash" : "72cd1f1a3eee09505e036106146dc1949dc5dc87",
    "build_timestamp" : "2015-11-18T22:40:03Z",
    "build_snapshot" : false,
    "lucene_version" : "5.3.1"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="121583587">15381</key><summary>Config `index.mapper.dynamic: false` is not honored.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zamblauskas</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-12-10T21:41:38Z</created><updated>2015-12-30T21:23:48Z</updated><resolved>2015-12-30T21:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T09:35:42Z" id="163886896">It appears that this parameter is ignored during the index creation process, eg:

```
PUT t 
{
  "settings": {
    "index.mapper.dynamic": false
  }
}
```

This throws an exception correctly:

```
PUT t/x/1
{}
```

But start a node with `--index.mapper.dynamic false`, then:

```
DELETE t
```

This creates the `t` index with the `x` type:

```
PUT t/x/1
{}    
```

And this then fails with the correct exception:

```
PUT t/y/2
{}
```
</comment><comment author="davidvgalbraith" created="2015-12-14T17:35:47Z" id="164504314">I took a stab at this one: https://github.com/elastic/elasticsearch/pull/15424. Had a little trouble with the test setup so could use some input.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 1.7.2 nodes periodically stop communicating among themselves</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15380</link><project id="" key="" /><description>I've also seen this happen w/ an ES 1.7.3 system we have.  I have two different production clusters running two nodes each.  The nodes will both continue to respond to client requests but seem to lose communication w/ each other and never attempt to re-connect.  Restarting one of the nodes will reestablish communication but, so far, at some point I know they will lose communication again.  We are running our clusters in Azure on Windows Server 2012 VMs.  We have been running 1.7.2 successfully for several weeks now and just started experiencing this issue a couple of days ago.

In the logs, there really appears to be no activity at all on the system and then all of a sudden, the master node, ES1, we see:
[2015-12-07 16:52:30,396][DEBUG][action.admin.cluster.node.stats] [ES1] failed to execute on node [GO31X74ET_aFfQUQHqakhw]
org.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][cluster:monitor/nodes/stats[n]] disconnected
[2015-12-07 16:52:30,398][DEBUG][action.search.type       ] [ES1] [ix1_v11][3], node[GO31X74ET_aFfQUQHqakhw], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6a57a770] lastShard [true]
org.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][indices:data/read/search[phase/query]] disconnected
[2015-12-07 16:52:30,542][DEBUG][action.admin.indices.stats] [ES1] [ix2_v2][10], node[GO31X74ET_aFfQUQHqakhw], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5a10f0df]
org.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][indices:monitor/stats[s]] disconnected
[2015-12-07 16:52:30,532][DEBUG][action.admin.indices.stats] [ES1] [ix3_v1][11], node[GO31X74ET_aFfQUQHqakhw], [R], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@5a10f0df]
org.elasticsearch.transport.NodeDisconnectedException: [ES2][inet[/10.0.0.5:9300]][indices:monitor/stats[s]] disconnected

On the non-master node, ES2, we see:
[2015-12-07 16:52:30,505][INFO ][discovery.zen            ] [ES2] master_left [[ES1][j5IETSp1S1OxOiqByeuaVA][es1][inet[/10.0.0.4:9300]]], reason [transport disconnected]
[2015-12-07 16:52:30,536][WARN ][discovery.zen            ] [ES2] master left (reason = transport disconnected), current nodes: {[ES2][GO31X74ET_aFfQUQHqakhw][es2][inet[/10.0.0.5:9300]],}
[2015-12-07 16:52:30,583][INFO ][cluster.service          ] [ES2] removed {[ES1][j5IETSp1S1OxOiqByeuaVA][es1][inet[/10.0.0.4:9300]],}, reason: zen-disco-master_failed ([ES1][j5IETSp1S1OxOiqByeuaVA][es1][inet[/10.0.0.4:9300]])
[2015-12-07 16:52:30,614][DEBUG][action.bulk              ] [ES2] observer timed out. notifying listener. timeout setting [1s], time since start [8.3s]
[2015-12-07 16:52:35,339][INFO ][cluster.service          ] [ES2] new_master [ES2][GO31X74ET_aFfQUQHqakhw][es2][inet[/10.0.0.5:9300]], reason: zen-disco-join (elected_as_master)
[2015-12-07 16:52:35,355][INFO ][cluster.routing          ] [ES2] delaying allocation for [161] unassigned shards, next check in [59.8s]

After this, as I said, the sync process will not recover unless we restart ES2.  Note that, so far, we haven't had to restart ES1, just restarting ES2 seems to do the trick.  I have suspected that we've reached some memory threshold or something but we haven't been adding large amounts of data to our system either.  Our VMs are running w/ 7GB machines of which 3.5GB we have configured to be available to the JVM (as recommended) to leave the other half for the Lucene file system cache.  The Task Manager will show the ES process as using anywhere from 1.2-1.4 GB (ie. not the 3.5GB we've configured to allow for it) and typically runs at ~98% memory usage.  I've always assumed the lion share of the rest was used by Lucene and despite running at that high percentage of memory usage, the system has performed just fine for many weeks now (probably ~8 weeks or so) so something has emerged to change that somehow.  Is this something that others have experienced?

Thanks,
Tom
</description><key id="121577785">15380</key><summary>ElasticSearch 1.7.2 nodes periodically stop communicating among themselves</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tdoman</reporter><labels /><created>2015-12-10T21:08:35Z</created><updated>2015-12-11T18:03:28Z</updated><resolved>2015-12-11T09:22:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T09:22:45Z" id="163884085">Disconnects happen for many reasons, eg network, slow garbage collection, etc.  The problem is that you are running Elasticsearch with only two nodes, and without `minimum_master_nodes` set to a quorum of nodes in the cluster (which in your case would be two).  So when the nodes are unable to communicate with each other, each node thinks the other has disappeared and forms its own cluster.  These nodes will never reconnect.

You should have a minimum of 3 nodes in the cluster, and have `minimum_master_nodes` set to 2.  That way, if a single node disconnects then it will not form its own cluster but will try to rejoin the other two nodes.  If all 3 nodes disconnect from each other, your cluster will be non-functional until they reconnect.
</comment><comment author="tdoman" created="2015-12-11T18:02:52Z" id="164004953">Great, thank you very much for the prompt response Clinton, we'll make those changes.  We've gotten away w/ 2 nodes for well over a year now (impressive no?) and since ES performed so well for us, always worked like a charm, and more nodes cost $$$, we never added the recommended 3rd node for performance or any other needs.  Looks like we have grown into that place where that 3rd node is not only recommended but necessary.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo: Fix small error in distance normalization in test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15379</link><project id="" key="" /><description>A CI test failure at http://build-us-00.elastic.co/job/es_core_master_suse/2731/ caught my attention and I did some digging. The test failure is:

```
java.lang.AssertionError: 
Expected: a numeric value within &lt;1.0E-6&gt; of &lt;1055.6179277598308&gt;
     but: &lt;849.7492361579754&gt; differed by &lt;205.8686906018554&gt;
    at __randomizedtesting.SeedInfo.seed([C512CE5226BF19E6:32E9CC6C573CDC0C]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.index.query.GeoDistanceQueryBuilderTests.assertGeoPointQuery(GeoDistanceQueryBuilderTests.java:206)
```

The query causing it is 

```
  "geo_distance" : {
    "mapped_geo_point" : [ -35.05150139609236, -89.99617044659995 ],
    "distance" : 1055.6179277598308,
    "distance_type" : "sloppy_arc",
    "optimize_bbox" : "memory",
    "validation_method" : "STRICT",
    "boost" : 0.10526316,
    "_name" : "pw8"
  }
}
```

It looks like for this particular setting, the code generating the lucene query applies some additional normalization to the distance via `GeoUtils.maxRadialDistance(center, normDistance)` which was currently not reflected in the test code. Added this line to the test. @nknize could you check if this is okay or maybe this is a strange glitch in the code generating the lucene query that need further investigation?
</description><key id="121561733">15379</key><summary>Geo: Fix small error in distance normalization in test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T19:39:14Z</created><updated>2016-01-12T18:08:40Z</updated><resolved>2016-01-12T18:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-01-11T13:44:37Z" id="170552452">This test fails rarely, but it happened once again at http://build-us-00.elastic.co/job/es_core_master_strong/6002/.
Just rebased the fix for the test, @nknize could you take a look if adding this to the test is right for the GeoPointDistanceQuery after version 2.2.0?
</comment><comment author="nknize" created="2016-01-12T17:57:47Z" id="170994279">LGTM
</comment><comment author="cbuescher" created="2016-01-12T18:03:12Z" id="170996268">@nknize thanks, will backport also to all branches that contain this test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove RuntimePermission("accessDeclaredMembers")</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15378</link><project id="" key="" /><description>removing this adds roadblocks to dangerous reflection, strengthens system boundaries, etc.

all reflection code is fixed, all impacted methods are banned in forbidden apis. a couple plugins need the permission due to problems in third party code.

Upgrades lucene to 5.5.0-1719088, randomizedtesting to 2.3.2, and securemock to 1.2
</description><key id="121560649">15378</key><summary>Remove RuntimePermission("accessDeclaredMembers")</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T19:33:16Z</created><updated>2015-12-11T09:16:54Z</updated><resolved>2015-12-10T20:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-10T19:44:09Z" id="163729935">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic mappings fail when a single document generates inconsistent mapping updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15377</link><project id="" key="" /><description>For instance indexing the following document would fail if the `foo.bar` field is not mapped explicitly:

```
{
  "foo": [
    {
      "bar": "baz"
    },
    {
      "bar": 42
    }
  ]
}
```

The reason is that mapping updates for the 2 sub documents are generated independently (one triggers the creation of a string field and the other one of a long field) but they can't be reconciliated in order to generate the mapping update that will be sent to the master node.

Bug report courtesy of  Benjamin Gathmann at https://discuss.elastic.co/t/coerce-long-to-double-not-working/36882.
</description><key id="121550393">15377</key><summary>Dynamic mappings fail when a single document generates inconsistent mapping updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label></labels><created>2015-12-10T18:38:27Z</created><updated>2016-04-05T23:42:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="davidvgalbraith" created="2015-12-21T08:59:18Z" id="166240587">Right! So what's the intended behavior?
</comment><comment author="jpountz" created="2015-12-21T09:17:59Z" id="166243452">@davidvgalbraith Good question, I think I'm leaning towards documenting this as a limitation of dynamic mappings. I think it's fair that dynamic mappings don't work if a single document does not use consistent json types for fields that have the same path?

We could potentially avoid conflicts by using the most generic type (ie. string &gt; double, double &gt; long, etc.) but I'm afraid this would hide issues more than it would solve problems. I'd rather rely on something more explicit like dynamic templates.
</comment><comment author="abulhol" created="2015-12-21T10:18:09Z" id="166260932">Thank you for mentioning dynamic templates. I will try this out. The point is that like in my case, you simply have to deal with the data you have, and throwing this data into Elasticsearch is not quite as easy as I expected. 
</comment><comment author="clintongormley" created="2016-03-24T17:13:44Z" id="200932558">Here's another example where the fact that it doesn't work is more unexpected:

```
PUT /speed/record/1
{
  "speed_rec": [
    {
      "speed": 61.23
    },
    {
      "speed": 61
    }
  ]
}
```
</comment><comment author="jpountz" created="2016-03-24T18:20:45Z" id="200958674">True, but when not all numbers have a dot, you are going to have problems anyway. For instance if the first document only contains integers, the field will be mapped as a long and the decimal part of documents that come afterwards will be truncated. (Related to #16018)
</comment><comment author="cssccarvalho" created="2016-04-05T11:12:37Z" id="205758632">Hello. I would like to add that if someone wants to use the global coerce setting to disable this kind of validation. It doesn't work. I have tried with the latest docker elasticsearch official image.
</comment><comment author="ibrahima" created="2016-04-05T23:42:32Z" id="206037732">I recognize that this is not necessarily a "solvable" problem for the int vs float case, but isn't it reasonable to just index all other fields anyway and make a note of the error? The current behavior I'm experiencing is the same as described by @clintongormley , and it results in that log message just being dropped from the index. Is there some way I can at least let this message be parsed and indexed and just keep those conflicted fields out of the index?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use HighlightBuilder in SearchSourceBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15376</link><project id="" key="" /><description>After HighlightBuilder implements Writable now, we can remove the temporary solution for transporting the highlight section in SearchSourceBuilder from the coordinating node to the shard as BytesReference and use HighlightBuilder instead.

Relates to #15044 
</description><key id="121545402">15376</key><summary>Use HighlightBuilder in SearchSourceBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T18:09:22Z</created><updated>2015-12-14T19:35:07Z</updated><resolved>2015-12-14T19:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-14T13:36:46Z" id="164440252">@cbuescher I left one minor comment but LGTM
</comment><comment author="cbuescher" created="2015-12-14T16:05:16Z" id="164477028">@colings86 thanks, as we discussed I changed the internal representation of the order to an enum now instead of a bool and left the string setter for bwc. Let me know if this look good to you.
</comment><comment author="colings86" created="2015-12-14T16:15:15Z" id="164481016">@cbuescher I left a comment about adding a test for Order but otherwise it LGTM
</comment><comment author="cbuescher" created="2015-12-14T16:17:09Z" id="164481771">@colings86 thanks, will add the test you mentioned and merge afterwards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContentBuilder throws NumberFormatException for Date field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15375</link><project id="" key="" /><description>This index:

```
        IndexResponse response = client.prepareIndex('twitter', 'tweet', '1')
                .setSource(jsonBuilder()
                .startObject()
                .field('user', 'dan')
                .field('postDate', new Date())
                .field('message', 'rocking out')
                .endObject()
        ).get();
```

Throws a NumberFormatException on the date field

```
MapperParsingException[failed to parse [postDate]]; nested: NumberFormatException[For input string: "2015-12-10T16:18:26.876Z"];
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:339)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:314)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:441)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:267)
    at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:128)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:318)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:537)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:528)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.prepareIndexOperationOnPrimary(TransportReplicationAction.java:1049)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1060)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:170)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "2015-12-10T16:18:26.876Z"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:589)
    at java.lang.Long.parseLong(Long.java:631)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:145)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:276)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:213)
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:331)
    ... 17 more
```
</description><key id="121524478">15375</key><summary>XContentBuilder throws NumberFormatException for Date field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">drmaas</reporter><labels /><created>2015-12-10T16:25:28Z</created><updated>2015-12-14T22:40:44Z</updated><resolved>2015-12-11T12:50:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-11T09:50:35Z" id="163892756">Hi @drmaas,
it looks like the `postDate` field is already mapped to `long`, so it tries to parse the date as a long value. Is it possible that you indexed a document into the same index where the `postDate` field was interpreted as a numeric value by dynamic mapping? This might also have been a document with a different type, because fields with the same name in different document types in the same index map to the same Lucene field name internally (see this [blogpost](https://www.elastic.co/blog/great-mapping-refactoring#conflicting-mappings) for more details). 
Hope this solves your problem. 
</comment><comment author="clintongormley" created="2015-12-11T12:50:46Z" id="163930140">As @cbuescher says, this may well be caused by having different mappings on different shards (something you can't diagnose with the get mapping API, but which is prevented in 2.0).  Closed by #8870
</comment><comment author="drmaas" created="2015-12-14T22:40:44Z" id="164582943">Thanks for clarifying!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Space on begin of line in elasticsearch.yml keeps service from starting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15374</link><project id="" key="" /><description>Hi,

In Elasticsearch 2.1 on CentOS 7 I hit some weird behaviour that took me some time to figure out.

While testing I only have 2 lines of configuration in `elasticsearch.yml` (and the comments that are in it per default)

```
network.host: "_non_loopback:ipv4_"
discovery.zen.ping.unicast.hosts: ["192.168.56.101"]
```

When I add a Space to the second line the daemon won't start and doesn't write any log to `/var/log/elasticsearch`. 

```
network.host: "_non_loopback:ipv4_"
 discovery.zen.ping.unicast.hosts: ["192.168.56.101"]
```

Only in `/var/log/messages` there are some cryptic hints about errors in the syntax.

Since an extra space is something that might happen easily when un-commenting one of the examples, I'm quite sure this might hit some other users too. Could you please make Elasticsearch a little bit less picky or at least add a check that writes something helpful about this issue into the standard logfile?

Thanks in advance,
</description><key id="121523230">15374</key><summary>Space on begin of line in elasticsearch.yml keeps service from starting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">widhalmt</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-12-10T16:19:37Z</created><updated>2015-12-21T19:03:09Z</updated><resolved>2015-12-21T19:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T16:55:35Z" id="164493319">Elasticsearch is throwing an exception on STDERR when it discovers the bad YAML.   (which is correct behaviour).  We can't log the message to the log file, because we haven't yet parsed the settings to find out where the log file should be.

I don't think there is anything we can do here?
</comment><comment author="jasontedor" created="2015-12-21T19:03:09Z" id="166390913">&gt; Could you please make Elasticsearch a little bit less picky

We parse according to the YAML spec; since [indentation](http://www.yaml.org/spec/1.2/spec.html#id2777534) is important in YAML, there is no room for leniency here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tribe node: failed to send join request to master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15373</link><project id="" key="" /><description>I'm running 2 clusters and cannot get a tribe node to connect to them. Im running ES 2.1 on every ES nodes, including the tribe node.

ClusterA has only one node and clusterB has only one node

&gt; ClusterA:
&gt; cluster.name: clusterA 
&gt; node.name: nodeA
&gt; node.master: true
&gt; node.data: true
&gt; network.host: 0.0.0.0
&gt; http.port: 9200
&gt; transport.tcp.port: 9300
&gt; discovery.zen.ping.multicast.enabled: false
&gt; discovery.zen.ping.unicast.hosts: ["dns-name-for-clusterA:9300"]

Configuration for tribe node (for simplicity sake I only trying here to connect to clusterA):

&gt; cluster.name: elasticsearch-tribe
&gt; node.name: tribe-node
&gt; tribe.t1.path.conf: /etc/elasticsearch/tribe.yml  #14573 
&gt; tribe.t1.cluster.name: clusterA
&gt; tribe.t1.discovery.zen.multicast.enabled: false
&gt; tribe.t1.discovery.zen.ping.unicast.hosts: ["dns-name-for-clusterA:9300"]
&gt; network.host: 0.0.0.0
&gt; http.port: 9201
&gt; transport.tcp.port: 9301

When starting both tribe and cluster this is shown in the ES tribe log:

&gt; [2015-12-10 16:30:29,576][INFO ][discovery                ] [tribe-node/t1] clusterA/bcjwWrFiRw6ixzNxW9pLRA
&gt; [2015-12-10 16:30:48,723][INFO ][discovery.zen            ] [tribe-node/t1] failed to send join request to master [{....}{R-LGKQuaRaObh89PJbmPig}{ip-number}{ip-number:9300}{max_local_storage_nodes=1, master=true}], reason [RemoteTransportException[[dns-name][10.85.96.85:9300][internal:discovery/zen/join]]; nested: IllegalStateException[Node [{dns-name}{R-LGKQuaRaObh89PJbmPig}{ip-number}{ip-number:9300}{max_local_storage_nodes=1, master=true}] not master for join request]; ]
&gt; [2015-12-10 16:30:59,577][WARN ][discovery                ] [tribe-node/t1] waited for 30s and no initial state was set by the discovery
&gt; [2015-12-10 16:30:59,579][INFO ][node                     ] [tribe-node/t1] started
&gt; [2015-12-10 16:30:59,579][INFO ][node                     ] [tribe-node] started
&gt; [2015-12-10 16:32:25,784][INFO ][discovery.zen            ] [tribe-node/t1] failed to send join request to master [{ip-number}{R-LGKQuaRaObh89PJbmPig}{ip-number}{ip-number:9300}{max_local_storage_nodes=1, master=true}], reason [RemoteTransportException[[dns-name][ipnumber:9300][internal:discovery/zen/join]]; nested: IllegalStateException[Node [{dnsname}{R-LGKQuaRaObh89PJbmPig}{ipnumber}{ipnumber:9300}{max_local_storage_nodes=1, master=true}] not master for join request]; ]

And on clusterA:s log it adds the tribe node and removes it repeatedly:

&gt; [2015-12-10 16:32:28,807][INFO ][cluster.service          ] [dnsname] new_master {dnsname}{R-LGKQuaRaObh89PJbmPig}{ip-number}{ip-number:9300}{max_local_storage_nodes=1, master=true}, r
&gt; eason: zen-disco-join(elected_as_master, [0] joins received)
&gt; [2015-12-10 16:32:28,849][INFO ][cluster.service          ] [dnsname] added {{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false, client=true},}, reason: zen-disco-join(join from
&gt;  node[{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false, client=true}])
&gt; [2015-12-10 16:32:58,854][WARN ][discovery.zen.publish    ] [dnsname] timed out waiting for all nodes to process published state [14](timeout [30s], pending nodes: [{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA
&gt; }{127.0.0.1}{127.0.0.1:9300}{data=false, client=true}])
&gt; [2015-12-10 16:32:58,857][WARN ][cluster.service          ] [dnsname] cluster state update task [zen-disco-join(join from node[{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false
&gt; , client=true}])] took 30s above the warn threshold of 30s
&gt; [2015-12-10 16:32:58,860][INFO ][cluster.service          ] [dnsname] removed {{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false, client=true},}, reason: zen-disco-node_failed(
&gt; {tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false, client=true}), reason failed to ping, tried [3] times, each with maximum [30s] timeout

What am I doing wrong here? I've tried several different configuration changes but cannot get it to work. Also I have had no problem with the exact same setup wiith ES 1.4.4
</description><key id="121516297">15373</key><summary>tribe node: failed to send join request to master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rogerwelin</reporter><labels><label>:Tribe Node</label><label>feedback_needed</label></labels><created>2015-12-10T15:49:45Z</created><updated>2016-05-10T18:34:30Z</updated><resolved>2016-02-14T19:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-13T15:02:08Z" id="164266850">Looking at the following line in the log of cluster A

```
[2015-12-10 16:32:28,849][INFO ][cluster.service ] [dnsname] added {{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false, client=true},}, reason: zen-disco-join(join from
node[{tribe-node/t1}{bcjwWrFiRw6ixzNxW9pLRA}{127.0.0.1}{127.0.0.1:9300}{data=false, client=true}])
```

it seems that `tribe-node/t1` has as [publish address](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html) `127.0.0.1:9300`. Cluster A cannot find the tribe node under that address (as I assume it's on a different host).

As of ES 2.0, Elasticsearch will only bind to localhost per default (see https://www.elastic.co/blog/elasticsearch-unplugged). The solution is to define `network.host` for the tribe node (`tribe.t1.network.host`). It does not inherit this setting from the top-level `network.host` setting.
</comment><comment author="clintongormley" created="2015-12-14T14:27:08Z" id="164450786">`network.host: 0.0.0.0` causes it to bind to localhost.
</comment><comment author="clintongormley" created="2015-12-14T14:34:57Z" id="164452512">Whoops - that is incorrect.  0.0.0.0 should bind to available interfaces I think.  Hopefully this will be fixed by https://github.com/elastic/elasticsearch/pull/15300

Could you let us know if it does once 2.1.1 is out?
</comment><comment author="Zenexer" created="2015-12-20T10:20:04Z" id="166105112">Change to:

```
network.bind_host: 0.0.0.0
network.publish_host: A.SPECIFIC.IP.ADDRESS
```

Working, but unintuitive, and it's a major change from 1.x that isn't well-publicized.
</comment><comment author="thn-dev" created="2015-12-31T13:31:22Z" id="168193882">WRT ES v2.1.1, I have to do the following to get the tribe node talking to two different clusters: cluster A and cluster B

**# tribe node's configuration (elasticsearch.yml)**
**network.host:** 0.0.0.0
**transport.tcp.port:** 9300
**http.port:** 9200
**http.enabled:** true

tribe.**t1**.cluster.name: **&lt;cluster A&gt;**
tribe.**t1**.discovery.zen.ping.unicast.hosts: **&lt;cluster A's master node&gt;**
tribe.**t1**.discovery.zen.ping.multicast.enabled: false
tribe.**t1**.path.conf: **&lt;valid path/to/conf&gt;**
tribe.**t1**.path.plugins: **&lt;valid path/to/plugin&gt;**
tribe.**t1**.network.bind_host: **0.0.0.0**
tribe.**t1**.network.publish_host: **&lt;tribe node's IP&gt;**
tribe.**t1**.transport.tcp.port: **&lt;optional but different from tribe node port above&gt;**

_repeat the same block but replace "t1" to "t2" for cluster B and fill in proper info related to cluster B but keep the tribe.t2.network.\* the same with different tribe.t2.transport.tcp.port value from t1 if specified_
</comment><comment author="clintongormley" created="2016-02-14T19:00:22Z" id="183950300">The networking docs have been greatly improved. i don't think there is any more to do here, so I'll close
</comment><comment author="chinmoydas1" created="2016-05-06T17:03:46Z" id="217499857">@Tri Nguyen
The post is helpful, but I am not able to fix the issue, even with the suggested configuration. Qns are as below:
1. tribe.t1.discovery.zen.ping.unicast.hosts: &lt;cluster A's master node&gt; -- Is it cluster A's master node's host:port or just the node name?
1. Are the following two lines mandatory in the tribes elasticsearch.yml:
   tribe.t1.path.conf: &lt;valid path/to/conf&gt;
   tribe.t1.path.plugins: &lt;valid path/to/plugin&gt; 
2. tribe.t1.transport.tcp.port: &lt;optional but different from tribe node port above&gt; -- transport.tcp.port was mentioned above as 9300, what else can be mentioned for tribe.t1.transport.tcp.port?
</comment><comment author="thn-dev" created="2016-05-09T15:11:54Z" id="217893185">regarding ***t1.*.unicast.hosts**, if you only provide cluster A's master node's host name or IP address, it will try to connect using the default port number. I would specify both here to make sure you have the correct settings.

regarding **_.path._ parameters**, I did not have to do it when I was using v1.7.3 but had to explicitly set it in v2.1.1 (even though I was told that I don't need to set them) I have not tested v2.3.x so I can't tell much here but I expect they fix that bug since it was reported as a bug at some point (if I remembered correctly)

regarding ***.t1.transport.tcp.port**, you can ignore this one
</comment><comment author="chinmoydas1" created="2016-05-10T17:35:54Z" id="218232600">@Tri Nguyen:
tribe.t1.path.conf: I hope this is the config folder, as I could not find any folder named conf in elasticsearch 2.1.1.
</comment><comment author="thn-dev" created="2016-05-10T18:33:34Z" id="218249028">yes it is... if you install ES using .rpm file, by default, it's in /etc/elasticsearch

Here is the link for ES 2.1 that you should be using
https://www.elastic.co/guide/en/elasticsearch/reference/2.1/setup-dir-layout.html

Anyway, this ticket has been closed, I suggest you use the "discussion"
board from ES... the link below from the discussion is somewhat related to
what you are looking for

https://discuss.elastic.co/t/tribe-node-connect-to-specific-ips/45721/10
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove recovery threadpools and throttle outgoing recoveries on the master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15372</link><project id="" key="" /><description>Today we throttle recoveries only for incoming recoveries. Nodes that have a lot
of primaries can get overloaded due to too many recoveries. To still keep that at bay
we limit the number of threads that are sending files to the target to overcome this problem.

The right solution here is to also throttle the outgoing recoveries that are today unbounded on
the master and don't start the recovery until we have enough resources on both source and target nodes.

The concurrency aspects of the recovery source also added a lot of complexity and additional threadpools that are hard to configure. This commit removes the concurrent streams notion completely and sends files in the thread that drives the recovery simplifying the recovery code considerably.
Outgoing recoveries are not throttled on the master via a allocation decider.

Note: this PR is still a bit rough but all tests pass and it contains a lot of improvements. I just wanted to get a pair of eyes on it for initial feedback.
</description><key id="121511683">15372</key><summary>Remove recovery threadpools and throttle outgoing recoveries on the master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T15:31:01Z</created><updated>2016-01-10T18:38:27Z</updated><resolved>2015-12-28T14:43:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-10T16:17:15Z" id="163674683">I really like how this cleans up the RecoverySourceHandler!

One thing I don't understand is how many times the `sendFiles(...)` method will actually be called concurrently? I'm not familiar enough with the code to know exactly where it is called. Is it going to be called sequentially for every recovery operation now, with the number of recovery operations limited by the new throttling mechanism?
</comment><comment author="nik9000" created="2015-12-10T16:22:23Z" id="163676091">&gt; Nodes that have a lot
&gt; of primaries can get overloaded due to too many recoveries.

I've certainly noticed this in the past.
</comment><comment author="s1monw" created="2015-12-11T09:32:10Z" id="163886154">&gt; One thing I don't understand is how many times the sendFiles(...) method will actually be called concurrently? I'm not familiar enough with the code to know exactly where it is called. Is it going to be called sequentially for every recovery operation now, with the number of recovery operations limited by the new throttling mechanism?

Yeah so previously we had one thread _driving_ the recovery on the source that passed all file sending task to the threadpools and then blocked until all files where transferred. Now we don't have the threadpools and just use that _driver_ thread to send all the files sequentially. We only use throttling to limit the rate of how fast we are sending. Now with the changes on the allocation end we also limit how many senders we have per node to not overload the node. But essentially we use a single thread for this IO bound problem instead of multiple.

&gt; I've certainly noticed this in the past.

thats a nice side-effect of this simplification
</comment><comment author="s1monw" created="2015-12-22T14:00:33Z" id="166624286">@bleskes @dakrone I rebased this to current master - reviews would be appreciated
</comment><comment author="dakrone" created="2015-12-22T16:54:05Z" id="166672224">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resolves the conflict between alias routing and parent routing by applying the alias routing and ignoring the parent routing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15371</link><project id="" key="" /><description>Separates routing and parent in all documentrequest in order to be able to distinguish an explicit routing value from a parent routing.

The final value for the routing is returned by MetaData.resolveIndexRouting which
resolves conflicts between routing, parent routing and alias routing with the following rules:
- If the routing is specified in the request then parent routing and alias routing are ignored.
- If the routing is not specified:
  - The parent routing is ignored if there is an alias routing that matches the request.
  - Otherwise the parent routing is applied.
    Fixes #3068
</description><key id="121506582">15371</key><summary>Resolves the conflict between alias routing and parent routing by applying the alias routing and ignoring the parent routing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T15:07:24Z</created><updated>2016-04-05T16:20:46Z</updated><resolved>2015-12-21T08:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2015-12-11T12:03:32Z" id="163922822">I reverted my commits and kept only the alias/parent routing conflict resolution. If they do not match then the alias routing is always chosen (and it does it silently ;) ).
</comment><comment author="rjernst" created="2015-12-18T07:57:57Z" id="165707438">LGTM, thanks for the tests!
</comment><comment author="Analect" created="2016-04-05T15:18:52Z" id="205854569">@jimferenczi 
Am I right in understanding that this fix will only be part of v5-alpha ... as and when that's coming? Thanks.
</comment><comment author="jimczi" created="2016-04-05T16:16:17Z" id="205878349">@Analect, starting with version 5 yes. 
v5.0.0-alpha1is released: https://www.elastic.co/blog/elasticsearch-5-0-0-alpha1-released. 
Keep in mind that it's an alpha so not ready for production.
</comment><comment author="Analect" created="2016-04-05T16:20:46Z" id="205880770">Great. Thanks @jimferenczi 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More like this query doesn't work with document id, works with text match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15370</link><project id="" key="" /><description>``` javascript
{
    "query": {
        "more_like_this": {
            "fields": ["article_title", "article_contents"],
            "like" : 
                {
                    "_index" : "alias_default",
                    "_type" : "articles",
                    "_id" : "1000000000001000000000000000001"
                },

            "min_doc_freq": 0,
            "min_term_freq" : 0,
            "minimum_should_match":0,
            "include":true
        }
    },
    "from": 0,
    "size": 6
}
```

This doesn't work

where as if i provide textual like it works
"like" : "Once upon a time",

on ES 2.0

mapping created as below

``` javascript
articles" : {
        "properties" : {
          "article_contents" : {
            "type" : "string",
            "term_vector" : "yes"
          },
          "article_title" : {
            "type" : "string",
            "term_vector" : "yes"
          },
```
</description><key id="121504589">15370</key><summary>More like this query doesn't work with document id, works with text match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aneesingh</reporter><labels><label>:More Like This</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-12-10T14:57:27Z</created><updated>2016-03-16T19:39:14Z</updated><resolved>2016-03-04T15:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mstockerl" created="2016-03-03T18:52:46Z" id="191911396">As **index: alias_default** seems to use a alias in the like field and the items specified in like are added to the same **item list** as the ones from the docs field, I suppose this issue is a duplicate to [#14944](https://github.com/elastic/elasticsearch/issues/14944) 
</comment><comment author="clintongormley" created="2016-03-04T15:34:09Z" id="192325231">@mstockerl good spot, closing in favour of #14944
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>score and explanation for dfs queries don't match  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15369</link><project id="" key="" /><description>Tested 1.7 vs 2.1. The following example:

```
POST test/doc/1
{
  "text": "Score and explanation should match I think?"
}

POST test/doc/2
{
  "text": "Score and explanation did match in 1.7..."
}


GET test/_search?search_type=dfs_query_then_fetch&amp;explain
{
  "query": {
    "match": {
      "text": "score"
    }
  }
}
```

for 1.7.0 returns:

``````
{
   "took": 48,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0.22295055,
      "hits": [
         {
            "_shard": 2,
            "_node": "5IoTIXdmROKObfLRmqCQYQ",
            "_index": "test",
            "_type": "doc",
            "_id": "1",
            "_score": 0.22295055,
            "_source": {
               "text": "Score and explanation should match I think?"
            },
            "_explanation": {
               "value": 0.22295056,
               "description": "weight(text:score in 0) [PerFieldSimilarity], result of:",
               "details": [
                  {
                     "value": 0.22295056,
                     "description": "score(doc=0,freq=1.0), product of:",
                     "details": ```
         {
            "_shard": 2,
            "_node": "8AbQaexnTwKbvcL1Va4yfw",
            "_index": "test",
            "_type": "doc",
            "_id": "1",
            "_score": 0.22295055,
            "_source": {
               "text": "Score and explanation should match I think?"
            },
            "_explanation": {
               "value": 0.22295056,
               "description": "weight(text:score in 0) [PerFieldSimilarity], result of:",[
                        {
                           "value": 0.99999994,
                           "description": "queryWeight, product of:",
                           "details": [
                              {
                                 "value": 0.5945349,
                                 "description": "idf(docFreq=2, maxDocs=2)"
                              },
                              {
                                 "value": 1.681987,
                                 "description": "queryNorm"
                              }
                           ]
                        },
                        {
                           "value": 0.22295058,
                           "description": "fieldWeight in 0, product of:",
                           "details": [
                              {
                                 "value": 1,
                                 "description": "tf(freq=1.0), with freq of:",
                                 "details": [
                                    {
                                       "value": 1,
                                       "description": "termFreq=1.0"
                                    }
                                 ]
                              },
                              {
                                 "value": 0.5945349,
                                 "description": "idf(docFreq=2, maxDocs=2)"
                              },
                              {
                                 "value": 0.375,
                                 "description": "fieldNorm(doc=0)"
                              }
                           ]
                        }
                     ]
                  }
               ]
            }
         },
         {
            "_shard": 3,
            "_node": "5IoTIXdmROKObfLRmqCQYQ",
            "_index": "test",
            "_type": "doc",
            "_id": "2",
            "_score": 0.22295055,
            "_source": {
               "text": "Score and explanation did match in 1.7..."
            },
            "_explanation": {
               "value": 0.22295056,
               "description": "weight(text:score in 0) [PerFieldSimilarity], result of:",
               "details": [
                  {
                     "value": 0.22295056,
                     "description": "score(doc=0,freq=1.0), product of:",
                     "details": [
                        {
                           "value": 0.99999994,
                           "description": "queryWeight, product of:",
                           "details": [
                              {
                                 "value": 0.5945349,
                                 "description": "idf(docFreq=2, maxDocs=2)"
                              },
                              {
                                 "value": 1.681987,
                                 "description": "queryNorm"
                              }
                           ]
                        },
                        {
                           "value": 0.22295058,
                           "description": "fieldWeight in 0, product of:",
                           "details": [
                              {
                                 "value": 1,
                                 "description": "tf(freq=1.0), with freq of:",
                                 "details": [
                                    {
                                       "value": 1,
                                       "description": "termFreq=1.0"
                                    }
                                 ]
                              },
                              {
                                 "value": 0.5945349,
                                 "description": "idf(docFreq=2, maxDocs=2)"
                              },
                              {
                                 "value": 0.375,
                                 "description": "fieldNorm(doc=0)"
                              }
                           ]
                        }
                     ]
                  }
               ]
            }
         }
      ]
   }
}

``````

and in 2.1:

```
{
   "took": 46,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 0.22295055,
      "hits": [
         {
            "_shard": 2,
            "_node": "hcC0CtAfTSSILbEq32T_fw",
            "_index": "test",
            "_type": "doc",
            "_id": "2",
            "_score": 0.22295055,
            "_source": {
               "text": "Score and explanation did match in 1.7..."
            },
            "_explanation": {
               "value": 0.11506981,
               "description": "weight(text:score in 0) [PerFieldSimilarity], result of:",
               "details": [
                  {
                     "value": 0.11506981,
                     "description": "fieldWeight in 0, product of:",
                     "details": [
                        {
                           "value": 1,
                           "description": "tf(freq=1.0), with freq of:",
                           "details": [
                              {
                                 "value": 1,
                                 "description": "termFreq=1.0",
                                 "details": []
                              }
                           ]
                        },
                        {
                           "value": 0.30685282,
                           "description": "idf(docFreq=1, maxDocs=1)",
                           "details": []
                        },
                        {
                           "value": 0.375,
                           "description": "fieldNorm(doc=0)",
                           "details": []
                        }
                     ]
                  }
               ]
            }
         },
         {
            "_shard": 3,
            "_node": "hcC0CtAfTSSILbEq32T_fw",
            "_index": "test",
            "_type": "doc",
            "_id": "1",
            "_score": 0.22295055,
            "_source": {
               "text": "Score and explanation should match I think?"
            },
            "_explanation": {
               "value": 0.11506981,
               "description": "weight(text:score in 0) [PerFieldSimilarity], result of:",
               "details": [
                  {
                     "value": 0.11506981,
                     "description": "fieldWeight in 0, product of:",
                     "details": [
                        {
                           "value": 1,
                           "description": "tf(freq=1.0), with freq of:",
                           "details": [
                              {
                                 "value": 1,
                                 "description": "termFreq=1.0",
                                 "details": []
                              }
                           ]
                        },
                        {
                           "value": 0.30685282,
                           "description": "idf(docFreq=1, maxDocs=1)",
                           "details": []
                        },
                        {
                           "value": 0.375,
                           "description": "fieldNorm(doc=0)",
                           "details": []
                        }
                     ]
                  }
               ]
            }
         }
      ]
   }
}

```
</description><key id="121492679">15369</key><summary>score and explanation for dfs queries don't match  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>bug</label></labels><created>2015-12-10T14:07:16Z</created><updated>2016-08-12T10:22:13Z</updated><resolved>2016-08-12T10:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-10T19:21:21Z" id="163724291">@brwe Can you explain what "doesn't match"? The explanations and scores look the same to me, but maybe I'm just not seeing it in the page-of-json. :)
</comment><comment author="jpountz" created="2015-12-10T21:40:57Z" id="163757492">@rjernst I wondered the same but I think I see it now: in the 2.1 output `_score` is different from `_explanation.value`.
</comment><comment author="clintongormley" created="2015-12-11T09:30:34Z" id="163885427">Related to https://github.com/elastic/elasticsearch/issues/2612?
</comment><comment author="sylvinus" created="2016-01-07T23:39:33Z" id="169841504">I have the same issue. Any clue on how to fix or a workaround?
</comment><comment author="gmoskovicz" created="2016-05-06T13:37:21Z" id="217442729">I can confirm that this still happens in `2.3`. 

Looks like when using dfs, the explain ignores the fact that we need to merge the results from the information of the shards. The explain value when using `dfs` is the same score that you get if you do not use `dfs`, but instead just query then fetch.
</comment><comment author="gmoskovicz" created="2016-05-06T14:04:01Z" id="217448982">```
        "_score": 0.2202036,     &lt;---------- dfs score
        "_source": {
          "city": "montevideo",
          "cityAliases": "mvd"
        },
        "_explanation": {
          "value": 0.2417773,       &lt;----------- the score for non-dfg
          "description": "max of:",
          "details": [
            {
              "value": 0.2417773,
```
</comment><comment author="gmoskovicz" created="2016-05-06T14:09:49Z" id="217450326">Are there any test cases that verifies that the explanation root value is equals to the hit score? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Joint parsing of common global Hightlighter and subfield parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15368</link><project id="" key="" /><description>The top-level highlighter has many options that can be overwritten per field. Currently there is very similar code for this in two places. This PR pulls out the parsing of the common parameters into AbstractHighlighterBuilder for better reuse and to keep parsing of common parameters more consistent.

Closes #15285
</description><key id="121485681">15368</key><summary>Joint parsing of common global Hightlighter and subfield parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T13:33:34Z</created><updated>2015-12-14T14:19:03Z</updated><resolved>2015-12-14T14:19:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-14T13:39:54Z" id="164440899">Left a minor comment but LGTM
</comment><comment author="cbuescher" created="2015-12-14T14:18:52Z" id="164449031">Thanks for the review, addressed your comments and will merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature Request] "_highlight" API for indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15367</link><project id="" key="" /><description>Indices already have an __analyze_ API from which we can see how analyzers work.  

We need the "same" API to highlight sample text with a query according to index settings (same thing is already implemented in [sphinx](http://sphinxsearch.com/docs/current.html#sphinxql-call-snippets) ).

This can be useful when we do not need to store text in document, but need to highlight some text.   

For now, if we want to highlight sample text, we have to index "tmp" text, highlight it, and then delete it.
</description><key id="121482016">15367</key><summary>[Feature Request] "_highlight" API for indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aeryaguzov</reporter><labels /><created>2015-12-10T13:19:08Z</created><updated>2015-12-14T16:35:54Z</updated><resolved>2015-12-14T16:26:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aeryaguzov" created="2015-12-10T13:25:00Z" id="163613661">As I understand, there is no way to highlight childs in parent/child query (https://github.com/elastic/elasticsearch/issues/1764), so this API can also be a workaround to highlight childs after search is done.
</comment><comment author="clintongormley" created="2015-12-14T16:26:09Z" id="164484374">Hi @aeryaguzov 

The analyze API works because it is a process which happens before data is indexed.  Highlighters require indexed terms for query matching and for term statistics.  So in the end, you have to index the text.

For parent/child, you can now highlight the child documents using inner_hits:
https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-inner-hits.html#parent-child-inner-hits
</comment><comment author="nik9000" created="2015-12-14T16:35:54Z" id="164487663">&gt; So in the end, you have to index the text.

Its a bit more complex than that. 2/3 of the highlighters require indexed text. The plain highlighter always reanalyzes on the fly, last I checked. It'd _technically_ be possible to implement "here is some text and a query please highlight it" for all highlighters using a MemoryIndex. Its probably overkill though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Your repository is broken again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15366</link><project id="" key="" /><description>http://packages.elastic.co/elasticsearch/2.0/debian returns the following:

```
&lt;Error&gt;
&lt;Code&gt;NoSuchKey&lt;/Code&gt;
&lt;Message&gt;The specified key does not exist.&lt;/Message&gt;
&lt;Key&gt;elasticsearch/2.0/debian&lt;/Key&gt;
&lt;RequestId&gt;13E7BBDCE56955F6&lt;/RequestId&gt;
&lt;HostId&gt;vCn6AScakPhudxmzaNZr2S62mcmOoS/ynRIHM49znkdvS4rFPrpSo6OFfTJnBeqhbtz07dUEz10=&lt;/HostId&gt;
&lt;/Error&gt;
```
</description><key id="121478479">15366</key><summary>Your repository is broken again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jalle19</reporter><labels /><created>2015-12-10T13:02:17Z</created><updated>2015-12-10T14:03:09Z</updated><resolved>2015-12-10T13:11:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-10T13:11:12Z" id="163610429">You want to [use](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html) `elasticsearch/2.x/debian`, not `elasticsearch/2.0/debian`. Note the `x`.
</comment><comment author="Jalle19" created="2015-12-10T13:27:16Z" id="163614086">I've obviously tried that. http://packages.elastic.co/elasticsearch/2.x/debian returns the following:

```
&lt;Error&gt;
&lt;Code&gt;NoSuchKey&lt;/Code&gt;
&lt;Message&gt;The specified key does not exist.&lt;/Message&gt;
&lt;Key&gt;elasticsearch/2.x/debian&lt;/Key&gt;
&lt;RequestId&gt;D7610B929E37958E&lt;/RequestId&gt;
&lt;HostId&gt;MInY3HHxQzSymUK1KBNxDlqQjvV3xKiY5DF8Dc8ulf5ntCrSF6CMgNjgxLHORPNBDoZM6gEwZg0=&lt;/HostId&gt;
&lt;/Error&gt;
```
</comment><comment author="jasontedor" created="2015-12-10T13:37:39Z" id="163620537">Are you trying to access in a browser? That's not how the repositories work. If it is not the case that you're trying to access in a browser, can you please provide all the steps to reproduce?

```
vagrant@vagrant-ubuntu-vivid-64:~$ date
Thu Dec 10 13:36:24 UTC 2015
vagrant@vagrant-ubuntu-vivid-64:~$ uname -a
Linux vagrant-ubuntu-vivid-64 3.19.0-26-generic #28-Ubuntu SMP Tue Aug 11 14:16:32 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
vagrant@vagrant-ubuntu-vivid-64:~$ wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
OK
vagrant@vagrant-ubuntu-vivid-64:~$ echo "deb http://packages.elastic.co/elasticsearch/2.x/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.x.list
deb http://packages.elastic.co/elasticsearch/2.x/debian stable main
vagrant@vagrant-ubuntu-vivid-64:~$ sudo apt-get update -o Dir::Etc::sourcelist="sources.list.d/elasticsearch-2.x.list" -o Dir::Etc::sourceparts="-" -o APT::Get::List-Cleanup="0"
Ign http://packages.elastic.co stable InRelease
Get:1 http://packages.elastic.co stable Release.gpg [473 B]
Get:2 http://packages.elastic.co stable Release [1,234 B]
Get:3 http://packages.elastic.co stable/main amd64 Packages [1,065 B]
Ign http://packages.elastic.co stable/main Translation-en_US
Ign http://packages.elastic.co stable/main Translation-en
Fetched 2,772 B in 0s (3,528 B/s)
Reading package lists... Done
vagrant@vagrant-ubuntu-vivid-64:~$ sudo apt-get install elasticsearch
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 28.9 MB of archives.
After this operation, 32.4 MB of additional disk space will be used.
Get:1 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main elasticsearch all 2.1.0 [28.9 MB]
Fetched 28.9 MB in 5s (5,602 kB/s)        
Selecting previously unselected package elasticsearch.
(Reading database ... 63985 files and directories currently installed.)
Preparing to unpack .../elasticsearch_2.1.0_all.deb ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Unpacking elasticsearch (2.1.0) ...
Processing triggers for systemd (219-7ubuntu6) ...
Processing triggers for ureadahead (0.100.0-19) ...
Setting up elasticsearch (2.1.0) ...
Processing triggers for systemd (219-7ubuntu6) ...
Processing triggers for ureadahead (0.100.0-19) ...
vagrant@vagrant-ubuntu-vivid-64:~$ sudo /bin/systemctl start elasticsearch.service
vagrant@vagrant-ubuntu-vivid-64:~$ curl -XGET localhost:9200/
{
  "name" : "Serpentina",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.1.0",
    "build_hash" : "72cd1f1a3eee09505e036106146dc1949dc5dc87",
    "build_timestamp" : "2015-11-18T22:40:03Z",
    "build_snapshot" : false,
    "lucene_version" : "5.3.1"
  },
  "tagline" : "You Know, for Search"
}
vagrant@vagrant-ubuntu-vivid-64:~$ 
```
</comment><comment author="Jalle19" created="2015-12-10T13:47:18Z" id="163622861">Yes I was testing it via a browser since `apt-get update` started failing with 404 errors. I'll try reproducing now with `2.x` instead of `2.0` (for the record, `2.0` used to work).
</comment><comment author="jasontedor" created="2015-12-10T13:51:01Z" id="163624154">&gt; I'll try reproducing now with 2.x instead of 2.0 (for the record, 2.0 used to work).

`elasticsearch/2.0/debian` was used for the betas and release candidates, but the [stable releases were distributed](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html) under `elasticsearch/2.x/debian` and `elasticsearch/2.0/debian` was [removed](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_plugin_and_packaging_changes.html#_repository_naming_structure_changes). Sorry you experienced the confusion.
</comment><comment author="Jalle19" created="2015-12-10T13:59:41Z" id="163627014">Okay well that's strange, it worked now that I used `2.x`. I assumed since neither worked in a browser (which is strange in itself since `apt-get` downloads stuff over HTTP) the repository was completely broken.
</comment><comment author="jasontedor" created="2015-12-10T14:03:09Z" id="163628052">&gt; Okay well that's strange, it worked now that I used 2.x. 

Glad you're running now.

&gt; I assumed since neither worked in a browser (which is strange in itself since apt-get downloads stuff over HTTP) the repository was completely broken.

It does use HTTP, but it's not just a simple GET request on that path. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve pluginmanager permissions output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15365</link><project id="" key="" /><description>For 2.2, we let plugins define their own permissions. `/bin/plugin` lists the permissions, points you to Oracle documentation, and asks you to confirm Y/N.

It would be better to output the current stuff only in verbose mode and give simpler information otherwise. For the multicast plugin, instead of:

```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.net.SocketPermission localhost:1024- listen,resolve
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]
```

something like:

```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* Bind to network ports
Specify the -v option for more details.

Continue with installation? [y/N]
```

We don't need fine-grained explanation of every single permission in java. We can bucket many of them into categories, e.g. "Access Java internals", "Modify JVM-wide settings", "Violate system boundaries", and so on.
</description><key id="121473498">15365</key><summary>improve pluginmanager permissions output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-10T12:33:55Z</created><updated>2016-03-16T19:34:05Z</updated><resolved>2016-03-15T18:12:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-10T14:24:29Z" id="163633921">&gt; We don't need fine-grained explanation of every single permission in java. We can bucket many of them into categories, e.g. "Access Java internals", "Modify JVM-wide settings", "Violate system boundaries", and so on.

More and more like android every day.... Its a good idea.

Do we trust plugins to define these buckets themselves in a config file or something or should just just all live in core and when we don't recognize one we bounce the user to Oracle's docs? I think the latter? I'd prefer to give users cryptic but exact permissions rather than allow a malicious plugin to tuck a permission away somewhere were it shouldn't.
</comment><comment author="rmuir" created="2015-12-10T14:25:22Z" id="163634160">I'm only recommending changing plugin manager's output. That is all.
</comment><comment author="clintongormley" created="2015-12-14T16:31:45Z" id="164486549">+1
</comment><comment author="skearns64" created="2016-01-27T14:49:49Z" id="175664944">+1
</comment><comment author="djschny" created="2016-03-15T18:03:33Z" id="196951584">Additionally a command line option or something similar to bypass the prompt for stdin (user input) is really helpful, as these prompts make automated installs very difficult if not possible.
</comment><comment author="rmuir" created="2016-03-15T18:12:01Z" id="196955004">It would not be helpful at all. there is already one, but you didn't bother to look?
</comment><comment author="djschny" created="2016-03-15T18:15:13Z" id="196956657">I did, because I pulled up the class to make sure it was Elasticsearch code and not JVM code:

https://github.com/elastic/elasticsearch/blob/5f3d0067f8ef0c1216ec47c7c360d018355f58ee/core/src/main/java/org/elasticsearch/plugins/PluginSecurity.java

But I missed the batch flag option. My bad.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the MissingQueryBuilder which was deprecated in 2.2.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15364</link><project id="" key="" /><description>As a replacement use ExistsQueryBuilder inside a mustNot() clause.
So instead of using `new ExistsQueryBuilder(name)` now use:
`new BoolQueryBuilder().mustNot(new ExistsQueryBuilder(name))`.

Closes #14112
</description><key id="121467764">15364</key><summary>Remove the MissingQueryBuilder which was deprecated in 2.2.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Query DSL</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T11:56:43Z</created><updated>2016-07-29T12:08:58Z</updated><resolved>2015-12-11T16:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-11T16:17:37Z" id="163978733">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use transport service to handle RetryOnReplicaException to execute replica action on the current node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15363</link><project id="" key="" /><description>Transport service has logic to handle the replica locally.
</description><key id="121467329">15363</key><summary>Use transport service to handle RetryOnReplicaException to execute replica action on the current node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T11:53:15Z</created><updated>2015-12-15T14:10:17Z</updated><resolved>2015-12-15T08:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-10T13:14:16Z" id="163610951">Thanks @martijnvg . Left a minor comment.
</comment><comment author="martijnvg" created="2015-12-11T08:55:14Z" id="163878645">thanks for looking at this @bleskes. I have updated the PR.
</comment><comment author="martijnvg" created="2015-12-11T10:08:07Z" id="163897746">@boaz I added a base class for delegating response handling to transport channel and made the retry replica operation use it.
</comment><comment author="bleskes" created="2015-12-11T10:32:18Z" id="163904418">Thanks @martijnvg . I think it's much cleaner now. Left a minor comment that I think will make it even cleaner.
</comment><comment author="martijnvg" created="2015-12-11T10:59:25Z" id="163908882">@bleskes agreed, that makes it better. I've update the PR.
</comment><comment author="bleskes" created="2015-12-14T19:32:32Z" id="164535921">LGTM. Can you label this PR? I think 3.0 and 2.x?
</comment><comment author="martijnvg" created="2015-12-15T14:10:17Z" id="164775543">back port to 2.x branch: 7ff74a86548266b72a2e3659f9cbebebb360f23a 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Doc] missing field name in Upsert api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15362</link><project id="" key="" /><description>missing filed `name` in Upsert api
</description><key id="121459018">15362</key><summary>[Doc] missing field name in Upsert api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">boliza</reporter><labels><label>docs</label></labels><created>2015-12-10T11:02:05Z</created><updated>2015-12-31T16:08:33Z</updated><resolved>2015-12-31T13:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T13:47:31Z" id="163940024">@dadoonet could you review please
</comment><comment author="dadoonet" created="2015-12-31T13:37:47Z" id="168194293">Thanks for the PR.

I don't think `name` field is missing here. It's an update request and we wanted to only update the `gender` field.

Closing but feel free to reopen if you think differently and explain why.
</comment><comment author="boliza" created="2015-12-31T14:56:10Z" id="168207014">@dadoonet  I still confused. In update.asciidoc, after line 95

&gt; If the document index/type/1 already exists, we will have after this operation a document like:

```
{
    "name"  : "Joe Dalton",
    "gender": "male"
}
```
</comment><comment author="dadoonet" created="2015-12-31T15:03:55Z" id="168208457">What is meant here is the following.

Let say you have an existing document:

``` js
{
     "name"  : "Joe Dalton"
}
```

If you apply the upsert call, it will become:

``` json
{
    "name"  : "Joe Dalton",
    "gender": "male"
}
```

If the document does not exist, it will be created with the index request content and will be:

``` json
{
    "name"  : "Joe Dalton",
    "gender": "male"
}
```

Makes sense?
</comment><comment author="boliza" created="2015-12-31T15:19:13Z" id="168210136">But the example souce code in [update.asciidoc](https://github.com/elastic/elasticsearch/blob/master/docs/java-api/docs/update.asciidoc) is 

``` java
IndexRequest indexRequest = new IndexRequest("index", "type", "1")
                .source(JsonXContent.contentBuilder()
                        .startObject()
                        .field("name", "Joe Smith")
                        .field("gender", "male")
                        .endObject());
UpdateRequest updateRequest = new UpdateRequest("index", "type", "1")
                .doc(JsonXContent.contentBuilder()
                        .startObject()
                        .field("gender", "male")
                        .endObject())
                .upsert(indexRequest);
client.update(updateRequest).get();
```
</comment><comment author="dadoonet" created="2015-12-31T15:32:07Z" id="168211514">So we have an update part:

``` java
JsonXContent.contentBuilder()
                        .startObject()
                        .field("gender", "male")
                        .endObject())
```

And an upsert request:

``` java
JsonXContent.contentBuilder()
                        .startObject()
                        .field("name", "Joe Smith")
                        .field("gender", "male")
                        .endObject());
```

Does it seem wrong to you?
</comment><comment author="boliza" created="2015-12-31T15:36:01Z" id="168211816">this will be correct if the upsert quest just update the `gender`
</comment><comment author="boliza" created="2015-12-31T15:39:01Z" id="168212058">I have test this code ,this match the update.asciidoc says

``` java
IndexRequest indexRequest = new IndexRequest("index", "type", "1")
                .source(JsonXContent.contentBuilder()
                        .startObject()
                        .field("name", "Joe Smith")
                        .field("gender", "male")
                        .endObject());
UpdateRequest updateRequest = new UpdateRequest("index", "type", "1")
                .doc(JsonXContent.contentBuilder()
                        .startObject()
                        .field("name", "Joe Dalton")
                        .field("gender", "male")
                        .endObject())
                .upsert(indexRequest);
client.update(updateRequest).get();
```

but test this code 

``` java
IndexRequest indexRequest = new IndexRequest("index", "type", "1")
                .source(JsonXContent.contentBuilder()
                        .startObject()
                        .field("name", "Joe Smith")
                        .field("gender", "male")
                        .endObject());
UpdateRequest updateRequest = new UpdateRequest("index", "type", "1")
                .doc(JsonXContent.contentBuilder()
                        .startObject()
                        .field("gender", "male")
                        .endObject())
                .upsert(indexRequest);
client.update(updateRequest).get();
```

nothing changed whatever the document exists or not exists
</comment><comment author="dadoonet" created="2015-12-31T15:42:32Z" id="168212338">I think that the example I wrote is like:

``` sh
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "doc" : {
      "gender": "male"
    },
    "upsert" : {
      "name"  : "Joe Dalton",
      "gender": "male"
    }
}'
```

Which means to me that if doc `test/type/1` exists, then `gender` field will be merged. If not, the full doc with `gender` and `name` will be added.

Is this wrong?
</comment><comment author="boliza" created="2015-12-31T15:48:39Z" id="168212962">in your comment is right,but update.asciidoc in master branch is 
![qq 20151231234624](https://cloud.githubusercontent.com/assets/1076043/12065812/e7945f24-b018-11e5-9f9c-f1a5513848a2.png)
</comment><comment author="dadoonet" created="2015-12-31T15:50:34Z" id="168213103">I'm probably blind but I don't see what is wrong... May be I need some rest...
</comment><comment author="boliza" created="2015-12-31T15:56:20Z" id="168214080">In a other word. in [update.asciidoc](https://github.com/elastic/elasticsearch/blob/master/docs/java-api/docs/update.asciidoc).  the field `name` in request source is `Joe Smith`, but in document the field `name` has changed to `Joe Dalton`. it' dose't make sense.
</comment><comment author="boliza" created="2015-12-31T16:03:52Z" id="168214729">By the way Happy new year 
</comment><comment author="dadoonet" created="2015-12-31T16:08:33Z" id="168215110">I see. So the document was existing first with a name as `Joe Dalton`. We added a `gender` to it.
If it does not exist, it's created with `Joe Smith` and a `gender`.

That what it means.

Happy new year as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>node setting update api request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15361</link><project id="" key="" /><description>nowsday, we can update cluster setting through API, like this:
PUT _cluster/settings
{
    "persistent" : {
        "threadpool.search.size" : 128
    }
}
is it possible to make it can update node like this:
PUT _cluster/nodename/settings
{
    "persistent" : {
        "threadpool.search.size" : 128
    }
}
</description><key id="121456339">15361</key><summary>node setting update api request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Settings</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-12-10T10:46:38Z</created><updated>2015-12-11T13:46:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve network docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15360</link><project id="" key="" /><description>This makes some minor improvements (does not fix all problems!)

It reorders unicast disco in elasticsearch.yml to be right after the network host,
for better locality.

It removes the warning (unreleased) about publish addresses, lets try to really discourage setting
that unless you need to (behind a proxy server). Most people should be fine with `network.host`

Finally it reorganizes the network docs page a bit:

We add a table of 4 "basic" settings at the very beginning:
- network.host
- discovery.zen.ping.unicast.hosts
- http.port
- transport.tcp.port

The first two being the most important, which addresses to bind and talk to, and the other two
being the port numbers.

The rest of the stuff I tried to simplify and reorder under "advanced" headers.

This is just a quick stab, I still think we need more effort into this thing, but we gotta start somewhere.

Closes #15343
</description><key id="121456105">15360</key><summary>Improve network docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T10:45:37Z</created><updated>2015-12-10T14:01:13Z</updated><resolved>2015-12-10T11:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-10T11:00:03Z" id="163579062">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>thread_pool unconsistency issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15359</link><project id="" key="" /><description>when config thread pool in confi file, there is size parameter
when GET _nodes/thread_pool, there are min and max
when GET _nodes/stats/thread_pool, there is threads
can it be unifinied to cause less confuse?
</description><key id="121454374">15359</key><summary>thread_pool unconsistency issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">makeyang</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-10T10:37:32Z</created><updated>2016-01-11T15:00:33Z</updated><resolved>2016-01-11T15:00:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-01-11T15:00:33Z" id="170579671">In this case, the thread pool reporting for the two different endpoints is intended to report different things:

GET _nodes/thread_pool is meant to show how the node is configured with regards to each thread pool.  This is static information on the node, and 'min' and 'max' here indicate the core pool size and max thread pool size with respect to the ThreadPoolExecutor.

GET _nodes/stats/thread_pool is meant to show the current state of each thread pool, where 'threads' in the latter is showing a dynamic reporting of the current number of live threads for the given thread pool (based on ThreadPoolExecutor's getPoolSize()).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make XContentGenerator.writeRaw* safer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15358</link><project id="" key="" /><description>This method currently allows to write arbitrary bytes in an xcontent stream.
I changed it so that it can only write data to the same stream as the xcontent
(the bos parameter is removed) and that it yells at you if you try to write
raw bytes that can't be recognized as xcontent. Also the logic to copy the
structure instead of appending the bytes directly if the source and target
are of a different xcontent type have been moved to the low-level
XContentGenerator.
</description><key id="121436360">15358</key><summary>Make XContentGenerator.writeRaw* safer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:REST</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T09:21:27Z</created><updated>2015-12-11T16:19:57Z</updated><resolved>2015-12-11T11:18:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-12-10T10:26:35Z" id="163569371">LGTM, this is much cleaner
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows service fails to start es: 2.1.0 default settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15357</link><project id="" key="" /><description>I'm using a clean windows 7 with elastic search 2.1.0 with the default settings.
When i run the process with elasticsearch.bat everything is ok.
When i try to run the service via the service.bat start i get this log in the stderr file:

"Failed to start service
2015-12-10 09:11:57 Commons Daemon procrun stderr initialized
Exception in thread "main" tion: path.home is not configured
    at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:99)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:81)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:107)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:100)
    at org.elasticsearch.bootstrap.BootstrapCLIParser.&lt;init&gt;(BootstrapCLIParser.java:48)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:241)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
The data area passed to a system call is too small.
"

I installed the process properly and i've seen that this is a known issue that was supposedly fixed. This isn't the case. 
Please assist.
Thanks.

One last thing: when i installed the service i got this error:
Error: Don't modify the classpath with ES_CLASSPATH, Best is to add
additional elements via the plugin mechanism, or if code must really be
added to the main classpath, add jars to lib\, unsupported
The service 'elasticsearch-service-x64' has been installed.
</description><key id="121435833">15357</key><summary>Windows service fails to start es: 2.1.0 default settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">randude</reporter><labels /><created>2015-12-10T09:19:02Z</created><updated>2015-12-10T10:31:48Z</updated><resolved>2015-12-10T10:17:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="randude" created="2015-12-10T10:31:48Z" id="163570594">solved it, it was my machine's problem...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove benchmark package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15356</link><project id="" key="" /><description>Tons of ancient "benchmarks" exist in elasticsearch. These are main
methods that do some kind of construction of ES classes and time various
things. The problem with these is they are not maintained, and not run.
Refactorings that touch anything that is common in these classes is very
painful. Going through these, almost all would simply not work in 2.x
without modifications (because they do not set path.home).

This change removes the entire benchmark package. If someone needs to
run a benchmark like this, they can look at history for examples if
necessary (although these examples are often not realistic and should
just start real elasticsearch processes in a shell script). Longer term,
we should make this easier to do by having the build support adding real
benchmarks which can be run in jenkins (so we know they actually run,
instead of doing refactorings with pure guesswork as to whether the
benchmark would run correctly).
</description><key id="121428374">15356</key><summary>Remove benchmark package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T08:23:41Z</created><updated>2016-03-15T15:38:05Z</updated><resolved>2015-12-11T03:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-10T08:33:45Z" id="163533219">+1 but I know this can be controversial so I would suggest to wait a bit before merging
</comment><comment author="jasontedor" created="2015-12-10T11:39:45Z" id="163586206">I've pondered the same thing myself many times during some refactoring efforts; I'm in favor of this.

LGTM.
</comment><comment author="kimchy" created="2015-12-10T11:44:25Z" id="163587906">I am +1 on this change. What I would ask is that we capture the intent of some of these benchmarks into a meta issue in in our perf testing infra to make sure we end up having coverage for those and testing them in a continuous manner. Things like indexing perf for json doc with "just" string and a number, many aliases tests, ... (/cc @danielmitterdorfer)
</comment><comment author="danielmitterdorfer" created="2015-12-10T11:49:36Z" id="163589205">Funny, I have also came across the benchmark classes and felt the same way. So +1 too. I'll create a meta issue over in the Rally repo and reference this ticket so we don't forget it. Thanks for bringing it to my attention @kimchy.
</comment><comment author="jpountz" created="2015-12-10T21:25:29Z" id="163753972">&gt; I would suggest to wait a bit before merging

OK let's merge now :)
</comment><comment author="ywelsch" created="2016-03-15T15:38:05Z" id="196884314">@danielmitterdorfer I have removed two more benchmark classes in d14ae5f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting error while connecting to kibana after configuring SearchGuard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15355</link><project id="" key="" /><description>Hi Everyone

 I Have Implemented  &#8220;search-guard&#8221; Elasticsearch
--&gt;
_First i have created a &#8220;SearchGuard&#8221; Index
*Then I have Installed and Configured it in logging.yml and Elasticsearch.yml 
*In elasticsearch.yml i have given a user name:admin and password:admin
note:I have given all permissions to admin user in &#8220;searchguard&#8221; index
*_HERE is my index

curl -XPUT 'http://localhost:9200/searchguard/ac/ac' -d '{
    "acl": [
    {  
        "**Comment**": "By default no filters are executed and no filters a by-passed. In such a case an exception is thrown and access will be denied.",
        "filters_bypass": [],
        "filters_execute": []
     },
     {
           "**Comment**": "For role _admin_ all filters are bypassed (so none will be executed). This means unrestricted access.",
           "roles": [
               "admin"
           ],
           "filters_bypass": [],
           "filters_execute": [&#8220;actionrequestfilter.kibanaUser&#8221;]
     }
     ]
}'

**Here is my elasticsearch.yml
searchguard.enabled: true
searchguard.rewrite_get_as_search: true
searchguard.allow_all_from_loopback: true
searchguard.config_index_name: searchguard
searchguard.key_path: /var/lib/elasticsearch/searchguard_node.key
## User settings

searchguard.authentication.settingsdb.user.admin: admin
searchguard.actionrequestfilter.names: ["kibanaUser","admincopy"]
searchguard.actionrequestfilter.kibanaUser.allowed_actions:["indices:admin/get","indices:data/read/_", "_monitor*"]

---&gt;Here iam getting &#8220;timeout&#8221; Error.
As i am new to this concept.Not getting.Please someone help me out in this??  

Thanks And Regards
Balaji
</description><key id="121426644">15355</key><summary>Getting error while connecting to kibana after configuring SearchGuard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">balubollam</reporter><labels /><created>2015-12-10T08:10:10Z</created><updated>2015-12-10T14:24:05Z</updated><resolved>2015-12-10T14:24:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sarwarbhuiyan" created="2015-12-10T12:38:22Z" id="163598373">You may want to post this on the search guard GitHub
</comment><comment author="balubollam" created="2015-12-10T14:22:31Z" id="163633491">Ya Sure
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove NodeBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15354</link><project id="" key="" /><description>The NodeBuilder is currently used to construct a Node. However, this is
really just yet-another-builder that wraps around a Settings.Builder
witha couple convenience methods. But there are very few uses of these
convenience methods.  This change removes NodeBuilder, in favor of just
using the Node constructor.
</description><key id="121426499">15354</key><summary>Remove NodeBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2015-12-10T08:09:00Z</created><updated>2016-11-07T19:45:36Z</updated><resolved>2015-12-11T03:19:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-10T08:42:03Z" id="163534526">Left some minor comments (missing `start()` calls and typo), o.w. LGTM.
</comment><comment author="s1monw" created="2015-12-10T10:44:44Z" id="163575606">+1
</comment><comment author="jasontedor" created="2015-12-10T22:10:51Z" id="163764188">I'm in favor of this change, but not in favor of making a change that will break compilation of client code in a minor release (this is currently labeled v2.2.0).
</comment><comment author="rjernst" created="2015-12-10T22:17:12Z" id="163765531">@jasontedor NodeBuilder has always been an internal API. The fact that some users (who embed ES) use it instead of the Node constructor is not a reason we should hold back internal refactoring. Users who embed ES are advanced enough that they can update to use the Node constructor.
</comment><comment author="rjernst" created="2015-12-11T03:19:51Z" id="163825954">I removed the 2.2 label, and opened an issue (#15383) to change the docs so constructing an in memory Node is never recommended.
</comment><comment author="s1monw" created="2015-12-11T07:58:56Z" id="163868884">&gt; I'm in favor of this change, but not in favor of making a change that will break compilation of client code in a minor release (this is currently labeled v2.2.0).

Since we are in java you have to recompile your client code anyway so the compiler will tell you what's going on. This is a great way to move on with APIs - when I am in the situation I was always happy to see a hard break to make the changes early and often.
</comment><comment author="clintongormley" created="2015-12-11T09:46:50Z" id="163891332">@rjernst can we have a note in the breaking changes docs please
</comment><comment author="s1monw" created="2015-12-11T10:30:05Z" id="163904039">&gt; @rjernst can we have a note in the breaking changes docs please

is this necessary? It's unsupported really and a Java API
</comment><comment author="dadoonet" created="2015-12-11T10:34:04Z" id="163904739">Well. It has always been documented in the Java API doc. So that makes it a bit "official" IMO.
</comment><comment author="bleskes" created="2015-12-11T10:36:31Z" id="163905145">It's how we now officially communicate on how to start a client.  https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/node-client.html 

+1 on clear messaging as to why we are changing it - to me, breaking changes feel natural.
</comment><comment author="mikemccand" created="2015-12-11T11:19:37Z" id="163913736">+1 to freely break this in 2.x and explain why in the breaking changes docs: javac makes it completely clear to these very expert users on upgrade that they need to use the `Node` constructor instead.
</comment><comment author="bleskes" created="2015-12-11T13:07:03Z" id="163932763">&gt; these very expert users 

I think this is the source of where people have different feelings. These are not expert settings - it's what we recommend on our website and many Java people use. I'm totally +1 with changing that (on 3.0 I would go as far as not treating this as a client at all). We should clear document this, explain why and offer people an alternative.
</comment><comment author="1ambda" created="2016-11-07T11:00:14Z" id="258806579">I think this is missing in 5.0 API breaking changes. 

https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-5.0.html
</comment><comment author="clintongormley" created="2016-11-07T12:40:55Z" id="258825678">&gt; I think this is missing in 5.0 API breaking changes.

True.  @rjernst could you add to the breaking changes please?
</comment><comment author="rjernst" created="2016-11-07T19:45:36Z" id="258941491">Added:
5.x: 71bc4c6
5.0: bc5d7a9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>analyzer multiple word synonym contraction problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15353</link><project id="" key="" /><description>I have an analyzer for lowercasing and contracting synonyms that is reported as such:

```
"services_analyzer": {
              "filter": [
                "lowercase",
                "services_synonym_filter",
                "length_filter",
                "super_stopwords_filter"
              ],
              "type": "custom",
              "tokenizer": "comma"
            }

"services_synonym_filter": {
              "type": "synonym",
              "synonyms": [
                "web development,web sites,coding =&gt; web",
                "search engine optimization =&gt; seo",
                "search engine marketing =&gt; sem",
                "ppc advertising, click advertising, pay per click =&gt; ppc"
              ]
            }

"length_filter": {
              "type": "length",
              "min": "2"
            }

"super_stopwords_filter": {
              "type": "stop",
              "stopwords": [
                "my",
                "new",
                "get",
                "like",
                "i",
                "a",
                "an",
                "and",
                "are",
                "as",
                "at",
                "be",
                "but",
                "by",
                "for",
                "if",
                "in",
                "into",
                "is",
                "it",
                "no",
                "not",
                "of",
                "on",
                "or",
                "such",
                "that",
                "the",
                "their",
                "then",
                "there",
                "these",
                "they",
                "this",
                "to",
                "was",
                "will",
                "with",
                "c",
                "llc",
                "inc",
                "c"
              ]
            }

"tokenizer": {
            "ac_edge_ngram": {
              "min_gram": "1",
              "side": "front",
              "type": "edgeNGram",
              "max_gram": "50"
            },
            "comma": {
              "pattern": ",",
              "type": "pattern"
            }
          }
        }

```

according to the documentation, the analyzer should contract the synonyms

```
web development 
web sites 
coding 
```

into token:

```
web
```

however, when I test the analyzer through the analyzer api:
`localhost:9200/companies/_analyze?analyzer=services_analyzer&amp;text=web development`

```
{
  "tokens": [
    {
      "token": "web development",
      "start_offset": 0,
      "end_offset": 15,
      "type": "word",
      "position": 0
    }
  ]
}
```

```
localhost:9200/companies/_analyze?analyzer=services_analyzer&amp;text=web sites
```

```
{
  "tokens": [
    {
      "token": "web sites",
      "start_offset": 0,
      "end_offset": 9,
      "type": "word",
      "position": 0
    }
  ]
}
```

```
localhost:9200/companies/_analyze?analyzer=services_analyzer&amp;text=coding
```

```
{
  "tokens": [
    {
      "token": "web",
      "start_offset": 0,
      "end_offset": 6,
      "type": "SYNONYM",
      "position": 0
    }
  ]
}
```

only one synonym is recognized, and contracted

I've spent a lot of time reading the documentation and I feel like I'm doing what I'm supposed to do, which is why I'm reporting this as an issue.

Any help is greatly appreciated!
:)
</description><key id="121373269">15353</key><summary>analyzer multiple word synonym contraction problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heyalexchoi</reporter><labels /><created>2015-12-10T00:25:53Z</created><updated>2015-12-10T21:21:37Z</updated><resolved>2015-12-10T21:21:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="heyalexchoi" created="2015-12-10T21:21:37Z" id="163753081">I overcame this by setting the synonym filter's tokenizer to 'keyword':

```
"services_synonym_filter": {
              "type": "synonym",
              "tokenizer": "keyword",
              "synonyms": [
                "web development,web sites,coding =&gt; web",
                "search engine optimization =&gt; seo",
                "search engine marketing =&gt; sem",
                "ppc advertising, click advertising, pay per click =&gt; ppc"
              ]
            }
```

which unfortunately doesn't seem to be mentioned in the documentation at all...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for proxy authentication for s3 and ec2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15352</link><project id="" key="" /><description>Backport of #15293 in 2.x branch

When using S3 or EC2, it was possible to use a proxy to access EC2 or S3 API but username and password were not possible to be set.

This commit adds support for this. Also, to make all that consistent, proxy settings for both plugins have been renamed:
- from `cloud.aws.proxy_host` to `cloud.aws.proxy.host`
- from `cloud.aws.ec2.proxy_host` to `cloud.aws.ec2.proxy.host`
- from `cloud.aws.s3.proxy_host` to `cloud.aws.s3.proxy.host`
- from `cloud.aws.proxy_port` to `cloud.aws.proxy.port`
- from `cloud.aws.ec2.proxy_port` to `cloud.aws.ec2.proxy.port`
- from `cloud.aws.s3.proxy_port` to `cloud.aws.s3.proxy.port`

New settings are `proxy.username` and `proxy.password`.

``` yml
cloud:
    aws:
        protocol: https
        proxy:
            host: proxy1.company.com
            port: 8083
            username: myself
            password: theBestPasswordEver!
```

You can also set different proxies for `ec2` and `s3`:

``` yml
cloud:
    aws:
        s3:
            proxy:
                host: proxy1.company.com
                port: 8083
                username: myself1
                password: theBestPasswordEver1!
        ec2:
            proxy:
                host: proxy2.company.com
                port: 8083
                username: myself2
                password: theBestPasswordEver2!
```

Note that `password` is filtered with `SettingsFilter`.

We also fix a potential issue in S3 repository. We were supposed to accept key/secret either set under `cloud.aws` or `cloud.aws.s3` but the actual code never implemented that.

It was:

``` java
account = settings.get("cloud.aws.access_key");
key = settings.get("cloud.aws.secret_key");
```

We replaced that by:

``` java
String account = settings.get(CLOUD_S3.KEY, settings.get(CLOUD_AWS.KEY));
String key = settings.get(CLOUD_S3.SECRET, settings.get(CLOUD_AWS.SECRET));
```

Also, we extract all settings for S3 in `AwsS3Service` as it's already the case for `AwsEc2Service` class.

Related to #15268.
</description><key id="121366097">15352</key><summary>Add support for proxy authentication for s3 and ec2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-12-09T23:31:01Z</created><updated>2015-12-30T17:02:40Z</updated><resolved>2015-12-11T21:44:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-09T23:31:46Z" id="163436506">@imotov Could you review this please? It's the backported version of #15293 you already reviewed. Thanks!
</comment><comment author="imotov" created="2015-12-10T22:02:37Z" id="163762362">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disallow unquoted field names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15351</link><project id="" key="" /><description>Closes #9800 - this change is breaking for 3.0 to disallow unquoted JSON which breaks a load of elasticsearch clients who expect Content-Type: application/json to be, well JSON :)
</description><key id="121342432">15351</key><summary>Disallow unquoted field names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels><label>:REST</label><label>breaking</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2015-12-09T21:27:53Z</created><updated>2016-04-06T20:38:16Z</updated><resolved>2016-04-06T20:38:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T21:38:39Z" id="163401449">Left one comment, other than that this looks good to me.
</comment><comment author="dakrone" created="2015-12-09T22:04:59Z" id="163410488">@jimmyjones2 actually, this causes failures in `DoSectionParserTests`, can you fix those tests before I merge this?
</comment><comment author="jimmyjones2" created="2015-12-09T22:09:20Z" id="163413025">Humm, I wonder if this is breaking YAML... looks like a tomorrow job to me.
</comment><comment author="clintongormley" created="2016-03-10T13:37:13Z" id="194840851">Hiya @jimmyjones2 

Did you manage to look into this any further?
</comment><comment author="jimmyjones2" created="2016-03-19T19:24:27Z" id="198774058">@clintongormley Sorry completely forgot about this. A load of testcases were using unquoted JSON, have fixed them and rebased on master. Hope I haven't missed the boat for 5.0!
</comment><comment author="dakrone" created="2016-04-06T17:38:56Z" id="206482372">Thanks @jimmyjones2 this looks great! I will run test and merge it
</comment><comment author="dakrone" created="2016-04-06T20:38:15Z" id="206554257">Committed this to master (5.0), thanks again @jimmyjones2 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix OOM in AbstractXContentParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15350</link><project id="" key="" /><description>closes #15338
</description><key id="121333554">15350</key><summary>Fix OOM in AbstractXContentParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:REST</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T20:37:04Z</created><updated>2016-03-17T13:42:15Z</updated><resolved>2016-03-17T13:42:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-12-10T13:15:20Z" id="163611128">@nik9000 @jpountz thanks for your reviews. Actually I've been too quick in submitting the fix and forgot to commit the other part in `UpdateRequest`.

I also updated the `for` loop condition to stop once the parser is fully consumed.
</comment><comment author="clintongormley" created="2016-03-10T13:35:26Z" id="194840364">@tlrx are you coming back to this one?
</comment><comment author="jpountz" created="2016-03-17T10:48:49Z" id="197816407">I am good with merging the change as it fixes the OOM issue but I think we need a follow-up PR in order to make list parsing less lenient. For instance it feels wrong to me that we consume tokens until the next END_ARRAY without ensuring that we had a START_ARRAY in the first place.
</comment><comment author="tlrx" created="2016-03-17T13:22:07Z" id="197877271">Thanks @jpountz 

&gt; For instance it feels wrong to me that we consume tokens until the next END_ARRAY without ensuring that we had a START_ARRAY in the first place.

I agree and I think this is the only way to correctly fix this issue... I updated the code according to your comment. Tests passed.
</comment><comment author="jpountz" created="2016-03-17T13:34:48Z" id="197881347">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows service installer doesn't escape file path in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15349</link><project id="" key="" /><description>When installing the Windows service for Elasticsearch 2.x, some part of the batch file breaks when the ES_HOME path contains parentheses. This behavior is not present in 1.7.

Example:

&gt; C:\Program Files (x86)\elasticsearch-2.1.0&gt; bin\service.bat install
&gt; Installing service      :  "elasticsearch-service-x64"
&gt; Using JAVA_HOME (64-bit):  "C:\Program Files\Java\jdk1.8.0_45"
&gt; \elasticsearch-2.1.0/lib/elasticsearch-2.1.0.jar was unexpected at this time.
</description><key id="121331164">15349</key><summary>Windows service installer doesn't escape file path in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">kstachowiak-kcura</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-12-09T20:23:22Z</created><updated>2016-01-21T14:33:47Z</updated><resolved>2016-01-21T14:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T13:31:44Z" id="163936731">@gmarz could you take a look at this please
</comment><comment author="kstachowiak-kcura" created="2015-12-11T18:21:59Z" id="164008932">It looks like this originates from the new if statement around setting ES_CLASSPATH in elasticsearch.in.bat
</comment><comment author="gmarz" created="2015-12-15T04:48:10Z" id="164644260">@kstachowiak-kcura that looks like it may be part of the issue, but the service also fails to start even after properly escaping the path in the bat file.  I'll dig into this further tomorrow.
</comment><comment author="tundrax" created="2015-12-22T04:40:55Z" id="166503597">In my case, the problem occurs when the path where ES is installed contains `)` - closing parantheses. In this case `Program Files (x86)`. Just tested with different patterns, like: `Program Files (x86`, `Program Files x86` all work fine. The closing parentheses is causing the problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plain highlighter's no_match trims trailing characters on short strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15348</link><project id="" key="" /><description>I'm not sure if it also does it incorrectly for longer strings, but here is the reproduction:
https://discuss.elastic.co/t/highlighter-trim-a-field-why/36779
</description><key id="121322285">15348</key><summary>Plain highlighter's no_match trims trailing characters on short strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>adoptme</label><label>bug</label></labels><created>2015-12-09T19:38:07Z</created><updated>2016-11-25T14:57:07Z</updated><resolved>2016-11-25T14:57:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-11-25T14:57:07Z" id="262972647">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Task Management: Add framework for registering and communicating with tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15347</link><project id="" key="" /><description>Adds task manager class and enables all activities to register with the task manager. Currently, the immutable `Transport*Activity` class represents activity itself shared across all requests. This PR adds and an additional structure Task that keeps track of currently running requests and can be used to communicate with these requests using `TransportTaskAction`. 

Related to #15117
</description><key id="121320887">15347</key><summary>Task Management: Add framework for registering and communicating with tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T19:30:28Z</created><updated>2016-01-10T13:53:34Z</updated><resolved>2016-01-05T17:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-11T10:22:41Z" id="163902748">@imotov  I left a bunch of comments and some requests on how to make the change way smaller and way less intrusive. I want those to be addressed before reviewing more of this. I don't think we can go with adding Task and friends everywhere this is just not the way we can expose a feature like this. I am happy to discuss the alternatives but I gave you examples in my comments
</comment><comment author="imotov" created="2015-12-13T23:24:30Z" id="164310107">@s1monw I pushed the changes that you have requests. Could you take another look when you have a chance?
</comment><comment author="s1monw" created="2015-12-15T13:53:25Z" id="164771313">@imotov this is way more digestable... I really liked it and left a bunch of comments! thanks for all the unittests
</comment><comment author="s1monw" created="2015-12-21T22:24:18Z" id="166440282">@imotov  do you have an update on this?
</comment><comment author="imotov" created="2015-12-21T22:31:29Z" id="166441509">@s1monw yes, sorry for the delay, was traveling last week. I am wrapping up with the requested changes, just need to review them one more time, rerun the build to make sure I didn't mess anything up and I will submit changes in the next few hours. Should be ready for you in the morning.
</comment><comment author="imotov" created="2015-12-22T04:23:02Z" id="166502108">@s1monw I pushed the changes.
</comment><comment author="s1monw" created="2015-12-22T08:24:02Z" id="166551662">I left some minor comments - LGTM otherwise
</comment><comment author="nik9000" created="2015-12-22T18:16:06Z" id="166694462">Left minor comments. LGTM as well. I'm excited to get this merged and start integrating with it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score with scrip score multiplies the query score twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15346</link><project id="" key="" /><description>When I use function score that has a script_score and a main query and has the boost_mode as multiplication. The ES calculates the score for the main query and multiplies it to the script_score twice. Once as part of the script_score and once as part of the final score. If you use the explain mode and dig into the scores, you can see the behavior. 
</description><key id="121318308">15346</key><summary>Function score with scrip score multiplies the query score twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keikha</reporter><labels /><created>2015-12-09T19:16:39Z</created><updated>2015-12-10T18:43:41Z</updated><resolved>2015-12-10T12:41:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-12-10T11:00:26Z" id="163579141">Can you please add the query that you use? What does the script look like? Also, what version of elasticsearch are you using?
</comment><comment author="clintongormley" created="2015-12-10T12:41:17Z" id="163598807">Only if you actually include `_score *` in the script and leave the `boost_mode` as `multiply`, but for eg this doesn't do what you describe:

```
PUT t/t/1
{
  "foo": "bar"
}

GET t/_search?explain
{
  "query": {
    "function_score": {
      "query": {
        "match": {
          "foo": "bar"
        }
      },
      "functions": [
        {
          "script_score": {
            "script": "2.0"
          }
        }
      ]
    }
  }
}
```

This sounds like a question that would be better answered in the forums: https://discuss.elastic.co/
</comment><comment author="keikha" created="2015-12-10T18:43:41Z" id="163713689">You are right. 
The confusion came from the fact that "log" function in the script calculates "Ln" not "Log base 10".  And in my tests, the number seemed very close to log*score.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Similarity module is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15345</link><project id="" key="" /><description>There are multiple similarity measures available, but apparently only the default one and BM25 work. I'm trying to use LMDirichlet similarity, creating a simple mapping such as:

```
"mappings": {
"item1": {
"properties": {
"title1": {
"type": "string" } } },
"item2": {
"properties": {
"title2": {
"similarity": "BM25",
"type": "string" } } },
"item3": {
"properties": {
"title3": {
"type": "string",
"similarity":"LMDirichlet"}  } }
}
```

ES ignores the LMDirichlet similarity and just uses the default one. Other similarity modules such as DFR, LMJelinekMercer also have same problem.
</description><key id="121316639">15345</key><summary>Similarity module is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">keikha</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-09T19:06:44Z</created><updated>2016-01-22T11:34:15Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-10T12:36:02Z" id="163598016">You need to configure a custom similarity for all but the Default and BM25 similarities.  You can see how to do so here: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html#configuration
</comment><comment author="clintongormley" created="2015-12-10T13:05:41Z" id="163607221">Reopening: we should complain if using a similarity that requires configuration, instead of just silently accepting it.
</comment><comment author="keikha" created="2015-12-10T15:18:29Z" id="163655565">I agree that a complain would be helpful.

But beside that, LMDirichlet has parameter that has a default value of 2000. It gives the impression that if I don't configure anything, it should use the default value. I don't see why I need configuration if I want to use the default value.
</comment><comment author="keikha" created="2015-12-10T16:33:23Z" id="163679811">Adding the configuration to the index settings helped. But now the scores that I get back are all zero for the LM similarity. Here is the steps to reproduce the problem:
1) Created the index:

{ "settings": { "similarity": { "LMSimilarity": { "type": "LMDirichlet", "mu": 2500 } } }, "mappings": { "item": { "properties": { "title": { "type": "string", "similarity": "LMSimilarity" } } } } }

2) Indexed two documents:

{"title":"This is a test for search similarity when we search by other search options."}
{"title&#8221;:&#8221;Search looks weird when use other search possibilities. Numbers are not clear. Just adding new stuff to make the document longer. Document norm looks weird."}

3) Run a simple query:

{ "explain": "true", "query": { "match": { "title": "search" } } }

If you look at the returned scores, there are multiple weird numbers:
1) The score for all documents is zero
2) The collection probability, a term property that is independent from individual documents, is different for each document. I expect this number to be the same for all documents for a given term.
3) Document norm has a negative value, probably it's the log of another number, but I can't match these numbers to the LM formula. 
</comment><comment author="keikha" created="2015-12-11T16:25:01Z" id="163980708">Since this issue was closed I'll open a new issue with the latest problem.
</comment><comment author="clintongormley" created="2015-12-14T18:19:51Z" id="164516046">Sorry, meant to reopen this.
</comment><comment author="keikha" created="2016-01-19T18:06:19Z" id="172936284">@clintongormley Did you get a chance to look at the LM scoring problem. I'm wondering if you have any suggestion about it or if there is a workaround. 
</comment><comment author="tlmnw" created="2016-01-19T22:46:11Z" id="173012616">@keikha 
I've looked a bit around and I don't think that this a bug related to elasticsearch.
The computation is done by Apache Lucene's [LMDirichletSimilarity](http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/core/src/java/org/apache/lucene/search/similarities/LMDirichletSimilarity.java). If you'll have a look inside the code, you'll see that the computation of the score is done in the _score(BasicStats stats, float freq, float docLen)_-method:

``` java
protected float score(BasicStats stats, float freq, float docLen) {
    float score = stats.getBoost() * (float)(Math.log(1 + freq /
        (mu * ((LMStats)stats).getCollectionProbability())) +
        Math.log(mu / (docLen + mu)));
    return score &gt; 0.0f ? score : 0.0f;
  }
```

Because the stats.getBoost() is in your case always 1.0f, the computation of the score consists of the sum of _term weight_ (the first log-expression) and the _document norm_ (the second log-expression). 
Since the latter is negative and the _term weight_-value not big enough, the score gets negative (or zero).
For one document of your data _docLen_ was 28.44 and _freq_ was 2.0.

In short, I think, that the data you've used for testing is too small to get good statistics calculation or you have to adjust mu.
</comment><comment author="clintongormley" created="2016-01-20T14:52:08Z" id="173226715">@tlmnw many thanks for diving into this and providing the answer. We should still throw an exception when trying to use a similarity that requires configuration without providing said config.
</comment><comment author="keikha" created="2016-01-20T17:17:59Z" id="173280708">@tlmnw @clintongormley Thank you for following it up. 
I had another look and played with smaller mu values. As @tlmnw mentioned they way that the score is calculated it assigns zero to any document that has the term with lower probability than the collection.

I still have concerns about it although I'm not sure if I should mention them here since they are more related to Lucene. I don't know if ES team care about underneath Lucene problems.  

1) When I look at the explained score, there are different values for collection probability. I expect this to be independent of documents. This cause the term weight to have totally weird values.
2) I think it is really bad to not distinguish between two documents even though they don't have enough very high term frequencies. In my case one document is clearly more relevant than the other one, but both of them get score zero. 
</comment><comment author="adrianocrestani" created="2016-01-20T17:26:37Z" id="173289723">@keikha Those issues should probably be raised at java-user@lucene.apache.org mailing list
</comment><comment author="tlmnw" created="2016-01-20T21:28:10Z" id="173364394">@clintongormley 
I've probably misunderstood something, but why should we throw an error if no configuration was given, since all SimilarityProviders have default values? Or should we raise a warning?
</comment><comment author="clintongormley" created="2016-01-21T14:06:31Z" id="173578864">@tlmnw no it may be me who has misunderstood.  I thought that all similarities except default (now classic) and bm25 required config, but i may well be wrong?
</comment><comment author="keikha" created="2016-01-21T15:06:26Z" id="173598796">@tlmnw @clintongormley That was the reason I opened this ticket in this first place. 
If no configuration is provided, ES ignores the similarity module and uses the default one ( Even if you want to use the default parameters). As @clintongormley mentioned, this is not the case for BM25. 
</comment><comment author="tlmnw" created="2016-01-21T19:34:36Z" id="173684393">Okay, got it. I'll try to fix this.
</comment><comment author="tlmnw" created="2016-01-21T20:22:03Z" id="173696952">@keikha 
Which version of elasticsearch do you use?
</comment><comment author="keikha" created="2016-01-21T20:43:32Z" id="173702299">@tlmnw  I tried it with different versions including 1.7 and 2.1. 
</comment><comment author="clintongormley" created="2016-01-22T11:34:15Z" id="173891995">It's true.  This can be seen by doing the following:

```
PUT t
{
  "mappings": {
    "item1": {
      "properties": {
        "title1": {
          "type": "string"
        }
      }
    },
    "item2": {
      "properties": {
        "title2": {
          "similarity": "BM25",
          "type": "string"
        }
      }
    },
    "item3": {
      "properties": {
        "title3": {
          "type": "string",
          "similarity": "LMDirichlet"
        }
      }
    }
  }
}

GET _mapping/field/title*?include_defaults
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove back compat for the `_source` compression options.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15344</link><project id="" key="" /><description>These options have been deprecated before 2.0 so they don't need to be supported
by 3.0.
</description><key id="121310936">15344</key><summary>Remove back compat for the `_source` compression options.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T18:36:01Z</created><updated>2015-12-11T16:01:17Z</updated><resolved>2015-12-11T16:01:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T18:43:17Z" id="163353569">LGTM, can you add a note to the migration guide documentation and label this as breaking?
</comment><comment author="jpountz" created="2015-12-09T18:45:31Z" id="163354100">@dakrone it's already in the 2.0 docs since these settings were removed in 2.0, we only kept the code for backcompat with old indices: https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_20_mapping_changes.html#_compress_and_compress_threshold
</comment><comment author="dakrone" created="2015-12-11T15:03:19Z" id="163957996">@jpountz ahh okay, makes sense, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve network docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15343</link><project id="" key="" /><description>This makes some minor improvements (does not fix all problems!)

It reorders unicast disco in elasticsearch.yml to be right after the network host,
for better locality.

It removes the warning (unreleased) about publish addresses, lets try to really discourage setting
that unless you need to (behind a proxy server). Most people should be fine with `network.host`

Finally it reorganizes the network docs page a bit:

We add a table of 4 "basic" settings at the very beginning:
- network.host
- discovery.zen.ping.unicast.hosts
- http.port
- transport.tcp.port

The first two being the most important, which addresses to bind and talk to, and the other two
being the port numbers.

The rest of the stuff I tried to simplify and reorder under "advanced" headers.

This is just a quick stab, I still think we need more effort into this thing, but we gotta start somewhere.
</description><key id="121309603">15343</key><summary>Improve network docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>docs</label></labels><created>2015-12-09T18:28:17Z</created><updated>2015-12-10T11:02:23Z</updated><resolved>2015-12-10T11:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-09T18:42:31Z" id="163353330">I love the new doc! :D

+1. Just wondering if we should keep the log as debug instead of removing but not a big deal...
</comment><comment author="dakrone" created="2015-12-09T18:44:23Z" id="163353823">LGTM
</comment><comment author="rmuir" created="2015-12-09T18:53:39Z" id="163356201">docs changes are invalid. hard to tell what's wrong... but i think something about one of the tables confuses asciidoc

```
asciidoc: modules.asciidoc: line 88: reading: /home/rmuir/workspace/elasticsearch/docs/reference/modules/network.asciidoc
asciidoc: ERROR: network.asciidoc: line 35: [tabledef-default] total width less than 100%: 0
asciidoc: ERROR: network.asciidoc: line 35: [tabledef-default] missing leading separator: (?msu)((?&lt;!\S)((?P&lt;span&gt;[\d.]+)(?P&lt;op&gt;[*+]))?(?P&lt;align&gt;[&lt;\^&gt;.]{,3})?(?P&lt;style&gt;[a-z])?)?\|
asciidoc: WARNING: network.asciidoc: line 73: table row 1: empty spanned row
```
</comment><comment author="nik9000" created="2015-12-09T19:06:48Z" id="163359597">LGTM if you can get asciidoc happy.
</comment><comment author="rmuir" created="2015-12-09T19:13:11Z" id="163361139">I pushed commits fixing table and added a missing header and grammar fix.

Now you can see a reasonable approximation of the structure when you view the whole file in the review: https://github.com/rmuir/elasticsearch/blob/improve_network_docs/docs/reference/modules/network.asciidoc
</comment><comment author="rmuir" created="2015-12-09T19:31:13Z" id="163366021">ok, i think i am done tweaking the wording for now :)
</comment><comment author="nik9000" created="2015-12-09T19:40:54Z" id="163368674">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add more tests to network service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15342</link><project id="" key="" /><description>Follow up for #15340

We test that bind with wilcard IP + fixed IP it raises an exception
We test binding multiple IPs
</description><key id="121299212">15342</key><summary>add more tests to network service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T17:34:49Z</created><updated>2015-12-30T17:00:59Z</updated><resolved>2015-12-09T22:16:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-09T17:35:29Z" id="163334310">@rmuir I created that on 2.x for now as I was on that branch but I'll obviously port that to master once accepted!
</comment><comment author="rmuir" created="2015-12-09T18:34:09Z" id="163351245">Looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[FEATURE REQUEST]: Standalone indexer for very large bulk indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15341</link><project id="" key="" /><description>## The problem

Indexing (or reindexing) large amount of data (10xTB) it is a very painful process. Some of the index level changes requires a re-indexing of the data and there is no other solution that go through a full index reload. In very large clusters or very large amount of data this is a huge burden.
## The proposed solution

Most people who use ElasticSearch have another source of the data. This it can be another database such as Cassandra or raw documents stores in deep storage such as S3 or HDFS.
When processing several TB of data it is common to use BigData solutions such as Hadoop and Spark.
Loads of other frameworks which work on top of these allow arbitrary data processing.
Now let's assume that the ElasticSearch indexing, as currently available via the REST API, was also available as separate standalone function, this could be used as a lambda over the data to index.

The standalone indexer would need only a index name or configuration object and some mappings or index templates,
and accept a document in the form of a JSON object or JSON string.
Such indexer combined with the power of Hadoop &amp; Spark could be used to create the Lucene indexes in the exact same way of a full ElasticSearch cluster, but without having to worry about ELS installations, configuration, replication, cluster load etc.

In fact in such solution a query capability wouldn't be necessary, and the standalone indexer could focus on building the indices without having to merge segments, refresh indexes etc.
The standalone indexer would require that all records for a particular index must be indexed by the same instance of the indexer. This can be easily achieved by a logical GROUP-BY the index name in the scripts which process the data and deliver all records to the reducers with the specific index.

Once the indexing is complete a finalisation function such as `optimize()` could merge/compact the index to be performant.
Now to make the newly created indices available to the cluster we could simply use the ElasticSearch repository function. For example each index could be uploaded to a deep storage (S3 or HDFS) and then restored by the main ELS cluster from the same location.
## Advantages

Advantages are:
- very high indexing throughput via high parallelism of Hadoop/Spark
- no need to setup ELS in the Hadoop cluster
- no HTTP requests required for the indexing path
- no cluster overhead while indexing

Current solutions which involve the installation of a ELS node tend to be brittle and fail very easily.

Bruno
</description><key id="121296953">15341</key><summary>[FEATURE REQUEST]: Standalone indexer for very large bulk indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">BrunoBonacci</reporter><labels><label>:CRUD</label><label>feedback_needed</label></labels><created>2015-12-09T17:23:56Z</created><updated>2016-02-14T18:56:56Z</updated><resolved>2016-02-14T18:56:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T16:20:54Z" id="164482911">Sorry @BrunoBonacci but this is a complete non-starter. We'd have to rip out so much of the code that makes Elasticsearch Elasticsearch, then deal with the multitude of bugs that would crop up in such a refactoring, then deal with all of the feature requests for people wanting to add back things they need but aren't supported in this locked down indexer.

We have customers doing 1m docs/s.  There are plenty of things you can do to increase indexing throughput in ES, such as disabling refreshes, or disabling replicas until the data is fully indexed.

I'd much rather focus on this comment:

&gt; Current solutions which involve the installation of a ELS node tend to be brittle and fail very easily.

Why?  What can be done to improve this situation instead?
</comment><comment author="BrunoBonacci" created="2015-12-14T17:14:35Z" id="164498867">I understand the challenges behind a Standalone Indexer, I would assume that most of the are derived by the concurrency management, which in the case of this type of indexer, as described above, it wouldn't be necessary.
Every indexer would fall in the Single Writer Principle, so no need to handle queues of requests etc, and document index operation is executed in a single thread environment (reducer).
The idea is just to wrap lucene with the ELS substratum which produces indices which are in the same format as the online version.

&gt; We have customers doing 1m docs/s. 
&gt; On how many nodes? and what type of hardware? what size of documents?

The issue with having hadoop indexing directly into ELS via the API are:
- online users request are impacted by high volume high throughput indexing activities (eg: you don't wan't your production users not being able to query the cluster because the capacity is fully utilised by the indexers). Ensuring that such activities don't have negative impact on the live usage it is paramount.
- alternatively need to setup dedicated cluster for indexing (which is overhead)
- even in this case the overall throughput is still much less that what it can be achieved with a standalone indexer (no remote call, just I/O bound indexing)
-  a standalone indexer wouldn't require to "rip things apart", but just to provide a wrapper class inside the same elasticsearch.jar distribution. The extent of refactoring would be limited to the ability of the wrapper to reach the information it needs to index the same field with the same format into lucene.
  - I think that conceptually having the indexer producing fully optimized immutable segments, which are uploaded to a repository (S3) and having the cluster just to restore them as a normal backup/restore operation it is a much nicer design that having to hammer a cluster and contend the resources with users who have latency sensitive queries. It's just bytes copying at this point, and it is a nice separation of writes from reads.

Other systems, such as druid.io, provide similar feature and it is much easier to re-index several TB of data with new mapping configuration or index/shard configuration.
I'm sure that if you ask your larger users they will agree that such feature would be a great feature for them too.
</comment><comment author="clintongormley" created="2016-02-14T18:56:56Z" id="183949646">This isn't something we have any plans to support, so I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>network.bind_host does not support arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15340</link><project id="" key="" /><description>When you define in `elasticsearch.yml`:

``` yml
network.bind_host: "NONEXSISTINGADDRESS"
```

It fails as expected:

```
Exception in thread "main" BindTransportException[Failed to resolve host [null]]; nested: UnknownHostException[NONEXSISTINGADDRESS: unknown error];
Likely root cause: java.net.UnknownHostException: NONEXSISTINGADDRESS: unknown error
    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)
    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)
    at java.net.InetAddress.getAllByName0(InetAddress.java:1276)
    at java.net.InetAddress.getAllByName(InetAddress.java:1192)
    at java.net.InetAddress.getAllByName(InetAddress.java:1126)
    at org.elasticsearch.common.network.NetworkUtils.getAllByName(NetworkUtils.java:203)
    at org.elasticsearch.common.network.NetworkService.resolveInetAddress(NetworkService.java:200)
    at org.elasticsearch.common.network.NetworkService.resolveBindHostAddress(NetworkService.java:111)
    at org.elasticsearch.transport.netty.NettyTransport.bindServerBootstrap(NettyTransport.java:430)
    at org.elasticsearch.transport.netty.NettyTransport.doStart(NettyTransport.java:319)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:68)
    at org.elasticsearch.transport.TransportService.doStart(TransportService.java:170)
    at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:68)
    at org.elasticsearch.node.Node.start(Node.java:254)
    at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:221)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:287)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

When you wrap it in an array, it's totally ignored:

``` yml
network.bind_host: ["NONEXSISTINGADDRESS"]
```

```
[2015-12-09 18:00:29,443][INFO ][transport                ] [Collector] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[fe80::1]:9300}, {[::1]:9300}
```

It means that you can't define a list of bind_host as:

``` yml
network.bind_host: ["_eth0:ipv4_", "_lo:ipv4_"]
```

From discussion: https://discuss.elastic.co/t/es-2-1-only-bind-on-localhost/36720
</description><key id="121293095">15340</key><summary>network.bind_host does not support arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label></labels><created>2015-12-09T17:04:43Z</created><updated>2015-12-09T17:22:20Z</updated><resolved>2015-12-09T17:18:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-09T17:15:03Z" id="163328768">This was never supported, we added it for 2.2: #13954

Although IMO, separately the real problem here is that the wrong data type was passed in for a setting, since that one did not support arrays, and nothing gave an error about that
</comment><comment author="dadoonet" created="2015-12-09T17:18:16Z" id="163329520">Thanks! I was wondering why it was correct in code and when I was running a unit test...
Was on 2.x branch! :(

I'm closing this then. I think your comment is already covered by another issue IIRC.
</comment><comment author="dadoonet" created="2015-12-09T17:19:26Z" id="163329815">BTW I think we don't have unit test for multiple addresses in 2.x. I just looked at `NetworkServiceTests`.
Do you think I should send a PR to add this test?

``` java
public void testBindMultipleAddresses() throws Exception {
    NetworkService service = new NetworkService(Settings.EMPTY);
    InetAddress[] addresses = service.resolveBindHostAddresses(new String[]{"127.0.0.1", "127.0.0.2"});
    assertThat(addresses.length, is(2));
}
```
</comment><comment author="rmuir" created="2015-12-09T17:22:20Z" id="163330567">as long as it does not actually bind or rely on local configuration (e.g. number of interfaces) its a good idea.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows 7 Java 1.8 Elastic Search 2.1 Unsupported major.minor version 51.0 Error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15339</link><project id="" key="" /><description>Hi

My apologies if I am doing something dumb but I have been trying to get this working for a while now. 

I have downloaded and unpacked elasticsearch 2.1 (ZIP from elastic.co) and get the error below. I have reinstalled Java 1.8 and cleaned up any old installations but still get the same error. Any suggestions? I have repeated the download and install of both ElasticSearch and Java a couple of times to be sure.

JAVA_HOME set to C:\Progra~1\Java\jdk1.8.0_25

Thanks in advance

CM

Microsoft Windows [Version 6.1.7601]
Copyright (c) 2009 Microsoft Corporation.  All rights reserved.

C:\WorkSpace\elasticsearch-2.1.0\bin&gt;java -version
java version "1.8.0_25"
Java(TM) SE Runtime Environment (build 1.8.0_25-b18)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)

C:\WorkSpace\elasticsearch-2.1.0\bin&gt;elasticsearch.bat
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClassCond(ClassLoader.java:630)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:614)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:305)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:246)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch.  Program will exit.
</description><key id="121290707">15339</key><summary>Windows 7 Java 1.8 Elastic Search 2.1 Unsupported major.minor version 51.0 Error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CMcMe</reporter><labels><label>:Exceptions</label><label>discuss</label><label>enhancement</label></labels><created>2015-12-09T16:54:03Z</created><updated>2015-12-11T13:44:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-09T19:24:39Z" id="163364362">&gt; Unsupported major.minor version 51.0

This means you are trying to run 2.1.0 (which requires java 7) with java 6 or something older than that. I really think we should do a better job of catching these errors and reporting them with less noise so I'm not going to just close this, though its probably a duplicate and something that you should have asked on discuss.elastic.co.

To debug which version of Java Elasticsearch is picking up you have to hack around in the bat file.
</comment><comment author="CMcMe" created="2015-12-10T10:14:16Z" id="163565338">Thanks for the reply and my apologies, it looks like you are right. Whilst 'java -version' is returning 1.8 it looks like hidden away within my machine is some odd configuration that means 1.6 is being detected by elastic search. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OutOfMemoryError[Java heap space] when executing update with bad fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15338</link><project id="" key="" /><description>Hi!
Running elasticsearch 2.1.0 I encountered weird behavior when trying to execute update. Tried to execute this query:

```
POST my-index/my-type/my_id/_update
{"doc": {"my_field_name": "blabla"}, "doc_as_upsert": true, "fields": "_source"}
```

and got **OutOfMemoryError[Java heap space]** (it took Elasticsearch a few seconds to answer).

As I understand it, the format of the query above is wrong. The value of "fields" should be list, not string (indeed when correcting the query the update worked fine). In a case like that I would expect to get some kind of an error indicating my request is not valid but It seems like some internal error have happen in elasticsearch and resulted in OutOfMemoryError.

This is the stack trace:
`java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2219)
        at java.util.ArrayList.grow(ArrayList.java:242)
        at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:216)
        at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:208)
        at java.util.ArrayList.add(ArrayList.java:440)
        at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readList(AbstractXContentParser.java:290)
        at org.elasticsearch.common.xcontent.support.AbstractXContentParser.readList(AbstractXContentParser.java:253)
        at org.elasticsearch.common.xcontent.support.AbstractXContentParser.list(AbstractXContentParser.java:218)
        at org.elasticsearch.action.update.UpdateRequest.source(UpdateRequest.java:672)
        at org.elasticsearch.rest.action.update.RestUpdateAction.handleRequest(RestUpdateAction.java:101)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:348)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
`
</description><key id="121279295">15338</key><summary>OutOfMemoryError[Java heap space] when executing update with bad fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">anyatch</reporter><labels><label>:REST</label><label>bug</label><label>low hanging fruit</label><label>v2.4.0</label></labels><created>2015-12-09T16:03:43Z</created><updated>2016-03-17T13:42:06Z</updated><resolved>2016-03-17T13:42:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T15:28:31Z" id="164467106">thanks for reporting @anyatch - nice catch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove XContentParser.estimatedNumberType().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15337</link><project id="" key="" /><description>The goal of this method is to know whether the xcontent impl knows how to
differenciate floats from doubles or longs from ints or if it's just guessing.
However, all implementations return true (which is correct for yaml and json,
but cbor and smile should be able to differenciate). I first tried to implement
this method correctly but it raised many issues because eg. most impls write a
long as an integer when it is small enough. So I suggest that we remove this
method and just treat cbor and smile like yaml and json, which is already what
is happening today anyway.
</description><key id="121247049">15337</key><summary>Remove XContentParser.estimatedNumberType().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:REST</label><label>non-issue</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T14:06:32Z</created><updated>2015-12-11T13:40:14Z</updated><resolved>2015-12-10T09:09:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-09T19:55:37Z" id="163372412">&gt; cbor and smile

Doing anything so fancy for them is probably a trap anyway because so few people use them intentionally. Its just not code that would get much practice.

LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resolve index routing conflicts with priorities</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15336</link><project id="" key="" /><description>Separates routing and parent in all documentrequest in order to be able to distinguish an explicit routing value from a parent routing.

The final value for the routing is returned by MetaData.resolveIndexRouting which
resolves conflicts between routing, parent routing and alias routing with the following rules:
- If the routing is specified in the request then parent routing and alias routing are ignored.
- If the routing is not specified:
  - The parent routing is ignored if there is an alias routing that matches the request.
  - Otherwise the parent routing is applied.
    Fixes #3068
</description><key id="121243542">15336</key><summary>Resolve index routing conflicts with priorities</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Aliases</label><label>enhancement</label><label>review</label></labels><created>2015-12-09T13:48:12Z</created><updated>2015-12-10T15:08:06Z</updated><resolved>2015-12-10T15:08:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-09T18:34:21Z" id="163351291">I left some comments, mainly about back compat, but otherwise it looks to me like the right way to fix the problem!
</comment><comment author="jimczi" created="2015-12-10T08:42:34Z" id="163534608">Thanks @jpountz for the quick review (again!), I'll add the back compat.
</comment><comment author="jimczi" created="2015-12-10T13:47:49Z" id="163622967">After chatting with @jpountz we realized that adding the backward compatibility breaks the behavior. If a request comes from a node with a version older than 2.2 then we don't have access to the parent and we go back into the same situation as before: we don't know if the routing comes from the parent or the user.   In such case we don't know if we must override the routing with the alias routing (in 2.2 the alias routing overrides the routing only if it was set from the parent). Bottom line is that the fix cannot be properly handle in 2.x and should be done only in master where we don't need to care about backward compat. 
</comment><comment author="jpountz" created="2015-12-10T13:51:44Z" id="163624285">That works for me. This is probably an issue that existed for a very long time so it can wait a bit more. Then you can ignore my comments about back compat since 3.x doesn't need to be able to talk with 2.x.
</comment><comment author="jimczi" created="2015-12-10T15:08:02Z" id="163652911">Close the pull request and open a new one for master here: https://github.com/elastic/elasticsearch/pull/15371
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unsupported methods on REST endpoints should respond with status code 405</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15335</link><project id="" key="" /><description>Today, if a request is executed against an endpoint with a method in the request line that that endpoint does not support, Elasticsearch responds with a response like:

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "illegal_argument_exception",
      "reason" : "No feature for name [_forcemerge]"
    } ],
    "type" : "illegal_argument_exception",
    "reason" : "No feature for name [_forcemerge]"
  },
  "status" : 400
}
```

This is confusing for users*, semantically wrong, and contrary to the HTTP spec. Instead, Elasticsearch should respond with [HTTP status code 405](https://tools.ietf.org/html/rfc2616#section-10.4.6) (Method Not Allowed) and include in the Allow header a list of methods that the endpoint does support.

*: [Googling "no feature for name elasticsearch"](https://www.google.com/search?q=no+feature+for+name+elasticsearch) just leads to pages describing issues when an [index is missing](https://github.com/elastic/elasticsearch/blob/20bff773eda14804a77f6b76d98d8ea63ba6f4b9/core/src/main/java/org/elasticsearch/action/admin/indices/get/GetIndexRequest.java#L75-L82).
</description><key id="121235682">15335</key><summary>Unsupported methods on REST endpoints should respond with status code 405</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:REST</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-09T13:14:51Z</created><updated>2016-05-19T15:28:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T16:16:59Z" id="163310807">+1
</comment><comment author="nik9000" created="2015-12-09T19:51:44Z" id="163371445">+1
</comment><comment author="jbertouch" created="2016-04-05T04:48:43Z" id="205643086">@jasontedor Made a start on this, but it still needs unit tests. Are there existing http endpoint exception/http status code tests where new tests could be included? Had a quick look and couldn't find an appropriate home for them.

Also noticed that the `RestController#executeHandler` didn't have a complete implementation of the [http OPTIONS method](https://tools.ietf.org/html/rfc2616#page-52), so I cleaned it up. The prior code just returned 200 OK for all requests, and didn't included allowed methods for the endpoint as recommended in the spec.
</comment><comment author="dakrone" created="2016-04-05T14:45:57Z" id="205839643">&gt; Are there existing http endpoint exception/http status code tests where new tests could be included? Had a quick look and couldn't find an appropriate home for them.

Usually this would go into the YAML REST tests, but for a special case like this you need a bit more powerful matching than that provides, so probably a test subclassing `ESSingleNodeTestCase` that uses `java.net.URLConnection` would work?
</comment><comment author="jbertouch" created="2016-04-05T15:07:16Z" id="205849581">Thanks @dakrone 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce a common base response class to all single doc write ops</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15334</link><project id="" key="" /><description>IndexResponse, DeleteResponse and UpdateResponse share some logic. This can be unified to a single DocWriteResponse base class. On top, some replication actions are now not about write operations anymore. This commit renames ActionWriteResponse to ReplicationResponse

Last some toXContent is moved from the Rest layer to the actual response classes, for more code re-sharing.

This is ported over from the `feature/seq_no` branch to make maintaining that branch simpler
</description><key id="121224222">15334</key><summary>Introduce a common base response class to all single doc write ops</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T12:15:40Z</created><updated>2015-12-10T14:17:52Z</updated><resolved>2015-12-10T14:17:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-09T12:15:55Z" id="163207465">@ywelsch can you please take a look?
</comment><comment author="ywelsch" created="2015-12-09T14:38:08Z" id="163265775">Minor comments, o.w. LGTM
</comment><comment author="bleskes" created="2015-12-10T07:44:31Z" id="163525641">@ywelsch @martijnvg I addressed all your feedback and rebased on master (there was a big change to replication action there). Can you take another look?
</comment><comment author="ywelsch" created="2015-12-10T09:26:42Z" id="163544406">More lines removed than added -&gt; I like this.
Left two suggestions, o.w. LGTM
</comment><comment author="martijnvg" created="2015-12-10T10:18:08Z" id="163566790">Good stuff. LGTM
</comment><comment author="bleskes" created="2015-12-10T12:50:36Z" id="163602077">@ywelsch pushed another round with your suggestions. 
</comment><comment author="ywelsch" created="2015-12-10T12:56:24Z" id="163603351">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to recover shards after disk is full</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15333</link><project id="" key="" /><description>Today, the disk got full and ElasticSearch is not able to go back again. Isn't there a built-in system that prevents such failures. I agree that we should be monitoring the hard space and not let this happen in first place, but some times things happen.

My setup is a single node at present. Using ES 2.1.0, which was supposed to have this fix.

I don't see a clear way to recover the node. A post at https://t37.net/how-to-fix-your-elasticsearch-cluster-stuck-in-initializing-shards-mode.html seemed to help, but still few indices got corrupted and I have no way to recovering them.

At the end, I ended up deleted the indices, but that's not the way it should be. Such things must be taken care of ultimately. But this is clearly a bug with ES 2.1.0
</description><key id="121213523">15333</key><summary>Failure to recover shards after disk is full</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kpcool</reporter><labels><label>:Recovery</label><label>:Translog</label><label>blocker</label><label>bug</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T11:11:10Z</created><updated>2016-07-29T08:02:02Z</updated><resolved>2015-12-14T17:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-09T11:13:22Z" id="163191243">maybe you can tell us what prevented you from starting up again?
</comment><comment author="kpcool" created="2015-12-09T11:53:33Z" id="163200034">There were 15 indices on the node. Of those 15 indices, 9 indices had issues with their shards and ES status was red.

Issue curl -XGET http://localhost:9200/_cat/shards command, listed 52 shards UNASSIGNED and 4 shards in INITIALIZING status.

I issued reroute command (localhost:9200/_cluster/reroute) to move UNASSIGNED to force shard allocation. 

However, the shards that were in INITIALIZING status stay there. The CPU usage was 100% (8-cores busy) for more than 4 hours, before I gave up and started deleting all indices that were causing the problem. Data was about 50GB and 6 Million records.

Even issuing systemctl stop elasticsearch.service took forever.

Is this what you were looking for, if not let me know what you are looking for and I will reply ASAP
</comment><comment author="s1monw" created="2015-12-09T15:40:08Z" id="163296271">there are lots of open questions, do you have some logs telling why the shards where unassigned? did you just upgrade? Why do you force them to allocate? did you run into any disk space issues?
</comment><comment author="kpcool" created="2015-12-09T15:59:51Z" id="163303480">Yes, the disk got full and then after the issue started happening as ES
stopped responding

Regards,
Ketan

On Dec 9, 2015, at 9:11 PM, Simon Willnauer notifications@github.com
wrote:

there are lots of open questions, do you have some logs telling why the
shards where unassigned? did you just upgrade? Why do you force them to
allocate? did you run into any disk space issues?

&#8212;
Reply to this email directly or view it on GitHub
https://github.com/elastic/elasticsearch/issues/15333#issuecomment-163296271
.
</comment><comment author="clintongormley" created="2015-12-10T12:42:43Z" id="163599063">@kpcool Please could you provide the logs and also answers for the questions asked by @s1monw .  The information you have provided up until now provides no clues at to why the shards were not reassigned, etc.
</comment><comment author="kpcool" created="2015-12-10T13:02:02Z" id="163604699">Here's the log around that time.

```
[2015-12-09 00:00:18,560][ERROR][index.engine             ] [Mister Jip] [topbeat-2015.12.09][3] failed to merge
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:65)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
        at java.nio.channels.Channels.writeFully(Channels.java:101)
        at java.nio.channels.Channels.access$000(Channels.java:61)
        at java.nio.channels.Channels$1.write(Channels.java:174)
        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)
        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)
        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)
        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)
        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)
        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)
        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)
        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)
        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)
        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)
[2015-12-09 00:00:18,997][WARN ][index.engine             ] [Mister Jip] [topbeat-2015.12.09][3] failed engine [already closed by tragic event]
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:65)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
        at java.nio.channels.Channels.writeFully(Channels.java:101)
        at java.nio.channels.Channels.access$000(Channels.java:61)
        at java.nio.channels.Channels$1.write(Channels.java:174)
        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)
        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)
        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)
        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)
        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)
        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)
        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)
        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)
        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)
        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)
[2015-12-09 00:00:19,015][WARN ][indices.cluster          ] [Mister Jip] [[topbeat-2015.12.09][3]] marking and sending shard failed due to [engine failure, reason [already closed by tragic event]]
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:65)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
        at java.nio.channels.Channels.writeFully(Channels.java:101)
        at java.nio.channels.Channels.access$000(Channels.java:61)
        at java.nio.channels.Channels$1.write(Channels.java:174)
        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)
        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)
        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)
        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)
        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)
        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)
        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)
        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)
        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)
        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)
[2015-12-09 00:00:19,015][WARN ][cluster.action.shard     ] [Mister Jip] [topbeat-2015.12.09][3] received shard failed for [topbeat-2015.12.09][3], node[HmS7B_CdRFqPFT1UeUZEfA], [P], v[5], s[INITIALIZING], a[id=3Yn-3bO6QtClvHUDwYnClw], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-09T04:58:37.185Z], details[engine failure, reason [merge failed], failure MergeException[java.io.IOException: No space left on device]; nested: IOException[No space left on device]; ]], indexUUID [rvUixkXqTty2osh3-PMubw], message [engine failure, reason [already closed by tragic event]], failure [IOException[No space left on device]]
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:65)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
        at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
        at java.nio.channels.Channels.writeFully(Channels.java:101)
        at java.nio.channels.Channels.access$000(Channels.java:61)
        at java.nio.channels.Channels$1.write(Channels.java:174)
        at org.apache.lucene.store.FSDirectory$FSIndexOutput$1.write(FSDirectory.java:271)
        at java.util.zip.CheckedOutputStream.write(CheckedOutputStream.java:73)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
        at org.apache.lucene.store.OutputStreamIndexOutput.writeBytes(OutputStreamIndexOutput.java:53)
        at org.apache.lucene.store.RateLimitedIndexOutput.writeBytes(RateLimitedIndexOutput.java:73)
        at org.apache.lucene.store.DataOutput.writeBytes(DataOutput.java:52)
        at org.apache.lucene.util.packed.DirectWriter.flush(DirectWriter.java:86)
        at org.apache.lucene.util.packed.DirectWriter.add(DirectWriter.java:78)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:218)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addNumericField(Lucene50DocValuesConsumer.java:80)
        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesConsumer.addSortedNumericField(Lucene50DocValuesConsumer.java:470)
        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addSortedNumericField(PerFieldDocValuesFormat.java:126)
        at org.apache.lucene.codecs.DocValuesConsumer.mergeSortedNumericField(DocValuesConsumer.java:417)
        at org.apache.lucene.codecs.DocValuesConsumer.merge(DocValuesConsumer.java:236)
        at org.apache.lucene.index.SegmentMerger.mergeDocValues(SegmentMerger.java:150)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:105)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4089)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3664)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:588)
        at org.elasticsearch.index.engine.ElasticsearchConcurrentMergeScheduler.doMerge(ElasticsearchConcurrentMergeScheduler.java:94)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:626)

[2015-12-09 00:00:19,887][WARN ][index.translog           ] [Mister Jip] [topbeat-2015.12.09][0] failed to delete temp file /var/lib/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/0/translog/translog-6857015315422195400.tlog
java.nio.file.NoSuchFileException: /var/lib/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/0/translog/translog-6857015315422195400.tlog
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-09 00:00:24,760][WARN ][cluster.routing.allocation.decider] [Mister Jip] high disk watermark [90%] exceeded on [HmS7B_CdRFqPFT1UeUZEfA][Mister Jip][/var/lib/elasticsearch/DC_Reports/nodes/0] free: 1.3mb[0%], shards will be relocated away from this node
[2015-12-09 00:00:24,760][INFO ][cluster.routing.allocation.decider] [Mister Jip] rerouting shards: [high disk watermark exceeded on one or more nodes]

[2015-12-09 00:00:24,851][INFO ][rest.suppressed          ] /dealscornerin-50 Params: {index=dealscornerin-50}
[dealscornerin-50] IndexAlreadyExistsException[already exists]
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:168)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:520)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$200(MetaDataCreateIndexService.java:97)
        at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:241)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-09 00:00:54,241][ERROR][marvel.agent             ] [Mister Jip] background thread had an uncaught exception
ElasticsearchException[failed to flush exporter bulks]
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)
        at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)
        at java.lang.Thread.run(Thread.java:745)
        Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution, only the first 100 failures are printed:
[0]: index [.marvel-es-2015.12.09], type [index_recovery], id [AVGFHCn_dr-UG15JaoIa], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[1]: index [.marvel-es-2015.12.09], type [indices_stats], id [AVGFHCn_dr-UG15JaoIb], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[2]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[3]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[4]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[5]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[6]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[7]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[8]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[9]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[10]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.04:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[11]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.04:0:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[12]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[13]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[14]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[15]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[16]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[17]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[18]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:topbeat-2015.12.03:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[19]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:_na:topbeat-2015.12.03:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
--------------Similar logs--------------
[98]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:dealscornerin-49:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]
[99]: index [.marvel-es-2015.12.09], type [shards], id [nbwEWrIlSBWjVgm32O2hAA:HmS7B_CdRFqPFT1UeUZEfA:packetbeat-2015.12.03:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@3e3d36d4]]]
                at org.elasticsearch.marvel.agent.exporter.local.LocalBulk.flush(LocalBulk.java:114)
                at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:101)
                ... 3 more
[2015-12-09 00:00:55,213][WARN ][cluster.routing.allocation.decider] [Mister Jip] high disk watermark [90%] exceeded on [HmS7B_CdRFqPFT1UeUZEfA][Mister Jip][/var/lib/elasticsearch/DC_Reports/nodes/0] free: 20kb[3.8E-5%], shards will be relocated away from this node
[2015-12-09 00:01:04,257][DEBUG][action.admin.indices.stats] [Mister Jip] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][4], node[HmS7B_CdRFqPFT1UeUZEfA], [P], v[5], s[INITIALIZING], a[id=n5bBcfxdS7ey8IpgEyxwzA], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-09T04:58:37.227Z], details[engine failure, reason [merge failed], failure MergeException[java.io.IOException: No space left on device]; nested: IOException[No space left on device]; ]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][4]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][4]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-09 00:01:04,257][DEBUG][action.admin.indices.stats] [Mister Jip] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][2], node[HmS7B_CdRFqPFT1UeUZEfA], [P], v[33], s[INITIALIZING], a[id=hsMorSXnRYCQa28IlkksYQ], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-09T04:58:37.227Z], details[engine failure, reason [already closed by tragic event], failure IOException[No space left on device]]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
```
</comment><comment author="clintongormley" created="2015-12-10T13:09:34Z" id="163609853">OK, so here the disk is full.  What happened in the logs after you cleared out space on the disk?
</comment><comment author="kpcool" created="2015-12-11T04:30:37Z" id="163836623">Here's the log when I tried to start the ES after shutting it down:

```
[2015-12-09 01:53:35,386][WARN ][bootstrap                ] If you are logged in interactively, you will have to re-login for the new limits to take effect.
[2015-12-09 01:53:35,632][INFO ][node                     ] [Maxam] version[2.1.0], pid[5742], build[72cd1f1/2015-11-18T22:40:03Z]
[2015-12-09 01:53:35,632][INFO ][node                     ] [Maxam] initializing ...
[2015-12-09 01:53:36,167][INFO ][plugins                  ] [Maxam] loaded [license, marvel-agent], sites [kopf]
[2015-12-09 01:53:36,219][INFO ][env                      ] [Maxam] using [1] data paths, mounts [[/home (/dev/mapper/centos-home)]], net usable_space [826.2gb], net total_space [872.6gb], spins? [possibly], types [xfs]
[2015-12-09 01:53:38,666][INFO ][node                     ] [Maxam] initialized
[2015-12-09 01:53:38,666][INFO ][node                     ] [Maxam] starting ...
[2015-12-09 01:53:38,877][INFO ][transport                ] [Maxam] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
[2015-12-09 01:53:38,897][INFO ][discovery                ] [Maxam] DC_Reports/ywHqZlB2Ty6FKboZPgRoZQ
[2015-12-09 01:53:41,926][INFO ][cluster.service          ] [Maxam] new_master {Maxam}{ywHqZlB2Ty6FKboZPgRoZQ}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-12-09 01:53:41,939][INFO ][http                     ] [Maxam] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}
[2015-12-09 01:53:41,940][INFO ][node                     ] [Maxam] started
[2015-12-09 01:53:46,499][INFO ][license.plugin.core      ] [Maxam] license [07b70bf8-cc41-45d7-900c-67a16d05b960] - valid
[2015-12-09 01:53:46,500][ERROR][license.plugin.core      ] [Maxam]
#
# License will expire on [Thursday, December 31, 2015]. If you have a new license, please update it.
# Otherwise, please reach out to your support contact.
#
# Commercial plugins operate with reduced functionality on license expiration:
# - marvel
#  - The agent will stop collecting cluster and indices metrics
[2015-12-09 01:53:47,706][INFO ][gateway                  ] [Maxam] recovered [27] indices into cluster_state
[2015-12-09 01:53:48,164][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][3] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/3/translog/translog-4819800625171304865.tlog
java.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/3/translog/translog-4819800625171304865.tlog
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-09 01:53:48,172][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][2] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/2/translog/translog-4092628000966967177.tlog
java.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/2/translog/translog-4092628000966967177.tlog
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-09 01:53:48,172][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][1] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/1/translog/translog-1515358772559515929.tlog
java.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/1/translog/translog-1515358772559515929.tlog
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-09 01:53:48,164][WARN ][index.translog           ] [Maxam] [topbeat-2015.12.09][4] failed to delete temp file /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/4/translog/translog-7914277937547324566.tlog
java.nio.file.NoSuchFileException: /home/elkuser/elasticsearch/DC_Reports/nodes/0/indices/topbeat-2015.12.09/4/translog/translog-7914277937547324566.tlog
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
        at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
        at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
        at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
        at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
        at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
        at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
        at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
        at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
        at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
        at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
        at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-12-09 01:53:48,819][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][1], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=beI3ZtSZRLSjmp382hpjGA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][1]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][1]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-09 01:53:48,827][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][4], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=ibydvhMyTG-8uFU_y2Gx1g], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][4]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][4]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-09 01:53:48,835][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][3], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=bOgSHC15TWakUYPIMhEz7A], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][3]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][3]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-09 01:53:48,839][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][2], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=QqLsO7WMRn-P24LXF1LuiQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-09 01:53:49,020][DEBUG][action.admin.indices.stats] [Maxam] [indices:monitor/stats] failed to execute operation for shard [[topbeat-2015.12.09][2], node[ywHqZlB2Ty6FKboZPgRoZQ], [P], v[3], s[INITIALIZING], a[id=QqLsO7WMRn-P24LXF1LuiQ], unassigned_info[[reason=CLUSTER_RECOVERED], at[2015-12-09T06:53:42.007Z]]]
[topbeat-2015.12.09][[topbeat-2015.12.09][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: [topbeat-2015.12.09][[topbeat-2015.12.09][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
        at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
        at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
        at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
        at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
        ... 7 more
[2015-12-09 01:54:49,046][ERROR][marvel.agent             ] [Maxam] background thread had an uncaught exception
ElasticsearchException[failed to flush exporter bulks]
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)
        at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)
        at java.lang.Thread.run(Thread.java:745)
        Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution, only the first 100 failures are printed:
[0]: index [.marvel-es-2015.12.09], type [index_recovery], id [AVGFhHRp_6d18XbNgIBf], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[1]: index [.marvel-es-2015.12.09], type [indices_stats], id [AVGFhHRp_6d18XbNgIBg], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[2]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[3]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[4]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[5]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[6]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[7]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[8]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[9]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[10]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[11]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.04:0:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[12]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[13]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[14]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[15]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[16]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:3:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[17]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:3:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[18]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:2:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[19]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:2:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[20]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:0:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[21]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.03:0:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[22]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:1:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[23]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:1:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[24]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:4:p], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
[25]: index [.marvel-es-2015.12.09], type [shards], id [KZr_5qQeRbiay0_pdQRUjw:_na:topbeat-2015.12.08:4:r], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
```
</comment><comment author="s1monw" created="2015-12-11T10:28:51Z" id="163903835">with your last log I can't see exceptions that indicate that recovery failed. Does the cluster come back up or do you get stuck in recoveries? The `failed to delete temp file /home/elkuser/elasticsearch/...` warn logs are annoying but harmless and fixed already. 
</comment><comment author="kpcool" created="2015-12-11T12:00:36Z" id="163922412">Here's the log where there's unhandled exception.

```
[2015-12-09 01:54:49,046][ERROR][marvel.agent             ] [Maxam] background thread had an uncaught exception
ElasticsearchException[failed to flush exporter bulks]
        at org.elasticsearch.marvel.agent.exporter.ExportBulk$Compound.flush(ExportBulk.java:104)
        at org.elasticsearch.marvel.agent.exporter.ExportBulk.close(ExportBulk.java:53)
        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:201)
        at java.lang.Thread.run(Thread.java:745)
        Suppressed: ElasticsearchException[failed to flush [default_local] exporter bulk]; nested: ElasticsearchException[failure in bulk execution, only the first 100 failures are printed:
[0]: index [.marvel-es-2015.12.09], type [index_recovery], id [AVGFhHRp_6d18XbNgIBf], message [UnavailableShardsException[[.marvel-es-2015.12.09][0] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@1a525917]]
```

Also, the cluster stuck in recovery for more than 4 hours, before I gave up and started removing indices giving problem. Basically, shards were in UNASSIGNED state and some were in INITIALIZING state.

I can upload the whole log file which about 42MB if that would help (for entire day).
</comment><comment author="s1monw" created="2015-12-11T12:56:30Z" id="163930894">&gt; I can upload the whole log file which about 42MB if that would help (for entire day).

please

&gt; Also, the cluster stuck in recovery for more than 4 hours, before I gave up and started removing indices giving problem. Basically, shards were in UNASSIGNED state and some were in INITIALIZING state.

I can't see why this is happening. so the logs would be awesome
</comment><comment author="kpcool" created="2015-12-12T03:59:06Z" id="164107977">[dc20151208.tar.gz](https://github.com/elastic/elasticsearch/files/60052/dc20151208.tar.gz)
[dc.tar.gz](https://github.com/elastic/elasticsearch/files/60050/dc.tar.gz)

dc20151208 - was when the disk was not full but about to get full
dc.tar.gz - is when the disk was full and es couldn't initialize all shards.
</comment><comment author="s1monw" created="2015-12-12T22:48:27Z" id="164199971">the interesting exceptions are here:

```
Caused by: [packetbeat-2015.12.09][[packetbeat-2015.12.09][1]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x203a77f9, got: 0x2c22706f];
    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:254)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:175)
    ... 11 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x203a77f9, got: 0x2c22706f];
    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1636)
    at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:299)
    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:290)
    at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
    ... 12 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x203a77f9, got: 0x2c22706f]
    at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1593)
    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1626)
    ... 17 more
```

I still need to investigate what's going on but can you tell me what system you are running this on? Is this a local machine or a cloud machine? I am also curious what filesystem you are using?
</comment><comment author="kpcool" created="2015-12-14T06:07:28Z" id="164351444">Its a standalone node. 
Machine Info: 16GB DDR3, 1TB HDD , Intel(R) Xeon(R) CPU E3-1245 V2 @ 3.40GHz. 
FileSystem: xfs
CentOS: 7.0 64Bit
Java : "1.8.0_65"
</comment><comment author="s1monw" created="2015-12-14T09:08:18Z" id="164383710">alright I think I found the issue here @kpcool your logfiles brought the conclusion thanks you very much. This is actually a serious issue with our transaction log which basically corrupts itself when you hit a disk-full exception. I will keep you posted on this issue. Thanks for baring with me and helping to figure this out.
</comment><comment author="s1monw" created="2015-12-14T09:12:30Z" id="164384660">What happens here is that when we hit a disk full expection while we are flushing the transaction log we might be able to write a portion of the data but we will try to flush the entire data block over and over again. Yet, in the most of the scenarios the disk-full happens during a merge and that merge will fail and release disk-space. Once that is done we might be able to flush the translog again but we already wrote big chunks of data to disk which are now 1. corrupted and 2. treated as non-existing since our internal offsets haven't advanced. 
</comment><comment author="robcza" created="2016-07-29T08:02:01Z" id="236119173">Encountered this issue on one of the older clusters after disk full issue. Is there any way to recover the index? Losing something from the translog is not a big issue for me.

Here is the expection while starting the node:

```
[2016-07-29 07:48:15,265][WARN ][indices.cluster          ] [Recorder] [[myindex][0]] marking and sending shard failed due to [failed recovery]
[myindex][[myindex][0]] IndexShardRecoveryException[failed to recovery from gateway]; nested: EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:250)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [myindex][[myindex][0]] EngineCreationFailureException[failed to recover from translog]; nested: EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:177)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1509)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1493)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:966)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:938)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:241)
    ... 5 more
Caused by: [myindex][[myindex][0]] EngineException[failed to recover from translog]; nested: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];
    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:240)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:174)
    ... 11 more
Caused by: TranslogCorruptedException[translog corruption while reading from stream]; nested: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266];
    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1717)
    at org.elasticsearch.index.translog.TranslogReader.read(TranslogReader.java:132)
    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.readOperation(TranslogReader.java:296)
    at org.elasticsearch.index.translog.TranslogReader$ReaderSnapshot.next(TranslogReader.java:287)
    at org.elasticsearch.index.translog.MultiSnapshot.next(MultiSnapshot.java:70)
    at org.elasticsearch.index.shard.TranslogRecoveryPerformer.recoveryFromSnapshot(TranslogRecoveryPerformer.java:105)
    at org.elasticsearch.index.shard.IndexShard$1.recoveryFromSnapshot(IndexShard.java:1578)
    at org.elasticsearch.index.engine.InternalEngine.recoverFromTranslog(InternalEngine.java:238)
    ... 12 more
Caused by: TranslogCorruptedException[translog stream is corrupted, expected: 0x88b7b1d6, got: 0x2c202266]
    at org.elasticsearch.index.translog.Translog.verifyChecksum(Translog.java:1675)
    at org.elasticsearch.index.translog.Translog.readOperation(Translog.java:1707)
    ... 19 more
```

These are the contents of the index translog directory:

```
elasticsearch/nodes/0/indices/myindex/0/translog# ls
translog-1445516620591.ckp  translog-1445516623424.ckp   translog-1445516631010.tlog  translog-1445516631019.tlog  translog-1445516631028.tlog  translog-1445516631037.tlog  translog-1445516631046.tlog
translog-1445516620592.ckp  translog-1445516624605.ckp   translog-1445516631011.ckp   translog-1445516631020.ckp   translog-1445516631029.ckp   translog-1445516631038.ckp   translog-1445516631047.ckp
translog-1445516620593.ckp  translog-1445516624606.ckp   translog-1445516631011.tlog  translog-1445516631020.tlog  translog-1445516631029.tlog  translog-1445516631038.tlog  translog-1445516631047.tlog
translog-1445516620594.ckp  translog-1445516624607.ckp   translog-1445516631012.ckp   translog-1445516631021.ckp   translog-1445516631030.ckp   translog-1445516631039.ckp   translog-1445516631048.ckp
translog-1445516620595.ckp  translog-1445516624608.ckp   translog-1445516631012.tlog  translog-1445516631021.tlog  translog-1445516631030.tlog  translog-1445516631039.tlog  translog-1445516631048.tlog
translog-1445516620596.ckp  translog-1445516624609.ckp   translog-1445516631013.ckp   translog-1445516631022.ckp   translog-1445516631031.ckp   translog-1445516631040.ckp   translog-1445516631049.ckp
translog-1445516620790.ckp  translog-1445516624610.ckp   translog-1445516631013.tlog  translog-1445516631022.tlog  translog-1445516631031.tlog  translog-1445516631040.tlog  translog-1445516631049.tlog
translog-1445516621043.ckp  translog-1445516624611.ckp   translog-1445516631014.ckp   translog-1445516631023.ckp   translog-1445516631032.ckp   translog-1445516631041.ckp   translog-1445516631050.ckp
translog-1445516621044.ckp  translog-1445516624612.ckp   translog-1445516631014.tlog  translog-1445516631023.tlog  translog-1445516631032.tlog  translog-1445516631041.tlog  translog-1445516631050.tlog
translog-1445516621237.ckp  translog-1445516625986.ckp   translog-1445516631015.ckp   translog-1445516631024.ckp   translog-1445516631033.ckp   translog-1445516631042.ckp   translog-1445516631051.ckp
translog-1445516621238.ckp  translog-1445516628096.ckp   translog-1445516631015.tlog  translog-1445516631024.tlog  translog-1445516631033.tlog  translog-1445516631042.tlog  translog-1445516631051.tlog
translog-1445516621239.ckp  translog-1445516628097.ckp   translog-1445516631016.ckp   translog-1445516631025.ckp   translog-1445516631034.ckp   translog-1445516631043.ckp   translog-1445516631052.ckp
translog-1445516621240.ckp  translog-1445516628624.ckp   translog-1445516631016.tlog  translog-1445516631025.tlog  translog-1445516631034.tlog  translog-1445516631043.tlog  translog-1445516631052.tlog
translog-1445516621380.ckp  translog-1445516628625.ckp   translog-1445516631017.ckp   translog-1445516631026.ckp   translog-1445516631035.ckp   translog-1445516631044.ckp   translog-1445516631053.tlog
translog-1445516623082.ckp  translog-1445516629747.ckp   translog-1445516631017.tlog  translog-1445516631026.tlog  translog-1445516631035.tlog  translog-1445516631044.tlog  translog.ckp
translog-1445516623417.ckp  translog-1445516631009.ckp   translog-1445516631018.ckp   translog-1445516631027.ckp   translog-1445516631036.ckp   translog-1445516631045.ckp
translog-1445516623418.ckp  translog-1445516631009.tlog  translog-1445516631018.tlog  translog-1445516631027.tlog  translog-1445516631036.tlog  translog-1445516631045.tlog
translog-1445516623423.ckp  translog-1445516631010.ckp   translog-1445516631019.ckp   translog-1445516631028.ckp   translog-1445516631037.ckp   translog-1445516631046.ckp
```

@s1monw Is there any theoretical chance to fix this? Maybe removing part of the translog a persuading elasticsearch it is complete?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove script from function score bwc test and move to BasicBWCIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15332</link><project id="" key="" /><description>Function score test was supposed to test that old nodes can still read
whatever a new node sends. However, currently there is no way to test
scripting outside the script packages.

Also, we don't really need to upgrade nodes for this test. We only need
a mixed cluster.

related to #13522
</description><key id="121212166">15332</key><summary>remove script from function score bwc test and move to BasicBWCIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-12-09T11:01:57Z</created><updated>2015-12-09T13:39:36Z</updated><resolved>2015-12-09T13:39:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-09T12:59:48Z" id="163222051">LGTM. I'd prefer not to expand "Basic"BackwardsCompatibiltyIT but its fine.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make IndexShard operation be more explicit about whether they are expected to run on a primary or replica (backport #15282)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15331</link><project id="" key="" /><description>This commit cherry picks some infrastructure changes from the `feature/seq_no` branch to make merging from master easier.

More explicitly, IndexShard current have prepareIndex and prepareDelete methods that are called both on the primary as the replica, giving it a different origin parameter. Instead, this commits creates two explicit prepareOnPrimary and prepareOnReplica methods. This has the extra added value of not expecting the caller to use an Engine enum.

Also, the commit adds some code reuse between TransportIndexAction and TransportDeleteAction and their TransportShardBulkAction counter parts.

This is a backport of #15282 
</description><key id="121210815">15331</key><summary>Make IndexShard operation be more explicit about whether they are expected to run on a primary or replica (backport #15282)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v2.2.0</label></labels><created>2015-12-09T10:53:46Z</created><updated>2015-12-11T10:40:13Z</updated><resolved>2015-12-11T10:40:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-09T10:54:08Z" id="163187608">@jasontedor this was a non-trivial back port. Would love a sanity check. Thx.
</comment><comment author="jasontedor" created="2015-12-10T19:43:53Z" id="163729877">Saw a minor typo. Looked carefully over the places where the code being changed deviates substantially from master but it looks like you got everything. LGTM.
</comment><comment author="bleskes" created="2015-12-11T10:40:09Z" id="163905716">pushed to 2.x Thanks @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make FsProbe configurable for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15330</link><project id="" key="" /><description /><key id="121196247">15330</key><summary>Make FsProbe configurable for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">beiske</reporter><labels><label>:Stats</label><label>low hanging fruit</label><label>review</label></labels><created>2015-12-09T09:38:41Z</created><updated>2016-02-01T10:29:08Z</updated><resolved>2016-02-01T10:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-09T12:12:59Z" id="163206581">i really, really, really dont think we can make things pluggable by allowing a class to be specified in settings.

plugging shit into the classpath directly is simply not supported.
</comment><comment author="beiske" created="2015-12-09T13:48:44Z" id="163241564">@rmuir An interface would be cleaner, but to be honest I was simply following the example of other extension points, like this: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/DiscoveryModule.java#L105

Different suggestions are welcome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to suppress DocumentAlreadyExistsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15329</link><project id="" key="" /><description>Currently when op_type=create is used, we write the following exception at the INFO level to the ES log file for each document that is rejected.  This can potentially write a ton of entries to the logs, drowning other more important log entries.

```
[2015-12-08 23:01:08,790][INFO ][rest.suppressed          ] /test/type/1 Params: {id=1, index=test, op_type=create, type=type}
[test][[test][3]] DocumentAlreadyExistsException[[type][1]: document already exists]
    at org.elasticsearch.index.engine.InternalEngine.innerCreateNoLock(InternalEngine.java:411)
    at org.elasticsearch.index.engine.InternalEngine.innerCreate(InternalEngine.java:369)
    at org.elasticsearch.index.engine.InternalEngine.create(InternalEngine.java:341)
    at org.elasticsearch.index.shard.IndexShard.create(IndexShard.java:517)
    at org.elasticsearch.index.engine.Engine$Create.execute(Engine.java:789)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1073)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:170)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Maybe we can provide an option to suppress these, or write them to the DEBUG level, etc..
</description><key id="121176677">15329</key><summary>Option to suppress DocumentAlreadyExistsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-09T07:05:25Z</created><updated>2016-03-13T20:56:06Z</updated><resolved>2016-03-13T20:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-12-09T07:22:14Z" id="163138787">I tend to agree, seems like we throw back the failure to the user, this seems like a DEBUG logging
</comment><comment author="bleskes" created="2015-12-09T08:05:27Z" id="163145587">This was introduced with #12991 where we stopped serializing complete stack traces to the user but rather  logging them on the server.  Personally I'm a bit conflicted between logging this as INFO or DEBUG in general. However, for document level exceptions like version conflicts and doc already exists, we typically log a trace message as it can be very verbose and also not an error per-se but rather normal API behavior. I think we should follow that standard here too. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Factor mustache -&gt; modules/lang-mustache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15328</link><project id="" key="" /><description>This is the last of the scripting engines to refactor to a module.

It removes a third party dependency from core, removes third party dependency from classpath, and allows us to isolate this thing better, plus it is in my way right now as far as cleanups to core, due to some of the reflection it does (we can submit fixes, but still, lets isolate it).
</description><key id="121167040">15328</key><summary>Factor mustache -&gt; modules/lang-mustache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T06:00:25Z</created><updated>2015-12-14T10:45:16Z</updated><resolved>2015-12-10T13:46:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-09T23:44:53Z" id="163438949">I left one comment about a rest test move but other than that it looks good to me
</comment><comment author="rmuir" created="2015-12-10T00:22:27Z" id="163445492">I renamed that rest test to 50_messy_test_msearch.yaml and added it to the messy tests list.
</comment><comment author="jpountz" created="2015-12-10T08:35:25Z" id="163533510">That works for me, thanks, it's more obvious the test is misplaced now. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>box_type error.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15327</link><project id="" key="" /><description>- Master node
- Hot node
- Warm node

The cluster have three nodes and I tried hot-warm architecture. (using es 2.1.0)
But I couldn't update box_type because i got a below exception.
I don't know why elasticsearch distinguish by a field name or create index.
So, I think it is bug.

[Run...]
$ curl -XPOST "http://localhost:9200/db/_settings" -d'
{
    "index.routing.allocation.require.box_type":"warm"
}'

[Exception...]
[2015-12-09 14:29:10,675][INFO ][rest.suppressed          ] /db/_settings Params: {index=db, type=_settings}
RemoteTransportException[[hot-node][127.0.0.1:9301][indices:data/write/index]]; nested: MapperParsingException[Field name [index.routing.allocation.require.box_type] cannot contain '.'];
Caused by: MapperParsingException[Field name [index.routing.allocation.require.box_type] cannot contain '.']
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseProperties(ObjectMapper.java:278)
    at org.elasticsearch.index.mapper.object.ObjectMapper$TypeParser.parseObjectOrDocumentTypeProperties(ObjectMapper.java:223)
    at org.elasticsearch.index.mapper.object.RootObjectMapper$TypeParser.parse(RootObjectMapper.java:139)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:210)
    at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:191)
    at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:387)
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$2.execute(MetaDataMappingService.java:386)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745) 

[Run...]
$ curl -XPOST "http://localhost:9200/db" -d'
{
  "settings": {
    "index.routing.allocation.require.box_type":"warm"
  }
}'

[Exception...]
[2015-12-09 14:49:50,825][INFO ][rest.suppressed          ] /db Params: {index=db}
[db] IndexAlreadyExistsException[already exists]
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:168)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:520)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$200(MetaDataCreateIndexService.java:97)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:241)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:388)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
</description><key id="121165111">15327</key><summary>box_type error.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HowookJeong</reporter><labels /><created>2015-12-09T05:42:19Z</created><updated>2015-12-10T02:03:38Z</updated><resolved>2015-12-10T01:49:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-09T15:12:29Z" id="163285433">Use `PUT` instead of `POST` for the request (see https://www.elastic.co/guide/en/elasticsearch/reference/2.1/shard-allocation-filtering.html).

The error message is misleading in this case. We will need to look at what can be done about it.
</comment><comment author="HowookJeong" created="2015-12-10T01:49:02Z" id="163460782">Thank you for your comment.

I succeeded it.

[Good]
$ curl -XPUT "http://localhost:9200/db/_settings" -d'
{
    "index.routing.allocation.require.box_type":"warm"
}'

[Bad]
$ curl -XPUT "http://localhost:9200/db" -d'
{
  "settings": {
    "index.routing.allocation.require.box_type":"warm"
  }
}'
- bad request error messages.
  {
  "error": {
    "root_cause": [
       {
          "type": "index_already_exists_exception",
          "reason": "already exists",
          "index": "db"
       }
    ],
    "type": "index_already_exists_exception",
    "reason": "already exists",
    "index": "db"
  },
  "status": 400
  }

Close this ticket.

Thanks.
</comment><comment author="jasontedor" created="2015-12-10T02:03:38Z" id="163463055">&gt; The error message is misleading in this case. We will need to look at what can be done about it.

@ywelsch See #15335.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add gradle plugin for "messy" tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15326</link><project id="" key="" /><description>We have some tests which have crazy dependencies, like on other plugins.
This change adds a "messy-test" gradle plugin which can be used for qa
projects that these types of tests can run in. What this adds over
regular standalone tests is the plugin properties and metadata on the
classpath, so that the plugins are properly initialized.
</description><key id="121146051">15326</key><summary>Add gradle plugin for "messy" tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-09T02:23:04Z</created><updated>2015-12-09T03:48:57Z</updated><resolved>2015-12-09T03:48:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-09T02:26:35Z" id="163086602">Looks good. nice to improve our temporary solutions so we can iterate and get these cleaned up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>path.home is not configured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15325</link><project id="" key="" /><description>Hey

I am trying to do unit tests for ES, and tried to setup a local node using the following code.

 Node server = nodeBuilder()
                .node();

But I got the following error when I try to create this

java.lang.IllegalStateException: path.home is not configured
    at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:99)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:81)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:135)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:152)

Then I tried the following without luck

Settings settings = Settings.settingsBuilder().put("es.path.home", "").build();
Node server = nodeBuilder().settings(settings).node();

Do you guys have any idea? Thanks!
</description><key id="121138884">15325</key><summary>path.home is not configured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">YunchaoGongSC</reporter><labels /><created>2015-12-09T01:16:34Z</created><updated>2016-07-15T21:57:27Z</updated><resolved>2015-12-09T06:23:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="socurites" created="2015-12-09T05:15:40Z" id="163110662">How about trying this one: 

``` java
NodeBuilder.nodeBuilder()
    .settings(Settings.builder()
        .put("path.home", "/path/to/elasticsearch/home/dir")
    .node();
```

Reference:
[#13155] 2.0.beta1 - Exception "path.home is not configured" when starting ES in transport and client mode
</comment><comment author="YunchaoGongSC" created="2015-12-09T05:21:57Z" id="163112994">Thanks! I think I also tried "path.home" and an empty string as input, but with no luck. Will try the real elasticsearch home dir and see what happens. 
</comment><comment author="YunchaoGongSC" created="2015-12-09T05:22:08Z" id="163113090">Please note I am using ES version 2.1.0
</comment><comment author="YunchaoGongSC" created="2015-12-09T05:31:11Z" id="163115629">Another question, it seems by using nodeBuilder, we still need a local copy of ES. If we only want to setup a local in memory test server in our unit tests, when we develop stuff using ES's Java API, what is the best setting for this? Thanks!
</comment><comment author="dadoonet" created="2015-12-09T06:23:43Z" id="163123327">Please ask your questions on discuss.elastic.co.
We keep this space only for confirmed issues.
</comment><comment author="TannerMoore" created="2016-07-15T21:57:10Z" id="233079563">I have had more of my problems solved by Github than discuss.elastic.co. Not sure if that is Google's fault, GitHub's fault, or the community's fault.

This particular one helped me out a ton.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable HighlightBuilder to create SearchContextHighlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15324</link><project id="" key="" /><description>This enables the HighlightBuilder to procude a SeachContextHighlight object which contains the merged global options and field options. Also adding tests that make sure the produced SearchContextHighlighter is similar to the one we would get when parsing the xContent directly with the current HighlightParseElement. The duplication of the parsing logic in HighlightBuilder and SearchContextHighlighter will be removed in subsequent PRs.
</description><key id="121078403">15324</key><summary>Enable HighlightBuilder to create SearchContextHighlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T19:16:56Z</created><updated>2015-12-10T10:15:55Z</updated><resolved>2015-12-10T10:15:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-09T15:24:54Z" id="163290140">@cbuescher I left a few comments
</comment><comment author="cbuescher" created="2015-12-09T17:09:42Z" id="163326972">@colings86 thanks, pushed an additional commit addressing your comments, let me know if this is okay now.
</comment><comment author="colings86" created="2015-12-10T09:17:57Z" id="163540939">LGTM
</comment><comment author="cbuescher" created="2015-12-10T10:15:55Z" id="163565939">@colings86  Thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>URL param parsing should not be lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15323</link><project id="" key="" /><description>First commit to allow the team to check if this approach is OK.

At this point, I've just implemented the validation do GET requests because it invokes the method on the server and just before returning I check if all parameters have been used. This is Ok for read operations, but not allowed on write operations (there we've to change all single request before server invocation).

Please, let me know if this implementation is fine for GET operations and if the error format is correct.

Closes #14719
</description><key id="121078390">15323</key><summary>URL param parsing should not be lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinstuga</reporter><labels><label>:REST</label><label>breaking</label><label>review</label></labels><created>2015-12-08T19:16:52Z</created><updated>2015-12-16T01:42:37Z</updated><resolved>2015-12-15T23:46:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-14T20:14:43Z" id="164546781">@rjernst could you review this please?
</comment><comment author="martinstuga" created="2015-12-15T00:13:26Z" id="164601029">Please, consider ONLY this commit: https://github.com/martinstuga/elasticsearch/commit/768da9a7b3934a8f349ac9a64669c95e563b44d0

I've done some mistakes (that resulted in the other commits) working with GitHub and I don't know how to undo it. Help would be welcome.
</comment><comment author="clintongormley" created="2015-12-15T09:52:00Z" id="164706303">@martinstuga either rebase your branch on master, or just create a new branch and cherry pick the relevant commit, then force push your branch (or create a new PR)
</comment><comment author="martinstuga" created="2015-12-15T11:42:26Z" id="164739159">Thank you @clintongormley , I'm going to do that. Please, wait for my next PR. Sorry for the inconvenience. 
</comment><comment author="martinstuga" created="2015-12-15T23:46:18Z" id="164937793">@clintongormley and @rjernst , please, now consider this PR #15462.
Sorry about that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make slow tests more obvious</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15322</link><project id="" key="" /><description /><key id="121066627">15322</key><summary>make slow tests more obvious</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T18:15:48Z</created><updated>2016-01-15T22:19:41Z</updated><resolved>2015-12-09T20:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-08T18:22:40Z" id="162968553">Very cute. If we're not wiling to add a jar file for the gradle launcher though I wonder about a wav file for a cute sound.
</comment><comment author="nik9000" created="2015-12-08T18:27:05Z" id="162969791">Also my poor macbook air keeps making noises! I know its a slow machine....
</comment><comment author="mikemccand" created="2015-12-08T18:32:58Z" id="162972450">LGTM, this is nice.  I tested on my mac and it worked very well :)
</comment><comment author="jdconrad" created="2015-12-08T18:35:51Z" id="162973901">Seconded on the LGTM.  We seem to have quite a few slow tests.
</comment><comment author="jpountz" created="2015-12-08T18:51:00Z" id="162978823">+1
</comment><comment author="purbon" created="2015-12-08T18:53:11Z" id="162979950">I like this idea, we should do something alike in Logstash! :+1: 
</comment><comment author="rjernst" created="2015-12-08T20:45:42Z" id="163012909">LGTM
</comment><comment author="rmuir" created="2015-12-09T01:37:31Z" id="163078762">@nik9000 I think a sound or image is quite different than executable code? If you are opposed, I will not push the commit though. I figure, I wait on these tests day and night, we could use a little fun.
</comment><comment author="nik9000" created="2015-12-09T01:44:11Z" id="163079674">Fine by me. I don't want to block anyone's fun.
On Dec 8, 2015 8:37 PM, "Robert Muir" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 I think a sound or image is quite
&gt; different than executable code? If you are opposed, I will not push the
&gt; commit though. I figure, I wait on these tests day and night, we could use
&gt; a little fun.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/15322#issuecomment-163078762
&gt; .
</comment><comment author="drewr" created="2016-01-11T18:13:27Z" id="170638339">I like this in theory, but it's causing a lot of failures in the build environment, for example:

```
HEARTBEAT J1 PID(7647@slave-ffa9f226.build.us-west-2b.elasticnet.co): 2016-01-11T15:40:48, stalled for 11.7s at: IndexedExpressionTests.testAllOpsDisabledIndexedScripts
[ant:junit4] PulseAudioMixer: got interrupted while waiting for the EventLoop to initialize
```

We're getting that with about a third of the tests, seems to be only on Debian-derived EC2 systems:
- Debian 8.2, Linux 3.16.0-4-amd64, 1.8.0_66-internal-b17
- Ubuntu 14.04, Linux 3.13.0-74-generic, 1.8.0_72-internal-b05
- Ubuntu 15.04, Linux 3.19.0-42-generic, 1.8.0_45-internal-b14

Anybody happen to know a thread to pull here on a solution? Searching around I only see references to the exception text in the source.
</comment><comment author="nik9000" created="2016-01-11T18:22:39Z" id="170641594">&gt; Anybody happen to know a thread to pull here on a solution? Searching around I only see references to the exception text in the source.

I'd make a system setting that lets you disable it entirely and configure it on the build nodes. I'd do the same on my headless build box.
</comment><comment author="rmuir" created="2016-01-11T18:39:05Z" id="170647595">Its not the cause. The cause is whoever is calling thread.interrupt. we should stay completely focused on that.
</comment><comment author="drewr" created="2016-01-15T22:19:41Z" id="172111556">After unsetting `PULSE_SERVER` in the calling env we haven't seen this in a few days.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct typo in class name of StatsAggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15321</link><project id="" key="" /><description>@colings86 Here is the new PR to this issue. Excuse me, I'm new to this and I've done some mistakes with git.

That should be ok!

Closes #14730
</description><key id="121061219">15321</key><summary>Correct typo in class name of StatsAggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinstuga</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T17:47:37Z</created><updated>2015-12-14T10:34:39Z</updated><resolved>2015-12-09T13:39:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-09T13:54:59Z" id="163243365">This has now been merged into the master and 2.x branches.

@martinstuga thanks for the PR
</comment><comment author="martinstuga" created="2015-12-09T15:03:28Z" id="163282337">You're welcome. Thank you too for all support and patience. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix command line options for windows bat file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15320</link><project id="" key="" /><description>Options should be passed after `start`.

Closes #15284 
Closes #16086

Tests pass on windows and linux but I don't know much `ant` or `bat` so any help with improving the test is more than welcome. 
</description><key id="121060358">15320</key><summary>fix command line options for windows bat file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v2.0.3</label><label>v2.1.2</label><label>v2.2.0</label></labels><created>2015-12-08T17:42:46Z</created><updated>2016-01-27T12:55:28Z</updated><resolved>2016-01-20T15:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-12-08T17:43:16Z" id="162957799">I will make a separate pr for master once I figured out gradle.
</comment><comment author="clintongormley" created="2016-01-20T14:10:07Z" id="173214521">@Mpdreamz could you review this please? would be good to get it in
</comment><comment author="Mpdreamz" created="2016-01-20T15:20:49Z" id="173235831">LGTM :+1: 

no more `-Des.` ty @brwe !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamically map floating-point numbers as floats instead of doubles.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15319</link><project id="" key="" /><description>Close #13851
</description><key id="121055792">15319</key><summary>Dynamically map floating-point numbers as floats instead of doubles.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T17:20:56Z</created><updated>2015-12-09T07:38:28Z</updated><resolved>2015-12-09T07:38:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-08T18:26:06Z" id="162969348">LGTM
</comment><comment author="rmuir" created="2015-12-08T19:09:12Z" id="162984662">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CancellableThreads should also treat ThreadInterruptedException as InterruptedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15318</link><project id="" key="" /><description>RecoverySource uses the RateLimiter under a cancelable thread. The SimpleRateLimiter used in throws   ThreadInterruptedException on interruption. We should treat it as InterruptedException
</description><key id="121049876">15318</key><summary>CancellableThreads should also treat ThreadInterruptedException as InterruptedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T16:52:11Z</created><updated>2015-12-09T14:11:10Z</updated><resolved>2015-12-09T08:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-08T17:03:37Z" id="162946242">LGTM
</comment><comment author="s1monw" created="2015-12-09T08:56:15Z" id="163153889">lucene should really stop hiding interrupts @mikemccand  :)
</comment><comment author="mikemccand" created="2015-12-09T14:11:10Z" id="163248569">@s1monw I agree ... we have a longstanding issue open for this: https://issues.apache.org/jira/browse/LUCENE-4649 but it's a major effort ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate geo_shape orientation parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15317</link><project id="" key="" /><description>`geo_shape` `orientation` was originally added to handle polygonal ambiguity issues by complying with the right-hand rule defined by the OGC specification. The parameter adds leniency by enabling users to explicitly override the specification for backward compatibility with existing data. Unfortunately this leniency propogates "bad" (non-compliant) data.  Since the spec strictly requires the right-hand rule, ES should comply with the specification. Issues regarding "bad data" should be directed toward offline data repair tools prior to indexing - perhaps I will provide a tool bundled with ES.
</description><key id="121037494">15317</key><summary>Deprecate geo_shape orientation parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Mapping</label><label>breaking</label></labels><created>2015-12-08T15:57:27Z</created><updated>2016-02-14T18:48:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-11T10:15:34Z" id="163901148">+1 We should follow the spec.  Potentially we could add a processor to the ingest node that would support reordering points to follow the spec.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transaction support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15316</link><project id="" key="" /><description>I'd like to see functionality added to Elasticsearch that specifically supports transaction reporting, and I'd like to see that exposed for Kibana visualizations.  

"Stitching together" transactions from a set of Elasticsearch documents  isn't presently well-supported by existing aggregations and the Kibana visualization interface.   What I think is missing is:
1.  an aggregation for the Duration (min - max) timestamp of a group. 
2.  more specific options for inclusion in the group:
- starts_with field=value or field=regex_pattern
- ends_with field=value  or field=regex_pattern
- contains field=value or field=regex_pattern ## to report on subset of transactions that took a particular path or went through a particular host, etc.

Kibana's Visualization editor would need:
- An Additional Data Constraints section (in addition to metrics and buckets) which would solicit the starts_with, ends_with, and contains field and value.
- the metrics Aggregation lookup would need to include "Duration"

With these added I think ELK users would be in a much better position to report on business transactions across complex heterogeneous stacks out of the box (without custom development). 

Thoughts?
</description><key id="121037413">15316</key><summary>Transaction support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rferrante1966</reporter><labels><label>:CRUD</label><label>enhancement</label><label>feature</label><label>stalled</label></labels><created>2015-12-08T15:57:08Z</created><updated>2015-12-11T12:17:48Z</updated><resolved>2015-12-10T23:31:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmoskovicz" created="2015-12-08T16:16:44Z" id="162933009">@rferrante1966 

Currently i think that this can be done by Scripted Metric Aggregations. However they are expensive and i don't think that we support them in Kibana yet.

A new aggregation can be useful to provide the insight of the data in a pretty straight forward way. But i am not sure if this is too specific to a "Transactions" user case.

@clintongormley  any thoughts?
</comment><comment author="jpountz" created="2015-12-08T19:20:42Z" id="162987549">I can't find them but I think we had other issues related to that topic. In short it is not something that we can implement efficiently at query time. The best option today would be to index in an entity-centric way so that components of a transaction are already grouped together in a single document in the index and ready to be aggregated.
</comment><comment author="gmoskovicz" created="2015-12-08T19:27:39Z" id="162989338">Thanks @jpountz i do think that having them grouped using nested or parent child can work. For this a reindex is needed so then you can change the mapping and structure.
</comment><comment author="rferrante1966" created="2015-12-08T19:47:42Z" id="162994426">It may not be easy to implement at query time, but it may worth considering as demand for cross stack transaction reporting is huge and looks to only be growing. ...   

Without this feature, I was thinking along the same lines as @jpountz  that the best we could do was create transaction records outside of Elasticsearch (the single document @jpountz referenced) and insert them as a basis for reporting. 
</comment><comment author="jpountz" created="2015-12-09T08:39:42Z" id="163150931">@rferrante1966 If you already have transaction ids in your events, you might be able to use them as document ids in elasticsearch and use the upsert capabilities to either create the document if it does not exist or append to it otherwise.
</comment><comment author="gmoskovicz" created="2015-12-09T09:29:00Z" id="163160972">@jpountz @rferrante1966  I think that for a regular indexing use case, that can work, however for a logging use case, this is not straight forward. Also, using upsert can impact the cluster for too many updates, i think that the best option is to pull the documents and re-index them in a way that you can actually perform some operations that gives you the result. Other than than, Scripted Metric Aggregation can be expensive, but depending how many times per day you want to run this, it can be an option.
</comment><comment author="clintongormley" created="2015-12-09T09:32:08Z" id="163162259">@rferrante1966 Have a look at @markharwood 's talk about entity centric indexing https://www.youtube.com/watch?v=yBf7oeJKH2Y , where he goes into the whys and hows of doing what you're after.  This is not something we can do efficiently at query time, but we are thinking about how we can make it easier to do at index time, possibly using the changes API(https://github.com/elastic/elasticsearch/issues/1242)
</comment><comment author="rferrante1966" created="2015-12-10T23:31:04Z" id="163785854">Mark's presentation is certainly on target.  I hope to spend some time looking at the example he provided in which he routinely processes the raw events and creates session (transaction) records for optimized search. Thanks.
</comment><comment author="gmoskovicz" created="2015-12-11T12:17:48Z" id="163925146">@rferrante1966 this was one of the ideas that we had at the beginning of this thread. Some times we need to change the way that the data is represented to be able to query in a simpler way. However, for this we should:
1. Change the data, create new mapping, create new structures.
2. Change how the data is stored: Index and Documents.
3. Change the way that we think about logs: Log Centric vs Profile/Transaction Centric.

While this will require extra work, you should be able to decrease the amount of time spend in writing queries or thinking about aggregation, for time of extracting the data and adding to the `profile` or the information that you need.

Thanks @clintongormley for pasting the youtube video, i think that it clarifies more how this can be done!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log when cannot allocate memory.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15315</link><project id="" key="" /><description>After debugging the issue at https://discuss.elastic.co/t/elasticsearch-1-7-3-crashed-and-doesnt-restart-no-logs/36080 where ElasticSearch wouldn't start and not indicate anything in the logs or console turned out to be due to memory allocation issue.

As of now, when it fails to start due to memory issues, it will just tell that it failed and no reason will be provided in the logs (or on the screen).

In this case, it should indicate the issue in the logs. Or have add a command in the init.d script to debug such issue such as 'debugstart' where it would display startup/error messages in the console.
</description><key id="121034888">15315</key><summary>Log when cannot allocate memory.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ThomasdOtreppe</reporter><labels /><created>2015-12-08T15:45:54Z</created><updated>2017-04-14T02:49:41Z</updated><resolved>2015-12-08T16:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-08T16:29:18Z" id="162936383">&gt; As of now, when it fails to start due to memory issues, it will just tell that it failed and no reason will be provided in the logs (or on the screen).

I'm perplexed here. The JVM will produce error messages and a log file if it dies because it can't allocate memory. For example, if I try to start Elasticsearch requesting 2g of heap on a system with 1g of physical memory I see:

```
Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x000000008a660000, 1973026816, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 1973026816 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/hs_err_pid4572.log
```

and the first few lines of that log file:

```
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 1973026816 bytes for committing reserved memory.
# Possible reasons:
#   The system is out of physical RAM or swap space
#   In 32 bit mode, the process size limit was hit
# Possible solutions:
#   Reduce memory load on the system
#   Increase physical memory or swap space
#   Check if swap backing store is full
#   Use 64 bit Java on a 64 bit OS
#   Decrease Java heap size (-Xmx/-Xms)
#   Decrease number of Java threads
#   Decrease Java thread stack sizes (-Xss)
#   Set larger code cache with -XX:ReservedCodeCacheSize=
# This output file may be truncated or incomplete.
#
#  Out of Memory Error (os_linux.cpp:2627), pid=4572, tid=140018280015616
```

&gt; In this case, it should indicate the issue in the logs.

If it has died or never started, it can not.

&gt;  Or have add a command in the init.d script to debug such issue such as 'debugstart' where it would display startup/error messages in the console.

By default, the information that I showed above should already be showing. You might want to investigate on your system why this is not the case.
</comment><comment author="ThomasdOtreppe" created="2015-12-08T16:37:12Z" id="162938807">Jason, it worked before and suddenly crashed then never restarted using 'service elasticsearch start' until I decreased the amount of allocated memory significantly.

I don't know how you are starting it, I'm using ElasticSearch 1.7.3 from Elastic repositories and it didn't log anything at all.

I had no idea it is supposed to log/display it since I never saw it. I have no idea what is wrong and what to look for and if you'd like I can give you access to the system via TeamViewer or something similar so you can see for yourself.
</comment><comment author="jasontedor" created="2015-12-08T16:38:47Z" id="162939484">&gt; I don't know how you are starting it, I'm using ElasticSearch 1.7.3 from Elastic repositories and it didn't log anything at all.

For the example above, I'm also using Elasticsearch from the repositories, same version as you, and I'm starting Elasticsearch as a service on CentOS.
</comment><comment author="ThomasdOtreppe" created="2015-12-08T16:55:56Z" id="162944065">Just tested a brand new install of ElasticSearch (2.1) on Ubuntu 14.04 and then allocated more memory than the system had and when I restarted it (using service elasticsearch restart), it told me it started but didn't start and didn't log anything.

Here are the changes to the configuration:

```
network.host: 127.0.0.1
discovery.zen.ping.multicast.enabled: false
node.local: true
bootstrap.mlockall: true
```

Logging was increased to debug and swapiness was disabled (set to 1).

Could you please reopen the ticket?
</comment><comment author="rmuir" created="2015-12-08T16:58:31Z" id="162944941">its impossible because es is overconfigurable: we can't setup crashes to go to the logs directory, because you can configure logs directory not just via shell script/env, but also inside elasticsearch.yml.
</comment><comment author="ThomasdOtreppe" created="2015-12-08T17:01:09Z" id="162945610">So, would it be possible to have a 'debugstart' in the init.d script so that we can see what is wrong when it starts?
</comment><comment author="rmuir" created="2015-12-08T17:03:04Z" id="162946089">that just adds more ways to run elasticsearch and more options. we need less not more.
</comment><comment author="ThomasdOtreppe" created="2015-12-08T17:14:06Z" id="162949011">rmuir, that sounds contradictory to what Elastic is doing, they are adding features to ElasticSearch.

Right now, debugging it is very complicated if you are not a developer and know Java well enough. 

In my case, it took a week to figure out what the issue was after begging (and if you look at the post in the forum, I searched quite a bit before posting).

Unless there is a serious troubleshooting page that covers such issues ([this one](https://www.elastic.co/guide/en/elasticsearch/hadoop/current/troubleshooting.html) doesn't), then there is a need for such option.
</comment><comment author="jasontedor" created="2015-12-08T17:37:03Z" id="162955778">&gt; Just tested a brand new install of ElasticSearch (2.1) on Ubuntu 14.04 and then allocated more memory than the system had and when I restarted it (using service elasticsearch restart), it told me it started but didn't start and didn't log anything.

No matter what is going on with the output, JVMs produce an error log file in `/tmp` by default on Linux, or you can overwrite with `-XX:ErrorFile=path`.
</comment><comment author="ThomasdOtreppe" created="2015-12-08T17:56:32Z" id="162961893">@jasontedor I had no idea. That should be documented on the troubleshooting page.
</comment><comment author="jasontedor" created="2015-12-08T18:19:56Z" id="162967903">&gt; Just tested a brand new install of ElasticSearch (2.1) on Ubuntu 14.04 and then allocated more memory than the system had and when I restarted it (using service elasticsearch restart), it told me it started but didn't start and didn't log anything.

The key to understanding the problem here is to understand `systemd` logging.

I built an Ubuntu VM, installed openjdk-7-jdk, and Elasticsearch 1.7.3 from the Elastic repository. I started Elasticsearch with a heap configuration that would force the JVM to die with a failure to allocate. I reproduced your issue that no log messages are produced on the console (however, as mentioned, there is a JVM error log in `/tmp`).

Now, let's check the `systemd.exec` configuration in [/usr/lib/systemd/system/elasticsearch.service](https://github.com/elastic/elasticsearch/blob/6f996e9e26ae05440aa8d374d5cedcc3752ebfec/distribution/src/main/packaging/systemd/elasticsearch.service#L29-L30) and focus on these lines:

```
# Connects standard output to /dev/null
StandardOutput=null
```

You can change this to `journal` as in

```
# Connects standard output to /dev/null
StandardOutput=journal
```

then reload the daemon

```
# /bin/systemctl daemon-reload
```

and start Elasticsearch (again forcing it to die):

```
# /bin/systemctl start elasticsearch.service
```

and finally check the journal:

```
# journalctl
```

gives

```
Dec 08 18:06:38 vagrant-ubuntu-vivid-64 elasticsearch[16129]: OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000006fad30000, 4207738880, 0) failed; error='Cannot allocate memory' (errno=12)
Dec 08 18:06:38 vagrant-ubuntu-vivid-64 elasticsearch[16129]: #
Dec 08 18:06:38 vagrant-ubuntu-vivid-64 elasticsearch[16129]: # There is insufficient memory for the Java Runtime Environment to continue.
Dec 08 18:06:38 vagrant-ubuntu-vivid-64 elasticsearch[16129]: # Native memory allocation (malloc) failed to allocate 4207738880 bytes for committing reserved memory.
Dec 08 18:06:38 vagrant-ubuntu-vivid-64 elasticsearch[16129]: # An error report file with more information is saved as:
Dec 08 18:06:38 vagrant-ubuntu-vivid-64 elasticsearch[16129]: # /tmp/hs_err_pid16129.log
```

Make sure that you have the correct permissions when you use `journalctl`.

Check the man pages for `systemd.exec`, `sytstemd,journald`, and `journalctl` for additional configuration options.

And I do not think that this should be the default lest we spam the journal with Elasticsearch logs during normal operation. But feel free to configure as you see fit.

&gt; Could you please reopen the ticket?

With the above, I do not think that there is an issue here.
</comment><comment author="jasontedor" created="2015-12-08T18:32:20Z" id="162972285">&gt; Unless there is a serious troubleshooting page that covers such issues (this one doesn't), then there is a need for such option.

You can configure this as you desire with the `systemd` configuration.
</comment><comment author="ThomasdOtreppe" created="2015-12-08T19:24:27Z" id="162988491">It would be nice to put all that in the documentation since it doesn't seem that a lot of people know that (from what you can see in the forum post, nobody ever mentioned any of it). I developed in Java a while back and I'm fairly good with Linux but I didn't know the things you mentioned, @jasontedor 
</comment><comment author="Oliboy50" created="2017-01-11T23:38:31Z" id="272030921">same error, it works just fine with default params on ES 1.x and 2.x, but when I try to start ES 5.1.1, it 
fails with:
```
Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x000000008a660000, 1973026816, 0) failed; error='Cannot allocate memory' (errno=12)
```

I'm not a Java developer at all, so if someone could explain how to decrease the memory required to run ES 5.x. It would be so nice :)

Note: I just want to use ES 5.x in my development environment. Don't need to run a huge cluster :)</comment><comment author="jasontedor" created="2017-01-12T01:58:35Z" id="272054329">The default heap size is 2g. If you need to decrease this, edit the `jvm.options` file. The [docs](https://www.elastic.co/guide/en/elasticsearch/reference/5.1/heap-size.html) explain this in more detail.

There are two main reasons that you see a difference between 1.x/2.x and 5.x:
 - 1.x and 2.x defaulted to a starting heap size of 256m and a max heap size of 1g while 5.x defaults to a starting and max heap size of 2g
 - Elasticsearch 5.x touches all pages of the heap at startup forcing those pages to be allocated from the OS

</comment><comment author="Oliboy50" created="2017-01-12T21:05:48Z" id="272283286">@jasontedor thanks
I reduced to `-Xms1g` and `-Xmx1g` (instead of 2g for both) and it works just as before &#128077; </comment><comment author="surajr" created="2017-04-14T02:49:41Z" id="294073081">Even I reduced to -Xms1g and -Xmx1g and it works just as before</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update store.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15314</link><project id="" key="" /><description /><key id="121031310">15314</key><summary>Update store.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iamdto</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-12-08T15:32:41Z</created><updated>2016-03-10T13:34:41Z</updated><resolved>2016-03-10T13:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T19:33:45Z" id="163366634">Thanks @iamdto, can you sign the [CLA](https://www.elastic.co/contributor-agreement/) so I can merge this in?
</comment><comment author="clintongormley" created="2016-03-10T13:34:41Z" id="194840167">CLA not signed. Treating as issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make mappings immutable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15313</link><project id="" key="" /><description>Today mappings are mutable because of two APIs:
- Mapper.merge, which expects changes to be performed in-place
- IncludeInAll, which allows to change whether values should be put in the
  `_all` field in place.

This commit changes both APIs to return a modified copy instead of modifying in
place so that mappings can be immutable. For now, only the type-level object is
immutable, but in the future we can imagine making them immutable at the
index-level so that mapping updates could be completely atomic at the index
level.

Close #9365
</description><key id="121024111">15313</key><summary>Make mappings immutable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T14:56:38Z</created><updated>2015-12-15T09:23:36Z</updated><resolved>2015-12-15T09:23:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-08T19:42:26Z" id="162993149">LGTM but I've been reviewing code for about 5 hours now and I'm starting to have some trouble focussing so I'm probably not to be trusted.
</comment><comment author="jpountz" created="2015-12-09T08:12:11Z" id="163146600">No worries @nik9000, I've seen you been a very active reviewer recently indeed!
</comment><comment author="rjernst" created="2015-12-15T07:34:57Z" id="164673567">+1, I left some minor comments.
</comment><comment author="jpountz" created="2015-12-15T09:21:07Z" id="164696662">I just pushed a commit to address review comments. I'll merge soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CreateIndexIT.testCreateAndDeleteIndexConcurrently fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15312</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_centos/8910/testReport/junit/org.elasticsearch.action.admin.indices.create/CreateIndexIT/testCreateAndDeleteIndexConcurrently/

Stacktrace:

```
UnavailableShardsException[[test][1] Primary shard is not active or isn't assigned to a known node. Timeout: [1m], request: index {[test][test][AVGADS6IhWB4gLTwZw1z], source[{"index_version":1}]}]
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.retryBecauseUnavailable(TransportReplicationAction.java:665)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:381)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$3.onTimeout(TransportReplicationAction.java:522)
    at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:236)
    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:628)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Interesting log lines:

```
[2015-12-08 05:25:16,274][WARN ][org.elasticsearch.indices.cluster] [node_t0] [[test][1]] marking and sending shard failed due to [failed to create shard]
org.apache.lucene.store.LockObtainFailedException: Can't lock shard [test][1], timed out after 5000ms
    at org.elasticsearch.env.NodeEnvironment$InternalShardLock.acquire(NodeEnvironment.java:556)
    at org.elasticsearch.env.NodeEnvironment.shardLock(NodeEnvironment.java:485)
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:236)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:591)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:491)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:185)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:526)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:596)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

...

[2015-12-08 05:25:26,295][DEBUG][org.elasticsearch.cluster.routing.allocation] [node_t0] [test][0] ignoring shard failure, unknown allocation id in [test][0], node[w7ZOpTaqRi6UI-q9EjfG2Q], [P], v[1], s[INITIALIZING], a[id=WmeldM5ORfuNr8rWhbHvYQ], unassigned_info[[reason=INDEX_CREATED], at[2015-12-08T05:25:11.268Z]] ([reason=ALLOCATION_FAILED], at[2015-12-08T05:25:26.295Z], details[master {node_t0}{w7ZOpTaqRi6UI-q9EjfG2Q}{local}{local[182]}[mode=&gt;local] marked shard as initializing, but shard is marked as failed, resend shard failure])

...

[2015-12-08 05:25:26,303][DEBUG][org.elasticsearch.gateway] [node_t0] [test][0] found 0 allocations of [test][0], node[null], [P], v[2], s[UNASSIGNED], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-12-08T05:25:26.294Z], details[failed to create shard, failure LockObtainFailedException[Can't lock shard [test][0], timed out after 5000ms]]], highest version: [-1]
```

Analysis:

What's happening is that index creation fails as locks cannot be obtained for allocating shards (I'm not sure why that is). Shards are marked as failed: This changes UnassignedInfo of the ShardRouting objects by setting reason from INDEX_CREATED to ALLOCATION_FAILED. What this entails is that the PrimaryShardAllocator does not treat these unassigned shards as "new index creation" anymore, but requires that shard data is available before assigning them. Relates to #15241.
</description><key id="121004282">15312</key><summary>CreateIndexIT.testCreateAndDeleteIndexConcurrently fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-12-08T13:18:44Z</created><updated>2016-01-08T13:19:31Z</updated><resolved>2016-01-08T13:19:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-08T14:02:12Z" id="162888186">&gt; What's happening is that index creation fails as locks cannot be obtained for allocating shards (I'm not sure why that is). 

That's a good question and probably the source of this trouble

&gt; What this entails is that the PrimaryShardAllocator does not treat these unassigned shards as "new index creation" anymore, but requires that shard data is available before assigning them.

I think this will be fixed by #15281 or a follow up where we wouldn't need the allocatedPostIndexCreate() flag for primary allocations as the activeAllocaitonId will tell us.
</comment><comment author="bleskes" created="2015-12-08T14:02:22Z" id="162888260">and good analysis :)
</comment><comment author="ywelsch" created="2015-12-16T09:52:23Z" id="165051497">and once more:

http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/2196/testReport/junit/org.elasticsearch.action.admin.indices.create/CreateIndexIT/testCreateAndDeleteIndexConcurrently/
</comment><comment author="bleskes" created="2016-01-08T13:19:30Z" id="170001181">Closing as a duplicate of #14932 , w.r.t shard locking issue.  The primary allocation part has been fixed with https://github.com/elastic/elasticsearch/pull/15281
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>punch thru symlinks when loading plugins/modules</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15311</link><project id="" key="" /><description>this ensures the codebase URL matches the permission grant (see matching toRealPath in Security.java) in the case of symlinks or other shenanigans.

this is best effort, if we really want to support symlinks in any way, we need
e.g. qa or vagrant tests that configure a bunch of symlinks for things and ensure that in jenkins.
this should be easier to do with gradle, as we can just create a symlink'd home if we want
</description><key id="121002762">15311</key><summary>punch thru symlinks when loading plugins/modules</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>PITA</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T13:09:12Z</created><updated>2015-12-14T10:29:59Z</updated><resolved>2015-12-09T06:35:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-09T01:19:46Z" id="163076287">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Streamline put &amp; delete pipeline responses with index &amp; delete responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15310</link><project id="" key="" /><description>The put and delete pipeline apis are small wrappers around the index and delete apis, but the responses the put and delete pipeline apis return is different then the index and delete apis. This PR makes the responses consistent. This also cleans up some code on the ingest side. On the ES side the toxcontent logic has moved from the rest action to the request classes, so that both ingest and core use the same logic.
</description><key id="121001646">15310</key><summary>Streamline put &amp; delete pipeline responses with index &amp; delete responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-08T13:02:01Z</created><updated>2015-12-09T10:30:57Z</updated><resolved>2015-12-09T10:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-08T14:33:00Z" id="162899114">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle cancel exceptions on recovery target if the cancel comes from the source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15309</link><project id="" key="" /><description>Today we only handle correctly if the `ExecutionCancelledException` comes from the
local execution. Yet, this can also come from remote and should be handled identically.
</description><key id="120986011">15309</key><summary>Handle cancel exceptions on recovery target if the cancel comes from the source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T11:13:56Z</created><updated>2015-12-14T10:19:41Z</updated><resolved>2015-12-08T14:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-08T14:09:46Z" id="162891269">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only text fields should accept analyzer and term vector settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15308</link><project id="" key="" /><description>Currently almost all our fields accept the `analyzer` and `term_vector` settings
although they only make sense on text fields. This commit forbids those settings
on all fields but `string` and `_all` for indices created on or after version
2.2.0.
</description><key id="120975188">15308</key><summary>Only text fields should accept analyzer and term vector settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T10:07:04Z</created><updated>2015-12-09T07:55:33Z</updated><resolved>2015-12-09T07:55:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-08T17:36:19Z" id="162955517">Lgtm
</comment><comment author="nik9000" created="2015-12-08T18:24:21Z" id="162968905">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: plugin-security.policy seem to be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15307</link><project id="" key="" /><description>When creating a zip distribution and trying to start it, it fails due to a `java.security.AccessControlException`. It seems as if the `plugin-security.policy` are ignored. This is the log when starting up

```
[2015-12-08 10:50:31,291][INFO ][node                     ] [Polaris] version[3.0.0-SNAPSHOT], pid[20776], build[0809e4a/2015-12-08T09:16:54.109Z]
[2015-12-08 10:50:31,292][INFO ][node                     ] [Polaris] initializing ...
[2015-12-08 10:50:31,583][ERROR][bootstrap                ] Exception
ElasticsearchException[Failed to load plugin class [org.elasticsearch.script.expression.ExpressionPlugin]]; nested: ExceptionInInitializerError; nested: AccessControlException[access de
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:453)
        at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:402)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:114)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:150)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:131)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:179)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:293)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.ExceptionInInitializerError
        at org.elasticsearch.script.expression.ExpressionPlugin$1.run(ExpressionPlugin.java:45)
        at org.elasticsearch.script.expression.ExpressionPlugin$1.run(ExpressionPlugin.java:41)
        at java.security.AccessController.doPrivileged(Native Method)
        at org.elasticsearch.script.expression.ExpressionPlugin.&lt;clinit&gt;(ExpressionPlugin.java:41)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:445)
        ... 8 more
Caused by: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "getClassLoader")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.ClassLoader.checkClassLoaderPermission(ClassLoader.java:1525)
        at java.lang.Class.getClassLoader(Class.java:683)
        at org.apache.lucene.expressions.js.JavascriptCompiler.checkFunction(JavascriptCompiler.java:706)
        at org.apache.lucene.expressions.js.JavascriptCompiler.&lt;clinit&gt;(JavascriptCompiler.java:695)
        ... 17 more
```

P.S. Added a comment [in #15253](https://github.com/elastic/elasticsearch/pull/15253#issuecomment-162834286), but figured out it makes more sense to have an own issue

havent tested on 2.2
</description><key id="120974261">15307</key><summary>Security: plugin-security.policy seem to be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>bug</label></labels><created>2015-12-08T10:01:04Z</created><updated>2016-03-10T18:38:49Z</updated><resolved>2015-12-08T11:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-08T11:22:56Z" id="162854132">works for me, works for jenkins
</comment><comment author="spinscale" created="2015-12-08T12:00:38Z" id="162861420">found it: running the distribution from inside $TMPDIR does not seem to be supported... guess that's a feature?
</comment><comment author="rmuir" created="2015-12-08T12:03:58Z" id="162862160">If you want to support it, then it must be tested in jenkins.
</comment><comment author="rmuir" created="2015-12-08T13:10:25Z" id="162875309">For the record, the issue is unrelated to that permission, or any recent change in general: its because you are on a mac and your /tmp is symlinked.

I opened a PR, in case we want to fix the symlink issue: https://github.com/elastic/elasticsearch/pull/15311

Or not, I am equally happy with symlink = security exception.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range query does not support _name anymore in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15306</link><project id="" key="" /><description>QueryParsingException[[range] query does not support [_name]]
    at org.elasticsearch.index.query.RangeQueryParser.parse(RangeQueryParser.java:108)

Looking at RangeQueryParser the _name field has been removed between 1.7 and 2.0. Possibly related to #11744 ?

It appears to be fixed again in 'master' but may need to be fixed independently for the 2.x line.

Following query was used

```
{
  "nested": {
    "path": "inner_name_only",
    "score_mode": "avg",
    "query": {
      "range": {
        "inner_name_only.inner_date_field": {
          "gte" : "1999/01/01",
          "lte" : "2020/01/01",
       "_name" : "query1"
        }
      }
    }
  }
}
```
</description><key id="120971763">15306</key><summary>Range query does not support _name anymore in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">SMUnlimited</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.2.0</label></labels><created>2015-12-08T09:48:45Z</created><updated>2015-12-14T09:24:36Z</updated><resolved>2015-12-14T09:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-09T15:30:11Z" id="163292448">Thanks for reporting this, to me it looks like the expected place for the "_name" parameter was changed once with a0af88e9 to be on the top level, so in 2.x the following should work:

```
"range": {
      "inner_name_only.inner_date_field": {
        "gte": "1999/01/01",
        "lte": "2020/01/01"
      },
      "_name": "query1"
    }
```

However, as you mentioned in #11974 support for "_name" was added again on the main level alongside e.g. "boost" on master, but this was not ported to the 2.x branch. I think we should also add this back again on 2.x since the original move to the top-level of the query seems incidental. @javanna wdyt?
</comment><comment author="jpountz" created="2015-12-11T12:53:54Z" id="163930558">Agreed it's weird to have back compat on master that does not exist on 2.x. So we should either remove the back compat from master or add it to 2.x.
</comment><comment author="cbuescher" created="2015-12-11T13:49:55Z" id="163940427">@jpountz thanks, I just checked the situation on master again and will add the test if you agree. The version we seem to be deprecating on master is the only one allowed on 2.x, so I'd be in favour of changing that to the behaviour on master (allowing both places, but using ParseField to deprecate the use on the top level). 
</comment><comment author="cbuescher" created="2015-12-14T09:24:36Z" id="164387207">Closed by #15394
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support to _aliases endpoint to specify multiple indices and aliases in one action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15305</link><project id="" key="" /><description>Adds support to `_aliases` REST endpoint to specify multiple indices and aliases in one action.

For example, to add an alias to two indices in one go:

```
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "add" : { "indices" : ["test1", "test2"], "alias" : "alias1" } }
    ]
}'
```

or to add two aliases to an index:

```
curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        { "add" : { "index" : "test1", "aliases" : ["alias1", "alias2"] } }
    ]
}'
```

Note that, using the Java client, specifying multiple aliases was already syntactically possible, but forbidden by a later check. This check has been removed.

Relates to #15186.
</description><key id="120968609">15305</key><summary>Add support to _aliases endpoint to specify multiple indices and aliases in one action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Aliases</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T09:31:59Z</created><updated>2016-02-09T18:13:47Z</updated><resolved>2015-12-10T08:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-08T10:26:03Z" id="162841855">LGTM. Can you update the details of the PR? we use the PRs to drive change lists etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15304</link><project id="" key="" /><description /><key id="120954982">15304</key><summary>Fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cjohansen</reporter><labels><label>docs</label></labels><created>2015-12-08T07:58:17Z</created><updated>2015-12-15T09:57:01Z</updated><resolved>2015-12-15T09:56:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T09:57:01Z" id="164707314">thanks @cjohansen - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>backport plugin bundling to 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15303</link><project id="" key="" /><description>This is the backport of #15233 but for maven.

It has the same filesystem layout and semantics.

The way it works in maven is, most plugin logic moves to modules/pom.xml, and plugin is a child of that, only it overrides integ logic to call install-plugin instead of install-module.

distributions for each packaging format (except integ-zip) basically just unpack modules into their modules/ directory.

The oddities/hacks/etc needed (IMO these are minor and such things are expected):
- Modules pom has a unnecessary project-level dependency on the integ-zip, that forces proper build order in the "nuclear reactor".
- Distribution pom requires distributions to "opt-in" to modules by enabling them for their specific distro. This way it itself does not depend on them and the integ-zip works, no loops, no downloading.
- Licensing works different than gradle, we inspect packaging here for the check. our module.jars are not third party dependencies: we have to generate an ignore pattern for them. 
- Licensing for modules gets "duplicated": its specified in the module itself and in the distribution. We could disable it for modules, but since I think publishing artifacts for these is unavoidable with maven, and since the license-checker is packaging-based, its correct.
</description><key id="120942904">15303</key><summary>backport plugin bundling to 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.2.0</label></labels><created>2015-12-08T06:31:48Z</created><updated>2015-12-08T18:43:10Z</updated><resolved>2015-12-08T18:43:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-08T15:42:48Z" id="162921950">I've added this to the top of my review.
</comment><comment author="nik9000" created="2015-12-08T18:05:34Z" id="162964449">&gt; Modules pom has a unnecessary project-level dependency on the integ-zip, that forces proper build order in the "nuclear reactor".
&gt; Distribution pom requires distributions to "opt-in" to modules by enabling them for their specific distro. This way it itself does not depend on them and the integ-zip works, no loops, no downloading.
&gt; Licensing works different than gradle, we inspect packaging here for the check. our module.jars are not third party dependencies: we have to generate an ignore pattern for them.
&gt; Licensing for modules gets "duplicated": its specified in the module itself and in the distribution. We could disable it for modules, but since I think publishing artifacts for these is unavoidable with maven, and since the license-checker is packaging-based, its correct.

These are all hacks that I expected to get this working in maven.
</comment><comment author="nik9000" created="2015-12-08T18:14:58Z" id="162966693">LGTM. Thanks for the tour of the hacks.
</comment><comment author="rmuir" created="2015-12-08T18:43:06Z" id="162976010">Thanks for looking this over @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add system CPU percent to OS stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15302</link><project id="" key="" /><description>This commit backports commit 450aa7b3ce95fb58c4eac95a903466249aa55edc
from master to 2.x.
</description><key id="120932520">15302</key><summary>Add system CPU percent to OS stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Stats</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-12-08T05:11:20Z</created><updated>2016-01-31T13:47:48Z</updated><resolved>2015-12-08T19:02:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-08T17:17:06Z" id="162949844">LGTM
</comment><comment author="jasontedor" created="2015-12-08T19:02:21Z" id="162982970">Thanks for reviewing @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elastic search edit distance logic seems wrong  [ and the documentation is bad ]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15301</link><project id="" key="" /><description>the query is 

``` json
{
    "query" : {
         "bool" : {
             "must" : [
                 {
                     "match" : {
                        "full_name" : {
                             "query" : "meena mishra",
                             "fuzziness" : 2,
                             "operator" : "and"
                        }
                     }
                 },
                 ...
             ]
         }
    } 
}
```

the document has  

{ 
   "full_name" :  "Mina MISHRA" 
}

document was indexed using standard analyzers. All default.
edit distance should be &lt;= 2 and the document should match.
I have tried several queries and figured that , the "query" value "mna mishra"  succeeds.
"mina mishra", unsurprisingly succeeds. however as soon I change it to "mena mishra", it starts failing.

Is this related to the uncontrollable parameter _min_similarity_ which prevents this match?
How to debug the already done analysis of a document in an index?

version : 1.7
platform : centos

thanks.
</description><key id="120929252">15301</key><summary>elastic search edit distance logic seems wrong  [ and the documentation is bad ]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">parcompute</reporter><labels /><created>2015-12-08T04:38:25Z</created><updated>2015-12-11T10:11:18Z</updated><resolved>2015-12-11T10:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-11T10:11:13Z" id="163899394">@parcompute works for me, so could you check your analyzer settings. Closing this for now since I doubt this is an issue with the query. Please feel free to ask any questions regarding debugging options in the [forums](https://discuss.elastic.co/c/elasticsearch). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix tribe node to load config file for internal client nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15300</link><project id="" key="" /><description>The tribe node creates one local client node for each cluster it
connects to. Refactorings in #13383 broke this so that each local client
node now tries to load the full elasticsearch.yml that the real tribe
node uses.

This change fixes the problem by adding a TribeClientNode which is a
subclass of Node. The Environment the node uses is now passed in (in
place of Settings), and the TribeClientNode simply does not use
InternalSettingsPreparer.prepareEnvironment.

The tests around tribe nodes are not great. The existing tests pass, but
I also manually tested by creating 2 local clusters, and configuring and
starting a tribe node. With this I was able to see in the logs the tribe
node connecting to each cluster.

closes #14573
</description><key id="120927057">15300</key><summary>Fix tribe node to load config file for internal client nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Tribe Node</label><label>bug</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T04:19:33Z</created><updated>2015-12-09T11:33:10Z</updated><resolved>2015-12-08T16:07:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-08T13:22:05Z" id="162878014">thanks a lot for taking care of this @rjernst , LGTM
</comment><comment author="rjernst" created="2015-12-08T16:09:46Z" id="162931090">2.x: 7f27850
2.1: 11ba5de
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of Collections#shuffle(List) and Random#&lt;init&gt;()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15299</link><project id="" key="" /><description>This commit removes and now forbids all uses of
Collections#shuffle(List) and Random#&lt;init&gt;() across the codebase. The
rationale for removing and forbidding these methods is to increase test
reproducibility. As these methods use non-reproducible seeds, production
code and tests that rely on these methods contribute to
non-reproducbility of tests.

Instead of Collections#shuffle(List) the method
Collections#shuffle(List, Random) can be used. All that is required then
is a reproducible source of randomness. Consequently, the utility class
Randomness has been added to assist in creating reproducible sources of
randomness.

Instead of Random#&lt;init&gt;(), Random#&lt;init&gt;(long) with a reproducible seed
or the aforementioned Randomess class can be used.

Closes #15287
</description><key id="120926603">15299</key><summary>Remove and forbid use of Collections#shuffle(List) and Random#&lt;init&gt;()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-08T04:15:01Z</created><updated>2015-12-11T16:50:28Z</updated><resolved>2015-12-11T16:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-08T09:30:35Z" id="162827659">This is great. Left some minor questions/suggestions.
</comment><comment author="jasontedor" created="2015-12-08T13:29:39Z" id="162879765">&gt; This is great. Left some minor questions/suggestions.

@bleskes Thanks. I responded to your feedback.
</comment><comment author="jasontedor" created="2015-12-11T14:39:29Z" id="163951160">@rmuir I pushed some commits incorporating your ideas. A few notes:
1. the static `Random` field in `RoutingNodes` has been removed
2. if test classes are found on the classpath but `tests.seed` is not set an `IllegalStateException` will be thrown; this is important for IntelliJ's JUnit integration where the `tests.seed` is not set and will now require developers to set a seed in the VM options (say `-Dtests.seed=CAFEBABE00000034`)
3. I've incorporated your beautiful idea to delegate to `RandomziedContext` via reflection into `Randomness`

Would you mind taking another look?
</comment><comment author="rmuir" created="2015-12-11T15:22:42Z" id="163962354">By the way, I also don't think tests.seed check is necessary now that RandomizedRunner is used. It just means if you dont specify a seed, you get a random one! But it will still reproduce.

Its no different than not specifying a seed when running tests from a commandline.
</comment><comment author="jasontedor" created="2015-12-11T15:42:46Z" id="163967509">&gt; By the way, I also don't think tests.seed check is necessary now that RandomizedRunner is used.

@rmuir Agree. 

&gt; It just means if you dont specify a seed, you get a random one! But it will still reproduce.

Yes, which is better than a constant fixed seed.

&gt; Its no different than not specifying a seed when running tests from a command line.

The only difference is that IntelliJ at least doesn't always give a _nice_ reproducibility error message specifying the seed to reuse (but in the cases that it doesn't, I think it be extracted from the stack trace).

Removed in commit bbba4508187b3288197136831eee3fa969df4781.
</comment><comment author="rmuir" created="2015-12-11T15:53:08Z" id="163971689">looks good. i am not happy with intellij's wildcard importing (this makes source code shittier), but it doesnt need to block the change. We need to disable everyone's IDE from doing this horseshit and kill it for good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>guava18 of ex2.x conflicts with guava12 of hbase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15298</link><project id="" key="" /><description>guava versions conflict if i use hbase and es2.x java client together ,  then i have to keep the 1.x version of es
</description><key id="120913970">15298</key><summary>guava18 of ex2.x conflicts with guava12 of hbase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arronli</reporter><labels /><created>2015-12-08T02:08:05Z</created><updated>2016-05-04T02:08:39Z</updated><resolved>2015-12-08T05:01:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-08T05:01:42Z" id="162763647">guava has been removed as a dependency from the master branch. You will need to shade either hbase or ES until the next major release.

Please ask questions like this (assuming question was "what should I do?") on http://discuss.elastic.co
</comment><comment author="arronli" created="2015-12-08T07:14:13Z" id="162797504">thx !
</comment><comment author="roycehaynes" created="2016-03-24T18:47:57Z" id="200966679">@arronli - Did you have solve this issue? If so, what was the solution?
</comment><comment author="arronli" created="2016-05-04T02:08:39Z" id="216723034">@roycehaynes  
using hive external table  or 
read from hbase using mapreduce  api
hope helpful
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geolocation sorting inaccurate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15297</link><project id="" key="" /><description>Problem:
    When using the geolocation sorting functionality to sort based on distance from a point (lat, lon), we are observing inaccurate distance computations.  In some cases these are off by a little bit, but in others they are wildly off.  Changing the sort accuracy to ARC does not seem to help much.
    When we switch to using a Groovy script to calculate these values we get completely accurate results.  We are trying to understand where the differences could come from, and how such fundamental functionality could be broken.

Elasticsearch version: 
    1.5.1

Type Mapping:

{
 "lms_event": {
            "dynamic": "strict",
            "_all": {
               "enabled": false
            },
            "properties": {
               "acl": {
                  "properties": {
                     "ou_with_subs": {
                        "type": "integer"
                     },
                     "ou_without_subs": {
                        "type": "integer"
                     },
                     "user_with_subs": {
                        "type": "integer"
                     },
                     "user_without_subs": {
                        "type": "integer"
                     }
                  }
               },
               "event_id": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "rating": {
                  "type": "integer",
                  "doc_values": true
               },
               "session": {
                  "type": "nested",
                  "properties": {
                     "date_end": {
                        "type": "date",
                        "format": "dateOptionalTime"
                     },
                     "date_start": {
                        "type": "date",
                        "format": "dateOptionalTime"
                     },
                     "location": {
                        "type": "geo_point",
                        "doc_values": true
                     },
                     "location_ou_id": {
                        "type": "integer"
                     },
                     "schedule_id": {
                        "type": "string",
                        "index": "not_analyzed"
                     },
                     "session_id": {
                        "type": "string",
                        "index": "not_analyzed"
                     }
                  }
               },
               "subject_id": {
                  "type": "integer",
                  "doc_values": true
               }
            }
}

Query 1 (default ES geolocation sorting):

GET /dev.devrelease/lms_event/_search
{
   "_source": [
      "event_id"
   ],
   "query": {
      "filtered": {
         "query": {
            "match_all": {}
         },
         "filter": {
            "bool": {
               "must": [
                  {
                     "term": {
                        "event_id": "54231e7d-e725-480f-a66d-4358fa31ba46"
                     }
                  },
                  {
                     "nested": {
                        "path": "session",
                        "filter": {
                           "geo_distance": {
                              "distance": "1mi",
                              "location": {
                                 "lon": -118.486974,
                                 "lat": 34.01956
                              }
                           }
                        },
                        "inner_hits": {
                           "size": 1000,
                           "sort": [
                              {
                                 "_geo_distance": {
                                    "session.location": {
                                 "lon": -118.486974,
                                 "lat": 34.01956
                                    },
                                    "order": "asc",
                                    "unit": "mi"
                                 }
                              },
                              {
                                 "date_start": {
                                    "order": "asc"
                                 }
                              }
                           ]
                        }
                     }
                  }
               ]
            }
         }
      }
   }
}
}

Query 1 Result (inaccurate)

```
    {
                       "_index": "dev.devrelease.3034",
                       "_type": "lms_event",
                       "_id": "54231e7d-e725-480f-a66d-4358fa31ba46",
                       "_nested": {
                          "field": "session",
                          "offset": 11
                       },
                       "_score": null,
                       "_source": {
                          "session_id": "940c8051-3b9f-45e4-8d2c-b85168c0eaad",
                          "schedule_id": "4ca2b22e-ebf4-4188-8ecb-0217bab50a69",
                          "location": {
                             "lon": -118.486974,
                             "lat": 34.01956
                          },
                          "date_start": "2016-06-28T16:00:00.0000000",
                          "date_end": "2016-06-28T17:05:00.0000000",
                          "location_ou_id": 764362
                       },
                       "sort": [
                          2.553454168690986,
                          1467129600000
                       ]
                    }
```

Query 2 (groovy script)

{
GET /dev.devrelease/lms_event/_search
{
   "_source": [
      "event_id"
   ],
   "query": {
      "filtered": {
         "query": {
            "match_all": {}
         },
         "filter": {
            "bool": {
               "must": [
                  {
                     "term": {
                        "event_id": "54231e7d-e725-480f-a66d-4358fa31ba46"
                     }
                  },
                  {
                     "nested": {
                        "path": "session",
                        "filter": {
                           "geo_distance": {
                              "distance": "1mi",
                              "location": {
                                 "lon": -118.486974,
                                 "lat": 34.01956
                              }
                           }
                        },
                        "inner_hits": {
                           "size": 1000,
                           "sort": [
                              {
                                 "_script": {
                                    "lang": "groovy",
                                    "script": "doc['session.location'].arcDistanceInMiles(34.01956, -118.486974)",
                                    "type": "number",
                                    "order": "asc"
                                 }
                              },
                              {
                                 "date_start": {
                                    "order": "asc"
                                 }
                              }
                           ]
                        }
                     }
                  }
               ]
            }
         }
      }
   }
}
}

Query 2 Results (accurate)

```
     {
                       "_index": "dev.devrelease.3034",
                       "_type": "lms_event",
                       "_id": "54231e7d-e725-480f-a66d-4358fa31ba46",
                       "_nested": {
                          "field": "session",
                          "offset": 8
                       },
                       "_score": null,
                       "_source": {
                          "session_id": "d54e485b-9e3d-42fc-a23a-e36dffb59fe9",
                          "schedule_id": "957d7df0-98d8-4318-93cf-3bedb9aa70b4",
                          "location": {
                             "lon": -118.473667,
                             "lat": 34.028449
                          },
                          "date_start": "2016-06-30T16:00:00.0000000",
                          "date_end": "2016-06-30T17:05:00.0000000",
                          "location_ou_id": 764360
                       },
                       "sort": [
                          0.9787904132112273,
                          1467302400000
                       ]
                    }
```
</description><key id="120902961">15297</key><summary>Geolocation sorting inaccurate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sudhanne</reporter><labels /><created>2015-12-08T00:40:47Z</created><updated>2015-12-16T11:33:41Z</updated><resolved>2015-12-15T12:31:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T12:31:22Z" id="164750285">It is really helpful if you add a recreation that I can just copy and paste.  I've spent about an hour trying to replicate your example, without any luck.  Here's a demonstration of this working correctly:

```
PUT test
{
  "mappings": {
    "test": {
      "properties": {
        "loc": {
          "type": "geo_point"
        },
        "session": {
          "type": "nested",
          "properties": {
            "location": {
              "type": "geo_point",
              "doc_values": true
            }
          }
        }
      }
    }
  }
}

PUT test/test/1
{
  "session": {
    "location": {
      "lon": -118.473667,
      "lat": 34.028449
    }
  },
  "loc": {
    "lon": -118.473667,
    "lat": 34.028449
  }
}

PUT test/test/2
{
  "session": {
    "location": {
      "lon": -118.486974,
      "lat": 34.01956
    }
  },
  "loc": {
    "lon": -118.486974,
    "lat": 34.01956
  }
}

GET /test/test/_search?_source=false
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "nested": {
                "path": "session",
                "filter": {
                  "geo_distance": {
                    "distance": "1mi",
                    "session.location": {
                      "lon": -118.486974,
                      "lat": 34.01956
                    }
                  }
                },
                "inner_hits": {
                  "sort": [
                    {
                      "_geo_distance": {
                        "location": {
                          "lon": -118.486974,
                          "lat": 34.01956
                        },
                        "order": "asc",
                        "unit": "mi"
                      }
                    }
                  ]
                }
              }
            }
          ]
        }
      }
    }
  },
  "sort": [
    {
      "_geo_distance": {
        "loc": {
          "lon": -118.486974,
          "lat": 34.01956
        },
        "order": "asc",
        "unit": "mi"
      }
    },
    {
      "_script": {
        "lang": "groovy",
        "script": "doc['loc'].arcDistanceInMiles(34.01956, -118.486974)",
        "type": "number",
        "order": "asc"
      }
    }
  ]
}
```

You will see that all geo sort values (nested, top-level, groovy) are the same.  Tested on 1.5.1 and 1.7.3.  note: I had to change the nested sort field from `session.location` to just `location`, otherwise it didn't find the field.
</comment><comment author="sudhanne" created="2015-12-16T00:05:10Z" id="164940742">Hi Clinton,

Try this:

``` javascript
PUT test1
{
  "mappings": {
    "test": {
      "properties": {
        "id": {
          "type": "integer"
        },
        "session": {
          "type": "nested",
          "properties": {
            "location": {
              "type": "geo_point",
              "doc_values": true
            }
          }
        }
      }
    }
  }
}
```

``` javascript
PUT test1/test/1
{
   "id": 1,
   "session": {
      "location": {
         "lon": -118.486974,
         "lat": 34.01956
      }
   }
}
```

``` javascript
PUT test1/test/2
{
   "id": 2,
   "session": {
      "location": {
         "lon": -118.486974,
         "lat": 34.02956
      }
   }
}
```

``` javascript
GET /test1/test/_search
{
   "query": {
      "filtered": {
         "filter": {
            "bool": {
               "must": [
                  {
                     "nested": {
                        "path": "session",
                        "filter": {
                           "geo_distance": {
                              "distance": "1mi",
                              "location": {
                                 "lon": -118.486974,
                                 "lat": 34.01956
                              }
                           }
                        },
                        "inner_hits": {
                           "size": 1000,
                           "sort": [
                              {
                                  "_script": {
                                    "lang": "groovy",
                                    "script": "doc['session.location'].arcDistanceInMiles(34.01956, -118.486974)",
                                    "type": "number",
                                    "order": "asc"
                                  }                                  
                              },
                              {
                                 "_geo_distance": {
                                    "session.location": {
                                       "lon": -118.486974,
                                       "lat": 34.01956
                                    },
                                    "order": "asc",
                                    "unit": "mi"
                                 }
                              }
                           ]
                        }
                     }
                  }
               ]
            }
         }
      }
   }
}
```

``` javascript
{
   "took": 1,
   "timed_out": false,
   "_shards": {
      "total": 1,
      "successful": 1,
      "failed": 0
   },
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "test1",
            "_type": "test",
            "_id": "1",
            "_score": 1,
            "_source": {
               "id": 1,
               "session": {
                  "location": {
                     "lon": -118.486974,
                     "lat": 34.01956
                  }
               }
            },
            "inner_hits": {
               "session": {
                  "hits": {
                     "total": 1,
                     "max_score": null,
                     "hits": [
                        {
                           "_index": "test1",
                           "_type": "test",
                           "_id": "1",
                           "_nested": {
                              "field": "session",
                              "offset": 0
                           },
                           "_score": null,
                           "_source": {
                              "location": {
                                 "lon": -118.486974,
                                 "lat": 34.01956
                              }
                           },
                           "sort": [
                              0,
                              1.7976931348623157e+308
                           ]
                        }
                     ]
                  }
               }
            }
         },
         {
            "_index": "test1",
            "_type": "test",
            "_id": "2",
            "_score": 1,
            "_source": {
               "id": 2,
               "session": {
                  "location": {
                     "lon": -118.486974,
                     "lat": 34.02956
                  }
               }
            },
            "inner_hits": {
               "session": {
                  "hits": {
                     "total": 1,
                     "max_score": null,
                     "hits": [
                        {
                           "_index": "test1",
                           "_type": "test",
                           "_id": "2",
                           "_nested": {
                              "field": "session",
                              "offset": 0
                           },
                           "_score": null,
                           "_source": {
                              "location": {
                                 "lon": -118.486974,
                                 "lat": 34.02956
                              }
                           },
                           "sort": [
                              0.6909863387230968,
                              1.7976931348623157e+308
                           ]
                        }
                     ]
                  }
               }
            }
         }
      ]
   }
}
```

Notice that the inner hit sorting by geolocation does not appear to work properly unless I use custom Groovy script.  Thanks.

-Sudheer
</comment><comment author="clintongormley" created="2015-12-16T11:33:40Z" id="165075625">@sudhanne Did you see this line in my previous comment?

&gt; note: I had to change the nested sort field from `session.location` to just `location`, otherwise it didn't find the field.

In 2.0, you should use `session.location` in the geodistance sort, but in 1.x, you leave out the nested path.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add IOPS to OS stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15296</link><project id="" key="" /><description>Similar to https://github.com/elastic/elasticsearch/pull/14741 , it will be helpful to have IOPS metrics available as well.
</description><key id="120898323">15296</key><summary>Add IOPS to OS stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-08T00:03:08Z</created><updated>2016-05-17T20:16:39Z</updated><resolved>2016-05-17T20:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="McStork" created="2016-03-31T17:31:53Z" id="204040664">Also, differentiate sequential and random iops would be awesome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin install failure exit codes are not unique</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15295</link><project id="" key="" /><description>I was just debugging a problem in libesvm where installations failed to download, and was using the exit code (74) to handle that case.

This was fine until I ran it several times and the plugin was already installed, since that exit code was the same (74). 

This meant that I needed to write a workaround to parse stdout, which isn't ideal. It would be great if these (and perhaps other) errors used different exit codes so that scripts that consume the plugin tool could more easily react differently based on the reason for failure.
</description><key id="120895984">15295</key><summary>Plugin install failure exit codes are not unique</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">w33ble</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-07T23:44:50Z</created><updated>2017-03-21T20:56:01Z</updated><resolved>2017-03-21T20:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-08T19:44:41Z" id="162993702">&gt; This meant that I needed to write a workaround to parse stdout

Bleh. Makes sense to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ThreadLocalRandom to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15294</link><project id="" key="" /><description>[`ThreadLocalRandom`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadLocalRandom.html) is used in several places in our code base. This source of randomness [can not be seeded](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadLocalRandom.html#setSeed-long-) preventing reproducible sources of randomness. This prevents reproducible build failures.

[`ThreadLocalRandom`](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadLocalRandom.html) should be added to the list of forbidden APIs.
</description><key id="120883097">15294</key><summary>Add ThreadLocalRandom to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label></labels><created>2015-12-07T22:27:02Z</created><updated>2016-01-08T17:59:59Z</updated><resolved>2016-01-08T17:59:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add support for proxy authentication for s3 and ec2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15293</link><project id="" key="" /><description>When using S3 or EC2, it was possible to use a proxy to access EC2 or S3 API but username and password were not possible to be set.

This commit adds support for this. Also, to make all that consistent, proxy settings for both plugins have been renamed:
- from `cloud.aws.proxy_host` to `cloud.aws.proxy.host`
- from `cloud.aws.ec2.proxy_host` to `cloud.aws.ec2.proxy.host`
- from `cloud.aws.s3.proxy_host` to `cloud.aws.s3.proxy.host`
- from `cloud.aws.proxy_port` to `cloud.aws.proxy.port`
- from `cloud.aws.ec2.proxy_port` to `cloud.aws.ec2.proxy.port`
- from `cloud.aws.s3.proxy_port` to `cloud.aws.s3.proxy.port`

New settings are `proxy.username` and `proxy.password`.

``` yml
cloud:
    aws:
        protocol: https
        proxy:
            host: proxy1.company.com
            port: 8083
            username: myself
            password: theBestPasswordEver!
```

You can also set different proxies for `ec2` and `s3`:

``` yml
cloud:
    aws:
        s3:
            proxy:
                host: proxy1.company.com
                port: 8083
                username: myself1
                password: theBestPasswordEver1!
        ec2:
            proxy:
                host: proxy2.company.com
                port: 8083
                username: myself2
                password: theBestPasswordEver2!
```

Note that `password` is filtered with `SettingsFilter`.

We also fix a potential issue in S3 repository. We were supposed to accept key/secret either set under `cloud.aws` or `cloud.aws.s3` but the actual code never implemented that.

It was:

``` java
account = settings.get("cloud.aws.access_key");
key = settings.get("cloud.aws.secret_key");
```

We replaced that by:

``` java
String account = settings.get(CLOUD_S3.KEY, settings.get(CLOUD_AWS.KEY));
String key = settings.get(CLOUD_S3.SECRET, settings.get(CLOUD_AWS.SECRET));
```

Also, we extract all settings for S3 in `AwsS3Service` as it's already the case for `AwsEc2Service` class.

Closes #15268.
</description><key id="120880442">15293</key><summary>Add support for proxy authentication for s3 and ec2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-07T22:13:36Z</created><updated>2015-12-09T23:32:00Z</updated><resolved>2015-12-09T23:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-07T22:14:17Z" id="162683480">@imotov Could you add this one to your list of reviews ? :D 
</comment><comment author="imotov" created="2015-12-09T21:52:19Z" id="163405843">Left one minor comment. Otherwise LGTM.
</comment><comment author="dadoonet" created="2015-12-09T23:03:46Z" id="163430099">Merged. I think I should try to backport this in 2.2 but this will probably need another review as manual merging is required...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rounding issue on long values.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15292</link><project id="" key="" /><description>Hi using ES 2.1.0

We have noticed a rounding issue with "big" long values.

I have provided a text file with the bulk operations executed and the search result of the index. You will notice that the value inserted does not match the initial bulk operation. It seems to be rounding/doing something funny.

We used default settings on the index. The mapping is detected as long.

[elastic-2.1.0-rounding-issue.txt](https://github.com/elastic/elasticsearch/files/54618/elastic-2.1.0-rounding-issue.txt)
</description><key id="120879047">15292</key><summary>Rounding issue on long values.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javadevmtl</reporter><labels /><created>2015-12-07T22:06:04Z</created><updated>2015-12-15T10:24:16Z</updated><resolved>2015-12-07T23:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-07T23:32:53Z" id="162705121">The json spec has precision issues with large longs, because it represents all numbers as doubles. Set up your mappings (don't use dynamic mappings) as long, and pass the value in a json string. The `coerce` setting (which defaults to true), will use Long.parseLong on the string value.
</comment><comment author="javadevmtl" created="2015-12-08T14:55:42Z" id="162906273">Ok but the source will still show as string when I return the search result?

Just a thought, unless I missed it, the docs don't explain this phenomenon, maybe it can be mentioned.
</comment><comment author="clintongormley" created="2015-12-15T10:24:16Z" id="164715054">To be clear, this isn't a problem with the JSON spec per se, which says nothing about precision.  It is javascript which represents all numbers as doubles (and so a number of json encoders will do the same thing).  

Take your example:

```
PUT t
{
  "mappings": {
    "mytype": {
      "properties": {
        "my_huge": {
          "type": "long"
        }
      }
    }
  }
}

POST t/mytype/_bulk
{"index": {"_id": "10000000000001487" }}
{"my_huge":10000000000001487}
{"index": {"_id": "10000000000001489" }}
{"my_huge":10000000000001489}

GET t/_search?pretty
{
  "aggs": {
    "myhuge": {
      "terms": {
        "field": "my_huge"
      }
    }
  }
}
```

If you run the above in Sense, it will output:

```
"hits": {
  "total": 2,
  "max_score": 1,
  "hits": [
    {
      "_index": "t",
      "_type": "mytype",
      "_id": "10000000000001487",
      "_score": 1,
      "_source": {
        "my_huge": 10000000000001488
      }
    },
    {
      "_index": "t",
      "_type": "mytype",
      "_id": "10000000000001489",
      "_score": 1,
      "_source": {
        "my_huge": 10000000000001488
      }
    }
  ]
},
"aggregations": {
  "myhuge": {
    "doc_count_error_upper_bound": 0,
    "sum_other_doc_count": 0,
    "buckets": [
      {
        "key": 10000000000001488,
        "doc_count": 1
      },
      {
        "key": 10000000000001488,
        "doc_count": 1
      }
    ]
  }
```

The number have all been rounded to 10000000000001488.

However, repeat the same search request in the console with curl and you get:

```
"hits" : {
  "total" : 2,
  "max_score" : 1.0,
  "hits" : [ {
    "_index" : "t",
    "_type" : "mytype",
    "_id" : "10000000000001487",
    "_score" : 1.0,
    "_source":{"my_huge":10000000000001487}
  }, {
    "_index" : "t",
    "_type" : "mytype",
    "_id" : "10000000000001489",
    "_score" : 1.0,
    "_source":{"my_huge":10000000000001489}
  } ]
},
"aggregations" : {
  "myhuge" : {
    "doc_count_error_upper_bound" : 0,
    "sum_other_doc_count" : 0,
    "buckets" : [ {
      "key" : 10000000000001487,
      "doc_count" : 1
    }, {
      "key" : 10000000000001489,
      "doc_count" : 1
    } ]
  }
}
```

In other words, Elasticsearch is doing the right thing, and the Sense example is rounded because Javascript parsed the response JSON.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighter tags extraneous terms when used with match_phrase query (v2.1.0)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15291</link><project id="" key="" /><description>Hello,

I've got an application which uses match_phrase queries to look for, and highlight, exact matches within file texts. It worked fantastically on ElasticSearch version 2.0.0 and 2.0.1, but breaks on 2.1.0.

The query appears to run correctly but the highlighter highlights ALL instances of terms in the query instead of just those that appear within matched phrases. For example, if the query contains the word "is", then every instance of the word "is" - throughout the entire document - is highlighted.
# Example:

As an example, I've prepared a series of commands in the Sense chrome plugin to replicate the problem:

```
POST /test_index
{
   "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 0
   },
       "mappings" : {
        "file" : {
            "properties" : {
                "text" : { "type" : "string"}
            }
        }
    }
}

POST /test_index/file
{
    "text": "The quick brown fox jumped over the other, lazier, fox."
}

POST /test_index/file
{
    "text": "Doc Brown is one quick fox."
}

POST /test_index/file/_search
{
   "query": {
      "match_phrase": {
         "text": "quick brown fox"
      }
   },
   "highlight": {
      "fields": {
         "text": {
            "number_of_fragments": 0
         }
      }
   }
}
```
## Version 2.0.1 results

Under version 2.0.1, the search call returns the following hits array:

``` JSON
"hits": [
    {
        "_index": "test_index",
        "_type": "file",
        "_id": "AVF-PCoeysoVw9xNBYa3",
        "_score": 0.55737644,
        "_source":{
            "text": "The quick brown fox jumped over the other, lazier, fox."
        },
        "highlight": {
            "text": [
                "The &lt;em&gt;quick&lt;/em&gt; &lt;em&gt;brown&lt;/em&gt; &lt;em&gt;fox&lt;/em&gt; jumped over the other, lazier, fox."
            ]
        }
    }
]
```

This is the expected and desired output.
## Version 2.1.0 results

Under version 2.1.0, the search call returns the following hits array:

``` JSON
"hits": [
    {
        "_index": "test_index",
        "_type": "file",
        "_id": "AVF-MdXsEcdhEnh7dUcO",
        "_score": 0.55737644,
        "_source": {
            "text": "The quick brown fox jumped over the other, lazier, fox."
        },
        "highlight": {
            "text": [
                "The &lt;em&gt;quick&lt;/em&gt; &lt;em&gt;brown&lt;/em&gt; &lt;em&gt;fox&lt;/em&gt; jumped over the other, lazier, &lt;em&gt;fox&lt;/em&gt;."
            ]
        }
    }
]
```

This is the same match, and even has the same score, but the word "fox" is incorrectly highlighted when it occurs without the rest of the phrase.
## Tested solutions

In attempt to rectify this problem, I attempted two things:
1. First, on the theory that perhaps it had switched to the postings highlighter rather than the plain highlighter, I attempted to force the use of the plain version with the `"type": "plain"` flag. Unfortunately, this had no visible effect.
2. Second, duplicated the original query within a `"highlight_query"` block. This too, unfortunately, had no visible effect.
# Theory

Currently, my theory is that this problem is related to the upgrade to lucene-5.3.0. (https://github.com/elastic/elasticsearch/pull/13239)
Specifically, I wonder if it has to do with how NearSpansOrdered are processed. (https://issues.apache.org/jira/browse/LUCENE-6537)

That said, this is purely conjecture. It's quite possible - or even likely - that the problem lies elsewhere.
I'm hoping that an elasticsearch expert might have a better idea.

Thank you.
</description><key id="120870471">15291</key><summary>Highlighter tags extraneous terms when used with match_phrase query (v2.1.0)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jftanner</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-12-07T21:20:37Z</created><updated>2015-12-17T16:23:22Z</updated><resolved>2015-12-17T16:12:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-17T16:06:14Z" id="165495402">Thanks for the detailed bug report, I could easily reproduce the issue. Actually the problem does not come from Lucene but that a change that I did when integrating the 5.3 release. I opened a pr at #15516
</comment><comment author="jftanner" created="2015-12-17T16:23:22Z" id="165499987">Fantastic, thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write log entries of external nodes to console</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15290</link><project id="" key="" /><description>Before external nodes wrote logs only to a file which makes it tricky to debug
test failures.
This also removes the port information which we cannot get anymore because
we do not scan the output. But is was not needed anyway.

A side effect seems to be that now on windows one has to press Ctrl+C several times instead of only once to interrupt the bwc tests. Not sure how much of a problem that is. I did not find a workaround for that. 

If this is too bad than we can still go for #15028
</description><key id="120836863">15290</key><summary>Write log entries of external nodes to console</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-12-07T18:26:48Z</created><updated>2015-12-09T10:42:19Z</updated><resolved>2015-12-09T10:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-12-07T19:17:10Z" id="162629313">Thanks for the review! I addressed all comments I got so far.
</comment><comment author="brwe" created="2015-12-08T17:46:58Z" id="162958757">@nik9000 can you take another look?
</comment><comment author="nik9000" created="2015-12-08T18:16:10Z" id="162966983">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper parsers should not check for a `tokenized` property.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15289</link><project id="" key="" /><description>I don't recall of this property of any of our field mappers and it's not in our
docs so I suspect it's very old. The removal of this property will not fail
version upgrades since none of the field mappers use it in toXContent.
</description><key id="120835177">15289</key><summary>Mapper parsers should not check for a `tokenized` property.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-07T18:16:49Z</created><updated>2015-12-08T08:17:57Z</updated><resolved>2015-12-08T08:02:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-07T18:43:06Z" id="162619345">LGTM
</comment><comment author="rjernst" created="2015-12-07T19:22:39Z" id="162630982">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NumberFormatException with ip type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15288</link><project id="" key="" /><description>Hi,

I am using:

 elasticsearch: 2.0.0
 logstash: 2.0.0

When using the 'ip' type in elasticsearch I get a NumberFormatException.

template is:

``` javascript
{
    "template" : "frameaccess-*",
    "aliases" : { },
    "mappings" : {
        "frameaccess" : {
            "properties" : {
                "@timestamp" : {"type" : "date","format" : "dateOptionalTime"},
                "@version" : {"type" : "string", "index" : "not_analyzed"},
                "host" : {"type" : "string", "index" : "not_analyzed"},
                "message" : {"type" : "string"},
                "clientip" : {"type" : "ip", "store": "true", "index" : "not_analyzed"},
                "username" : {"type" : "string", "index" : "not_analyzed"},
                "method" : {"type" : "string", "index" : "not_analyzed"},
                "request" : {"type" : "string"},
                "http_version" : {"type" : "float"},
                "http_status_code" : {"type" : "integer"},
                "size" : {"type" : "long"},
                                "responsetime" : {"type" : "double"},
                "source" : {"type" : "string", "index" : "not_analyzed"}
            }
        }
    },
    "settings" : {
        "index" : {
            "number_of_shards" : "3",
            "number_of_replicas" : "1"
        }
    }
}
```

logstash config is:

``` javascript
input { 
   stdin { codec =&gt; plain }
}
output { 
   elasticsearch {
      hosts =&gt; "elasticsearchurl.company.com:80"
      index =&gt; "frameaccess-%{+YYYY.MM.dd}"
   }
}
filter {
  grok {
    match =&gt; [
      "message",
      '(-|%{IPORHOST:clientip}) \- (-|%{HTTPDUSER:username}) \[%{HTTPDATE:timestamp}\] "%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:http_version}" (-|%{INT:http_status_code}) (-|%{INT:size}) (-|%{NUMBER:responsetime})'
    ]
  }

  date {
    locale =&gt; "en"
    timezone =&gt; "Europe/London"
    match =&gt; [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
  }

  mutate {
    add_field =&gt; { "source" =&gt; "appname" }
    replace =&gt; { "host" =&gt; "mynodehostname.company.com" }
    remove_field =&gt; [ "timestamp" ]
  }
}
```

Test input is:

`192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] "POST /ctx/minimumEntry.jsp HTTP/1.1" 200 48005 0.285`

Result is:

```
io/console not supported; tty will not be manipulated
Default settings used: Filter workers: 4
Logstash startup completed
?[33mFailed action.  {:status=&gt;400, :action=&gt;["index", {:_id=&gt;nil, :_index=&gt;"frameaccess-2015.09.28", :_type=&gt;"logs", :_routing=&gt;nil}, #&lt;LogStash::Event:0x48d26c4b @metadata={"retry_count"=&gt;0}, @accessors=#&lt;LogStash::Util::Accessors:0x7ae71f79 @store={"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, @lut={"host"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "host"], "message"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z","host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "message"], "clientip"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "clientip"], "username"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r","@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "username"], "timestamp"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "timestamp"], "method"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "method"], "request"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "request"], "http_version"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r","@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "http_version"], "http_status_code"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "http_status_code"], "size"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "size"], "responsetime"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "responsetime"], "@timestamp"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "@timestamp"], "source"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs","method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "source"], "type"=&gt;[{"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, "type"]}&gt;, @data={"message"=&gt;"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r", "@version"=&gt;"1", "@timestamp"=&gt;"2015-09-28T23:00:00.000Z", "host"=&gt;"mynodehostname.company.com", "clientip"=&gt;"192.168.0.1", "username"=&gt;"fbloggs", "method"=&gt;"POST", "request"=&gt;"/ctx/minimumEntry.jsp", "http_version"=&gt;"1.1", "http_status_code"=&gt;"200", "size"=&gt;"48005", "responsetime"=&gt;"0.285", "source"=&gt;"appname"}, @metadata_accessors=#&lt;LogStash::Util::Accessors:0x124e57b @store={"retry_count"=&gt;0}, @lut={}&gt;, @cancelled=false&gt;], :response=&gt;{"create"=&gt;{"_index"=&gt;"frameaccess-2015.09.28", "_type"=&gt;"logs", "_id"=&gt;"AVF9g1sPPii72JEyi-z_", "status"=&gt;400, "error"=&gt;{"type"=&gt;"mapper_parsing_exception", "reason"=&gt;"failed to parse [clientip]", "caused_by"=&gt;{"type"=&gt;"number_format_exception", "reason"=&gt;"For input string: \"192.168.0.1\""}}}}, :level=&gt;:warn}?[0m
Logstash shutdown completed
```

Log says:

```
[2015-12-07 16:58:23,968][DEBUG][action.bulk              ] [MACHINENAME] [frameaccess-2015.09.28][2] failed to execute bulk item (index) index {[frameaccess-2015.09.28][logs][AVF9YT5FPii72JEyi-z9], source[{"message":"192.168.0.1 - fbloggs [29/Sep/2015:00:00:00 +0100] \"POST /ctx/minimumEntry.jsp HTTP/1.1\" 200 48005 0.285\r","@version":"1","@timestamp":"2015-09-28T23:00:00.000Z","host":"mynodehostname.company.com","clientip":"192.168.0.1","username":"fbloggs","method":"POST","request":"/ctx/minimumEntry.jsp","http_version":"1.1","http_status_code":"200","size":"48005","responsetime":"0.285","source":"appname"}]}
MapperParsingException[failed to parse [clientip]]; nested: NumberFormatException[For input string: "192.168.0.1"];
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:339)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:314)
    at org.elasticsearch.index.mapper.DocumentParser.parseAndMergeUpdate(DocumentParser.java:762)
    at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:676)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:447)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:267)
    at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:128)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:318)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:503)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:494)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.prepareIndexOperationOnPrimary(TransportReplicationAction.java:1052)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1060)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:338)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:131)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: For input string: "192.168.0.1"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Long.parseLong(Long.java:589)
    at java.lang.Long.parseLong(Long.java:631)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:145)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:276)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:213)
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:331)
    ... 20 more
```
</description><key id="120833416">15288</key><summary>NumberFormatException with ip type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timmgrant</reporter><labels /><created>2015-12-07T18:06:09Z</created><updated>2015-12-07T19:12:27Z</updated><resolved>2015-12-07T19:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-07T19:12:27Z" id="162628125">We can see from the stack trace the request is trying to dynamically update the mappings:
`at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:676)`

This means the template is not getting applied. The type in your mappings is "frameaccess", but logstash defaults to a type of "logs", and we see this in the request debugging info printed:
`[frameaccess-2015.09.28]`**`[logs]`**`[AVF9YT5FPii72JEyi-z9]`

I think you should be using logstash to manage your templates? If you have problems with that, please open an issue with logstash.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Collections#shuffle(List) to forbidden APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15287</link><project id="" key="" /><description>Several tests are using [`Collections#shuffle(List)`](https://docs.oracle.com/javase/8/docs/api/java/util/Collections.html#shuffle-java.util.List-) which uses a default source of randomness rather than using [`Collections#shuffle(List, Random)`](https://docs.oracle.com/javase/8/docs/api/java/util/Collections.html#shuffle-java.util.List-java.util.Random-) which enables a reproducible source of randomness. This prevents reproducible build failures. One example is [NodeJoinControllerTests](https://github.com/elastic/elasticsearch/blob/0c2c7e7ef527393b23fe211f0617e1c2b006cc6c/core/src/test/java/org/elasticsearch/discovery/zen/NodeJoinControllerTests.java#L247) but there are many others. 

The method [`Collections#shuffle(List)`](https://docs.oracle.com/javase/8/docs/api/java/util/Collections.html#shuffle-java.util.List-) should be added to forbidden APIs.
</description><key id="120827144">15287</key><summary>Add Collections#shuffle(List) to forbidden APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label></labels><created>2015-12-07T17:32:35Z</created><updated>2015-12-11T16:24:56Z</updated><resolved>2015-12-11T16:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-07T17:35:24Z" id="162602068">+1
</comment><comment author="nik9000" created="2015-12-07T17:35:50Z" id="162602181">&gt; The method Collections#shuffle(List) should be added to forbidden APIs.

The list for tests or the list for all classes?
</comment><comment author="jasontedor" created="2015-12-07T17:36:04Z" id="162602243">&gt; The list for tests or the list for all classes?

I intend to completely forbid it lest there still remain non-reproducible sources of randomness. I discussed this with @rmuir and we have a path forward.
</comment><comment author="rmuir" created="2015-12-07T17:37:23Z" id="162602611">its the only way. if something is non-test and uses random behavior, give it an optional setting like initialSeed and set it in the test framework.
</comment><comment author="nik9000" created="2015-12-07T17:41:51Z" id="162603693">&gt; its the only way. if something is non-test and uses random behavior, give it an optional setting like initialSeed and set it in the test framework.

I figured as much but its nice to have it confirmed. IIRC its used in a few places.
</comment><comment author="jasontedor" created="2015-12-07T17:50:25Z" id="162606006">This method is also used inside [ESTestCase#randomSubsetOf](https://github.com/elastic/elasticsearch/blob/0c2c7e7ef527393b23fe211f0617e1c2b006cc6c/test-framework/src/main/java/org/elasticsearch/test/ESTestCase.java#L569) greatly increasing the surface area of non-reproducible tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creating a java client node throws "script file extension not supported [groovy]", but script works as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15286</link><project id="" key="" /><description>I have two groovy files placed in the config/scripts directory of the elasticsearch home directory. Since upgrading to elasticsearch 2.1.0 (from 1.6.0), I have started seeing the following stacktrace upon launching my application:

`java.lang.IllegalArgumentException: script file extension not supported [groovy]
    at org.elasticsearch.script.ScriptService.getScriptEngineServiceForFileExt(ScriptService.java:221)
    at org.elasticsearch.script.ScriptService.access$1200(ScriptService.java:83)
    at org.elasticsearch.script.ScriptService$ScriptChangesListener.onFileInit(ScriptService.java:537)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.onFileCreated(FileWatcher.java:256)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:166)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.createChild(FileWatcher.java:173)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.listChildren(FileWatcher.java:188)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.onDirectoryCreated(FileWatcher.java:299)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:162)
    at org.elasticsearch.watcher.FileWatcher$FileObserver.access$000(FileWatcher.java:75)
    at org.elasticsearch.watcher.FileWatcher.doInit(FileWatcher.java:65)
    at org.elasticsearch.watcher.AbstractResourceWatcher.init(AbstractResourceWatcher.java:36)
    at org.elasticsearch.watcher.ResourceWatcherService.add(ResourceWatcherService.java:133)
    at org.elasticsearch.watcher.ResourceWatcherService.add(ResourceWatcherService.java:126)
    at org.elasticsearch.script.ScriptService.&lt;init&gt;(ScriptService.java:192)`

It seems like there is no engine for groovy files. However, when I later perform an operation that uses the scripts, everything works with no problems at all. Seeing in the source code, I can see the "scriptEnginesByExt" map being populated before the FileWatcher gets initialised. So, I cannot see how the engine for groovy scripts could be being added later. 

Also note that this occurs when I am creating a Node client (configured with `.client(true)`). If I create a transport client instead, I do not get the stacktrace. Before upgrading, such a stacktrace was not present while still using a Node client.

I would be thankful for any ideas on why this is happening.
</description><key id="120819167">15286</key><summary>Creating a java client node throws "script file extension not supported [groovy]", but script works as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dimitris-athanasiou</reporter><labels /><created>2015-12-07T16:54:51Z</created><updated>2015-12-08T10:07:37Z</updated><resolved>2015-12-08T06:32:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-08T06:32:17Z" id="162788102">Scripts do not run on client nodes, which explains why the system works fine even though you see the error. The transport client is just a client (it does not create a node, so no script service).

I would guess that you are missing groovy in your classpath, which would cause the 2.1 code to skip registering groovy as a script language. Most likely you were using a shaded jar in 1.6, which we no longer have in 2.x, meaning you must include dependencies yourself when using ES as a client.
</comment><comment author="dimitris-athanasiou" created="2015-12-08T10:07:37Z" id="162838382">Thank you for the explanation!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse common attributes of top-level HightlightBuilder and Field in same method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15285</link><project id="" key="" /><description>Today we have parsing code for the top-level highlighter in the search request and very similar but slightly different code for parsing the parameters that are specified for individual fields. It would be good if we could pull out the parsing of the common parameters into the recently introduced AbstractHighlighterBuilder for better code reuse and future consistency.
</description><key id="120818533">15285</key><summary>Parse common attributes of top-level HightlightBuilder and Field in same method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label></labels><created>2015-12-07T16:51:53Z</created><updated>2015-12-14T14:19:03Z</updated><resolved>2015-12-14T14:19:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>command line parameters like --help and -p pidfile broken on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15284</link><project id="" key="" /><description>The bat script is broken on windows. The parameters `!newparams!` should be placed at the ned of the line in bat script here: https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch.bat#L46 as @spinscale pointed out.

We need a test for that too. This was not caught because currently we only test for the `-Des.pidfile` parameter: https://github.com/elastic/elasticsearch/blob/2.x/dev-tools/src/main/resources/ant/integration-tests.xml#L169 (have not looked at master yet). 

I can do that just want to open the issue in case someone else is dying to do that...
</description><key id="120811515">15284</key><summary>command line parameters like --help and -p pidfile broken on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-12-07T16:27:34Z</created><updated>2016-02-14T18:46:07Z</updated><resolved>2016-02-14T18:46:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-07T17:18:23Z" id="162597717">We do the same (set the `pidfile` setting) in master:
https://github.com/elastic/elasticsearch/blob/70107c5c3cf6958ad4c38a15fc33d25e66673610/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/ClusterFormationTasks.groovy#L206
</comment><comment author="clintongormley" created="2016-02-14T18:46:07Z" id="183948847">Fixed by #15320
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for transient metadata to IngestDocument</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15283</link><project id="" key="" /><description>IngestDocument now holds an additional map of transient metadata. The only field that gets added automatically is `timestamp`, which contains the timestamp of ingestion in ISO8601 format. In the future it will be possible to eventually add or modify these fields, which will not get indexed, but they will be available via templates to all of the processors.

Transient metadata will be returned by the simulate api, although they will never get indexed. Moved `WriteableIngestDocument` to the simulate package as it's only used by simulate and it's now modelled for that specific usecase.

 Also taken the chance to remove one IngestDocument constructor used only for testing (accepting only a subset of es metadata fields). While doing that introduced some more randomizations to some existing processor tests.

TODO: 
1) define name for the timestamp transient field and the ancestor object for the transient metadata fields in the simulate api (will be aligned with how we decide to expose those fields through templates)
2) expose failure as a transient metadata field when something breaks, maybe should be done after the new on_failure handling is in

Closes #15036
</description><key id="120809875">15283</key><summary>Add support for transient metadata to IngestDocument</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-12-07T16:21:30Z</created><updated>2015-12-09T17:59:47Z</updated><resolved>2015-12-09T17:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-08T11:51:17Z" id="162859979">&gt; define name for the timestamp transient field and the ancestor object for the transient metadata fields in the simulate api (will be aligned with how we decide to expose those fields through templates)

I think the `timestamp` is good name. They way we're going to refer to it will be `_ingest.timestamp` so that looks good to.

&gt; expose failure as a transient metadata field when something breaks, maybe should be done after the new on_failure handling is in

I think this should be done as part of the on_failure task #14548.

Beyond the comment about the different maps in IngestDocument this looks good. 
</comment><comment author="javanna" created="2015-12-09T17:10:00Z" id="163327058">@martijnvg I pushed new commits
</comment><comment author="martijnvg" created="2015-12-09T17:42:17Z" id="163335950">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make IndexShard operation be more explicit about whether they are expected to run on a primary or replica</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15282</link><project id="" key="" /><description>This commit cherry picks some infrastructure changes from the `feature/seq_no` branch to make merging from master easier.

More explicitly, IndexShard current have  prepareIndex and prepareDelete methods that are called both on the primary as the replica, giving it a different origin parameter. Instead, this commits creates two explicit prepare_OnPrimary and prepare_OnReplica methods. This has the extra added value of not expecting the caller to use an Engine enum.

Also, the commit adds some code reuse between TransportIndexAction and TransportDeleteAction and their TransportShardBulkAction counter parts.
</description><key id="120801401">15282</key><summary>Make IndexShard operation be more explicit about whether they are expected to run on a primary or replica</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-07T15:43:48Z</created><updated>2015-12-11T13:16:44Z</updated><resolved>2015-12-08T08:35:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-07T15:45:18Z" id="162562687">@jasontedor this should be familiar can you take a look?

After proving it's stable on master, I think it makes sense to cherry pick to 2.x to keep the code similar. 
</comment><comment author="jasontedor" created="2015-12-07T15:55:01Z" id="162565353">LGTM.
</comment><comment author="bleskes" created="2015-12-07T18:26:38Z" id="162614985">@jasontedor pushed another commit. It looks like we weren't ready for this yet on master.
</comment><comment author="bleskes" created="2015-12-08T08:36:28Z" id="162813521">merged to master. I'll give it a day and merge to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allocate primary shards based on allocation IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15281</link><project id="" key="" /><description>- Add allocation IDs to `TransportNodesListGatewayStartedShards` action.
- Use the above to assign a primary shard on recovery.
- Also add allocation id to indices shard store response (`/some_index/_shard_stores`)

Relates to #14739
</description><key id="120785950">15281</key><summary>Allocate primary shards based on allocation IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-07T14:31:59Z</created><updated>2015-12-17T14:59:29Z</updated><resolved>2015-12-17T14:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-09T14:50:10Z" id="163272735">Pushed changes related to our discussion:
- Use allocation ids and index creation version to determine `allocatedPostIndexCreate`
- Added integration test for #15241 and #14671
</comment><comment author="bleskes" created="2015-12-14T11:42:41Z" id="164416932">Thanks @ywelsch . I think this looks great. I left some comments and also want to ping @dakrone to discuss the recover on any node option. I hope we can get this simpler... 
</comment><comment author="ywelsch" created="2015-12-15T11:31:16Z" id="164736324">@bleskes I pushed a new set of changes that address your comments.
</comment><comment author="bleskes" created="2015-12-15T13:30:32Z" id="164765221">This looks great. Left some minor comment and one important one about the recover on any node settings. Also let's have another discussion with @dakrone about the failing test.
</comment><comment author="ywelsch" created="2015-12-16T16:26:59Z" id="165164779">Pushed another set of changes, dealing with recover_on_any_node.
</comment><comment author="bleskes" created="2015-12-17T11:53:49Z" id="165434038">LGTM. Left some extremely minor comments. No need for another review. Just merge after they are addressed. Thanks @ywelsch ! I can't tell you how happy I am for having this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhance Help for plugin authors page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15280</link><project id="" key="" /><description>This page is often confusing for the new 2.x release. Some links don't work (EG: https://github.com/elastic/elasticsearch/blob/master/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties , and the information on how to create plugins is not very clear.

It will be great to have a better description on how to create plugins.
</description><key id="120783576">15280</key><summary>Enhance Help for plugin authors page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">gmoskovicz</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2015-12-07T14:21:39Z</created><updated>2016-11-14T21:33:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T11:30:59Z" id="164736277">I've fixed the URL for the plugin-descriptor file in 2.x, and updated this page for gradle in master.

@rjernst you wanted to provide more info for plugin authors?
</comment><comment author="rjernst" created="2016-01-18T23:23:35Z" id="172679603">@clintongormley I will jot my notes about info for plugin authors here. I'm not sure what the best place for them is, but I figure you will know.

In gradle, users do the equivalent of the shared pom by importing the `elasticsearch.esplugin` gradle plugin. In a `build.gradle` file, that looks something like this:

```
buildscript {
  repositories {
    maven {
      name 'sonatype-snapshots'
      url "https://oss.sonatype.org/content/repositories/snapshots/"
    }
  }
  dependencies {
    classpath "org.elasticsearch.gradle:build-tools:3.0.0-snapshot"
  }
}
apply plugin: 'elasticsearch.esplugin'
```

Note that the maven part should be changed to `mavenCentral()` once we've actually released master.

Once a plugin author has applied this gradle plugin to their build, they get a number of builtin tasks, as well as configuration options.

## Configuration

Configuration of the ES plugin happens using the `esplugin` extension. A full configured `esplugin` with all options specified would look like the following:

```
esplugin {
  name 'my-plugin' // required, the name of the plugin which will be used as a unique identifier within ES
  version '3.0.0' // optional, this is an arbitrary version number for your plugin, and defaults to the version of elasticsearch
  description 'My plugin does foo and bar' // required, a short description of what the plugin does
  classname 'com.foo.MyPlugin' // required, the full qualified class name that implements o.e.plugins.Plugin
}
```

## Tasks

In addition to simply building the plugin, a number of tasks are added, which are the same as Elasticsearch itself uses to check the software.

### precommit

These tasks do static checks on the plugin code. They do things like check for jarhell (before actually running tests, rather than get crazy errors in the tests themselves), check for forbidden patterns like `nocommit`, run forbidden apis, etc. (We can list these out explicitly along with better descriptions, but these can and will change in the future so the list may get out of sync.

### test

These are unit tests, run with the same randomized runner that Lucene and ES tests are based on. All test classes should end with the name "Tests".

### integTest

These are REST integration tests. They spin up an ES node, and run rest tests against it. The yaml files for the rest tests should be in `src/test/resources/rest-api-spec/test`.

I'm sure I'm missing something, but that is a simple breakdown of what `esplugin` provides.
</comment><comment author="clintongormley" created="2016-01-19T18:06:14Z" id="172936254">@rjernst could you also provide a few common commands, eg how do you build, test, package, how do you run the precommit task?  how do you deploy?
</comment><comment author="rjernst" created="2016-01-19T19:35:19Z" id="172961408">I'm not sure we should write a gradle tutorial...there are plenty out there.

For deploying to nexus, they can add the following to their file, and the esplugin is already set up to generate a correct pom:

```
apply plugin: 'com.bmuschko.nexus'
```

All 3 of the tasks i mentioned (`precommit`, `test` and `integTest`) are run as part of `check`, which is a standard gradle task you get out of the box.

I also forgot to mention before some additional things:
- Transitive dependencies are not allowed, at least in the standard `compile` and `testCompile` configurations. 
- Standard repositories are added automatically by applying the `elasticsearch.esplugin` plugin. These are maven central, sonatype snapshots, and the lucene snapshot repo for snapshot builds of ES.
- javac is configured with the `compact3` profile by default (can be changed by setting the `compactProfile` setting in the project). It is also set to run doclint.
- The jar built for the plugin automatically includes information about the version of ES, lucene and java used to build it, as well as repository information (if git is used).
- A licenses directory is expected with licenses for each dependency, and shas. The shas can be updated using `gradle updateShas`, and the task can be disabled with `dependencyLicenses.enabled = false`.
</comment><comment author="villasv" created="2016-10-25T14:50:57Z" id="256057621">I came across this same issue https://github.com/spinscale/elasticsearch-ingest-opennlp/issues/8 here with gradle 2.10 (from ubuntu apt). Are there still gradle version limitations for the `esplugin`?
</comment><comment author="nik9000" created="2016-10-25T14:55:34Z" id="256059036">&gt; Are there still gradle version limitations for the esplugin?

Yes. We've got verbal assurance from the gradle folks that they'll fix the issue on their side so we're just waiting.... For now 2.13 is required. No less and no more, sadly.
</comment><comment author="stephenprater" created="2016-11-14T19:56:06Z" id="260444130">This is weird, since you can't actually even get Gradle 2.13 from the gradle distribution site anymore. 

Edit - You can get it here. 

```
  url "https://services.gradle.org/distributions/gradle-2.13-bin.zip"
  sha256 "0f665ec6a5a67865faf7ba0d825afb19c26705ea0597cec80dd191b0f2cbb664"
```
</comment><comment author="dadoonet" created="2016-11-14T21:33:03Z" id="260469449">What I did recently is to create an empty Gradle project and run 

```
gradle wrapper --gradle-version 2.13
```

to get a wrapper. Then I copied files to elasticsearch dir.

HTH
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace MovingAvgModel.Streams with NamedWriteable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15279</link><project id="" key="" /><description>At the moment MovingAvgModels are written and read to the wire format by the MovingAvgModel.Streams class. This pre-dated the NamedWritable interface but we should now move it over to this framework as its the preferred method of serializing named implementations to the wire format

Relates to #10217
</description><key id="120757672">15279</key><summary>Replace MovingAvgModel.Streams with NamedWriteable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2015-12-07T11:52:14Z</created><updated>2017-02-14T14:34:57Z</updated><resolved>2017-02-14T14:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T18:45:44Z" id="183948785">@colings86 has this been addressed yet?
</comment><comment author="colings86" created="2016-02-15T09:39:16Z" id="184134176">Not yet. This is a non-urgent issue since there is no bug here it would just be cleaner if it used the NamedWritable infrastructure. It will at least need to wait until the agg refactoring is merged into master
</comment><comment author="nik9000" created="2017-02-14T14:34:57Z" id="279723261">I did it!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add infrastructure to transactionally apply and reset dynamic settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15278</link><project id="" key="" /><description>This commit adds the infrastructure to make settings that are updateable
resetable and changes the application of updates to be transactional. This means
setting updates are either applied or not. If the application failes all values are rejected.

This initial commit converts all dynamic cluster settings to make use of the new infrastructure.
All cluster level dynamic settings are not resettable to their defaults or to the node level settings.
The infrastructure also allows to list default values and descriptions which is not fully implemented yet.

Values can be reset using a list of key or simple regular expressions. This has only been implemented on the java
layer yet. For instance to reset all recovery settings to their defaults a user can just specify `indices.recovery.*`.

This commit also adds strict settings validation, if a setting is unknown or if a setting can not be applied the entire
settings update request will fail.
</description><key id="120757417">15278</key><summary>Add infrastructure to transactionally apply and reset dynamic settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-07T11:50:39Z</created><updated>2016-01-12T16:19:25Z</updated><resolved>2015-12-18T09:57:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-07T13:45:44Z" id="162528050">this issue basically closes #7704 and is a first cut at #6732
</comment><comment author="nik9000" created="2015-12-08T14:47:48Z" id="162904353">Briefly looked at it last night but I'll actually review it now.
</comment><comment author="nik9000" created="2015-12-08T18:03:29Z" id="162963913">OK! I made it through. I left a bunch of comments/suggestions but I can't promise that I caught everything. The first points worth thinking about are:
1. If you make `Settings.Builder`'s `put` and `get` accept `Setting` objects **and** you don't rename the settings from their old names (when they were just strings) then you could shrink this diff considerably. I think that'd be nice just for review purposes.
2. I wonder if `Setting.get` could return an `AtomicReference` to the setting value so it'd force client code to take into account its volatility. `Setting.get` would have to change signatures but maybe its ok?
3. I wonder if it'd be easier to read the `Settings` creation if it were done with a builder instead of helper methods. Its difficult for me as a reviewer to hold the proper order of the arguments in my head.

I think third set of eyes should look at this, particularly to make sure that all the validation is intact.
</comment><comment author="nik9000" created="2015-12-08T21:11:18Z" id="163019016">Cool. Thanks for making the changes. They look great. I'll look at it again in my morning so I can have fresh eyes but don't hold it up for me if someone else reviews it first. Its not worth blocking, even if it does introduce a bunch of methods will parameter lists that are hard to read. :stuck_out_tongue_closed_eyes:

Its a good change and if I can come up with something to replace the parameter lists I'll send a PR after its merged.
</comment><comment author="s1monw" created="2015-12-08T21:22:00Z" id="163021506">@nik9000 thanks so much for reviewing this beast. I am going to work on the other changes in my morning.
</comment><comment author="nik9000" created="2015-12-08T21:23:52Z" id="163021886">&gt; this beast

Took a long time too! And if its still not merged when I get up in the morning I'll go over the whole thing again so I can make sure we aren't dropping any validation. That is faster than accidentally removing it I think.
</comment><comment author="clintongormley" created="2015-12-09T11:17:52Z" id="163191992">Awesome!  REST part LGTM
</comment><comment author="s1monw" created="2015-12-09T19:20:49Z" id="163363064">@bleskes @clintongormley @nik9000 can you take another look? I addressed all comments so far
</comment><comment author="nik9000" created="2015-12-09T19:39:25Z" id="163368259">@bleskes said he was going to read it and try to understand it so I'm going to give him a chance before I do another review. Got to share the love.
</comment><comment author="bleskes" created="2015-12-10T11:11:19Z" id="163580917">I like this a _lot_. Left some comments/questions about the infrastructure. Will do a bigger review (on all the small changes) when those a resolved. Will be a shame to do it twice...
</comment><comment author="s1monw" created="2015-12-10T17:07:11Z" id="163689251">@bleskes I pushed all commits necessary for the next round
</comment><comment author="bleskes" created="2015-12-11T15:32:14Z" id="163964963">I reviewed the production code and quickly glanced at the tests (sorry, I really have to go now :)). This looks great. I left a couple of suggestions , all minor in nature.
</comment><comment author="s1monw" created="2015-12-15T15:40:30Z" id="164801692">@bleskes pushed a new commit and merge with master
</comment><comment author="bleskes" created="2015-12-16T09:34:43Z" id="165046642">LGTM except for the array setting support. I'm fine with doing this as a followup as it seems we only have one dynamic cluster level setting that relies on that and that one can live just fine with the comma based. Note though that the array notation is used in more places which are non-dynamic (discovery.zen.ping.unicast.hosts, for example). I think we should do those as well (as a followup)?
</comment><comment author="s1monw" created="2015-12-16T11:26:29Z" id="165072674">@bleskes I added yet another special case for setting which pisses me off but I have no choice. this has been fucked up in the past and is hard to fix for the future. Anyway I think it's ready and I will push very soon
</comment><comment author="bleskes" created="2015-12-16T12:27:46Z" id="165091643">@s1monw thanks for the extra effort. Sadly, I don't think this is enough as we still don't deal correctly with how we parse json arrays into key.0 , key.1 etc. The request I gave above still fails and we move none dynamic settings into this infra we will have similar options. As I said before - I think we should push this in as iterate on this. We do need to support `discovery.zen.ping.unicast.hosts: [ ... ]` and `network.bind_host: [ ... ]` etc.
</comment><comment author="s1monw" created="2015-12-16T13:17:15Z" id="165101231">@bleskes ok I added several tests that this works :)
</comment><comment author="bleskes" created="2015-12-16T14:14:19Z" id="165119151">@s1monw thanks. I dug deeper as it still didn't work. Here's the problem, I think:

```
Index: core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
&lt;+&gt;UTF-8
===================================================================
--- core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java    (date 1450271602000)
+++ core/src/main/java/org/elasticsearch/common/settings/AbstractScopedSettings.java    (revision )
@@ -43,7 +43,7 @@
             if (entry.getScope() != scope) {
                 throw new IllegalArgumentException("Setting must be a cluster setting but was: " + entry.getScope());
             }
-            if (entry.isGroupSetting()) {
+            if (entry.hasComplexMatcher()) {
                 complexMatchers.put(entry.getKey(), entry);
             } else {
                 keySettings.put(entry.getKey(), entry);

```

It seems we need one more test?
</comment><comment author="s1monw" created="2015-12-16T19:18:05Z" id="165216814">more tests tests and tests I push this now it's not going to be better.
</comment><comment author="derjohn" created="2016-01-12T16:19:25Z" id="170962604">\o/\o/\o/ THX !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Best approach for searching doc against fixed list of keywords </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15277</link><project id="" key="" /><description>Hi all,

I have a fixed list of String keywords  against which i want to search my document and then extract the matching keywords. I read about term lookup as one of the approaches but that would work for not_analyzed String. I want to do full text search.
Can anyone suggest the best approach for same.

Thanks!!
</description><key id="120748581">15277</key><summary>Best approach for searching doc against fixed list of keywords </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shagga</reporter><labels /><created>2015-12-07T11:06:04Z</created><updated>2015-12-08T03:53:26Z</updated><resolved>2015-12-08T03:53:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>es 2.X Start time&#65292;load average very high</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15276</link><project id="" key="" /><description>es version 2.1
java version "1.8.0_65"
Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
top - 16:21:05 up 121 days, 21:04,  2 users,  load average: 22.55, 7.90, 3.23
2322 tomcat    20   0 24.6g  19g  14m S 100.4 15.7   1:10.81 java
</description><key id="120739185">15276</key><summary>es 2.X Start time&#65292;load average very high</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">z-star</reporter><labels /><created>2015-12-07T10:10:14Z</created><updated>2015-12-15T11:13:21Z</updated><resolved>2015-12-15T11:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T11:13:21Z" id="164731475">@z-star i suggest asking in the forums: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PUT `index/_mapping` doesn't work atm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15275</link><project id="" key="" /><description>At the moment we define an endpoint which takes a PUT verb and `{index}/_mapping` as path. How every, when you try to use it, it fails:

```
PUT index/_mapping
{
  "type": {
    "properties": {
      "field": { "type": "string"}
    }
  }
}
```

Results in:

```
{
   "error": {
      "root_cause": [
         {
            "type": "action_request_validation_exception",
            "reason": "Validation Failed: 1: mapping type is missing;"
         }
      ],
      "type": "action_request_validation_exception",
      "reason": "Validation Failed: 1: mapping type is missing;"
   },
   "status": 400
}
```

Sniffing in the code it seems one has to supply a type in the URL, i.e., `PUT index/_mapping/type` and in which case you can omit or keep the type in the body (both works). However, trying to use two types, silently drops second type (if the first one matches the url pattern), or gets annoyed:

```
PUT index/_mapping/type
{
  "type2":{
    "properties": {
      "field": { "type": "string"}
    }
  },
  "type":{
    "properties": {
      "field": { "type": "string"}
    }
  }
}
```

resulting in

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "Root mapping definition has unsupported parameters:  [type2 : {properties={field={type=string}}}] [type : {properties={field={type=string}}}]"
         }
      ]
}
```

Imho we should support multiple types in one request, or alternatively disable the endpoint.

/dd @pmusa 
</description><key id="120738395">15275</key><summary>PUT `index/_mapping` doesn't work atm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Index APIs</label><label>discuss</label><label>enhancement</label></labels><created>2015-12-07T10:05:41Z</created><updated>2016-11-04T14:05:47Z</updated><resolved>2016-11-04T14:05:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alicia-c" created="2016-11-02T16:55:40Z" id="257927251">+1
</comment><comment author="clintongormley" created="2016-11-04T14:05:47Z" id="258440026">I'm going to close this in favour or removing types altogether (https://github.com/elastic/elasticsearch/issues/15613)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Doc] Redundant indefinite article removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15274</link><project id="" key="" /><description /><key id="120729895">15274</key><summary>[Doc] Redundant indefinite article removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">murnieza</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-12-07T09:20:03Z</created><updated>2015-12-11T13:38:13Z</updated><resolved>2015-12-11T13:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-09T19:34:31Z" id="163366895">Thanks @murnieza, can you sign the [CLA](https://www.elastic.co/contributor-agreement/) so I can merge this in?
</comment><comment author="murnieza" created="2015-12-10T07:58:02Z" id="163527697">I already did that. Can you guide me through this? I guess only some minor checkbox or button click is missing somewhere.
</comment><comment author="clintongormley" created="2015-12-11T13:38:06Z" id="163937813">The CLA has showed up.  Thanks @murnieza - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PHP Elastica Client v3.1.6 has error on indexing after upgrade from ES 1.1.1 to 1.7.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15273</link><project id="" key="" /><description>I have just upgrade ES version from 1.1.1 to 1.7.3. Followed this guide: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/setup-upgrade.html#rolling-upgrades

Everything is fine, except that PHP Elastica Client (version 3.1.6) show errors when indexing documents into new Elasticsearch 1.7.3 (This error didn't happen in version 1.1.1 before upgrading):

[message] =&gt; Error in one or more bulk request actions:
update: /indexname/type/5078 caused EsRejectedExecutionException[rejected execution (queue capacity 50) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1@6ec548ff]

Is there any difference on default queue capacity of bulk indexing between 2 version of Elasticsearch (1.1.1 - 1.7.3)?
</description><key id="120729065">15273</key><summary>PHP Elastica Client v3.1.6 has error on indexing after upgrade from ES 1.1.1 to 1.7.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jackiekhuu-work</reporter><labels /><created>2015-12-07T09:15:05Z</created><updated>2015-12-15T11:03:29Z</updated><resolved>2015-12-15T11:03:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T11:03:29Z" id="164729388">Hi @jackiekhuu-work 

Lots has changed between 1.1 and 1.7.  It looks like you are flooding ES with too many bulk requests without waiting for responses.  If you have further questions, I'd ask them in the forum instead: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CircuitBreaker Issue  /flow of data from  types  to  heap size </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15272</link><project id="" key="" /><description>incase if  I have a diffferent types  and having a common name of  a field created by.When  I querying  with that field with respect to a particular type , will it load  the data from  all type to the heap memory  or only from the type which i have refered. due to this  frequent circuitbreaker exception  occuring 
</description><key id="120714649">15272</key><summary>CircuitBreaker Issue  /flow of data from  types  to  heap size </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justies</reporter><labels /><created>2015-12-07T07:15:53Z</created><updated>2015-12-07T16:20:31Z</updated><resolved>2015-12-07T16:20:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-07T16:20:30Z" id="162574417">Hi @justies,

Please ask future questions on https://discuss.elastic.co , Github is used for communication about bugs and enhancements to Elasticsearch. To answer your question, all types' fielddata will be loaded. You should switch to [doc values](https://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html#doc-values) to alleviate frequent circuit breaking exceptions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is CONFIG_SECCOMP in kernel necessary to run es?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15271</link><project id="" key="" /><description>I found some warnings when I run elasticsearch.

```
[2015-12-07 14:27:23,687][WARN ][bootstrap                ] unable to install syscall filter: seccomp unavailable: CONFIG_SECCOMP not compiled into kernel, CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER are needed
```

Here is the check:

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/bootstrap/Seccomp.java#L348

Would bad things happen if I ignore it?
</description><key id="120709864">15271</key><summary>Is CONFIG_SECCOMP in kernel necessary to run es?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">c9n</reporter><labels /><created>2015-12-07T06:43:13Z</created><updated>2015-12-07T12:43:12Z</updated><resolved>2015-12-07T12:43:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-07T12:43:11Z" id="162515052">No, its just that if you became vulnerable to a security bug in java itself, you would be left defenseless.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight not working along with term lookup filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15270</link><project id="" key="" /><description>Hi All,
I'm new to elastic search and have started exploring it from the past few days. My requirement is to get the matched keywords using term lookup highlighted.    

So have 2 indices 

http://localhost:9200/lookup/type/1?pretty
Output
{
  "_index" : "lookup",
  "_type" : "type",
  "_id" : "1",
  "_version" : 1,
  "found" : true,
  "_source":{"terms":["Apache
Storm","Kafka","MR","Pig","Hive","Hadoop","Mahout"]}
}

And another one as following:-

http://localhost:9200/skillsetanalyzer/resume/_search?fields=keySkills
output
{"took":19,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":3,"max_score":1.0,"hits":[{"_index":"skillsetanalyzer","_type":"resume","_id":"1","_score":1.0,"fields":{"keySkills":["Core
Java","J2EE","Struts 1.x","SOAP based
Web Services using JAX-WS","Maven","Ant","JMS","Apache
Storm","Kafka","RDBMS
(MySQL","Tomcat","Weblogic","Eclipse","Toad","TIBCO
product Suite (Administrator","Business
Work","Designer","EMS)","CVS","SVN"]}},

And below query returns the correct results but does not highlight the matched keywords. 

curl -XGET 'localhost:9200/skillsetanalyzer/resume/_search?pretty' -d '
{ 

"query":
   {"filtered": 
       {"filter":
            {"terms":
                   {"keySkills":
                         {"index":"lookup", 
                           "type":"type",
                            "id":"1",
                             "path":"terms"
                   },
                   "_cache_key":"1"
             }
         }
     }
 },
"highlight": {
           "fields":{
                "keySkills":{}
                }
        }
}'

Field "KeySkills" is not analyzed and its type is String. I'm not able to make out what is wrong with the
query.
Please help in providing the necessary pointers.

Thanks !!
</description><key id="120706827">15270</key><summary>Highlight not working along with term lookup filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shagga</reporter><labels><label>:Highlighting</label></labels><created>2015-12-07T06:21:30Z</created><updated>2015-12-09T02:42:25Z</updated><resolved>2015-12-09T02:42:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-07T18:10:04Z" id="162611135">A couple of things:
1. This is probably something better asked on the forum. In general highlighting is funky and its kind of "best effort" so this isn't so much a bug as yet another wart. But its cool to keep it here because most of the rest of Elasticsearch is less funky and things should work.
2. You should totally surround stuff in ``` for multiline code blocks or ` for inline code blocks. It makes reading these easier.
3. Requests you include in issues should include the `pretty` url parameter. The responses should be generated with that. They are so much easier to scan!
4. I think your issue is actually caused by the way in which highlighting extracts the terms to highlight - can you try using the `terms` query instead of the `terms` filter?
</comment><comment author="shagga" created="2015-12-09T02:42:25Z" id="163088544">Thanks Nik !!  I'll take care of the points in future. 
HIghlighting worked with term query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hidden files should be ignored by the ScriptsService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15269</link><project id="" key="" /><description>I do store ES configs in git repo.
It required to have `scripts` sub folder, but i can't commit to git empty folder, need to put there .gitkeep file. 
I got this exception:

```
es_data_1     | java.lang.IllegalArgumentException: script file extension not supported [gitkeep]
es_data_1     |     at org.elasticsearch.script.ScriptService.getScriptEngineServiceForFileExt(ScriptService.java:221)
es_data_1     |     at org.elasticsearch.script.ScriptService.access$1200(ScriptService.java:83)
es_data_1     |     at org.elasticsearch.script.ScriptService$ScriptChangesListener.onFileInit(ScriptService.java:537)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.onFileCreated(FileWatcher.java:256)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:166)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.createChild(FileWatcher.java:173)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.listChildren(FileWatcher.java:188)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.onDirectoryCreated(FileWatcher.java:299)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.init(FileWatcher.java:162)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher$FileObserver.access$000(FileWatcher.java:75)
es_data_1     |     at org.elasticsearch.watcher.FileWatcher.doInit(FileWatcher.java:65)
es_data_1     |     at org.elasticsearch.watcher.AbstractResourceWatcher.init(AbstractResourceWatcher.java:36)
es_data_1     |     at org.elasticsearch.watcher.ResourceWatcherService.add(ResourceWatcherService.java:133)
es_data_1     |     at org.elasticsearch.watcher.ResourceWatcherService.add(ResourceWatcherService.java:126)
es_data_1     |     at org.elasticsearch.script.ScriptService.&lt;init&gt;(ScriptService.java:192)
es_data_1     |     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
es_data_1     |     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
es_data_1     |     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
es_data_1     |     at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
```
</description><key id="120698625">15269</key><summary>Hidden files should be ignored by the ScriptsService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nizsheanez</reporter><labels><label>:Scripting</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-12-07T04:42:58Z</created><updated>2016-06-24T11:01:52Z</updated><resolved>2016-03-09T22:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-07T18:16:52Z" id="162612700">It looks like you can work around this by creating a file without any extension - those are ignored by the ScriptsService.

I'll let someone more opinionated than I am decide if this is a bug or a feature. I will say that it wouldn't be a ton of work to ignore hidden files in this directory. To declare them "not a script" and just log a WARNING or something.
</comment><comment author="nik9000" created="2015-12-07T18:17:44Z" id="162612889">I'm guessing this just showed up in 2.0. @nizsheanez, can you confirm when you first saw it?
</comment><comment author="rjernst" created="2015-12-07T19:17:26Z" id="162629380">We check for this with plugins, we should use the same utility: `FileSystemUtils.isHidden`
</comment><comment author="nik9000" created="2015-12-07T19:25:31Z" id="162632161">&gt; We check for this with plugins, we should use the same utility: FileSystemUtils.isHidden

Yeah. Also it feels wrong that we should ever be ok running hidden scripts. That just screams "abuse me!"
</comment><comment author="nik9000" created="2015-12-07T19:26:16Z" id="162632433">@nizsheanez I've changed the title to make it more obvious what behavior you were relying on before and marked this as a bug.
</comment><comment author="rjernst" created="2015-12-07T19:28:35Z" id="162633413">&gt;  Also it feels wrong that we should ever be ok running hidden scripts. 

I don't think this would actually happen, because the "hidden script" would have an empty string for a name (and I assume, maybe incorrectly, that the script service checks for an empty string when a file script is specified)
</comment><comment author="fforbeck" created="2016-01-27T01:47:35Z" id="175336180">Hey,
I was looking at the ScriptService.java, mainly in the inner class ScriptChangesListener.
We could ignore the hidden files on the **onFileInit(Path file)** method if we add the check that was suggested above: _FileSystemUtils.isHidden_, then add some warning as well. 
If you guys agree, can I take this one?
</comment><comment author="rjernst" created="2016-01-27T02:09:52Z" id="175343569">@fforbeck Please do!
</comment><comment author="fforbeck" created="2016-01-28T10:17:09Z" id="176106808">@rjernst 
Alright! I sent a PR. 
Could you please take a look when possible?

Thanks
</comment><comment author="mostolog" created="2016-06-24T08:01:02Z" id="228281993">Is there any chance this being backported to 2.3.x ? 
</comment><comment author="jasontedor" created="2016-06-24T11:01:52Z" id="228317585">&gt; Is there any chance this being backported to 2.3.x ?

That is very unlikely.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>S3 Snaphot via Corporate Proxy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15268</link><project id="" key="" /><description>Currently unable to access S3 for snapshot using a proxy that requires authentication.
Cloud-AWS plugin does not include options proxy_username proxy_password.
See the following thread:
https://discuss.elastic.co/t/snapshot-to-s3-via-corporate-proxy/36431
</description><key id="120670741">15268</key><summary>S3 Snaphot via Corporate Proxy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">berniemoran</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>enhancement</label></labels><created>2015-12-06T23:25:44Z</created><updated>2015-12-09T23:01:43Z</updated><resolved>2015-12-09T23:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-07T08:29:21Z" id="162447253">I was looking at it this morning and I think it's easy to implement it using [ClientConfiguration#setProxyHost](http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/ClientConfiguration.html#setProxyHost%28java.lang.String%29).
Same for `setProxyPort`, `setProxyUsername` and `setProxyPassword`.

I'll look at it later unless anyone else would like to contribute this "small" change.
</comment><comment author="dadoonet" created="2015-12-07T19:34:48Z" id="162635017">Actually we support `cloud.aws.proxy_host` and `cloud.aws.proxy_port`. Just missing `username` and `password`.

I'd suggest BTW to: 
- rename `cloud.aws.proxy_host` to `cloud.aws.proxy.host`
- rename `cloud.aws.proxy_port` to `cloud.aws.proxy.port`
- rename `cloud.aws.s3.proxy_host` to `cloud.aws.s3.proxy.host`
- rename `cloud.aws.s3.proxy_port` to `cloud.aws.s3.proxy.port`
- add `cloud.aws.proxy.username`
- add `cloud.aws.proxy.password`
- add `cloud.aws.s3.proxy.username`
- add `cloud.aws.s3.proxy.password`

And same for `discovery-ec2`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field data loading is forbidden on [FIELDNAME]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15267</link><project id="" key="" /><description>when i Click &#8220;visualize&#8221; and then ,display error "Visualize:java.lang.lllegalStateException:Field daVisualize",or i used dashboard module ,before logstash auto update index,it was normal;when time is 8:00 am,dashboard module will not work,and display the error.

error details:

1.kibana page error:
Error: Request to Elasticsearch failed: {"error":{"root_cause":[{"type":"exception","reason":"java.lang.IllegalStateException: Field data loading is forbidden on path"}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"logstash-2015.11.29","node":"ODyds4KgQsqrxczx1ana8A","reason":{"type":"exception","reason":"java.lang.IllegalStateException: Field data loading is forbidden on path","caused_by":{"type":"unchecked_execution_exception","reason":"java.lang.IllegalStateException: Field data loading is forbidden on path","caused_by":{"type":"illegal_state_exception","reason":"Field data loading is forbidden on path"}}}}]}} KbnError@http://www.xxx.cn:80/bundles/c ... 64:30 RequestFailure@http://www.xxx.cn:80/bundles/c ... 97:19 http://www.xxx.cn:80/bundles/k ... 05:57 http://www.xxx.cn:80/bundles/c ... 91:28 http://www.xxx.cn:80/bundles/c ... 60:31 map@[native code] map@http://www.xxx.cn:80/bundles/c ... 59:34 callResponseHandlers@http://www.xxx.cn:80/bundles/k ... 77:26 http://www.xxx.cn:80/bundles/k ... 84:37 processQueue@http://www.xxx.cn:80/bundles/c ... 09:31 http://www.xxx.cn:80/bundles/c ... 25:40 $eval@http://www.xxx.cn:80/bundles/c ... 53:29 $digest@http://www.xxx.cn:80/bundles/c ... 64:37 $apply@http://www.xxx.cn:80/bundles/c ... 61:32 done@http://www.xxx.cn:80/bundles/c ... 10:54 completeRequest@http://www.xxx.cn:80/bundles/c ... 08:16 requestLoaded@http://www.xxx.cn:80/bundles/commons.bundle.js:37749:25

2.elasticsearch error log:

[2015-11-30 00:04:53,395][DEBUG][action.search.type       ] [Thumbelina] [logstash-2015.11.29][2], node[acrTX4O0RciN8ppbSdfoww], [P], v[4], s[STARTED], a[id=ogwkZP0yQCCgatW0_tnvnw]: Failed to execute [org.elasticsearch.action.search.SearchRequest@aef2ffe] lastShard [true]
RemoteTransportException[[Thumbelina][192.168.1.76:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on response]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is forbidden on response]; nested: IllegalStateException[Field data loading is forbidden on response];
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on response]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is forbidden on response]; nested: IllegalStateException[Field data loading is forbidden on response];
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
        at org.elasticsearch.shield.transport.ShieldServerTransportService$ProfileSecuredRequestHandler.messageReceived(ShieldServerTransportService.java:165)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on response]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is forbidden on response]; nested: IllegalStateException[Field data loading is forbidden on response];
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:82)
        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$FieldData.bytesValues(ValuesSource.java:195)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.getLeafCollector(StringTermsAggregator.java:73)
        at org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:132)
        at org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:38)
        at org.apache.lucene.search.MultiCollector.getLeafCollector(MultiCollector.java:117)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:763)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
        ... 11 more
Vian  00:18:36
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Field data loading is forbidden on response
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:156)
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:76)
        &#8230; 19 more
Caused by: java.lang.IllegalStateException: Field data loading is forbidden on response
        at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.fail(DisabledIndexFieldData.java:68)
        at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.loadDirect(DisabledIndexFieldData.java:54)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:163)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:156)
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        &#8230; 23 more
</description><key id="120644830">15267</key><summary>Field data loading is forbidden on [FIELDNAME]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">velion</reporter><labels /><created>2015-12-06T16:45:25Z</created><updated>2016-05-25T10:41:01Z</updated><resolved>2015-12-11T09:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mirnujAtom" created="2015-12-07T14:14:55Z" id="162534602">Having the same issue: kibana 4.3, elasticsearch 2.1.0
On a testing server it works fine, the only difference I see is java version, 1.8.0_40 on testing and 1.8.0_65 on a problem one.
</comment><comment author="Isabaellchen" created="2015-12-09T16:57:00Z" id="163323635">Same here, had to re-index with the same data twice, because a shutdown caused my translogs to be corrupted, now, the second time, i get "Field data loading is forbidden on timestamp" when i want to do anything in kibana... Had this error before on other fields, which i circled around by using raw fields. Maybe i am managing my data in an undesired way (different indeces for different parts of our architecture each split by date again, yes templates are present and i can use raw fields as well as geoip data, so this shouldnt be a problem). Standalone usage btw. on java 1.8.0_45

```
[2015-12-09 16:26:42,037][DEBUG][action.search.type       ] [White Tiger] [apache-inter-2015.11.28][2], node[kxW8JOWWQSCBtblWQkggKA], [P], v[2], s[STARTED], a[id=6WMOShhmQ9eeY9wKDH9FVA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4737155a] lastShard [true]
RemoteTransportException[[White Tiger][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: SearchParseException[failed to parse search source [{"size":500,"sort":[{"timestamp":{"order":"desc","unmapped_type":"boolean"}}],"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"range":{"@timestamp":{"gte":1446332400000,"lte":1448924399999,"format":"epoch_millis"}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}},"require_field_match":false,"fragment_size":2147483647},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"12h","time_zone":"Europe/Berlin","min_doc_count":0,"extended_bounds":{"min":1446332400000,"max":1448924399999}}}},"fields":["*","_source"],"script_fields":{},"fielddata_fields":["@timestamp","parameters.kw_termin"]}]]; nested: IllegalStateException[Field data loading is forbidden on timestamp];
Caused by: SearchParseException[failed to parse search source [{"size":500,"sort":[{"timestamp":{"order":"desc","unmapped_type":"boolean"}}],"query":{"filtered":{"query":{"query_string":{"analyze_wildcard":true,"query":"*"}},"filter":{"bool":{"must":[{"range":{"@timestamp":{"gte":1446332400000,"lte":1448924399999,"format":"epoch_millis"}}}],"must_not":[]}}}},"highlight":{"pre_tags":["@kibana-highlighted-field@"],"post_tags":["@/kibana-highlighted-field@"],"fields":{"*":{}},"require_field_match":false,"fragment_size":2147483647},"aggs":{"2":{"date_histogram":{"field":"@timestamp","interval":"12h","time_zone":"Europe/Berlin","min_doc_count":0,"extended_bounds":{"min":1446332400000,"max":1448924399999}}}},"fields":["*","_source"],"script_fields":{},"fielddata_fields":["@timestamp","parameters.kw_termin"]}]]; nested: IllegalStateException[Field data loading is forbidden on timestamp];
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:848)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:651)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:617)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalStateException: Field data loading is forbidden on timestamp
    at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.fail(DisabledIndexFieldData.java:68)
    at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.comparatorSource(DisabledIndexFieldData.java:64)
    at org.elasticsearch.search.sort.SortParseElement.addSortField(SortParseElement.java:255)
    at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:189)
    at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:87)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:831)
    ... 10 more
```
</comment><comment author="alexzhuustc" created="2015-12-10T03:28:23Z" id="163474772">I have the same issue. 
logstash-2.1.0-1.noarch
elasticsearch-2.1.0-1.noarch
kibana-4.3.0-linux-x64

Just a simplest sample data set.
</comment><comment author="cybacolt" created="2015-12-10T08:01:23Z" id="163528170">same issue here:
Ubuntu 14.04
logstash 2.1.0-1
elasticsearch 2.1.0
Kibana 4.3.0 build 9369

i've also tried restarting all services and reindexing on elasticsearch. permissions on /var/lib/elasticsearch look fine.

i'll try rolling elastic back to an earlier version shortly.

**Kibana error:**

```
Error: Request to Elasticsearch failed: {"error":{"root_cause":[{"type":"exception","reason":"java.lang.IllegalStateException: Field data loading is forbidden on message"}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"logstash-2015.12.10","node":"wRLwuXSsSv2M0sKnxPR56g","reason":{"type":"exception","reason":"java.lang.IllegalStateException: Field data loading is forbidden on message","caused_by":{"type":"unchecked_execution_exception","reason":"java.lang.IllegalStateException: Field data loading is forbidden on message","caused_by":{"type":"illegal_state_exception","reason":"Field data loading is forbidden on message"}}}}]}}
    at http://xxxxx/bundles/kibana.bundle.js:78760:39
    at Function.Promise.try (http://xxxxx/bundles/commons.bundle.js:60967:23)
    at http://xxxxx/bundles/commons.bundle.js:60936:31
    at Array.map (native)
    at Function.Promise.map (http://xxxxx/bundles/commons.bundle.js:60935:31)
    at callResponseHandlers (http://xxxxx/bundles/kibana.bundle.js:78732:23)
    at http://xxxxx/bundles/kibana.bundle.js:78239:17
    at processQueue (http://xxxxx/bundles/commons.bundle.js:42339:29)
    at http://xxxxx/bundles/commons.bundle.js:42355:28
    at Scope.$eval (http://xxxxx/bundles/commons.bundle.js:43583:29)
```

**elasticsearch log error:**

```
        ... 22 more
[2015-12-10 14:06:32,799][DEBUG][action.search.type       ] [Brother Voodoo] All shards failed for phase: [query]
RemoteTransportException[[Brother Voodoo][localhost/127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on message]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is forbidden on message]; nested: IllegalStateException[Field data loading is forbidden on message];
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on message]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is forbidden on message]; nested: IllegalStateException[Field data loading is forbidden on message];
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:343)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:106)
        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:363)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:375)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: ElasticsearchException[java.lang.IllegalStateException: Field data loading is forbidden on message]; nested: UncheckedExecutionException[java.lang.IllegalStateException: Field data loading is forbidden on message]; nested: IllegalStateException[Field data loading is forbidden on message];
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:82)
        at org.elasticsearch.search.aggregations.support.ValuesSource$Bytes$FieldData.bytesValues(ValuesSource.java:195)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.getLeafCollector(StringTermsAggregator.java:73)
        at org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:132)
        at org.elasticsearch.search.aggregations.AggregatorBase.getLeafCollector(AggregatorBase.java:38)
        at org.apache.lucene.search.MultiCollector.getLeafCollector(MultiCollector.java:117)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:763)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:486)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:324)
        ... 10 more
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.IllegalStateException: Field data loading is forbidden on message
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937)
        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:156)
        at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:76)
        ... 18 more
Caused by: java.lang.IllegalStateException: Field data loading is forbidden on message
        at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.fail(DisabledIndexFieldData.java:68)
        at org.elasticsearch.index.fielddata.plain.DisabledIndexFieldData.loadDirect(DisabledIndexFieldData.java:54)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:163)
        at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:156)
        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
        ... 22 more
```
</comment><comment author="cybacolt" created="2015-12-10T08:41:49Z" id="163534485">this seems to have had a good outcome (visualizations are working, at least atm)... be aware this will wipe all previous index data:

```
service elasticsearch stop
service logstash stop
apt-cache remove elasticsearch
mv /var/lib/elasticsearch /var/lib/elasticsearch.back
apt-get install elasticsearch=2.0.1
service elasticsearch restart
service logstash restart
```

then repeat these steps to reinstall 2.1.0 with the good 2.0.1 `/var/lib/elasticsearch` contents.

perhaps i'd broken my indexes somehow... not sure... eitherway, its working atm.
</comment><comment author="peerster" created="2015-12-10T09:38:46Z" id="163552996">I have the same issue:
centos7
logstash-2.1.1-1.noarch
elasticsearch-2.1.0-1.noarch
kibana 4.3.0 Build 9369

EDIT: Like @mirnujAtom I also use java 1.8.0_65
</comment><comment author="Isabaellchen" created="2015-12-10T09:41:53Z" id="163555676">@cybacolt
What do you mean with "the good 2.0.1 `/var/lib/elasticsearch` content"? I am using a downloaded binary package. Is that stuff cached somewhere outside of my unzipped folder? `/var/lib/elasticsearch` does not exist here.
</comment><comment author="mirnujAtom" created="2015-12-10T09:56:09Z" id="163560659">Fixed my problem with similar to what @cybacolt suggested - moved /var/lib/elasticsearch from a working server (which was upgraded from 2.0 to 2.1) and dropped all data it used to have.
Looks like there is an issue if you do a fresh install of elastic2.1

@Isabaellchen there is a variable called  path.data: in elasticsearch.yml, check where does it refer to in your configuration.
</comment><comment author="PhaedrusTheGreek" created="2015-12-10T15:24:28Z" id="163657397">The problem happens when `"fielddata" : { "format" : "disabled" }` is set in a field's mapping, which is the default as of Logstash 2.1, due to [this issue](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/308).

Common Variations of this error:
- Field data loading is forbidden on type
- Field data loading is forbidden on path
- Field data loading is forbidden on host

In default Logstash 2.1 with Kibana 4.3+, it may come down to the fact that you have visualizing on an analyzed field. 

![screen shot 2015-12-10 at 10 11 22 am](https://cloud.githubusercontent.com/assets/4387023/11718946/6b0f727a-9f26-11e5-9237-a03e8bf0b446.png)

Running an aggregation on an analyzed field will produce the same error:

```
GET logstash-2015.12.10/_search
{
   "size": 0, 
   "aggregations": {
      "the_test": {
         "terms": {
            "field": "host"
         }
      }
   }
}
```

One should be able to solve the problem by using the `.raw` version of the field instead.

Wiping the /var/lib/elasticsearch directory also deletes the [index templates](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-templates.html) which are installed by Logstash, so that might explain why previous comments were able to workaround by doing this.  
</comment><comment author="Isabaellchen" created="2015-12-10T15:54:15Z" id="163668079">As stated above, i received the error on the timestamp field, rendering kibana completely infunctional until i reindexed. Also, i had the same data indexed before, without receiving this error, until my translogs got corrupted and i had to start from scratch.
</comment><comment author="PhaedrusTheGreek" created="2015-12-10T18:14:30Z" id="163706762">@Isabaellchen are you using Logstash?  I'm wondering because Logstash uses the @timestamp field, whereas your error shows just _timestamp_ (without the @ symbol).   Did your timestamp field's mapping happen to have `"fielddata" : { "format" : "disabled" }` set ?  
</comment><comment author="Raggyman" created="2015-12-11T05:39:10Z" id="163845815">A similar things happen if the field is a string.  In my case I have JSON that is inserted and when trying to get a unique count it fails.  This is with all the latest production versions.  
</comment><comment author="vianer" created="2015-12-11T08:03:18Z" id="163869891">thanks @PhaedrusTheGreek  ,through using the .raw version of the field instead,i solution the problem.
</comment><comment author="Isabaellchen" created="2015-12-11T09:47:36Z" id="163891572">@PhaedrusTheGreek 
Thanks for your reply. Yes i am using logstash.
Since i am reading jboss files, i generated the timestamp from the filename and the logline, thus creating an extra timestamp field to be parsed by the date filter. Maybe i accidentally chose that field for my indeces on accident, i can not check that anymore. `"fielddata" : { "format" : "disabled" }` however, was not set in my template.
</comment><comment author="clintongormley" created="2015-12-11T09:55:59Z" id="163894259">This is not a bug.  It is a safeguard.  The logstash template now disables fielddata loading where it makes sense, eg see https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/master/lib/logstash/outputs/elasticsearch/elasticsearch-template.json#L15

You get this message when you try to sort or run aggregations or scripts on analyzed fields.  Fulfilling this request would cause massive amounts of memory usage on your cluster, and it almost certainly isn't what you want anyway, eg `Field data loading is forbidden on path"`... You don't want to aggregate on the analyzed field `path`, you want to aggregate on the not analyzed field `path.raw`, which uses doc values not heap memory.

This is by design.
</comment><comment author="yodog" created="2016-01-13T13:18:09Z" id="171287264">the good old 'its not a bug, its a feature' :)

but seriously, this kind of change breaks the application to the point where we have to reindex our data.

besides the 'breaking changes' section, where else should i look before updating?
</comment><comment author="argais" created="2016-01-20T18:24:52Z" id="173314953">Now how do we turn off this safeguard so we can keep using the solutions we had in place before without changing them?
</comment><comment author="clintongormley" created="2016-01-21T14:02:37Z" id="173577963">@argais you can do it by updating the mapping for string fields as follows:

```
PUT my_index/_mapping/my_type
{
  "properties": {
    "text": {
      "type": "string",
      "fielddata": {
        "format": "paged_bytes"
      }
    }
  }
}
```
</comment><comment author="dkirrane" created="2016-01-22T13:21:29Z" id="173919272">This is happening when I sort on a column in the Kibana Discover page.
And there are no raw fields to add to the table. I can add `path` field for instance but not `path.raw`
So anytime I sort of `path` I get this exception
</comment><comment author="xamox" created="2016-01-28T04:26:26Z" id="175966840">I am also getting this using:
Official docker image Elasticsearch 2.1 as well as official docker image 2.1. And Kibana 4.3.1.  

I'm just testing with basic example from here:
https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html

Full error:

```
Error: Request to Elasticsearch failed: {"error":{"root_cause":[{"type":"illegal_state_exception","reason":"Field data loading is forbidden on timestamp"}],"type":"search_phase_execution_exception","reason":"all shards failed","phase":"query","grouped":true,"failed_shards":[{"shard":0,"index":"logstash-2016.01.28","node":"X3VNkMLlTgOuY0w1BGYTzA","reason":{"type":"illegal_state_exception","reason":"Field data loading is forbidden on timestamp"}}]}}
    at http://localhost:5601/bundles/kibana.bundle.js:89030:39
    at Function.Promise.try (http://localhost:5601/bundles/commons.bundle.js:63741:23)
    at http://localhost:5601/bundles/commons.bundle.js:63710:31
    at Array.map (native)
    at Function.Promise.map (http://localhost:5601/bundles/commons.bundle.js:63709:31)
    at callResponseHandlers (http://localhost:5601/bundles/kibana.bundle.js:89002:23)
    at http://localhost:5601/bundles/kibana.bundle.js:88507:17
    at processQueue (http://localhost:5601/bundles/commons.bundle.js:41837:29)
    at http://localhost:5601/bundles/commons.bundle.js:41853:28
    at Scope.$eval (http://localhost:5601/bundles/commons.bundle.js:43081:29)
```
</comment><comment author="wdtmatt" created="2016-02-04T20:05:16Z" id="180025956">So in the use case below, what would be the appropriate solution with this new feature?    

I'm using logstash to parse web logs.   I use kv to capture all the query strings, which in one case is a comma separated list which gets stored as string in elasticsearch, eg layers.raw:"w,x,y,z"    The results for each log entry could be any combination of w,x,y,z or all of them.      I was building my vizualization off the analyzed field "layers", because I could get a count of each call of w.   That is no longer possible with this issue.
</comment><comment author="soichih" created="2016-02-08T18:22:43Z" id="181509450">I've been hit by this issue for the last couple of weeks, and I still can't figure out what exactly I need to do to fix it.. I don't know what "field data" is for one thing.

Am I correct to assume that, I am doing something wrong with the way I am feeding the data? Is there something I need to do on my filter?

Interestingly, I am not having this problem on an ELK instance where I am loading data from AMQP (input:rabbimq, output:elasticsearch on my logstash) without any filter. 
</comment><comment author="clintongormley" created="2016-02-13T20:11:01Z" id="183746431">OK, to explain what all this means...  Analyzed string fields are for full text search.  Not-analyzed string fields are for concrete values eg HTTP method, URI, status code, etc.  Typically these fields are used for filtering (rather than full text search) and are often used in aggregations.

It seldom makes sense to aggregate on an analyzed string field, eg an analyzed string field containing "Quick brown fox" indexes the terms "quick", "brown", and "fox", and a terms aggregation on this field would aggregate on each of these terms (instead of the full "Quick brown fox", which is probably what you wanted).

On top of that, not_analyzed strings can use "doc_values", ie their values get stored on a disk in a way that they can be looked up for each document in a very efficient way while running aggregations.  Analyzed strings do not support doc values.  If you try to aggregate on an analyzed string, it reads the full index, "uninverts the values", and stores the result in memory.  This can use an ENORMOUS amount of memory and even crash your server.  You don't want to do this.

The change in https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/309 means that:
1. You can no longer run aggregations on the analyzed `message` field.
2. All dynamically added strings are added with two forms: `name` (an analyzed string field to be used for full text search but not for aggregations) and `name.raw` which can be used for filtering and aggregations.

So, instead of aggregating on field `foo`,  try aggregation on `foo.raw` instead.
</comment><comment author="installboy" created="2016-02-15T08:30:29Z" id="184110213">That makes sense this far. But is it intended, that the *.raw field is empty, althoug it is dynamically created?

I am just starting out with ELK and doing some tests with the syslog input plugin. It imports the data, but won't let me visualize it. Now your explanation makes sense to me, but the *.raw fields are all empty on my side. Wouldn't it make sense to dynamically add the data to the fields? Or am I missing something?
</comment><comment author="clintongormley" created="2016-02-15T12:37:53Z" id="184189206">@installboy the `.raw` field won't be added to the document `_source`, as it is just a duplicate of the main field. But the whole value will be indexed and stored as doc values (depending on how the field is mapped), and so can be used to search for the whole term (not for full text search) and can be used in aggregations.
</comment><comment author="soichih" created="2016-02-15T16:45:31Z" id="184294462">@clintongormley Thanks for the explanation. 

Is there anyway to make Kibana use "host.raw" (for example) field instead of "host" when user tries to visualize (or "aggregate") analyzed field? Right now, Kibana doesn't even show "host.raw" under "Available Fields" list, so I didn't know I should've used that field instead.

Currently, In order to visualize the hostname, our user needs to do following.

1) Click Visualize bar under quick count view for "host" field.
2) Dismiss "Error Visualize: java.lang.IllegalStateException"
3) Copy &amp; Paste the URL to a text editor, and replace "host" with "host.raw".
4) Reload page with the new URL.

Obviously.. having to ask all of our users to this is not an option. 

I feel that the better approach is to tell ElasticSearch to use non-analyzed field for fields like "host". If it is, is there a documentation on how to do this?
</comment><comment author="clintongormley" created="2016-02-17T15:14:02Z" id="185246791">@rashidkpc see https://github.com/elastic/elasticsearch/issues/15267#issuecomment-184294462

i was under the (possibly mistaken?) impression that Kibana automatically selected the `.raw` field if available, but this appears not to be the case here?
</comment><comment author="kmoe" created="2016-02-24T18:21:23Z" id="188390242">@clintongormley it doesn't for me - in fact, I get a `Could not locate that index-pattern-field (id: level.raw)` when I try to use the `.raw` version of `level`. Is there any reason why the `.raw` fields might be missing?
</comment><comment author="soichih" created="2016-02-25T15:18:03Z" id="188832969">&gt; Kibana automatically selected the .raw field

Not for me either. But if it did, it would solve this issue for me. 
</comment><comment author="kmoe" created="2016-02-25T16:18:51Z" id="188860397">The fix for me turned out to be to regenerate the index pattern. This can be done in the Kibana GUI by going to Settings, deleting the index pattern, and recreating it with default settings. The `.raw` fields then appeared.
Of course, be careful not to lose your data!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If we can't get a MAC address for the node, use a dummy one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15266</link><project id="" key="" /><description>We already use a dummy MAC address if we hit a `SocketException` while trying; this PR just widens that to any `Throwable`.

Closes #10099 
</description><key id="120626865">15266</key><summary>If we can't get a MAC address for the node, use a dummy one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-06T11:11:36Z</created><updated>2015-12-06T13:20:01Z</updated><resolved>2015-12-06T13:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-12-06T12:27:36Z" id="162310183">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Doc] Replace "now" by "no" in introduction sentence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15265</link><project id="" key="" /><description>Also add a missing comma.
</description><key id="120625163">15265</key><summary>[Doc] Replace "now" by "no" in introduction sentence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">damienalexandre</reporter><labels><label>docs</label></labels><created>2015-12-06T10:42:46Z</created><updated>2015-12-07T15:45:55Z</updated><resolved>2015-12-07T15:42:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-07T06:20:21Z" id="162421545">LGTM, I will merge this in the morning unless someone beats me to it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct typo in class name of StatsAggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15264</link><project id="" key="" /><description>Closes #14730
</description><key id="120604931">15264</key><summary>Correct typo in class name of StatsAggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">martinstuga</reporter><labels><label>:Aggregations</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-06T03:31:43Z</created><updated>2015-12-07T15:46:21Z</updated><resolved>2015-12-07T15:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-12-07T08:45:24Z" id="162449650">@martinstuga thanks for the PR, could you sign the Contributer Licence Agreeement (https://www.elastic.co/contributor-agreement/) so we can merge this?
</comment><comment author="martinstuga" created="2015-12-07T08:49:31Z" id="162450507">@colings86 I've already signed CLA twice. I don't know why, but it's still appearing as not signed. May you help me please?
</comment><comment author="colings86" created="2015-12-07T09:55:04Z" id="162467494">@martinstuga it looks like the email you used to sign the CLA is different from the email on your commit. You could add the email address you use in your commits to your Github profile (you can make it private so it doesn't show up on your public profile) and this will allow our CLA checker tool to link your Github commits with your CLA signature in the future.
</comment><comment author="martinstuga" created="2015-12-07T12:07:20Z" id="162506549">Thanks for the help about the CLA. I'm seeing that you're going ahead with this issue.
</comment><comment author="colings86" created="2015-12-07T14:41:27Z" id="162543765">@martinstuga unfortunately at the moment the build fails with your change applied. Could you update this PR to fix the error, rebase your change onto the latest master and ensure `gradle assemble` runs successfully?

The error I get when running `gradle assemble` is:

```
$ES_REPO/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/StatsParser.java:37: error: package StatsAggegator does not exist
        return new StatsAggegator.Factory(aggregationName, config);
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#14719  url params parsing should not be lenient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15263</link><project id="" key="" /><description>There is my initial commit with a generic and centralized parameter validation.

This validation is being done just for GET requests. Validation on the other methods will have to be done en every single rest request.
</description><key id="120604009">15263</key><summary>#14719  url params parsing should not be lenient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinstuga</reporter><labels /><created>2015-12-06T03:05:21Z</created><updated>2015-12-07T15:48:32Z</updated><resolved>2015-12-07T15:48:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martinstuga" created="2015-12-06T23:01:49Z" id="162361452">I've signed the CLA, don't know why GitHub is complaining.

Anybody can help me?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter classes loaded by scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15262</link><project id="" key="" /><description>Since 2.2 we run all scripts with minimal privileges, similar to applets in your browser.
The problem is, they have unrestricted access to other things they can muck with (ES, JDK, whatever).
So they can still easily do tons of bad things.

This PR restricts what classes scripts can load via the classloader mechanism, to make life more difficult.
The "standard" list was populated from the old list used for the groovy sandbox: though
a few more were needed for tests to pass (java.lang.String, java.util.Iterator, nothing scary there).

Additionally, each scripting engine typically needs permissions to some runtime stuff.
That is the downside of this "good old classloader" approach, but I like the transparency and simplicity,
and I don't want to waste my time with any feature provided by the engine itself for this, I don't trust them.

This is not perfect and the engines are not perfect but you gotta start somewhere. For expert users that
need to tweak the permissions, we already support that via the standard java security configuration files, the specification is simple, supports wildcards, etc (though we do not use them ourselves).
</description><key id="120603754">15262</key><summary>Filter classes loaded by scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>breaking</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-06T02:57:12Z</created><updated>2016-01-31T12:20:44Z</updated><resolved>2015-12-06T15:29:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-06T06:09:39Z" id="162276170">LGTM.
</comment><comment author="s1monw" created="2015-12-07T08:32:39Z" id="162447802">it's awesome that we can do that now!!! thanks @rmuir 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.x doc: Minimum required java version is incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15261</link><project id="" key="" /><description>On https://www.elastic.co/guide/en/elasticsearch/reference/2.x/setup.html#jvm-version, first line says that minimum java requirement is version 8. The next line says that it is works with version 7 update 55 as well (although recommended version is 8). 

First line should be fixed to say that ES 2.x minimum required Java is version 7 and not 8.
</description><key id="120595792">15261</key><summary>ES 2.x doc: Minimum required java version is incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">au78</reporter><labels /><created>2015-12-05T23:34:39Z</created><updated>2015-12-05T23:58:15Z</updated><resolved>2015-12-05T23:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-05T23:58:15Z" id="162258159">Thanks for reporting this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default include_global_state to "false" for restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15260</link><project id="" key="" /><description>Currently when performing a restore from a snapshot, `include_global_state` defaults to true. I would like to propose that we change this to default to `false` as it is safer.

This way when restoring a single index, a user does not accidentally overwrite their index templates and similar items with older versions.
</description><key id="120583108">15260</key><summary>default include_global_state to "false" for restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2015-12-05T19:52:16Z</created><updated>2016-05-25T13:23:45Z</updated><resolved>2016-05-25T13:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-15T10:53:49Z" id="164726274">+1
</comment><comment author="clintongormley" created="2016-05-25T13:23:45Z" id="221574517">Closed in favour of #18569
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify shard inactive logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15259</link><project id="" key="" /><description>This commit simplifies shard inactive debug logging to only log when the
physical shard is marked as inactive. This eliminates duplicate logging
that existed in IndexShard#checkIdle and
IndexingMemoryController#checkIdle, and eliminates excessive logging
that was occurring when the shard was already inactive as a result of
the work in #15252.
</description><key id="120569491">15259</key><summary>Simplify shard inactive logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-05T16:24:53Z</created><updated>2015-12-22T02:15:18Z</updated><resolved>2015-12-05T16:30:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-05T16:38:27Z" id="162219781">Received implied LGTM from @bleskes through another channel.
</comment><comment author="makeyang" created="2015-12-18T01:34:17Z" id="165633020">@jasontedor  how do u define inactive? without write or witheout read and write?
</comment><comment author="nik9000" created="2015-12-18T02:46:35Z" id="165654269">&gt; how do u define inactive

Just writes. This is for the purpose of apportioning the index buffer.
</comment><comment author="jasontedor" created="2015-12-22T02:15:18Z" id="166481401">&gt; Just writes.

Yup. Note that a delete counts as a write. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistency naming between mapping's "analysis" and _analyze API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15258</link><project id="" key="" /><description>_analyze API uses `filters`/`token_filters` while mapping's "analysis" uses `filter` and `char_filter`.
I feel it's better if both use the same parameter.
</description><key id="120536739">15258</key><summary>Inconsistency naming between mapping's "analysis" and _analyze API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Analysis</label></labels><created>2015-12-05T06:55:22Z</created><updated>2015-12-05T13:00:11Z</updated><resolved>2015-12-05T13:00:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T13:00:11Z" id="162183473">Duplicate of #15189
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_analyze API fails if char_filters is set in body.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15257</link><project id="" key="" /><description>If it's passed as URL parameter, it works.

It looks _analyze API sets char_filters as token_filters:
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/action/admin/indices/analyze/RestAnalyzeAction.java#L132
</description><key id="120536531">15257</key><summary>_analyze API fails if char_filters is set in body.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">masaruh</reporter><labels><label>:Analysis</label><label>bug</label></labels><created>2015-12-05T06:48:42Z</created><updated>2015-12-10T16:15:26Z</updated><resolved>2015-12-10T16:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T12:58:10Z" id="162182903">@johtani want to take a look?
</comment><comment author="johtani" created="2015-12-10T08:22:39Z" id="163531498">@clintongormley Yes. And I will fix in #11660.
</comment><comment author="johtani" created="2015-12-10T16:15:26Z" id="163674157">Closed by master : fab44398d9d48f12319bc018d4b436f723b6508e 
2.x : f427cf4ae459b864894c8ba1a3eff589ad5248a0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cli tools: Use verbose flag for printing stack trace instead of a system property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15256</link><project id="" key="" /><description>Cli tools currently have a special system property the look for to
indicate whether error should have their stack trace printed. It is the
only use of this "debug" property, and confusing (and more work for the
user to set it). This change reuses the existing verbose mode for
printing stack traces, and removes the debug property. If someone says
they want verbose output, that includes stack traces.
</description><key id="120505560">15256</key><summary>Cli tools: Use verbose flag for printing stack trace instead of a system property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>enhancement</label></labels><created>2015-12-04T23:14:06Z</created><updated>2016-02-02T14:51:26Z</updated><resolved>2016-02-02T00:38:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-02-02T00:38:45Z" id="178274150">Closing in favor of #16359
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running junit tests in IntelliJ 15 results in "JAVA_HOME must be set" error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15255</link><project id="" key="" /><description>I discovered this while attempting to build master as of last night.

I found that I can run unit tests in IntelliJ 14, but not IntelliJ 15. This is the error in 15:

```
FAILURE: Build failed with an exception.
* Where:
Build file '[removed]/elasticsearch/core/build.gradle' line: 24
* What went wrong:
A problem occurred evaluating project ':core'.
&gt; Failed to apply plugin [id 'elasticsearch.build']
   &gt; JAVA_HOME must be set to build Elasticsearch
* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.
```

The error appears to be happening on [this line](https://github.com/elastic/elasticsearch/blob/master/buildSrc/src/main/groovy/org/elasticsearch/gradle/BuildPlugin.groovy#L124). It looks like the "idea.active" property is not being set in my copy of Intellij15. 

Locally, I found a property the intellij 15 did set, and altered the code to also check that property:

``` java
if (System.getProperty("idea.active") != null || System.getProperty("idea.home.path") != null) 
```

It seems like this kind of solution might get out of hand, so I'm not sure if it's a good/workable solution or not. I assume there are disadvantages to using `Jvm.current().javaHome` for all cases, or else the original author of the code might have done that. Can anyone explain further why the findJavaHome method is structured the way it is?
</description><key id="120503881">15255</key><summary>Running junit tests in IntelliJ 15 results in "JAVA_HOME must be set" error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martiansnoop</reporter><labels><label>build</label><label>feedback_needed</label></labels><created>2015-12-04T22:59:55Z</created><updated>2015-12-08T07:59:53Z</updated><resolved>2015-12-08T07:59:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-04T23:16:56Z" id="162107738">@martiansnoop Did you import into intellij as a gradle project, or simply open the generated file based project `gradle idea` produces?
</comment><comment author="rjernst" created="2015-12-04T23:29:13Z" id="162109607">&gt; Can anyone explain further why the findJavaHome method is structured the way it is?

This is due to gradle being broken with java 9. In order to allow running tests with jdk9 / jigsaw we use JAVA_HOME to indicate which java to use for compiling/testing. This means we can set org.gradle.java.home to tell gradle which jvm to use, and separately set JAVA_HOME. 

In intellij,they don't set JAVA_HOME when they spin up jvms for tests or gradle tasks. Unfortunately they don't see this as a problem...
https://youtrack.jetbrains.com/issue/IDEA-147331

Also, the fact your hitting this suggest to me you tried running the test from intellij through "gradle" instead of JUnit? This is something I hope we can support, but at least right now I do not believe it works (I think it tries running all unit tests when you try running a single one).
</comment><comment author="martiansnoop" created="2015-12-05T23:51:04Z" id="162257785">&gt; Also, the fact your hitting this suggest to me you tried running the test from intellij through "gradle" instead of JUnit?

Yes, this is correct, I was trying to run tests through gradle (IntelliJ 15 defaulted to it). I set up a JUnit configuration instead and tests run for me in intellij now without modifying the line in `BuildPlugin.groovy`. Thanks for the suggestion!

&gt; Did you import into intellij as a gradle project, or simply open the generated file based project gradle idea produces?

I ran `gradle idea`, then went to `File &gt; New &gt; Project from existing sources...`, selected the directory where I cloned the repo, and followed the menus after that. (I am unfamiliar with the workings of gradle, so this may have been a suboptimal sequence.)
</comment><comment author="rjernst" created="2015-12-08T07:59:53Z" id="162806979">@martiansnoop Thanks for the info. Since you confirmed running junit tests do work, I'm closing this. Hopefully sometime in the future we can make running tests through gradle from the IntelliJ work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove duplicate runs of packaged rest tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15254</link><project id="" key="" /><description>We currently use the full suite of packaged rest tests for each
distribution. We also used to run rest tests within core integ tests,
but this stopped working when we split out the test-framework, since the
test files are in there.

This change simplifies the code to run packaged rest tests just once,
for the integ-test-zip, and removes the unused rest tests from
test-framework. Distributions rest tests now check that all modules
were loaded.
</description><key id="120492490">15254</key><summary>Remove duplicate runs of packaged rest tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T21:43:34Z</created><updated>2015-12-04T23:39:15Z</updated><resolved>2015-12-04T23:39:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-04T23:36:22Z" id="162110533">+1 this is great.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ban RuntimePermission("getClassLoader")</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15253</link><project id="" key="" /><description>This is checked by `Class.forName()` and `Class.getClassLoader()`. It gives more isolation between modules and plugins: e.g. plugin A does not need to be worrying about plugin B's classloader.

most stuff is clean here already, however in some cases some third party code has problems, but we contain it as usual and add TODOs to fix these problems and clean up.
</description><key id="120486431">15253</key><summary>Ban RuntimePermission("getClassLoader")</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T21:07:59Z</created><updated>2016-02-01T11:31:17Z</updated><resolved>2015-12-04T23:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-04T21:12:08Z" id="162083026">LGTM
</comment><comment author="spinscale" created="2015-12-08T09:51:46Z" id="162834286">This seems to have broken master. When starting an elasticsearch instance from a zip file, I get this:

```
[2015-12-08 10:50:31,291][INFO ][node                     ] [Polaris] version[3.0.0-SNAPSHOT], pid[20776], build[0809e4a/2015-12-08T09:16:54.109Z]
[2015-12-08 10:50:31,292][INFO ][node                     ] [Polaris] initializing ...
[2015-12-08 10:50:31,583][ERROR][bootstrap                ] Exception
ElasticsearchException[Failed to load plugin class [org.elasticsearch.script.expression.ExpressionPlugin]]; nested: ExceptionInInitializerError; nested: AccessControlException[access de
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:453)
        at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:402)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:114)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:150)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:131)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:179)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:293)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.ExceptionInInitializerError
        at org.elasticsearch.script.expression.ExpressionPlugin$1.run(ExpressionPlugin.java:45)
        at org.elasticsearch.script.expression.ExpressionPlugin$1.run(ExpressionPlugin.java:41)
        at java.security.AccessController.doPrivileged(Native Method)
        at org.elasticsearch.script.expression.ExpressionPlugin.&lt;clinit&gt;(ExpressionPlugin.java:41)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.elasticsearch.plugins.PluginsService.loadPlugin(PluginsService.java:445)
        ... 8 more
Caused by: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "getClassLoader")
        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
        at java.security.AccessController.checkPermission(AccessController.java:884)
        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
        at java.lang.ClassLoader.checkClassLoaderPermission(ClassLoader.java:1525)
        at java.lang.Class.getClassLoader(Class.java:683)
        at org.apache.lucene.expressions.js.JavascriptCompiler.checkFunction(JavascriptCompiler.java:706)
        at org.apache.lucene.expressions.js.JavascriptCompiler.&lt;clinit&gt;(JavascriptCompiler.java:695)
        ... 17 more
```
</comment><comment author="rmuir" created="2015-12-08T11:17:23Z" id="162853210">This change has been in jenkins for 4 days. its fine. You got some other problem, perhaps specific to you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify IndexingMemoryController#checkIdle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15252</link><project id="" key="" /><description>This commit further simplifies IndexingMemoryController#checkIdle after
the changes in #15251.
</description><key id="120485913">15252</key><summary>Simplify IndexingMemoryController#checkIdle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T21:04:57Z</created><updated>2015-12-05T16:40:47Z</updated><resolved>2015-12-04T21:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-04T21:08:43Z" id="162082366">LGTM, nice!
</comment><comment author="bleskes" created="2015-12-05T15:36:48Z" id="162212681">This is a great simplifications. It has a side effect for logging as we now log every 30s:

```
[2015-12-05 16:29:15,043][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:29:15,044][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:29:45,046][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:29:45,046][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:30:15,049][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:30:15,049][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:30:45,050][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:30:45,051][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:31:15,056][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:31:15,056][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:31:45,061][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:31:45,061][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:32:15,062][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:32:15,062][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:32:45,065][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:32:45,065][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:33:15,070][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:33:15,071][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:33:45,077][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:33:45,077][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
[2015-12-05 16:34:15,083][DEBUG][indices.memory           ] [Squirrel Girl] marking shard [index][0] as inactive (inactive_time[5m]) indexing wise
[2015-12-05 16:34:15,083][DEBUG][indices.memory           ] [Squirrel Girl] no active shards
```

we should change this to only log on change...
</comment><comment author="jasontedor" created="2015-12-05T16:27:10Z" id="162218253">&gt; It has a side effect for logging as we now log every 30s:

Good catch. We were duplicate logging the first time the shard went inactive from the logging in [`IndexShard#checkIdle`](https://github.com/elastic/elasticsearch/blob/73a0cc6488744518dd7e6890ea417cf8def9c9ca/core/src/main/java/org/elasticsearch/index/shard/IndexShard.java#L1037) and [`IndexingMemoryController#checkIdle`](https://github.com/elastic/elasticsearch/blob/73a0cc6488744518dd7e6890ea417cf8def9c9ca/core/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java#L297-L299) so I think we can just move all of this logging to `IndexShard#checkIdle` and only log when the physical shard is marked as inactive. I opened #15259.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexingMemoryController should not track shard index states</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15251</link><project id="" key="" /><description>This commit modifies IndexingMemoryController to be stateless. Rather
than statefully tracking the indexing status of shards,
IndexingMemoryController can grab all available shards, check their idle
state, and then resize the buffers based on the number of and which
shards are not idle.

The driver for this change is a performance regression that can arise in
some scenarios after #13918. One scenario under which this performance
regression can arise is if an index is deleted and then created
again. Because IndexingMemoryController was previously statefully
tracking the state of shards via a map of ShardIds, the new shards with
the same ShardIds as previously existing shards would not be detected
and therefore their version maps would never be resized from the
defaults. This led to an explosion in the number of merges causing a
degradation in performance.

Closes #15225
</description><key id="120475757">15251</key><summary>IndexingMemoryController should not track shard index states</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T20:02:44Z</created><updated>2015-12-04T20:18:42Z</updated><resolved>2015-12-04T20:18:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-12-04T20:08:31Z" id="162069239">LGTM, this is a great simplification, thanks @jasontedor!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>:CAT API: remove space at the end of a line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15250</link><project id="" key="" /><description>Rebase and fixes for inactive pull request: https://github.com/elastic/elasticsearch/pull/10865
Fixes #9464
</description><key id="120466314">15250</key><summary>:CAT API: remove space at the end of a line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T19:11:03Z</created><updated>2015-12-14T10:21:10Z</updated><resolved>2015-12-08T14:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-07T15:48:51Z" id="162563615">Left one really minor comment, other than that LGTM
</comment><comment author="jimczi" created="2015-12-08T13:40:42Z" id="162882409">Thanks @dakrone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighter doesn't work with String Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15249</link><project id="" key="" /><description>Highlighting doesn't work with Query String Query.
After upgrading from ES 1.7.1 to 2.1, highlighting stopped working with Query String Query.
does work 

sample query:
All fields are stored in the mapping and source is stored as well

curl -XPOST '192.168.56.101:9201/_search?pretty' -d '{
  "from" : 0,
  "size" : 1,
  "timeout" : 120000,
  "query" : {
    "query_string" : {
      "query" : "alex"
    }
  },
  "highlight" : {
    "fields" : {
      "*" : { }
    }
  }
}'
</description><key id="120460714">15249</key><summary>Highlighter doesn't work with String Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidmav</reporter><labels /><created>2015-12-04T18:41:02Z</created><updated>2015-12-04T18:59:18Z</updated><resolved>2015-12-04T18:46:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-04T18:46:28Z" id="162048496">You are querying the `_all` field and highlighting every field in your documents. In general it would be much better to highlight the same fields that you are querying against, but in this case you can't because the `_all` field is not stored. You need to set `require_field_match` to `false` for that to work, see #10627 . 
</comment><comment author="davidmav" created="2015-12-04T18:59:18Z" id="162052262">Appreciate the super fast response, I don't know how are you doing this!
Works now!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] extract core ingest to separate module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15248</link><project id="" key="" /><description>At the moment the processors are in the same module and have the same classpath as the ingest plugin. We should change that and make the processors a different module that doesn't depend on elasticsearch (so we can actually enforce that rather than depending on reviews), while the rest of the codebase is the actual integration between the processors and elasticsearch.
</description><key id="120447687">15248</key><summary>[Ingest] extract core ingest to separate module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label></labels><created>2015-12-04T17:24:39Z</created><updated>2016-01-11T17:05:38Z</updated><resolved>2016-01-11T17:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-01-11T17:05:38Z" id="170619516">The plan has changed quite a bit since I created this issue. Most of the code has been moved to es core. There are a few classes though where we try not to depend on es, under `o.e.ingest.core` , which one day we might want to take out and make a separate library. The main processors are also part of core. grok processor is a module, while geoip stays as a plugin for now. Nothing to do here at this point, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Also tests certain combination of processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15247</link><project id="" key="" /><description>What it comes to testing right now we have this:
- Unit tests that test all processors in isolation. 
- Java integration tests, that test all the Java level api integration. 
- Rest tests running in a real ES node with ingest installed, that test the rest api level integrations and test if processors are able to be used in pipelines. (e.g. not running in class loader issues or security manager issues)

What currently is lacking is that there aren't really tests, that test pipelines with all the possible combination of processors that can exist. Adding a true random test for this is a lot of work and we will end up with a test that will be hard to maintain, but what we can do it add certain pipeline configurations that are likely to be common and test that with realistic documents. This test can just extend from `ESTestCase` and we can create the pipeline configurations programatically. (no need to start a test with ES nodes)
</description><key id="120447131">15247</key><summary>[Ingest] Also tests certain combination of processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T17:21:07Z</created><updated>2016-02-13T22:00:02Z</updated><resolved>2016-02-02T21:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-02-02T21:40:36Z" id="178840649">Fixed via #16352 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Azure Service Management API V2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15246</link><project id="" key="" /><description>Follow up #15209.

Azure team released new versions of their Java SDK.

According to https://github.com/Azure/azure-sdk-for-java/wiki/Azure-SDK-for-Java-Features, it comes with 2 versions.
We updated to V1 0.9.0 with #15232.

We should consider moving to the new APIs (V2).

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
    &lt;artifactId&gt;azure-mgmt-compute&lt;/artifactId&gt;
    &lt;version&gt;0.9.0&lt;/version&gt;
&lt;/dependency&gt;
```
</description><key id="120440139">15246</key><summary>Update Azure Service Management API V2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>upgrade</label></labels><created>2015-12-04T16:46:00Z</created><updated>2016-07-21T12:06:43Z</updated><resolved>2016-07-21T12:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-21T12:06:43Z" id="234234535">Actually same as #19146. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make MappedFieldType.checkTypeName part of MappedFieldType.checkCompatibility.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15245</link><project id="" key="" /><description>This does not really fix anything, but I hope that by reducing the number
of methods we have to validate mappings, we can make them simpler.
</description><key id="120434629">15245</key><summary>Make MappedFieldType.checkTypeName part of MappedFieldType.checkCompatibility.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T16:19:01Z</created><updated>2015-12-07T17:43:01Z</updated><resolved>2015-12-07T17:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-04T19:20:12Z" id="162058138">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Format mapping not working in 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15244</link><project id="" key="" /><description>Using the latest version of Elasticsearch 2.1.0 on Windows 7

The date format mapping appears to be ignored.

Set a format in mappings

``` JSON
curl -XPUT http://localhost:9200/myindex1 '{
  "mappings": {
    "document": {
      "properties": {
        "date_field": {
          "type": "date",
          "format": "yyyy/MM/dd"
        }
      }
    }
  }
}
```

Ingest a document

``` JSON
curl -XPUT http://localhost:9200/myindex1/mydoc/1 '{
    "date_field": "2013/12/23"
}'

```

Throws following exception

```
15:01:30,123 INFO  [elasticsearch[platform-webapp-ISVGREPC0000738][index][T#2]:rest.suppressed]: /com.detica.elasticsearch.platform.module.bulk.bulkidsretrievecomponentit1/mydoc/1 Params: {id=1, index=com.detica.elasticsearch.platform.module.bulk.bulkidsretrievecomponentit1, type=mydoc}
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [date_field]
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:339) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:314) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.parseAndMergeUpdate(DocumentParser.java:762) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.parseDynamicValue(DocumentParser.java:676) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:447) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:267) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:127) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:318) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:551) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:542) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.action.support.replication.TransportReplicationAction.prepareIndexOperationOnPrimary(TransportReplicationAction.java:1049) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.action.support.replication.TransportReplicationAction.executeIndexRequestOnPrimary(TransportReplicationAction.java:1060) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:170) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.performOnPrimary(TransportReplicationAction.java:579) [elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase$1.doRun(TransportReplicationAction.java:452) [elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-2.1.0.jar:2.1.0]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_79]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_79]
    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]
Caused by: java.lang.IllegalArgumentException: Invalid format: "2013/12/23" is malformed at "/12/23"
    at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.8.2.jar:2.8.2]
    at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.8.2.jar:2.8.2]
    at org.elasticsearch.index.mapper.core.DateFieldMapper$DateFieldType.parseStringValue(DateFieldMapper.java:360) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:526) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:213) ~[elasticsearch-2.1.0.jar:2.1.0]
    at org.elasticsearch.index.mapper.FieldMapper.parse(FieldMapper.java:331) ~[elasticsearch-2.1.0.jar:2.1.0]
    ... 19 more
```

**Yml settings file**
Single es node

cluster.name: myTestCluster
index.number_of_replicas: 0
index.number_of_shards: 1
http.enabled: true
http.cors.enabled: true
http.cors.allow-origin: "*"

**Funky Scary Strange Part**
I ended up debugging line by line to see if I could catch what the format was being set to, or overridden to cause this issue. This was in:
DateFieldMapper.dateTimeFormatter()
DateFieldMapper.setDateTimeFormatter()

But the document index action worked this time once it went through the DocumentParser. But the weird thing is the mapping had magically changed by itself from

"yyyy/MM/dd" to "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis"

I.e i did a "_mapping" GET request to confirm and it was no longer yyyy/MM/dd. I havn't a clue how that is even possible. This never happens when it is not working but whatever it did fixed being able to add documents at that moment.

Note if I manually set the date format myself to "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis" in a new index I still cannot add a document. 

In any case when the add document is failing the date format appears to be using the 'default' rather than my custom format. 
</description><key id="120428754">15244</key><summary>Date Format mapping not working in 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SMUnlimited</reporter><labels /><created>2015-12-04T15:49:40Z</created><updated>2015-12-05T12:33:11Z</updated><resolved>2015-12-05T12:33:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SMUnlimited" created="2015-12-04T16:55:00Z" id="162020315">Never mind, it helps to have the same document type in your mapping and ingest. I was using "mydoc" for the ingest and "document" for the mapping.

I believe I have an idea how the odd scary behaviour happened. I think I somehow added a document that autogenerated the schema for a new document type but because it was the same field the default date format gets merged in with the custom specified mapping.

If that scenario is valid and covers the funky scary part of this issue, feel free to close. 
</comment><comment author="clintongormley" created="2015-12-05T12:33:11Z" id="162177978">thanks @SMUnlimited 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate that fields are defined only once.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15243</link><project id="" key="" /><description>There are two ways that a field can be defined twice:
- by reusing the name of a meta mapper in the root object (`_id`, `_routing`,
  etc.)
- by defining a sub-field both explicitly in the mapping and through the code
  in a field mapper (like ExternalMapper does)

This commit adds new checks in order to make sure this never happens.

Close #15057
</description><key id="120427550">15243</key><summary>Validate that fields are defined only once.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T15:44:10Z</created><updated>2015-12-15T10:53:41Z</updated><resolved>2015-12-15T09:38:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-04T15:44:46Z" id="162000139">Still to be defined: how to deal with pre-existing mappings that have the problem? Should we ignore the problem for old versions or should we try to "fix" the mapping?
</comment><comment author="rjernst" created="2015-12-04T19:19:19Z" id="162057797">The change LGTM. For how to deal with existing broken mappings, I think we must fail? eg if a user has an object and field mapper with the same path, what would we do? "choose" one, effectively removing the other? The configuration is broken, the user should be alerted so they can fix their mappings (which will require a reindex).
</comment><comment author="clintongormley" created="2015-12-05T12:22:33Z" id="162177157">I don't know how to replicate the ExternalMapper case, but for meta-fields that are also defined as properties, I think it would be safe to drop the version inside properties when upgrading.  In 1.7:

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "_id": {
          "type": "string",
          "index": "analyzed"
        }
      }
    }
  }
}

GET t/_mapping/t/field/*?include_defaults
```

The `_id` meta field is defined correctly (with `index: no`) and the `_id` body field doesn't exist (even though the field exists in the JSON mapping).

Taking it one step further, if i define the `_id` body field to be an object:

```
DELETE t

PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "_id": {
          "type": "object",
          "properties": {
            "foo": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}

GET t/_mapping/t/field/*?include_defaults
```

... the `_id` meta field is still mapped correctly and the `foo` sub-field doesn't exist.  In fact, if you try to index `_id` as an object:

```
PUT t/t/1
{
  "_id": {"foo": 5}, "bar": 5
}

GET t/_mapping
```

... it screws up the parsing: `foo` is treated as though it were top-level, not as part of the `_id` object, then it reaches the `}` before `bar` and thinks it has finished parsing the document, so `bar` doesn't get parsed at all.
</comment><comment author="jpountz" created="2015-12-08T19:09:58Z" id="162984854">Thanks @clintongormley. So from what I can tell, this won't be an issue when upgrading directly from 1.x as mapping updates that try to add metadata fields under the root are ignored. On the other hand 2.x allows to put such meta fields under the root object, and the upgrade would fail if we start rejecting it. However, DocumentParser rejects documents that have a metadata field at the root, so this can only happen with an explicit mapping update. So in the end I suspect that very few users would be impacted, so Ryan's proposal to fail at parsing time sounds more appealing to me than attempting to fix broken mappings (which is dangerous)?
</comment><comment author="jpountz" created="2015-12-09T08:09:52Z" id="163146279">I also pushed a new commit that tests that this validation also works for indices created on 2.x, which have their metadata mappers added to the root object.
</comment><comment author="clintongormley" created="2015-12-14T11:21:16Z" id="164413355">@jpountz sounds good to me.  Rather make this solid than introduce complications to handle edge cases.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>--path.config should be --path.conf</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15242</link><project id="" key="" /><description>In https://www.elastic.co/guide/en/elasticsearch/reference/master/rolling-upgrades.html `--path.config` should be `--path.conf` (in "Step 3: Stop and upgrade a single node").
</description><key id="120410234">15242</key><summary>--path.config should be --path.conf</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/palecur/following{/other_user}', u'events_url': u'https://api.github.com/users/palecur/events{/privacy}', u'organizations_url': u'https://api.github.com/users/palecur/orgs', u'url': u'https://api.github.com/users/palecur', u'gists_url': u'https://api.github.com/users/palecur/gists{/gist_id}', u'html_url': u'https://github.com/palecur', u'subscriptions_url': u'https://api.github.com/users/palecur/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1779279?v=4', u'repos_url': u'https://api.github.com/users/palecur/repos', u'received_events_url': u'https://api.github.com/users/palecur/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/palecur/starred{/owner}{/repo}', u'site_admin': False, u'login': u'palecur', u'type': u'User', u'id': 1779279, u'followers_url': u'https://api.github.com/users/palecur/followers'}</assignee><reporter username="">cwurm</reporter><labels><label>docs</label></labels><created>2015-12-04T14:17:18Z</created><updated>2015-12-05T12:31:27Z</updated><resolved>2015-12-05T12:31:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T12:31:27Z" id="162177822">thanks @cwurm - fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index creation context lost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15241</link><project id="" key="" /><description>If an index is created and not all shards are allocated (i.e. go to state `STARTED`) before full restart of cluster, the restarted cluster tries to recover the index using Gateway but cannot find any metadata for it and shards remain unassigned.

The same happens for snapshot restore. If some shards don't finish recovery before full restart, they remain unassigned after restart. Even worse, if there are shards of a closed index that were there when we started the restore process, these are used to recover.

The same also happens when a shard that is initializing due to index creation fails (e.g. due to shard store being read-only).

To reproduce for index creation case:
- Set `cluster.routing.allocation.enable` to `none` as `transient` setting.
- create new index.
- restart cluster

The reason for this behavior is that the shard routing table, which contains creation context (created as new index or restore from snapshot), is not persisted with the index metadata.

Relates to #14739
</description><key id="120410083">15241</key><summary>Index creation context lost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2015-12-04T14:16:23Z</created><updated>2016-12-05T16:52:36Z</updated><resolved>2015-12-30T10:06:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-12-30T10:06:35Z" id="167971886">Closed by #15281
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade documentation missing info on how to upgrade from 2.0 to 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15240</link><project id="" key="" /><description>On https://www.elastic.co/guide/en/elasticsearch/reference/master/setup-upgrade.html the table is missing a line on how to upgrade from 2.0 to 2.1 (or more general from 2.x to 2.1 and later).
</description><key id="120407720">15240</key><summary>Upgrade documentation missing info on how to upgrade from 2.0 to 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/palecur/following{/other_user}', u'events_url': u'https://api.github.com/users/palecur/events{/privacy}', u'organizations_url': u'https://api.github.com/users/palecur/orgs', u'url': u'https://api.github.com/users/palecur', u'gists_url': u'https://api.github.com/users/palecur/gists{/gist_id}', u'html_url': u'https://github.com/palecur', u'subscriptions_url': u'https://api.github.com/users/palecur/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1779279?v=4', u'repos_url': u'https://api.github.com/users/palecur/repos', u'received_events_url': u'https://api.github.com/users/palecur/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/palecur/starred{/owner}{/repo}', u'site_admin': False, u'login': u'palecur', u'type': u'User', u'id': 1779279, u'followers_url': u'https://api.github.com/users/palecur/followers'}</assignee><reporter username="">cwurm</reporter><labels><label>docs</label></labels><created>2015-12-04T14:03:14Z</created><updated>2015-12-05T12:28:49Z</updated><resolved>2015-12-05T12:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-05T12:28:49Z" id="162177722">thanks @cwurm - fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to get and set ttl as a time value/string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15239</link><project id="" key="" /><description>Specifying the ttl as date math is possible only through the REST layer at the moment. Same should be possible using the java api, namely IndexRequest, which should keep track of the time value as a TimeValue object instead. This makes it possible for plugins like ingest to intercept an index request, eventually retrieve the _ttl, which can then be retrieved as a string, and modify/set it back to the index request, again as a string which will get parsed into a time value expression.

This is the 2.x backport of #15047, it's exactly the same change but it also takes care of backwards compatibility on the transport layer.
</description><key id="120398187">15239</key><summary>Allow to get and set ttl as a time value/string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label><label>v2.2.0</label></labels><created>2015-12-04T13:08:49Z</created><updated>2015-12-14T10:22:10Z</updated><resolved>2015-12-08T15:06:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-04T13:09:05Z" id="161963262">@bleskes @martijnvg can you please have a look?
</comment><comment author="martijnvg" created="2015-12-04T14:29:24Z" id="161980346">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>missing transitive dependencies are not detected anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15238</link><project id="" key="" /><description>When you add or modify a dependency in a plugin, for example, changing azure SDK from 0.7.0 to 0.9.0, you need with gradle to explicitly declare all transitive dependencies.

For example in #15232, upgrading from `com.microsoft.azure:azure-core:0.7.0` to `com.microsoft.azure:azure-core:0.9.0` pulls another transitive dependency as we can see when we run `gradle dependencies`:

```
_transitive_com.microsoft.azure:azure-core:0.9.0
\--- com.microsoft.azure:azure-core:0.9.0
     +--- org.apache.httpcomponents:httpclient:4.3.6
     |    +--- org.apache.httpcomponents:httpcore:4.3.3
     |    +--- commons-logging:commons-logging:1.1.3
     |    \--- commons-codec:commons-codec:1.6 -&gt; 1.10
     +--- commons-codec:commons-codec:1.10
     +--- commons-lang:commons-lang:2.6
     +--- javax.mail:mail:1.4.5
     |    \--- javax.activation:activation:1.1
     +--- javax.inject:javax.inject:1
     +--- com.sun.jersey:jersey-client:1.13
     |    \--- com.sun.jersey:jersey-core:1.13
     \--- com.sun.jersey:jersey-json:1.13
          +--- org.codehaus.jettison:jettison:1.1
          |    \--- stax:stax-api:1.0.1
          +--- com.sun.xml.bind:jaxb-impl:2.2.3-1
          |    \--- javax.xml.bind:jaxb-api:2.2.2
          |         +--- javax.xml.stream:stax-api:1.0-2
          |         \--- javax.activation:activation:1.1
          +--- org.codehaus.jackson:jackson-core-asl:1.9.2
          +--- org.codehaus.jackson:jackson-mapper-asl:1.9.2
          |    \--- org.codehaus.jackson:jackson-core-asl:1.9.2
          +--- org.codehaus.jackson:jackson-jaxrs:1.9.2
          |    +--- org.codehaus.jackson:jackson-core-asl:1.9.2
          |    \--- org.codehaus.jackson:jackson-mapper-asl:1.9.2 (*)
          +--- org.codehaus.jackson:jackson-xc:1.9.2
          |    +--- org.codehaus.jackson:jackson-core-asl:1.9.2
          |    \--- org.codehaus.jackson:jackson-mapper-asl:1.9.2 (*)
          \--- com.sun.jersey:jersey-core:1.13
```

The new dependency is `commons-lang:commons-lang:2.6`.

As we did not include it previously in gradle config file, it's never packaged within the final ZIP file and never tested by the license_checker.

When back porting to 2.x, 2.x is using transitive maven dependencies so it's automatically added to the build and to the final ZIP file. That means also that the license_checker catch this:

```
main:
[license check] Running license check
     [exec] LICENSE DIR: /Users/dpilato/Documents/Elasticsearch/dev/es-2.x/elasticsearch/plugins/cloud-azure/licenses/
     [exec] SOURCE: /Users/dpilato/Documents/Elasticsearch/dev/es-2.x/elasticsearch/plugins/cloud-azure/target/releases/cloud-azure-2.2.0-SNAPSHOT.zip
     [exec] IGNORE: cloud-azure-2.2.0-SNAPSHOT
     [exec] commons-codec-1.10.jar.sha1: SHA is missing
     [exec] commons-io-2.4.jar.sha1: SHA is missing
     [exec] commons-lang-2.6.jar.sha1: SHA is missing
     [exec] Extra SHA files present for: commons-codec-1.6.jar.sha1
     [exec] Extra LICENCE file present: commons-codec
     [exec] Extra NOTICE file present: commons-codec

```

We should try to:
- have a gradle task which checks transitive dependencies vs the ones explicitly added in gradle file
  Having an output like the following would be awesome to copy and paste the result in gradle file :):

```
Missing deps:
  compile 'commons-lang:commons-lang:2.6'
```
- may be `WARN` when we build
</description><key id="120392016">15238</key><summary>missing transitive dependencies are not detected anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>adoptme</label><label>build</label></labels><created>2015-12-04T12:28:09Z</created><updated>2016-11-18T11:20:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-02-14T18:42:24Z" id="183948272">In master we don't use transitive dependencies.
</comment><comment author="dadoonet" created="2016-11-18T11:20:52Z" id="261509240">Reopening the issue as per the discussion we had on FixIt Friday session about #21373.

The goal is to have a gradle task which now detects:
- when a transitive dependency differs from the version one of our dependencies depends on
- when a transitive dependency is missing from the list of dependencies we are manually declaring

This task could be called as part of the `check` task and can fail the build.

Bonus points:
- be able to provide a list of jars/expected version/actual version which we can ignore (and not fail the build). Note that expected version and actual version are important here as we want to ask ourselves anytime we update a dependency in our project.
- provide a list of what has to be changed in the gradle build file
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use azure Token based auth instead of certificates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15237</link><project id="" key="" /><description>Followup for #15232

It sounds like we can rid of Certificate complicated based auth to token based auth.

https://msdn.microsoft.com/en-us/library/azure/dn798668.aspx
</description><key id="120366218">15237</key><summary>Use azure Token based auth instead of certificates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>enhancement</label></labels><created>2015-12-04T09:43:07Z</created><updated>2016-07-26T12:41:09Z</updated><resolved>2016-07-26T12:41:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-26T12:41:09Z" id="235255213">Will be supported by Azure ARM plugin #19146. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disable_allocation was replaced by enable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15236</link><project id="" key="" /><description>The cluster.routing.allocation settings (disable_allocation, disable_new_allocation and disable_replica_location) have been replaced by the single setting
</description><key id="120362953">15236</key><summary>disable_allocation was replaced by enable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">socurites</reporter><labels><label>docs</label></labels><created>2015-12-04T09:21:57Z</created><updated>2015-12-04T09:56:55Z</updated><resolved>2015-12-04T09:54:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-04T09:56:55Z" id="161925529">thanks @socurites - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore chunksize of 512kb on recovery and remove configurability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15235</link><project id="" key="" /><description>This commit restores the chunk size of 512kb lost in a previous but unreleased
refactoring. At the same time it removes the configurability of:
- `indices.recovery.file_chunk_size` - now fixed to 512kb
- `indices.recovery.translog_ops` - removed without replacement
- `indices.recovery.translog_size` - now fixed to 512kb
- `indices.recovery.compress` - file chunks are not compressed due to lucene's compression but translog operations are.

The compress option is gone entirely and compression is used where it makes sense. On sending files of the index
we don't compress as we rely on the lucene compression for stored fields etc.

Relates to #15161
</description><key id="120362368">15235</key><summary>Restore chunksize of 512kb on recovery and remove configurability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T09:18:05Z</created><updated>2015-12-14T17:09:00Z</updated><resolved>2015-12-11T14:27:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-04T11:18:12Z" id="161940804">Production code LGTM. I left some concerns about the testing change.
</comment><comment author="s1monw" created="2015-12-08T11:09:23Z" id="162851948">@bleskes I had to add one more commit - can you take a quick look?
</comment><comment author="bleskes" created="2015-12-08T11:39:06Z" id="162857881">LGTM 
</comment><comment author="s1monw" created="2015-12-11T08:47:51Z" id="163876922">@bleskes I refined this change one more time to allow tests to override the chunksize which I think is a good tool to have (pkg private setter) and I also fixed the stream closing issue. I think it's ready now. Do you wanna take one more look?
</comment><comment author="bleskes" created="2015-12-11T13:38:56Z" id="163937953">LGTM (better than before). left some minor optional suggestions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ancient deprecated and alternative recovery settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15234</link><project id="" key="" /><description>Several settings have been deprecated or are replaced with new settings after refactorings
in version 1.x. This commit removes the support for these settings.

The settings are:
- `index.shard.recovery.translog_size`
- `index.shard.recovery.translog_ops`
- `index.shard.recovery.file_chunk_size`
- `index.shard.recovery.concurrent_streams`
- `index.shard.recovery.concurrent_small_file_streams`
- `indices.recovery.max_size_per_sec`
</description><key id="120357918">15234</key><summary>Remove ancient deprecated and alternative recovery settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T08:49:19Z</created><updated>2015-12-15T11:45:48Z</updated><resolved>2015-12-07T20:15:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-04T09:06:41Z" id="161913547">Please could you add a list of settings that are removed to the breaking changes docs for 3.0?
</comment><comment author="s1monw" created="2015-12-04T09:10:12Z" id="161914791">&gt; Please could you add a list of settings that are removed to the breaking changes docs for 3.0?

yeah - we should also upgrade them in 2.0 I think I will open a followup
</comment><comment author="s1monw" created="2015-12-07T08:54:52Z" id="162451768">@clintongormley pushed a new commit
</comment><comment author="clintongormley" created="2015-12-07T19:23:19Z" id="162631229">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add modules to distributions, and move lang-expression and lang-groovy to them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15233</link><project id="" key="" /><description>With the work to lock down permissions of scripts, we removed expressions and groovy from core, into plugins. However, we would still like the ability to include these languages, and future extractions of features to plugins, in distributions of elasticsearch.

This change adds the concept of a "module" to the elasticsearch repository. These are simply plugins that come included with distributions, but in a way that does not affect plugin management. They cannot be removed, or even show up, with bin/plugin. Also added here is a new distribution, integ-test-zip. This is the minimal distribution that includes no modules, which plugins use for their integ tests. QA tests use the full zip as their distribution.

Note that we will attempt to backport this to 2.x, but it will be done in a separate PR.

closes #15233 
</description><key id="120356575">15233</key><summary>Add modules to distributions, and move lang-expression and lang-groovy to them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T08:37:55Z</created><updated>2015-12-04T19:44:31Z</updated><resolved>2015-12-04T19:44:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-04T09:00:23Z" id="161912245">I gave a first look and it looks really great.

Some thoughts:
- we can have users who already installed groovy plugin in 2.0 or 2.1. We need to either detect that and fail starting elasticsearch in that case, asking them to remove the plugin.
- And/or we need to document it in breaking changes
- we need to adapt the documentation (move from docs/plugins dir to docs/modules?) That could obviously come within another PR. cc @clintongormley 
</comment><comment author="rjernst" created="2015-12-04T09:05:58Z" id="161913303">Thanks for looking @dadoonet.

&gt; we can have users who already installed groovy plugin in 2.0 or 2.1. We need to either detect that and fail starting elasticsearch in that case, asking them to remove the plugin.

This is not true. We did not move groovy to a plugin until after 2.1. There has not been any release of 2.0 or 2.1 that did not include groovy and expressions.
</comment><comment author="dadoonet" created="2015-12-04T09:08:47Z" id="161914499">&gt; @rjernst This is not true. 

You are 100% right!
</comment><comment author="rmuir" created="2015-12-04T09:09:10Z" id="161914620">- If someone installed the groovy plugin from 2.0 or 2.1, we will already fail because it is for the wrong elasticsearch version.
- That is not a breaking change because that behavior exists already, this PR does not change the situation. We don't start with old plugins.
- I don't think we need to document modules. The big picture here is that after discussion with @nik9000 , we realized the most important goal is for packaging to differentiate `modules`, which do not require any user interaction and are packaging's responsibility from `plugins`, which require interaction with the user. This allows us to not cause pain in the upgrade experience: we are using the plugin "format" and mechanism as an implementation detail at the moment: it could change in the future to be something else (such as new features provided by java 9). Plugins are currently problematic (e.g. we don't yet have distribution packages for them and upgrades are less than stellar), so a "transparent" solution would introduce problems: but modules don't need to have these problems: they are system code just like `lib/` and we don't document that directory. If we want to document these directories then i can only recommend `NO TOUCHY`. 
</comment><comment author="rjernst" created="2015-12-04T09:09:35Z" id="161914682">&gt; &gt; we can have users who already installed groovy plugin in 2.0 or 2.1. We need to either detect that and fail starting elasticsearch in that case, asking them to remove the plugin.
&gt; 
&gt; This is not true. We did not move groovy to a plugin until after 2.1. There has not been any release of 2.0 or 2.1 that did not include groovy and expressions.

However, we can and should improve the error messaging. In 3.0 (this change) we won't be publishing zips for modules. But with the backport to 2.x, we have no choice with maven (I think?). So for 2.x, we should look at having probably a static list to throw an error on if the user tries installing the plugin.
</comment><comment author="rmuir" created="2015-12-04T09:10:09Z" id="161914785">There is already a check in PluginManager in this PR for that case.
</comment><comment author="rjernst" created="2015-12-04T09:11:21Z" id="161914971">&gt; &gt; So for 2.x, we should look at having probably a static list to throw an error on if the user tries installing the plugin.
&gt; 
&gt; There is already a check in PluginManager in this PR for that case.

Ah yes I forgot!
</comment><comment author="dadoonet" created="2015-12-04T09:11:38Z" id="161915016">&gt; we need to adapt the documentation (move from docs/plugins dir to docs/modules?)

I was wrong. You can totally ignore my comment apart `that is great!` :D 
</comment><comment author="rmuir" created="2015-12-04T09:25:35Z" id="161918028">I also want to mention the downsides of this approach (versus just preinstalling plugins):
- there isn't yet really supported way to remove modules. we just want to be able to provide more isolation, reliability, security to the system, and remove core third party dependencies. But we don't offer this capability today, and the workaround is to use the `rm` command, still easier than removing groovy scripting today.
- we may have confusion caused by the fact that, `lang-groovy` and `lang-expressions` may exist in maven central as published plugins for the 2.x series. Keep in mind they are never documented as being plugins, so its not the end of the world, and its temporary: I think this is simply unavoidable with the maven backport if we want to be practical about things.
- we have a little more complexity (code changes in plugin code here), than if we just did the straightforward transparent approach of simply preinstalling plugins. But its not too terrible, we only special case enough so that we can report back minimal information in startup logs and nodes api, so things can be debuggable. Otherwise its all under the hood still.

IMO these are minor compared to the advantages we get of better packaging and upgrade behavior.
</comment><comment author="bleskes" created="2015-12-04T10:00:45Z" id="161926650">Thanks @rmuir . I was thinking about those downsides as  well. Another use case which I thing is valid, is something wanting to have no groovy installed at all. Just doesn't need scripting and doesn't want to have any overhead/potential security risk. If understand things correctly, this will be impossible with the current suggestion.

What are the downsides of the "just preinstalled plugins" approach? 
</comment><comment author="rjernst" created="2015-12-04T10:03:23Z" id="161927118">&gt; something wanting to have no groovy installed at all. Just doesn't need scripting and doesn't want to have any overhead/potential security risk. If understand things correctly, this will be impossible with the current suggestion.

It is possible, just not using bin/plugin. The user just deletes the directory from modules. But this is a super advanced use case, so I think having to manually remove a directory (like manually removing an optional dependency in lib that ES includes) is fine.
</comment><comment author="rjernst" created="2015-12-04T10:09:39Z" id="161928317">&gt; What are the downsides of the "just preinstalled plugins" approach? 

I think this was already mentioned in comments above?

&gt; The big picture here is that after discussion with @nik9000 , we realized the most important goal is for packaging to differentiate modules, which do not require any user interaction and are packaging's responsibility from plugins, which require interaction with the user. This allows us to not cause pain in the upgrade experience
</comment><comment author="rmuir" created="2015-12-04T10:10:13Z" id="161928412">&gt; Thanks @rmuir . I was thinking about those downsides as well. Another use case which I thing is valid, is something wanting to have no groovy installed at all. Just doesn't need scripting and doesn't want to have any overhead/potential security risk. If understand things correctly, this will be impossible with the current suggestion.

its not impossible, they have to use `rm`. But the real bug is us shipping such an insecure thing by default IMO. I am intentionally steering clear of this problem on this issue, its not related, we don't offer this capability today, and @jdconrad has been working on the right solution.

&gt; What are the downsides of the "just preinstalled plugins" approach?

As mentioned above, IMO the biggest problem is that upgrades would be rocky with our packaging. That is because plugins are "under the users control". So we want clear separation: `modules` are an implementation detail for us just like `lib` only with more isolation, plugins are configurable by the user.
</comment><comment author="bleskes" created="2015-12-04T10:32:55Z" id="161932506">&gt; But the real bug is us shipping such an insecure thing by default IMO. I am intentionally steering clear of this problem on this issue, its not related, we don't offer this capability today, and @jdconrad has been working on the right solution.

Ok, so I read this to say - once we're ready, we will move groovy to a plugin. If so, I'm fine. Thanks.

&gt; As mentioned above, IMO the biggest problem is that upgrades would be rocky with our packaging. That is because plugins are "under the users control". So we want clear separation: modules are an implementation detail for us just like lib only with more isolation, plugins are configurable by the user.

I see the added value of being to just override directories when upgrading, without worrying too much about what's in them. Thanks for clarifying. 
</comment><comment author="nik9000" created="2015-12-04T14:30:59Z" id="161980663">\o/

Left minor comment buts it looks good to me. I wonder if we should poke anyone else who might have an opinion on this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Azure Service Management API to 0.9.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15232</link><project id="" key="" /><description>Azure team released new versions of their Java SDK.

According to https://github.com/Azure/azure-sdk-for-java/wiki/Azure-SDK-for-Java-Features, it comes with 2 versions.
We should at least update to `0.9.0` of V1 but also consider moving to the new APIs (V2).

This commit first updates to latest API V1.

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
    &lt;artifactId&gt;azure-svc-mgmt-compute&lt;/artifactId&gt;
    &lt;version&gt;0.9.0&lt;/version&gt;
&lt;/dependency&gt;
```

Closes #15209
</description><key id="120350432">15232</key><summary>Update Azure Service Management API to 0.9.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T07:45:16Z</created><updated>2015-12-04T16:46:36Z</updated><resolved>2015-12-04T16:42:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-04T07:46:12Z" id="161900161">@tlrx Really small change. Can you review? 
</comment><comment author="tlrx" created="2015-12-04T08:14:19Z" id="161905528">@dadoonet do you have a link to the changelog and an explanation why the library moves to API V2? I'm curious to know the benefit/tradeoffs of the new version.
</comment><comment author="dadoonet" created="2015-12-04T08:32:17Z" id="161908083">Note that my change does not move to the V2.

And for now, I can't find any documentation on how to adapt from V1 to V2. Some APIs are not there anymore :(
</comment><comment author="tlrx" created="2015-12-04T08:43:50Z" id="161909682">&gt; Note that my change does not move to the V2.

Sure, I'm just asking for a changelog of 0.90.0 V1
</comment><comment author="dadoonet" created="2015-12-04T09:22:11Z" id="161917447">@tlrx 

2015.10.21 Version 0.9.0
- New Azure Resource Manager Storage Management library
- Fix a few bugs

2015.9.22 Version 0.8.3
- Fix unexpected exceptions thrown in some long running operations

2015.9.22 Version 0.8.2
- Revert a previous breaking change on enum casing
- Fix string comparison on long running operation status

2015.9.15 Version 0.8.1
- Use commons library for Base64 encoding

2015.8.21 Version 0.8.0
- Support token based credentials
- Support Azure Resource Management

The great news is that we might be able to change auth from certificate based auth to token based auth...
I would prefer doing that in another PR though.
</comment><comment author="tlrx" created="2015-12-04T09:42:38Z" id="161921430">"fix few bugs" I guess it motivates an update... LGTM
</comment><comment author="dadoonet" created="2015-12-04T09:44:08Z" id="161921799">haha! Yeah. The changelog is super obvious...

I guess it basically says: read this: https://github.com/Azure/azure-sdk-for-java/commits/master
</comment><comment author="dadoonet" created="2015-12-04T11:55:15Z" id="161949982">After trying to backport to 2.x branch, I found that some transitive dependencies are actually missing. I need to update gradle build file.
Will push another commit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed minor typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15231</link><project id="" key="" /><description /><key id="120333265">15231</key><summary>fixed minor typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bt5e</reporter><labels><label>docs</label></labels><created>2015-12-04T04:54:15Z</created><updated>2015-12-04T09:01:37Z</updated><resolved>2015-12-04T09:01:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-04T09:01:37Z" id="161912420">thanks @bt5e - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add .settings for buildSrc on gradle eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15230</link><project id="" key="" /><description>We have eclipse settings added to all projects when running gradle
eclipse, but buildSrc is its own special project that is not
encapsulated by allprojects blocks. This adds eclipse settings to
buildSrc.
</description><key id="120327143">15230</key><summary>Add .settings for buildSrc on gradle eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T03:55:17Z</created><updated>2015-12-04T04:00:22Z</updated><resolved>2015-12-04T03:59:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Clarify what "number" is for weighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15229</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-weight) it'd be nice if clarified exactly what we mean by number; integer, float, pi?
</description><key id="120325380">15229</key><summary>Clarify what "number" is for weighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2015-12-04T03:33:46Z</created><updated>2016-09-27T16:19:43Z</updated><resolved>2016-09-27T16:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T16:19:43Z" id="249916687">Closing this as the documentation now states "The number value is of type float."
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cli tools: Use toString instead of getMessage for exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15228</link><project id="" key="" /><description>When not in debug mode, we currently only print the message of an
exception. However, this is not usually useful without knowing what the
exception type was. This change makes cli tools use toString() on the
exception so we get the type + message.
</description><key id="120311979">15228</key><summary>Cli tools: Use toString instead of getMessage for exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-04T01:28:35Z</created><updated>2016-03-10T18:53:37Z</updated><resolved>2015-12-04T03:01:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-04T02:59:50Z" id="161858349">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15227</link><project id="" key="" /><description>Fixed minor typo in the example of native scripting request.
</description><key id="120311527">15227</key><summary>Update scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">popol1991</reporter><labels><label>docs</label></labels><created>2015-12-04T01:24:35Z</created><updated>2015-12-04T08:58:44Z</updated><resolved>2015-12-04T08:58:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-04T08:58:44Z" id="161911970">thanks @popol1991 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix updateShas to not barf on disabled license checks and even compile correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15226</link><project id="" key="" /><description>These were just results of not testing properly after refactoring.

closes #15224
</description><key id="120295144">15226</key><summary>Fix updateShas to not barf on disabled license checks and even compile correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T23:22:03Z</created><updated>2016-03-10T18:39:08Z</updated><resolved>2015-12-04T07:34:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-04T07:33:04Z" id="161898650">Just tested your branch with my changes and everything is good! 

+1 to merge!

Thanks Ryan.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing rate is ~10X lower in Elasticsearch 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15225</link><project id="" key="" /><description>I am upgrading from Elasticsearch 1.7.1 to 2.1. I have few test cases that I wanted to validate before upgrading. Attached [BulkProcessorTest.txt](https://github.com/elastic/elasticsearch/files/51364/BulkProcessorTest.txt) is one among them.

Here are the steps
1) create object with random values and corresponding JSONs are 3K Bytes. 
2) Index 200 such records (could be any number) to index "test"
3) Drop index "test"
4) Index 200K Records to index "test"

In 1.7.1 step 4 used to take ~40 seconds.
In 2.1.0 step 4 is taking ~600 seconds
I reran the same test on 2.0.1 it is taking ~40 seconds. 

If step 3 is not executed Step 4 takes only ~40 seconds in 2.1.0.

I saw lot of these warnings when the performance was slow: 
high disk watermark [90%] exceeded on [ZkeRJaTUR4iu3h8DmIiV2w][Thing]... free: 11gb[4.7%], shards will be relocated away from this node.
Also, disk and cpu utilization were significantly higher.

These warnings and high utilization  were not seen when step 3 was not executed 2.1.0.
</description><key id="120284629">15225</key><summary>Indexing rate is ~10X lower in Elasticsearch 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">ksundeepsatya</reporter><labels><label>:Core</label><label>bug</label></labels><created>2015-12-03T22:24:39Z</created><updated>2016-07-11T13:51:24Z</updated><resolved>2015-12-04T20:18:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-04T08:54:01Z" id="161911262">This is because your are benchmarking an abusive and unrealistic use of Elasticsearch by randomly generating field names.  This is hitting two changes in particular:

## Mapping changes must be confirmed by the master

Previously it was possible for the same field to be added to an index on different shards with different mapping.  This could result in incorrect results and even data loss down the line.  Now, any mapping update must be confirmed by the master (which then publishes a new cluster state to all nodes and waits for confirmation that it has ben received) before continuing with indexing.

You are adding new random field names in every document, so every document triggers a cluster state update. Not surprising that this is slow!

## Doc values on by default

Aggregations, sorting, and scripting need to be able to retrieve the value of a field for a particular document.  Previously we used in-memory fielddata to do this, but that had two downsides:
- A speed bump when loading fielddata from a big segment
- It filled up your heap and could cause OOMs

Now these values are written to disk at index time as doc values, a format which makes random access very fast.  This format requires a value for every field in every document (it is not sparse).  You are adding random field names each with one value, so your doc values matrix is growing exponentially.  This becomes even worse when small segments are merged into big segments.

(Actually, in Lucene 5.4 there is an optimization for sparse doc values (LUCENE-6863)[https://issues.apache.org/jira/browse/LUCENE-6863] which kicks in for fields that are present in less than 1% of documents).

## Realistic benchmarks

We optimize for the way people actually use our software. Benchmarks are useless unless they test real world use cases.
</comment><comment author="ksundeepsatya" created="2015-12-04T10:57:28Z" id="161937286">@clintongormley I am not generating random field names each time, I am generating random values for the same object. There are 30 different fields and that is constant through out the test. In the attached BulkProcessor.txt you may notice that. I think that is a realistic use case. 

BulkProccessor is not slow in all cases. If I am adding 200K documents, it takes 40 seconds. But, if I add some documents, let's say 1000 and delete that index, and add 200K records to the same index, this time it takes 600 seconds. 

I repeated the same test on 2.0.1 and 2.0. In both the cases it takes only ~40 seconds. 
</comment><comment author="clintongormley" created="2015-12-04T12:14:56Z" id="161953239">&gt;  In the attached BulkProcessor.txt you may notice that.

OK - that wasn't evident in BulkProcessor - you just refer to a RandomValueGenerator but the code isn't there.

@jasontedor has managed to replicate a slow down here.  Reopening
</comment><comment author="jasontedor" created="2015-12-04T12:29:11Z" id="161955356">@ksundeepsatya I've managed to reproduce a slow down here, and we have an understanding what is causing the slowdown. However, I was only observing a 2x slowdown, not a 10x slowdown. Given our understanding of what is causing the slowdown, are you by chance on spinning disks instead of SSD? That could explain the difference.
</comment><comment author="ksundeepsatya" created="2015-12-04T12:36:02Z" id="161956578">@jasontedor I am using SSD. I was able to see a 10X slow down on multiple machines all having SSD. 
</comment><comment author="jasontedor" created="2015-12-04T12:38:46Z" id="161957186">&gt; I am using SSD. I was able to see a 10X slow down on multiple machines all having SSD.

@ksundeepsatya Okay, thanks. I don't have a good explanation for observing a 2x difference locally vs your reported 10x, but either way there is a performance regression here that we will address.
</comment><comment author="ksundeepsatya" created="2015-12-04T12:44:32Z" id="161958950">@jasontedor just curious to know if if this could be reproducible in any other flow other the way I mentioned. 
I am seeing it in 2.1, but not in 2.0.1. If this is a isolated case, I can upgrade to 2.1 instead of 2.0.1. 
</comment><comment author="sarwarbhuiyan" created="2015-12-04T14:44:22Z" id="161983387">&gt; high disk watermark [90%] exceeded on [ZkeRJaTUR4iu3h8DmIiV2w][Thing]... free: 11gb[4.7%], shards will be relocated away from this node.

How many nodes are you running and how much free disk are you having before runs? It looks like your'e running out of disk too.
</comment><comment author="ksundeepsatya" created="2015-12-04T15:06:03Z" id="161988794">@sarwarbhuiyan I am running Embedded Elasticsearch, single node. I have 328G free space on my disk. 
Here is the code snippet
Settings.Builder elasticsearchSettings = Settings.settingsBuilder()
                .put("http.enabled", "false").put("path.data", dataDirectory)
                .put("path.home", dataDirectory).put("script.indexed", "on");

```
    client = nodeBuilder().local(true)
            .settings(elasticsearchSettings.build()).node().client();
```
</comment><comment author="jasontedor" created="2015-12-04T15:08:29Z" id="161989317">&gt; just curious to know if if this could be reproducible in any other flow other the way I mentioned. 

@ksundeepsatya We are still investigating the scenarios under which this can occur. The reason that I asked about SSD vs. spinning disk is because the underlying cause appears to be due to how frequently merges are occurring.
</comment><comment author="jasontedor" created="2015-12-04T15:09:16Z" id="161989480">&gt; How many nodes are you running and how much free disk are you having before runs? It looks like your'e running out of disk too.

@sarwarbhuiyan It's a single embedded node in @ksundeepsatya's test code. The disk space is a red herring and not related to the underlying issue.
</comment><comment author="jasontedor" created="2015-12-04T20:13:20Z" id="162070256">&gt; just curious to know if if this could be reproducible in any other flow other the way I mentioned. 

@ksundeepsatya Coming back to this question, we have a pretty good understanding now of the underlying cause of the performance regression, why that regression happened, and the scenarios under which it occurred.

The performance regression was caused by a substantial increase in the number of merges. The merges were caused by a certain indexing buffer (the version map) not being increased in size after bulk indexing started. The reason that the version map was not resized is because the controller that manages the indexing buffers did not detect the change in state for the underlying shards. Finally, the reason that it did not detect the state change is because of the create -&gt; index -&gt; delete -&gt; create -&gt; index sequence that you went through. The second time that the index was created, the shards were regenerated with the same ShardIds that they previously had. The state from the previous shards had not cleared and so the controller did not resize the buffer.

So, it appears the scenarios under which this regression can occur are rare and special. That said, it led us to rework the controller code so that it is not stateful and we are less likely to run into scenarios like this one.

I've opened #15251 to address this and we will be working to have it released in version 2.1.1.

Thank you for discovering and reporting this issue.
</comment><comment author="javanna" created="2015-12-05T07:43:46Z" id="162158976">Let me clarify that this issue was not a problem in the BulkProcessor (java api code) as the title suggested, but in the indexing code on the server side. I updated the title to reflect that.
</comment><comment author="lwolters" created="2015-12-10T12:13:06Z" id="163593128">Hi javanna / jason.

Experiencing the same over here. Just upgraded our ES server to 2.1.0 and suddenly test execution times are running up 10 times. (Only after deleting the index and reinserting test data)

But luckily you found the issue and hopefully it is fixed. Can I verify the fix by running a snapshot? if so, where can I find the latest snapshot? (sonatype?)

Thanks in advance
</comment><comment author="jasontedor" created="2015-12-10T12:48:31Z" id="163601434">&gt; Experiencing the same over here. Just upgraded our ES server to 2.1.0 and suddenly test execution times are running up 10 times. (Only after deleting the index and reinserting test data)

That sounds like it could be the same issue.

&gt; Can I verify the fix by running a snapshot? if so, where can I find the latest snapshot? (sonatype?)

Yes. Clone the 2.1 branch of the source repository and package it using `mvn package -DskipTests`. This will produce a `tar.gz` that you can deploy.

&gt; Thanks in advance

Thank you! The sooner that you're able to let us know if you're still seeing performance issues after running against a build with the fix, the more likely it is that we'll be able to address in advance of the next release.
</comment><comment author="lwolters" created="2015-12-10T13:27:46Z" id="163614320">Hi Jason,

Verified. When running with the 2.1.1-SNAPSHOT release from sonatype it simply runs as expected. Rerunning the tests (i.e. deleting / recreating indices) show similar performances when inserting data.
</comment><comment author="jasontedor" created="2015-12-10T13:46:52Z" id="163622775">&gt; Verified. When running with the 2.1.1-SNAPSHOT release from sonatype it simply runs as expected. Rerunning the tests (i.e. deleting / recreating indices) show similar performances when inserting data.

Thanks much for verifying!
</comment><comment author="tapit69" created="2016-05-28T14:01:45Z" id="222310111">"Previously it was possible for the same field to be added to an index on different shards with different mapping. This could result in incorrect results and even data loss down the line. Now, any mapping update must be confirmed by the master (which then publishes a new cluster state to all nodes and waits for confirmation that it has ben received) before continuing with indexing."  

You should be able to turn this off when creating many types under an index. And then be able to turn it back on when finish.  For example if I want to create an index with 200  types  all with the same mapping but have different type names. Like aa,ab,ac etc. In 1.7,2 I could just run a script and it would take about 10 Minutes now with 2.3. it takes HOURS. I never could finish I had to go back to 1.7.2
</comment><comment author="bleskes" created="2016-05-29T21:05:31Z" id="222382676">&gt; For example if I want to create an index with 200 types all with the same mapping but have different type names

If all mappings are the same, maybe you could use a field with a value mapped to what you currently store as type. It will make things much simpler and easier to manage, with the one downside of having to add a filter to each search. This downside can also be mitigated by setting aliases with a filter.

Another option is to create the mapping in advance, before you index.

&gt; On 2.3 it takes HOURS..

That's peculiar as mapping updates are batched and processed together. Any idea why? Do you have hot threads of the master?
</comment><comment author="ayco-at-naturalis" created="2016-06-24T13:32:35Z" id="228346259">We have one index with 3 fairly large and deeply nested types. On ES1 it tooks us 38 minutes to import a dataset of 4.4 million records. With ES 2 (2.1, 2.2, 2.3) it now takes us 2 hours and 39 minutes. Both metrics come from imports on my dev machine (1 shard, 0 replicas).

That's almost 5 times as slow.

Below is the mapping for one of the three types:

&lt;details&gt;

```
{
  "properties" : {
    "sourceSystem" : {
      "properties" : {
        "code" : {
          "type" : "string",
          "index" : "not_analyzed"
        },
        "name" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        }
      }
    },
    "sourceSystemId" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "recordURI" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "sourceInstitutionID" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "sourceID" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "owner" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "licenceType" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "licence" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "unitID" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "collectionType" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "title" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "caption" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "description" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "serviceAccessPoints" : {
      "type" : "nested",
      "properties" : {
        "accessUri" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "format" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "variant" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        }
      }
    },
    "type" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "taxonCount" : {
      "type" : "integer"
    },
    "creator" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "copyrightText" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "associatedSpecimenReference" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "associatedTaxonReference" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "specimenTypeStatus" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "multiMediaPublic" : {
      "type" : "boolean"
    },
    "subjectParts" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "subjectOrientations" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "phasesOrStages" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "sexes" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "gatheringEvents" : {
      "type" : "nested",
      "properties" : {
        "projectTitle" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "worldRegion" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "continent" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "country" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "iso3166Code" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "provinceState" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "island" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "locality" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "city" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "sublocality" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "localityText" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            },
            "like" : {
              "type" : "string",
              "analyzer" : "like_analyzer"
            }
          }
        },
        "dateTimeBegin" : {
          "type" : "date"
        },
        "dateTimeEnd" : {
          "type" : "date"
        },
        "method" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "altitude" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "altitudeUnifOfMeasurement" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "depth" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "depthUnitOfMeasurement" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "gatheringPersons" : {
          "type" : "nested",
          "properties" : {
            "agentText" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "fullName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                },
                "like" : {
                  "type" : "string",
                  "analyzer" : "like_analyzer"
                }
              }
            },
            "organization" : {
              "properties" : {
                "agentText" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "name" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                }
              }
            }
          }
        },
        "gatheringOrganizations" : {
          "type" : "nested",
          "properties" : {
            "agentText" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "name" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "siteCoordinates" : {
          "type" : "nested",
          "properties" : {
            "longitudeDecimal" : {
              "type" : "double"
            },
            "latitudeDecimal" : {
              "type" : "double"
            },
            "gridCellSystem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "gridLatitudeDecimal" : {
              "type" : "double"
            },
            "gridLongitudeDecimal" : {
              "type" : "double"
            },
            "gridCellCode" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "gridQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "point" : {
              "type" : "geo_shape"
            }
          }
        },
        "bioStratigraphy" : {
          "type" : "nested",
          "properties" : {
            "youngBioDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngBioName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngFossilZone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngFossilSubZone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngBioCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngStratType" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bioDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bioPreferredFlag" : {
              "type" : "boolean"
            },
            "rangePosition" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldBioName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bioIdentifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldFossilzone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldFossilSubzone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldBioCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldBioStratType" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "chronoStratigraphy" : {
          "type" : "nested",
          "properties" : {
            "youngRegionalSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngRegionalStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngRegionalSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternSystem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternErathem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternEonothem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngChronoName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "chronoPreferredFlag" : {
              "type" : "boolean"
            },
            "oldRegionalSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldRegionalStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldRegionalSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternSystem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternErathem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternEonothem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldChronoName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "chronoIdentifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "lithoStratigraphy" : {
          "type" : "nested",
          "properties" : {
            "qualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "preferredFlag" : {
              "type" : "boolean"
            },
            "member2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "member" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "informalName2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "informalName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "importedName2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "importedName1" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "lithoIdentifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formation2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formationGroup2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formationGroup" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formation" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "certainty2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "certainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bed2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bed" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        }
      }
    },
    "identifications" : {
      "type" : "nested",
      "properties" : {
        "taxonRank" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "scientificName" : {
          "properties" : {
            "fullScientificName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "taxonomicStatus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "genusOrMonomial" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                },
                "like" : {
                  "type" : "string",
                  "analyzer" : "like_analyzer"
                }
              }
            },
            "subgenus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "specificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                },
                "like" : {
                  "type" : "string",
                  "analyzer" : "like_analyzer"
                }
              }
            },
            "infraspecificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "infraspecificMarker" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "nameAddendum" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "authorshipVerbatim" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "author" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "year" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "references" : {
              "type" : "nested",
              "properties" : {
                "titleCitation" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "citationDetail" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "uri" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "author" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "fullName" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        },
                        "like" : {
                          "type" : "string",
                          "analyzer" : "like_analyzer"
                        }
                      }
                    },
                    "organization" : {
                      "properties" : {
                        "agentText" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string",
                              "index" : "analyzed"
                            },
                            "ci" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        },
                        "name" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string",
                              "index" : "analyzed"
                            },
                            "ci" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        }
                      }
                    }
                  }
                },
                "publicationDate" : {
                  "type" : "date"
                }
              }
            },
            "experts" : {
              "type" : "nested",
              "properties" : {
                "agentText" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "fullName" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    },
                    "like" : {
                      "type" : "string",
                      "analyzer" : "like_analyzer"
                    }
                  }
                },
                "organization" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "name" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        },
        "defaultClassification" : {
          "properties" : {
            "kingdom" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "phylum" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "className" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "order" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "superFamily" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "family" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "genus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "subgenus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "specificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "infraspecificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "infraspecificRank" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "systemClassification" : {
          "type" : "nested",
          "properties" : {
            "rank" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "name" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "vernacularNames" : {
          "type" : "nested",
          "properties" : {
            "name" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "language" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "preferred" : {
              "type" : "boolean"
            },
            "references" : {
              "type" : "nested",
              "properties" : {
                "titleCitation" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "citationDetail" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "uri" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "author" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "fullName" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        },
                        "like" : {
                          "type" : "string",
                          "analyzer" : "like_analyzer"
                        }
                      }
                    },
                    "organization" : {
                      "properties" : {
                        "agentText" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string",
                              "index" : "analyzed"
                            },
                            "ci" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        },
                        "name" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string",
                              "index" : "analyzed"
                            },
                            "ci" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        }
                      }
                    }
                  }
                },
                "publicationDate" : {
                  "type" : "date"
                }
              }
            },
            "experts" : {
              "type" : "nested",
              "properties" : {
                "agentText" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "fullName" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string",
                      "index" : "analyzed"
                    },
                    "ci" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    },
                    "like" : {
                      "type" : "string",
                      "analyzer" : "like_analyzer"
                    }
                  }
                },
                "organization" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "name" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string",
                          "index" : "analyzed"
                        },
                        "ci" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        },
        "identificationQualifiers" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string",
              "index" : "analyzed"
            },
            "ci" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "dateIdentified" : {
          "type" : "date"
        },
        "identifiers" : {
          "type" : "nested",
          "properties" : {
            "agentText" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string",
                  "index" : "analyzed"
                },
                "ci" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        }
      }
    },
    "identifyingEpithets" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "theme" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string",
          "index" : "analyzed"
        },
        "ci" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    }
  },
  "dynamic" : "strict"
}
Log file: /home/ayco/projects/nba/v2/import/log/print-mapping.2016_06_24_15_06.log
{
  "properties" : {
    "sourceSystem" : {
      "properties" : {
        "code" : {
          "type" : "string",
          "index" : "not_analyzed"
        },
        "name" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        }
      }
    },
    "sourceSystemId" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "recordURI" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "sourceInstitutionID" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "sourceID" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "owner" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "licenceType" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "licence" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "unitID" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "collectionType" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "title" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "caption" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "description" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "serviceAccessPoints" : {
      "type" : "nested",
      "properties" : {
        "accessUri" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "format" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "variant" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        }
      }
    },
    "type" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "taxonCount" : {
      "type" : "integer"
    },
    "creator" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "copyrightText" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "associatedSpecimenReference" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "associatedTaxonReference" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "specimenTypeStatus" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "multiMediaPublic" : {
      "type" : "boolean"
    },
    "subjectParts" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "subjectOrientations" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "phasesOrStages" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "sexes" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "gatheringEvents" : {
      "type" : "nested",
      "properties" : {
        "projectTitle" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "worldRegion" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "continent" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "country" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "iso3166Code" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "provinceState" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "island" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "locality" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "city" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "sublocality" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "localityText" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            },
            "like" : {
              "type" : "string",
              "analyzer" : "like_analyzer"
            }
          }
        },
        "dateTimeBegin" : {
          "type" : "date"
        },
        "dateTimeEnd" : {
          "type" : "date"
        },
        "method" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "altitude" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "altitudeUnifOfMeasurement" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "depth" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "depthUnitOfMeasurement" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "gatheringPersons" : {
          "type" : "nested",
          "properties" : {
            "agentText" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "fullName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                },
                "like" : {
                  "type" : "string",
                  "analyzer" : "like_analyzer"
                }
              }
            },
            "organization" : {
              "properties" : {
                "agentText" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "name" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                }
              }
            }
          }
        },
        "gatheringOrganizations" : {
          "type" : "nested",
          "properties" : {
            "agentText" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "name" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "siteCoordinates" : {
          "type" : "nested",
          "properties" : {
            "longitudeDecimal" : {
              "type" : "double"
            },
            "latitudeDecimal" : {
              "type" : "double"
            },
            "gridCellSystem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "gridLatitudeDecimal" : {
              "type" : "double"
            },
            "gridLongitudeDecimal" : {
              "type" : "double"
            },
            "gridCellCode" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "gridQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "point" : {
              "type" : "geo_shape"
            }
          }
        },
        "bioStratigraphy" : {
          "type" : "nested",
          "properties" : {
            "youngBioDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngBioName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngFossilZone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngFossilSubZone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngBioCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngStratType" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bioDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bioPreferredFlag" : {
              "type" : "boolean"
            },
            "rangePosition" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldBioName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bioIdentifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldFossilzone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldFossilSubzone" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldBioCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldBioStratType" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "chronoStratigraphy" : {
          "type" : "nested",
          "properties" : {
            "youngRegionalSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngRegionalStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngRegionalSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternSystem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternErathem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngInternEonothem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngChronoName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "youngCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldDatingQualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "chronoPreferredFlag" : {
              "type" : "boolean"
            },
            "oldRegionalSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldRegionalStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldRegionalSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternSystem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternSubstage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternStage" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternSeries" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternErathem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldInternEonothem" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldChronoName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "chronoIdentifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "oldCertainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "lithoStratigraphy" : {
          "type" : "nested",
          "properties" : {
            "qualifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "preferredFlag" : {
              "type" : "boolean"
            },
            "member2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "member" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "informalName2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "informalName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "importedName2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "importedName1" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "lithoIdentifier" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formation2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formationGroup2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formationGroup" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "formation" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "certainty2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "certainty" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bed2" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "bed" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        }
      }
    },
    "identifications" : {
      "type" : "nested",
      "properties" : {
        "taxonRank" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "scientificName" : {
          "properties" : {
            "fullScientificName" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "taxonomicStatus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "genusOrMonomial" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                },
                "like" : {
                  "type" : "string",
                  "analyzer" : "like_analyzer"
                }
              }
            },
            "subgenus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "specificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                },
                "like" : {
                  "type" : "string",
                  "analyzer" : "like_analyzer"
                }
              }
            },
            "infraspecificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "infraspecificMarker" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "nameAddendum" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "authorshipVerbatim" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "author" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "year" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "references" : {
              "type" : "nested",
              "properties" : {
                "titleCitation" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "citationDetail" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "uri" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "author" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "fullName" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        },
                        "like" : {
                          "type" : "string",
                          "analyzer" : "like_analyzer"
                        }
                      }
                    },
                    "organization" : {
                      "properties" : {
                        "agentText" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string"
                            },
                            "ignoreCase" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        },
                        "name" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string"
                            },
                            "ignoreCase" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        }
                      }
                    }
                  }
                },
                "publicationDate" : {
                  "type" : "date"
                }
              }
            },
            "experts" : {
              "type" : "nested",
              "properties" : {
                "agentText" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "fullName" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    },
                    "like" : {
                      "type" : "string",
                      "analyzer" : "like_analyzer"
                    }
                  }
                },
                "organization" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "name" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        },
        "defaultClassification" : {
          "properties" : {
            "kingdom" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "phylum" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "className" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "order" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "superFamily" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "family" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "genus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "subgenus" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "specificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "infraspecificEpithet" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "infraspecificRank" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "systemClassification" : {
          "type" : "nested",
          "properties" : {
            "rank" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "name" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        },
        "vernacularNames" : {
          "type" : "nested",
          "properties" : {
            "name" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "language" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            },
            "preferred" : {
              "type" : "boolean"
            },
            "references" : {
              "type" : "nested",
              "properties" : {
                "titleCitation" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "citationDetail" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "uri" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "author" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "fullName" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        },
                        "like" : {
                          "type" : "string",
                          "analyzer" : "like_analyzer"
                        }
                      }
                    },
                    "organization" : {
                      "properties" : {
                        "agentText" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string"
                            },
                            "ignoreCase" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        },
                        "name" : {
                          "type" : "string",
                          "index" : "not_analyzed",
                          "fields" : {
                            "analyzed" : {
                              "type" : "string"
                            },
                            "ignoreCase" : {
                              "type" : "string",
                              "analyzer" : "case_insensitive_analyzer"
                            }
                          }
                        }
                      }
                    }
                  }
                },
                "publicationDate" : {
                  "type" : "date"
                }
              }
            },
            "experts" : {
              "type" : "nested",
              "properties" : {
                "agentText" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    }
                  }
                },
                "fullName" : {
                  "type" : "string",
                  "index" : "not_analyzed",
                  "fields" : {
                    "analyzed" : {
                      "type" : "string"
                    },
                    "ignoreCase" : {
                      "type" : "string",
                      "analyzer" : "case_insensitive_analyzer"
                    },
                    "like" : {
                      "type" : "string",
                      "analyzer" : "like_analyzer"
                    }
                  }
                },
                "organization" : {
                  "properties" : {
                    "agentText" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    },
                    "name" : {
                      "type" : "string",
                      "index" : "not_analyzed",
                      "fields" : {
                        "analyzed" : {
                          "type" : "string"
                        },
                        "ignoreCase" : {
                          "type" : "string",
                          "analyzer" : "case_insensitive_analyzer"
                        }
                      }
                    }
                  }
                }
              }
            }
          }
        },
        "identificationQualifiers" : {
          "type" : "string",
          "index" : "not_analyzed",
          "fields" : {
            "analyzed" : {
              "type" : "string"
            },
            "ignoreCase" : {
              "type" : "string",
              "analyzer" : "case_insensitive_analyzer"
            }
          }
        },
        "dateIdentified" : {
          "type" : "date"
        },
        "identifiers" : {
          "type" : "nested",
          "properties" : {
            "agentText" : {
              "type" : "string",
              "index" : "not_analyzed",
              "fields" : {
                "analyzed" : {
                  "type" : "string"
                },
                "ignoreCase" : {
                  "type" : "string",
                  "analyzer" : "case_insensitive_analyzer"
                }
              }
            }
          }
        }
      }
    },
    "identifyingEpithets" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    },
    "theme" : {
      "type" : "string",
      "index" : "not_analyzed",
      "fields" : {
        "analyzed" : {
          "type" : "string"
        },
        "ignoreCase" : {
          "type" : "string",
          "analyzer" : "case_insensitive_analyzer"
        }
      }
    }
  },
  "dynamic" : "strict"
}
```

&lt;/details&gt;
</comment><comment author="KarimJedda" created="2016-07-06T09:47:39Z" id="230727419">I was looking for an answer to the same issue, and the snippet you just pasted broke my finger.
</comment><comment author="clintongormley" created="2016-07-06T12:44:00Z" id="230760274">@ayco-at-naturalis i've added `&lt;details&gt;` tags to your incredibly long comment.  I think your issue is probably to do with a change to the default `distance_error_pct` (see https://github.com/elastic/elasticsearch/issues/17907)
</comment><comment author="ayco-at-naturalis" created="2016-07-11T07:15:11Z" id="231658465">@clintongormley  Thanks for your reply and sorry for the amount of text. It does seem though as if it is indeed the (deep) nesting that is causing the trouble - not de geo stuff that you mention. As soon as I stopped providing data (while indexing) voor "nested" objects (which themselves contain "nested" objects), perfomance went back to what it was in ES 1.3.4

Of course, since that data is not optional, I would still need to load that data in a separate round.

Note that it isn't the mapping itself that makes it slow. It's when you actually provide data for the nested structures defined in the mapping.
</comment><comment author="clintongormley" created="2016-07-11T13:51:23Z" id="231740036">&gt; it is indeed the (deep) nesting that is causing the trouble - not de geo stuff that you mention

my mistake, the geo issue only kicks in if you specify a `precision`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can not update SHA1 within a module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15224</link><project id="" key="" /><description>I'm trying to upgrade some libs in `plugins/discovery-azure`. So I need to update the `SHA1` files.

``` sh
cd plugins/discovery-azure
gradle updateSHAs
```

It gives:

```
...
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.8
  OS Info               : Mac OS X 10.11.1 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_60 [Java HotSpot(TM) 64-Bit Server VM 25.60-b23]
:plugins:discovery-azure:updateShas FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':plugins:discovery-azure:updateShas'.
&gt; Could not find property 'SHA_EXTENSION' on task ':plugins:discovery-azure:updateShas'.
```
</description><key id="120272381">15224</key><summary>Can not update SHA1 within a module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label></labels><created>2015-12-03T21:19:44Z</created><updated>2015-12-04T07:34:33Z</updated><resolved>2015-12-04T07:34:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-03T21:27:19Z" id="161789608">Strange. I tried from the root project dir and it failed with:

```
:core:updateShas FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:updateShas'.
&gt; java.io.FileNotFoundException: /Users/dpilato/Documents/Elasticsearch/dev/es-gradle/elasticsearch/core/licenses
```

@rjernst Any idea?
</comment><comment author="rjernst" created="2015-12-03T23:23:15Z" id="161821796">Sorry @dadoonet! I opened a PR to fix these problems here: #15226
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `GET` option for /_forcemerge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15223</link><project id="" key="" /><description>POST should be used to indicate this is not just a retrieval operation.

Resolves #15165
</description><key id="120268202">15223</key><summary>Remove `GET` option for /_forcemerge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Index APIs</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T20:56:40Z</created><updated>2016-03-03T19:12:11Z</updated><resolved>2015-12-04T17:39:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-03T22:21:18Z" id="161803948">LGTM.
</comment><comment author="jasontedor" created="2015-12-09T13:15:05Z" id="163227371">I've been thinking that rather than addressing methods that aren't supported for an endpoint this way, we should instead register a handler that replies with HTTP status code 405. I opened #15335 for discussion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FVH not highlighting with proximity queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15222</link><project id="" key="" /><description>Any updates on this issue below? https://github.com/elastic/elasticsearch/issues/1986

I dont see anyone assigned to it.
</description><key id="120264892">15222</key><summary>FVH not highlighting with proximity queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels /><created>2015-12-03T20:39:49Z</created><updated>2015-12-04T08:33:19Z</updated><resolved>2015-12-04T08:33:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-04T08:33:19Z" id="161908206">Hi @vineet85 

Please keep discussion on the original ticket
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid trace logging allocations in TransportBroadcastByNodeAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15221</link><project id="" key="" /><description>This commit wraps the trace logging statements in
TransportBroadcastByNodeAction in trace enabled checks to avoid
unnecessarily allocating objects.

The most egregious offenders were the two trace logging statements in
BroadcastByNodeTransportRequestHandler#onShardOperation. Aside from the
usual object allocations that occur when invoking ESLogger#trace (the
allocated object array for the varargs Object... parameter), these two
logging statements were invoking ShardRouting#shortSummary generating a
bunch of char arrays and Strings (from the StringBuilder, and so a bunch
of array copies as well). In a scenario where there are a lot of shards
and this method is being invoked frequently (e.g., constantly hitting
the _stats endpoint), these two unprotected trace logging statements
were generating a lot of unnecessary allocations.
</description><key id="120252194">15221</key><summary>Avoid trace logging allocations in TransportBroadcastByNodeAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T19:40:09Z</created><updated>2016-02-07T18:10:29Z</updated><resolved>2015-12-03T22:17:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-03T20:17:58Z" id="161770948">Using jmc, I profiled the difference in allocations with this change. On an otherwise quiet node but pinging indices stats every second over a sixty-second period, this change gave an 18% reduction in allocations (with a little over 50% being reduced char array allocations).

This will help reduce allocations implicitly from Marvel and other managements tools that are periodically hitting endpoints that ultimately land in `TransportBroadcastByNodeAction`.
</comment><comment author="dakrone" created="2015-12-03T20:59:22Z" id="161782870">LGTM
</comment><comment author="bleskes" created="2015-12-04T12:23:44Z" id="161954568">&gt; Using jmc, I profiled the difference in allocations with this change. On an otherwise quiet node but pinging indices stats every second over a sixty-second period, this change gave an 18% reduction in allocations (with a little over 50% being reduced char array allocations).

Just wondering (I'm good with the change) - how much bytes are we talking about? anything that's not super easy to clean for the young gen GC?
</comment><comment author="jasontedor" created="2016-02-07T18:07:50Z" id="181069498">&gt; Just wondering (I'm good with the change) - how much bytes are we talking about? anything that's not super easy to clean for the young gen GC?

Just noticed this, sorry. I don't have the numbers from the profile anymore, but one observation that I made from the profile was a meaningful difference in the number of young generation collections performed. All else equal, higher allocation rates means more pauses (for collectors that pause), and every current OpenJDK young generation collector pauses for minor GCs. Additionally, increased collection pressure means more CPU burn for collections so keeping allocation rates down helps keep load averages and CPU utilization down.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>search problem with query_string and special characters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15220</link><project id="" key="" /><description>Searching with AND operator and special characters with query_string it returns more results than it should.

Having two documents with one field called "searchField", with the next values:
- First document, value: AAA{BBB}CCC
- Second document, value: AAA{BBB}DDD

First case: Returns both documents searching for the value: AAA{BBB}CCC.
Second case: Returns only the first document searching for the value: AAA BBB CCC.

I guess both queries should return only the first document as the search is done with AND operator.

The query used in the first case (which I guess is the wrong one) is the following:
"query":{
    "filtered":{
        "query":{
            "bool":{
                "must":[{
                    "query_string":{
                        "query":"AAA\{BBB\}CCC*",
                        "fields":["searchField"],
                        "analyze_wildcard":true,
                        "default_operator":"and"
                    }
                }]
            }
        }
    }
}

Note: Using Java with JestClient client.
The reason why I use the bool query is that is created dynamically so it could have more parameters.

This "issue" happens with several special characters. Not sure if I could be doing something wrong.

Thank you very much!
</description><key id="120245509">15220</key><summary>search problem with query_string and special characters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">david-tejedor</reporter><labels /><created>2015-12-03T19:06:41Z</created><updated>2015-12-03T19:16:51Z</updated><resolved>2015-12-03T19:16:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-03T19:16:50Z" id="161751911">Mostly because of analysis process.
Read the user guide about it. Really helpful.

Please ask questions on discuss.elastic.co.

Not an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup azure settings usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15219</link><project id="" key="" /><description>This commit simplifies and checks azure discovery settings.

Previously, when a user was entering bad azure configuration in its `elasticsearch.yml`, elasticsearch was complaining about Guice injection errors with cryptic messages.

With this commit, when a user starts elasticsearch with wrong settings, `discovery-azure` plugin will fail and will **block starting elasticsearch** with a more obvious message!

```
[2015-12-03 10:03:55,539][INFO ][org.elasticsearch.node   ] [Sean Cassidy] version[3.0.0-SNAPSHOT], pid[38249], build[Unknown/Unknown]
[2015-12-03 10:03:55,540][INFO ][org.elasticsearch.node   ] [Sean Cassidy] initializing ...
[2015-12-03 10:03:55,551][INFO ][org.elasticsearch.plugins] [Sean Cassidy] loaded [discovery-azure], sites []
[2015-12-03 10:03:55,555][WARN ][org.elasticsearch.plugins] [Sean Cassidy] Plugin: discovery-azure implementing onModule by the type is not of Module type class org.elasticsearch.index.IndexModule
[2015-12-03 10:03:55,662][INFO ][org.elasticsearch.env    ] [Sean Cassidy] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [20.7gb], net total_space [464.7gb], spins? [unknown], types [hfs]
[2015-12-03 10:03:55,981][ERROR][org.elasticsearch.plugin.discovery.azure] one or more azure discovery settings are missing. Check elasticsearch.yml file. Should have [cloud.azure.management.subscription.id], [cloud.azure.management.cloud.service.name], [cloud.azure.management.keystore.path] and [cloud.azure.management.keystore.password].
Exception in thread "main" ElasticsearchException[one or more azure discovery settings are missing. Check elasticsearch.yml file. Should have [cloud.azure.management.subscription.id], [cloud.azure.management.cloud.service.name], [cloud.azure.management.keystore.path] and [cloud.azure.management.keystore.password].]
    at org.elasticsearch.plugin.discovery.azure.AzureDiscoveryPlugin.nodeModules(AzureDiscoveryPlugin.java:78)
    at org.elasticsearch.plugins.PluginsService.nodeModules(PluginsService.java:233)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:172)
    ...
```

Also, because we have split `cloud-azure` plugin, we know that this `discovery-azure` plugin is used **only** for discovery purpose on Azure platform. So we can detect easily that a user is embedding a plugin which is not used. And we can now warn him:

```
[2015-12-03 10:13:22,385][INFO ][org.elasticsearch.node   ] [Gregor Shapanka] version[3.0.0-SNAPSHOT], pid[38632], build[Unknown/Unknown]
[2015-12-03 10:13:22,386][INFO ][org.elasticsearch.node   ] [Gregor Shapanka] initializing ...
[2015-12-03 10:13:22,395][INFO ][org.elasticsearch.plugin.discovery.azure] You added discovery-azure plugin but not using it. Check your config or remove it.
[2015-12-03 10:13:22,413][INFO ][org.elasticsearch.plugins] [Gregor Shapanka] loaded [discovery-azure], sites []
```

Sadly, azure Java SDK is swallowing some exceptions and just log with its own logger when something wrong is happening.

So we can't really detect efficiently when the Azure client is created that something is wrong.
We only detect when we start using the client!

This commit immediately checks after the client is created that the client is running fine.
This allows us to also check that at least one service exists for our account and that the service we are supposed to join exists in Azure and fail to start Elasticsearch if it's not the case (see below).

``` java
HostedServiceListResponse list = computeManagementClient.getHostedServicesOperations().list();
if (list.getHostedServices().isEmpty()) {
    throw new ElasticsearchException("no hosted service seems to be available for your azure account.");
}
boolean serviceFound = false;
for (HostedServiceListResponse.HostedService hostedService : list) {
    if (hostedService.getServiceName().equals(serviceName)) {
        serviceFound = true;
    }
}
```

If [Azure issue 565](https://github.com/Azure/azure-sdk-for-java/issues/565) get fixed, we will be able to fail with better messages such as:
- pkstore file does not exist or is not readable
- password for pkstore is incorrect
- pkstore format is incorrect
- ...

If the service name is unknown, elasticsearch will now fail to start with a clear explanation:

```
Caused by: ElasticsearchException[[dadoonet] does not exist for your azure account. To list available services, run in debug mode or check your azure console at https://portal.azure.com/.]
```

The user can run in `debug` mode and get the list of known services:

```
[2015-12-03 14:26:32,435][TRACE][org.elasticsearch.cloud.azure.management] [Seamus Mellencamp] creating new Azure client for [account id here], [dadoonet]
[2015-12-03 14:26:33,292][DEBUG][org.elasticsearch.cloud.azure.management] [Seamus Mellencamp] checking azure client...
[2015-12-03 14:26:34,878][DEBUG][org.elasticsearch.cloud.azure.management] [Seamus Mellencamp] Known hosted services for your azure account:
[2015-12-03 14:26:34,879][DEBUG][org.elasticsearch.cloud.azure.management] [Seamus Mellencamp] - dadoonet-node1
[2015-12-03 14:26:34,879][DEBUG][org.elasticsearch.cloud.azure.management] [Seamus Mellencamp] - dadoonet-tests
```

Closes https://github.com/elastic/elasticsearch-cloud-azure/issues/86.
</description><key id="120216755">15219</key><summary>Cleanup azure settings usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>:Settings</label></labels><created>2015-12-03T16:39:26Z</created><updated>2016-07-11T21:08:39Z</updated><resolved>2016-07-11T21:08:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-03T16:41:16Z" id="161709992">@imotov Could you please review this one please? :D 
</comment><comment author="imotov" created="2015-12-10T23:42:34Z" id="163787867">I understand that this is a hack to work around guice and azure issues, but I am still concerned about removal of cleanup code. So, I am wondering if we can make it a bit less hacky by moving code from `doStart()` to some function that will be called from `configure()`, this function will do all initialization and then return `computeManagementClient` that you can store in a singleton and inject to  `AzureComputeServiceImpl`. This way `AzureComputeServiceImpl` can keep it lifecycle logic and do necessary cleanup in `close()`. What do you think?
</comment><comment author="dadoonet" created="2015-12-11T22:33:49Z" id="164067414">I'll give a try to your suggestion. Thanks for the comments so far. Reassigning the PR to me now.
I'll ping you when updated.
</comment><comment author="dadoonet" created="2015-12-18T13:37:37Z" id="165780366">@imotov I rebased on master and added a new commit.

I still don't see from which class I should call `close()` method. Any idea?
</comment><comment author="imotov" created="2015-12-22T17:09:28Z" id="166676459">&gt; I still don't see from which class I should call close() method. Any idea?

@dadoonet I tried to explain in my previous comment but I probably wasn't very clear. Could you ping me when you have time and we can talk about it? It might be easier to explain in person.
</comment><comment author="clintongormley" created="2016-03-10T13:35:50Z" id="194840444">@dadoonet are you coming back to this one?
</comment><comment author="dadoonet" created="2016-03-10T13:48:45Z" id="194847300">I tried recently to just merge changes from master in my branch but that was not as easy as I'd expect.
I definitely want to do it so please let's keep it opened and assigned to me.
</comment><comment author="dadoonet" created="2016-07-11T21:08:39Z" id="231865672">As we deprecated azure-classic in #19186, I think this PR is pretty much useless now.
I'm closing it.

Thanks @imotov for the review though!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Configuring HTTP module in ec2 to be talked to from browser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15218</link><project id="" key="" /><description>This seems like an incredibly basic question, but I am a bit stuck so hopefully someone can quickly unblock me. I have an ec2 linux machine in AWS that is running ElasticSearch. It is running on port 9200, and if I hit `localhost:9200` with curl I get the expected response. However, if I am outside the linux box, and try to hit the ec2 address in a browser it comes back with nothing. I have the security group on the box wide open right now so that I can hit it from the outside.

I suspect that I have something configured wrong in terms of the http module. The settings I have currently (and yes they will need to be filtered down once I get this scenario working) are as follows:

```
http.enable: true
http.cors.enable: true
http.cors.allow-origin: "*"
```

Clearly I believe I am missing something basic as it is not clear to me why this fails to work.
</description><key id="120202370">15218</key><summary>Configuring HTTP module in ec2 to be talked to from browser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kylegalbraith</reporter><labels /><created>2015-12-03T15:51:26Z</created><updated>2015-12-03T16:44:26Z</updated><resolved>2015-12-03T16:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="davidcai19840412" created="2015-12-03T16:29:01Z" id="161705920">elastic search stop defaultly binding all ip address from v2.0.
Edit the elasticsearch config file,and change the value of network.publish_host and network.bind_host as follows:
network.publish_host: xxx.xxx.xxx.xxx #publish ip
network.bind_host: 0.0.0.0 #bind all ips
</comment><comment author="kylegalbraith" created="2015-12-03T16:35:56Z" id="161708524">@davidcai19840412 thank you for the quick reply. Is `network.publish_host` for inter node communication? What should this be set to? It appears that bind_host is for outward facing communication correct?
</comment><comment author="dadoonet" created="2015-12-03T16:44:25Z" id="161710789">Please ask your questions on discuss.elastic.co. More users might help you there.
Thanks.

This space is for confirmed issues.

Does not sound like an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify MetaDataMappingService.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15217</link><project id="" key="" /><description>Now that we create all types on indices that receive a mapping update, creation
of temporary indices could get simpler.
</description><key id="120194126">15217</key><summary>Simplify MetaDataMappingService.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T15:17:54Z</created><updated>2015-12-03T19:21:53Z</updated><resolved>2015-12-03T15:26:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-03T15:23:44Z" id="161673848">LGTM. Thank you. :)
</comment><comment author="jasontedor" created="2015-12-03T19:21:53Z" id="161753208">I [cherry-picked this](https://github.com/jasontedor/elasticsearch/commit/9011695d063563abb65293cd7700b68877145ac2) into #15159 for eventual integration into 2.x when that pull request is merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix copy_to when the target is a dynamic object field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15216</link><project id="" key="" /><description>Tentative fix for #111237.
Fixes #11237
</description><key id="120185466">15216</key><summary>Fix copy_to when the target is a dynamic object field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T14:39:16Z</created><updated>2015-12-11T13:13:14Z</updated><resolved>2015-12-07T13:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-03T19:40:30Z" id="161757924">I left some comments but this looks like a good start!
</comment><comment author="jpountz" created="2015-12-04T13:08:09Z" id="161963117">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Removed pipeline tests with a simpler tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15215</link><project id="" key="" /><description>The PipelineTests tried to test if the configured map/list in set processor wasn't modified while documents were ingested. Creating a pipeline programmatically created more noise than the test needed. The new tests in IngestDocumentTests have the same goal, but is much smaller and clearer by directly testing against IngestDocument.
</description><key id="120181506">15215</key><summary>[Ingest] Removed pipeline tests with a simpler tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-03T14:20:02Z</created><updated>2015-12-03T14:44:36Z</updated><resolved>2015-12-03T14:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-03T14:38:50Z" id="161658464">LGTM thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryBuilders.QueryString Not Considering Boost If Only One Field is provided.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15214</link><project id="" key="" /><description>Hi,

We are using ElasticSearch 1.6.0.
QueryBuilders.QueryString Not Considering Boost If only One Field is provided. But if i repeat the same field with same boost multiple times it considers the boost value.

QueryString With Single Field : 

```
            {
              "query_string": {
                "query": "samsung mobiles",
                "fields": [
                  "title^100.0"
                ],
                "auto_generate_phrase_queries": true,
                "minimum_should_match": "100%"
              }
```

Explain : true

{"value": 0.013228737,"description": "sum of:","details": [{"value": 0.008120831,"description": 

"weight(title:samsung in 74534) 

[PerFieldSimilarity], result of:","details": [{"value": 0.008120831,"description": "score(doc=74534,freq=1.0), product of:","details": [{"value": 0.0027710546,"description": "queryWeight, product of:","details": [{"value": 4.6889477,"description": "idf(docFreq=7708, maxDocs=308381)"},{"value": 0.0005909758,"description": "queryNorm"}]},{"value": 2.9305923,"description": "fieldWeight in 74534, product of:","details": [{"value": 1,"description": "tf(freq=1.0), with freq of:","details": [{"value": 1,"description": "termFreq=1.0"}]},{"value": 4.6889477,"description": "idf(docFreq=7708, maxDocs=308381)"},{"value": 0.625,"description": "fieldNorm(doc=74534)"}]}]}]},{"value": 0.005107906,"description": "weight(title:mobil in 74534) [PerFieldSimilarity], result of:","details": [{"value": 0.005107906,"description": "score(doc=74534,freq=1.0), product of:","details": [{"value": 0.0021976894,"description": "queryWeight, product of:","details": [{"value": 3.7187467,"description": "idf(docFreq=20339, maxDocs=308381)"},{"value": 0.0005909758,"description": "queryNorm"}]},{"value": 2.3242166,"description": "fieldWeight in 74534, product of:","details": [{"value": 1,"description": "tf(freq=1.0), with freq of:","details": [{"value": 1,"description": "termFreq=1.0"}]},{"value": 3.7187467,"description": "idf(docFreq=20339, maxDocs=308381)"},{"value": 0.625,"description": "fieldNorm(doc=74534)"}]}]}]}]}

QueryString With Same Field Twice : 

```
            {
              "query_string": {
                "query": "samsung mobiles",
                "fields": [
                  "title^100.0",
                  "title^100.0"
                ],
                "auto_generate_phrase_queries": true,
                "minimum_should_match": "100%"
              }
```

Explain : true

{"value": 1.2471766,"description": "sum of:","details": [{"value": 0.7656144,"description": "max of:","details": [{"value": 0.7656144,"description": 

"weight(title:samsung^100.0 in 74534) [PerFieldSimilarity],

 result of:","details": [{"value": 0.7656144,"description": "score(doc=74534,freq=1.0), product of:","details": [{"value": 0.26124904,"description": "queryWeight, product of:","details": [{"value": 100,"description": "boost"},{"value": 4.6889477,"description": "idf(docFreq=7708, maxDocs=308381)"},{"value": 0.0005571592,"description": "queryNorm"}]},{"value": 2.9305923,"description": "fieldWeight in 74534, product of:","details": [{"value": 1,"description": "tf(freq=1.0), with freq of:","details": [{"value": 1,"description": "termFreq=1.0"}]},{"value": 4.6889477,"description": "idf(docFreq=7708, maxDocs=308381)"},{"value": 0.625,"description": "fieldNorm(doc=74534)"}]}]}]},{"value": 0.7656144,"description": "weight(title:samsung^100.0 in 74534) [PerFieldSimilarity], result of:","details": [{"value": 0.7656144,"description": "score(doc=74534,freq=1.0), product of:","details": [{"value": 0.26124904,"description": "queryWeight, product of:","details": [{"value": 100,"description": "boost"},{"value": 4.6889477,"description": "idf(docFreq=7708, maxDocs=308381)"},{"value": 0.0005571592,"description": "queryNorm"}]},{"value": 2.9305923,"description": "fieldWeight in 74534, product of:","details": [{"value": 1,"description": "tf(freq=1.0), with freq of:","details": [{"value": 1,"description": "termFreq=1.0"}]},{"value": 4.6889477,"description": "idf(docFreq=7708, maxDocs=308381)"},{"value": 0.625,"description": "fieldNorm(doc=74534)"}]}]}]}]},{"value": 0.48156232,"description": "max of:","details": [{"value": 0.48156232,"description": "weight(title:mobil^100.0 in 74534) [PerFieldSimilarity], result of:","details": [{"value": 0.48156232,"description": "score(doc=74534,freq=1.0), product of:","details": [{"value": 0.20719339,"description": "queryWeight, product of:","details": [{"value": 100,"description": "boost"},{"value": 3.7187467,"description": "idf(docFreq=20339, maxDocs=308381)"},{"value": 0.0005571592,"description": "queryNorm"}]},{"value": 2.3242166,"description": "fieldWeight in 74534, product of:","details": [{"value": 1,"description": "tf(freq=1.0), with freq of:","details": [{"value": 1,"description": "termFreq=1.0"}]},{"value": 3.7187467,"description": "idf(docFreq=20339, maxDocs=308381)"},{"value": 0.625,"description": "fieldNorm(doc=74534)"}]}]}]},{"value": 0.48156232,"description": "weight(title:mobil^100.0 in 74534) [PerFieldSimilarity], result of:","details": [{"value": 0.48156232,"description": "score(doc=74534,freq=1.0), product of:","details": [{"value": 0.20719339,"description": "queryWeight, product of:","details": [{"value": 100,"description": "boost"},{"value": 3.7187467,"description": "idf(docFreq=20339, maxDocs=308381)"},{"value": 0.0005571592,"description": "queryNorm"}]},{"value": 2.3242166,"description": "fieldWeight in 74534, product of:","details": [{"value": 1,"description": "tf(freq=1.0), with freq of:","details": [{"value": 1,"description": "termFreq=1.0"}]},{"value": 3.7187467,"description": "idf(docFreq=20339, maxDocs=308381)"},{"value": 0.625,"description": "fieldNorm(doc=74534)"}]}]}]}]}]}
</description><key id="120153852">15214</key><summary>QueryBuilders.QueryString Not Considering Boost If Only One Field is provided.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vishnuchilamakuru</reporter><labels /><created>2015-12-03T11:54:43Z</created><updated>2015-12-03T17:51:30Z</updated><resolved>2015-12-03T17:51:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T17:51:30Z" id="161730194">In the one field case, the boost is normalized away as it only makes sense to boost one field in comparison with another.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>throw exception if a copy_to is within a multi field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15213</link><project id="" key="" /><description>Copy to within multi field is ignored from 2.0 on, see #10802.
Instead of just ignoring it, we should throw an exception if this
is found in the mapping when a mapping is added. For already
existing indices we should at least log a warning.

related to #14946 

I made it so that a warning is logged each time a mapping is parsed that has a copy_to in a multi field. 
But the copy_to is not removed from the mapping so now the exception is logged each time the mapping is parsed. Is that OK?
</description><key id="120151557">15213</key><summary>throw exception if a copy_to is within a multi field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v2.0.1</label><label>v2.1.1</label><label>v2.2.0</label></labels><created>2015-12-03T11:39:12Z</created><updated>2015-12-08T09:24:32Z</updated><resolved>2015-12-08T09:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-12-03T11:49:10Z" id="161608247">Just found that versions are wrong. Will fix.
</comment><comment author="clintongormley" created="2015-12-03T11:56:18Z" id="161610176">Is it possible to remove the `copy_to` as part of the mapping upgrade?
</comment><comment author="brwe" created="2015-12-03T17:51:09Z" id="161730129">Discussed with @clintongormley and we decided to just leave it as is as the warning will not flood logs code with log messages.

Ready for review.
</comment><comment author="rjernst" created="2015-12-03T23:44:05Z" id="161826143">Thanks @brwe! I left a few comments.
</comment><comment author="rjernst" created="2015-12-07T17:07:56Z" id="162595140">Since the user can explicitly remove the troublesome `copy_to` when they see the warnings, I am fine with keeping the code as is, for 2.x. For master I think we should still remove this parsing logic.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ignore RejectedExecutionException in NodesFaultDetection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15212</link><project id="" key="" /><description>Test failure: http://build-us-00.elastic.co/job/es_core_master_strong/5680/

Interesting log lines:

```
WARNING: Uncaught exception in thread: Thread[elasticsearch[node_t0][generic][T#4],5,TGRP-RareClusterStateIT]
EsRejectedExecutionException[rejected execution of org.elasticsearch.discovery.zen.fd.NodesFaultDetection$1@406bad99 on EsThreadPoolExecutor[generic, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@51f591fd[Shutting down, pool size = 3, active threads = 3, queued tasks = 0, completed tasks = 42]]]
    at __randomizedtesting.SeedInfo.seed([A9A984299976420]:0)
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
    at org.elasticsearch.discovery.zen.fd.NodesFaultDetection.notifyNodeFailure(NodesFaultDetection.java:148)
    at org.elasticsearch.discovery.zen.fd.NodesFaultDetection.handleTransportDisconnect(NodesFaultDetection.java:143)
    at org.elasticsearch.discovery.zen.fd.NodesFaultDetection$NodeFD$1.handleException(NodesFaultDetection.java:213)
    at org.elasticsearch.transport.TransportService$3.run(TransportService.java:327)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

dets 02, 2015 3:53:09 PM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException
```

Reason: Race between threadpools shutting down and NodesFaultDetection still receiving node disconnect event. It's not an issue for production. Test infrastructure just complains that exception was not caught.

How to reproduce error: Add Thread.sleep(1000) as first statement to `NodesFaultDetection.notifyNodeFailure()`.
</description><key id="120145049">15212</key><summary>Ignore RejectedExecutionException in NodesFaultDetection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>jenkins</label><label>test</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T10:58:33Z</created><updated>2015-12-04T15:36:08Z</updated><resolved>2015-12-04T15:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-03T11:59:15Z" id="161611290">LGTM. Left one minor comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15211</link><project id="" key="" /><description /><key id="120134777">15211</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfr3000</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-12-03T10:08:39Z</created><updated>2015-12-03T21:12:01Z</updated><resolved>2015-12-03T21:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T16:12:21Z" id="161694143">Hi @jfr3000 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jfr3000" created="2015-12-03T21:11:50Z" id="161786079">sorry, that's a bit much legalese for an apostrophe ;) perhaps you can simply fix it...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup integ test classes in buildSrc to be less groovyish</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15210</link><project id="" key="" /><description>Also document them.
</description><key id="120131168">15210</key><summary>Cleanup integ test classes in buildSrc to be less groovyish</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T09:54:19Z</created><updated>2015-12-04T19:24:31Z</updated><resolved>2015-12-04T19:24:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-03T14:29:10Z" id="161655729">I like it for the most part. Left some comments. I think its getting easier to understand!
</comment><comment author="rmuir" created="2015-12-03T14:52:55Z" id="161662421">I agree!
</comment><comment author="nik9000" created="2015-12-04T13:34:56Z" id="161969200">FWIW I think if we're going to Java-ify groovy we should just write these as Java files. It'll be much more consistent. But I'll go along with it if it make you too happy. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Azure Service Management API to 0.9.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15209</link><project id="" key="" /><description>Azure team released new versions of their Java SDK.

According to https://github.com/Azure/azure-sdk-for-java/wiki/Azure-SDK-for-Java-Features, it comes with 2 versions.
We should at least update to `0.9.0` of V1 but also consider moving to the new APIs (V2).
## API V1

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
    &lt;artifactId&gt;azure-svc-mgmt-compute&lt;/artifactId&gt;
    &lt;version&gt;0.9.0&lt;/version&gt;
&lt;/dependency&gt;
```
## API V2

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;com.microsoft.azure&lt;/groupId&gt;
    &lt;artifactId&gt;azure-mgmt-compute&lt;/artifactId&gt;
    &lt;version&gt;0.9.0&lt;/version&gt;
&lt;/dependency&gt;
```
</description><key id="120127661">15209</key><summary>Update Azure Service Management API to 0.9.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>upgrade</label></labels><created>2015-12-03T09:33:11Z</created><updated>2015-12-04T16:42:07Z</updated><resolved>2015-12-04T16:42:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Plugin: List commercial plugins that can be installed by referring to&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15208</link><project id="" key="" /><description>&#8230; their name as well.

This is listed when running `bin/plugin install -h`...
</description><key id="120123009">15208</key><summary>Plugin: List commercial plugins that can be installed by referring to&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>docs</label></labels><created>2015-12-03T09:08:39Z</created><updated>2016-02-21T22:08:30Z</updated><resolved>2016-02-21T22:08:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-03T09:50:05Z" id="161570849">I like the change. I just think that we should list our commercial plugins under a commercial plugins list.

```
OFFICIAL PLUGINS

    The following plugins are officially supported and can be installed by just referring to their name

    - analysis-icu
    - analysis-kuromoji
    - analysis-phonetic
    - analysis-smartcn
    - analysis-stempel
    - delete-by-query
    - discovery-azure
    - discovery-ec2
    - discovery-gce
    - discovery-multicast
    - lang-expression
    - lang-groovy
    - lang-javascript
    - lang-python
    - mapper-attachments
    - mapper-murmur3
    - mapper-size
    - repository-azure
    - repository-s3
    - store-smb

COMMERCIAL PLUGINS

    The following plugins are officially supported and can be installed by just referring to their name

    - license
    - marvel-agent
    - shield
    - watcher
```
</comment><comment author="spinscale" created="2016-02-21T22:08:30Z" id="186928683">closing, deprecated
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>forbidden third-party-signatures -&gt; core-signatures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15207</link><project id="" key="" /><description>This is a relic from shading where it was trickier to implement.
Third party signatures are already in e.g. the test list, there
is no reason to separate them out.

Instead, we could have a third party signatures that does
something different... like keep tabs on third party libraries.
</description><key id="120106120">15207</key><summary>forbidden third-party-signatures -&gt; core-signatures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T06:49:09Z</created><updated>2015-12-03T16:05:19Z</updated><resolved>2015-12-03T07:27:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-03T06:55:44Z" id="161535278">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>empty '_source' with exist = true for documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15206</link><project id="" key="" /><description>We deployed ES 2.0 on 3 nodes as a cluster, for this index we have 5 shards and 2 replica.

However for some of the documents, when we execute 'get' or 'search' request through http:9200 or transport client:9300, it only return _source field with result one out of 3 times, the other two simple has this:

``` json
{
   "_index": "business-1",
   "_type": "business",
   "_id": "5605e65dd7d77897cbaa4a29",
   "_version": 2,
   "_routing": "business",
   "found": true
}
```

What could possibly cause this? Really lost, thanks.
</description><key id="120090837">15206</key><summary>empty '_source' with exist = true for documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">naivefun</reporter><labels><label>feedback_needed</label></labels><created>2015-12-03T04:11:21Z</created><updated>2016-02-14T18:41:25Z</updated><resolved>2016-02-14T18:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T14:29:05Z" id="161655707">@naivefun I have no idea what your question is about.  Please provide a full recreation of what you are doing, what you expect, and what you're getting instead?

My best guess would be that you're using custom routing, and you're not providing the same routing when you try to GET the document.
</comment><comment author="clintongormley" created="2016-02-14T18:41:25Z" id="183947922">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update repository-s3.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15205</link><project id="" key="" /><description>Documentation of AWS VPC public vs. private subnets and their affects on accessing S3.
</description><key id="120064707">15205</key><summary>Update repository-s3.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">imacube</reporter><labels><label>:Plugin Repository S3</label><label>docs</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-03T00:09:45Z</created><updated>2015-12-04T18:09:46Z</updated><resolved>2015-12-04T07:59:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-03T07:58:34Z" id="161544871">Thanks a lot. This looks great. Could you please sign the CLA and fix the comments I left so I can merge it?
</comment><comment author="imacube" created="2015-12-03T18:33:56Z" id="161741185">I've signed the CLA, not sure how long it takes to propagate into the system.
</comment><comment author="dadoonet" created="2015-12-04T08:06:19Z" id="161904442">Thanks @imacube. Pushed in master, 2.x and 2.1 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve / Fix Inaccurate Transport Sniffing Docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15204</link><project id="" key="" /><description>The current transport sniffing docs imply that sniffing only adds nodes to the current node list. After discussing this with @ywelsch and @polyfractal it appears that is incorrect, and that the node list is fully replaced with data nodes. This new documentation reflects that.

I have yet to functionally verify that this works as described, but the [code](https://github.com/elastic/elasticsearch/blob/2.1/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L483) @ywelsch linked me to seems to clearly replace the values.
</description><key id="120064667">15204</key><summary>Improve / Fix Inaccurate Transport Sniffing Docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">andrewvc</reporter><labels><label>docs</label></labels><created>2015-12-03T00:09:24Z</created><updated>2016-03-10T13:22:05Z</updated><resolved>2016-03-10T13:22:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrewvc" created="2015-12-03T22:59:35Z" id="161816360">@javanna I've incorporated your feedback and clarified these docs. How's it read to you now?
</comment><comment author="andrewvc" created="2015-12-08T17:00:56Z" id="162945558">@javanna do you have any more feedback here? Wondering if its good to merge 
</comment><comment author="javanna" created="2015-12-09T20:46:36Z" id="163387174">sorry @andrewvc I had missed your pings. I left two small comments, good to merge once you fixed those. Thanks!
</comment><comment author="andrewvc" created="2015-12-15T17:30:40Z" id="164835227">@javanna thanks for the review! I fixed the typo, but I was confused by your other comment. Let me know if you can elaborate!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an internal refresh api that makes pipeline changes visible on all ingest nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15203</link><project id="" key="" /><description>instead of only relying on async background task that periodically loads pipelines into memory. This async loading mechanism remains for loading pipelines during startup.
- Pipeline store can now only be used when there is no .ingest index or all primary shards of .ingest have been started
- IngestPlugin adds`node.ingest` setting to `true`. This is used to figure out to what nodes to send the refresh request too. This setting isn't yet configurable. This will be done in a follow up issue.
- Async background loading mechanism only runs now every 30 minutes instead of every second.
</description><key id="120056649">15203</key><summary>Add an internal refresh api that makes pipeline changes visible on all ingest nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-02T23:08:25Z</created><updated>2015-12-18T17:25:12Z</updated><resolved>2015-12-18T17:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-03T11:57:09Z" id="161610303">I've updated this PR to move some initialisation logic out of the store into a new class called PipelineStoreBootstrapper. The pipeline store became a bit convoluted with initialisation logic.  
</comment><comment author="talevy" created="2015-12-03T21:26:21Z" id="161789364">would it be beneficial to have refresh action/request tests in `IngestClientIT`?
</comment><comment author="martijnvg" created="2015-12-03T21:40:29Z" id="161792774">I think it makes sense to add a test for this api, even though this is an internal api that is designed to be used only via put and delete pipeline apis. The reason that I did not add an integration test for this yet, is that it is already tests by the put and delete pipeline rest and ClientIT tests.
</comment><comment author="talevy" created="2015-12-03T21:42:49Z" id="161793350">~~but these are just one-node tests, no? since this gets info from cluster-state about multiple nodes (some of which may not be ingest nodes), maybe it makes sense to test those cases as well~~

edit: never-mind, that is handled from within the pipelinestore
</comment><comment author="martijnvg" created="2015-12-07T09:39:55Z" id="162464603">@javanna @talevy I've updated the PR. Changed:
- Renamed refresh action to reload pipelines action.
- Made the reload pipelines action an internal api (Java clients cannot invoke it).
- Removed the `refreshed` field from the response. Whether the reload was successful is ignored and only logged. We should remain to rely on the background pipeline reloading mechanism.
- Set the reload pipeline background task interval from 30 minutes to 30 seconds.
</comment><comment author="martijnvg" created="2015-12-07T11:35:17Z" id="162496967">@s1monw I've updated this PR. Only the `PipelineStoreBootstrapper` is now managed by Guice. I'm still using a Guice `Provider` for the `Client` dependency. There is a cyclic dependency, the bootstrapper class is injected in the transport action classes, but Client is also injected into the bootstrapper class. I'm not sure how to get around this. 

This is a general problem. If a component is used in a transport action in a plugin and that component either directly or indirectly depends on Client then there is a cyclic dependency. They way we have been dealing with this by injecting client lazily (either by using a Guice `Provider`or directly using Guice's injector).
</comment><comment author="s1monw" created="2015-12-07T11:50:25Z" id="162500858">I will look and try to help @martijnvg 
</comment><comment author="martijnvg" created="2015-12-14T22:15:32Z" id="164577117">I've updated this PR and removed the background pipeline updater and added logic to deal with master blocks and when the pipeline loading itself fails.
</comment><comment author="martijnvg" created="2015-12-17T08:55:42Z" id="165387192">@javanna @s1monw I've updated this PR. I also replaced the Injector&lt;Client&gt; workaround by a client setter instead. This is cleaner, but in an ideal world Client should be a constructor parameter, but that requires some changes how setup `NodeClient`.
</comment><comment author="martijnvg" created="2015-12-17T09:47:06Z" id="165401005">I opened #15504 to track the cyclic dependency issues.
</comment><comment author="javanna" created="2015-12-18T16:09:51Z" id="165818963">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NPE when a segment with an empty cache gets closed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15202</link><project id="" key="" /><description>This is due to the fact that the query cache will still call the
onDocIdSetEviction callback in this case but with a number of entries equal to
zero.

Close #15043
</description><key id="120035586">15202</key><summary>Fix NPE when a segment with an empty cache gets closed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T21:05:26Z</created><updated>2015-12-18T09:41:22Z</updated><resolved>2015-12-03T08:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-02T22:21:25Z" id="161452518">The change looks good, but should this ultimately be fixed in Lucene? The docs for `onDocIdSetEviction` state "callback when one or more {@link DocIdSet}s are removed from this", implying numEntries &gt;=1?
</comment><comment author="jpountz" created="2015-12-03T08:31:00Z" id="161549878">Agreed @rjernst, I opened https://issues.apache.org/jira/browse/LUCENE-6918
</comment><comment author="sconts" created="2015-12-18T09:32:44Z" id="165725322">```
[2015-12-18 17:22:18,073][DEBUG][action.admin.indices.stats] [es-03] [indices:monitor/stats] failed to execute operation for shard [[2015-12-18][2], node[n5wFTChzTpOHU5xIskqcjQ], [R], v[12], s[INITIALIZING], a[id=XPoAFJEVRpeHC5zAwylHMA], unassigned_info[[reason=NODE_LEFT], at[2015-12-18T09:21:35.817Z], details[node_left[pA27t8SiTzqkLfXqbjUBUg]]]]
[2015-12-18][[2015-12-18][2]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [2015-12-18][[2015-12-18][2]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
    at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
    at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
    ... 7 more
[2015-12-18 17:22:19,601][DEBUG][action.admin.indices.stats] [es-03] [indices:monitor/stats] failed to execute operation for shard [[.marvel-es-data][0], node[n5wFTChzTpOHU5xIskqcjQ], [R], v[11], s[INITIALIZING], a[id=KCP4jFv3QomktoNW5KgOmA], unassigned_info[[reason=NODE_LEFT], at[2015-12-18T09:21:35.817Z], details[node_left[pA27t8SiTzqkLfXqbjUBUg]]]]
[.marvel-es-data][[.marvel-es-data][0]] BroadcastShardOperationFailedException[operation indices:monitor/stats failed]; nested: IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]];
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:405)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:382)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:371)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [.marvel-es-data][[.marvel-es-data][0]] IllegalIndexShardStateException[CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]]
    at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:974)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:808)
    at org.elasticsearch.index.shard.IndexShard.docStats(IndexShard.java:628)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:131)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:401)
... 7 more
```

os: centos 6.7
es 2.1.1
jdk 1.8u66

similar is not ok.
</comment><comment author="s1monw" created="2015-12-18T09:40:06Z" id="165729883">@jpountz I guess we have todo something like this:

``` diff
     public DocsStats docStats() {
+
         try (Engine.Searcher searcher = acquireSearcher("doc_stats")) {
             return new DocsStats(searcher.reader().numDocs(), searcher.reader().numDeletedDocs());
+        } catch (IllegalIndexShardStateException | AlreadyClosedException ex) {
+            return new DocsStats();
         }
     }
```
</comment><comment author="jasontedor" created="2015-12-18T09:41:21Z" id="165730187">@aaae That's a different but harmless issue; the verbose logging that results has been addressed by #14950.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15201</link><project id="" key="" /><description>The time has come for a Elasticsearch to implement a native API for reindexing! The first request I've found for this is (#492) filed back in 2010. With the Task Management API (#15117) will make this easier to manage. This meta ticket will cover the following use cases:
- [x] Resharding
- [x] Incompatible mapping updates
- [x] `touch`ing documents to pick up mapping updates made on the fly
- [x] Limiting the reindexed documents using a query
- [x] Update-by-query style changes to portions of the index at a time

&lt;s&gt;\* [ ] Copying an index from a remote cluster into this one&lt;/s&gt;

NOTICE: This meta issue gets fairly rambly from here on out. It will change. The list above will change. Everything is up for negotiation and everything needs to be prototyped before we're sure of anything. Things higher on the list are more likely to be in the final product.
# Resharding

It'd work like:

``` bash
# Stop writes to index

curl -XPUT localhost:9200/index_v2 -d'{
  "settings": {
    "number_of_shards": 10
  }
}'

curl -XPOST localhost:9200/a_single_command_to_start_copying_all_documents_from_index_v1_to_index_v2
# Save the returned task id

while [ curl -s "localhost:9200/_task/$TASK_ID?pretty&amp;awaitComplete" ]; do
  echo "not done"
done

# Do any manual checks that index_v2 is ok. Maybe warm it. Maybe raise its number of replicas if you built it with 0 replicas.

curl -XPOST localhost:9200/_aliases -d '{
    "actions": [
        { "remove": { "alias": "index", "index": "index_v1" }},
        { "add":    { "alias": "index", "index": "index_v2" }}
    ]
}
'

curl -XDELETE localhost:9200/index_v2

# Resume writes to index
```

You see from the example that its not automatic or atomic. It's still an event and it's very similar to [an old blog post](https://www.elastic.co/blog/changing-mapping-with-zero-downtime) about changing mappings with no downtime. The advantages of this as opposed to the scroll implementation proposed in the blog post are:
1. Elasticsearch can handle the messy details of the scroll API like `sort: "_doc"` and clearing the context when the copy is done and retrying when things fail.
2. Elasticsearch can to optimize the process to the point where it can do filesystem level things rather than scroll. The first implementation of reindex won't support such optimizations but they are totally possible and could cut the runtime down significantly.

The two curl commands in the middle are the new bits. This should start a background task to perform the copy:

```
curl -XPOST localhost:9200/a_single_command_to_start_copying_all_documents_from_index_v1_to_index_v2
```

and this should block for a while waiting for the task to complete:

```
curl -s "localhost:9200/_task/$TASK_ID?pretty&amp;awaitComplete"
```

This all piggy backs on the Task Management API (#15117) which isn't done yet, so it'll likely change. The reason this reindex command is a task is because it can take a long time. I, @nik9000, have personally seen these scroll type reindexes take hours for pretty big indexes. So if its going to take hours you'll need a way to cancel it or throttle it. And the task management API should have those ways though I have no idea what they'll look like.

You may ask "Why don't you combine the index creation, alias swap, and index delete into one task?" And that'd be a good question. It won't be part of the first implementation of this but might be part of later ones. Right now I don't like the idea very much. Keep reading. Maybe you'll agree with me. Maybe not. Leave a comment?
# Incompatible mapping updates

These'll work almost just like resharding. So much so that I won't give a curl example because I trust you, dear reader, can figure it out. The manual check of the index becomes much more important in this case. It's fairly believable that you'd want to keep both indexes alive for a period of time to test both. A/B testing or something.

The other way that mapping updates differ from resharding is that filesystem level optimization are much much less likely.
# `touch`ing documents to pick up mapping updates made on the fly

Some mapping updates can be made to an index on the fly but aren't picked up:
- Adding a new [field](https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-fields.html) to a property
- Adding a new property to a type when `"dynamic": false`

[This](https://gist.github.com/nik9000/3dcd921f7c6656d67430) offers a fairly complete example of adding a field using the PUT mapping API works and how you could use the reindex API to `touch` the documents.

This use case differs from the resharding and incompatible mapping update use cases in that the document isn't being added to an empty index, its being updated in an existing index. So if the the reindex process goes to touch the document but its changed between the time that the scroll took its snapshot of the index then the document shouldn't be changed. Luckily, Elasticsearch has built in support for optimistic concurrency control.
# Limiting the reindexed documents using a query

This seems like the logical extension to the other use cases more than a use case on its own. Its just a useful optimization on top of the other use cases. For example, you could use a query to only `touch` documents modified after a certain time.
# Update-by-query style changes to portions of the index at a time

"Increment `counter` on all documents matching this query" is a fairly normal operation on a relational database and Elasticsearch could have it too. Its fairly different internally from the other proposals but could be quite compelling though I admit to not having a good use case for it in mind. The trouble with this use case is that it tempts "increment `counter` on all documents" operations which are fairly inefficient in Elasticsearch. Its fairly inefficient in any system with concurrency control and most of them implement it anyway, but Elasticsearch makes an effort to make it difficult to do very inefficient things. Its inefficient because in Elasticsearch an update is an atomic delete and index operation and both of those operations are more expensive their relational counterparts. The delete itself is just as cheap but deleted document have to be reclaimed segment at a time rather than the aggressive measures relational datbases use. The index is much more expensive because the whole document has to be reanalyzed.

In many cases it'd be faster to copy the documents to a new index and then do the alias swap dance on it rather than update than it would be to touch every document in the index.

Even with all that it may be a fairly useful API to implement.
# Copying an index from a remote cluster into this one

Maybe the most ambitious use case on the list, the idea here is to scroll on a remote cluster and index into the cluster handling the request. This seems like a sensible way to implement basic disaster recovery. It'd be better if the query could subscribe to updates and get them streamed back, but even as is it'd fairly nice to run daily/hourly updates. Especially if the documents had a `last_modified_time` style column.
</description><key id="120034683">15201</key><summary>Reindex API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>feature</label><label>Meta</label></labels><created>2015-12-02T20:59:53Z</created><updated>2016-05-26T11:37:10Z</updated><resolved>2016-04-26T20:26:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="naivefun" created="2015-12-03T04:55:20Z" id="161513978">Exactly +1
</comment><comment author="lukas-vlcek" created="2015-12-03T08:36:01Z" id="161551128">@nik9000 nice! Is there any idea which ES version is targeted?
</comment><comment author="niemyjski" created="2015-12-03T15:13:48Z" id="161670774">Please make this available in 3.0! Currently we use foundatio to help us with this and it would be really really nice to have this sooner than later: https://github.com/exceptionless/Foundatio/blob/master/src/Elasticsearch/Jobs/ReindexWorkItemHandler.cs
</comment><comment author="nik9000" created="2015-12-03T15:42:32Z" id="161680962">&gt; @nik9000 nice! Is there any idea which ES version is targeted?

3.0.0 initially but I really want to backport it to 2.3 as well.

Another thing I should mention: I talked with @imotov who is doing the task management. All tasks will have an option for `wait_until_completion` to wait for the copy. It'll make using the API simpler for smaller copies but isn't something you'd want to use for large copies.

I've started the implementation for the first 4 use cases in #15125. Right now its a plugin - I believe it'll be a prebundled plugin which will be a new thing in 2.3.
</comment><comment author="xiaoshi2013" created="2015-12-05T13:41:51Z" id="162186952">Very nice looking
</comment><comment author="bdharrington7" created="2016-01-06T17:57:38Z" id="169404386">@nik9000 I'm curious, in your example for resharding you have comments indicating that we would have to stop indexing. What kind of steps would have to be taken if this wasn't possible?
</comment><comment author="nik9000" created="2016-01-06T18:03:14Z" id="169405719">&gt; @nik9000 I'm curious, in your example for resharding you have comments indicating that we would have to stop indexing. What kind of steps would have to be taken if this wasn't possible?

You'll want to have some way of replaying the same updates on you application side against both indexes. At some point I'd like to be able to install a redirect mechanism in Elasticsearch for the duration of the reindex operation. It seems like an obvious thing. Its not part of what I'm working on now and it introduces yet more complexity around versioning but its important.
</comment><comment author="nik9000" created="2016-02-04T15:59:57Z" id="179915944">I've updated the list of things that reindex will do. Right now we don't have external cluster support and I don't know when that'll become a priority.

Right now this is what is left before we can merge the feature/reindex branch down to master:
- [x] Progress from the task API: #16461
- [x] Retry bulk failures if they are safe to retry. Like rejection exceptions. #16556
- [x] Cancelation #16613
- [x] Move it from a plugin to a module so it ships with Elasticsearch by default #16619
- [x] Actually merge it to master #16861

Here are things that are left to do in the first phase of the project:
-  [x] Throttling
-  [x] Backporting to 2.x
</comment><comment author="HonzaKral" created="2016-03-22T17:01:42Z" id="199908032">This is indeed a super useful API, cannot wait!

Would it be possible to also, in future versions, provide additional functionality to allow `update` on the target index except of only `index` operations? My use case for this is entity centric indexing - imagine you have an index containing events and wish to group them by session. With the reindex api it should be possible to read the source events, apply a script (or just extract a field) to get the ID of a target document and pass it as a parameter to a specified update script.

Another use case we see a lot with users is that they want to move some data out of one index to another. Would it be possible to combine the reindex with delete-by-query essentially? After a document is indexed in the target index a `delete` operation will be issued on the source index. Of course this couldn't be done atomically, but even on best effort basis this would be super useful for a lot of people - essentially executing reindex and delete-by-query at the same time (on the same point in time snapshot of the index) with no additional guarantees than those two operations have individually.

I am happy to create individual issues for these use cases if they make sense to people.
</comment><comment author="nik9000" created="2016-04-26T20:26:16Z" id="214875333">I'm going to close this because reindex is done and live in 2.3.0 and 5.0.0-alpha1. I think @HonzaKral's point is really another feature request. @HonzaKral, can you make a new issue for it? Sorry!
</comment><comment author="HonzaKral" created="2016-04-26T21:03:51Z" id="214885545">Done as #17998 and #17997 
</comment><comment author="nik9000" created="2016-04-26T21:05:46Z" id="214886044">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add nicer error message when a plugin descriptor is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15200</link><project id="" key="" /><description>Currently, when a user tries to install an old plugin (pre 2.x) on a 2.x
node, the error message is cryptic (just printing the file path that was
missing, when looking for the descriptor). This improves the message to
be more explicit that the descriptor is missing, and suggests the
problem might be the plugin was built before 2.0.

closes #15197
</description><key id="120030713">15200</key><summary>Add nicer error message when a plugin descriptor is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T20:35:40Z</created><updated>2015-12-04T22:26:07Z</updated><resolved>2015-12-04T22:24:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-02T20:44:00Z" id="161428679">The change looks good to me and can be merged.
But I'm unsure if it fixes https://github.com/elastic/elasticsearch/issues/15197.
</comment><comment author="bleskes" created="2015-12-03T08:37:29Z" id="161551356">LGTM. Thanks @rjernst 
</comment><comment author="rjernst" created="2015-12-04T22:13:01Z" id="162096922">@dadoonet @bleskes I pushed a new commit which is a little better. We already check the plugin descriptor exists when unzipping a new plugin for installation, so what I had before was duplicating this at a lower level. Instead, I now detect this case where we are loading existing plugins and the descriptor is missing.
</comment><comment author="dadoonet" created="2015-12-04T22:15:43Z" id="162097394">LGTM
</comment><comment author="rjernst" created="2015-12-04T22:26:07Z" id="162099333">2.x change: 845bc97
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for user to set watcher metadata mapping </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15199</link><project id="" key="" /><description>Is there a way for user to set metadata mapping for watcher?  
I want to set all properties in metadata to not_analyzed strings.

Related discuss posts:  
https://discuss.elastic.co/t/is-it-possible-to-modify-the-default-watch-history-mapping/29431
https://discuss.elastic.co/t/customize-watch-and-or-watch-history-index-template/28539/2
</description><key id="120026397">15199</key><summary>Support for user to set watcher metadata mapping </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CTJyeh</reporter><labels /><created>2015-12-02T20:13:41Z</created><updated>2015-12-03T13:49:00Z</updated><resolved>2015-12-03T13:49:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T13:49:00Z" id="161644906">Hi @CTJyeh 

The place to discuss watcher is in the forums, where you linked to.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>support lt/lte and gt/gte in range aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15198</link><project id="" key="" /><description>The [range aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/search-aggregations-bucket-range-aggregation.html) currently based upon documentation and my testing only supports `to` and `from` terminology.

In the theme to avoid ambiguity and prefer `lt`, `lte`, `gt`, and `gte` it would be nice if the aggregation query DSL support this.

Current

```
{
    "aggs" : {
        "price_ranges" : {
            "range" : {
                "field" : "price",
                "ranges" : [
                    { "to" : 50 },
                    { "from" : 50, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

Proposed

```
{
    "aggs" : {
        "price_ranges" : {
            "range" : {
                "field" : "price",
                "ranges" : [
                    { "lte" : 50 },
                    { "gte" : 50, "lt" : 100 },
                    { "gte" : 100 }
                ]
            }
        }
    }
}
```

@dakrone 
</description><key id="120017361">15198</key><summary>support lt/lte and gt/gte in range aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Aggregations</label></labels><created>2015-12-02T19:28:03Z</created><updated>2015-12-03T13:56:26Z</updated><resolved>2015-12-03T13:56:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T21:44:20Z" id="161443214">This sounds similar to #5249 which has been closed as won't fix.
</comment><comment author="clintongormley" created="2015-12-03T13:56:26Z" id="161646638">Agreed. Closing as duplicate of #5249
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin installation fails with: "ERROR: /usr/share/elasticsearch/plugins/migration/plugin-descriptor.properties"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15197</link><project id="" key="" /><description>When an 1.x plugin (such as the migration plugin) exists in the plugins directory, new plugin installations fail with this error:

```
[root@max elasticsearch]# bin/plugin install license --verbose
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.1.0/license-2.1.0.zip ...
Downloading .......DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.1.0/license-2.1.0.zip checksums if available ...
Downloading .DONE
ERROR: /usr/share/elasticsearch/plugins/abest/plugin-descriptor.properties
```

The problem can be recreated in ES &gt;= 2.0 by simply making some empty directory within the plugins directory, and then attempting installation of any plugin.
</description><key id="120016872">15197</key><summary>Plugin installation fails with: "ERROR: /usr/share/elasticsearch/plugins/migration/plugin-descriptor.properties"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels /><created>2015-12-02T19:25:17Z</created><updated>2015-12-04T22:24:52Z</updated><resolved>2015-12-04T22:24:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-02T20:13:24Z" id="161420193">This is expected. Beginning with 2.0, you need to remove any plugins that are not built for the version of elasticsearch that you are running.
</comment><comment author="dadoonet" created="2015-12-02T20:32:16Z" id="161424989">If the plugin manager has to check all existing plugins when installing plugin X Y or Z, then it should do this at first before downloading anything.

Here I think it's not the expected behavior for the install command of the plugin manager but expected behavior for plugin service (when elasticsearch starts).

I might be wrong though.
</comment><comment author="rjernst" created="2015-12-02T20:34:37Z" id="161426318">This is the expected behavior always: we don't want to install a plugin that would later (possibly) fail when starting elasticsearch. This is why we unzip to a temp directory before actually "installing" (ie copying into the plugins dir). However, we can improve the error message.
</comment><comment author="dadoonet" created="2015-12-02T20:40:49Z" id="161427956">I agree that we check the plugin we are trying to install and fail in that case. Totally expected.

Here if I'm not mistaken we fails on plugin A when installing plugin X. I will check tomorrow.
</comment><comment author="rjernst" created="2015-12-02T20:58:41Z" id="161432145">&gt; Here if I'm not mistaken we fails on plugin A when installing plugin X

That is correct. Due to the ability to disable isolation for a plugin, we must load info about all plugins when installing a new plugin.
</comment><comment author="clintongormley" created="2015-12-03T13:55:42Z" id="161646462">This confusion has been noted a few times (ie installing plugin A then failing because of plugin B).  Perhaps it is as simple as adding a "Checking all plugins..." line to the output
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve "Analyzer [my_analyzer] must have a type associated with it" error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15196</link><project id="" key="" /><description>Trying to create a custom analyzer as part of index settings, while creating an index, if you forget to specify the "tokenizer" (or you misspell tokenizer) you get back an "Analyzer [my_analyzer] must have a type associated with it" error, which seems to be the wrong error message. It should rather say that the tokenizer is missing.

```
PUT test
{
  "settings" : {
    "analysis": {
      "analyzer": {
        "my_analyzer" : {
          "tokenizer_misspelled" : "whitespace",
          "filter" : ["lowercase", "stop", "snowball"]
        }
      }
    }
  }
}
```

The fact that a misspelled key is accepted although it will get ignored relates to #15183
</description><key id="119998897">15196</key><summary>Improve "Analyzer [my_analyzer] must have a type associated with it" error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Analysis</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-02T17:50:38Z</created><updated>2016-07-27T12:40:13Z</updated><resolved>2016-07-27T12:40:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T13:45:17Z" id="161643528">An analyzer can be based on another analyzer (by specifying `type`) or it defaults to being a `type: custom` analyzer if you specify `tokenizer`.  I'm not sure that we want to require an explicit `type: custom` line for every analyzer, but the error message could say: "An analyzer must either have a `type` or a `tokenizer` parameter"
</comment><comment author="javanna" created="2015-12-03T14:24:21Z" id="161654444">I see that same error in a couple of places in `AnalysisRegistry`. One occurrence relates to the specific case when "tokenizer" is not there, so I think we can update that error safely without affecting the other one that relates to "type". https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/analysis/AnalysisRegistry.java#L312
</comment><comment author="johtani" created="2016-07-27T12:37:56Z" id="235572107">In 5.0.0-alpha4, I got "analyzer [my_analyzer] must specify either an analyzer type, or a tokenizer" as an error message.
I think this already closed by #18455 .
Related #15492
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take ingored unallocated shards into account when making allocation decision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15195</link><project id="" key="" /><description>ClusterRebalanceAllocationDecider did not take unassigned shards into account
that are temporarily marked as ignored. This can cause unexpected behavior
when gateway allocator is still fetching shards or has marked shards as ignored
since their quorum is not met yet.

Closes #14670
Closes #14678

this is a backport of #14678 and closes #14776 

@bleskes can you take a look, I had to backport the UnassignedIterator as well from 2.0 but I think it makes things cleaner here as well.
</description><key id="119992927">15195</key><summary>Take ingored unallocated shards into account when making allocation decision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-12-02T17:22:05Z</created><updated>2015-12-16T14:50:23Z</updated><resolved>2015-12-03T09:16:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-12-03T08:53:16Z" id="161555300">LGTM. +1 on back porting the UnassignedIterator . It's simpler and this code is complex enough we should try to keep it the same. Thanks @s1monw 
</comment><comment author="s1monw" created="2015-12-03T09:17:06Z" id="161559874">I unlabeled it since we we already have https://github.com/elastic/elasticsearch/pull/14678 labeled accordingly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Request cache is not populated when using warmer API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15194</link><project id="" key="" /><description>It looks as the request cache is not populated when registering a warmer and the index has configured `index.requests.cache.enable`.  Reproduce like this

```
PUT /test
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index.requests.cache.enable": true
  },
  "warmers": {
    "warmer": {
      "source": {
        "aggs": {
          "aggs_1": {
            "terms": {
              "field": "field"
            }
          }
        }
      }
    }
  }
}

# Check stats, hit count should be 0
GET /test/_stats/request_cache?filter_path=indices.test.total.request_cache

# refresh runs the warmer
PUT /test/test/1?refresh
{
  "field" : "somevalue"
}

# Check stats, hit count should be 0 again
GET /test/_stats/request_cache?filter_path=indices.test.total.request_cache

# Execute agg
GET /test/_search?search_type=count
{
  "aggs": {
    "foo": {
      "terms": {
        "field": "field",
        "size": 10
      }
    }
  }
}

# PROBLEM HERE: Check stats, hit count should be 1, but is 0, missing count is increased
GET /test/_stats/request_cache?filter_path=indices.test.total.request_cache

# Execute agg a second time
GET /test/_search?search_type=count
{
  "aggs": {
    "foo": {
      "terms": {
        "field": "field",
        "size": 10
      }
    }
  }
}

# Finally we have a cache hit but too late
GET /test/_stats/request_cache?filter_path=indices.test.total.request_cache
```

The issue seems particularly troubling to me, because this means, that the user will always get one slow query (and I do not see any external possibility to prevent this, like running the query manually and hoping it gets executed before the first user query hits). This means there will always be a slow agg, if I understood it correctly.

This also happens with the query shard cache in 1.7.
</description><key id="119988498">15194</key><summary>Request cache is not populated when using warmer API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Warmers</label><label>bug</label></labels><created>2015-12-02T17:00:34Z</created><updated>2016-01-07T09:53:23Z</updated><resolved>2016-01-07T09:53:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T21:51:57Z" id="161445058">I would agree it's a shame that it does not get cached. But we have several ideas to remove warmers, and in-particular query-based warmers (#9331) so I wouldn't spend too much energy on fixing it.
</comment><comment author="jpountz" created="2016-01-07T09:53:23Z" id="169613394">Closing: warmers will go away in 3.0 #15614
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update geo_shape/query docs, fix TermStrategy defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15193</link><project id="" key="" /><description>This PR adds the following:
- SpatialStrategy documentation to the `geo-shape` reference docs.
- Updates spatial relation documentation in `geo-shape-query` reference docs.
- Updates `GeoShapeFieldMapper` to set `points_only` to `true` if `term` strategy is used (to be consistent with documentation)

closes #2358
</description><key id="119986353">15193</key><summary>Update geo_shape/query docs, fix TermStrategy defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nknize</reporter><labels><label>docs</label><label>review</label><label>v1.7.4</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T16:50:38Z</created><updated>2015-12-11T23:15:57Z</updated><resolved>2015-12-11T23:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T13:19:39Z" id="161636540">Some minor comments, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HDFS Snapshot/Restore plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15192</link><project id="" key="" /><description>**Associated issue (#15191)**
Migrated from ES-Hadoop. Contains several improvements regarding:
- Security
  Takes advantage of the pluggable security in ES 2.2 and uses that in order
  to grant the necessary permissions to the Hadoop libs. It relies on a
  dedicated DomainCombiner to grant permissions only when needed only to the
  libraries installed in the plugin folder
- Testing
  Uses a customized Local FS to perform actual integration testing of the
  Hadoop stack (and thus to make sure the proper permissions and ACC blocks
  are in place) however without requiring extra permissions for testing.
  If needed, a MiniDFS cluster is provided (though it requires extra
  permissions to bind ports)
- Build system
  Picks the build system used in ES (still Gradle)
</description><key id="119985960">15192</key><summary>HDFS Snapshot/Restore plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Plugin Repository HDFS</label><label>feature</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T16:48:58Z</created><updated>2016-02-01T13:54:43Z</updated><resolved>2015-12-14T19:52:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2015-12-02T16:50:49Z" id="161361867">effectively supersedes #13350
</comment><comment author="costin" created="2015-12-02T17:03:17Z" id="161365364">Note the documentation for the plugin can be found [here](https://github.com/elastic/elasticsearch-hadoop/tree/master/repository-hdfs).

In particular note that there are 3 `flavors` of jars offered by this plugin depending on the version of the library used: default (no Hadoop jar embedded, it should be available on the classpath), Hadoop1 and Hadoop2. 
As the plugin needs to work with Hadoop it requires its client libraries - depending on the version or distro used (CDH, HDP, etc...) one can use the already provided builds or simply add their own.
Note that the plugin permissions are tested on Hadoop 1 and 2 and apply only to the jars available under `hadoop-libs` folder. The user is free to customize this as she wants.
</comment><comment author="rjernst" created="2015-12-02T17:06:49Z" id="161366615">Thanks Costin. One thing I still dont' understand: is this one plugin (ie one zip) or is it multiple plugins that must be published separately? I see there are multiple zips, but it is unclear to me if they are actually plugins.
</comment><comment author="costin" created="2015-12-02T17:21:01Z" id="161370324">@rjernst The plugin binaries are the same since the API is stable across Hadoop versions. The different zips are just convenience so the user can select whatever _flavor_ she needs. Currently this was handled through qualifiers which means different naming - the plugin name (`repository-hdfs`) is the same but the installed binary is different.

&gt; However, I'm concerned that we are making multiple jars here, but not testing them all...

Valid point. I can create a different test that picks up the hadoop2 jar. In practice actually this means simply running the tests against the `hadoop2` plugin zip.
This works in gradle but I can't see an easy solution for the IDE run (which currently is using Hadoop1 simply because it is smaller and cleaner).
</comment><comment author="rjernst" created="2015-12-02T19:22:55Z" id="161404871">This change looks ok from the build side, with the caveats of 2 follow up issues I suggested (which I can take care of once merged). My only concern is the lack of any real test, since the only test I see would only be run as a one off. Could we have some minimal unit test on the HdfsRepository class itself?
</comment><comment author="rmuir" created="2015-12-03T01:02:01Z" id="161483123">i am fine with costin's approach to the security permissions (the evil domaincombiner): he explored a lot of alternatives, and this one is best. Actually pretty creative :) I left a few comments/suggestions.

basically i don't think we should do a lot of cleanup/investigation to the craziness of hadoop before merging the PR in, or even immediately afterwards (though if you already understand the problems, any comments in the source are helpful). 

i would instead prefer after merging that we prioritize improving the integration tests setup (e.g. rest tests) to really realistically simulate the plugin: e.g. get gradle to fire up the HDFS minicluster or some mock of it or what have you.

This way, its easier to clean it up, anyone can try to help, and if tests pass we are good :) Without solid integ tests though, personally I would never want to touch it, i don't have confidence that anything i do wouldn't completely break the plugin.
</comment><comment author="dadoonet" created="2015-12-03T07:20:50Z" id="161539487">Could you also pick up this file in your PR and adjust it if needed? https://github.com/dadoonet/elasticsearch/blob/4ccef59cffdfc956e7f84833b9945fe317b9dd99/docs/plugins/repository-hdfs.asciidoc
</comment><comment author="costin" created="2015-12-03T08:35:27Z" id="161551040">@dadoonet Of course. I'll include the docs in the next revision (which should follow up shortly today).
</comment><comment author="costin" created="2015-12-03T17:15:21Z" id="161720209">Hi, I have updated the PR (as a separate commit just in case).
I'll add the notes with references to the code based on the previous comments so they don't get hidden by Github (outdated diff)
</comment><comment author="costin" created="2015-12-07T12:34:26Z" id="162511956">Folks, should I merge the branch in master or are there any other things to take of ? Thanks,
</comment><comment author="rmuir" created="2015-12-07T13:12:52Z" id="162520463">I'm just repeating my big concern one last time: there are a ton of jars and security permissions needed for this thing, but are the tests sufficient? I only see one test, and its a unit test, and those tests live an in an unrealistic approximation of reality: "fantasy land"

If its pushed as is without test improvements, its reasonable to expect I will break the plugin completely in a matter of days (as we are still aggressively cleaning things up). The only way to avoid this is with tests.

For such situations we need a real integ test, this means gradle spins up HDFSMiniCluster or whatever is needed during `integTest` and there is a rest test that does a snapshot and a restore.
</comment><comment author="costin" created="2015-12-07T13:30:47Z" id="162525247">Point taken, I'll follow up with @rjernst to properly configure `integTest` to use the appropriate jar and make sure a snapshot/restore is executed.
For what it's worth, the unit test does apply the permissions - in fact if one is removed from the policy, the test will fail. Also it does perform an actual snapshot and restore (hence why in the initial commit I had logging enabled); what it doesn't do is handle class-loading since the unit test/IDE structure is different from the actual context at runtime.
</comment><comment author="s1monw" created="2015-12-07T13:41:49Z" id="162527316">&gt; Point taken, I'll follow up with @rjernst to properly configure integTest to use the appropriate jar and make sure a snapshot/restore is executed.

thanks @costin !!
</comment><comment author="rmuir" created="2015-12-07T14:08:42Z" id="162533397">&gt; For what it's worth, the unit test does apply the permissions - in fact if one is removed from the policy, the test will fail.

Yes, this is good, but unit tests do not know which jars are belonging to what thing (just a massive classpath) so it "approximates" and is overly generous: https://github.com/elastic/elasticsearch/blob/master/test-framework/src/main/java/org/elasticsearch/bootstrap/BootstrapForTesting.java#L175-L178

I agree, the proper integTest is a bigger issue (we need similar for cloud plugins too), it should not block merge to master, but we should open a followup and try to tackle it.
</comment><comment author="costin" created="2015-12-07T21:57:24Z" id="162678113">Hi,

I've added another integration test that checks the HDFS repository and delete it. Probably it can be improved (to snapshot an index) but I couldn't find a working example and gave up ...

```
[2015-12-07 23:39:56,805][INFO ][plugins                  ] [Professor X] loaded [repository-hdfs], sites []
[2015-12-07 23:39:56,807][WARN ][plugins                  ] [Professor X] Plugin: repository-hdfs implementing onModule by the type is not of Module type class org.elasticsearch.index.IndexModule
[2015-12-07 23:39:56,831][INFO ][env                      ] [Professor X] using [1] data paths, mounts [[WORK (E:)]], net usable_space [40gb], net total_space [111.7gb], spins? [unknown], types [NTFS]
[2015-12-07 23:39:57,159][INFO ][plugin.hadoop.hdfs       ] Loaded Hadoop [1.2.1] libraries from file:/E:/work/elasticsearch/master/plugins/repository-hdfs/build/cluster/integTest%20node0/elasticsearch-3.0.0-SNAPSHOT/plugins/repository-hdfs/
[2015-12-07 23:39:58,594][INFO ][node                     ] [Professor X] initialized
[2015-12-07 23:39:58,594][INFO ][node                     ] [Professor X] starting ...
[2015-12-07 23:39:58,704][WARN ][common.network           ] [Professor X] publish host: [_local_] resolves to multiple addresses, auto-selecting {127.0.0.1} as single publish address
[2015-12-07 23:39:58,704][INFO ][transport                ] [Professor X] publish_address {127.0.0.1:9500}, bound_addresses {127.0.0.1:9500}, {[::1]:9500}
[2015-12-07 23:39:58,711][INFO ][discovery                ] [Professor X] plugins_repository-hdfs_integTest/1LHITVsLRkGbnG4Qtj5lzg
[2015-12-07 23:40:01,727][INFO ][cluster.service          ] [Professor X] new_master {Professor X}{1LHITVsLRkGbnG4Qtj5lzg}{127.0.0.1}{127.0.0.1:9500}[testattr=&gt;test], reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-12-07 23:40:01,767][INFO ][gateway                  ] [Professor X] recovered [0] indices into cluster_state
[2015-12-07 23:40:01,794][WARN ][common.network           ] [Professor X] publish host: [_local_] resolves to multiple addresses, auto-selecting {127.0.0.1} as single publish address
[2015-12-07 23:40:01,795][INFO ][http                     ] [Professor X] publish_address {127.0.0.1:9400}, bound_addresses {127.0.0.1:9400}, {[::1]:9400}
[2015-12-07 23:40:01,795][INFO ][node                     ] [Professor X] started
[2015-12-07 23:40:08,247][INFO ][repositories             ] [Professor X] put repository [test_repo_hdfs_1]
[2015-12-07 23:40:08,302][INFO ][repositories             ] [Professor X] delete repository [test_repo_hdfs_1]
```

Note that there are several changes made to the build system to make this work as I couldn't figure a better way to do this:
1. `bundlePlugin` configuration cannot be changed since, at least [currently](https://github.com/elastic/elasticsearch/blob/4e80a5e0994f25caa19c3e78573bdc9fa6bcc4c6/buildSrc/src/main/groovy/org/elasticsearch/gradle/plugin/PluginBuildPlugin.groovy#L40),  the Gradle plugin relies on `project.afterEvaluate` which means whatever configuration is added, the plugin comes in last and overrides it.
   To overcome this, I have changed the default bundlePlugin to include the hadoop1 libraries and thus, to use that when testing.
   Going forward, this should not be the case and a different jar could be specified instead.
2. Instead of starting a MiniHDFSCluster (which is still possible but heavy-weight and can leave threads hanging), I've used the `TestingFs` which is a full-blown local HDFS implementation that runs in process. However I'm not aware of a way to add this jar to the plugin classpath as a separate jar hence why I have moved it into from testing to the main source so it's always included.
   Next step would be to move this back to the testing area and configure the build appropriately.

I went for 1 and 2 to have the tests running properly - as both are packaging problems they seem to me less important (though need to be fixed before releasing) then making sure the code is tested constantly.

Let me know what you think,
</comment><comment author="costin" created="2015-12-09T15:46:17Z" id="163298248">Hi,

Unless somebody objects, I'm going to merge this into master. I'll revert the integration test added last (to address @rjernst comment) so that packaging is not affected. I'll follow up with another ticket to pursue this further.

Once it is merged in master and jenkins picks it up, I'd like to merge it into 2.x/2.2. Any objections? /cc @s1monw @clintongormley (I'll raise a different issue for this one just in case).
</comment><comment author="costin" created="2015-12-14T19:54:08Z" id="164541292">I've squashed the PR, rebased it and merged it in master. Will let it sit for a few hours to be picked up by the build system and if everything is fine, follow up with the improvements for Gradle and backport for 2.2.

Thanks everyone for the feedback!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move repository-hdfs plugin from ES-Hadoop to Elasticsearch core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15191</link><project id="" key="" /><description>With the pluggable security in ES 2.2, the `repository-hdfs` plugin (from https://github.com/elastic/elasticsearch-hadoop) can be moved in the core as it does not require any extra changes.

Will raise a PR with the code rebased on master. Hopefully it will pass the review and be ported to 2.2 as well.
</description><key id="119985561">15191</key><summary>Move repository-hdfs plugin from ES-Hadoop to Elasticsearch core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Plugin Repository HDFS</label></labels><created>2015-12-02T16:47:16Z</created><updated>2016-05-03T13:51:38Z</updated><resolved>2016-05-03T13:51:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T13:51:38Z" id="216533561">This plugin has been merged into core for v5.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>analyze api hangs when the analyzer is not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15190</link><project id="" key="" /><description>The analyze api currently hangs when e.g. you refer to a custom analyzer and you forget to specify the index in the url. I remember from previous versions a nice "analyzer not found" error but that now (2.0) just hangs and the logs show:

```
[2015-12-02 15:27:46,621][ERROR][transport                ] [Smart Alec] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@41c75d23]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="119979172">15190</key><summary>analyze api hangs when the analyzer is not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Analysis</label><label>bug</label></labels><created>2015-12-02T16:20:52Z</created><updated>2015-12-03T13:13:58Z</updated><resolved>2015-12-03T13:13:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T13:13:58Z" id="161635258">Duplicate of #15148
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>analyze api accepts "filters" while create index relies on "filter"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15189</link><project id="" key="" /><description>The analyze api seems to accept a `filters` element, which makes sense, but create index only knows about `filter` instead (singular) when creating a custom analyzer. Can we make the two consistent?
</description><key id="119977477">15189</key><summary>analyze api accepts "filters" while create index relies on "filter"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Analysis</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-12-02T16:13:44Z</created><updated>2016-04-21T09:59:03Z</updated><resolved>2016-04-21T09:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-12-02T16:34:50Z" id="161357271">+1
</comment><comment author="dakrone" created="2015-12-02T17:17:55Z" id="161369565">+1
</comment><comment author="clintongormley" created="2015-12-05T12:59:53Z" id="162183458">Same goes for `char_filters`/`char_filter`.

@johtani want to take a look?
</comment><comment author="johtani" created="2015-12-28T02:02:40Z" id="167457895">I am planning to delete `filters`/`char_filters from` `_analyze` API  in master, and deprecated them in 2.x.
Does it make sense? 
</comment><comment author="clintongormley" created="2016-01-10T18:33:45Z" id="170380169">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More explicit limitations around compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15188</link><project id="" key="" /><description>Taken from @nik9000 comments...
</description><key id="119970623">15188</key><summary>More explicit limitations around compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels /><created>2015-12-02T15:45:41Z</created><updated>2016-03-10T13:11:08Z</updated><resolved>2016-03-10T13:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of the type-unsafe empty Collections fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15187</link><project id="" key="" /><description>This commit removes and now forbids all uses of the type-unsafe empty
Collections fields Collections#EMPTY_LIST, Collections#EMPTY_MAP, and
Collections#EMPTY_SET. The type-safe methods Collections#emptyList,
Collections#emptyMap, and Collections#emptySet should be used instead.
</description><key id="119969924">15187</key><summary>Remove and forbid use of the type-unsafe empty Collections fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T15:42:40Z</created><updated>2015-12-02T16:15:36Z</updated><resolved>2015-12-02T16:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-02T15:46:53Z" id="161340005">LGTM. I wonder if you can remove any `@SuppressWarnings` annotations now? That can come in another PR too. Whatever works for you.
</comment><comment author="ywelsch" created="2015-12-02T15:49:08Z" id="161340949">LGTM
</comment><comment author="jasontedor" created="2015-12-02T15:58:56Z" id="161344147">@nik9000 Good thought. The only relevant ones I found have been eliminated with commit 199a05311ece371a883b2d3490b8a73417a34139.

I found these with `git show --pretty="format:" --name-only 05430a788a383ee228c9140c0fd9c1268bb4557c | xargs grep unchecked` where 05430a788a383ee228c9140c0fd9c1268bb4557c is the commit making the change.
</comment><comment author="jasontedor" created="2015-12-02T16:04:46Z" id="161346054">Thanks for reviewing @nik9000 and @ywelsch (and thanks for the [nit](https://github.com/elastic/elasticsearch/pull/15150#discussion_r46420107) on #15150 that led to this quick pull request).
</comment><comment author="jpountz" created="2015-12-02T16:06:36Z" id="161346324">Thank you so much for this @jasontedor, it was very annoying to get all those compiler warnings!
</comment><comment author="nik9000" created="2015-12-02T16:07:15Z" id="161346564">&gt; Thank you so much for this @jasontedor, it was very annoying to get all those compiler warnings!

+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing huge number of aliases make the cluster unstable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15186</link><project id="" key="" /><description>ES version 1.4.4

I have 250K aliases in my ES cluster pointing the same index. Removing huge number (over 1000) aliases in 1 batch request (https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html) makes the whole cluster unstable. Master node uses 100% CPU and a lot of search and indexing requests ends up with timeout at client side. I am not sure if these are requests that query/index data from shards placed on master node only or from any node, but a lot of requests to any node time out.

Once removing aliases query finishes everything gets back to normal.
</description><key id="119964465">15186</key><summary>Removing huge number of aliases make the cluster unstable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdojwa</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2015-12-02T15:20:28Z</created><updated>2015-12-10T13:34:08Z</updated><resolved>2015-12-10T13:34:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T21:54:32Z" id="161445689">&gt; Master node uses 100% CPU

Interesting. Any chance that you can capture [hot threads](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html) on the master node to see where this cpu goes?
</comment><comment author="mdojwa" created="2015-12-03T10:03:22Z" id="161576905">Hi, sure.
Here https://gist.github.com/mdojwa/a54d3c97738836952f83 you will find 5 hot_threads dumps done sequentially while the request deleting aliases was running. The master node was es6-1

I was asked to create this ticket by ES support team. They said: "...When ES tries to resolve what it needs to do, for each command, it ends up looping over all the aliases each time."
</comment><comment author="ywelsch" created="2015-12-04T17:33:06Z" id="162029698">Which API are you using?

Currently, having an aliases request of the form

```
POST /_aliases
{
    "actions" : [
        { "remove" : { "index" : "test1", "alias" : "alias1" } },
        { "remove" : { "index" : "test1", "alias" : "alias2" } },
        { "remove" : { "index" : "test1", "alias" : "alias3" } },
        { "remove" : { "index" : "test1", "alias" : "alias4" } },
       ...
    ]
}
```

indeed leads to the issue that the alias name is resolved against all alias names for the index for each "remove" entry.

Would be nice if you could specify

```
POST /_aliases
{
    "actions" : [
        { "remove" : { "index" : "test1", "alias" : "alias1,alias2,alias3,alias4,..." } },
       ...
    ]
}
```

but that's currently not possible.

Only the `_alias` endpoint provides this at the moment:

```
/{index}/_alias/alias1,alias2,alias3,...
```

With that though you might run into request header size limits.
</comment><comment author="clintongormley" created="2015-12-05T12:34:52Z" id="162178369">Good thinking @ywelsch 
</comment><comment author="bleskes" created="2015-12-05T15:15:35Z" id="162211686">agreed this would be the simplest now (as opposed to be smarted about executing arbitrary individual actions). Can we also support an array format?

```
POST /_aliases
{
    "actions" : [
        { "remove" : { "index" : "test1", "alias" : [ "alias1" ,"alias2" ... ] } },
       ...
    ]
}
```
</comment><comment author="ywelsch" created="2015-12-08T10:45:32Z" id="162846225">I have implemented @bleskes's suggestion in #15305 by adding an array syntax for aliases (as well as indices). A minor difference to his suggestion is that I used a different field name ("aliases" instead of "alias") for the array syntax so that the types (simple value vs array) are not confused. The old syntax is still valid.

As an example, the following will remove multiple aliases in one go:

```
POST /_aliases
{
    "actions" : [
        { "remove" : { "index" : "test1", "aliases" : [ "alias1" ,"alias2" ... ] } },
       ...
    ]
}
```

Note that, using the Java client, specifying multiple aliases was already syntactically possible, but forbidden by a later check. This check has been removed in #15305.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>changing search_analyzer on existing field returns error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15185</link><project id="" key="" /><description>Trying to change the search analyzer for an existing field through put mapping api:

```
PUT test/my_type/1
{
  "field":"value"
}

PUT test/_mapping/my_type
{
  "properties": {
    "field" : {
      "type": "string",
      "search_analyzer": "keyword"
    }
  }
}
```

I get back "analyzer on field [field] must be set when search_analyzer is set". The error seems to imply that I have to add the "analyzer" too, although it is the default one already. Adding `"analyzer": "standard"` as follows:

```
PUT test/_mapping/my_type
{
  "properties": {
    "field" : {
      "type": "string",
      "search_analyzer": "keyword",
      "analyzer": "standard"
    }
  }
}
```

causes a "Merge failed with failures {[mapper [field] has different [analyzer]]}". I am not sure what I am doing wrong or whether there is a bug somewhere, but at least the errors here really need to be improved and clarified.
</description><key id="119957712">15185</key><summary>changing search_analyzer on existing field returns error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2015-12-02T14:50:18Z</created><updated>2016-02-14T22:14:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-02T15:18:42Z" id="161329663">That sounds like trouble. I'm pretty sure I remember changing the search analyzer on the fly a few times.
</comment><comment author="jimczi" created="2015-12-02T15:40:07Z" id="161337516">By default the analyzer is set with the name "default" (and not "standard" though default points to "standard" ...). If you change the analyzer name to "default" you'll be able to change the search_analyzer ;)
</comment><comment author="javanna" created="2015-12-02T16:10:33Z" id="161348338">cool thanks @jimferenczi !  still I don't get why one has to specify the analyzer (and has to know to use default instead of standard). It might make sense for new mappings to be explicit, not for fields that are already defined. Seems like bad user experience to me.
</comment><comment author="rjernst" created="2015-12-02T19:27:41Z" id="161406531">I agree it makes sense only for new mappings. I think the problem is simply that standard and default are not considered "the same" in the validation code.
</comment><comment author="rjernst" created="2015-12-02T19:37:21Z" id="161409550">Scratch my last statement. We have no way to distinguish when this is a "new mapping" vs an "update".
</comment><comment author="rjernst" created="2015-12-02T19:42:00Z" id="161410791">I also think this is actually a good thing. It makes it very clear to the user that they now have 2 analyzers acting on the field.
</comment><comment author="djschny" created="2015-12-02T20:47:37Z" id="161429532">Agree with @javanna that this is a bad user experience. I fail to see how this makes it very clear to the user.

Perhaps instead make it so when you `GET` a mapping instead of having implied values that are not present, we always explicitly include in the json the analyzer so that way users will be able to see the `default` vs `standard` situation.
</comment><comment author="rjernst" created="2015-12-02T21:27:29Z" id="161438791">@djschny I was not referring to the problem with `default` vs `standard`. The request in first post here that included `analyzer` should have worked. We should fix that. I was only referring to the fact that `analyzer` must be included when updating `search_analyzer`.
</comment><comment author="javanna" created="2015-12-02T23:21:30Z" id="161465083">I understand in general that being explicit is better, but if we just want to change the search_analyzer, and an analyzer is already defined for the field, and we don't want to change that analyzer, why should one be forced to specify again that same analyzer, which is also the only one that will be accepted (as the index analyzer cannot be changed)? That seems misleading from a user perspective, the fact that technically it's challenging to distinguish between new mappings and updates is a different story, that we may or may not have to deal with internally.
</comment><comment author="jpountz" created="2015-12-21T10:48:27Z" id="166268862">Mappings have a way to apply a default source. For instance this is how the `_default_` mapping is applied. On the contrary when a mapping already exists for a given type, we don't apply any defaults. I'm wondering that maybe this issue could be solved by applying the current mapping as a default source when mappings for a type already exist?
</comment><comment author="clintongormley" created="2016-02-14T18:34:12Z" id="183946381">@jpountz that sounds like a plan
</comment><comment author="clintongormley" created="2016-02-14T18:35:10Z" id="183946427">And honestly the requirement to have to be explicit about setting an analyzer when adding a search_analyzer is just annoying - I see no real benefit in this requirement.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disallow index template pattern to be the same as an alias name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15184</link><project id="" key="" /><description>This can cause index creation to fail down the line.

Follow up of PR #14842
</description><key id="119954298">15184</key><summary>Disallow index template pattern to be the same as an alias name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index Templates</label><label>enhancement</label><label>review</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T14:32:56Z</created><updated>2015-12-03T12:36:12Z</updated><resolved>2015-12-03T12:36:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-02T17:19:50Z" id="161370002">LGTM thanks @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>create index accept "filters" but it will be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15183</link><project id="" key="" /><description>If you create an index specifying settings which contain a custom analyzer, and you specify its filters, you need to use the singular it seems (`filter` is the way to go, but is not enforced). Otherwise the filters will get accepted but won't be used.

```
PUT test
{
  "settings" : {
    "analysis": {
      "analyzer": {
        "my_analyzer" : {
          "tokenizer" : "whitespace",
          "filters" : ["lowercase"]
        }
      }
    }
  }
}

GET test/_analyze?text="This is my text"&amp;analyzer=my_analyzer
```

Note that the filters element ends up in the cluster state, returned by the get index api, but it won't have any effect. I assume that whatever not supported element will not be rejected and be silently ignored, we should make this stricter when parsing. Same happens if you misspell e.g. char_filters .
</description><key id="119953660">15183</key><summary>create index accept "filters" but it will be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Index APIs</label><label>bug</label></labels><created>2015-12-02T14:29:53Z</created><updated>2015-12-03T13:08:28Z</updated><resolved>2015-12-03T13:08:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-03T13:08:28Z" id="161633160">This is not just to do with analysis - any index settings are lenient. Closing in favour of https://github.com/elastic/elasticsearch/issues/6732
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UnicastBackwardsCompatibilityIT fails frequently on 2.1 Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15182</link><project id="" key="" /><description>This test fails quiet frequently over the last few days on 2.1 branch, going through the failure mails it seems its only on Windows 2012 machine. Last seen was

http://build-us-00.elastic.co/job/es_core_21_window-2012/129/

and the top of the stacktrace reads like

```
java.lang.IllegalStateException: Failed to start elasticserch. Responses: [couldn't find elasticsearch bound to localhost:29301!]
    at __randomizedtesting.SeedInfo.seed([24181440F4C4423C]:0)
    at org.elasticsearch.test.ExternalNodeServiceClient.interact(ExternalNodeServiceClient.java:85)
    at org.elasticsearch.test.ExternalNodeServiceClient.stop(ExternalNodeServiceClient.java:60)
    at org.elasticsearch.test.ExternalNode.stop(ExternalNode.java:231)
    at org.elasticsearch.test.ExternalNode.close(ExternalNode.java:257)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:97)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:84)
    at org.elasticsearch.test.CompositeTestCluster.close(CompositeTestCluster.java:266)
    at org.apache.lucene.util.IOUtils.close(IOUtils.java:97)
    at org.elasticsearch.test.ESIntegTestCase.clearClusters(ESIntegTestCase.java:588)
    at org.elasticsearch.test.ESIntegTestCase.afterClass(ESIntegTestCase.java:1978)
```

Going throught the output it is not clear to me if this is a follow up error cause by something else.
</description><key id="119953225">15182</key><summary>UnicastBackwardsCompatibilityIT fails frequently on 2.1 Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-12-02T14:27:49Z</created><updated>2016-02-14T18:30:21Z</updated><resolved>2016-02-14T18:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-02T14:50:53Z" id="161321054">If I had to guess blindly I'd guess that I left a bug in ExternalNode that let it try to shut down the node twice. Why it only comes up on windows is a mystery to me though.
</comment><comment author="clintongormley" created="2016-02-14T18:30:21Z" id="183945141">Old test failure. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] The geoip processor should only try to read *.mmdb files from the geoip config directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15181</link><project id="" key="" /><description>This build failure is caused by the fact this processor tries to read all files in the config directory:
http://build-us-00.elastic.co/job/es_feature_ingest/2948/

The test framework adds extra files in temp directories to see if code is able to handle that. This is common on certain operation systems (like: .DS_Store, thumbs.db, .nfsXXX files). (see ExtrasFS)
</description><key id="119943003">15181</key><summary>[Ingest] The geoip processor should only try to read *.mmdb files from the geoip config directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-02T13:43:16Z</created><updated>2015-12-03T20:17:09Z</updated><resolved>2015-12-03T20:17:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-02T14:34:05Z" id="161316959">Looks good! so ExtrasFS was useful for once, and not just an annoyance :)
</comment><comment author="martijnvg" created="2015-12-02T15:56:07Z" id="161343093">yes, ExtrasFS was indeed useful :)
</comment><comment author="talevy" created="2015-12-03T16:41:53Z" id="161710150">should we add a test with a directory that does have extra files?

other than that, LGTM

edit: I guess it is being tested indirectly via the higher-level integration testing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations Refactor: Refactor Moving Average Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15180</link><project id="" key="" /><description /><key id="119927411">15180</key><summary>Aggregations Refactor: Refactor Moving Average Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>:Search Refactoring</label><label>review</label></labels><created>2015-12-02T12:04:18Z</created><updated>2015-12-07T11:52:41Z</updated><resolved>2015-12-07T11:00:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-03T14:54:19Z" id="161662801">Left a few small comments, would improve readability if readFrom/writeTo for the Models would be somehow symetric, but maybe that's hard to achieve with the current serialization infra. Otherwise looks good to me.
</comment><comment author="jpountz" created="2015-12-03T19:03:01Z" id="161748480">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geohash query not working after migrating to Elasticsearch 2.0/2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15179</link><project id="" key="" /><description>I am having trouble with my query after migrating to Elasticsearch 2.0/2.1. Previously used version 1.4. I am trying to do a GeoDistance query using geohash. That's the excerpt from ES response:

```
"root_cause": [
  {
    "type": "query_parsing_exception",
    "reason": "failed to find geo_point field [geohash]",
    "index": "campaigns",
    "line": 1,
    "col": 234
  }
]
```

The minimal index usable to reproduce this problem is as follows:

```
curl -s -X PUT "http://localhost:9200/campaigns" -d'{
    "settings": {
            "analysis": {
                "filter": {
                    "name_ngram": {
                        "type": "nGram",
                        "min_gram": "2",
                        "max_gram": "50"
                    }
                },
                "analyzer": {
                    "search_analyzer": {
                        "filter": [
                            "standard",
                            "asciifolding",
                            "lowercase",
                            "trim"
                        ],
                        "tokenizer": "standard"
                    },
                    "name_analyzer": {
                        "filter": [
                            "name_ngram",
                            "standard",
                            "asciifolding",
                            "lowercase",
                            "trim"
                        ],
                        "tokenizer": "standard"
                    }
                }                                                                                                                                                                                                                                                                
            }                                                                                                                                                                                                                                                                    
    },                                                                                                                                                                                                                                                                           
    "mappings": {                                                                                                                                                                                                                                                                
        "campaign": {                                                                                                                                                                                                                                                            
            "properties": {                                                                                                                                                                                                                                                      
                "locations": {                                                                                                                                                                                                                                                   
                    "properties": {                                                                                                                                                                                                                                              
                        "id": {                                                                                                                                                                                                                                                  
                            "include_in_all": false,                                                                                                                                                                                                                             
                            "type": "integer"                                                                                                                                                                                                                                    
                        },                                                                                                                                                                                                                                                       
                        "geohash": {                                                                                                                                                                                                                                             
                            "type": "geo_point"                                                                                                                                                                                                                                  
                        }                                                                                                                                                                                                                                                        
                    },                                                                                                                                                                                                                                                           
                    "type": "nested"                                                                                                                                                                                                                                             
                },                                                                                                                                                                                                                                                               
                "id": {                                                                                                                                                                                                                                                          
                    "type": "integer"                                                                                                                                                                                                                                            
                }                                                                                                                                                                                                                                                                
            },                                                                                                                                                                                                                                                                   
            "_all": {                                                                                                                                                                                                                                                            
                "search_analyzer": "search_analyzer",                                                                                                                                                                                                                            
                "analyzer": "name_analyzer"                                                                                                                                                                                                                                      
            }                                                                                                                                                                                                                                                                    
        }                                                                                                                                                                                                                                                                        
    }                                                                                                                                                                                                                                                                            
}'
```

I am using two analyzers here (as this is what I am using in my project, tried also with one analyzer and the result is the same).

The minimal data indexed in ES is as follows:

```
curl -s -X POST "http://localhost:9200/campaigns/campaign" -d'{
    "id": 12,
    "locations": [
        {
            "id": 13,
            "geohash": {
                "geohash": "s7ws01wyd7ws"
            }
        }
    ]
}'
```

The mapping in ES:

```
curl -s -X GET "http://localhost:9200/campaigns/_mapping?pretty"
{
  "campaigns" : {
    "mappings" : {
      "campaign" : {
        "_all" : {
          "analyzer" : "name_analyzer",
          "search_analyzer" : "search_analyzer"
        },
        "properties" : {
          "id" : {
            "type" : "integer"
          },
          "locations" : {
            "type" : "nested",
            "properties" : {
              "geohash" : {
                "type" : "geo_point"
              },
              "id" : {
                "type" : "integer",
                "include_in_all" : false
              }
            }
          }
        }
      }
    }
  }
}
```

Working query to search by "locations.id":

```
curl -s -X POST "http://localhost:9200/campaigns/_search" -d@'{
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "locations",
          "filter": {
            "term": {
              "locations.id": "13"
            }
          }
        }
      }
    }
  }
}' | jq ''
{
  "took": 5,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "campaigns",
        "_type": "campaign",
        "_id": "AVFYQn4uic5h_a2BXp1k",
        "_score": 1,
        "_source": {
          "id": 12,
          "locations": [
            {
              "id": 13,
              "geohash": {
                "geohash": "s7ws01wyd7ws"
              }
            }
          ]
        }
      }
    ]
  }
}
```

Not working query to search by "geohash" (only when the data is posted from file, but the same response is returned in my application):

```
cat query_geohash.json 
{
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "locations",
          "filter": {
            "geo_distance": {
              "distance": "100.0km",
              "geohash": "s7ws01wyd7ws"
            }
          }
        }
      }
    }
  }
}
curl -s -X POST "http://localhost:9200/campaigns/_search" -d@query_geohash.json | jq ''
{
  "error": {
    "root_cause": [
      {
        "type": "query_parsing_exception",
        "reason": "failed to find geo_point field [geohash]",
        "index": "campaigns",
        "line": 1,
        "col": 234
      }
    ],
    "type": "search_phase_execution_exception",
    "reason": "all shards failed",
    "phase": "query",
    "grouped": true,
    "failed_shards": [
      {
        "shard": 0,
        "index": "campaigns",
        "node": "cqrNlEmRSPGcGewKuUvlnA",
        "reason": {
          "type": "query_parsing_exception",
          "reason": "failed to find geo_point field [geohash]",
          "index": "campaigns",
          "line": 1,
          "col": 234
        }
      }
    ]
  },
  "status": 400
}
```

Strangely, the same query works OK when the query is not read from file, but posted "inline":

```
curl -s -X POST "http://localhost:9200/campaigns/_search" -d@'{
  "query": {
    "filtered": {
      "filter": {
        "nested": {
          "path": "locations",
          "filter": {
            "geo_distance": {
              "distance": "100.0km",
              "geohash": "s7ws01wyd7ws"
            }
          }
        }
      }
    }
  }
}' | jq ''
{
  "took": 11,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "campaigns",
        "_type": "campaign",
        "_id": "AVFYQn4uic5h_a2BXp1k",
        "_score": 1,
        "_source": {
          "id": 12,
          "locations": [
            {
              "id": 13,
              "geohash": {
                "geohash": "s7ws01wyd7ws"
              }
            }
          ]
        }
      }
    ]
  }
}
```

What am I missing here? The geohash query itself looks OK (response is the same when I use "locations.geohash" instead of "geohash"). The only difference is the way the query is fed to curl. Also, both versions work correctly in ES 1.4.

I am not sure if this is a bug in Elasticsearch or if I made some mistake in my query. Anyway, I'll be really grateful for any help!
</description><key id="119925607">15179</key><summary>Geohash query not working after migrating to Elasticsearch 2.0/2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jmatraszek</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-12-02T11:51:48Z</created><updated>2016-03-09T15:36:33Z</updated><resolved>2016-03-09T15:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-02T12:55:53Z" id="161283766">Hi @jmatraszek 

Thanks for the full recreation.  

&gt; Strangely, the same query works OK when the query is not read from file, but posted "inline":

I get the same failure whether I use a file or post inline.  The first issue is that you always need to specify full paths for fields, so you should specify `locations.geohash` instead of just `geohash`.  However, when you do this, you now get the exception `failed to find geo_point field [locations]`.

The reason for this is the use of the `{ "field": {"geohash": "s7ws01wyd7ws" }}` format, which is undocumented and I was certainly unaware that it existed.  The documented format is `{"field": "s7ws01wyd7ws"}`.

However, this undocumented format is interfering with your query as it strips `geohash` off the path name to leave just `locations`. It works if you run it as follows:

```
POST /campaigns/_search
{
  "query": {
    "nested": {
      "path": "locations",
      "filter": {
        "geo_distance": {
          "distance": "100.0km",
          "locations.geohash.geohash": "s7ws01wyd7ws"
        }
      }
    }
  }
}
```

I think we should just remove the ability to specify a geohash with `{ "field": { "geohash": "s7ws01wyd7ws"}}` and this problem will be solved.
</comment><comment author="nknize" created="2016-01-09T04:50:40Z" id="170195183">This is an interesting one. Turned out to be a very good catch! Its actually not a problem that you can specify a geohash with `{"field" : { "geohash": "..." }}`  Its superfluous, but has been supported since 1.0.0.Beta1 (see PR #3352 where documentation is mentioned but not actually added). Removing it will have no effect here but I'll open a separate issue to remove it anyway.

In this case its a simple name clash. The mapping defines a sub-field `"geohash"` for the `"locations"` nested field. The string "geohash" is the same as the `geohash` extension that signals to the query parser the point is encoded as a geohash.  

So if you pass `"locations.geohash"` the query parser sees the `.geohash` extension, parses the point, strips the extension, then fails to find a `geo_point` type for the `locations` nested type.

One immediate solution is exactly what @clintongormley suggested: pass the full `"locations.geohash.geohash"` path. This tells the query parser, "the subfield named 'geohash', for nested field 'locations' will be in a geohash format". 
The other solution is to simply rename the `geohash` sub-field so there's no naming clash.

In pre-2.x (before GeoPointV2) its actually more scary if you name clash with multi-fields. No exception will be thrown because it won't query on the expected `geohash` sub-field. Certainly problematic for prefix queries. So again, great find - this is definitely trappy! I'll open a PR to fix it on master and backport.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sort implement writable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15178</link><project id="" key="" /><description>In the current state of the search refactoring the SearchSourceBuilder still holds the sort as BytesReference which needs to be parsed later on the shard. This was done to speed up the query refactoring but in order to parse it on the coordinating, we need to make implement Writeable and override equals and hashcode for testing.

Relates to #10217
</description><key id="119911250">15178</key><summary>Make sort implement writable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label></labels><created>2015-12-02T10:23:44Z</created><updated>2016-04-12T10:13:26Z</updated><resolved>2016-04-12T10:13:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-04-12T10:13:26Z" id="208832561">Closing as all depending issues/PRs are done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove MergeMappingException.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15177</link><project id="" key="" /><description>Failures to merge a mapping can either come as a MergeMappingException if they
come from Mapper.merge or as an IllegalArgumentException if they come from
FieldTypeLookup.checkCompatibility. I think we should settle on one: this pull
request replaces all usage of MergeMappingException with
IllegalArgumentException.
</description><key id="119908161">15177</key><summary>Remove MergeMappingException.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T10:05:28Z</created><updated>2015-12-04T12:55:54Z</updated><resolved>2015-12-04T12:55:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T10:05:41Z" id="161245687">For the record, I plan to make this change 3.0-only.
</comment><comment author="nik9000" created="2015-12-02T15:26:27Z" id="161331819">LGTM. Left minor comment that shouldn't block merging.
</comment><comment author="rjernst" created="2015-12-02T16:01:36Z" id="161344989">+1!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>v2.1 failed to delete temp file Exception when start node </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15176</link><project id="" key="" /><description>---
## Exception

[WARN ][index.translog           ] [es_sqmall_dev_1] [index][1] failed to delete temp file /home/sauna.test/server/elasticsearch/data/es_sqmall_dev/nodes/0/indices/index/1/translog/translog-2613417349747513949.tlog
java.nio.file.NoSuchFileException: /home/sauna.test/server/elasticsearch/data/es_sqmall_dev/nodes/0/indices/index/1/translog/translog-2613417349747513949.tlog
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
    at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
    at java.nio.file.Files.delete(Files.java:1079)
    at org.elasticsearch.index.translog.Translog.recoverFromFiles(Translog.java:324)
    at org.elasticsearch.index.translog.Translog.&lt;init&gt;(Translog.java:166)
    at org.elasticsearch.index.engine.InternalEngine.openTranslog(InternalEngine.java:209)
    at org.elasticsearch.index.engine.InternalEngine.&lt;init&gt;(InternalEngine.java:152)
    at org.elasticsearch.index.engine.InternalEngineFactory.newReadWriteEngine(InternalEngineFactory.java:25)
    at org.elasticsearch.index.shard.IndexShard.newEngine(IndexShard.java:1408)
    at org.elasticsearch.index.shard.IndexShard.createNewEngine(IndexShard.java:1403)
    at org.elasticsearch.index.shard.IndexShard.internalPerformTranslogRecovery(IndexShard.java:906)
    at org.elasticsearch.index.shard.IndexShard.performTranslogRecovery(IndexShard.java:883)
    at org.elasticsearch.index.shard.StoreRecoveryService.recoverFromStore(StoreRecoveryService.java:245)
    at org.elasticsearch.index.shard.StoreRecoveryService.access$100(StoreRecoveryService.java:56)
    at org.elasticsearch.index.shard.StoreRecoveryService$1.run(StoreRecoveryService.java:129)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
</description><key id="119905259">15176</key><summary>v2.1 failed to delete temp file Exception when start node </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidcai19840412</reporter><labels /><created>2015-12-02T09:50:31Z</created><updated>2015-12-02T09:55:34Z</updated><resolved>2015-12-02T09:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-02T09:55:33Z" id="161243062">Fixed in https://github.com/elastic/elasticsearch/pull/14872
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check mapping compatibility up-front.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15175</link><project id="" key="" /><description>Today we only check mapping compatibility when adding mappers to the
lookup structure. However, at this stage, the mapping has already been merged
partially, so we can leave mappings in a bad state. This commit removes the
compatibility check from Mapper.merge entirely and performs it _before_ we
call Mapper.merge.

One minor regression is that the exception messages don't group together errors
that come from MappedFieldType.checkCompatibility and Mapper.merge. Since we
run the former before the latter, Mapper.merge won't even have a chance to let
the user know about conflicts if conflicts were discovered by
MappedFieldType.checkCompatibility.

Close #15049
</description><key id="119902077">15175</key><summary>Check mapping compatibility up-front.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T09:35:18Z</created><updated>2015-12-04T13:46:32Z</updated><resolved>2015-12-04T13:46:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-12-02T15:27:27Z" id="161332063">&gt; Mapper.merge won't even have a chance to let
&gt; the user know about conflicts if conflicts were discovered by
&gt; MappedFieldType.checkCompatibility.

Worth it.
</comment><comment author="rjernst" created="2015-12-04T11:47:21Z" id="161948876">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`phase_slop` question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15174</link><project id="" key="" /><description>I add `phase_slop` in `query_string`, the query dsl as follow:

```
{
   "query": {
      "bool": {
         "must": [
           {
             "term": {
               "platform": 0
             }
           },
            {
               "query_string": {
                  "default_field": "text",

                  "query": "\"&#32654;&#20029;&#30340;&#34013;&#33394;&#22810;&#29785;&#27827;\"",
                 "phrase_slop": 1
               }
            }
         ]
      }
   },
  "highlight": {
    "fields": {
      "text": {
        "fragment_size": 150
      }
    }
  }
}
```

But I get shard failures result as follows:

```
{
    "took": 528,
    "timed_out": false,
    "_shards": {
        "total": 30,
        "successful": 29,
        "failed": 1,
        "failures": [
            {
                "index": "listening_v2_201511",
                "shard": 7,
                "status": 500,
                "reason": "RemoteTransportException[[es13][inet[/10.10.3.98:9300]][indices:data/read/search[phase/fetch/id]]]; nested: FetchPhaseExecutionException[[listening_v2_201511][7]: query[filtered(+platform:[0 TO 0] +(text:\"&#32654;&#20029; &#20029; &#34013;&#33394; &#34013; &#33394; &#22810;&#29785;&#27827; &#27827;\"~1))-&gt;cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@b5b6c454)],from[0],size[10]: Fetch Failed [Failed to highlight field [text]]]; nested: StringIndexOutOfBoundsException[String index out of range: -1]; "
            }
        ]
    },
    "hits": {
        "total": 117,
        "max_score": 87.00564
    ...
```

How solve it?
</description><key id="119891330">15174</key><summary>`phase_slop` question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels><label>:Highlighting</label><label>feedback_needed</label></labels><created>2015-12-02T08:31:36Z</created><updated>2015-12-02T13:12:44Z</updated><resolved>2015-12-02T13:12:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-02T12:08:33Z" id="161273145">This works for me in 1.7.0 and 2.1.0:

```
PUT t/t/1
{
  "text": "\"&#32654;&#20029;&#30340;&#34013;&#33394;&#22810;&#29785;&#27827;\"",
  "platform": 0
}

GET _search
{
   "query": {
      "bool": {
         "must": [
           {
             "term": {
               "platform": 0
             }
           },
            {
               "query_string": {
                  "default_field": "text",

                  "query": "\"&#32654;&#20029;&#30340;&#34013;&#33394;&#22810;&#29785;&#27827;\"",
                 "phrase_slop": 1
               }
            }
         ]
      }
   },
  "highlight": {
    "fields": {
      "text": {
        "fragment_size": 150
      }
    }
  }
}
```

Please could you provide a full recreation of the problem, plus the stack trace that you're seeing in the logs, and the version of ES that you're using.
</comment><comment author="feifeiiiiiiiiiii" created="2015-12-02T12:33:44Z" id="161278805">ES version:

```
version: {
    number: "1.5.2",
    build_hash: "62ff9868b4c8a0c45860bebb259e21980778ab1c",
    build_timestamp: "2015-04-27T09:21:06Z",
    build_snapshot: false,
    lucene_version: "4.10.4"
}
```

Analyzer:

use `chinese` [IK](https://github.com/medcl/elasticsearch-analysis-ik)(modify source)

part mapping as follow:

```
{
   text: {
      mapping: {
          searchAnalyzer: "ik",
          indexAnalyzer: "ik",
          include_in_all: false,
          omit_norms: true,
          store: "no",
          norms: {
              enabled: false
          },
          term_vector: "with_positions_offsets",
          type: "string"
     },
     path_match: "text"
  }
}
```
</comment><comment author="clintongormley" created="2015-12-02T13:12:44Z" id="161286518">I've tried the following on 1.5.2:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "text": {
          "omit_norms": true,
          "store": "no",
          "norms": {
            "enabled": false
          },
          "term_vector": "with_positions_offsets",
          "type": "string"
        }
      }
    }
  }
}

PUT t/t/1
{
  "text": "\"&#32654;&#20029;&#30340;&#34013;&#33394;&#22810;&#29785;&#27827;\"",
  "platform": 0
}

GET _search
{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "platform": 0
          }
        },
        {
          "query_string": {
            "default_field": "text",
            "query": "\"&#32654;&#20029;&#30340;&#34013;&#33394;&#22810;&#29785;&#27827;\"",
            "phrase_slop": 1
          }
        }
      ]
    }
  },
  "highlight": {
    "fields": {
      "text": {
        "fragment_size": 150
      }
    }
  }
}
```

And it works correctly.  I haven't tried with the `ik` analyzer as it is not supported by Elasticsearch.  I suggest trying to recreate without the `ik` analyzer.  If you can do that, then reopen this issue here, otherwise open an issue on https://github.com/medcl/elasticsearch-analysis-ik
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable es_include at init</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15173</link><project id="" key="" /><description>`es_include` is not exported at init so it is not loaded from the right /etc/default/elasticsearch when users have custom default files causing gc.log file not to load. 
</description><key id="119871736">15173</key><summary>Enable es_include at init</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">rhoml</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T05:43:35Z</created><updated>2016-04-05T04:02:28Z</updated><resolved>2016-03-10T19:31:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rhoml" created="2015-12-02T05:44:59Z" id="161187481">Mmm I just signed the agreement
</comment><comment author="nik9000" created="2016-02-10T14:43:54Z" id="182402392">Hi @rhoml! I'm sorry this sat for so long.

It looks like setting ES_INCLUDE opts you out from sourcing `/usr/share/elasticsearch/elasticsearch.in.sh` or similar files. I suspect setting it is more likely to break things than not. My understanding is that the right place to put parameters like ES_JVM_ARGS is /etc/defaults/elasticsearch in Debian/Ubunutu. Would that work for you?
</comment><comment author="rjernst" created="2016-02-10T17:20:04Z" id="182489416">&gt; parameters like ES_JVM_ARGS

I think using `JAVA_OPTS` is better, since it is the standard environment variable for this purpose? At some point we should clean up the env vars we accept, `ES_JVM_ARGS` shouldn't be necessary.
</comment><comment author="rhoml" created="2016-02-11T00:31:58Z" id="182656438">@nik9000 the problem is that by default `/usr/share/elasticsearch/elasticsearch.in.sh` gets loaded but if you change the name of the file to `/usr/share/elasticsearch/logstash.elasticsearch.in.sh` for example it never gets loaded. Causing problem in situations like when you have multiple Elasticsearch instances running on the same box.
</comment><comment author="nik9000" created="2016-02-11T01:25:05Z" id="182667174">&gt; Causing problem in situations like when you have multiple Elasticsearch instances running on the same box.

Now I understand. Could you make the same change in the init script for the rpm and maybe the systemd files? We have these Vagrant tests where you can test it but I imagine they are a bit stale lately because we haven't been diligent about getting jenkins running them....
</comment><comment author="rhoml" created="2016-02-11T01:50:03Z" id="182670419">Yes, I will do it right away
</comment><comment author="rhoml" created="2016-02-11T02:34:21Z" id="182677095">I've already added it to DEB, and RPM.
</comment><comment author="clintongormley" created="2016-03-10T12:56:49Z" id="194830515">@nik9000 could you take a look when you have a moment, please?
</comment><comment author="nik9000" created="2016-03-10T19:11:32Z" id="195005048">OK - the change looks good to me. I'll see about merging to 2.x where the bats tests are still happy and validate there and then merge into master.

I'd love to have a bats tests are still happy but that is a large ask.
</comment><comment author="nik9000" created="2016-03-10T19:11:59Z" id="195005156">&gt; I'd love to have a bats tests are still happy but that is a large ask.

Er, what I meant to say is:
I'd love to have a bats test for this but that is a large ask.
</comment><comment author="nik9000" created="2016-03-10T19:32:59Z" id="195011483">OK! Merged to master and cherry picked to 2.x with cd7a5e5f1707aef51595457110a3d9f2acd5a6a0 and 291515726d291e1b92d0d6be281f6423f81ff358. Thanks @rhoml ! Sorry this took so long.
</comment><comment author="rhoml" created="2016-04-05T04:02:28Z" id="205633131">No worries @nik9000 thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove unused core dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15172</link><project id="" key="" /><description>Was doing some analysis on our third party deps, and was surprised/horrified to see ones that are totally unnecessary. We should aggressively prune these, dependencies are expensive.

snakeyaml is not needed: we pull in jackson which already brings in yaml support shaded into itself.
LZF is no longer needed in master (2.0 uses deflate, see https://github.com/elastic/elasticsearch/pull/11279)
</description><key id="119865841">15172</key><summary>remove unused core dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T04:36:19Z</created><updated>2016-03-10T18:53:44Z</updated><resolved>2015-12-02T12:20:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-12-02T09:51:30Z" id="161241836">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>v2.1  it seems cannot bind multi ip address by network.bind_host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15171</link><project id="" key="" /><description> it seems cannot bind multi ip address by config network.bind_host as follows
# 
# elasticsearch.yml

network.publish_host: 10.165.33.223
network.bind_host: [ "10.165.33.223", "localhost" ]
# 
# boot logs:

[2015-12-02 11:56:23,014][INFO ][node                     ] [es_sqmall_dev_1] initialized
[2015-12-02 11:56:23,014][INFO ][node                     ] [es_sqmall_dev_1] starting ...
[2015-12-02 11:56:23,115][INFO ][transport                ] [es_sqmall_dev_1] publish_address {10.165.33.223:9300}, bound_addresses {127.0.0.1:9300}
[2015-12-02 11:56:23,127][INFO ][discovery                ] [es_sqmall_dev_1] es_sqmall_dev/4QEx0_CEQWWheCwG9s0zNA
[2015-12-02 11:56:26,171][INFO ][cluster.service          ] [es_sqmall_dev_1] new_master {es_sqmall_dev_1}{4QEx0_CEQWWheCwG9s0zNA}{10.165.33.223}{10.165.33.223:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-12-02 11:56:26,187][INFO ][http                     ] [es_sqmall_dev_1] publish_address {10.165.33.223:9200}, bound_addresses {127.0.0.1:9200}
[2015-12-02 11:56:26,188][INFO ][node                     ] [es_sqmall_dev_1] started
[2015-12-02 11:56:26,230][INFO ][gateway                  ] [es_sqmall_dev_1] recovered [0] indices into cluster_state
</description><key id="119863195">15171</key><summary>v2.1  it seems cannot bind multi ip address by network.bind_host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidcai19840412</reporter><labels /><created>2015-12-02T04:03:03Z</created><updated>2015-12-02T04:17:00Z</updated><resolved>2015-12-02T04:05:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-02T04:05:48Z" id="161175142">its not a bug, this was never supported. you can bind to all addresses (0.0.0.0) or just one.
</comment><comment author="davidcai19840412" created="2015-12-02T04:17:00Z" id="161176609">thx for replying
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facing errors in ES 2.1.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15170</link><project id="" key="" /><description>Hi Experts,

I have downloaded new version of ES. I am getting the following errors while starting ES.
1) It says Unable to install syscall filter as it not supported for OS windows 8
![image](https://cloud.githubusercontent.com/assets/11420150/11521220/56c44522-98d0-11e5-8fa1-dc0ef27e9b30.png)
2) Getting a huge java error message 
![image](https://cloud.githubusercontent.com/assets/11420150/11521257/9d35846c-98d0-11e5-9c1f-d131c6443323.png)

Please help me to understand how I can rid off with the above 2 problems in new ES

Thanks 
VG
</description><key id="119857779">15170</key><summary>Facing errors in ES 2.1.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vg15</reporter><labels /><created>2015-12-02T03:13:14Z</created><updated>2015-12-02T06:37:30Z</updated><resolved>2015-12-02T06:37:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-02T06:37:30Z" id="161195074">Please ask questions on discuss.elastic.co.
We can help there.
Instead of doing screen capture, copy and paste the full text. You can also look at this in logs/ dir.

The first warn is just a warn. You can ignore it. 
The second line is incomplete. Can't comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Average seems to be wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15169</link><project id="" key="" /><description>I seem to be getting the wrong result when performing average aggregations via Kibana 4. As you can see, the values being returned for the are steadily right around 8,500,000,000. 
![screenshot from 2015-12-01 21-38-22](https://cloud.githubusercontent.com/assets/3859825/11520754/5665d390-9874-11e5-93d5-7d15b8e3aa58.png)

Yet, somehow, I get an average around 4,755,736,842,123,345,920! 
![screenshot from 2015-12-01 21-37-56](https://cloud.githubusercontent.com/assets/3859825/11520761/603d9286-9874-11e5-87f5-3b0d415408d6.png)

![screenshot from 2015-12-01 21-42-30](https://cloud.githubusercontent.com/assets/3859825/11520767/6db9d9ce-9874-11e5-9488-ca2da29b1970.png)

Running ES 1.7.2, JVM 1.8.0_31
</description><key id="119854846">15169</key><summary>Average seems to be wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bigl0af</reporter><labels /><created>2015-12-02T02:46:03Z</created><updated>2015-12-02T13:20:05Z</updated><resolved>2015-12-02T13:20:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-02T13:20:05Z" id="161287758">Hi @bigl0af 

It looks like you have an index which has conflicting mappings between shards. In other words, the same field has been mapped differently on different shards, resulting in this weird calculation.  The only thing to do here is to reindex the data with the correct mappings.

This issue has been fixed (along with many others) in the great mapping cleanup in 2.0 (see #8870)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup precommit task gradle code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15168</link><project id="" key="" /><description>This change attempts to simplify the gradle tasks for precommit. One
major part of that is using a "less groovy style", as well as being more
consistent about how tasks are created and where they are configured. It
also allows the things creating the tasks to set up inter task
dependencies, instead of assuming them (ie decoupling from tasks
eleswhere in the build).
</description><key id="119845040">15168</key><summary>Cleanup precommit task gradle code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-12-02T01:12:25Z</created><updated>2015-12-02T07:50:57Z</updated><resolved>2015-12-02T07:50:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-12-02T01:42:15Z" id="161152592">+1, this is a lot easier to follow!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add ability to define custom grok patterns within processor config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15167</link><project id="" key="" /><description>adds ability to do something like this:

``` json
{
  "grok" : {
    ...
    "pattern_definitions" : {
      "MY_PATTERN" : "hello!"
    }
  }
}
```
</description><key id="119839478">15167</key><summary>add ability to define custom grok patterns within processor config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-02T00:22:59Z</created><updated>2015-12-03T16:27:38Z</updated><resolved>2015-12-03T16:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-12-02T10:14:57Z" id="161247504">One small docs comment OTT LTGM
</comment><comment author="talevy" created="2015-12-03T16:27:29Z" id="161705181">updated doc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>move PatternUtils#loadBankFromStream into GrokProcessor.Factory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15166</link><project id="" key="" /><description>result of this discussion: https://github.com/s1monw/elasticsearch/commit/042ef8c20476459f92ea1c2ea12bc1950211e693#diff-d6c93604d5ab9be5d576bf73cda704f6R30
</description><key id="119835144">15166</key><summary>move PatternUtils#loadBankFromStream into GrokProcessor.Factory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-01T23:47:23Z</created><updated>2015-12-03T16:08:05Z</updated><resolved>2015-12-03T16:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-12-02T07:17:25Z" id="161204788">LGTM
</comment><comment author="martijnvg" created="2015-12-02T08:49:06Z" id="161223215">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove GET from _forcemerge API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15165</link><project id="" key="" /><description>Currently it supports `GET` as though it were a `POST`, but since it does have an impact, this seems like a misleading use of the API. As it is a breaking change, it should only happen in 3.0.

Also, if we ever choose to provide any sort of diagnostic information (e.g., from a task management API) about force merge operations, then this change won't be _as_ breaking at that time.
</description><key id="119834850">15165</key><summary>Remove GET from _forcemerge API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>breaking</label><label>low hanging fruit</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T23:45:17Z</created><updated>2015-12-04T17:39:46Z</updated><resolved>2015-12-04T17:39:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fixing typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15164</link><project id="" key="" /><description>Replace "Too shade or not to shade..." with "To shade or not to shade..."
</description><key id="119818427">15164</key><summary>Fixing typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">xcoulon</reporter><labels><label>docs</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T22:04:24Z</created><updated>2015-12-02T08:27:54Z</updated><resolved>2015-12-02T08:22:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-12-02T06:48:18Z" id="161199235">Thank you Xavier! Did you sign the CLA?
</comment><comment author="xcoulon" created="2015-12-02T08:08:00Z" id="161214756">@dadoonet I just signed it, yes.
</comment><comment author="dadoonet" created="2015-12-02T08:26:12Z" id="161217294">Thanks! Pushed!
</comment><comment author="xcoulon" created="2015-12-02T08:27:54Z" id="161217542">great ;-) thanks !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Ingest] Removed the lazy cache in DatabaseReaderService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15163</link><project id="" key="" /><description>and eagerly build all available databases in the geoip config directory.

Based on review in: https://github.com/s1monw/elasticsearch/commit/042ef8c20476459f92ea1c2ea12bc1950211e693#diff-97831e392a1ed947f718c65e2a5d0a96R32
</description><key id="119814973">15163</key><summary>[Ingest] Removed the lazy cache in DatabaseReaderService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-12-01T21:45:11Z</created><updated>2015-12-02T10:20:10Z</updated><resolved>2015-12-02T10:20:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-12-02T00:36:01Z" id="161141708">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 1.7.3 crashed and doesn't restart - no logs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15162</link><project id="" key="" /><description>System info:
- Ubuntu 14.04.3 64 bit LTS
- ElasticSearch 1.7.3 from Elastic repo
- 24Gb RAM
- ~300Gb SSD (more than 250Gb left)
- No swap

I've been using that install for quite a while with no problem. It suddenly crashed for an unknown reason. Unfortunately, there is no log about the crash and starting it using 'service elasticsearch start' doesn't work; it hangs for about 10 seconds then fails and no logs are generated in /var/log/elasticsearch about it (yes, there are logs but there are from before it crashed).

Here are the last few lines in the log file if that's any useful (indexing_slowlog and search_slowlog are both empty):

```
[2015-11-30 10:23:17,551][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.01.30] update_mapping [monit] (dynamic)
[2015-11-30 10:57:01,403][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [snort] (dynamic)
[2015-11-30 11:43:31,081][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [sophos] (dynamic)
[2015-11-30 12:26:45,955][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [fortinet] (dynamic)
[2015-11-30 12:28:10,743][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [fortinet] (dynamic)
[2015-11-30 13:36:47,798][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [auth] (dynamic)
[2015-11-30 13:37:04,885][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [auth] (dynamic)
[2015-11-30 14:34:35,672][INFO ][cluster.metadata         ] [Tomorrow Man] [logstash-2015.11.30] update_mapping [iptables-dropped] (dynamic)
[2015-11-30 14:51:38,409][WARN ][monitor.jvm              ] [Tomorrow Man] [gc][young][2427240][80285] duration [1s], collections [1]/[1.7s], total [1s]/[1.7h], memory [10.6gb]-&gt;[10.2gb]/[18.9gb], all_pools {[young] [396.7mb]-&gt;[1mb]/[399.4mb]}{[survivor] [24.4mb]-&gt;[23.2mb]/[49.8mb]}{[old] [10.2gb]-&gt;[10.2gb]/[18.5gb]}
[2015-11-30 14:57:44,552][WARN ][monitor.jvm              ] [Tomorrow Man] [gc][young][2427567][80301] duration [4.1s], collections [1]/[5.1s], total [4.1s]/[1.7h], memory [10.6gb]-&gt;[10.2gb]/[18.9gb], all_pools {[young] [398.5mb]-&gt;[9.5mb]/[399.4mb]}{[survivor] [22.2mb]-&gt;[22.9mb]/[49.8mb]}{[old] [10.2gb]-&gt;[10.2gb]/[18.5gb]}
```

I checked to see if /var/run/elasticsearch had the right privileges/permission and it has (owned by 'elasticsearch' user):

```
~# ls /var/run/ -al | grep elastic
drwxr-xr-x  2 elasticsearch elasticsearch   60 Nov  2 12:23 elasticsearch
```

I found http://sandlininc.com/?p=747 and added the log line

```
log_daemon_msg "sudo -u $ES_USER $DAEMON $DAEMON_OPTS"
```

right before the deamon startup (with start-stop-daemon). Starting it with the sudo command displayed on the screen when trying to start with 'service' works fine and stays up (and displays no errors) but starting it through 'service' still fails:

```
sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch
```

Unfortunately, updating to 2.1.0 is not an option at this time since another tool we use depends on 1.7.3 to work.

As mentioned by someone else, there is more than enough disk space (more than 250Gb left). RAM isn't an issue either. the system has 24Gb and there is 19Gb given to ElasticSearch (ES_HEAP_SIZE=19g, in /etc/default/elasticsearch).

I got a little bit of help from the [forum](https://discuss.elastic.co/t/elasticsearch-1-7-3-crashed-and-doesnt-restart-no-logs/) and here is the output of 'bash -x /etc/init.d/elasticsearch start':

```
+ PATH=/bin:/usr/bin:/sbin:/usr/sbin
+ NAME=elasticsearch
+ DESC='Elasticsearch Server'
+ DEFAULT=/etc/default/elasticsearch
++ id -u
+ '[' 0 -ne 0 ']'
+ . /lib/lsb/init-functions
+++ run-parts --lsbsysinit --list /lib/lsb/init-functions.d
++ for hook in '$(run-parts --lsbsysinit --list /lib/lsb/init-functions.d 2&gt;/dev/null)'
++ '[' -r /lib/lsb/init-functions.d/20-left-info-blocks ']'
++ . /lib/lsb/init-functions.d/20-left-info-blocks
++ for hook in '$(run-parts --lsbsysinit --list /lib/lsb/init-functions.d 2&gt;/dev/null)'
++ '[' -r /lib/lsb/init-functions.d/50-ubuntu-logging ']'
++ . /lib/lsb/init-functions.d/50-ubuntu-logging
+++ LOG_DAEMON_MSG=
++ FANCYTTY=
++ '[' -e /etc/lsb-base-logging.sh ']'
++ true
+ '[' -r /etc/default/rcS ']'
+ . /etc/default/rcS
++ UTC=yes
+ ES_USER=elasticsearch
+ ES_GROUP=elasticsearch
+ JDK_DIRS='/usr/lib/jvm/java-8-oracle/ /usr/lib/jvm/j2sdk1.8-oracle/ /usr/lib/jvm/jdk-7-oracle-x64 /usr/lib/jvm/java-7-oracle /usr/lib/jvm/j2sdk1.7-oracle/ /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-armhf /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/default-java'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-8-oracle//bin/java -a -z '' ']'
+ JAVA_HOME=/usr/lib/jvm/java-8-oracle/
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/j2sdk1.8-oracle//bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/jdk-7-oracle-x64/bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-oracle/bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/j2sdk1.7-oracle//bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk/bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk-amd64//bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk-armhf/bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk-i386//bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/default-java/bin/java -a -z /usr/lib/jvm/java-8-oracle/ ']'
+ export JAVA_HOME
+ ES_HOME=/usr/share/elasticsearch
+ MAX_OPEN_FILES=65535
+ LOG_DIR=/var/log/elasticsearch
+ DATA_DIR=/var/lib/elasticsearch
+ WORK_DIR=/tmp/elasticsearch
+ CONF_DIR=/etc/elasticsearch
+ CONF_FILE=/etc/elasticsearch/elasticsearch.yml
+ MAX_MAP_COUNT=262144
+ PID_DIR=/var/run/elasticsearch
+ '[' -f /etc/default/elasticsearch ']'
+ . /etc/default/elasticsearch
++ ES_HEAP_SIZE=19g
+ PID_FILE=/var/run/elasticsearch/elasticsearch.pid
+ DAEMON=/usr/share/elasticsearch/bin/elasticsearch
+ DAEMON_OPTS='-d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch'
+ export ES_HEAP_SIZE
+ export ES_HEAP_NEWSIZE
+ export ES_DIRECT_SIZE
+ export ES_JAVA_OPTS
+ test -x /usr/share/elasticsearch/bin/elasticsearch
+ case "$1" in
+ checkJava
+ '[' -x /usr/lib/jvm/java-8-oracle//bin/java ']'
+ JAVA=/usr/lib/jvm/java-8-oracle//bin/java
+ '[' '!' -x /usr/lib/jvm/java-8-oracle//bin/java ']'
+ '[' -n '' -a -z 19g ']'
+ log_daemon_msg 'Starting Elasticsearch Server'
+ '[' -z 'Starting Elasticsearch Server' ']'
+ log_use_fancy_output
+ TPUT=/usr/bin/tput
+ EXPR=/usr/bin/expr
+ '[' -t 1 ']'
+ '[' xxterm '!=' x ']'
+ '[' xxterm '!=' xdumb ']'
+ '[' -x /usr/bin/tput ']'
+ '[' -x /usr/bin/expr ']'
+ /usr/bin/tput hpa 60
+ /usr/bin/tput setaf 1
+ '[' -z ']'
+ FANCYTTY=1
+ case "$FANCYTTY" in
+ true
+ /usr/bin/tput xenl
++ /usr/bin/tput cols
+ COLS=144
+ '[' 144 ']'
+ '[' 144 -gt 6 ']'
++ /usr/bin/expr 144 - 7
+ COL=137
+ log_use_plymouth
+ '[' n = y ']'
+ plymouth --ping
+ printf ' * Starting Elasticsearch Server       '
 * Starting Elasticsearch Server       ++ /usr/bin/expr 144 - 1
+ /usr/bin/tput hpa 143
                                                                                                                                               + printf ' '
 ++ pidofproc -p /var/run/elasticsearch/elasticsearch.pid elasticsearch
++ local pidfile base status specified pid OPTIND
++ pidfile=
++ specified=
++ OPTIND=1
++ getopts p: opt
++ case "$opt" in
++ pidfile=/var/run/elasticsearch/elasticsearch.pid
++ specified=specified
++ getopts p: opt
++ shift 2
++ '[' 1 -ne 1 ']'
++ base=elasticsearch
++ '[' '!' specified ']'
++ '[' -n /var/run/elasticsearch/elasticsearch.pid -a -r /var/run/elasticsearch/elasticsearch.pid ']'
++ read pid
++ '[' -n '' ']'
++ '[' -n specified ']'
++ '[' -e /var/run/elasticsearch/elasticsearch.pid -a '!' -r /var/run/elasticsearch/elasticsearch.pid ']'
++ return 3
+ pid=
+ '[' -n '' ']'
+ mkdir -p /var/log/elasticsearch /var/lib/elasticsearch /tmp/elasticsearch
+ chown elasticsearch:elasticsearch /var/log/elasticsearch /var/lib/elasticsearch /tmp/elasticsearch
+ '[' -n /var/run/elasticsearch ']'
+ '[' '!' -e /var/run/elasticsearch ']'
+ '[' -n /var/run/elasticsearch/elasticsearch.pid ']'
+ '[' '!' -e /var/run/elasticsearch/elasticsearch.pid ']'
+ '[' -n 65535 ']'
+ ulimit -n 65535
+ '[' -n '' ']'
+ '[' -n 262144 -a -f /proc/sys/vm/max_map_count ']'
+ sysctl -q -w vm.max_map_count=262144
+ log_daemon_msg 'sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch'
+ '[' -z 'sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch' ']'
+ log_use_fancy_output
+ TPUT=/usr/bin/tput
+ EXPR=/usr/bin/expr
+ '[' -t 1 ']'
+ '[' xxterm '!=' x ']'
+ '[' xxterm '!=' xdumb ']'
+ '[' -x /usr/bin/tput ']'
+ '[' -x /usr/bin/expr ']'
+ /usr/bin/tput hpa 60
+ /usr/bin/tput setaf 1
+ '[' -z 1 ']'
+ true
+ case "$FANCYTTY" in
+ true
+ /usr/bin/tput xenl
++ /usr/bin/tput cols
+ COLS=144
+ '[' 144 ']'
+ '[' 144 -gt 6 ']'
++ /usr/bin/expr 144 - 7
+ COL=137
+ log_use_plymouth
+ '[' n = y ']'
+ plymouth --ping
+ printf ' * sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch       '
 * sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch       ++ /usr/bin/expr 144 - 1
+ /usr/bin/tput hpa 143
                                                                                                                                               + printf ' '
 + start-stop-daemon --start -b --user elasticsearch -c elasticsearch --pidfile /var/run/elasticsearch/elasticsearch.pid --exec /usr/share/elasticsearch/bin/elasticsearch -- -d -p /var/run/elasticsearch/elasticsearch.pid --default.config=/etc/elasticsearch/elasticsearch.yml --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.work=/tmp/elasticsearch --default.path.conf=/etc/elasticsearch
+ return=0
+ '[' 0 -eq 0 ']'
+ i=0
+ timeout=10
+ exit 0
```

And I also have a strace file from 'strace -f bash /etc/init.d/elasticsearch start' that I can give for analysis.
</description><key id="119808630">15162</key><summary>ElasticSearch 1.7.3 crashed and doesn't restart - no logs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ThomasdOtreppe</reporter><labels /><created>2015-12-01T21:09:59Z</created><updated>2015-12-02T11:34:05Z</updated><resolved>2015-12-02T11:34:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ThomasdOtreppe" created="2015-12-01T21:17:13Z" id="161098393">I had a similar issue previously on another install (where there was no logs) where it ran out of disk space, which made ElasticSearch crash and even after expanding disk (from 16Gb to 250Gb), it wouldn't restart.
</comment><comment author="clintongormley" created="2015-12-02T11:34:05Z" id="161267605">Hi @ThomasdOtreppe 

I think the best place to continue this discussion is on the [thread in the forum](https://discuss.elastic.co/t/elasticsearch-1-7-3-crashed-and-doesnt-restart-no-logs/36080/9). If you find an actual bug then for sure let's open an issue here, but for the moment it sounds like something wrong on your system, which we should figure out in the forum instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`indices.recovery.file_chunk_size` got lost in #13840</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15161</link><project id="" key="" /><description>The setting `indices.recovery.file_chunk_size` got lost in  https://github.com/elastic/elasticsearch/pull/13840 - I found out while reviewing some settings and their defaults. the `512kb` default for this one caught my attention and I was surprised we are using such a rather big buffer to send across the network. From my perspective we should use an 8kb for writing most of the time which we now do by accident (we use `Streams#copy(InputStream in, OutputStream out)` which uses a 8k buffer. I wonder why we have these low level settings and if we:
1. bring it back / make it adjustable at all
2. drop more super low level settings in `RecoverySettings`

there are a couple of settings that are questinable:
- `indices.recovery.compress` - lets either compress or not but why make this adjustable?
- `indices.recovery.concurrent_streams` / `indices.recovery.concurrent_small_file_streams` why do we have this and why can't this come from some ThreadPool instead. I also wonder if the small / large differentiation is worth the trouble?
- `index.shard.recovery.translog_size` / `index.shard.recovery.translog_ops` I think we can drop both and just hide sending behind a stream and just keep on writing to the stream and flush when necessary (8kb) or just use 8kb as a fixed size?

I might be wrong with the 8kb since this is really an RPC rather than a stream but I wonder if we can find a good default we can just go with instead of having these expert settings. I would be ok with moving some of these settings to system properties to signal how expert they are but the amount of timeouts and things here is really not maintainable.
</description><key id="119794010">15161</key><summary>`indices.recovery.file_chunk_size` got lost in #13840</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>blocker</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T19:53:24Z</created><updated>2016-03-02T19:38:43Z</updated><resolved>2016-03-02T19:38:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-12-01T20:19:23Z" id="161083856">&gt; `indices.recovery.concurrent_streams` / `indices.recovery.concurrent_small_file_streams` why do we have this and why can't this come from some ThreadPool instead. I also wonder if the small / large differentiation is worth the trouble?

It was [originally added](https://github.com/elastic/elasticsearch/pull/3584) because large relocations can block things like the recovery of newly created indices, so you end up waiting a long time before the newly created index is available.
</comment><comment author="s1monw" created="2015-12-01T20:22:26Z" id="161084611">&gt; It was originally added because large relocations can block things like the recovery of newly created indices, so you end up waiting a long time before the newly created index is available.

for stuff like this I think we should just use the coordinating thread and do it sequentially instead of in-parallel. But thanks for the pointer @dakrone !!
</comment><comment author="jpountz" created="2015-12-02T08:48:02Z" id="161223017">I'm not familiar enough with the recovery process to comment but I'm all for having fewer low-level settings. If we don't fully trust some components, we can have an escape hatch using system properties like you suggested.
</comment><comment author="bleskes" created="2015-12-02T09:04:10Z" id="161227518">&gt; indices.recovery.compress 

+1 on removing ^^ never heard about a case where it was used.

&gt; indices.recovery.concurrent_streams / indices.recovery.concurrent_small_file_streams

What Lee said re that we need a solution to let new indices get to green quickly, even if a big file is being copied. Doesn't have to this solution as Simon said.

Also, these setting protect a node holding many primaries from being overloaded by outbound recoveries. I think this is simpler to implement on the master with a decision allocator (like we do for target relocation). The was an e-mail thread about this where @kimchy thought at the time that this will be complicated (the thread at the time was about the tension between max_bytes_per_sec and these settings).

&gt; index.shard.recovery.translog_size / index.shard.recovery.translog_ops I think we can drop both and just hide sending behind a stream and just keep on writing to the stream and flush when necessary (8kb) or just use 8kb as a fixed size?

We already read ops from the translog one by one, so writing to a blocking output stream will easy to implement here.

&gt; I would be ok with moving some of these settings to system properties to signal how expert they are

+1 to simplify and remove escape hatches which we haven't used for a while. Even better if we can simplify the mechanics to have one setting do more (like the 8k buffer which holds for many transmissions). One note about system properties - our current settings are dynamically updatable so in case of trouble you can tweak them without going down. system properties aren't. As these settings are relevant for all nodes of the cluster, having them updatable from code means they can be changed in one command rather than a full cluster restart.  To me this is a case by case basis judgment call. 

If we move to system properties, we need some kind of mechanism to report they were used and changed. Right now asking for the elasticsearch.yml file + cluster settings gives you a complete picture. In general we are moving towards using more and more cluster settings and less the yml file. I'm worried moving to system properties will go agains this.
</comment><comment author="s1monw" created="2015-12-02T09:10:55Z" id="161228722">@kimchy can you tell us where the `512kb` came from vs. `8k` buffer. 
</comment><comment author="kimchy" created="2015-12-03T19:32:28Z" id="161755836">- compress: I agree that we can remove this setting, I don't think someone really sets it. My suggestion is actually not to have it on when transferring segments (they are already compressed), but still do compression on translog entries as those tend to be highly compressed json documents.
- Regarding concurrent streams, I agree that if we can make better decision when allocating a shard, for example, prefer new indices to allocate almost ignoring deciders that throttle, will help. We will still need to somehow prioritize "the streams" of segments, though in that case, a single recovery thread would be enough. I do think that it is still good to have concurrent streams for beefy shard recovery as it will speed it up compared to single thread streaming data.
- To do blocking streaming, we would simulate it over evented networking, deciding when to "flush". We should have higher than 8k buffer to flush, as network buffers can transfer in one go much more, and the 8k one we do in Lucene for files doesn't apply in this case (copy to native direct buffer). I am +1 on removing the setting, since 512k is a good default and I doubt someone would want to change it. Note that higher flush buffer starts to hurt you when doing evented networking.
- Regarding translog size, I agree that we can remove this setting as well, and just flush based on bytw size, and use the same flush size we decide to use for segments here as well (512k).
</comment><comment author="s1monw" created="2015-12-04T09:15:49Z" id="161916319">&gt; Regarding concurrent streams, I agree that if we can make better decision when allocating a shard, for example, prefer new indices to allocate almost ignoring deciders that throttle, will help. We will still need to somehow prioritize "the streams" of segments, though in that case, a single recovery thread would be enough. I do think that it is still good to have concurrent streams for beefy shard recovery as it will speed it up compared to single thread streaming data.

@kimchy I think we should just use the recovery thread itself to send such small files. That way it's simplified and we can rely on throttling to not overload the node. The recovery thread is blocking anyway and sending small files should be very very quick anyhow.
</comment><comment author="kimchy" created="2015-12-10T17:20:44Z" id="163693288">@s1monw right, we might still want to then not throttle recoveries for freshly created indices? otherwise they might backlog?
</comment><comment author="s1monw" created="2015-12-10T17:21:36Z" id="163693669">&gt; @s1monw right, we might still want to then not throttle recoveries for freshly created indices? otherwise they might backlog?

I don't see that really but that has to be a sep change since all our tests will fail on that one I tried it
</comment><comment author="clintongormley" created="2016-02-14T18:27:09Z" id="183944984">@s1monw is this issue closed by https://github.com/elastic/elasticsearch/pull/15235 or is there more to do?
</comment><comment author="s1monw" created="2016-03-02T19:38:43Z" id="191391430">@clintongormley you are right I missed that... closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow deletion of mapping/type if no documents are present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15160</link><project id="" key="" /><description>I understand that deletion of a type is no longer allowed in 2.x. However, I have a scenario where it would be helpful. I have a dedicated index for percolators which contains mappings from the types which flow into daily indices. These types are not stored in the percolators index so there (presumably) is no way for them to leave corruption behind. If I change a type mapping and the percolator that corresponds to it I now have to delete the entire percolators index? This seems pretty undesirable. Why can't the corruption be avoided or why can't it simply allow the deletion if no documents are stored corresponding to the type being deleted? Either way this seems like a weird limitation and I would appreciate some guidance.
</description><key id="119791088">15160</key><summary>Allow deletion of mapping/type if no documents are present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rossbrower</reporter><labels><label>:Mapping</label><label>:Percolator</label></labels><created>2015-12-01T19:37:57Z</created><updated>2015-12-11T11:50:06Z</updated><resolved>2015-12-11T10:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-12-02T11:05:30Z" id="161262101">Interesting point.  I wonder if a better way to solve this would be to have the percolator fetch the mapping from the source index, instead of storing it locally. Not sure if that's possible.
</comment><comment author="rossbrower" created="2015-12-02T18:24:34Z" id="161389436">That would certainly work for my purposes as writing the mapping twice is a bit redundant. If you are able to percolate a document that lives in a different index it would be more intuitive if the mappings were fetched/cached from the source index. 
</comment><comment author="martijnvg" created="2015-12-11T10:47:07Z" id="163906884">The percolator queries are parsed based on that doc type mapping in the percolator index, so removing that mapping and would make the already added percolator queries invalid. So it is important that the doc type mapping can't be deleted or existing fields to be changed. 

The only solution here would be to reindex the all the percolator queries. Once the [reindex api](https://github.com/elastic/elasticsearch/issues/15201) is added this will be easier to manage.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split cluster state update tasks into roles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15159</link><project id="" key="" /><description>This commit backports commits c4a229819406deb4407d8401d698453d936186cf
and c2e50b010b35d17a4bd42ca629da7aa4b6445e14 from master to 2.x.
</description><key id="119785038">15159</key><summary>Split cluster state update tasks into roles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-12-01T19:05:40Z</created><updated>2015-12-07T16:41:05Z</updated><resolved>2015-12-07T16:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-12-01T19:13:16Z" id="161066199">This is the backport of #14899 and #15130 from master to 2.x.
</comment><comment author="bleskes" created="2015-12-03T13:14:49Z" id="161635419">LGTM. (still waiting to her if the mapping part needs a careful review).
</comment><comment author="bleskes" created="2015-12-03T14:06:37Z" id="161649623">LGTM+2  (including a more careful review of MetaDataMappingService)
</comment><comment author="jasontedor" created="2015-12-03T15:00:17Z" id="161665779">@bleskes and @jpountz Would you be able to review MetaDataMappingService.java one more time since I've rebased on 2.x to pick up [@jpountz's changes](https://github.com/elastic/elasticsearch/commit/535aa71d9da9778f5c86ac04d6e9a26d542ae22c) there before integrating this PR? It wasn't a clean rebase, and I want to make sure that we don't lose those changes.
</comment><comment author="jpountz" created="2015-12-07T16:25:35Z" id="162577097">Changes to MetaDataMappingService look good to me.
</comment><comment author="jasontedor" created="2015-12-07T16:31:48Z" id="162580724">Thanks @bleskes and @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make rename processor less error prone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15158</link><project id="" key="" /><description>Rename processor now checks whether the field to rename exists and throws exception if it doesn't. It also checks that the new field to rename to doesn't exist yet, and throws exception otherwise. Also we make sure that the rename operation is atomic, otherwise things may break between the remove and the set and we'd leave the document in an inconsistent state.

Note that the requirement for the new field name to not exist simplifies the usecase for e.g. { "rename" : { "list.1": "list.2"} } as such a rename wouldn't be accepted if list is actually a list given that either list.2 already exists or the index is out of bounds for the existing list. If one really wants to replace an existing field, that field needs to be removed first through remove processor and then rename can be used.
</description><key id="119778058">15158</key><summary>Make rename processor less error prone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label></labels><created>2015-12-01T18:25:51Z</created><updated>2015-12-01T19:06:06Z</updated><resolved>2015-12-01T19:05:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2015-12-01T18:49:44Z" id="161060438">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Add fromXContent method to HighlightBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/15157</link><project id="" key="" /><description>For the search refactoring the HighlightBuilder needs a way to create new instances by parsing xContent. For bwc this PR start by moving over and slightly modifying the parsing from HighlighterParseElement and keeps parsing for top level highlighter and field options separate. Also adding tests for roundtrip of random builder (rendering it to xContent and parsing it and making sure the original builder properties are preserved).
</description><key id="119766308">15157</key><summary> Add fromXContent method to HighlightBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Highlighting</label><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-12-01T17:22:06Z</created><updated>2015-12-08T18:39:01Z</updated><resolved>2015-12-08T18:39:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-12-01T17:24:45Z" id="161039536">There's some weirdness going on with the first two commits I see, the diff looks okay though. Will try to squash this somehow.
Edit: looks better now.
</comment><comment author="colings86" created="2015-12-07T11:14:27Z" id="162487889">@cbuescher This is looking good so far, I left a few comments
</comment><comment author="cbuescher" created="2015-12-07T16:16:28Z" id="162573319">@colings86 thanks for the review, I added commits that switch to using ParseFields in HighlightBuilder and added checks for unknown field names in the parser code (only in HighlightBuilder since the HighlightParseElement will be removed at some point in the future as far as I understand). Would be glad if you could take a second look.
</comment><comment author="cbuescher" created="2015-12-08T11:28:29Z" id="162855566">Hi @colings86, anything else to add here after ParseFields and additional exceptions?
</comment><comment author="colings86" created="2015-12-08T14:49:24Z" id="162904709">Left a couple of comments but otherwise LGTM
</comment><comment author="cbuescher" created="2015-12-08T15:01:59Z" id="162908172">Thanks for the review, added the checks for unexpected token while parsing, will merge in a bit.
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>