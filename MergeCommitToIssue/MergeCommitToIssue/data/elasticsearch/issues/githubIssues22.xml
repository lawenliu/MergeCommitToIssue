<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>null pointer exception in 19.8. same code path exists in master branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2547</link><project id="" key="" /><description>Upon restarting an elasticsearch server, I received a "failed to recover commit_point" message from Elasticsearch. The cause was a null pointer being passed into a concurrent hashmap. The null comes from PercolatorExecutor.parseQuery, which seems to always parse a query or raise an exception. If there is no "query" token, however, the parseQuery method appears to return null.

I patched code in PercolatorService.QueryiesLoaderCollector.collect to check the return value of parseQuery for null before inserting it into the map, and that seemed to fix it.

I'm not familiar enough with ES to know if that's actually the fix. Maybe parseQuery should raise an exception, or maybe the _percolator index needs better sanitization. But in 19.8, at least, it seems possible to get the _percolator index into a state that makes the data unrecoverable without edits to the ES source.

org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [_percolator][0] failed to recover commit_point [commit-19]/[45]
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:424)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:881)
        at java.util.concurrent.ConcurrentHashMap.putAll(ConcurrentHashMap.java:909)
        at org.elasticsearch.index.percolator.PercolatorExecutor.addQueries(PercolatorExecutor.java:259)
        at org.elasticsearch.index.percolator.PercolatorService.loadQueries(PercolatorService.java:128)
        at org.elasticsearch.index.percolator.PercolatorService.access$700(PercolatorService.java:52)
        at org.elasticsearch.index.percolator.PercolatorService$ShardLifecycleListener.afterIndexShardStarted(PercolatorService.java:220)
        at org.elasticsearch.indices.InternalIndicesLifecycle.afterIndexShardStarted(InternalIndicesLifecycle.java:86)
        at org.elasticsearch.index.shard.service.InternalIndexShard.start(InternalIndexShard.java:277)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recoverTranslog(BlobStoreIndexShardGateway.java:435)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:421)
</description><key id="9956632">2547</key><summary>null pointer exception in 19.8. same code path exists in master branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">joshbronson</reporter><labels /><created>2013-01-14T20:37:26Z</created><updated>2014-07-01T22:27:50Z</updated><resolved>2013-03-02T16:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-03T18:49:12Z" id="14352241">I also pushed this to 0.20
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>null pointer exception in 19.8. same code path exists in master branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2546</link><project id="" key="" /><description>Upon restarting an elasticsearch server, I received a "failed to recover commit_point" message from Elasticsearch. The cause was a null pointer being passed into a concurrent hashmap. The null comes from PercolatorExecutor.parseQuery, which seems to always parse a query or raise an exception. If there is no "query" token, however, the parseQuery method appears to return null.

I patched code in PercolatorService.QueryiesLoaderCollector.collect to check the return value of parseQuery for null before inserting it into the map, and that seemed to fix it. Here's the pull request: 

https://github.com/elasticsearch/elasticsearch/pull/2547

I'm not familiar enough with ES to know if that's actually the fix. Maybe parseQuery should raise an exception, or maybe the _percolator index needs better sanitization. But in 19.8, at least, it seems possible to get the _percolator index into a state that makes the data unrecoverable without edits to the ES source.

org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [_percolator][0] failed to recover commit_point [commit-19]/[45]
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:424)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:881)
        at java.util.concurrent.ConcurrentHashMap.putAll(ConcurrentHashMap.java:909)
        at org.elasticsearch.index.percolator.PercolatorExecutor.addQueries(PercolatorExecutor.java:259)
        at org.elasticsearch.index.percolator.PercolatorService.loadQueries(PercolatorService.java:128)
        at org.elasticsearch.index.percolator.PercolatorService.access$700(PercolatorService.java:52)
        at org.elasticsearch.index.percolator.PercolatorService$ShardLifecycleListener.afterIndexShardStarted(PercolatorService.java:220)
        at org.elasticsearch.indices.InternalIndicesLifecycle.afterIndexShardStarted(InternalIndicesLifecycle.java:86)
        at org.elasticsearch.index.shard.service.InternalIndexShard.start(InternalIndexShard.java:277)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recoverTranslog(BlobStoreIndexShardGateway.java:435)
        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:421)
</description><key id="9947594">2546</key><summary>null pointer exception in 19.8. same code path exists in master branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">joshbronson</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-01-14T16:24:12Z</created><updated>2013-03-03T18:49:36Z</updated><resolved>2013-01-14T20:39:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="joshbronson" created="2013-01-14T20:39:45Z" id="12238533">moving this to 2547. please excuse my poor github-foo.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"version":true only works if it appears before query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2545</link><project id="" key="" /><description>This returns a _version in the results:
{"from":0,"size":1,"version":true,"query":{"term":{"accounts.provider_uid":"xxxx","accounts.provider":"xxxx"}}}

This doesn't:

{"query":{"term":{"accounts.provider_uid":"xxxx","accounts.provider":"xxxx"}},"from":0,"size":1,"version":true}

The two queries are identical except for the order of the elements. I think the order of elements should not matter, right? 
</description><key id="9941210">2545</key><summary>"version":true only works if it appears before query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels /><created>2013-01-14T13:18:33Z</created><updated>2013-01-15T09:20:30Z</updated><resolved>2013-01-15T09:20:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-01-14T13:21:19Z" id="12218000">Your query is invalid - you need to define the relationship between your two term queries by wrapping them in a `bool` query.
</comment><comment author="jillesvangurp" created="2013-01-15T09:20:30Z" id="12258918">You are right, amazing it worked at all. Closing the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[crash] ES should reorder envelope corners</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2544</link><project id="" key="" /><description>It might happen that the coordinates for an envelope geo type arrive in top right bottom left. When that happens right now ES 0.20.2 the ES dies with oom. When you look at the hprof dump all you see is millions of org.elasticsearch.common.lucene.spatial.prefix.tree.GeohashPrefixTree$GhCell objects in my case that was about 6mio worth of them.

I see two solutions to this:
- be friendly and  correct the coordinates to be in proper order top left, bottom right
- return an error
</description><key id="9940887">2544</key><summary>[crash] ES should reorder envelope corners</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">mvrhov</reporter><labels><label>:Geo</label><label>bug</label><label>low hanging fruit</label><label>v1.4.3</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2013-01-14T13:03:11Z</created><updated>2014-12-30T17:58:51Z</updated><resolved>2014-12-30T17:58:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-02T15:58:37Z" id="14330212">hey, I think this has been fixed but can you provide a testcase for this to make sure I am not missing it?
</comment><comment author="mvrhov" created="2013-03-05T08:54:22Z" id="14429528">Hi, It has not been fixed.
e.g. ,`"coordinates":[[150.5981,-30.548016666667],[150.06046666666,-31.037683333334]]` will still break the 0.90b1
</comment><comment author="chilling" created="2013-06-10T09:29:03Z" id="19188763">Hi @mvrhov, can you provide an example how you use these coordinates. Do you use this in a filter or a query? I just tested it myself and I also do not receive any error response. So I'm going to fix this.
</comment><comment author="mvrhov" created="2013-06-10T09:51:10Z" id="19189689">@chilling: I haven't tested this since 0.90b1 come out. However the oom was happening when inserting a new record.
</comment><comment author="chilling" created="2013-06-10T10:35:09Z" id="19191292">Ok. I think currently this error will be ignored but I'm going to fixit. The solution will be returning an error rather than autocorrecting it.
</comment><comment author="missinglink" created="2014-12-28T19:25:41Z" id="68216690">ref: https://github.com/elasticsearch/elasticsearch/issues/9080
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>o.e.index.analysis.CustomAnalyzer doesn't handle properly char filters (lucene 4)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2543</link><project id="" key="" /><description>In the current Lucene4-based master, the class org.elasticsearch.index.analysis.CustomAnalyzer doesn't seem to handle char filters properly: it wraps provided Reader in the createComponents(...) method, while the Lucene 4 docs suggest to do it in the overridden initReader(...) method (see "Adding a CharFilter chain" in the bottom of https://builds.apache.org/job/Lucene-Artifacts-4.x/javadoc/core/org/apache/lucene/analysis/package-summary.html)

As a result, such an Analyzer is able to tokenize correctly only for the first time, and afterwards the output is not correct.

The fix is actually very simple: instead of 

```
@Override
protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
    Tokenizer tokenizer = tokenizerFactory.create(charFilterIfNeeded(reader));
    ...
}
```

do as Lucene 4 javadoc suggests:

```
@Override
protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
    Tokenizer tokenizer = tokenizerFactory.create(reader);
    ...
}

@Override
protected Reader initReader(String fieldName, Reader reader) {
    return charFilterIfNeeded(reader);
}
```
</description><key id="9939340">2543</key><summary>o.e.index.analysis.CustomAnalyzer doesn't handle properly char filters (lucene 4)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pgoncharik</reporter><labels><label>bug</label><label>v0.90.0.Beta1</label></labels><created>2013-01-14T11:55:03Z</created><updated>2013-01-14T18:04:45Z</updated><resolved>2013-01-14T18:04:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Issue with boost settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2542</link><project id="" key="" /><description>Hi, 
I think this is an issue. I posted in the user group but got no response. 
When the following query is created on an Index with lot of fields...

```
"bool" : {
    "must" : {
      "bool" : {
        "should" : [ {
          "bool" : {
            "must" : {
              "query_string" : {
                "query" : "\"microsoft\"",
                "fields" : [ "nameTokens" ],
                "analyzer" : "analyzerWithoutStop"
              }
            },
            "boost" : 5.0
          }
        }, {
          "bool" : {
            "should" : {
              "query_string" : {
                "query" : "\"insideview.com\"",
                "fields" : [ "companyDomain" ],
                "analyzer" : "analyzerWithoutStop"
              }
            },
            "boost" : 10.0
          }
        } ]
      }
    }
  }
}
```

On the "nameTokens" filed the boost parameter keeps increasing with the number of queries. Thats is
1. When the query is fired for the first time. The explanation i got for the two results, one matched the "nameToken" clause and the other matched the "companyDomain" clause:

1st result

```
3.1991732 = (MATCH) product of:
  6.3983464 = (MATCH) sum of:
    6.3983464 = (MATCH) weight(companyDomain:insideview.com^10.0 in 223), product of:
      0.93694496 = queryWeight(companyDomain:insideview.com^10.0), product of:
        10.0 = boost
        6.8289456 = idf(docFreq=2, maxDocs=1020)
        0.013720199 = queryNorm
      6.8289456 = (MATCH) fieldWeight(companyDomain:insideview.com in 223), product of:
        1.0 = tf(termFreq(companyDomain:insideview.com)=1)
        6.8289456 = idf(docFreq=2, maxDocs=1020)
        1.0 = fieldNorm(field=companyDomain, doc=223)
  0.5 = coord(1/2)
```

2nd result

```
0.89017844 = (MATCH) product of:
  1.7803569 = (MATCH) sum of:
    1.7803569 = (MATCH) weight(nameTokens:microsoft^5.0 in 209), product of:
      0.3494771 = queryWeight(nameTokens:microsoft^5.0), product of:
        5.0 = boost
        5.0943446 = idf(docFreq=16, maxDocs=1020)
        0.013720199 = queryNorm
      5.0943446 = (MATCH) fieldWeight(nameTokens:microsoft in 209), product of:
        1.0 = tf(termFreq(nameTokens:microsoft)=1)
        5.0943446 = idf(docFreq=16, maxDocs=1020)
        1.0 = fieldNorm(field=nameTokens, doc=209)
  0.5 = coord(1/2)
```

2nd time the same search result is fired:

```
2.5471714 = (MATCH) product of:
  5.0943427 = (MATCH) sum of:
    5.0943427 = (MATCH) weight(nameTokens:microsoft^15625.0 in 209), product of:
      0.99999964 = queryWeight(nameTokens:microsoft^15625.0), product of:
        15625.0 = boost
        5.0943446 = idf(docFreq=16, maxDocs=1020)
        1.2562947E-5 = queryNorm
      5.0943446 = (MATCH) fieldWeight(nameTokens:microsoft in 209), product of:
        1.0 = tf(termFreq(nameTokens:microsoft)=1)
        5.0943446 = idf(docFreq=16, maxDocs=1020)
        1.0 = fieldNorm(field=nameTokens, doc=209)
  0.5 = coord(1/2)
```

2nd result

```
 0.0029293338 = (MATCH) product of:
  0.0058586677 = (MATCH) sum of:
    0.0058586677 = (MATCH) weight(companyDomain:insideview.com^10.0 in 223), product of:
      8.5791684E-4 = queryWeight(companyDomain:insideview.com^10.0), product of:
        10.0 = boost
        6.8289456 = idf(docFreq=2, maxDocs=1020)
        1.2562947E-5 = queryNorm
      6.8289456 = (MATCH) fieldWeight(companyDomain:insideview.com in 223), product of:
        1.0 = tf(termFreq(companyDomain:insideview.com)=1)
        6.8289456 = idf(docFreq=2, maxDocs=1020)
        1.0 = fieldNorm(field=companyDomain, doc=223)
  0.5 = coord(1/2)
```

If you look at the boost parameter in the query explanation, it is increasing with every query of one clause. 
</description><key id="9910787">2542</key><summary>Issue with boost settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rohitletsbuzz</reporter><labels><label>bug</label></labels><created>2013-01-12T14:06:52Z</created><updated>2014-07-04T13:07:47Z</updated><resolved>2014-07-04T13:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Error upon opening the file elasticsearh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2541</link><project id="" key="" /><description>typing the commands:

macbookpro:elasticsearch-master cc$ bin/elasticsearch -f
bin/elasticsearch.in.sh: line 3: $ES_CLASSPATH:$ES_HOME/lib

I got this erroer:

/${project.build.finalName}.jar:$ES_HOME/lib/_:$ES_HOME/lib/sigar/_: bad substitution
You must set the ES_CLASSPATH var
macbookpro:elasticsearch-master cc$ 
</description><key id="9907725">2541</key><summary>Error upon opening the file elasticsearh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">craigcosmo</reporter><labels /><created>2013-01-12T08:08:43Z</created><updated>2013-01-12T17:07:12Z</updated><resolved>2013-01-12T17:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-01-12T11:01:57Z" id="12176602">Hey,

Sounds like you downloaded source code from github and tried to execute it directly?

Can you confirm that?

What are you trying to do?
- Build from sources?
- Play with elasticsearch 0.21.0.Beta1-SNAPSHOT? 
- Just want to play with Elasticsearch latest stable release?
</comment><comment author="craigcosmo" created="2013-01-12T11:52:22Z" id="12177052">yes I downloadd it from github and trying to execute it directly . I want to run elasticsearch locally to index stuff from my experiment app
</comment><comment author="dadoonet" created="2013-01-12T15:33:10Z" id="12179604">Here is the download link: http://www.elasticsearch.org/download/
For example, download the latest released version here: http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.20.2.zip

On Github, you just have downloaded sources.

Please close the issue as it is not an issue.

And have a look at this page: http://www.elasticsearch.org/help/
</comment><comment author="craigcosmo" created="2013-01-12T16:36:01Z" id="12180438">I downloaded from the link you recommended. still can't run. Check this demonstration video https://dl.dropbox.com/u/8032222/ES%20error.mp4
</comment><comment author="dadoonet" created="2013-01-12T16:38:14Z" id="12180465">Your link is invalid. Could you please copy and paste your logs?
</comment><comment author="craigcosmo" created="2013-01-12T16:47:36Z" id="12180588">Sorry it's being upload. It should be available now. Anyway the logs is as follow:

Last login: Sat Jan 12 22:30:14 on ttys000
macbookpro:~ craigcosmo$ /Users/craigcosmo/Desktop/elasticsearch-0.20.2/bin/elasticsearch ; exit;
logout

[Process completed]
</comment><comment author="dadoonet" created="2013-01-12T16:53:08Z" id="12180646">So why do you think elasticsearch is not launched?

Try `ps -ef | grep elasticsearch` and look if the process is runnning.
If not, have a look at logs dir and look inside log files.

BTW, when you start to play with elasticsearch, I recommand to launch it with `-f`option.

`bin/elasticsearch -f`
</comment><comment author="craigcosmo" created="2013-01-12T16:59:11Z" id="12180710">bin/elasticsearch -f 

That helps alot. it works now. 
</comment><comment author="dadoonet" created="2013-01-12T17:01:29Z" id="12180736">Thanks for the feedback. Please close the issue and ask for help next time in the mailing list.
</comment><comment author="craigcosmo" created="2013-01-12T17:03:35Z" id="12180762">can you give the link to mailing list?
</comment><comment author="dadoonet" created="2013-01-12T17:07:05Z" id="12180803">&gt; And have a look at this page: http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Open up HTTP headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2540</link><project id="" key="" /><description>For example it would be nice to have the number of hits sent back in the response HTTP headers for relevant APIs. It would allow to define easily various cache policies (like different TTLs for no results...) without parsing the response body when requesting ES through a cache layer (like Varnish in my precise use case)

Original post in the ML https://groups.google.com/d/topic/elasticsearch/cJVMbQun8Oo/discussion
</description><key id="9898169">2540</key><summary>Open up HTTP headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-01-11T23:11:35Z</created><updated>2013-05-10T16:03:40Z</updated><resolved>2013-05-10T16:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-02-20T15:16:05Z" id="13836686">This would also allow to implement #2654 as a http redirect using a "Location:" header instead of the html based rewrite.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make script cache configurable and bounded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2539</link><project id="" key="" /><description>Unbounded script cache may lead to running out of memory if a large number of scripts is generated on the client based on user input, time, or any other changing variables with unbounded range of values. To reproduce, start elasticsearch with 32M heap and run the following script for a few minutes: https://gist.github.com/7163ea88b52018cdb66c.

The script cache parameters should be configurable using the following settings:
- `script.cache.size` - allows to specify the maximum number of compiled scripts to be stored in the cache. 0 means no cache, -1 - unbounded cache. The default value is 100.
- `script.cache.expire` - allows to specify the length of time after a compiled script is last accessed that it should be automatically removed. The default value is no expiration. 
</description><key id="9897698">2539</key><summary>Make script cache configurable and bounded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2013-01-11T22:52:53Z</created><updated>2013-01-14T12:12:38Z</updated><resolved>2013-01-14T12:12:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Disable startup by default after package installation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2538</link><project id="" key="" /><description>Hi there,

Happy new year first.

This patch add envar used by init.d script to prevent from starting by default.
Often before you have to work on config files to fit your needs (manually or with provisioning tool like chef or puppet) and then only start the service.
This can be used also for tier programs that depends on ES libs but does not need for the service running on the same host.

Cheers,
</description><key id="9884553">2538</key><summary>Disable startup by default after package installation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dhardy92</reporter><labels /><created>2013-01-11T16:04:13Z</created><updated>2014-07-16T21:54:11Z</updated><resolved>2013-09-16T07:54:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dhardy92" created="2013-01-11T22:09:16Z" id="12165969">BTW this should close #1697     
</comment><comment author="spinscale" created="2013-09-16T07:54:03Z" id="24493693">This has been implemented in #3685 - for the RPM and debian package
</comment><comment author="dhardy92" created="2013-09-16T09:06:54Z" id="24496989">Hi,
Thank you for working on it.
BUT! :)
I am not really happy with this implementation as problem occurs mostly only on 1rst install of ES.
Config on upgrade  is already managed so I don't really care (upgrade on debian is not supposed to override config file automatically ...)
Deploying a new node with puppet was previously :
- install ES package (unwanted service start occurs)
- configure it (change settings with puppet is easier with package installed for directories and user created)
   -&gt; config change =&gt; restart the service (but default start may have created unwanted things like log file with default clustername, unwanted data dir where diskspace is limited, joining some "ghost" cluster of default-config-running-ES-nodes, ...)

And 1rst install is clearly off scope because of test on "$2" in postinst (at least for deb)
Problem is not solved.

The new one does not break anything as default is the old behavior.
I'd rather prefer a real break :) that may be simpler only by saying what occurs in the console when attempt of starting the service with default config.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent &amp; child queries can fail if a segment doesn't have documents with the targeted type or associated parent type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2537</link><project id="" key="" /><description>Originates from #2536 
</description><key id="9882537">2537</key><summary>Parent &amp; child queries can fail if a segment doesn't have documents with the targeted type or associated parent type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2013-01-11T15:05:03Z</created><updated>2013-01-11T15:06:20Z</updated><resolved>2013-01-11T15:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NullPointerException during parent/child query (ES 0.20.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2536</link><project id="" key="" /><description>1. create mapping
   curl -XPUT localhost:9200/index1/
   curl localhost:9200/index1/Parent/_mapping?pretty -d '
   {
   "Parent" : {
     "_all" : {
       "enabled" : false
     },
     "_source" : {
       "compress" : true
     },
     "properties" : {
       "_tenantid" : {
         "type" : "long",
         "store" : "yes"
       },
       "age" : {
         "type" : "long"
       },
       "name" : {
         "type" : "string"
       },
       "sex" : {
         "type" : "string"
       }
     }
   }
   }
   '
   curl localhost:9200/index1/Child/_mapping?pretty -d '
   {
   "Child" : {
     "_all" : {
       "enabled" : false
     },
     "_parent" : {
       "type" : "Parent"
     },
     "_routing" : {
       "required" : true
     },
     "_source" : {
       "compress" : true
     },
     "properties" : {
       "_tenantid" : {
         "type" : "long",
         "store" : "yes"
       },
       "age" : {
         "type" : "long"
       },
       "name" : {
         "type" : "string"
       },
       "shangshi" : {
         "type" : "boolean"
       }
     }
   }
   }
   '
2. Query

curl localhost:9200/index1/Parent/_search?pretty -d '
{ "sort" : [{ "age" : {"order" : "desc"} }],"query": { "filtered": { "query": { "has_child": { "type": "Child", "query": { "query_string": { "query": "+_tenantid:100002 +(-age:20)"} } } }, 
"filter": { "fquery": { "query": { "query_string": { "query": "+_tenantid:100002 +(name:dog  OR age:[10 TO 2147483647])" } }, "_cache": true } } } } }
'

OK, no result found, of course
1. Index one doc in an irrelevant type

curl localhost:9200/index1/Type1/key0 -d '
{"_tenantid": 100002, "name": "lilei", "age": -2, "sex": "f", "marry": false, "weight": 2.3, "height": 2.7E+23, "grade": 123, "bill": -120, "fee": 245, "money": 3453, "number": 65333, 
"phone": 4200000000, "small": 123.123456789123456789, "birthday": 20120306000000}
'
1. Exactly the same query again

curl localhost:9200/index1/Parent/_search?pretty -d '
{ "sort" : [{ "age" : {"order" : "desc"} }],"query": { "filtered": { "query": { "has_child": { "type": "Child", "query": { "query_string": { "query": "+_tenantid:100002 +(-age:20)"} } } }, 
"filter": { "fquery": { "query": { "query_string": { "query": "+_tenantid:100002 +(name:dog  OR age:[10 TO 2147483647])" } }, "_cache": true } } } } }
'

Now, NullPointerException!

"reason" : "QueryPhaseExecutionException[[index1][2]: query[filtered(filtered(ConstantScore(child_filter[Child/Parent](+_tenantid:[100002 TO 100002] +%28-age:[20 TO 20] +ConstantScore%28NotDeleted%28*:*%29%29%29)))-&gt;cache(QueryWrapperFilter(+_tenantid:[100002 TO 100002] +(name:dog age:[10 TO 2147483647]))))-&gt;cache(_type:Parent)],from[0],size[10],sort[&lt;custom:\"age\": org.elasticsearch.index.field.data.longs.LongFieldDataType$1@1663108&gt;!]: Query Failed [Failed to execute child query [+_tenantid:[100002 TO 100002] +(-age:[20 TO 20] +ConstantScore(NotDeleted(_:_)))]]]; nested: NullPointerException; "
</description><key id="9871896">2536</key><summary>NullPointerException during parent/child query (ES 0.20.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mashudong</reporter><labels><label>bug</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2013-01-11T07:26:47Z</created><updated>2013-01-18T08:49:47Z</updated><resolved>2013-01-18T08:49:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-01-11T14:49:11Z" id="12146965">Thanks for reporting this issue! I've been able to reproduce this issue on both 0.20.x and master. This issue also occurs with the `has_child` filter and  `has_parent` query and filter.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support path type on property level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2535</link><project id="" key="" /><description /><key id="9858823">2535</key><summary>Support path type on property level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-01-10T20:39:36Z</created><updated>2014-07-08T18:55:57Z</updated><resolved>2014-07-08T18:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:55:57Z" id="48384145">No longer relevant because of `copy_to`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for REST get ALL templates. Fix #2532</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2534</link><project id="" key="" /><description>Heya,

Here is a simple way to support a GET /_template to show all templates:

`curl localhost:9200/_template/`

will return all existing templates
</description><key id="9849522">2534</key><summary>Support for REST get ALL templates. Fix #2532</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-01-10T16:30:51Z</created><updated>2014-07-11T09:19:39Z</updated><resolved>2013-08-11T19:53:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-11T19:53:03Z" id="22463943">Closing this one I rebased it on top of 0.90: See https://github.com/dadoonet/elasticsearch/commit/9c1ad2b81f240e72a090f143811c193824367cb7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting percolator (0.20)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2533</link><project id="" key="" /><description>Resending PR #2387, this time against branch 0.20, merged with recent changes to the highlighter there. Full description here: https://github.com/elasticsearch/elasticsearch/pull/2387

I see 4 failing tests, but they also fail for the original 0.20 branch. Just sayin'.

Hopefully this is going to be merged soon...
</description><key id="9846471">2533</key><summary>Highlighting percolator (0.20)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-01-10T15:13:37Z</created><updated>2014-06-27T01:56:24Z</updated><resolved>2013-04-04T10:08:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-04-04T10:08:01Z" id="15889191">This became too stale, see #2586 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>enable GET /_template to show all templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2532</link><project id="" key="" /><description>/_template shows:
No handler found for uri [/_template] and method [GET]

It would make sense to list the templates as they are listed in the /_cluster/state call.

See also:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wnGOnT-JTQo
</description><key id="9845146">2532</key><summary>enable GET /_template to show all templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">q42jaap</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-01-10T14:38:22Z</created><updated>2017-03-29T05:23:36Z</updated><resolved>2013-09-13T13:10:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="trycoon" created="2015-10-01T06:56:58Z" id="144638149">This is only for listing "index-templates", right? But I want to list existing search-templates: https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-template.html
</comment><comment author="dadoonet" created="2015-10-01T08:15:52Z" id="144652345">Yes. You should ask this type of question on discuss.elastic.co.
</comment><comment author="Bklara" created="2017-03-27T10:45:52Z" id="289418202">Hi. Am i right, there is no solution for search templates yet?</comment><comment author="trycoon" created="2017-03-29T05:23:36Z" id="289985842">No API-support, not that I know of. But since search-templates are a ordinary index just with the name ".script" you could use the normal commands that you use for listing indices.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator responses become inconsistent with cluster after forceful reboot of a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2531</link><project id="" key="" /><description>We have seen this issue in a production environment (Running Elastic Search 0.19.12) on more than one occasion - the most recent occasion being after one of the servers in the cluster was unexpectedly rebooted by Amazon.

We have been able to reproduce this locally using multiple Vagrant boxes - although not every time (2/3 attempts will reproduce this)

This bug seems very similar / the same as https://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/percolator/elasticsearch/VZjslNupBNY/Q3VFfXDqt6sJ
## Steps to reproduce:
- Have a cluster of Elastic Search servers
- Index some Percolator queries
- Test that each node returns the same response to a percolation request
- Forcefully reboot one of the servers (e.g. sudo reboot -f)
- Wait until the rebooted node rejoins the cluster
- Test each node returns the same response
## Expected Result:
- Every node returns the same (correct) response
## Actual Result:
- The restarted node returns no matched percolation queries, despite the cluster being in the healthy (green) state and the expected number of docs showing in the _percolator index.
- Sometimes the correct result is returned by the restarted node for a short time, and _then_, after a few seconds, starts returning no matches.
- Restarting Elastic Search on the node seems to fix this.
- Nothing unusual appears in the log files.
</description><key id="9844400">2531</key><summary>Percolator responses become inconsistent with cluster after forceful reboot of a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">36degrees</reporter><labels /><created>2013-01-10T14:14:51Z</created><updated>2014-07-08T18:55:34Z</updated><resolved>2014-07-08T18:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="36degrees" created="2013-01-28T11:29:03Z" id="12777842">Any idea what might be causing this? /cc @kimchy
</comment><comment author="imotov" created="2013-02-01T13:13:25Z" id="12993488">That would be really nice to figure out but unfortunately I cannot reproduce it. If it's still reproducible in your setup, could you force the cluster into this state again and run this command: `curl "localhost:9200/_cluster/state?pretty=true"` on _any_ node as well as this command `curl "localhost:9200/_cluster/state?pretty=true"` on the node that you have rebooted as well as on some other node that stayed in the cluster and post the results here or send them to me directly? 
</comment><comment author="36degrees" created="2013-02-04T16:48:09Z" id="13086081">@imotov Thanks for looking into it. I've managed to reproduce this on 0.20.4 as well now. Did you mean to post the same URL in the two commands above? It seems as though I should be looking at two different things from the way you word it&#8230;

What's the best way to send the output?
</comment><comment author="imotov" created="2013-02-04T17:31:13Z" id="13088431">Sorry, I meant `curl "localhost:9200/_cluster/state?pretty=true&amp;local=true"` on the node that was rebooted and the same command on a node that that wasn't. Also, did you create a percolator for a new index right before rebooting the server by any chance?
</comment><comment author="36degrees" created="2013-02-04T17:36:29Z" id="13088678">Nope, it actually took a few reboots before it failed this time. The percolator had probably been in existence for a good 15-20 minutes.

I've run that command against a 'good' node and the 'bad' node. The files are the same (by md5 hash) and also match the response without `&amp;local=true`. Does that tell you everything you need to know from that or is it still worth taking a look at the files themselves?
</comment><comment author="imotov" created="2013-02-04T17:55:55Z" id="13089611">That still would be useful to take a look at one of them (since they are the same). I would like to take a look at how _percolator shards are allocated when it fails.
</comment><comment author="36degrees" created="2013-02-05T09:47:03Z" id="13121811">Hi Igor,

I've stuck the result on https://gist.github.com/36degrees/5fd574580f54911e04ed.

Ollie
</comment><comment author="36degrees" created="2013-02-13T14:41:33Z" id="13496547">@imotov Have you been able to get anything from that state info at all?
</comment><comment author="imotov" created="2013-02-13T14:47:02Z" id="13496782">No, state looks fine. 
</comment><comment author="36degrees" created="2013-02-13T14:56:37Z" id="13497292">Ok. Any idea where we should go next with this - anything else obvious to look at? We're still keen to get to the bottom of it. 
</comment><comment author="imotov" created="2013-02-13T16:20:58Z" id="13502451">I can think of a few things that can help us investigating the issue:
1. If you can reproduce it again run `curl "http://localhost:9200/_percolator/_count?preference=_local"` on each node to make sure that they all have the same number of documents. 
2. Restart all 3 nodes with `TRACE` log level for `gateway`, `index.gateway`, `indices.recovery`, `discovery` , `index.percolator` and  `cluster.routing` and `DEBUG` level for `cluster.service`, and post log files somewhere. 
3. I just tried again reproducing the issue using your descriptions in a VM, everything works fine. If you could come up with a way that would allow us to reliably reproduce the issue, that would be really help.
</comment><comment author="olimcc" created="2013-05-30T18:40:42Z" id="18699529">I believe my issue is a duplicate: https://github.com/elasticsearch/elasticsearch/issues/3121

I've noted cases where this happens (I can't consistently reproduce however) - I'm happy to help debug as needed, and would love to see this resolved.
</comment><comment author="36degrees" created="2013-05-31T08:23:01Z" id="18731602">Good to hear someone else is seeing this problem - been thinking that we're going mad here. I get the impression that not many people use percolation! Not been able to reproduce it reliably enough to provide much more information, yet.
</comment><comment author="ajhalani" created="2013-07-23T19:50:12Z" id="21440826">Facing similar issue with 0.90.1. We added 2 new nodes to  cluster, and request hitting one of the new instance don't match alerts. The cluster status is GREEN. 
For temporary resolution, I brought down the bad node, deleted _percolator index directory and brought it back up again. The alerts resynced and issues seems to be fixed. Not sure if this is enough for a permanent solution.
</comment><comment author="ajhalani" created="2013-08-20T18:00:20Z" id="22964561">After more research, I was able to narrow down the symptoms further. When a new node joins cluster, there are log traces like below - 

[2013-08-07 15:51:25,899][DEBUG][indices.cluster          ] [**node name**] 
[**index 1 name**] creating index
[2013-08-07 15:51:29,036][DEBUG][indices.cluster          ] [**node name**] 
[**_percolator**] creating index
[2013-08-07 15:51:29,366][DEBUG][indices.cluster          ] [**node name**] 
[**index 2 name**] creating index

Any index that is created after _percolator, alert matching doesn't work for it. The way to get it working again is to re-insert the alerts in percolator and then it starts working again. This issue happens happens somewhat  frequently with restarting a node in 4 node cluster (3 replicas) but also seen once with a 2 node cluster(1 replica). 
Also happens less frequently with less data (30-40 GB primary index size) vs a bigger data(~200 GB primary index size)
</comment><comment author="ascruggs" created="2014-01-20T17:46:43Z" id="32781205">Is there any progress on this ticket?
</comment><comment author="martijnvg" created="2014-01-21T12:41:44Z" id="32870573">@ascruggs This issue doesn't occur in the 1.0 percolator. Perhaps a better workaround for 0.90.x this is to close and open the the _percolator index instead of reindexing percolator queries. 
</comment><comment author="clintongormley" created="2014-07-08T18:55:34Z" id="48384083">Old percolators have been replaced
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update request with upsert may fail.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2530</link><project id="" key="" /><description>When executing multiple update requests concurrently with an upsert and using the same id for those update request may thrown a document already exists exception. Instead what would be expected is that the upsert is ignored and the normal update occurs.

Test case (sometimes fails):
http://pastie.org/private/vq5iqixhz17h8m0x6cplbg
</description><key id="9842752">2530</key><summary>Update request with upsert may fail.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2013-01-10T13:16:29Z</created><updated>2013-01-10T13:25:52Z</updated><resolved>2013-01-10T13:25:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow a parse through label on range facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2529</link><project id="" key="" /><description>If you define date facets like this.

``` json
  "facets" : {
    "expiry-date" : {
      "range" : {
        "field" : "expiry-date",
        "ranges" : [ {
          "to" : "2013-01-09T22:26:09.293+01:00"
        }, {
          "from" : "2013-01-09T22:26:09.293+01:00",
          "to" : "2013-01-10T22:26:09.293+01:00"
        }, {
          "from" : "2013-02-09T22:26:09.293+01:00"
        } ]
      }
    }
```

You will get results like this

``` json
"facets": {
  "expiry-date": {
    "_type": "range",
    "ranges": [
      {
        "to": 1357766769293,
        "to_str": "2013-01-09T22:26:09.293+01:00",
        "count": 1252,
        "min": 1357698130000,
        "max": 1357698730000,
        "total_count": 1252,
        "total": 1699838444540000,
        "mean": 1357698438130.9905,

      },
      {
        "from": 1357766769293,
        "from_str": "2013-01-09T22:26:09.293+01:00",
        "to": 1357853169293,
        "to_str": "2013-01-10T22:26:09.293+01:00",
        "count": 0,
        "total_count": 0,
        "total": 0,
        "mean": 0,

      },
      {
        "from": 1360445169293,
        "from_str": "2013-02-09T22:26:09.293+01:00",
        "count": 0,
        "total_count": 0,
        "total": 0,
        "mean": 0,

      }
    ]
  }
}
```

Then the java api will sometimes shuffle the results. Which makes it hard to know what range is what when you get the result, unless you compare the from and to dates or numbers.

If you introduce a pass through `"label":"my-range"` for each range, it would be easier to map the results to the datastructure (perhaos a GUI) that need the range numbers.

This would make the request and response look like this.

Request:

``` json
  "facets" : {
    "expiry-date" : {
      "range" : {
        "field" : "expiry-date",
        "ranges" : [ {
          "label" : "up-to-09th",
          "to" : "2013-01-09T22:26:09.293+01:00"
        }, {
          "label" : "from-09th-to-10th",
          "from" : "2013-01-09T22:26:09.293+01:00",
          "to" : "2013-01-10T22:26:09.293+01:00"
        }, {
          "label" : "10th-and-beyond",
          "from" : "2013-02-09T22:26:09.293+01:00"
        } ]
      }
    }
```

Response:

``` json
"facets": {
  "expiry-date": {
    "_type": "range",
    "ranges": [
      {
        "label" : "up-to-09th",
        "to": 1357766769293,
        "to_str": "2013-01-09T22:26:09.293+01:00",
        "count": 1252,
        "min": 1357698130000,
        "max": 1357698730000,
        "total_count": 1252,
        "total": 1699838444540000,
        "mean": 1357698438130.9905,

      },
      {
        "label" : "from-09th-to-10th",
        "from": 1357766769293,
        "from_str": "2013-01-09T22:26:09.293+01:00",
        "to": 1357853169293,
        "to_str": "2013-01-10T22:26:09.293+01:00",
        "count": 0,
        "total_count": 0,
        "total": 0,
        "mean": 0,

      },
      {
        "label" : "10th-and-beyond"
        "from": 1360445169293,
        "from_str": "2013-02-09T22:26:09.293+01:00",
        "count": 0,
        "total_count": 0,
        "total": 0,
        "mean": 0,

      }
    ]
  }
}
```
</description><key id="9823353">2529</key><summary>Allow a parse through label on range facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JesperTerkelsen</reporter><labels /><created>2013-01-09T22:14:55Z</created><updated>2013-09-12T12:56:22Z</updated><resolved>2013-09-12T12:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-10T18:20:47Z" id="12110856">Where does it shuffle the results? The range entries should return in the same order as the ones provided in the facet request...

Btw, another way to do this is to use the `filter` facet, with a range filter for each facet, you get  count for each one, and the "labeling" is done by the name of the facet.
</comment><comment author="JesperTerkelsen" created="2013-01-10T22:55:30Z" id="12123259">I am not 100% sure about the reordering, i got the -Infinity and Infinity ones shifted one position into the list from each side. I will create a testcase to verify this outside my application. Please allow me a few workdays to create that. 

Thanx for the tip about the filter facet, i will try to use that as well.
</comment><comment author="kimchy" created="2013-01-10T22:59:42Z" id="12123437">@JesperTerkelsen thanks up front for the testcase!, once you have it, we will definitely fix something if it doesn't work as expected.
</comment><comment author="spinscale" created="2013-06-26T16:18:10Z" id="20060441">@JesperTerkelsen does this behaviour still happen with current versions of elasticsearch? Any chance for the test case still then? :-)
</comment><comment author="javanna" created="2013-09-12T12:56:21Z" id="24316108">Closing for lack of feedback. The returned facet entries are not reordered by the java api, but returned in the same order they were requested. Adding labels will be considered in the new aggregation module though (issue #3300), there are already a few comments about that there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node doesn't fail gracefully on OOM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2528</link><project id="" key="" /><description>I think that when an node hit OOM it should at least be ejected from the cluster. I am running an index without any replicas, the node stays in the cluster but is in "zombie" state. Consequence; response time is abysmal (~800 secs) because the node that contains the shard doesn't answer the query but is still on the cluster. Ejecting the node would at least allow to maintain search active. 

Also there is many more OOM in the following minutes by the fact that the node is not ejected and still receive queries.

Even worse, I have to restart the whole cluster to get it working again as you will see in the log, restarting only the node will not help, because it wasn't able to rejoin the cluster.

Logs

First OOM

```
[2013-01-09 14:38:51,640][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [location] caused out of memory failure
java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.index.field.data.support.FieldDataLoader.load(FieldDataLoader.java:45)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldData.load(GeoPointFieldData.java:168)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:55)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:34)
        at org.elasticsearch.index.field.data.FieldData.load(FieldData.java:111)
        at org.elasticsearch.index.cache.field.data.support.AbstractConcurrentMapFieldDataCache.cache(AbstractConcurrentMapFieldDataCache.java:130)
        at org.elasticsearch.index.search.geo.GeoDistanceDataComparator.setNextReader(GeoDistanceDataComparator.java:131)
        at org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:95)
        at org.apache.lucene.search.TimeLimitingCollector.setNextReader(TimeLimitingCollector.java:159)
        at org.elasticsearch.common.lucene.MultiCollector.setNextReader(MultiCollector.java:65)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:576)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:190)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:149)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:487)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:400)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:176)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:242)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
```

Following OOM

```
[2013-01-09 14:47:01,502][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [type] caused out of memory failure
java.lang.OutOfMemoryError: Java heap space
[2013-01-09 14:47:11,775][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [location] caused out of memory failure
java.lang.OutOfMemoryError: Java heap space
[2013-01-09 14:47:14,307][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [type] caused out of memory failure
java.lang.OutOfMemoryError: Java heap space
[2013-01-09 14:47:14,319][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [location] caused out of memory failure
java.lang.OutOfMemoryError: Java heap space
[2013-01-09 14:47:19,537][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [location] caused out of memory failure
java.lang.OutOfMemoryError: Java heap space
[2013-01-09 14:47:26,766][WARN ][index.cache.field.data.resident] [es37b] [index] loading field [location] caused out of memory failure
```

After restart

```
java.lang.OutOfMemoryError: Java heap space
[2013-01-09 14:51:14,673][INFO ][node                     ] [es37b] {0.20.1}[26945]: stopping ...
[2013-01-09 14:51:36,310][INFO ][node                     ] [es37b] {0.20.1}[28200]: initializing ...
[2013-01-09 14:51:36,315][INFO ][plugins                  ] [es37b] loaded [], sites []
[2013-01-09 14:51:38,440][INFO ][node                     ] [es37b] {0.20.1}[28200]: initialized
[2013-01-09 14:51:38,440][INFO ][node                     ] [es37b] {0.20.1}[28200]: starting ...
[2013-01-09 14:51:38,525][INFO ][transport                ] [es37b] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.1.16.57:9300]}
[2013-01-09 14:51:42,438][WARN ][discovery.zen.ping.unicast] [es37b] failed to send ping to [[#zen_unicast_15#][inet[es23b/10.1.16.103:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es23b/10.1.16.103:9300]][discovery/zen/unicast] request_id [12] timed out after [3750ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:342)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
```
</description><key id="9819834">2528</key><summary>Node doesn't fail gracefully on OOM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgagnon1</reporter><labels /><created>2013-01-09T20:29:55Z</created><updated>2014-07-08T18:55:06Z</updated><resolved>2014-07-08T18:55:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:55:06Z" id="48384022">OOMs are difficult to handle gracefully. Instead we've added features like the circuit breaker to try to avoid the altogether.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>An acknowledged CreateIndexRequest followed by GetRequest sometimes fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2527</link><project id="" key="" /><description>See the issue on the forum below.
What I would like to suggest is to add a parameter to the CreateIndexRequest that waits for the shards to become available, or maybe wait for the cluster status to turn yellow.  
Like the refresh boolean that can be passed in with an IndexRequest. This way I wouldn't have to work around this behaviour with a separate prepareHealth().setWaitForYellowStatus()

See also
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/uz55XJO9LP4
</description><key id="9769626">2527</key><summary>An acknowledged CreateIndexRequest followed by GetRequest sometimes fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q42jaap</reporter><labels /><created>2013-01-08T14:29:55Z</created><updated>2014-07-08T18:54:24Z</updated><resolved>2014-07-08T18:54:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="parisholley" created="2013-07-11T05:29:35Z" id="20791161">second this, makes automated tests slower as I have to use arbitrary waits..
</comment><comment author="parisholley" created="2013-07-11T06:03:44Z" id="20791995">take that back, turns out health status checks are now available..i was able to fix by making a request:

_cluster/health/MYINDEXNAME?wait_for_status=yellow
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse error for simple match_all query depending on order of properties in map</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2526</link><project id="" key="" /><description>Here's a small shell script that showcases the error:

```
indexurl=http://localhost:9200/bugtestindex

echo "Resetting index"
curl -XDELETE $indexurl
echo
echo "Creating a document";
curl -XPOST $indexurl/myindex/123abc -d '{"foo":"foo"}'
echo
echo
echo "*** Result of query A"
curl -d '{"query":{"match_all":{},"size":1}}' $indexurl/myindex/_search
echo
echo "*** Result of query B"
curl -d '{"query":{"size":1,"match_all":{}}}' $indexurl/myindex/_search
echo
echo "Resetting index"
curl -XDELETE $indexurl
```

The output of the script is as follows:

```
Resetting index
{"error":"IndexMissingException[[bugtestindex] missing]","status":404}
Creating a document
{"ok":true,"_index":"bugtestindex","_type":"myindex","_id":"123abc","_version":1}

*** Result of query A
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
*** Result of query B
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[IJfl5R9CSveC7dd0EX7RZg][bugtestindex][3]: SearchParseException[[bugtestindex][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"size\":1,\"match_all\":{}}}]]]; nested: QueryParsingException[[bugtestindex] [_na] query malformed, no field after start_object]; }{[IJfl5R9CSveC7dd0EX7RZg][bugtestindex][2]: SearchParseException[[bugtestindex][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"size\":1,\"match_all\":{}}}]]]; nested: QueryParsingException[[bugtestindex] [_na] query malformed, no field after start_object]; }]","status":500}
Resetting index
```

The error can be seen on the 2nd to last line in the output of the script.

The order of the properties in the map should not matter, as it is very hard to control this. In many languages, maps are unsorted, making iteration order unspecified when iterating for JSON serialization. This is also not a documented feature, so I think it's safe to classify this as a bug.
</description><key id="9725526">2526</key><summary>Parse error for simple match_all query depending on order of properties in map</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">augustl</reporter><labels /><created>2013-01-07T05:31:46Z</created><updated>2013-10-30T10:06:58Z</updated><resolved>2013-10-30T10:06:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-01-07T12:57:06Z" id="11950582">Just want to mention that both queries are malformed (the size parameter should be one level higher, on the same level as the query element), but in the first case the query still executes regardless invalid syntax.
</comment><comment author="spinscale" created="2013-10-30T10:06:58Z" id="27377144">Closing as this is not a bug, but a malformed query - works if size is a top level parameter (regardless of the order).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field are not being serialized as arrays when it contain only one item and requesting specific fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2525</link><project id="" key="" /><description>Hi,

I have an issue that values are not being serialized as arrays when requesting specific fields and the array contain only one item.
I have a field named 'advancedFeatures' and it contains an array of integers.
When I'm sending the following request (no specific fields):

{
  "from": 5,
  "size": 6,
  "sort": {
    "iD": {
      "order": "desc"
    }
  },
  "filter": {
    "bool": {
      "must": [
        {
          "term": {
            "accountMapping": "21785"
          }
        },
        {
          "term": {
            "archived": "0"
          }
        },
        {
          "numeric_range": {
            "status": {
              "from": 8,
              "to": 10
            }
          }
        }
      ]
    }
  }
}

The 'advancedFeatures' field in the response is being serialized as array:

_index: ad,
_type: ad,
_id: 1129339,
_score: null,
_source: {
iD: 1129339,
advancedFeatures: [4],
.
.
.
}

When I'm requesting only the 'advancedFeatures' field:

{
  "from": 5,
  "size": 6,
  "sort": {
    "iD": {
      "order": "desc"
    }
  },
  "filter": {
    "bool": {
      "must": [
        {
          "term": {
            "accountMapping": "21785"
          }
        },
        {
          "term": {
            "archived": "0"
          }
        },
        {
          "numeric_range": {
            "status": {
              "from": 8,
              "to": 10
            }
          }
        }
      ]
    }
  },
  "fields": [
    "advancedFeatures"
  ]
}

The response is looks like this (for the same document ID):

_index: ad,
_type: ad,
_id: 1129339,
_score: null,
fields: {
advancedFeatures: 4
},
sort: [
1129339
]

You can see in the second response, the value '4' is appear as a number and not as an array (e.g. [4]).
As a result I'm getting an exception when I'm trying to deserialize the value to array.

I'm using ES version 0.20.
Meantime, as a workaround I will not request for specific fields.

I'll appreciate your help.
Amit.
</description><key id="9705047">2525</key><summary>Field are not being serialized as arrays when it contain only one item and requesting specific fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bh86</reporter><labels /><created>2013-01-05T11:48:12Z</created><updated>2013-01-10T18:00:50Z</updated><resolved>2013-01-10T18:00:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-01-07T13:27:35Z" id="11951382">There is a couple of things you can do. First of all, you can add a check to your deserialization logic so it can handle both array and a single integer. Alternatively, since you already store source, you can turn off storing advancedFeatures as a field. This way elasticsearch will always retrieve it from the source and you will get it as an array even if you request it as a specific field. 
</comment><comment author="bh86" created="2013-01-10T18:00:50Z" id="12109898">Hi,

Thanks for your help!
About the first option - I don't have much control over the deserialization of the Json because I'm using a .Net client.
But, the second option fixed the problem! I also realized that I do not have to store all the fields :)

Thanks again :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get fields as string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2524</link><project id="" key="" /><description>I tried to look everywhere but still not sure how this can be done. When I get selective fields using this method:

http://www.elasticsearch.org/guide/reference/api/search/fields.html

I want to get fields as string so that I can use my own object mapper, but it doesn't seem to have appropriate method, i.e. `getFieldsAsString`.

Is this a way to get what I need? I use java client.

Thanks,
</description><key id="9700945">2524</key><summary>Get fields as string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phungleson</reporter><labels /><created>2013-01-05T02:20:51Z</created><updated>2014-07-08T18:54:11Z</updated><resolved>2014-07-08T18:54:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-05T02:24:19Z" id="11907966">How about simply calling "toString" on the object you get back? It should work well for everything except binary ones (base64 shenanigans)
</comment><comment author="phungleson" created="2013-01-05T03:27:52Z" id="11909305">If I call `toString` on `SearchResponse` I get a very big JSON which I have no intention or parse it.

Ideally I would like to get fields as string per `SearchHit` only.
</comment><comment author="uboness" created="2013-01-05T04:07:11Z" id="11909658">``` java
SearchResponse res = ...;
for (SearchHit hit : res.hits()) {
    for (SearchHitField field : hit.fields().values()) {
         String name = field.name();
         String value = String.valueOf(field.value());

         // do your mappings
         ....
    }
}
```
</comment><comment author="phungleson" created="2013-01-05T04:22:50Z" id="11909792">Hmm ok, it is the way I am currently doing now, but the down side is that I have to do very manually, i.e. assigning every single fields.

I prefer to use some kind of jackson JSON mapping, the same way I use with `getSourceAsString()`

``` java
Comment comment = new ObjectMapper().readValue(
                        searchHit.getSourceAsString(), Comment.class);
```
</comment><comment author="uboness" created="2013-01-05T04:47:05Z" id="11909978">No, we don't have it now... the "problem" here from es perspective is that you get into discussions on the string representation of the fields - the format, the json structure (arrays vs. object, nested object vs. dot notation), etc... with the source it's not a problem, as the source you get back is exactly the source you push in. In your case I guess it'd be best to just use a utility method that converts the SearchHit to Jackson `JsonNode` (without the need to serialize/deserialize it to/from JSON string)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Binary Mapped Fields: Allow to not store them by default, and return BytesReference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2523</link><project id="" key="" /><description>Allow to not store binary mapped fields (they used to be stored by default, and it can't be changed). This allows to map a field as binary, but not have it stored, but still when asking specifically for the field getting hte binary form of it back.

Also, a breaking change for the Java API, the binary field is now returned as `BytesReference` compared to `byte[]` when mapped and explicitly asked for.
</description><key id="9698393">2523</key><summary>Binary Mapped Fields: Allow to not store them by default, and return BytesReference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-01-04T23:27:43Z</created><updated>2013-01-05T00:50:53Z</updated><resolved>2013-01-05T00:50:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>JAVA API: Get API GetField.getValue() returns inconsistent result type for binary fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2522</link><project id="" key="" /><description>After a Get request, Java API GetField.getValue() of a binary field may return different type of value depending on whether fetching the document was done from the transaction log or from the index.

When fetching from the index, the result is a very convenient byte[], while when fetching from the transaction log, getValue() returns the base64 encoded representation as a String.

Both ways should return a byte[].

This issue was already opened and fixed (see #1476), but the code was removed in 0.20 causing a regression. The provided fix caused dates to be re-encoded to longs, leaving the inconsistency with binary fields.
</description><key id="9697097">2522</key><summary>JAVA API: Get API GetField.getValue() returns inconsistent result type for binary fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ahfeel</reporter><labels /><created>2013-01-04T22:32:45Z</created><updated>2013-06-28T11:33:21Z</updated><resolved>2013-06-28T11:33:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ahfeel" created="2013-01-09T10:04:42Z" id="12038058">I'd say that it should return the same thing that you provided, which is a
byte[] :)

On Mon, Jan 7, 2013 at 8:31 PM, Igor Motov notifications@github.com wrote:

&gt; There are four cases when a binary field value can be returned by the get
&gt; operation:
&gt; 1. from source using transaction log
&gt; 2. from source using index
&gt; 3. from stored field using transaction log
&gt; 4. from stored field using index
&gt; 
&gt; In master, in all cases except 4., the field is returned as base64 encoded
&gt; string. So, for the sake of consistency, should we change 1.-3. to return
&gt; byte[] or should we change 4. to return a string?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2522#issuecomment-11966822.

## 

J&#233;r&#233;mie BORDIER
</comment><comment author="kimchy" created="2013-01-10T18:09:48Z" id="12110332">This issue has been fixed in master (upcoming 0.21). Fixing in 0.20 will require a bit more effort... (master/0.21 has simplified refactored mapping handling that really made this simple).
</comment><comment author="spinscale" created="2013-05-27T16:19:58Z" id="18505820">@ahfeel do you still have this issue with 0.90 or can we close this bug?
</comment><comment author="spinscale" created="2013-06-28T11:33:21Z" id="20183352">Closing. Happy to reopen with more information provided.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support transliteration in the ICU plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2521</link><project id="" key="" /><description>Not sure how easily it can be exposed, but would be excellent to support transliteration in the ICU analysis plugin

http://userguide.icu-project.org/transforms/general
</description><key id="9676365">2521</key><summary>Support transliteration in the ICU plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-01-04T12:02:58Z</created><updated>2013-03-03T12:59:42Z</updated><resolved>2013-03-03T12:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2013-01-06T14:12:43Z" id="11928871">http://lucene.apache.org/core/4_0_0/analyzers-icu/overview-summary.html#transform
</comment><comment author="s1monw" created="2013-01-06T14:19:52Z" id="11928957">this seems to be exposed but not documented....

see https://github.com/elasticsearch/elasticsearch-analysis-icu/blob/master/src/main/java/org/elasticsearch/index/analysis/IcuTransformTokenFilterFactory.java

its bound to "icu_transform" 
</comment><comment author="rmuir" created="2013-01-06T14:29:35Z" id="11929043">that one (like the lucene factory) is also limited in that it doesnt support custom rulesets. But its true you can do a lot with chains defined from the built-in ICU ones, supporting custom ones here is maybe too expert and overkill.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timeout on search not respected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2520</link><project id="" key="" /><description>I have seen many issues related to that, but nobody seems to have found an answer to this one, so I'm opening a new issue.

This is what I'm doing in the Java API for search;
https://gist.github.com/4444788

And even if I put a 2 sec timeout, I have often requests that take more than 5 and even 10 seconds that does not time out. But sometime, I have some query that take under 2 sec and says timed out. 

On the documentation it says; 
timeout  A search timeout, bounding the search request to be executed within the specified time value and bail with the hits accumulated up to that point when expired. Defaults to no timeout.

With that I'm expecting every query to be under (or roughly) around 2 second, but not 5 or 10 seconds.
</description><key id="9648514">2520</key><summary>Timeout on search not respected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgagnon1</reporter><labels /><created>2013-01-03T16:44:59Z</created><updated>2014-09-30T06:15:12Z</updated><resolved>2013-02-27T15:38:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-06T22:54:28Z" id="11936170">Sadly, it is a best effort timeout, its not being checked on all places. Specifically, if you send a query that ends up being rewritten into many terms (fuzzy, or wildcard), that part (the rewrite part) does not check for a timeout.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryBuilders and FilterBuilders have dependency on spatial4j which is optional.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2519</link><project id="" key="" /><description>IMHO, these classes shouldn't have hard dependencies on spatial4j library which is optional.
There is two way to resolve this issue:
- Make the dependency on spatial4j required (optional=false)
- or move geoShape\* to the separate GeoQueryBuilders and GeoFilterBuilders factory classes

The latest solution allows to keep the library optional. And it allows to use it when it is **really** necessary.

As one more solution, it is move geo-shape functionality to the separate library like a plugin.
</description><key id="9648094">2519</key><summary>QueryBuilders and FilterBuilders have dependency on spatial4j which is optional.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yurkom</reporter><labels /><created>2013-01-03T16:30:36Z</created><updated>2013-03-20T00:51:00Z</updated><resolved>2013-01-08T12:19:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-06T22:51:10Z" id="11936120">Do you get a failure in this case? I tested it, and it works fine even if you don't have the spatial4j lib in the classpath, assuming you don't call the builders that require it.
</comment><comment author="yurkom" created="2013-01-08T12:19:18Z" id="11995033">I have found that it is not an issue for Java. It is reproduced in the [Scala](http://www.scala-lang.org/) compiler (ver. 2.9.2). I tested two simple classes for Java and Scala. They did the same work. And the scala code got a failure.

``` scala
import org.elasticsearch.index.query.FilterBuilders._

class ElasticTest {
  def test {
    orFilter(termFilter("key", "phrase"))
  }
}
```

``` log
error: error while loading FilterBuilders, Missing dependency 'class com.spatial4j.core.shape.Shape', required by c:\lib\elastic.jar(org/elasticsearch/index/query/FilterBuilders.class)
ElasticTest.scala:5: error: not found: value orFilter
    orFilter(termFilter("key", "phrase"))
    ^
two errors found
```

About the issue:
https://issues.scala-lang.org/browse/SI-5343
http://www.scala-lang.org/node/12342
</comment><comment author="danklynn" created="2013-02-21T18:04:54Z" id="13903486">This issue is also present in the Groovy runtime:

``` groovy
// this throws a NoClassDefFoundError:
def query = QueryBuilders.boolQuery()
```

java.lang.NoClassDefFoundError: com/spatial4j/core/shape/Shape
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2427)
    at java.lang.Class.getDeclaredMethods(Class.java:1791)
    at org.codehaus.groovy.reflection.CachedClass$3$1.run(CachedClass.java:84)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:81)
    at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:79)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
    at org.codehaus.groovy.reflection.CachedClass.getMethods(CachedClass.java:250)
    at groovy.lang.MetaClassImpl.populateMethods(MetaClassImpl.java:305)
    at groovy.lang.MetaClassImpl.fillMethodIndex(MetaClassImpl.java:284)
    at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2904)
    at groovy.lang.ExpandoMetaClass.initialize(ExpandoMetaClass.java:483)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:166)
    at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:182)
    at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:227)
    at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:751)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallStaticSite(CallSiteArray.java:59)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallSite(CallSiteArray.java:146)
    at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
    at com.fullcontact.elasticsearchtests.TestHarness.queryElasticSearch(TestHarness.groovy:27)
</comment><comment author="kimchy" created="2013-02-21T18:24:28Z" id="13904448">@danklynn which groovy version?
</comment><comment author="pablomolnar" created="2013-03-20T00:47:42Z" id="15152833">@kimchy The issue is pretty issue to reproduce in Grails 2.2.1

1) grails create-app bug
2) Add inside the dependecy block in the BuildConfig.groovy:

```
    compile 'org.elasticsearch:elasticsearch:0.20.5'
    compile('org.elasticsearch:elasticsearch-lang-groovy:1.3.0')
```

3) Add this unit test:

class ClassNotFoundTests {

```
void testBug() {
    org.elasticsearch.index.query.QueryBuilders.matchAllQuery()
}
```

4) grails test-app report:

 Running 1 unit test... 1 of 1
| Failure:  testBug(bug.ClassNotFoundTests)
|  java.lang.NoClassDefFoundError: com/spatial4j/core/shape/Shape
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2451)
    at java.lang.Class.getDeclaredMethods(Class.java:1810)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
    at bug.ClassNotFoundTests.testBug(ClassNotFoundTests.groovy:12)
Caused by: java.lang.ClassNotFoundException: com.spatial4j.core.shape.Shape
    at org.codehaus.groovy.tools.RootLoader.findClass(RootLoader.java:175)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
    at org.codehaus.groovy.tools.RootLoader.loadClass(RootLoader.java:147)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
    ... 5 more
| Completed 1 unit test, 1 failed in 533ms
| Packaging Grails application.....
| Tests FAILED  - view reports in /private/tmp/bug/target/test-reports
</comment><comment author="pablomolnar" created="2013-03-20T00:51:00Z" id="15152940">Also, FYI the issue is not present using 0.90.0.Beta1:

```
    compile 'org.elasticsearch:elasticsearch:0.90.0.Beta1'
    compile('org.elasticsearch:elasticsearch-lang-groovy:1.4.0')
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incompatible mapping upgrade from `object` to `nested` ignored and "acknowledged"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2518</link><project id="" key="" /><description>When I create an index with simple mapping:

```
curl -X DELETE localhost:9200/articles/
curl -X POST localhost:9200/articles/ -d '{"mappings" : { "article" : { "properties" : { "title" : { "type" : "string", "analyzer" : "snowball" } } } } }'
```

And then index a document which contains `object` types (comments):

```
curl -X PUT localhost:9200/articles/article/1 -d '{"title" : "Just testing...", "comments" : [ {"author" : "John", "message" : "Boo"} ]}'
```

The mapping for the `comments` field is `object` (dynamic). When I now try to upgrade the mapping to `nested`:

```
curl -X PUT localhost:9200/articles/article/_mapping -d '
{"article" : {"properties" : { "comments" : { "type": "nested", "author": "string", "message": "string" } } } }
'
```

I receive a successful response:

```
{"ok":true,"acknowledged":true}
```

But the changes to mapping are _not_ applied. In this case, either the requested changes should be applied, or a `MergeMappingException` error should be raised.
</description><key id="9647356">2518</key><summary>Incompatible mapping upgrade from `object` to `nested` ignored and "acknowledged"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels><label>bug</label><label>v0.90.0.Beta1</label></labels><created>2013-01-03T16:06:43Z</created><updated>2013-01-04T08:51:57Z</updated><resolved>2013-01-03T23:37:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2013-01-04T08:51:57Z" id="11875661">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for changing the log level or the root logger at runtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2517</link><project id="" key="" /><description>Right now it's possible to change the log level at runtime only for specifically named components. It should be possible to change the root logger level as well using request like:

```
curl -XPUT 'localhost:9200/_cluster/settings' -d '{
    "transient" : {
        "logger._root": "DEBUG"
    }
}'
```
</description><key id="9632222">2517</key><summary>Support for changing the log level or the root logger at runtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label></labels><created>2013-01-03T00:20:18Z</created><updated>2014-08-22T18:25:25Z</updated><resolved>2013-01-03T00:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>m2e lifecycle-mapping to ignore maven-dependency-plugin in eclipse m2e</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2516</link><project id="" key="" /><description>add plugin configuration to make m2e ignore maven dependency plugin configuration that it cannot handle. This allows you to import the es pom with m2e without errors in the latest eclipse.
</description><key id="9614441">2516</key><summary>m2e lifecycle-mapping to ignore maven-dependency-plugin in eclipse m2e</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels /><created>2013-01-02T12:18:30Z</created><updated>2014-07-16T21:54:13Z</updated><resolved>2013-01-04T07:25:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-04T07:25:18Z" id="11874466">Pushed to master, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"The package is of bad quality"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2515</link><project id="" key="" /><description>Opening the .deb provided for elasticsearch 0.20.2 on ubuntu 12.10 gives me the following message:

The package is of bad quality

The installation of a package which violates the quality standards isn't allowed. This could cause serious problems on your computer. Please contact the person or organisation who provided this package file and include the details beneath.

```
Lintian check results for /home/francois/Downloads/elasticsearch-0.20.2.deb:
BFD: usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so: warning: sh_link not set for section `.IA_64.unwind'
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so
E: elasticsearch: control-file-has-bad-permissions conffiles 0755 != 0644
```
</description><key id="9565067">2515</key><summary>"The package is of bad quality"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">fbernier</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2012-12-28T20:00:40Z</created><updated>2013-06-07T11:53:42Z</updated><resolved>2013-06-07T11:53:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-23T14:51:53Z" id="18348356">Hey there,

I cannot reproduce this on Ubuntu. What exactly did you do?

It is ok that lintian produces these errors (it complains that we ship files that are not arch independent in a seemingly arch independent package). However we use these files only if elasticsearch is running on that architecture.

Anyway, do you get this exception when trying to install the debian package?
</comment><comment author="balaf" created="2013-06-07T07:07:44Z" id="19092216">I got the same error in ubuntu 13.04 and elasticsearch-0.90.1.
What I did was simply download the .deb file from [elastisearch.org](http://www.elasticsearch.org/download/), open it with the Ubuntu Software Center and select _install_. Then I get this error.

&gt; **The package is of bad quality**
&gt; The installation of a package which violates the quality standards isn't allowed. This could cause serious problems on your computer. Please contact the person or organisation who provided this package file and include the details beneath.

```
Lintian check results for /tmp/elasticsearch-0.90.1.deb:
BFD: usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so: warning: sh_link not set for section `.IA_64.unwind'
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so
E: elasticsearch: control-file-has-bad-permissions conffiles 0755 != 0644
```
</comment><comment author="spinscale" created="2013-06-07T08:16:23Z" id="19094471">Ah, I see, so you are not simply calling `dpkg -i elasticsearch-0.90.1.deb` but you are using a GUI.

Maybe there are parameters to make lintian ignore this kind of stuff. I need to check that (we try not to provide a per architecture package).

Thanks for clarifying!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get a JSon response when updating cluster settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2514</link><project id="" key="" /><description>Here is a change proposal for the cluster update settings API.

When you PUT new settings, you don't get a JSon answer:

``` sh
curl -XPUT localhost:9200/_cluster/settings -d '{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}'
```

The change proposal is to get a valid JSon as an answer : 

``` javascript
{ 
  "ok" : true
}
```

I found this when testing the [elasticsearch SPORE project](https://github.com/dadoonet/spore-elasticsearch). As the answer is an empty string, I can not evaluate the answer as a JSon content.

I hope this could help.
</description><key id="9559008">2514</key><summary>Get a JSon response when updating cluster settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-12-28T13:52:53Z</created><updated>2014-07-16T21:54:13Z</updated><resolved>2012-12-28T14:59:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2012-12-28T14:14:44Z" id="11732493">What's the added value of having such a json response over the current 200 http status?
</comment><comment author="dadoonet" created="2012-12-28T14:31:30Z" id="11732790">When you send something wrong to the API, you get back a valid JSon document: 

``` javascript
{
  "error":"ActionRequestValidationException[Validation Failed: 1: no settings to update;]",
  "status":500
}
```

But you're right, perhaps I should manage the HTTP status code in my project instead of waiting for a String or JSon response. 

It's the first ES API I've seen which don't give anything as a result. Is there another ones?

If it's the only one, I think we should be consistent.
If there are other APIs that only return a 200 status code with no content, we can close this pull request.

What do you think?
</comment><comment author="uboness" created="2012-12-28T14:48:00Z" id="11733101">two examples (though these HEAD requests):

http://www.elasticsearch.org/guide/reference/api/admin-indices-types-exists.html
http://www.elasticsearch.org/guide/reference/api/admin-indices-indices-exists.html

It's true that we have some api's that only return { "ok" : true } (e.g. indices update api) , which I believe should change an empty body to indeed keep things consistent. The status codes play an important part in the rest api and we should encourage clients to use/check them.
</comment><comment author="dadoonet" created="2012-12-28T14:59:35Z" id="11733328">You're definitly right. I modified my Java SPORE client code and I now manage HTTP return codes properly.

I close this PR. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move pom.xml to 0.20.3-SNAPSHOT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2513</link><project id="" key="" /><description /><key id="9554552">2513</key><summary>Move pom.xml to 0.20.3-SNAPSHOT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-12-28T10:09:58Z</created><updated>2014-07-16T21:54:14Z</updated><resolved>2012-12-28T12:56:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-12-28T12:56:20Z" id="11731245">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move pom.xml to 0.20.3-SNAPSHOT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2512</link><project id="" key="" /><description /><key id="9554534">2512</key><summary>Move pom.xml to 0.20.3-SNAPSHOT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-12-28T10:08:15Z</created><updated>2014-07-16T21:54:15Z</updated><resolved>2012-12-28T10:09:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-12-28T10:09:15Z" id="11728869">Sorry ! Bad PR !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>compare geo hash with equals, not == as GeoHashUtils.encode doesn't intern</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2511</link><project id="" key="" /><description /><key id="9552807">2511</key><summary>compare geo hash with equals, not == as GeoHashUtils.encode doesn't intern</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mebigfatguy</reporter><labels /><created>2012-12-28T07:32:18Z</created><updated>2014-07-08T18:53:40Z</updated><resolved>2014-07-08T18:53:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-28T07:34:17Z" id="11726974">can you squash the commit and I will pull it in...
</comment><comment author="mebigfatguy" created="2012-12-28T08:12:48Z" id="11727412">hmm, when i do 
  git rebase --preserve-merges -i bab91bb
which is 
bab91bb  "upgrade to jackson 2.1.1"
 the commit before the first merge commit, and i try to squash the two merge commits i get
Refusing to squash a merge: 816427a6e14a5f4f49a66111fdb11363c2e940d9

What am i doing wrong?
</comment><comment author="mebigfatguy" created="2012-12-28T08:38:40Z" id="11727705">ok, there we go... i think that's right.
</comment><comment author="kimchy" created="2012-12-28T18:41:13Z" id="11738299">Btw, the assumption made here is that you typical does end up using the same instance of the string (even though its not interned). For example, when you use a script, you would provide the geo hash as a parameter, so even if you call it multiple times in the script, it would still be the same string instance. So, the overhead of checking for equals, compared to simply checking on instance, make sense...
</comment><comment author="clintongormley" created="2014-07-08T18:53:40Z" id="48383837">This class no longer exists.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException on get with fresh index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2510</link><project id="" key="" /><description># Environment

ElasticSearch Server Version: 0.19.11
ElasticSearch Configuration: 
`cluster.name: sstarkey
   index.number_of_shards: 1
   index.number_of_replicas: 0
   gateway.type: none`

Using Node Client in Java API
# Steps
1. Create a new index on a previously running ElasticSearch node
2. Do something unrelated to the index that takes the "right" amount of time
3. Try to get something that doesn't exist
# Expected

A SearchResponse where isExists() returns false
# Actual

`Caused by: java.lang.NullPointerException
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:150)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:125)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:90)
        at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:175)
        at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:135)
        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        ... 58 more`
# Unfortunately...

This exception only occurs at the integration test layer (i.e. when executing REST calls against our application which wraps ElasticSearch as opposed to when running unit tests directly against the library which integrates with ElasticSearch), and only intermittently.  We have not been able to figure out how to write a standalone unit test which reproduces this problem consistently.
# Thanks!

Help is appreciated, as we'd really rather not catch this exception in our get service, and it smells of a kind of weird race condition.
</description><key id="9541125">2510</key><summary>NullPointerException on get with fresh index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">scstarkey</reporter><labels /><created>2012-12-27T16:41:54Z</created><updated>2013-06-28T11:35:17Z</updated><resolved>2013-06-28T11:35:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-01-02T15:29:43Z" id="11811589">Could you provide a standalone test that reproduces the problem intermittently?  
</comment><comment author="spinscale" created="2013-05-27T16:21:19Z" id="18505865">@scstarkey Is this problem still an issue for you in up-to-date elasticsearch versions? If so, can you provide more input or a gist to reproduce the issue?
</comment><comment author="spinscale" created="2013-06-28T11:35:17Z" id="20183421">Closing, as reproducing this without a valid test case is not easy. Happy to reopen if more information is provided (and still valid for the current release).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update settings: Allow to dynamically update thread pool settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2509</link><project id="" key="" /><description>Allow to dynamically update [thread pool settings](http://www.elasticsearch.org/guide/reference/modules/threadpool.html). The settings can be updated using [Cluster Update Settings API](http://www.elasticsearch.org/guide/reference/api/admin-cluster-update-settings.html). Both pool type and pool parameters can be changed dynamically. Minor changes, such as number of threads or queue size, are made to the existing thread pool executor. To apply major changes such as thread pool type or queue type changes, Elasticsearch replaces the old thread executor with a new executor. When this happens Elasticsearch creates a new thread pool first and starts executing all new tasks using the new pool, meanwhile all tasks that are currently executed in the old pool are allowed to finish before the old thread pool is stopped.
</description><key id="9538652">2509</key><summary>Update settings: Allow to dynamically update thread pool settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-27T14:29:25Z</created><updated>2015-02-25T04:20:59Z</updated><resolved>2012-12-27T14:43:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="matthuhiggins" created="2014-12-25T19:14:09Z" id="68110903">I frequently find that the queue_size is not picked up when using a live update. In my particular case it is for bulk.queue_size. Do all writes to the cluster need to completely stop to give ES time to switch over?
</comment><comment author="imotov" created="2015-01-25T02:06:34Z" id="71346966">@matthuhiggins could you share a bit more details here? Which version of elasticsearch are you using? How exactly do you try to change the bulk thread pool queue size? How do you verify that your changes were not picked up?
</comment><comment author="matthuhiggins" created="2015-01-28T06:19:08Z" id="71786819">I was using elasticsearch 1.4.0. I used the index update settings API to increase the bulk queue size capacity to 100. For up to 30 minutes after, I continued to see `[rejected execution (queue capacity 50) ..` for bulk requests. It was eventually picked up. (I unfortunately then started seeing `queue capacity 100` errors. I set the queue capacity to -1 and restarted the cluster because I continued to see errors with capacity of 100).

I tried both transient and persistent settings. The reason I think there is a bug is because of the case where I saw `queue capacity 50` errors for up to 30 minutes after the change. Without any changes on my part, the setting was finally picked up after all index writes were stopped.
</comment><comment author="imotov" created="2015-01-28T20:52:13Z" id="71913387">@matthuhiggins when queue size is updated elasticsearch retires old executor and replaces it with a new executor. So, this lingering error is only possible is some component is holding to the old instance of the executor and continues to reuse it instead of asking for a new one. I checked all places where we are using BULK thread pool and don't really see how it can continue clinging to the old instance for 30 minutes. Do you still have the log file with this error? I would love to see a complete stack trace to figure out who is hanging to this thread pool for so long. 
</comment><comment author="matthuhiggins" created="2015-01-28T21:11:09Z" id="71916728">I do not, and I imagine it will be hard to retrace. The cluster returns the new settings immediately after the update, and my only guess is that something held reference to the older executor. (In my case, we had a 5 node cluster with 80 processes writing to it).
</comment><comment author="imotov" created="2015-01-28T21:19:36Z" id="71918246">@matthuhiggins what did you use to bulk index data into elasticsearch?
</comment><comment author="matthuhiggins" created="2015-01-28T21:53:30Z" id="71924381">Resque workers reading from a postgres table and sending that raw data to elasticsearch. (Each worker was told to work on a range of the data). Were you curious or will this help you debug?
</comment><comment author="imotov" created="2015-01-28T22:19:23Z" id="71928809">@matthuhiggins I was just trying to rule out a remote possibility that you used a plugin that was hoarding old bulk executors.
</comment><comment author="matthuhiggins" created="2015-01-28T22:29:01Z" id="71930452">Nope. It's just simple bulk requests over http. I imagine that the scenario could be reproduced with a multi-node cluster and sufficient load - if not, no worries.
</comment><comment author="djdenv" created="2015-02-25T04:20:59Z" id="75903310">I had to do this as well with the bulk queue_size. In my scenario, I was using elastic search via the grails plugin. The plugin binds to the domain model and queues up index updates asynchronously. The application was performing an immense amount of database activity (importing a very large CSV file into database) and quickly surpassed the default queue_size of 50 (saw same error as @matthuhiggins. Using the Java API, I was able to update the queue size like so:

ThreadPool threadPool = elasticSearchAdminService.elasticSearchHelper.elasticSearchClient.admin().cluster().threadPool()
       threadPool.updateSettings(ImmutableSettings.settingsBuilder().put("threadpool.bulk.queue_size", "1000").build())

Note that "elasticSearchAdminService" is the injected grails service provided by the plugin.
Hope this is helpful to someone...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix npe due to copy/paste bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2508</link><project id="" key="" /><description>if stopWords is not null, it is handled above, otherwise, code should be using getWordList() value, but doesn't. code npe's if it gets here.
</description><key id="9529117">2508</key><summary>fix npe due to copy/paste bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mebigfatguy</reporter><labels /><created>2012-12-26T23:58:40Z</created><updated>2014-06-16T10:57:23Z</updated><resolved>2012-12-27T00:17:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-27T00:17:29Z" id="11697421">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins Installer: Allow to download plugins from download.elasticsearch.org</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2507</link><project id="" key="" /><description>The plugin script now will automatically download plugins from `download.elasticsearch.org` instead of github. It will also try maven (both central and sonatype) if it fails. 

Effectively, this change removes the ability to install plugins from the downloads section in github, since its no longer supported by github. Site plugins can still be installed by downloading the "repo".

Direct installation is still supported, using the `bin/plugin -url file://path/to/plugin -name plugin-name`.
</description><key id="9527909">2507</key><summary>Plugins Installer: Allow to download plugins from download.elasticsearch.org</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-26T22:16:09Z</created><updated>2013-01-10T18:13:17Z</updated><resolved>2012-12-26T22:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-12-27T08:10:53Z" id="11703132">What is your recommandation for external plugins?
Where should we store the ZIP file? Is Sonatype the "best" place for it or will you open `download.elasticsearch.org` for plugin developers ?
</comment><comment author="lukas-vlcek" created="2012-12-27T08:58:14Z" id="11703771">Hi Shay, did you have a chance to look at [elasticsearch-apps](https://github.com/jprante/elasticsearch-apps) by J&#246;rg? I think this is pretty good approach.
</comment><comment author="jprante" created="2012-12-27T09:29:07Z" id="11704226">@kimchy thanks for your kind feedback about you want to evaluate https://github.com/jprante/elasticsearch-apps
Will 3rd party developers have to register on elasticsearch.org or will they be able to offer additional download locations?

@dadoonet I agree that Maven repo addressing would be a viable solution for many plugins right now, it should be non-obtrusive to those who do not want to use Maven though. Beside Maven central / Sonatype, adding private repos should also be possible, to convince organizations that ES can rely on a self-controlled plugin ecosystem.

@lukas-vlcek thanks for the hint to https://github.com/jprante/elasticsearch-apps :) I have opened a showcase of an ES 0.20.1 patched version, using a tool `bin/app` as a replacement for `bin/plugin` to show how a Maven repo dependency mechanism can be used from within an ES node startup.

Just a note because github is now excluded: for small "github blobs" (around 1 MB or so?), some plugin zips could still be downloaded from within the repo area of github, via a HTTP GET from https://raw.github.com so I think it would also be nice to still include github, for example see https://github.com/elasticsearch/elasticsearch/pull/2477

I find it very useful, as hinted to me by @kimchy, to offer the plugin mechanism also to non-Java folks and to make the plugins more polyglot in the future.

By re-using existing repo infrastructure out there, the core team won't have to monitor download.elasticsearch.org for availability and packaging policies - less janitor duties means more focusing on Elasticsearch core development :)

There are the ruby gems and the python easy_install repos out there. Of course, perl has CPAN (but does not run within a JVM).

Maven 3 is getting more and more polyglot and is no longer Java-centric. 

Anyway, there is still work to do for real polyglot solutions. Smarter people than me were already busy with this. One approach showed how to package JRuby gems in a jar for Maven 

http://blog.nicksieger.com/articles/2009/01/10/jruby-1-1-6-gems-in-a-jar/

and another one is for standalone Jython scripts

http://mavenjython.sourceforge.net/

There is also a method for translating the POM into other languages - the Maven-specific polyglot approach

https://github.com/tobrien/polyglot-maven

I think it is not so relevant for ES plugin repo substitution to the missing github download right now, but it shows the power and flexibility of the Maven 3 core.

So I am optimistic there will be some usefulness in an unobtrusive Maven repo distribution method that may be valuable also for ES polyglot plugins.
</comment><comment author="dadoonet" created="2012-12-28T02:03:15Z" id="11723417">I've got my answer in the 0.20.2 release note:

&gt; Another change includes how plugins are downloaded. Now, &#8220;elasticsearch&#8221; plugins are downloaded automatically from our new download.elasticsearch.org service, but it can also download plugins from maven directly. Site plugins can stil be downloaded from github as &#8220;repositories&#8221;. In the future, we will also allow users to upload their own plugins to download.elasticsearch.org if they wish to.
</comment><comment author="karmi" created="2012-12-29T11:24:00Z" id="11751632">Just to offer a different perspective (non-Java developer, non-plugin developer): regardless of the support for "common plugin locations", you can install a plugin by using it's fully qualified URL, right? This is what eg. the [_Chef_ cookbook](https://github.com/karmi/cookbook-elasticsearch/blob/master/libraries/extensions/install_plugin.rb) supports, allowing to install public or private plugins.
</comment><comment author="kimchy" created="2013-01-06T22:28:06Z" id="11935501">Heya,

First, a fully qualified URL still works when installing plugins, as @karmi mentioned, so you can always host it wherever you wish to.

Regarding external plugins, except for the full qualified URL, the plugin mechanism will also try and install it from maven. So you can simply publish it to sonatype (or maven central through it), and it will download it from there. I guess in any case as a plugin developer you would like to provide the plugin on maven repo, so the current solution would work nicely there as well.

Last, we might also end up allowing for people to upload plugins to `download.elasticsearch.org`. We will see. Its not a big effort on our end, assuming that maven is not enough, we can definitely do that.

Lets see how this solution evolves, and see what is left needed, and we can tackle it then. Its very simple now, but covers a lot of cases, I tend to like these type of solutions :)
</comment><comment author="jprante" created="2013-01-07T08:58:06Z" id="11944606">With this solution, I think I have to clean up the artifact group ids of my plugin jars to something like org.xbib.elasticsearch.plugins and I also have publish full dropbox URLs or something in my plugin READMEs per version, because I'm not using the default repo at Maven central or Sonatype OSSRH.

Note, looking up a ZIP under http://search.maven.org/ is not using Maven in the intended way, it uses just another central download service add-on, like github before. That's not the way a Maven repo is supposed to work. The typical use of Maven is resolving dependencies, and having the opportunity for each user to add a lot of other public and private Maven repos too, with different scopes and terms of use.
</comment><comment author="spinscale" created="2013-01-07T12:40:58Z" id="11950179">I am all for allowing uploads to &lt;code&gt;download.elasticsearch.org&lt;/code&gt; - having a bunch of anywhere hosted plugins will make them every now and then unreachable. We could even build a nice web gui on top of that repo and make them searchable very easily (along with its documentation, all centralized)...
</comment><comment author="jprante" created="2013-01-07T14:21:16Z" id="11953068">I once assumed plugin development and making plugins downloadable should be decentralized, but I may be wrong.

Note, each download service should state to users who is responsible for the uploaded file. The legal stuff must be taken care of, such as different licensing of plugins, and copyright infringement protection need to be sorted out so that download.elasticsearch.org can't be taken down simply by a voluntary DMCA action against a suspicious plugin. What kind of copyright law applies, is not really clear to me, I think it's mostly the law of the country of the downloader.

Another option would be an obligation for donating plugin code to download.elasticsearch.org under the Apache license with a CLA. By doing this, plugins could easily be moved into the ES core at will.

Additionally, mirrors of download.elasticsearch.org should be considered, in case of planned or unplanned outages.

I guess maintaining download.elasticsearch.org has limited resources to handle all of these things. I'm afraid the volunteering technical staff will not scale very well with the hopefully ever growing number of Elasticsearch plugins.
</comment><comment author="kimchy" created="2013-01-10T18:13:17Z" id="12110508">@jprante agreed, those are good questions you asked for regarding the download service, which we need to figure out if we allow for people to upload plugins to `download.elasticsearch.org`. For now though, "code plugins" can easily be downloaded from maven as well, so I don't see a problem there (`plugin -install &lt;group_id&gt;/&lt;artifact_id&gt;/&lt;version&gt;`).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass topScorer=false to sub-scorers if a scorer is wrapped. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2506</link><project id="" key="" /><description>Wrapped BooleanQuery can return collect-only scorers. This closes #2505
</description><key id="9527771">2506</key><summary>Pass topScorer=false to sub-scorers if a scorer is wrapped. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2012-12-26T22:05:12Z</created><updated>2014-06-17T01:01:10Z</updated><resolved>2012-12-26T22:21:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-26T22:21:56Z" id="11695657">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom_filters_score causes UnsupportedOperationException </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2505</link><project id="" key="" /><description>Hi,

I am very new to ES. I am struggling with a strange issue with ES.
custom_filters_score causing UnsupportedOperationException when fuzzy query used for some partial entries.

My index contains the term "express". The following query fails to execute when I search "exp" and "expr". Surprisingly other variations(e,ex,expre,expres,express) works properly.

thanks.

{
    "query": {
        "custom_filters_score": {
            "query": {
                "fuzzy": {
                    "_all": {
                        "term": "exp",
                        "boost": 1,
                        "min_similarity": 0.5,
                        "prefix_length": 0,
                        "rewrite": "constant_score_auto"
                    }
                }
            },
            "boost": 1,
            "filters": [{
                "filter": {
                    "ids": {
                        "type": "productModel",
                        "values": ["1234"]
                    }
                },
                "boost": 3
            }]
        }
    }
}

[2012-12-26 22:04:16,854][DEBUG][action.search.type       ] [node1] [kang1_11][0], node[vmCjUzdBTI-SxvwU_fUEvg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@18515bf]
org.elasticsearch.search.query.QueryPhaseExecutionException: [kang1_11][0]: query[filtered(custom score (_all:exp~1, functions: [{filter(_uid:productModel#1234), function [boost[3.0]]}]))-&gt;cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@953b6c6c)],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:183)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:316)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:243)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.UnsupportedOperationException
    at org.apache.lucene.search.BooleanScorer.nextDoc(BooleanScorer.java:316)
    at org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorScorer.nextDoc(FiltersFunctionScoreQuery.java:283)
    at org.apache.lucene.search.Scorer.score(Scorer.java:61)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:588)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:199)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:465)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:421)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:264)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:164)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:252)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:179)
</description><key id="9526488">2505</key><summary>custom_filters_score causes UnsupportedOperationException </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">rkarakaya</reporter><labels><label>v0.90.0.Beta1</label></labels><created>2012-12-26T20:29:24Z</created><updated>2012-12-27T20:12:49Z</updated><resolved>2012-12-26T22:34:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-12-26T21:28:33Z" id="11694710">I can see the problem here. FiltersFunctionScoreQuery seems to pass topScorer = true down to the wrapped scorer which triggers a BooleanScorer which doesn't support nextDoc/advance. as a workaround until this is fixed you can use "rewrite" : "constant_score_filter" 
</comment><comment author="rkarakaya" created="2012-12-26T21:38:55Z" id="11694886">Thank you very much Simon..
</comment><comment author="kimchy" created="2012-12-26T22:24:14Z" id="11695700">@rkarakaya which version did you get this failure with? Are you running a custom build of master?
</comment><comment author="rkarakaya" created="2012-12-26T22:33:25Z" id="11695863">Hi Shay,

4 days ago, I downloaded the master using this link.

https://github.com/elasticsearch/elasticsearch/zipball/master

thanks.
</comment><comment author="kimchy" created="2012-12-26T22:34:12Z" id="11695878">Great, just wanted to make sure, since I don't see this failure can happen in 0.20, just in master with the new Lucene 4 upgrade.
</comment><comment author="rkarakaya" created="2012-12-26T22:37:14Z" id="11695926">yes.. I've been waiting for lucene 4 update long time. 
</comment><comment author="s1monw" created="2012-12-27T14:21:39Z" id="11708720">@rkarakaya can you confirm that this is fixed on current master?
</comment><comment author="rkarakaya" created="2012-12-27T14:27:26Z" id="11708826">Hi simon,

not tried yet. 
I am going to try a few hour later and return back.

thanks.
</comment><comment author="rkarakaya" created="2012-12-27T19:26:39Z" id="11716101">Hi Simon,

This patch solved the issue.

regards..
</comment><comment author="s1monw" created="2012-12-27T20:12:49Z" id="11717166">@rkarakaya thanks for confirming!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 3.6.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2504</link><project id="" key="" /><description>Contains important bug fixes

http://search-lucene.com/m/Rk0cz1eso31&amp;subj=+ANNOUNCE+Apache+Lucene+3+6+2+released
</description><key id="9512267">2504</key><summary>Upgrade to Lucene 3.6.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>v0.19.13</label><label>v0.20.2</label></labels><created>2012-12-25T17:21:27Z</created><updated>2012-12-25T22:09:27Z</updated><resolved>2012-12-25T22:09:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-12-25T22:09:27Z" id="11678292">Pushed to 0.19 and 0.20 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested query should support explain.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2503</link><project id="" key="" /><description /><key id="9500682">2503</key><summary>Nested query should support explain.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-24T12:38:56Z</created><updated>2012-12-24T12:52:35Z</updated><resolved>2012-12-24T12:52:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add scoring support to `has_child` and `has_parent` queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2502</link><project id="" key="" /><description>Currently the `has_parent` and `has_child` queries are basically `constant_score` query wrappers around the `has_parent` and `has_child` filters. This adds score support to both queries. Both queries will support a `score_type` option. The `has_child` will support the same options as the `top_children` query and the `none` option which is the default and yields the current behaviour. The `has_parent` query will support the score type options: `score` and `none`. The latter is the default and yields the current behaviour.

If the `score_type` is set to a value other than `none` then the `has_parent` query will map the matched parent score into the related children documents. The `has_child` query will then map the matched children documents into the related parent document.  The `score_type` on both queries defines how the children documents scores are mapped in the parent documents. Both queries are executed in two phases. First phase collects the parent uid values of matching documents with an aggregated score per parent uid value. In the second phase either child or parent typed documents are emitted as hit that have the same parent uid value as found during the first phase. The score computed in the first phase will be used as score.
</description><key id="9498260">2502</key><summary>Add scoring support to `has_child` and `has_parent` queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-24T09:43:37Z</created><updated>2012-12-24T10:39:53Z</updated><resolved>2012-12-24T10:39:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>top_children query fails with dfs_query_* searchtype and some queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2501</link><project id="" key="" /><description>The `top_children` query fails when `DFS_QUERY_THEN_FETCH` is used as `search_type` and wraps a query that gets rewritten (E.g wildcard query).
</description><key id="9470024">2501</key><summary>top_children query fails with dfs_query_* searchtype and some queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.13</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-21T16:57:13Z</created><updated>2012-12-21T17:10:35Z</updated><resolved>2012-12-21T17:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ArrayOutOfBoundException when using top_children in a must not clause.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2500</link><project id="" key="" /><description /><key id="9468135">2500</key><summary>ArrayOutOfBoundException when using top_children in a must not clause.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.13</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-21T15:45:56Z</created><updated>2012-12-21T15:47:46Z</updated><resolved>2012-12-21T15:47:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Problem with querying for boolean true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2499</link><project id="" key="" /><description>It seems that I'm unable to make elasticsearch query for boolean true values. Boolean false works fine.

Elasticsearch version: 0.20.1

Steps to reproduce (in shell script form):

```
curl -XDELETE http://localhost:9200/rctest
echo
curl -XPOST http://localhost:9200/rctest/attendants/123 -d '{"is_cool":true}'
echo
curl -XPOST http://localhost:9200/rctest/attendants/321 -d '{"is_cool":false}'
echo
echo "*** REFRESH"
curl -XPOST http://localhost:9200/rctest/_refresh
echo
echo "*** All documents"
curl -XGET http://localhost:9200/rctest/attendants/_search
echo
echo "*** Result of query A"
curl -XGET http://localhost:9200/rctest/attendants/_search -d '{"query":{"term":{"is_cool":true}}}'
echo
echo "*** Result of Query A as query params"
curl -XGET "http://localhost:9200/rctest/_search?q=is_cool:true"
echo
echo "*** REFRESH"
curl -XPOST http://localhost:9200/rctest/_refresh
echo
echo "*** All documents"
curl -XGET http://localhost:9200/rctest/attendants/_search
echo
echo "*** Result of query B"
curl -XGET http://localhost:9200/rctest/attendants/_search -d '{"query":{"term":{"is_cool":false}}}'
echo
echo "*** Result of Query B as query params"
curl -XGET "http://localhost:9200/rctest/_search?q=is_cool:false"
echo
```

Output of that shell script:

```
{"ok":true,"acknowledged":true}
{"ok":true,"_index":"rctest","_type":"attendants","_id":"123","_version":1}
{"ok":true,"_index":"rctest","_type":"attendants","_id":"321","_version":1}
*** REFRESH
{"ok":true,"_shards":{"total":10,"successful":5,"failed":0}}
*** All documents
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"rctest","_type":"attendants","_id":"321","_score":1.0, "_source" : {"is_cool":false}},{"_index":"rctest","_type":"attendants","_id":"123","_score":1.0, "_source" : {"is_cool":true}}]}}
*** Result of query A
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
*** Result of Query A as query params
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
*** REFRESH
{"ok":true,"_shards":{"total":10,"successful":5,"failed":0}}
*** All documents
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":2,"max_score":1.0,"hits":[{"_index":"rctest","_type":"attendants","_id":"321","_score":1.0, "_source" : {"is_cool":false}},{"_index":"rctest","_type":"attendants","_id":"123","_score":1.0, "_source" : {"is_cool":true}}]}}
*** Result of query B
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"rctest","_type":"attendants","_id":"321","_score":0.30685282, "_source" : {"is_cool":false}}]}}
*** Result of Query B as query params
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.30685282,"hits":[{"_index":"rctest","_type":"attendants","_id":"321","_score":0.30685282, "_source" : {"is_cool":false}}]}}
```

As you can see, the query for {is_cool: true} doesn't return any data.
</description><key id="9463222">2499</key><summary>Problem with querying for boolean true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">augustl</reporter><labels /><created>2012-12-21T12:18:56Z</created><updated>2013-11-25T10:57:28Z</updated><resolved>2013-10-30T10:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-12-21T12:22:56Z" id="11611082">It looks like a duplicate of #2487 and #2484.
</comment><comment author="spinscale" created="2013-10-30T10:03:56Z" id="27376966">Just tested with 0.90.5 - your sample works.

Happy to reopen, if I did something wrong. :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search failures are not serialized correctly in MultiSearchResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2498</link><project id="" key="" /><description>To reproduce, run the following request using Transport client:

``` java
client.prepareMultiSearch().add(client.prepareSearch("test-idx").setQuery(customScoreQuery(matchAllQuery()))).execute().actionGet();
```

It throws the following exception instead of returning a failure message:

```
Exception in thread "main" org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.search.MultiSearchResponse]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:150)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:127)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:793)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:458)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:439)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:84)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:471)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:332)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Expected handle header, got [80]
    at org.elasticsearch.common.io.stream.HandlesStreamInput.readUTF(HandlesStreamInput.java:64)
    at org.elasticsearch.search.SearchShardTarget.readFrom(SearchShardTarget.java:97)
    at org.elasticsearch.search.SearchShardTarget.readSearchShardTarget(SearchShardTarget.java:81)
    at org.elasticsearch.search.internal.InternalSearchHits.readFrom(InternalSearchHits.java:214)
    at org.elasticsearch.search.internal.InternalSearchHits.readFrom(InternalSearchHits.java:200)
    at org.elasticsearch.search.internal.InternalSearchHits.readSearchHits(InternalSearchHits.java:194)
    at org.elasticsearch.search.internal.InternalSearchResponse.readFrom(InternalSearchResponse.java:86)
    at org.elasticsearch.search.internal.InternalSearchResponse.readInternalSearchResponse(InternalSearchResponse.java:80)
    at org.elasticsearch.action.search.SearchResponse.readFrom(SearchResponse.java:284)
    at org.elasticsearch.action.search.MultiSearchResponse$Item.readFrom(MultiSearchResponse.java:86)
    at org.elasticsearch.action.search.MultiSearchResponse$Item.readItem(MultiSearchResponse.java:78)
    at org.elasticsearch.action.search.MultiSearchResponse.readFrom(MultiSearchResponse.java:135)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)
    ... 22 more
```

See also https://groups.google.com/d/topic/elasticsearch/7mipq2I8NpY/discussion
</description><key id="9444517">2498</key><summary>Search failures are not serialized correctly in MultiSearchResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-20T19:33:23Z</created><updated>2012-12-22T00:54:46Z</updated><resolved>2012-12-22T00:54:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tarballs all disappeared</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2497</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/downloads is empty, which breaks scripts which rely on it being there. Is there a new favored place for tarballs to be gotten from?
</description><key id="9416838">2497</key><summary>Tarballs all disappeared</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">temujin9</reporter><labels /><created>2012-12-19T22:14:51Z</created><updated>2012-12-19T22:31:25Z</updated><resolved>2012-12-19T22:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2012-12-19T22:15:50Z" id="11551318">We had to move it:

http://www.elasticsearch.org/blog/2012/12/17/new-download-service.html
</comment><comment author="temujin9" created="2012-12-19T22:30:03Z" id="11551837">Ah, thank you. Updating our cookbook now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards deleted from index (and from disk) on cluster restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2496</link><project id="" key="" /><description>I just had an issue with ES 0.20.0.RC1 that resulted on an cluster with 40 shards missing out of 250 on my production ready index. Note that the shards are not only missing in ElasticSearch **but also on the filesystem**. I have 51 nodes on my cluster.

This is the second time similar event happens to me, so I decided to file a bug on that.

 I will describe the timelime event with logs snipped that explain what I think happenned.

14:12:00 - Cluster is on green state everything is fine -&gt; sending shutdown via API
14:13:02 - First cluster restart. Restarting nodes 10 per 10 (I use tmux so I do it almost simultaneously)

Note: After the restart there are 8 nodes missing from the cluster.

Logs from one of the server that is NOT on the cluster .

```
[2012-12-18 14:13:17,942][WARN ][discovery.zen.ping.unicast] [es1b] failed to send ping to [[#zen_unicast_3#][inet[es12b.cx.wajam/10.1.
16.154:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es12b.cx.wajam/10.1.16.154:9300]][discovery/zen/unicast] request_
id [0] timed out after [3750ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:342)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
[2012-12-18 14:13:17,948][WARN ][discovery.zen.ping.unicast] [es1b] failed to send ping to [[#zen_unicast_9#][inet[es18b.cx.wajam/10.1.
16.160:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es18b.cx.wajam/10.1.16.160:9300]][discovery/zen/unicast] request_
id [9] timed out after [3750ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:342)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
[2012-12-18 14:13:17,971][WARN ][discovery.zen.ping.unicast] [es1b] failed to send ping to [[#zen_unicast_28#][inet[es35b.cx.wajam/10.1
.16.55:9300]]]
```

14:13:13-14:14:00 - Restarting the missing node on the cluster one by one (8 of them)

14:14:08 - NullPointerException on Master and showing unassigned shards (not the one that are missing ?)

Logs from master :

```
java.lang.NullPointerException
        at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:75)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:198)
        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:70)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:188)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:138)
        at org.elasticsearch.cluster.routing.RoutingService$1.execute(RoutingService.java:135)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:223)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
```

Logs from one of the node that have missing shards

```
[2012-12-18 14:14:10,041][WARN ][discovery.zen            ] [es1b] received a cluster state from [[es22b][4FgfHy7vTOOgBZBPodjW2A][inet[/10.1.16.102:9300]]{master=true}] and not part of the cluster, should not happen
[2012-12-18 14:14:10,190][DEBUG][action.admin.indices.status] [es1b] [wajam][119], node[aGZztLJ0TOWe5qMaTvDsBg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@19661482]
org.elasticsearch.transport.RemoteTransportException: [es5b][inet[/10.1.13.135:9300]][indices/status/s]
Caused by: org.elasticsearch.indices.IndexMissingException: [wajam] missing
        at org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:244)
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:152)
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:59)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
[2012-12-18 14:14:10,199][DEBUG][action.admin.cluster.node.stats] [es1b] failed to execute on node [AaYgqtpFStuwxI6fMlJs9w]
org.elasticsearch.transport.RemoteTransportException: [es8b][inet[/10.1.13.138:9300]][cluster/nodes/stats/n]
Caused by: java.lang.NullPointerException
        at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:66)
        at org.elasticsearch.action.admin.cluster.node.stats.NodeStats.writeTo(NodeStats.java:290)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:91)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:67)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:276)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)

```

The last part is repeated for each missing shards.

14:16:00 - At this point I have a red cluster with all the nodes but 40 shards missing. (51, 210). Looking into the filesystem, the shards folders are not there anymore. I'm wondering what's happening, everything has happened kind of fast.... 
</description><key id="9382337">2496</key><summary>Shards deleted from index (and from disk) on cluster restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgagnon1</reporter><labels /><created>2012-12-18T21:25:36Z</created><updated>2013-02-27T15:38:52Z</updated><resolved>2013-02-27T15:38:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-18T21:28:07Z" id="11505967">Hi, we fixed a similar issue (the NPE that caused problems) in 0.20 final release, and in latest 0.19 cases. Though a rare situation to hit, seems like you hit it... .

A good practice is to configure the gateway.recover_after_nodes config to make sure full cluster restart based recovery will only start once there are enough nodes in the cluster.
</comment><comment author="jgagnon1" created="2012-12-18T21:39:31Z" id="11506392">Just to be clear, did you fixed the NPE or the indices deletion ? Or both at the same time ? I tried to find a Specific commit for that and didn't find exactly. I will update to the latest 0.20 before restarting indexation, but just to be sure.

Update; My recover_after_nodes is set to 15. What is the recommended value for it ? I also have discovery.zen.minimum_master_nodes set to 30     
</comment><comment author="kimchy" created="2012-12-18T21:44:18Z" id="11506587">@jgagnon1 we fixed the NPE, and then we also fixed a corner case of the deletion case.

Regarding recover_after_nodes, since it only applied during cluster "startup", then you can set it to a high value, something like 40 or even 50.
</comment><comment author="jgagnon1" created="2012-12-18T21:47:08Z" id="11506720">Thank you very much. I will update my ES version and this settings before restarting anything.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ShardSearchFailure handling of exception does not take actual into account for status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2495</link><project id="" key="" /><description>code does

if (actual != null &amp;&amp; actual instanceof ElasticSearchException) {
            status = ((ElasticSearchException) t).status();
        }

should that be

status = ((ElasticSearchException) actual).status();
</description><key id="9358895">2495</key><summary>ShardSearchFailure handling of exception does not take actual into account for status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mebigfatguy</reporter><labels><label>bug</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-18T06:43:54Z</created><updated>2012-12-26T23:01:02Z</updated><resolved>2012-12-26T23:01:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-26T22:52:55Z" id="11696203">Indeed!, will fix it shortly!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support timeout parameter in Multi Search API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2494</link><project id="" key="" /><description>It appears that the Multi Search API does not support the timeout parameter as the _search endpoint does.  It would be great to have it as a feature and for symmetry.
</description><key id="9356482">2494</key><summary>Support timeout parameter in Multi Search API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qhoxie</reporter><labels /><created>2012-12-18T03:28:34Z</created><updated>2012-12-29T01:11:23Z</updated><resolved>2012-12-29T01:11:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-26T23:04:15Z" id="11696385">You can provide the `timeout` as part of each search request body. Its not supported as a "top level" parameter since its considered to be a "body" level parameter, but we could potentially support that as well as a URI parameter for multi search, is that what you are after?
</comment><comment author="qhoxie" created="2012-12-29T01:11:23Z" id="11745661">I could see it making sense as a URI parameter here.  That said, this came from confusion on my part - I expected timeout to be a header parameter.  Thanks for the clarification.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>match_all filter with empty array (instead of obj) fires exception when used with facets </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2493</link><project id="" key="" /><description>Granted an empty object is the correct syntax, but unfortunately in PHP this can get a little confusing since an empty assoc array json_encode()'s to an object, but of course encodes to an array when empty.

Very confusing for a little while.

Interestingly, if the facets are not included then it works either way.

Example: https://gist.github.com/4323746

running ES 0.19.9 (yeah still...)
</description><key id="9354814">2493</key><summary>match_all filter with empty array (instead of obj) fires exception when used with facets </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>enhancement</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-18T01:31:37Z</created><updated>2012-12-27T14:46:58Z</updated><resolved>2012-12-26T23:35:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-26T23:34:57Z" id="11696875">We can easily fix it, will push a fix for master and 0.20.
</comment><comment author="gibrown" created="2012-12-27T14:46:58Z" id="11709215">Awesome, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tiny request: Create Java static variables (or enums) for missing sort values "_last" and "_first"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2492</link><project id="" key="" /><description>I like nice clean code without hardcoding arbitrary values that might change. The ElasticSearch codebase is excellent and makes good use of Java static variables. Do not want to nitpick, but I would love to see something like:

public class FieldSortBuilder extends SortBuilder {
    public static final String LAST = "_last"; 
    public static final String FIRST = "_first"; 

This way I can use the statics in my client-side code. On the server side, these Strings are used in places such as FloatFieldDataType and IntFieldDataType.
</description><key id="9353006">2492</key><summary>Tiny request: Create Java static variables (or enums) for missing sort values "_last" and "_first"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">brusic</reporter><labels /><created>2012-12-17T23:58:59Z</created><updated>2014-03-17T11:28:56Z</updated><resolved>2014-03-17T11:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T10:01:32Z" id="27376805">Hey Ivan,

is this still valid?
</comment><comment author="brusic" created="2013-10-30T23:10:38Z" id="27448020">I haven't looked at the recent code (and I am not in front of an IDE right now).  Is there an enum or static fields in use?

Not a bug, just a suggestion.
</comment><comment author="spinscale" created="2014-03-17T11:28:56Z" id="37805815">closing this, we use a `SortOrder` enum in the meantime...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 11 dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2491</link><project id="" key="" /><description>Hi
I just updated my pom file from version 9 to 11: 

&lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
            &lt;version&gt;0.19.11&lt;/version&gt;
&lt;/dependency&gt;

but elasticsearch doesn't download lucene libraries anymore and I get an Exception when I run my tests.
I found out in the lasticsearch jar there are some lucene classes included, but not all classes needed.
the Exception is:
Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/lucene/store/IndexInput
...
Caused by: java.lang.ClassNotFoundException: org.apache.lucene.store.IndexInput
... 

ES looks for that class but it can't find it.
The ES pom file changed and now doesn't include lucene libraries anymore and within the es jar file that class isn't included neither.
I had to add lucene manuually to my pom file.
</description><key id="9335959">2491</key><summary>elasticsearch 11 dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ttati</reporter><labels /><created>2012-12-17T15:19:54Z</created><updated>2013-05-23T14:45:48Z</updated><resolved>2013-05-23T14:45:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2012-12-28T17:33:17Z" id="11736743">published pom looks fine: 

https://oss.sonatype.org/service/local/repo_groups/public/content/org/elasticsearch/elasticsearch/0.19.11/elasticsearch-0.19.11.pom

Maybe local repo caching issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A get on a compressed binary field returns the binary field uncompressed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2490</link><project id="" key="" /><description>Indexing compressed data into a binary field and then issuing a get on the document returns the data uncompressed. The expected result would be to get data as indexed. A search for the same document returns the field correctly.

Full recreation:
https://gist.github.com/4317178
</description><key id="9328373">2490</key><summary>A get on a compressed binary field returns the binary field uncompressed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">moscht</reporter><labels /><created>2012-12-17T09:59:07Z</created><updated>2012-12-17T14:16:28Z</updated><resolved>2012-12-17T14:15:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-12-17T10:27:06Z" id="11436638">What version of ES are you using?
</comment><comment author="moscht" created="2012-12-17T11:00:21Z" id="11437517">0.19.11
</comment><comment author="martijnvg" created="2012-12-17T14:07:23Z" id="11442648">This issue is fixed in the 0.20 releases. The following commit fixed it:
https://github.com/elasticsearch/elasticsearch/commit/09e4036823ff24789c466de09a08340380aff483

It hasn't been back ported to the 0.19 branch. 
</comment><comment author="moscht" created="2012-12-17T14:16:28Z" id="11443003">Ok, I didn't see that one, so I just close the issue then.

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mlt api fails to work when routing isn't id based.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2489</link><project id="" key="" /><description>This simple gists demonstrates the issue: https://gist.github.com/4317085
</description><key id="9327957">2489</key><summary>Mlt api fails to work when routing isn't id based.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.13</label><label>v0.20.2</label></labels><created>2012-12-17T09:41:22Z</created><updated>2012-12-17T10:00:33Z</updated><resolved>2012-12-17T10:00:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>minimum_master_nodes does not prevent split-brain if splits are intersecting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2488</link><project id="" key="" /><description>G'day,

I'm using ElasticSearch 0.19.11 with the unicast Zen discovery protocol.

With this setup, I can easily split a 3-node cluster into two 'hemispheres' (continuing with the brain metaphor) with one node acting as a participant in both hemispheres.  I believe this to be a significant problem, because now `minimum_master_nodes` is incapable of preventing certain split-brain scenarios.

Here's what my 3-node test cluster looked like before I broke it:

![](https://saj.beta.anchortrove.com/es-splitbrain-1.png)

Here's what the cluster looked like after simulating a communications failure between nodes (2) and (3):

![](https://saj.beta.anchortrove.com/es-splitbrain-2.png)

Here's what seems to have happened immediately after the split:
1. Node (2) and (3) lose contact with one another.  (`zen-disco-node_failed` ... `reason failed to ping`)
2. Node (2), still master of the left hemisphere, notes the disappearance of node (3) and broadcasts an advisory message to all of its followers.  Node (1) takes note of the advisory.
3. Node (3) has now lost contact with its old master and decides to hold an election.  It declares itself winner of the election.  On declaring itself, it assumes master role of the right hemisphere, then broadcasts an advisory message to all of its followers.  Node (1) takes note of this advisory, too.

At this point, I can't say I know what to expect to find on node (1).  If I query both masters for a list of nodes, I see node (1) in both clusters.

Let's look at `minimum_master_nodes` as it applies to this test cluster.  Assume I had set `minimum_master_nodes` to 2.  Had node (3) been completely isolated from nodes (1) and (2), I would not have run into this problem.  The left hemisphere would have enough nodes to satisfy the constraint; the right hemisphere would not.  This would continue to work for larger clusters (with an appropriately larger value for `minimum_master_nodes`).

The problem with `minimum_master_nodes` is that it does not work when the split brains are intersecting, as in my example above.  Even on a larger cluster of, say, 7 nodes with `minimum_master_nodes` set to 4, all that needs to happen is for the 'right' two nodes to lose contact with one another (a master election has to take place) for the cluster to split.

Is there anything that can be done to detect the intersecting split on node (1)?

Would #1057 help?

Am I missing something obvious? :)
</description><key id="9326377">2488</key><summary>minimum_master_nodes does not prevent split-brain if splits are intersecting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">saj</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2012-12-17T08:15:54Z</created><updated>2016-02-26T16:31:11Z</updated><resolved>2014-09-01T14:58:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="moscht" created="2012-12-18T08:43:56Z" id="11478027">We also had at some point a similar issue, where minimum_master_nodes did not prevent the cluster from having two different views of the nodes at the same time. 

As our indices were created automatically, some of the indices were created twice, once in every half of the cluster with the two masters broadcasting different states, and after a full cluster restart some shards were unable to be allocated, as the state has been mixed up. This was on 0.17. so I am not sure, if data would still be lost, as the state is now saved with the shards. But the other question is what happens when an index exists twice in the cluster (as it has been created on every master).

I think we should have a method to recover from such a situation. As I don't know how the zen discovery works exactly, I can not say how to solve it, but IMHO a node should only be in one cluster, in your second image node 1 should either be with 2, preventing 3 from becoming master, or with node 3, preventing 2 from staying master.
</comment><comment author="tallpsmith" created="2012-12-18T20:37:35Z" id="11503993">see Issue #2117 as well, I'm not sure if the Unicast discovery is making it worse for you, but I think we captured the underlying problem over on that issue, but would like your thoughts too.
</comment><comment author="saj" created="2012-12-20T00:31:49Z" id="11555570">From #2117:

&gt; The split brain occurs if the nodeId(UUID) of the disconnected node is such that the disconnected node picks itself as the next logical master while pinging the other nodes(NodeFaultDetection).

Ditto.

&gt; The split brain only occurs on the second time that the node is disconnected/isolated.

I see a split on the _first partial isolation_.  To me, these bug reports look like two different problems.
</comment><comment author="trollybaz" created="2013-04-03T16:23:29Z" id="15846991">I believe I ran into this issue yesterday in a 3 node cluster- a node elects itself master when the current master is disconnected from it.  The remaining partipant node toggles between having the other nodes as its master before settling on one.  Is this what you saw @saj?
</comment><comment author="saj" created="2013-04-03T23:12:54Z" id="15871337">Yes, @trollybaz.

I ended up working around the problem (in testing) by using [elasticsearch-zookeeper](https://github.com/sonian/elasticsearch-zookeeper) in place of Zen discovery.  We already had reliable Zookeeper infrastructure up for other applications, so this approach made a whole lot of sense to me.  I was unable to reproduce the problem with the Zookeeper discovery module.
</comment><comment author="tallpsmith" created="2013-04-04T23:31:22Z" id="15930675">I'm pretty sure we're suffering from this in certain situations, and I don't think that it's limited to unicast discovery.  

We've had some bad networking, some Virtual Machine stalls (result of SAN issues, or VMWare doing weird stuff), or even heavy GC activity can cause enough pauses for aspects of the split brain to occur.   

We were originally running pre-0.19.5 which contained an important fix for an edge case I thought we were suffering from, but since moving to 0.19.10 we've had at least one split brain (VMware-&gt;SAN related) that caused 1 of the 3 ES nodes to lose touch with the master, and declare itself master, while still then maintaing links back to other nodes.  

I'm going to be tweaking our ES logging config to output DEBUG level discovery to a separate file so that I can properly trace these cases, but there have just been too many of these not to consider ES not handling these adversarial environment cases.

I believe #2117 is still an issue and is an interesting edge case, but I think this issue here best represents the majority of the issues people are having.  My gut/intuition seems to indicate that the probability of this issue occurring does drop with a larger cluster, so the 3-node, minimum_master_node=2 is the most prevalent case.

It seems like when the 'split brain' new master connects to it's known child nodes, any node that already has an upstream connection to an existing master probably should be flagging it as a problem, and telling the newly connected master node "hey, I don't think you fully understand the cluster situation".
</comment><comment author="brusic" created="2013-04-05T04:00:52Z" id="15937973">I believe there are two issues at hand. One being the possible culprits for a node being disconnected from the cluster: network issues, large GC, discover bug, etc... The other issue, and the more important one IMHO, is the failure in the master election process to detect that a node belongs to two separate clusters (with different masters). Clusters should embrace node failures for whatever reason, but master election needs to be rock solid. Tough problem in systems without an authoritative process such as ZooKeeper.

To add more data to the issue: I have seen the issue on two different 0.20RC1 clusters. One having eight nodes, the other with four.
</comment><comment author="tallpsmith" created="2013-04-05T05:11:18Z" id="15939253">I'm not sure the former is really something ES should be actively dealing with, the latter I agree, and is the main point here, in how ES detects and recovers from cases where 2 masters have been elected.  

There was supposed to have been some code in, I think, 0.19.5 that 'recovers' from this state by choosing the side that has the most recent ClusterStatus object (see Issue #2042) , but it doesn't appear in practice to be working as expected, because we get these child nodes accepting connections from multiple masters.

I think gathering the discovery-level DEBUG logging from the multiple nodes and presenting it here is the only way to get further traction on this case.  

It's possible going through the steps in Issue #2117 may uncover edge cases related to this one (even though the source conditions are different); at least it might be a reproducible case to explore.

@s1monw nudge - have you had a chance to look into #2117 at all... ?  :)
</comment><comment author="brusic" created="2013-04-05T16:27:54Z" id="15966183">Paul, I agree that the former is not something to focus on. Should have stated that. :) The beauty of many of the new big data systems is that they embrace failure. Nodes will come and go, either due to errors or just simple maintenance. #2117 might have a different source condition, but the recovery process after the fact should be identical.

I have enabled DEBUG logging at the discovery level and I can pinpoint when a node has left/joined a cluster, but I still have no insights on the election process.
</comment><comment author="tallpsmith" created="2013-05-24T05:43:42Z" id="18387558">suffered from this the other day when an accidental provisioning error had a 4GB ES Heap instance running on a 4GB O/S memory, which was always going to end up in trouble.  The node swapped, process hung, and the intersection issue described here happened.

Yes, the provisioning error could have been avoided, yes, probably use of mlockall may have prevented the destined-to-die-a-horrible-swap-death, but there's other scenarios that could cause a hung process (bad I/O causing stalls for example) where the way ES handles the cluster state is poor, and leads to this problem.

we hope very much someone is looking hard into ways to make ES a bit more resilient when facing these situations to improve data integrity... (goes on bended knees while pleading)
</comment><comment author="otisg" created="2013-05-24T15:30:05Z" id="18411948">Btw. why not adopt ZK, which I believe would make this situation impossible(?)?  I don't love the extra process/management that the use of ZK would imply..... though maybe it could be embedded, like in SolrCloud, to work around that?
</comment><comment author="brusic" created="2013-05-24T15:44:14Z" id="18412830">From my understanding, the single embedded Zookeeper model is not ideal for production and that a full Zookeeper cluster is preferred. Never tried myself, so I cannot personally comment.
</comment><comment author="s1monw" created="2013-05-24T16:04:55Z" id="18414153">FYI - there is a zookeeper plugin for ES
</comment><comment author="otisg" created="2013-05-24T16:06:18Z" id="18414233">Oh, I didn't mean to imply a _single_ embedded ZK.  I meant N of them in different ES processes. Right Simon, there is the plugin, but I suspect people are afraid of using it because it's not clear if it's 100% maintained, if it works with the latest ES and such.  So my Q is really about adopting something like that and supporting it officially.  Is that a possibility?
</comment><comment author="mpalmer" created="2013-05-24T21:46:57Z" id="18431848">@otisg: The problem with the ZK plugin is that with clients being part of the cluster, they need to know about ZK in order to be able to discover the servers in the cluster.  Some client libraries (such as the one used by the application that started this bug report -- I'm a colleague of Saj's) doesn't support ZK discovery.  In order for ZK to be a useful alternative in general, there either needs to be universal support of ZK in client libraries, or a backwards-compatible way for non-ZK-aware client libraries to discover the servers (perhaps a ZK-to-Zen translator or something... I don't know, I've got bugger-all knowledge of how ES actually works under the hood).
</comment><comment author="aochsner" created="2013-06-10T15:26:37Z" id="19205978">We've gotten into this situation twice now in our QA environment.  3 nodes.  minimum_master_nodes = 2.  Log flies at https://gist.github.com/aochsner/5749640 (sorry they are big and repetitive).  

We are on 0.9.0 and using multicast

As a bit of a walkthrough.  sthapqa02 was the master and all it noticed was that sthapqa01 went bye bye and never rejoined.  According to sthapqa02, the cluster was sthapqa02 (itself) and sthapqa03.  

sthapqa01 is what appeared to have problems.  It couldn't reach sthapqa02 and decided to create a cluster between itself and sthapqa03.  

sthapqa03 went along w/ sthapqa01 to create a cluster and didn't notify sthapqa02.  

So 01 and 03 are in a cluster and 02 thinks it's in a cluster w/ 03.  
</comment><comment author="kimchy" created="2013-08-13T23:46:16Z" id="22606103">just an update that this behaves much better in 0.90.3 with dedicated master nodes deployment, but we are working on a better implementation down the road (with potential constraints on requiring fixed dedicated master nodes by the nature of some consensus algo impls, we will see how it goes...).
</comment><comment author="tallpsmith" created="2013-08-14T02:56:27Z" id="22612241">@kimchy that sounds promising, I would love to to understand more of the changes in that 0.90.x series that is in this area to understand what movements are going on ?  Is there a commit hash you could point to that you can remember that I could peek at ?

By dedicated master node, do you mean nodes that _just_ perform the master role, and not data role? (so additional nodes on top of existing data nodes).  This would sort of mimic how adding Zookeeper as a Master Election co-ordinator works?
</comment><comment author="phungleson" created="2013-08-14T03:05:05Z" id="22612451">@kimchy Does 0.90.2 has the same features or they are only available in 0.90.3? 
</comment><comment author="brusic" created="2013-08-14T17:06:22Z" id="22650798">Shay, thanks for the update.

For us, the problem has gone away with the adoption of 0.90.2. The actual underlying problem might not have been fixed, but the improved memory usage with elasticsearch 0.90/Lucene 4 has eliminated large GCs, which probably were the root cause of our disconnections. No disconnections means no need to elect another master.
</comment><comment author="btiernay" created="2013-09-20T16:21:47Z" id="24822318">This situation happened to us recently running 0.90.1 with `minimum_master_nodes` set to `N/2 + 1`, with `N = 15`. I'm not sure what the root cause was, but this shows that such a scenario is probable in larger clusters as well.
</comment><comment author="trevorreeves" created="2013-10-18T09:28:37Z" id="26582621">We have been frequently experiencing this 'mix brain' issue in several of our clusters - up to 3 or 4 times a week.  We have always had dedicated master eligible nodes (i.e. master=true, data=false), correctly configured minimum_master_nodes and have recently moved to 0.90.3, and seen no improvement in the situation.

As a side note, the initial cause of the disruption to our cluster is 'something' to do with the network links between the nodes I imagine - one of the master eligible nodes occasionally loses connectivity with the master node briefly - "transport disconnected (with verified connect)" is all we get in the logs.  We haven't figured out this issue yet (something is killing the tcp connection?), but this explains the frequency with which we are affected by this bug as it seems its a double hit due to the inability for the cluster to recover itself correctly when this disconnect occurs.

@kimchy Is there any latest status on the 'better implementation down the road' and when it might be delivered?

Sounds like zookeeper is our reluctant interim solution.
</comment><comment author="tallpsmith" created="2013-10-22T01:04:52Z" id="26770242">just as I was beginning plans to go to a set of dedicated master-only nodes I ready @trevorreeves post where he's still hitting the same problem.  Doh!

Our situation appears to be IOWait related, in that a master node (also a data-node) hits an issue that causes extensive IOWait (a _scroll based search can trigger this, we already cap the # streams and Mb/second recovery rate through settings), the JVM becomes unresponsive.  The other nodes that are doing the Master Fault Detection are configured with 3 x 30 second ping timeouts, all of which fail, and then they give up on the master.

I'm not really sure what is stalling the master node JVM, particularly when I'm positive it's not GC related, it's definitely linked to heavy IOWait.  We have one node in one installation with a 'tenuous' connection to a NetApp storage backing the volume used by the ES local disk image, and that seems to be the underlying root of our issues, but it is the way the ES cluster is failing to recover from this situation and not properly reestabling a consensus on the cluster that causes issues (I don't mind any weirdness during times of whacky IO patterns that form the split brain so much as I dislike the way ES is failing to keep track of who thinks who's who in the cluster).

At this point, it does seem like the Zookeeper based discovery/cluster management plugin is the most reliable way, though I'm not looking forward to setting up that up to be honest.  
</comment><comment author="nik9000" created="2013-11-21T19:25:27Z" id="29015092">We haven't hit this but this report is worrying - is this being worked on?  This is the kind of thing that'd make us switch to Zookeeper.
</comment><comment author="brusic" created="2013-11-21T19:40:25Z" id="29016374">Just wanted to point out to Nik a comment in the other related issue: https://github.com/elasticsearch/elasticsearch/issues/2117#issuecomment-16078340

_"Unfortunately, this situation can in-fact occur with zen discovery at this point. We are working on a fix for this issue which might take a bit until we have something that can bring a solid solution for this."_

I wonder what has happened since then and if their findings correspond to my scenario.

For my clusters, split-brains always occur when a node becomes isolated and then elects themselves as master. More visibility (logging) of the election process would be helpful. Re-discovery would be helpful as well since I rarely see the cluster self heal despite being in erroneous situations (nodes belongs to two clusters_. I am on version 0.90.2, so I am not sure if I am perhaps missing a critical update although I do scan the issues and commits.
</comment><comment author="aphyr" created="2013-12-11T21:16:44Z" id="30364795">Could you do me a huge favor and _not_ patch this until, like, May or so? I need to finish some other things before the next installation of Jepsen. ;-)
</comment><comment author="bitsofinfo" created="2014-01-02T15:10:35Z" id="31458129">Is there any update on this or timeline for when it will be fixed?
</comment><comment author="mayurkup" created="2014-01-15T18:38:00Z" id="32394812">Ran into this very problem on a 4 node cluster.

Node 1 and Node 2 got disconnected and elected themselves as masters, 
Node 3 and 4 remained followers for both Node 1 and Node 2.

We do not have the option of running ZK.

Does anyone know the election process is governed (I know it runs off the Praxos Consensus algorithm) but in layman's term does each follower vote exactly once or do they case multiple votes?
</comment><comment author="amitelad7" created="2014-02-08T05:37:44Z" id="34534672">We just ran into this problem on a 41 data node and 5 master node cluster running 0.90.9
@kimchy is your recommendation to use zookeeper and not zen?
</comment><comment author="mayurkup" created="2014-02-17T04:13:48Z" id="35227751">@amitelad7 
You have a few options running at Zen, you can increases the fd timeouts/retries/intervals if your network/node is unresponsive. The other option is to explicitly define master nodes, but in the case of yours where you have 5 masters it may get tricky.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>First indexing of a dynamic boolean field can cause it not to be indexed correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2487</link><project id="" key="" /><description>Effectively, indexing it as lowercase `t` instead of upper case `T`. See repo here: https://gist.github.com/adba5adb18555033f1c1.
</description><key id="9313133">2487</key><summary>First indexing of a dynamic boolean field can cause it not to be indexed correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-16T02:56:47Z</created><updated>2012-12-16T03:04:17Z</updated><resolved>2012-12-16T03:04:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>*:* query not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2486</link><project id="" key="" /><description>With a vanilla ES 0.20.1, the `*:*` query is not working.

Steps to reproduce:

``` shell
#!/bin/zsh

# elasticsearch 0.20.1 vanilla

curl -s -XPUT "http://localhost:9200/testidx" -d '{"index":{"number_of_shards":1,"number_of_replicas":0}}'
echo

curl -s -XPUT "http://localhost:9200/testidx/msg/1?refresh=true" -d '{"id":"1","message":"test"}'
echo

# this shows the document exists
curl -s "http://localhost:9200/testidx/msg/1?pretty=true"
echo

# this returns 0 hits (should return 1)
curl -s "http://localhost:9200/testidx/msg/_search?q=*:*&amp;pretty=true"
echo

```
</description><key id="9299725">2486</key><summary>*:* query not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>bug</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-12-14T22:32:31Z</created><updated>2013-10-10T15:00:54Z</updated><resolved>2012-12-14T22:47:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Reduced memory consumption of ordinal arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2485</link><project id="" key="" /><description>Here is proposed to solution to the problem described in #2468 - the large of amount of memory consumed by the ordinal arrays of the field cache in highly uneven distribution.

I have introduced a new dedicated storage class ( src/main/java/org/elasticsearch/index/field/data/MultiValueOrdinalArray.java ) . The storage class' space complexity is linear in both the number of segment documents as the number of ordinals needed. 

Performance is identical to the current implementation if documents have no value or only a single one. For documents with two or more ordinals there is a slight per doc overhead due to some integer arithmetics. I suspect this is more then compensated by a more efficient traversing of memory, but this has to be measured (the class walks memory linearly within a single array rather jump through multiple arrays).

I've changed all the Field Data classes to use the new container. To keep impact on the code minimal, I have not changed the initial loading of the ordinals but rather compile the ordinal arrays into the new data store. This means that loading the Field cache initially has the same memory requirement as before.

Another caveat to note: at the moment you can store up to ~2^30 ordinals in it. For me this is not a problem, but it is a limitation the current implementation doesn't have (with enough memory). 

My hope is that you can see this as a drop in replacement for the next 0.20.x release as I believe it is equivalent to the current implementation but has a much better memory signature. 

To help integrating it,  I merged it already with the laters 0.20 branch (I also have a 0.19 variant, which we currently run in production) . All unit tests pass running this new code and we run it in production.

I know the team is busy with Lucene 4 but I hope you can take a couple of hours for this.  I'd love to able to go back to vanilla ES rather than have the overhead of running a custom build.

Let me know if I can help in any way,
Boaz
</description><key id="9296510">2485</key><summary>Reduced memory consumption of ordinal arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2012-12-14T20:27:12Z</created><updated>2014-06-15T08:51:53Z</updated><resolved>2013-04-04T14:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-01-10T14:39:08Z" id="12099216">+1

For the Lucene 4.0 branch, I'd also look at implementing these buddies:

http://blog.jpountz.net/post/25530978824/how-fast-is-bit-packing
https://issues.apache.org/jira/browse/LUCENE-4602
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>spurious terms in boolean field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2484</link><project id="" key="" /><description>I get 'T'/'F' and also 't' terms for a boolean field, leading to problems when filtering on the field. The below gist reproduces this:-

https://gist.github.com/4284891

This is on elasticsearch 0.20.1, a quick check on 0.19.10 and I can't reproduce this.
</description><key id="9287526">2484</key><summary>spurious terms in boolean field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ianAndrewClark</reporter><labels /><created>2012-12-14T15:17:18Z</created><updated>2012-12-16T03:12:26Z</updated><resolved>2012-12-16T03:12:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2012-12-14T15:49:04Z" id="11380686">isn't it duplication of #2462?
</comment><comment author="imotov" created="2012-12-14T15:57:39Z" id="11380992">I can reproduce it. It is different from #2462. The problem is that the first record is indexed as lowercase "t" instead of upper case "T". All consecutive records are indexed correctly. Looking into it. 
</comment><comment author="imotov" created="2012-12-16T03:12:26Z" id="11413648">Fixed. See #2487 for more details. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Explain in AllocationDecider's Decisions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2483</link><project id="" key="" /><description>Currently the Decision class already supports an explain parameter. It would be helpful for development, debugging and for certain error messages like in MoveAllocationCommand to log the actual reasoning behind the decision.
</description><key id="9254697">2483</key><summary>Use Explain in AllocationDecider's Decisions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2012-12-13T15:13:19Z</created><updated>2014-03-12T20:36:17Z</updated><resolved>2014-02-27T17:12:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-02-04T19:54:34Z" id="34099748">We should add an `explain` parameter so people can get detailed feedback about why a shard can or cannot be allocated to a node. Something like this:

### For these commands:

``` sh
curl -XPOST 'localhost:9200/_cluster/reroute?explain&amp;pretty' -d '{
  "commands" : [
    {
      "cancel" : {
        "index" : "decide", "shard" : 0, "node": "IvpoKRdtRiGrQ_WKtt4_4w"
      }
    },
    {
      "move" : {
        "index" : "decide", "shard" : 0,
        "from_node" : "IvpoKRdtRiGrQ_WKtt4_4w", "to_node" : "IvpoKRdtRiGrQ_WKtt4_4w"
      }
    }
  ]
}'
```

### This result:

``` json
{
  "explanations" : [ {
    "command" : "cancel",
    "parameters" : {
      "index" : "decide",
      "shard" : 0,
      "node" : "IvpoKRdtRiGrQ_WKtt4_4w",
      "allow_primary" : false
    },
    "decisions" : [ {
      "decider" : "CancelAllocationCommand",
      "decision" : "NO",
      "explanation" : "can't cancel [decide][0] on node [Wysper][IvpoKRdtRiGrQ_WKtt4_4w][Xanadu.local][inet[/172.16.1.8:9300]], shard is primary and started"
    } ]
  }, {
    "command" : "move",
    "parameters" : {
      "index" : "decide",
      "shard" : 0,
      "from_node" : "IvpoKRdtRiGrQ_WKtt4_4w",
      "to_node" : "IvpoKRdtRiGrQ_WKtt4_4w"
    },
    "decisions" : [ {
      "decider" : "SameShard",
      "decision" : "NO",
      "explanation" : "shard cannot be allocated on same node [IvpoKRdtRiGrQ_WKtt4_4w] it already exists on"
    }, {
      "decider" : "Filter",
      "decision" : "YES",
      "explanation" : "node passes include/exclude/require filters"
    }, {
      "decider" : "ReplicaAfterPrimaryActive",
      "decision" : "YES",
      "explanation" : "shard is primary"
    }, {
      "decider" : "Throttling",
      "decision" : "YES",
      "explanation" : "below shard recovery limit of [2]"
    }, {
      "decider" : "Enable",
      "decision" : "YES",
      "explanation" : "allocation disabling is ignored"
    }, {
      "decider" : "Disable",
      "decision" : "YES",
      "explanation" : "allocation disabling is ignored"
    }, {
      "decider" : "Awareness",
      "decision" : "YES",
      "explanation" : "no allocation awareness enabled"
    }, {
      "decider" : "ShardsLimit",
      "decision" : "YES",
      "explanation" : "total shard limit disabled: [-1] &lt;= 0"
    }, {
      "decider" : "NodeVersion",
      "decision" : "YES",
      "explanation" : "target node version [2.0.0-SNAPSHOT] is same or newer than source node version [2.0.0-SNAPSHOT]"
    }, {
      "decider" : "DiskThreshold",
      "decision" : "YES",
      "explanation" : "disk threshold decider disabled"
    }, {
      "decider" : "SnapshotInProgress",
      "decision" : "YES",
      "explanation" : "no snapshots are currently running"
    } ]
  } ]
}
```
</comment><comment author="dakrone" created="2014-02-04T19:55:35Z" id="34099856">Related: #4380
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>allow index and type to be specified as arrays in MultiSearchRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2482</link><project id="" key="" /><description>currently MultiSearchRequest requries to specify indices and types as comma separated. This PR allows specification as arrays too.
</description><key id="9248722">2482</key><summary>allow index and type to be specified as arrays in MultiSearchRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2012-12-13T11:09:11Z</created><updated>2014-07-16T21:54:17Z</updated><resolved>2012-12-27T00:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-27T00:19:39Z" id="11697444">Pushed to both master and 0.20.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node lose the cluster state after heap dump taken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2481</link><project id="" key="" /><description>Hi,
I had to take a heap dump using jmap on one of our cluster nodes. during the dump the other nodes detected that the node is not responsive and kicked it out of the cluster, Hence the cluster health was yellow.
After the heap dump has finished and the node become responsive again, the node never joins back the cluster. and calling GET /_cluster/health on the node returns green, as if it was never kicked out of the cluster.
</description><key id="9247436">2481</key><summary>Node lose the cluster state after heap dump taken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fasher</reporter><labels /><created>2012-12-13T10:16:21Z</created><updated>2014-07-08T18:50:27Z</updated><resolved>2014-07-08T18:50:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:50:27Z" id="48383425">Probably the node formed its own cluster, and so was green. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2480</link><project id="" key="" /><description>Hi,

I get sometimes some inconsistency when I delete and reload my entries right after.

It looks like elasticsearch delete the index asynchronously, and i'm reloading the indexes before the index is actually removed.

How can I avoid this behavior ?

Thanks in advance
</description><key id="9238336">2480</key><summary>Inconsistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">informatic-revolution</reporter><labels /><created>2012-12-13T00:53:09Z</created><updated>2013-06-06T14:22:06Z</updated><resolved>2013-06-06T14:22:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-03T13:07:00Z" id="14346675">what kind of inconsistencies are you seeing here, can you provide a gist / set of commands that show the problem?
</comment><comment author="informatic-revolution" created="2013-03-04T13:09:18Z" id="14379897">Ok,

I delete an entity with doctrine: 

$this-&gt;objectManager-&gt;remove($entity);

I redirect to a new route, which will list my entities:

$entities = $this-&gt;repository-&gt;findBy($criteria, $orderBy, $limite, $offset);

And I got (sometimes) the following exception:

\RuntimeException: 'Cannot find corresponding Doctrine objects for all Elastica results.'

which is located in 

/vendor/exercise/elastica-bundle/FOQ/ElasticaBundle/Doctrine/AbstractElasticaToModelTransformer

```
    $objects = $this-&gt;findByIdentifiers($ids, $this-&gt;options['hydrate']);
    if (count($objects) &lt; count($elasticaObjects)) {
        throw new \RuntimeException('Cannot find corresponding Doctrine objects for all Elastica results.');
    };
```

A page refresh will always solve the problem. After digging in, it appears that my doctrine entity does not exist anymore, but it has not been deleted yet in elastica.

I see 2 solutions:
1. 

Do not throw this exception, and just get the entities we have in common.

2.

Use Elastica_Index::refresh() as it is mentioned to this issue

https://github.com/ruflin/Elastica/issues/292
</comment><comment author="spinscale" created="2013-06-06T14:22:06Z" id="19048291">Closing as this looks like an elastica problem and your elastica ticket seems to be solved.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make elasticsearch.in.sh more configurable via env</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2479</link><project id="" key="" /><description>It's easier to manage a configuration through environment variables than
through a config file if that config file has static values that need to
change each version (such as the ES_CLASSPATH). Trying to keep that in
Chef is tedious whereas just controlling these few settings with
environment variables like most of the other settings in here is simple
and straightforward.
</description><key id="9237093">2479</key><summary>Make elasticsearch.in.sh more configurable via env</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">philk</reporter><labels /><created>2012-12-12T23:54:02Z</created><updated>2014-06-14T12:29:33Z</updated><resolved>2012-12-19T20:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2012-12-14T12:56:16Z" id="11375310">&gt; Trying to keep that in Chef is tedious whereas just controlling these few settings with environment variables like most of the other settings in here is simple and straightforward.

In Chef, why not keep the settings in the `node` attributes and just re-compile the template with the proper settings?
</comment><comment author="philk" created="2012-12-14T17:34:59Z" id="11384780">Then you'd need to have another node attribute to replace the version in ES_CLASSPATH at the top. Not a huge issue since we have the version in the cookbook already. The bigger issue for me is there is that by having this file as a template in Chef we stop tracking upstream automatically so if there are any additional settings added that make sense as defaults we might not notice them.

I also think that making these controlled by environment variables matches the pattern of the rest of the file better.
</comment><comment author="karmi" created="2012-12-15T06:26:47Z" id="11400577">&gt; The bigger issue for me is there is that by having this file as a template in Chef we stop tracking upstream
&gt; automatically so if there are any additional settings added that make sense as defaults we might not notice them.

Yes, that's a consequence. But you have a template for `elasticsearch.yml` anyway, and have to keep track of the additions.

&gt; I also think that making these controlled by environment variables matches the pattern of the rest of the file better.

Absolutely. In my Chef cookbook, I have a template looking like this: https://github.com/karmi/cookbook-elasticsearch/blob/master/templates/default/elasticsearch-env.sh.erb.
</comment><comment author="martijnvg" created="2012-12-18T22:04:38Z" id="11507489">Hey @philk this looks good. Just one thing can you remove the ES_USE_JAVA7 option? This specific gc option might be default in the future or even disappear. We shouldn't be exposing options that are Java version specific.
</comment><comment author="philk" created="2012-12-18T22:29:44Z" id="11508497">@martijnvg That seems reasonable. Done.
</comment><comment author="martijnvg" created="2012-12-19T20:43:25Z" id="11547520">@philk Thanks! I merged your last commit with your first commit. I pushed it to the master and 0.20 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermsQueryBuilder ambiguous constructor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2478</link><project id="" key="" /><description>One can not use TermsQueryBuilder("test", 1, 2) because of float and double versions variable argument constructors make them ambiguous in nature.

I would suggest to use array or List instead of variable arguments in constructor.
</description><key id="9228924">2478</key><summary>TermsQueryBuilder ambiguous constructor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">yadu</reporter><labels /><created>2012-12-12T19:31:33Z</created><updated>2013-10-18T21:02:01Z</updated><resolved>2013-10-18T21:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-02T16:14:04Z" id="14330492">I agree this is odd, I will look into it. Thanks for reporting
</comment><comment author="javanna" created="2013-10-18T16:40:38Z" id="26610681">A constructor accepting a `Collection` of values was added with #2978. I think we can close this one, do you agree @yadu ?
</comment><comment author="yadu" created="2013-10-18T20:48:18Z" id="26628525">Yes @javanna that makes sense.
</comment><comment author="javanna" created="2013-10-18T21:01:54Z" id="26629446">Cool, thanks for your feedback @yadu !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add alternative github download URL to PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2477</link><project id="" key="" /><description>The Github downloads are gone

https://github.com/blog/1302-goodbye-uploads

so the PluginManager should be able to fetch zips from alternative location under https://raw.github.com

Plugin authors will have to commit their zip binaries to this different location:

```
https://raw.github.com/{user}/{repo}/master/downloads/{pluginname}-{version}.zip
```

which corresponds to a ${basename}/downloads folder in a Maven project.

Cheers,

J&#246;rg
</description><key id="9217755">2477</key><summary>Add alternative github download URL to PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-12-12T14:42:06Z</created><updated>2014-07-10T09:16:20Z</updated><resolved>2013-10-11T15:12:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-10-11T13:23:26Z" id="26136398">Hey J&#246;rg,

I'm not sure this PR still applicable. What do you think? Should we close it?

Cheers
</comment><comment author="jprante" created="2013-10-11T15:12:38Z" id="26144985">Hey David, I think we can close this. Thanks! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolate: Document fields concatenating; causing unexpected matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2476</link><project id="" key="" /><description>To recreate the issue (against ES 0.20.0 RC1), please see this bash script: https://gist.github.com/4263982

Basically a document with field1:A and field2:B will match against a percolator for the phrase "A B" even though that exact phrase does not occur in either field, but only when the fields are concatenated.
</description><key id="9202896">2476</key><summary>Percolate: Document fields concatenating; causing unexpected matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brett--anderson</reporter><labels /><created>2012-12-12T01:17:27Z</created><updated>2013-03-03T13:06:12Z</updated><resolved>2013-03-03T13:06:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-03T13:06:12Z" id="14346661">this happens becaue you search on the _all field with your query and the `position_offset_gap` is set to 0 by default. set it to a larger value on the _all field and this should not happen. It's rather a feature than a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>recursive path match on dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2475</link><project id="" key="" /><description>We have a custom field where users can enter any data.  Within our database we store all keys/values of this custom dict as strings.  However, it's not possible to specify a dynamic_template which supports recursive/nested mappings.  This is a problem because custom.A may be a string for one customer, but a number for another customer.

Possibly add path_match_nested?

```
        # Index all custom fields as strings for now.                                                                                                              
        'dynamic_templates': [
            {   
                'custom_as_string': {                                                                                                                              
                    'path_match': 'custom.*',  
                    'path_match_nested': true,                                                                                                                    
                    'mapping': {                                                                                                                                   
                        'type': 'string',                                                                                                                          
                    }
                }
            }
        ], 
```
</description><key id="9199931">2475</key><summary>recursive path match on dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">anemitz</reporter><labels /><created>2012-12-11T23:08:08Z</created><updated>2014-07-08T18:49:28Z</updated><resolved>2014-07-08T18:49:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-28T11:04:37Z" id="23406265">Sorry for the late reply! 

I'm not sure I got what you mean, could you elaborate a bit more on what is the mapping you would like to obtain? Maybe you could add an example of the problem you encountered, what happens with your current dynamic template and what you would like to see instead?
</comment><comment author="clintongormley" created="2014-07-08T18:49:28Z" id="48383274">No feedback in 2 years. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>limit results by types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2474</link><project id="" key="" /><description>as described here http://stackoverflow.com/questions/13818892/elastic-search-limit-results-for-types

i would like to limit the number of results for each type

use case: 5 types in index, show max 3 results for each type

currently only the first type in result gets limited, and the total count of results behaves a little weird
</description><key id="9177028">2474</key><summary>limit results by types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bazo</reporter><labels /><created>2012-12-11T11:51:59Z</created><updated>2013-03-03T13:03:14Z</updated><resolved>2013-03-03T13:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-03T13:03:14Z" id="14346623">you should use multi search for this and filter only on a single type. http://www.elasticsearch.org/guide/reference/api/multi-search.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature Request] Add century interval in Date Histogram facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2473</link><project id="" key="" /><description>Heya

I would like to have a century interval option in the Date Histogram Facet.

``` javascript
{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "histo" : {
            "date_histogram" : {
                "field" : "field_name",
                "interval" : "century"
            }
        }
    }
}
```
</description><key id="9131723">2473</key><summary>[Feature Request] Add century interval in Date Histogram facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-12-10T08:31:08Z</created><updated>2014-09-18T08:01:36Z</updated><resolved>2014-02-03T07:42:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-09-23T08:44:51Z" id="24905770">It's already supported with `10y` or `100y` options. No need to `fix`!
</comment><comment author="dadoonet" created="2013-09-26T16:24:47Z" id="25181848">Reopening as we don't support `10y` or `100y` for dates.
That said we can use `3650d` for decades and `36500d` for centuries.
</comment><comment author="dadoonet" created="2014-02-03T07:42:32Z" id="33929841">Closing. I think we won't support `century` interval in date histogram facet. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A field with 'A' is not faceted correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2472</link><project id="" key="" /><description>``` bash
curl -X DELETE "http://localhost:9200/articles"
curl -X POST "http://localhost:9200/articles/article" -d '{"title" : "One",   "tags" : ["A"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"title" : "Two",   "tags" : ["A", "X"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"title" : "Three", "tags" : ["A", "X", "baz"]}'
curl -X POST "http://localhost:9200/articles/article" -d '{"title" : "Twelve", "tags" : ["A"]}'

sleep 1
curl -X POST "http://localhost:9200/articles/_search?pretty=true" -d '
  {
    "query" : { "query_string" : {"query" : "T*"} },
    "facets" : {
      "tags" : { "terms" : {"field" : "tags"} }
    }
  }
'
```

Produces

```
  "facets" : {
    "tags" : {
      "_type" : "terms",
      "missing" : 1,
      "total" : 3,
      "other" : 0,
      "terms" : [ {
        "term" : "x",
        "count" : 2
      }, {
        "term" : "baz",
        "count" : 1
      } ]
    }
  }
```

So the tag 'A' is never returned, but 'X' is returned just fine.
</description><key id="9120043">2472</key><summary>A field with 'A' is not faceted correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jamescasbon</reporter><labels /><created>2012-12-09T12:51:56Z</created><updated>2012-12-09T17:01:06Z</updated><resolved>2012-12-09T17:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-12-09T13:20:31Z" id="11170942">Please consider using the mailing list before creating issues.

See help section on elasticsearch.org site.

Your concern is relative to mapping.
You use a default mapping that analyze your documents as english docs and remove all english common words like a, this, and...

That's the reason here.
</comment><comment author="jamescasbon" created="2012-12-09T17:01:06Z" id="11172894">OK thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Could not open or load main class org.elasticsearch.bootstrap.ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2471</link><project id="" key="" /><description>When runing elasticsearch.bat in win7, I get error above.Thanks in advice.
</description><key id="9110670">2471</key><summary>Could not open or load main class org.elasticsearch.bootstrap.ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hua3505</reporter><labels /><created>2012-12-08T12:53:14Z</created><updated>2013-06-06T14:23:02Z</updated><resolved>2013-06-06T14:23:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-08T18:56:16Z" id="11162100">Can you share what version of elasticsearch are you using? Also, how did you install it?
</comment><comment author="spinscale" created="2013-06-06T14:23:02Z" id="19048359">Please provide more input, if this problem still exists with the current elasticsearch version.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to download plugins from github</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2470</link><project id="" key="" /><description /><key id="9105996">2470</key><summary>Failure to download plugins from github</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.1</label><label>v0.90.0.Beta1</label></labels><created>2012-12-08T00:43:08Z</created><updated>2012-12-08T00:43:43Z</updated><resolved>2012-12-08T00:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-08T00:43:43Z" id="11152480">Fixed by 3a20080fb6f2036e573612d962bdff5eea5710f2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>restore deleted plugin path modification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2469</link><project id="" key="" /><description /><key id="9105424">2469</key><summary>restore deleted plugin path modification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alambert</reporter><labels /><created>2012-12-08T00:05:11Z</created><updated>2014-07-16T21:54:18Z</updated><resolved>2012-12-08T00:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-08T00:16:39Z" id="11151928">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unrealistic high memory consumption for faceting of infrequent array fields with many members</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2468</link><project id="" key="" /><description>Hi,

Our ElasticSearch instance contains circa 240 million messages. Each message can have one or more tags id associated with it. These ids are stored as an array of integers and we facet it using the standard terms facet.

The facet cache size for this field is ~55GB which is highly surprising as only 5 million messages actually do have tags (tagging is manual). Also the total number of tag applications is only ~15 million. 

Yesterday our ES cluster died due to lack of memory. 

Researching it I have pin down the issue to the way the MultiValued*Field caches work - For every segment it allocates memory space which is proportionate to the max number of values per docs \* maxDocs of that segment. 

In our case we had 3 messages with 100 tags which caused ElasticSearch to allocate 100*24 million  integers on 3 of the 10 shards we use (27.5 GB in total ). The rest of the shards each had at least one message with ~50 tags which is less dramatic but has a similar high consumption.

I understand why the current MultiValueIntFieldData implementation is set as it is right now, but in our case it leads to extreme results.

We currently worked around it by delete the tags from the top 200 messages which reduced memory considerably but this a short term solution.

I have started working on a an alternative data structure which will solve things for us. I will submit a pull request as soon as it is ready.

Cheers,
Boaz
</description><key id="9092797">2468</key><summary>Unrealistic high memory consumption for faceting of infrequent array fields with many members</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2012-12-07T16:13:34Z</created><updated>2013-06-06T11:38:09Z</updated><resolved>2013-06-06T11:38:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2012-12-10T18:23:26Z" id="11210353">I was going to suggest you read a thread from the mailing list, but then I noticed you were the original author!

Most of the ES code is structured nicely to use plugin storage/algorithms.I haven't looked at your code yet, but it would be great to have a different underlying storage structure. One size does not fit all.
</comment><comment author="drewr" created="2012-12-10T21:19:21Z" id="11218239">@bleskes Have you seen #1531?
</comment><comment author="bleskes" created="2012-12-10T21:43:22Z" id="11219375">@drewr yes, I have. This is another problem. Here the number of possible values is limited and these are integer types. The source of the problem is in the distribution of values across documents - only around 2% of the docs have a value to begin with. Within those the average is 3 values per doc. The issue arrises when you have very few (one per shard is enough if placed in a big segment) outliers with 100 values.
</comment><comment author="drewr" created="2012-12-10T21:50:01Z" id="11219681">That ticket may not read that way to you, but that's exactly the issue it's meant to address.  We're working on addressing it in a configurable way.
</comment><comment author="bleskes" created="2012-12-10T22:27:08Z" id="11221351">@drewr I'm not sure what you mean but if I understand correctly, you treat the other issue as a ticket to open up multiple storage options for the face field cache. This would be awesome, I agree. 

The reason I said it is different is that it seems to me that gustavobmaia (author of the issue) run into problems becomes of faceting fields with many different values for which the current facets are not designed. 

But, splitting hairs is not important :) - do you have an ETA for that alternative storage or a branch I can pull the code from? It is really an urgent problem for us which I much rather solve with something that will eventually become standard ES code. Right now I'm gearing up to develop a patch (pull request promised when I'm done).

Thanks,
Boaz
</comment><comment author="drewr" created="2012-12-10T22:32:59Z" id="11221640">I see the confusion.  I wasn't comparing your symptoms to his; merely jumping to the underlying cause.

I can't promise a date for the Real Solution, so if you want to patch locally you're free to do so.  We're happy to look at your pull request, but don't expect much since that's an area being redesigned.
</comment><comment author="bleskes" created="2012-12-10T22:39:30Z" id="11221906">Understood - no expectations set. My only long term goal is to see this issue solved in standard way such that I can happily go back to "vanilla" ElasticSearch. 

Cheers,
Boaz
</comment><comment author="bleskes" created="2012-12-14T20:29:07Z" id="11391088">@drewr just made the promised pull request. I know you're busy, but I hope you can find a moment to glance at it.

Have a good weekend,
Boaz
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In which cases I should wait for yellow/green status?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2467</link><project id="" key="" /><description>Hello.

We're using elasticsearch in our integration tests (We are using "tire" gem for Rails.)
Typical workflow is:

(I)

1) delete index
2) add items to index
3) immediately search in that index

OR (II)

1) delete index
2) add items
3) change or remove items
4) immediately search.

As I told we use it in unit tests. So, search step executed immediately after updating index.. And we need search to return always same, correct result.

We are not getting correct results now. Sometimes there are 2 items in index, while we expected one. Sometimes there are errors like HTTP 500 No active shards. Failures are random.

I think you mention here
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wPKUJXiG2mw
that wait for yellow/green status should be used in this case.

also another link
http://grokbase.com/t/gg/elasticsearch/12ag6njpyz/no-active-shards-in-tests-using-the-java-api

Is there any documentation which will explain usage of health request in our workflow?
</description><key id="9052356">2467</key><summary>In which cases I should wait for yellow/green status?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/karmi/following{/other_user}', u'events_url': u'https://api.github.com/users/karmi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/karmi/orgs', u'url': u'https://api.github.com/users/karmi', u'gists_url': u'https://api.github.com/users/karmi/gists{/gist_id}', u'html_url': u'https://github.com/karmi', u'subscriptions_url': u'https://api.github.com/users/karmi/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/4790?v=4', u'repos_url': u'https://api.github.com/users/karmi/repos', u'received_events_url': u'https://api.github.com/users/karmi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/karmi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'karmi', u'type': u'User', u'id': 4790, u'followers_url': u'https://api.github.com/users/karmi/followers'}</assignee><reporter username="">vsespb</reporter><labels /><created>2012-12-06T12:46:57Z</created><updated>2014-07-08T18:48:56Z</updated><resolved>2014-07-08T18:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2012-12-06T13:06:22Z" id="11084879">Normally, a `refresh` call on the index is the only thing you need to do; _Tire_ integration tests use only `refresh`.

If you'd run the tests against multiple elasticsearch nodes, it well may be needed to `wait_for_status`. Are you sure the cluster is not formed as a side effect, for instance when developers' machines form a cluster, or the CI environment launches multiple elasticsearch nodes which form a cluster?

---

Original at karmi/tire#537.
</comment><comment author="vsespb" created="2012-12-06T13:19:57Z" id="11085205">Didn't get it - what is cluster forming here? Also maybe it's setup problem - where can I read about important configuration options for our workflow? Don't see anything here http://www.elasticsearch.org/guide/reference/setup/configuration.html ?
</comment><comment author="karmi" created="2012-12-08T11:54:29Z" id="11157892">Hello, two points here:
1. The issue with failing tests should be fixed in karmi/tire/#540. No Tire test should fail now.
2. If you're repeatedly re-creating an index with different settings, it may take time until it's propagated in the cluster, causing "random" errors. 

@kimchy I tend to close this issue, since I cannot re-create it. Calling `_refresh` on the index should be enough for all test-related cases; see this [discussion](https://github.com/karmi/tire/issues/537#issuecomment-11157644) (karmi/tire#537).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ES version in node info api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2466</link><project id="" key="" /><description /><key id="9052191">2466</key><summary>Expose ES version in node info api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.0</label></labels><created>2012-12-06T12:40:59Z</created><updated>2012-12-06T14:22:43Z</updated><resolved>2012-12-06T14:22:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose fragmenter option for plain highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2465</link><project id="" key="" /><description>The plain highlighter exposes the ability how text should be broken up in highlight snippets. There are two possible options:
- `simple` - Breaks up text  into same sized fragments.
- `span` -  Same as the `simple` fragmenter, but tries not to break up text between highlighted terms (this is applicable when using phrase like queries). This is the default.

Example

```
curl -XGET 'localhost:9200/_search' -d '{
   "query" : {
       "match" : {
           "body" : {
               "some text"
           }
       }
    },
    "highlight":{
       "fields":{
         "body":{
            "fragment_size":200,
            "fragmenter" : "simple"
         }
      }
   }
}'
```
</description><key id="9052175">2465</key><summary>Expose fragmenter option for plain highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.0</label></labels><created>2012-12-06T12:40:32Z</created><updated>2012-12-07T10:22:43Z</updated><resolved>2012-12-06T13:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add support for ignoring settings in system properties.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2464</link><project id="" key="" /><description>An elasticsearch node can be instructed to ignore settings specified in system properties by setting node.ignore_system_properties setting to true.

Fixes #2312
</description><key id="9039913">2464</key><summary>Add support for ignoring settings in system properties.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-12-06T00:50:51Z</created><updated>2014-07-16T21:54:20Z</updated><resolved>2012-12-06T14:48:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-12-06T15:57:27Z" id="11090948">Fixed in [master](https://github.com/elasticsearch/elasticsearch/commit/d947dfde2be9772b50052ff4dd4e72ef2ce83379) and [0.20](https://github.com/elasticsearch/elasticsearch/commit/8c75d688312ba5e56a3e4dbee616c42147e38a1a)
</comment><comment author="NickPadilla" created="2012-12-17T04:36:38Z" id="11430334">After testing this in the 0.20.1 release, it works as expected.  However the setting is called 

config.ignore_system_properties

Thanks Igor!
Nick
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added options to DynamicTemplate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2463</link><project id="" key="" /><description>Dynamic templates are a great feature because they make it possible to control the dynamic mapping capabilities of ElasticSearch. One of the issues with dynamic templates is that it is necessary to have several templates to handle even very small differences in mapping objectives. 
For example, four templates are needed to handle the cases 'analyzed', 'not_analyzed',
'analyzed+stored' and 'not_analyzed+stored'. 

This change overcomes this limitation by adding three new features to dynamic templates
1. Options that are merged into the mapping based on keys specified in the field name
2. The ability to capture substrings from the field name and use these to parameterize the mapping and options 
3. The ContentPath is made available to the DynamicTemplate for use in parameterization 

Consider the following example:

{
  "template_1":{
    "match": "____",
    "capture":"(([a-zA-Z0-9]+?)(?:_[a-zA-Z0-9]+)*)__[a-zA-Z0-9]+", 
    "options" :{
      "s":{"type":"string"},
      "l":{"type":"long"},
      "m":{"index_name":"{1}"},
      "n":{"index_name":"{path}.{2}"},
      "i":{"index":"not_analyzed"},
      "j":{"index":"no"},
      "t":{"store":true},
      "a":{"include_in_all":false},
      "1":{"boost":1.5},
      "d":{"dynamic":false}
    },
    "mapping": {
      "index_name":"{path}.{1}"
    }
  }
}

The match, path_match, unmatch, path_unmatch are unchanged.
Two new optional fields: 
"capture" : This specifies a regular expression with capture groups that are used to
capture substrings from the field name that are made available as paramaeters ({1},{2}, etc)
for substitution into option and mapping key/value pairs in the same way that {name} and
{dynamic_type} are currently.

"options" : A list of key/value pairs where keys are single character identifiers
and values are objects containing mapping options that are merged into the "mapping".

If the field name has a double underscore "__" then the DynamicTemplate class treats any characters following it as option identifiers. The following example "person" document and resulting mapping illustrate how this one options enabled dynamic template does what would otherwise require 5 dynamic templates without options. 

Notes 
1. The {path} parameter strips out the option specifiers. A {raw_path} is also available.
2. Fields 'name_first_sn' and 'name_last_sn1' both map to the index field 'details.name'

index/person/id1
{
  "details__d":{
    "salutation":"Mr",
    "name_first__sn":"",
    "name_last__sn1":"",
    "marital_status__sit":"married",
    "num_children__l":3
  }
}

mapping for type person
{
  "person" :{
    "properties" : {
      "details__d" : {
        "type":"object", 
        "dynamic":false,                           // option 'd'
        "properties" {
          "salutation" : {
        "type":"string";                       // Not picked up by match, default mapping
          },
          "name_first__sn": {
            "type":"string",                       // option 's' 
            "index_name":"details.name"            // option 'n'
          },
          "name_last__sn1": {
            "type":"string",                       // option 's'
            "index_name":"details.name",           // option 'n'
            "boost":1.5                            // option '1'
          },
          "marital_status__sit": {
            "type":"string",                       // option 's'
            "index_name":"details.marital_status", // mapping
        "index":"not_analyzed",                // option 'i'
            "store":true                           // option 't'
          },
          "num_children__l":{
            "type":"long",                         // option l  
            "index_name":"details.num_children",   // mapping
          }
        }
      }
    }
  }  
}
</description><key id="9004909">2463</key><summary>Added options to DynamicTemplate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mauaht</reporter><labels /><created>2012-12-05T00:58:33Z</created><updated>2014-07-08T18:48:38Z</updated><resolved>2014-07-08T18:48:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:48:38Z" id="48383148">Hi @mauaht 

Thanks for the PR, but this just feels like way too complex and crytic an interface.  I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet value of boolean mapped fields should be "true" or "false"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2462</link><project id="" key="" /><description>When performing the following index and search operations I would expect the facet values of boolean fields to be "true" or "false" and not "T" and "F", since "T" and "F" are implementation detail of the BooleanFieldMapper.

To reproduce (using any version I tested, including current master):

```
curl -XPUT http://localhost:9200/boolfacet
curl -XPUT http://localhost:9200/boolfacet/auto/1 -d '{"foo":true}'
curl -XPOST http://localhost:9200/boolfacet/auto/_search?pretty=true -d '{"query":{"match_all":{}}, "facets":{"bar":{"terms":{"field":"foo"}}}}' 
```

``` json
{
   "took" : 1,
   "timed_out" : false,
   "_shards" : {
     "total" : 5,
    "successful" : 5,
     "failed" : 0
   },
   "hits" : {
     "total" : 1,
     "max_score" : 1.0,
     "hits" : [ {
       "_index" : "boolfacet",
       "_type" : "auto",
       "_id" : "1",
       "_score" : 1.0, "_source" : {"foo":true}
     } ]
   },
   "facets" : {
     "bar" : {
       "_type" : "terms",
       "missing" : 0,
       "total" : 1,
       "other" : 0,
       "terms" : [ {
         "term" : "T",
         "count" : 1
       } ]
     }
   }
 }
```

Note the "term": "T" line in the facets.
A preliminary patch follows shortly.
</description><key id="8981493">2462</key><summary>Facet value of boolean mapped fields should be "true" or "false"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kroepke</reporter><labels /><created>2012-12-04T12:02:32Z</created><updated>2014-10-15T19:43:31Z</updated><resolved>2014-07-04T10:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pcomte" created="2012-12-05T10:03:52Z" id="11035792">Hi,

Any idea when this will be included in the master branch?

Thanks!
</comment><comment author="martijnvg" created="2012-12-06T16:24:41Z" id="11092274">@kroepke This makes sense. We will get this in after the lucene4 refactorings.
</comment><comment author="kroepke" created="2012-12-06T19:56:46Z" id="11101141">@martijnvg ok, great. Until then we will build our own version with that patch then. 
Thx!
Let me know if you want me to clean anything up when this comes up again. 
</comment><comment author="kroepke" created="2012-12-07T10:59:38Z" id="11126143">A workaround for this problem is using the following script in the term fact.
Not pretty, but it solves the immediate problem for boolean mappers.

``` json
{
  "query": {
    "match_all": {}
  },
  "facets": {
    "bla": {
      "terms": {
        "term": true,
        "size": 10,
        "script": "doc['foo'].value == 'T' ? true : false"
      }
    }
  }
}
```

Results in:

``` json
{
  "facets": {
     "bla": {
        "_type": "terms",
        "missing": 0,
        "total": 1,
        "other": 0,
        "terms": [
             {
                 "term": "true",
                 "count": 1
              }
         ]
    }
}
```
</comment><comment author="karmi" created="2012-12-07T12:20:46Z" id="11128064">@kroepke Just interested, how does using `script` affect performance, do you have any numbers or a guess?
</comment><comment author="kroepke" created="2012-12-10T12:58:54Z" id="11192565">@karmi We haven't done any benchmarks yet, we hope to get this deployed the coming week.
</comment><comment author="kroepke" created="2012-12-13T16:22:03Z" id="11340880">@karmi I've just done a very unscientific test, using the queries above.
The index has 213198 documents and is 1.9gb in size.
The value distribution is extremely uneven towards "false" (upwards of 99% false), but that shouldn't actually matter too much.
It's running on a single server, all data fits into memory, nothing else is using that machine. Unfortunately I forgot to change the shard count, which is the default 5, so this is 5 shards on one box.
Without the script response times around between 8 and 9 ms, with the script field it's consistently between 48 and 50 ms, so the overhead is considerable.
It may or may not be feasible for people to use the workaround, but we will have to go with a patched version until this gets fixed upstream.
</comment><comment author="karmi" created="2012-12-13T16:26:23Z" id="11341096">@kroepke Thanks for the numbers! That is still quite a good performance, considering it has to scan all the matching documents and aggregate the counts.
</comment><comment author="s1monw" created="2013-05-03T21:17:43Z" id="17418763">this seems to be open for a long time... I will look into that soonish
</comment><comment author="lmenezes" created="2013-06-04T08:39:55Z" id="18896095">@s1monw 
Any updates on that? 
I understand that changing this at this point could lead to problems to people who count on this values being returned the way it is.
Perhaps the mapping could be configured to either map to true/false or not, and defaulting to the current behavior? 
</comment><comment author="kimchy" created="2013-06-04T12:38:48Z" id="18906027">our plan is to look into this for the 1.0 release. Your solution is definitely a possible way for us to support it.
</comment><comment author="kroepke" created="2013-06-05T12:10:48Z" id="18971496">The following request illustrates the new behavior:

```
curl -XPOST http://localhost:9200/boolfacet/auto/_search?pretty=true -d '{"query":{"match_all":{}}, "facets":{"bar":{"terms":{"field""foo", "map_boolean_values":"true"}}}}'
```

``` json
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "boolfacet",
      "_type" : "auto",
      "_id" : "1",
      "_score" : 1.0, "_source" : {"foo":true}
    } ]
  },
  "facets" : {
    "bar" : {
      "_type" : "terms",
      "missing" : 0,
      "total" : 1,
      "other" : 0,
      "terms" : [ {
        "term" : "true",
        "count" : 1
      } ]
    }
  }
}
```

Without the flag it's back to 'T' and 'F', also of course if it's set to 'false'.
Since you don't merge pull request here, feel free to pick that commit apart. Also I'm not sure what else needs to be done to properly support it in all code paths.
If you tell me what to test with I'd be very happy to implement the missing bits.
</comment><comment author="s1monw" created="2013-06-06T06:57:03Z" id="19028041">hey there, I just briefly looked at the pull request and I wonder if we can make this a little less intrusive. At the end of the day what counts here is what kind of representation we have to "True" and "False" so we could just load this through FieldData adding a feature next to the FieldDataFilter. I'd call it a _FieldDataTransformer_ that you can use to transform `"T" to "True"' when field data is loaded then your facets would just naturally come back as a full boolean string?
</comment><comment author="kroepke" created="2013-06-06T09:23:30Z" id="19033847">I'm not sure I can follow you there :)

From what I can see you still need to have the field mapper to do that for you, since that's what it does, no?
Or do you suggest moving the responsibility for resolving the conversion somewhere else?

I'm asking because this problem affects every field that stores values in a different representation, e.g. IPs. For those you get back the int based representation, which I actually find worse than the boolean case...
</comment><comment author="clintongormley" created="2013-06-06T11:32:09Z" id="19039334">Hiya @kroepke 

Been talking to @s1monw about this and two features seem to make sense:

## FieldData Transformer:

You would be able to specify a `script` on a field which allows you to transform each value as it is loaded into the fielddata cache.  This would be useful for, eg:
1. converting "T"/"F" to "true"/"false"
2. truncating `not_analyzed` strings to eg 15 characters for sorting purposes (on the fly, as opposed to having to reindex with the `truncate` token filter
3. capitalizing tokens for display
4. rounding datetimes to (eg) seconds instead of milliseconds

The benefit here is that there would be a one-time cost of conversion (just when loading the fielddata) and the same values would be exposed for facets, scripting and sorting.

## Facet Value Transformer:

A script which is applied just before the serialization of the facet responses into JSON which could, eg:
1. convert "T"/"F" to "true"/"false"
2. convert longs representing IPv4 addresses into their string representations
3. capitalize tokens for display
4. format datetime longs as date strings

These would be more efficient than the current facet scripts because they would only be run on the return values, not on every value. However, you would still have the raw longs loaded into memory which are more useful for calculations (eg easier to calc netmasks on a long rather than on a string IPv4 address)

I believe that the value transformers are planned for the new facets that we are planning for 1.0 (@uri?)
</comment><comment author="lmenezes" created="2013-06-06T11:50:11Z" id="19040020">The idea of having to specify a script for executing these mappings feel awkward, imho.

I think that everybody using elastic search will eventually run into this issue, and the script solution seems more like a work around rather than an "option" of something that should be transparent in the first place.

I could see this feature as a generic transformer(which i like) for the facets, but for elastic search builtin types(boolean, ip..) I think this should be done without the need of defining anything(at most a "flag", saying you want it to be done, and even that is just because of backward compatibility).

If internally, this "automatic" transformation for the builtin types is done using the script implementation you just described, cool. but having everybody defining its own script for mappings that most likely everybody will do, feels wrong.

that's my 2 cents anyway...

@clintongormley maybe the wrong uri :D @uboness 
</comment><comment author="clintongormley" created="2013-06-06T11:57:51Z" id="19040351">(heh, too many nicks in too many places)

Yes, I understand what you mean about the core types in ES.  I think `boolean` and `ip` are the only two types that this really applies to. `"true"` and `"false"` are less surprising than `"T"` and `"F"`, and IP addresses expressed as longs are pretty much useless in facets.

Perhaps for these two types, this should be the default return value in the new aggregations rewrite.  We don't have to maintain backward compatibility here because this is completely new functionality.  The old facets will continue to work as before.

For any other "label" transformations, the only generically useful way of doing them is via a script, as everybody will have their own particular requirements which would result in an explosion of options.
</comment><comment author="lmenezes" created="2013-06-06T12:02:02Z" id="19040516">@clintongormley Agreed. This way makes complete sense and I like the script approach as a generic way of doing this. Looking forward to it.
</comment><comment author="kroepke" created="2013-06-06T12:23:19Z" id="19041465">@clintongormley In fact this applies to dates as well, if you use terms facets (even if you might argue that terms facets on dates could be done with the date histogram facet instead).
While I see the value of what @s1monw is proposing, I have to agree with @lmenezes that it feels wrong to have to fix this with specifying it for every query just to get back what I put in.
The use case of doing netmask calculations is a valid one, but right now I consider terms facets to be broken because they leak internal representation. I doubt that it's useful to keep this for backwards compatibility reasons beyond 1.0, especially since most of the code has been touched in the last 7 months anyway.
(Sorry for sounding a little bitter, but this has been a continuous problem for us.) 
</comment><comment author="clintongormley" created="2013-06-06T12:27:25Z" id="19041638">@kroepke As I understand it, there are also plans for converting datetimes into strings, by specifying formats (rather than scripts).  However, for (eg) charting libraries, it is usually more efficient to hand them millis-since-the-epoch instead of date strings, so the default return value will probably remain as longs.
</comment><comment author="gpopovic" created="2014-01-04T14:49:17Z" id="31579825">Any news on this?
</comment><comment author="uboness" created="2014-01-10T02:44:01Z" id="31998484">we'll work on this one post 1.0
</comment><comment author="s1monw" created="2014-07-04T10:28:43Z" id="48029184">We deprecated facets already in `1.3` we decided to close this. We will fix this in aggregations instead.
</comment><comment author="aaneja" created="2014-10-08T23:25:57Z" id="58443806">Any updates on which ES release will have this ?
</comment><comment author="clintongormley" created="2014-10-15T19:43:31Z" id="59263866">@aaneja will probably be 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard query on non existent field matches all documents.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2461</link><project id="" key="" /><description>It turns into ConstantScore(NotDeleted(_:_)) and matches everything.
Shouldn't it return no hit?

https://gist.github.com/4200358

We are using 0.19.11. Please note that 0.19.4 returns no hit.
</description><key id="8973052">2461</key><summary>Wildcard query on non existent field matches all documents.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IoriH</reporter><labels><label>bug</label><label>v0.19.13</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-12-04T04:01:51Z</created><updated>2012-12-07T18:36:48Z</updated><resolved>2012-12-07T18:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-12-07T17:28:41Z" id="11138559">Its a bug, will work on fixing it across versions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes #2361 - geo_shape filter does not match false positives.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2460</link><project id="" key="" /><description>This fixes #2361, geo_shape filter should not match false positives, by indexing the shape's WKB and doing an actual comparison between the doc's shape and the filter's shape to verify results from the spatial prefix tree.  This comparison is optional, and used only when "fuzzy"="false" (fuzzy matching is "true" by default), as there will be a slight performance hit when used.

I would prefer you do not actually merge this, because there are a few things I don't love about the implementation; I wasn't able to find the appropriate solution by myself but instead request your advice about the best way to solve this problem:
1. Right now I base64-encode the WKB into a `StringFieldMapper`, which is wasteful. I had preferred to use the `BinaryFieldMapper`, but that gives "Fields with BytesRef values cannot be indexed at org.apache.lucene.document.Field.&lt;init&gt;(Field.java:222)".
2. I'd prefer to only store, _but not index,_ the WKB (with whatever mapper gets used).  However it seems that the fieldDataCache can only read indexed fields.  I am still not clear on the difference.

Once we establish the best way to resolve bullet 2, should this non-fuzzy filter be cachable or not?  Which data, precisely, is getting cached in memory (the Geometrys themselves or just a bitmask over docs)?  My inclination is not to cache by default, assuming that repetition of the exact same query shape will be low, but this depends on the end use case.

Please provide guidance around these concerns and I can iterate on this patchset and provide a follow-up pull request afterwards.
</description><key id="8959151">2460</key><summary>Fixes #2361 - geo_shape filter does not match false positives.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IamJeffG</reporter><labels /><created>2012-12-03T18:37:30Z</created><updated>2014-06-18T13:31:31Z</updated><resolved>2014-05-06T11:42:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-06T11:42:00Z" id="42292595">Fixed by #2803
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Experimental Snappy Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2459</link><project id="" key="" /><description>Snappy support was experimental, and proved to be the same in terms of performance (vs. LZF) and actually a bit worse when it comes to compression rate. Since the snappy module uses native code, and only applies where compiled, and the fact that its an overhead for us to maintain, we are deprecating it for 0.19, and 0.20, and remove it post 0.20.
</description><key id="8945814">2459</key><summary>Deprecate Experimental Snappy Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-12-03T11:14:34Z</created><updated>2012-12-05T22:59:29Z</updated><resolved>2012-12-03T11:22:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rb2k" created="2012-12-04T15:47:24Z" id="11002013">Is there a migration path from snappy -&gt; LZF?
</comment><comment author="kimchy" created="2012-12-04T16:13:00Z" id="11003168">@rb2k yea, you can change the compression type setting, and then run optimize with `max_num_segments` set to 1.
</comment><comment author="jprante" created="2012-12-05T22:59:29Z" id="11065148">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Shared Gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2458</link><project id="" key="" /><description>Shared gateways (shared FS storage or S3 for example) are problematic performance wise since they constantly need to snapshot the state of the index to a shared location, and then use that as the system of record. The local gateway on the other hand doesn't need it, and performs much better.

The main benefit of a shared gateway is the fact that the data is actually stored on another persistent location (i.e. using ephemeral disks on AWS, but still having the data on s3), but then its actually abusing the shared gateway design (to be used as a backup).

In the near future, we will have a proper snapshot(backup)/restore API, which will be the proper way to do backups, but relaying on the shared gateway for that is problematic. Note, backups can still be made by "rsync" the data location for each node "manually".
</description><key id="8944937">2458</key><summary>Deprecate Shared Gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-12-03T10:43:29Z</created><updated>2013-12-17T14:19:03Z</updated><resolved>2012-12-03T10:44:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ejain" created="2012-12-04T17:39:00Z" id="11006833">Is there an open issue for the snapshot/restore API yet?
</comment><comment author="jgriswoldinfogroup" created="2012-12-10T16:06:15Z" id="11202348">Why would you deprecate this feature prior to the availability of a backup/restore API?
</comment><comment author="fatemehmd" created="2012-12-13T06:27:10Z" id="11323926">Is there any tutorial on how to configure instances with EBS now that S3 is not an option?
</comment><comment author="ejain" created="2012-12-13T18:31:20Z" id="11346598">On Wed, Dec 12, 2012 at 10:27 PM, Fatemeh notifications@github.com wrote:

&gt; Is there any tutorial on how to configure instances with EBS now that S3 is not an option?

deprecated != removed

I do hope the backup/restore feature is implemented before support for
the S3 gateway is removed.
</comment><comment author="karmi" created="2012-12-21T12:13:45Z" id="11610914">@fatemehmd The http://www.elasticsearch.org/tutorials/2012/03/21/deploying-elasticsearch-with-chef-solo.html tutorial now walks exactly through that scenario, via the support in the Chef [cookbok](https://github.com/karmi/cookbook-elasticsearch).
</comment><comment author="youurayy" created="2013-01-06T15:22:45Z" id="11929669">Never noticed this in the logs, now my S3-gateway-configured cluster crashed after running out of JVM memory on one of the nodes.

Would be beneficial for other users to add this deprecation into the docs.
</comment><comment author="kimchy" created="2013-01-06T22:46:23Z" id="11935809">Indeed, deprecate does not mean we are going to remove it. It will not be removed before we have the snapshot/restore API, but even before, I would suggest running in local gateway mode on EBS for example, compared to using the s3 gateway, because of the overhead that comes with continuously snapshotting to it and treating it as the main source of truth.

@ypocat the OOM should not be caused because of the s3 gateway case, it probably happened because of other reasons (common one is faceting on fields that end up abusing memory, we are working on that as well...)
</comment><comment author="truthtrap" created="2013-03-11T08:16:18Z" id="14701423">We are actually quite happy with the S3 gateway. We use it with all of the clusters we run. The main use is as a backup, that we can restore from when the cluster hangs or dies. The advantage of this approach is that we are extremely flexible in how we work with nodes.

Working with EBS is not a solution. It would require a very complicated automated setup that manages instance with additional EBS volume(s). It is an approach we often use for things like Postgres and MongoDB. But part of the elasticsearch enthusiasm we feel is the ease of working with cluster technology.

A snapshotting feature is a good replacement. It would really great if we could have some sort of Point in Time Restore with it, but it is not (yet) required. I would like to ask you to leave some overlap of features after you release snapshotting. We do rely on S3 when we upgrade our clusters, for example.

So, please, at least one release with snapshotting and the (deprecated) S3 gateway.
</comment><comment author="karmi" created="2013-03-11T08:57:55Z" id="14702532">&gt; Working with EBS is not a solution. It would require a very complicated automated setup that manages instance with additional EBS volume(s).

I can understand why EBS volumes are not a good option in many scenarios, either from technical or economical standpoint. However, I'd say that the provisioning overhead is really low. Given how good abstraction the Fog (Ruby), jClouds (Java) and other libraries provide, I wouldn't describe it as "very complicated"...
</comment><comment author="truthtrap" created="2013-03-11T10:15:38Z" id="14705355">I don't want to discuss complexity of AWS related issues here. But, if you want to build a cluster-wide 'snapshot mechanism' with EBS that keeps the flexibility of the ElasticSearch (in combination with the AWS Cloud Plugin) you are in for quite a ride.

If you just want persistence of a node, 'plain EBS' is fine. Unfortunately, that is not enough for us. We want to scale a cluster (OUT or IN) within a couple of minutes. We need to be able to rotate all instances in a cluster very easily, without worrying about the data. We have to be able to replace a non-responsive ElasticSearch node by terminating the instance. Etc.

(If you are interested how we approach these things you can read [Resilience &amp; Reliability on AWS](http://www.amazon.com/Resilience-Reliability-AWS/dp/1449339190/). It has a dedicated chapter on ElasticSearch, just to show how incredibly impressed we are with it. Most of the work was already done.)
</comment><comment author="ejain" created="2013-03-11T23:01:41Z" id="14748686">I'll second that setting up EBS complicates things in a setup where nodes are added and removed frequently, especially if performance is an issue.
</comment><comment author="kimchy" created="2013-03-12T00:10:29Z" id="14751243">Snapshotting to s3 would bring both the advantages of the local gateway with the s3 gateway. we won't remove the s3 gateway before Snapshotting is in place at least for one major version
</comment><comment author="youurayy" created="2013-03-13T02:44:41Z" id="14818705">Just to add my 2 cents, the S3 shared gateway did not prevent my cluster from crashing into an irreparable state. I had to code an utility which went through the Lucene index files on disk and recovered / reindexed the data into a freshly initialized cluster. I believe I am much better off with the local gateway and daily snapshots of my EBS RAID5 arrays.
</comment><comment author="kimchy" created="2013-03-13T04:11:29Z" id="14823515">the idea here is that snapshot/restore with local gateway allows to strike the right balance between keeping up to date local recoverability with long term recoverability from something like s3
</comment><comment author="truthtrap" created="2013-03-14T08:09:19Z" id="14890725">@shay, thanks for leaving some overlap in the current s3 gateway and the
new snapshotting feature :)

for us 'local recoverability' is on shard level. we will always plan for
loss of an instance, without loosing the cluster. the cluster can recover
itself. we choose to treat nodes as ephemeral. with full cluster BREAKDOWN
a little bit of lag is not a problem. and for full cluster SHUTDOWN we can
manage this properly ourselves.

we are extreme fans of EBS, actually. and there is another interesting
application for EBS, and that is performance. ephemeral is a lot slower
with most rdbms we tried, for example. so, perhaps EBS is necessary in
cases of severe disk access. AWS has SSD ephemeral disks, but that is still
a bit above budget for most of our apps.

another interesting feature of EBS is that you can easily have 20 smaller
volumes, for the same price as a big volume. because of the nature of EBS
you increase your potential read/write throughput more or less linearly.
this principle could be applied to individual indexes, or even shards, if
they can be assigned to different parts on the filesystem. this would be
better manageable than raid, in case local (instance) recoverability is an
issue.

groet,
jurg.

On Wed, Mar 13, 2013 at 5:11 AM, Shay Banon notifications@github.comwrote:

&gt; the idea here is that snapshot/restore with local gateway allows to strike
&gt; the right balance between keeping up to date local recoverability with long
&gt; term recoverability from something like s3
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2458#issuecomment-14823515
&gt; .
</comment><comment author="oravecz" created="2013-04-01T13:08:54Z" id="15714616">We have been using the S3 shared gateway as a backup since 2010 in production. Our use case is perhaps a bit different from some ES users because we use ES to store smallish amounts of data. We also deploy to Elastic Beanstalk so instances are created and destroyed by Amazon and snapshotting and reuse of EBS is not appropriate. Sometimes we deploy a memory-only store with ES which only can rely on the shared gateway for any kind of cluster recovery.

I am hopeful that the S3 gateway will not go away altogether, or perhaps it is replaced with the snapshot to s3 that Shay had mentioned. My question however is what is the difference between the S3 Gateway now and the "Snapshot to S3" feature besides the frequency with which they will sync (which is customizable for the shared gateway)?
</comment><comment author="kimchy" created="2013-04-01T21:30:46Z" id="15738549">@oravecz effectively, a schedule snapshot to s3 using the future snapshot API will work in a similar manner to s3 gateway. Recovery will work a bit differently, where if you loose all the cluster data (loose all instances with ephemeral drives), you will need to explicitly "call recover" on the new cluster to recover the data from s3.
</comment><comment author="thomaswitt" created="2013-10-13T23:40:04Z" id="26229881">To be honest, we're not a big fan of the more EBS-centric way of running ElasticSearch.

Please do consider that nearly every major downtime at AWS had to do something with EBS (often in conjunction with the loss of data). EBS is - in my opinion - one of the most flawed services (just google "aws downtimes ebs"). Which is also not AWS' fault, we have quite some large customers who invested Millions of $ in their "unbreakable" or "fully redundant" SAN and they ALL had downtimes from some hours to several days.

So we're heavily relying on running all our elastic search stuff only on local instance storage and spread the copies to multiple nodes in multiple availability zones. The S3 gateway always seemed to be a big help in avoiding long reindexing times in times of catastrophic events.

In my opinion, it'd be a good idea to have an easy out-of-the-box-solution for ppl who don't want to run ElasticSearch on a non-local, distributed filesystem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing Slow Log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2457</link><project id="" key="" /><description>Add an indexing slow log, similar in function to the search slow log. The log file is ends with `index_indexing_slowlog.log` and the thresholds are configured in the `elasticsearch.yml` file.
</description><key id="8942360">2457</key><summary>Indexing Slow Log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-12-03T09:21:40Z</created><updated>2012-12-03T09:22:08Z</updated><resolved>2012-12-03T09:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.5.11</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2456</link><project id="" key="" /><description /><key id="8929620">2456</key><summary>Upgrade to Netty 3.5.11</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-12-02T21:29:34Z</created><updated>2012-12-02T21:29:54Z</updated><resolved>2012-12-02T21:29:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add types and stats to search slow log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2455</link><project id="" key="" /><description /><key id="8928975">2455</key><summary>Add types and stats to search slow log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-12-02T20:51:28Z</created><updated>2012-12-02T21:01:27Z</updated><resolved>2012-12-02T21:01:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Multi-cluster discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2454</link><project id="" key="" /><description>This tiny patch allows a cluster name of '*' (asterisk) in a multicast request to be accepted by all nodes.

The asterisk pseudo cluster name is intended for future discovery tools that want to discover all active clusters without knowing cluster names beforehand.

Additionally, the multicast response is extended by some basic node info (client mode, data mode, node name, node ID), so future discovery tools can take immediate actions depending on the type of the responding node.

Useful examples for multi-cluster discovery are
- data center wide multi-cluster multi-tenant ES node monitoring tools ("ES cluster radar")
-  zero-conf clients: by implementing multicast ping requesters in Perl/Ruby/Python/PHP, the current HTTP REST clients could be enhanced - no more bothering with cluster names, no static IP lists, no switching to unicast discovery

Nodes should respond with a list of transport / HTTP REST / other transport plugin addresses to the requester (optional addresses by using the node attributes). If there was only a single cluster, Perl/Ruby/Python/PHP clients could connect instantly to it.
</description><key id="8865983">2454</key><summary>Multi-cluster discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-11-30T17:07:03Z</created><updated>2014-07-08T18:46:03Z</updated><resolved>2014-07-08T18:46:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T10:54:36Z" id="39717122">Hi @jprante was this an attempt to expose something similar to what the tribe node does? Would the tribe node replace it potentially?
</comment><comment author="jprante" created="2014-04-07T11:14:55Z" id="39718450">The tribe node works on merging cluster states and is far more sophisticated than what I have in mind: Is it possible to request the discovery layer for a list of cluster names just running?

In monitor tools, the found cluster names could be displayed for further actions, e.g. connecting to a single cluster with standard client, or connecting to multiple clusters with tribe node.
</comment><comment author="javanna" created="2014-04-07T11:40:40Z" id="39720185">I see, thanks for your feedback!
</comment><comment author="clintongormley" created="2014-07-08T18:46:03Z" id="48382775">Honestly I'm not sure I like this idea.  Multicast already makes Elasticsearch quite promiscuous (just try firing up a cluster at a conference).  The idea of just joining any cluster (and as a default) seems to be asking for trouble.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support "ignore_unmapped":true for script based sorting on unmapped fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2453</link><project id="" key="" /><description>...Or perhaps make ignore_unmapped true by default only for script based sorting.
</description><key id="8823390">2453</key><summary>Support "ignore_unmapped":true for script based sorting on unmapped fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dannymcpherson</reporter><labels><label>adoptme</label></labels><created>2012-11-29T18:42:03Z</created><updated>2014-11-29T13:24:51Z</updated><resolved>2014-11-29T13:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tom-martin" created="2013-03-13T15:23:35Z" id="14847617">+1 on this. Anyone aware of any workarounds in the meantime?
</comment><comment author="tom-martin" created="2013-03-13T15:44:15Z" id="14848990">Testing doc.containsKey("whatevs") works :)
</comment><comment author="ethier" created="2013-05-28T21:20:22Z" id="18581015">+1 on this. I'm getting the following when specifying `ignore_unmapped true` on sorts for indices without one or all or the sort mappings. This is on 0.90.0 too.

```
ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ArrayIndexOutOfBoundsException[1]
```
</comment><comment author="clintongormley" created="2014-11-29T13:24:51Z" id="64951717">Thinking about this issue again, it seems to me that the best way to handle missing values in these cases is with `doc.containsKey()`, as suggested by @tom-martin.  You're already inside a script, which allows you to make whatever decision you want in order to handle the missing-value case.  Just adding (the now deprecated) `ignore_unmapped` parameter (see #7039) to this seems like a blunt instrument.

Closing this ticket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Porter2 Stemmer Token Filter should use English stemmer instead of Port...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2452</link><project id="" key="" /><description>...er Stemmer

Fixes #2451
</description><key id="8793092">2452</key><summary> Porter2 Stemmer Token Filter should use English stemmer instead of Port...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-29T01:36:24Z</created><updated>2014-07-16T21:54:21Z</updated><resolved>2012-12-03T16:09:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-12-03T16:09:19Z" id="10959076">Fixed by removing the reference to `porter2` in documentation. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The Porter2 Stemmer Token Filter is just Porter Stemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2451</link><project id="" key="" /><description>Repro: https://gist.github.com/4165996

Examples of the words that should be stemmed by [Porter](http://snowball.tartarus.org/algorithms/porter/stemmer.html) and [Porter2](http://snowball.tartarus.org/algorithms/english/stemmer.html) stemmers differently 

```
input        porter       porter2
-----------  -----------  -------
consolingly  consolingli  consol
his          hi           his
knightly     knightli     knight
stayed       stai         stay
```

See also: https://groups.google.com/d/topic/elasticsearch/HEW3Q9F4ocM/discussion
</description><key id="8792526">2451</key><summary>The Porter2 Stemmer Token Filter is just Porter Stemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label></labels><created>2012-11-29T01:12:19Z</created><updated>2014-06-10T10:21:40Z</updated><resolved>2013-02-09T19:37:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="speedplane" created="2012-11-29T03:06:46Z" id="10833380">I think you have it right. Currently (without the fix) the `porter` and `porter2` stemmers map to the `porter` stemmer.  The `english` stemmer maps to the `porter2` stemmer.

I also did a bit more investigation and I think the lovins stemmer may have a problem too. The proper output for the lovins stemmer is: 'consol', 'hi', 'knight', 'stay', however I remember getting something different.  I'll test it out and let you know.
</comment><comment author="imotov" created="2012-12-03T16:08:23Z" id="10959035">After some discussions we came to the conclusion that it would be safer to just [remove reference](https://github.com/elasticsearch/elasticsearch.github.com/commit/288dccaa0c22637b93ae7909f061aad2840f2c3a) to the `porter2` stemmer from documentation. Changing stemmer in elasticsearch might adversely affect users who are currently using it. Whoever really needs the `porter2` stemmer can simply use the `english` stemmer instead. 
</comment><comment author="speedplane" created="2012-12-03T18:23:55Z" id="10964769">That seems like the right move. If you could add a sentence explaining that `english` is implemented by the `porter2` stemmer, that would be nice.

As I am sure you already know, the reason it's important to be clear about the implementation is because sometimes you have to do stemming on the ES client side and you need to be sure that the client stemmer matches the ES stemmer.
</comment><comment author="imotov" created="2013-02-09T19:37:24Z" id="13337024">I [added links](https://github.com/elasticsearch/elasticsearch.github.com/commit/870121f8dc7c3f3282079269b3bcbe5b655c101e) to stemming algorithms. Closing this issue. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Calling doc().score() in a native script causes a NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2450</link><project id="" key="" /><description>Example with a trivial script, using example data:
https://gist.github.com/4165876

In this case doc() returns a DocLookup instance with a null scorer. Is this the expected behavior?
</description><key id="8791551">2450</key><summary>Calling doc().score() in a native script causes a NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rch</reporter><labels /><created>2012-11-29T00:35:31Z</created><updated>2013-07-22T16:25:11Z</updated><resolved>2013-07-22T16:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="KlausBrunner" created="2013-01-31T10:04:41Z" id="12935441">Note that score() doesn't work either (the corresponding field is only set when the script is used from a CustomScoreQuery, but not when called for a scripted field).
</comment><comment author="spinscale" created="2013-07-22T16:25:11Z" id="21356388">For future reference, here is the thread: https://groups.google.com/forum/#!msg/elasticsearch/mIbl9MgclII/m6KAEAECWx8J

It might make sense to return an exception with a useful error message, when someone tries to access call `score()` when it is not set, as this is most likely inside of a script field.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Improved explanation for match_phrase_prefix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2449</link><project id="" key="" /><description>The explanation provided by `/_validate` for the `match_phrase_prefix` is missing the prefix part - it just shows the same explanation as for `match_phrase` 

```
curl -XGET 'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "match_phrase" : {
      "text" : "one tw"
   }
}
'

# {
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "explanations" : [
#       {
#          "index" : "test",
#          "explanation" : "text:\"one tw\"",
#          "valid" : true
#       }
#    ],
#    "valid" : true
# }


curl -XGET 'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "match_phrase_prefix" : {
      "text" : "one tw"
   }
}
'

# {
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "explanations" : [
#       {
#          "index" : "test",
#          "explanation" : "text:\"one tw\"",
#          "valid" : true
#       }
#    ],
#    "valid" : true
# }
```
</description><key id="8768938">2449</key><summary>Query DSL: Improved explanation for match_phrase_prefix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2012-11-28T15:25:51Z</created><updated>2014-07-07T14:29:36Z</updated><resolved>2014-07-07T12:48:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-07T14:29:11Z" id="48185428">Closed by #6767 

The match_phrase_prefix provided the same explanation as the match_phrase
query. There was no indication that the last term was run as a prefix
query.

This change marks the last term (or terms if there are multiple terms
in the same position) with a *
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to disable reduce/gather phase on client nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2448</link><project id="" key="" /><description>We would like an option to have client nodes only do the smart load balancing, not reduce/gathering.

We run client nodes on each app server so app servers only talk to ES via the client node on localhost, so we avoid having to use a HTTP load balancer (since it's a single point of failure), and also avoid the extra hop.

An option to be able to turn off the reduce/gather phase for a client node would let us decrease memory usage and CPU burn.

Thanks!
</description><key id="8752790">2448</key><summary>Option to disable reduce/gather phase on client nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chendo</reporter><labels><label>discuss</label></labels><created>2012-11-28T05:55:04Z</created><updated>2014-07-25T08:41:55Z</updated><resolved>2014-07-25T08:41:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-11-29T01:51:18Z" id="10831916">You can setup nodes with HTTP for the networking load blancing from outside, while the other nodes don't need to have HTTP.

Other nodes without HTTP and no data can act as pass-through nodes. If you connect a transport client to such a node, you force Elasticsearch to route all the requests and response for the transport client via the pass-through node. The pass-through node will have to reduce/gather the results for the transport client. With a number of parallel http-less and data-less nodes, you are close to what you want.

From what I understand, the reduce/gathering should be performed as close as possible to the requesting client, otherwise responses can't be delivered back straight to the requester. Otherwise, it is open to me what the advantage is when nodes must be elected to perform reduce/gathering for other nodes just to pass the result back to them.

As a long term perspective, it could be viable to introduce a chunked stream response protocol between the nodes and the requesting client. This would ease some of the memory peaks of gathered large responses.
</comment><comment author="chendo" created="2012-11-29T02:40:09Z" id="10832892">I'm not sure if I understand how this will actually work or if it applies to our scenario. How would I force the HTTP enabled nodes to route through the pass-through/client nodes?

Our goal is to avoid SPOF with regards to ES as well as try to keep non-app server specific load off our app servers, and we don't want to either use a HTTP load balancer because it's not as smart as an cluster-aware client. We have seen memory pressure from our client nodes previously and didn't understand why until I found out they were also doing the reduction step.

I'm open to suggestions that satisfy our goals, but I can see a need for cluster-aware routing with minimal CPU/memory usage.
</comment><comment author="jprante" created="2012-11-29T08:20:50Z" id="10838723">I'm not completely sure if my suggestions fit to your environment, but I assume you are using Java app servers. With Java app servers, you have the TransportClient available. Other app servers would rely on the HTTP REST API.

The Java app server scenario is as follows. Configure some pass-thru nodes (without HTTP and without data, they also need a parameter they not become master). In each of your Java app instance, fire up a singleton TransportClient with a list of the pass-through node IPs. As a result, they only connect to those nodes and execute remotely on those nodes the Elasticsearch actions with gather/reduce load. The TransportClient manages the failover between the pass-through nodes. At least one of them must always be up, so monitor them.

The non-Java app server scenario is a little bit more challenging, since there is no automatic failover provided by Elasticsearch. A best practice is to start HTTP-enabled and data-less Elasticsearch nodes on each non-Java app server. The non-Java app connects to the Elasticsearch node at localhost, and delegates the gather/reduce and ES failover management to it. 

I assume this is the case you do not prefer, the sharing of a Java Elasticsearch node and the non-Java app node on the same machine. In case of this, you would have to assign each non-Java app server node to another corresponding Elasticsearch HTTP data-less node somewhere on the local network it can connect to, and you should monitor all those pairs for connection failures. Another option is to prepare a round-robin fashion selection or a kind of a random selection of one of  the HTTP-enabled data-less Elasticsearch nodes from your non-Java app instances for yourself, from a pre-configured IP list. I know, the Perl client for example comes with such an ability.
</comment><comment author="chendo" created="2012-11-29T10:33:16Z" id="10842175">Our stack is Ruby on Rails, so we access ES over the HTTP REST interface. We're already running client nodes locally on each app server, and we're okay with that because having a client node on another host is not something we want to have to do because it's not flexible.

Round-robin is also something we wouldn't want as that involves static IP lists, and if a node goes down, the entire stack would eventually hit the missing node, and then we have to deal with HTTP timeout and mark it as down etc, so not as intelligent as a client node.

We're okay with running a local client node on the box assuming it did the least amount of work as possible, and this is why we would like the option to disable a client node from performing the reduce step.
</comment><comment author="clintongormley" created="2014-07-25T08:41:55Z" id="50122907">The transport client doesn't do reduce, but does do sniffing and round robin, as does the ruby client.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation Bug: "and" filter accepts filters, not queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2447</link><project id="" key="" /><description>Documentation for the `and` filter states that: "A filter that matches documents using `AND` boolean operator on other queries."
http://www.elasticsearch.org/guide/reference/query-dsl/and-filter.html

However, I have tried to place a `span_near` query within an `and` filter and I get a 500 error. When I wrapped the `span_near` query within a `query` filter, it worked just fine. The `and` filter can only work with other filters, not queries.

Therefore, the documentation on that page should read: "A filter that matches documents using `AND` boolean operator on other _filters_."
</description><key id="8751213">2447</key><summary>Documentation Bug: "and" filter accepts filters, not queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">speedplane</reporter><labels /><created>2012-11-28T04:27:47Z</created><updated>2013-06-07T15:55:05Z</updated><resolved>2013-06-07T15:55:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T15:55:05Z" id="19115687">Closed by https://github.com/elasticsearch/elasticsearch.github.com/commit/ed2409d65caf7391babac6b035dc24f86e9d6210

Thanks for the report! Please put that into the documentation repo next time, so we spot a bit earlier! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StringIndexOutOfBoundsException when performing has_child filter (0.20.0RC1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2446</link><project id="" key="" /><description>When performing a search using a `has_child` filter, I'm running into a StringOutOfBoundsException.

**Parent Mapping "resource":**

``` javascript
{
  "resource": {
    "properties": {
      "access": {
        "type": "string",
        "index": "not_analyzed"
      },
      "general": {
        "type": "string"
      },
      "resourceType": {
        "type": "string",
        "index": "not_analyzed",
        "store": "yes"
      },
      "sort": {
        "type": "string",
        "index": "not_analyzed"
      },
      "tenantId": {
        "type": "string",
        "index": "not_analyzed",
        "store": "yes"
      },
      "thumbnailUrl": {
        "type": "string",
        "index": "no",
        "store": "yes"
      },
      "title": {
        "type": "string",
        "index": "no",
        "store": "yes"
      },
      "visibility": {
        "type": "string",
        "index": "not_analyzed",
        "store": "yes"
      }
    }
  }
}
```

**Child Mapping (resource_memberships):**

``` javascript
{
  "resource_memberships": {
    "_parent": {
      "type": "resource"
    },
    "_routing": {
      "required": true
    },
    "properties": {
      "direct_memberships": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}
```

**Query:**

``` javascript
     "query": {
        "filtered": {
          "query": {
            "match": {
              "general": {
                "query": "Content",
                "operator": "and"
              }
            }
          },
          "filter": {
            "and": [
              {
                "and": [
                  {
                    "term": {
                      "_type": "resource"
                    }
                  }
                ]
              },
              {
                "or": [
                  {
                    "has_child": {
                      "type": "resource_memberships",
                      "query": {
                        "terms": {
                          "direct_memberships": [
                            "u:gttest:TV2Ek7T1JM"
                          ]
                        }
                      }
                    }
                  },
                  {
                    "and": [
                      {
                        "or": [
                          {
                            "term": {
                              "visibility": "public"
                            }
                          },
                          {
                            "terms": {
                              "joinable": [
                                "yes",
                                "request"
                              ]
                            }
                          },
                          {
                            "and": [
                              {
                                "term": {
                                  "tenantId": "gttest"
                                }
                              },
                              {
                                "term": {
                                  "visibility": "loggedin"
                                }
                              }
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            ]
          }
        }
      },
      "size": "1",
      "sort": {
        "sort": "asc"
      },
      "fields": [
        "*",
        "_source.extra"
      ]
    }
```

**Error:**

```
"error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[W_ljaTkKQSqrhuZ_IK1Ugg][oaetest][0]: QueryPhaseExecutionException[[oaetest][0]: query[filtered(general:content)-&gt;++cache(_type:resource) +child_filter[resource_memberships/resource](filtered(direct_memberships:u:gttest:TV2Ek7T1JM)-&gt;cache(_type:resource_memberships)) +cache(visibility:public) cache(joinable:request joinable:yes) +cache(tenantId:gttest) +cache(visibility:loggedin)],from[0],size[1],sort[&lt;custom:\"sort\": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@698f0a46&gt;]: Query Failed [Failed to refresh id cache for child queries]]; nested: StringIndexOutOfBoundsException[String index out of range: -1]; }]",
      "status": 500
    }
```

**Other possibly useful info:**
- There is another mapping that is also a child of the `resource` document, it's mapping name is `resource_members`
- The `resource` document's child may not exist / have not been indexed, at the time of running that query
</description><key id="8675034">2446</key><summary>StringIndexOutOfBoundsException when performing has_child filter (0.20.0RC1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">mrvisser</reporter><labels /><created>2012-11-26T12:51:18Z</created><updated>2013-07-16T07:51:20Z</updated><resolved>2013-07-16T07:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrvisser" created="2012-11-26T13:10:28Z" id="10714713">Here is the trace logging for the resources created in the index at the time of this failure. Note that **the parent and child resources are created in the same bulk set of index operations**:

```
[2012-11-26T13:04:44.289Z] TRACE: elasticsearchclient/55300 on branden-macbook.local: Performing a bulk set of 2 operations.
    operations: [
      {
        "index": {
          "_type": "resource",
          "_id": "c:camtest:PV6D_se0MM",
          "_index": "oaetest"
        }
      },
      {
        "resourceSubType": "link",
        "id": "c:camtest:PV6D_se0MM",
        "tenantId": "camtest",
        "title": "Test Content 1",
        "visibility": "public",
        "general": "Test Content 1 Test content description 1",
        "sort": "Test Content 1 Test content description 1",
        "_type": "resource",
        "resourceType": "content"
      },
      {
        "index": {
          "_type": "resource_members",
          "_id": "c:camtest:PV6D_se0MM#members",
          "_parent": "c:camtest:PV6D_se0MM",
          "_index": "oaetest"
        }
      },
      {
        "_parent": "c:camtest:PV6D_se0MM",
        "_type": "resource_members",
        "id": "c:camtest:PV6D_se0MM#members",
        "direct_members": [
          "g:camtest:random-group-eeCDD_cT1fJ",
          "u:camtest:ePKA3kTNJM"
        ]
      }
    ]
[2012-11-26T13:04:44.295Z] TRACE: elasticsearchclient/55300 on branden-macbook.local: Search execution completed. (call=bulk)
    data: {
      "took": 3,
      "items": [
        {
          "index": {
            "_index": "oaetest",
            "_type": "resource",
            "_id": "c:camtest:PV6D_se0MM",
            "_version": 1,
            "ok": true
          }
        },
        {
          "index": {
            "_index": "oaetest",
            "_type": "resource_members",
            "_id": "c:camtest:PV6D_se0MM#members",
            "_version": 1,
            "ok": true
          }
        }
      ]
    }
```
</comment><comment author="mrvisser" created="2012-11-26T13:11:56Z" id="10714750">And the elasticsearch server-side error log:

```
[2012-11-26 08:04:44,508][DEBUG][action.search.type       ] [Shiver Man] [oaetest][0], node[H81AnrrxTaef_Rz3UrpCrA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@24367013]
org.elasticsearch.search.query.QueryPhaseExecutionException: [oaetest][0]: query[filtered(general:content)-&gt;++cache(_type:resource) +child_filter[resource_members/resource](filtered(direct_members:u:gttest:VTydFcP2JM)-&gt;cache(_type:resource_members)) +cache(visibility:public) cache(joinable:request joinable:yes) +cache(tenantId:gttest) +cache(visibility:loggedin)],from[0],size[1],sort[&lt;custom:"sort": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@4f19c297&gt;]: Query Failed [Failed to refresh id cache for child queries]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:89)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:308)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:242)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
    at java.lang.String.substring(String.java:1937)
    at org.elasticsearch.index.mapper.Uid.createUid(Uid.java:87)
    at org.elasticsearch.index.cache.id.simple.SimpleIdCache.refresh(SimpleIdCache.java:171)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:87)
    ... 9 more
```
</comment><comment author="mrvisser" created="2012-11-26T13:13:09Z" id="10714781">This appears to impact 0.9.11 as well.
</comment><comment author="mrvisser" created="2012-11-26T13:30:57Z" id="10715309">I noticed in Uid.java that a `#` might be a special ID character. I changed my _id's to use `_` and it doesn't solve the problem.
</comment><comment author="mrvisser" created="2012-11-26T15:11:04Z" id="10718761">I've resolved my errors, it appears there was bug where documents were being index as type `resource` that referenced other `resource` documents as their `_parent`. Fixing this bug such that those rogue documents don't get indexed allows the has_child filter to work without this error.

If there was some work to do on this ticket, it would be to either validate the parent-child relationship types when indexing, or to better handle this particular error.
</comment><comment author="kimchy" created="2012-11-26T15:13:20Z" id="10718859">@mrvisser will have a look, is there a chance for a repro to speed this up. I mean, a curl repro that setup an index with the mappings, index some sample data, and then executes a search that shows the failure? (we are close with what you provided up until now, missing sample data and a curl based repro).
</comment><comment author="mrvisser" created="2012-11-26T15:18:23Z" id="10719050">@kimchy I have an idea how it could be reproduced now. I'll post some commands that hopefully accomplish this.
</comment><comment author="mrvisser" created="2012-11-26T16:19:33Z" id="10721648">@kimchy Sorry, I've been unable to set up a case where this can be demonstrated. I created a parent mapping and a child mapping. Then indexed documents that are of the type of the parent mapping, and have _parent mapping to a parent document. ElasticSearch allowed this indexing to happen (perhaps it shouldn't?), but then a query was successful (returned 0 elements), rather than the StringOutOfBoundsException I encountered before.

Perhaps the exception is a result of some particular query mechanics.

I hope that what I've posted above provides enough clues.
</comment><comment author="spinscale" created="2013-07-15T16:57:28Z" id="20983665">Hey,

do you still have problems on 0.90 with this?
</comment><comment author="mrvisser" created="2013-07-15T17:10:51Z" id="20984530">Hi @spinscale , the bug in our application that manifested this error has been fixed for a long time so we wouldn't know if it still exists in 0.90
</comment><comment author="spinscale" created="2013-07-16T07:51:20Z" id="21026469">@mrvisser thanks for the fast reply! I will close this one for due to its age and see if it pops up again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem with Term and terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2445</link><project id="" key="" /><description>I am facing some issue with term and terms. This down query works Perfectly :+1: 
{
  query: {
     bool: {
        must: [
           {
            term: {
              CREATED: 2012-11-21T06:45:05+00:00
                       }
            }
         ]
}
}
}

But Not this :-1: 
{
  query: {
     bool: {
        must: [
           {
            term: {
              relate: H12C0
                       }
            }
         ]
}
}
}

Same thing for terms.......
The query is running on Unmapped documents. Can you please help...... We have Updated to the new search and this function stopped working.

I just changed the query as

query_string: {
default_field: "relate"
query: "H12C0 OR H12C2"

Which seems to be solved the problem as of now :+1: . But what happened with term or terms? Or am i using it wrong?
</description><key id="8665882">2445</key><summary>Problem with Term and terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">biswajitdas</reporter><labels /><created>2012-11-26T07:08:01Z</created><updated>2013-07-30T14:58:59Z</updated><resolved>2013-07-30T14:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-11-26T08:17:48Z" id="10707613">TermQuery is not analyzed. QueryString is.
I suppose it comes from here.

But, you should provide more details (have a look at the help page on elasticsearch.org site)
</comment><comment author="biswajitdas" created="2012-11-26T08:21:55Z" id="10707693">Thanks, As I told you, I had to change the Query.

What I want to achieve here is I want to filter those Items where field name "relate" is equal to "H12C0" OR "H12C2". Which I was doing it via terms. But its not happening now. :-1: 
</comment><comment author="dadoonet" created="2012-11-26T08:28:21Z" id="10707836">What I was suggesting is to gist a full curl recreation and post it on the mailing list, as described in the help page.
That way, we will be able to help you.

I suspect something wrong with your docs or mapping.
</comment><comment author="spinscale" created="2013-07-30T14:58:59Z" id="21796792">closing due to lack of information (mapping &amp; recreation). Happy to reopen once more information is provided. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Request: Document Routing level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2444</link><project id="" key="" /><description>I would like to perpose document routing level via alies. I want to be able to route documents not only to a single shard but a set number of shards and always the same number. There for If i have 3 nodes in my cluster then I can set the routing level to 3, and have the data existing on 3 shards (hopefully one per machine) but the point is my querys will not be limited to the maximum speed of each shard, The reason I say this is a user can be loaded onto a single shard and that shard for what ever reason is over loaded how do i move all those users to another shard I cant, so its at least better that there is a split.
</description><key id="8650869">2444</key><summary>Request: Document Routing level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wojons</reporter><labels /><created>2012-11-25T15:07:48Z</created><updated>2013-10-30T09:52:16Z</updated><resolved>2013-10-30T09:52:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-11-25T15:17:34Z" id="10694432">I am not sure I understand the difference between this proposal and simply enabling 2 replicas for the index. Could you clarify it a bit? 
</comment><comment author="wojons" created="2012-11-25T15:24:54Z" id="10694509">From my understanding in all databases unless you are running master-master replication which can become messed up quickly its hard to have replicas scale your writes. Considering that by default elasticsearch is not async replication so all writes would have to be written at once, and if it was async we have a higher chance of things falling out of sync. 

Also if we just add more replicas, I dont know if elasticsearch will split the query against each replica to report back a different range of data. as i understand it, if you use routing reads and writes for that route are limited by the performance of a single machine.
</comment><comment author="imotov" created="2012-11-25T15:36:07Z" id="10694652">When you specify routing, users requests are distributed among all replicas of the shard, which indeed can lead to somewhat different results while shard is being refreshed. If you want to provide a consistent view for a user, you can add a randomly generated [preference](http://www.elasticsearch.org/guide/reference/api/search/preference.html) to user's query. As long as the preference value stays the same and all shards are available, this user's query will be directed to the same shard. 
</comment><comment author="wojons" created="2012-11-25T17:01:08Z" id="10695551">So what your saying is i can make as many routes as I need to make related
to a single user. And place that as an alies for that user. Am I able to
have one alies that spans more then one index each with multiple routing
rules.
On Nov 25, 2012 7:36 AM, "Igor Motov" notifications@github.com wrote:

&gt; When you specify routing, users requests are distributed among all
&gt; replicas of the shard, which indeed can lead to somewhat different results
&gt; while shard is being refreshed. If you want to provide a consistent view
&gt; for a user, you can add a randomly generated preferencehttp://www.elasticsearch.org/guide/reference/api/search/preference.htmlto user's query. As long as the preference value stays the same and all
&gt; shards are available, this user's query will be directed to the same shard.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2444#issuecomment-10694652.
</comment><comment author="imotov" created="2012-11-25T17:45:50Z" id="10696026">No, you cannot have more than one routing value per request.
</comment><comment author="wojons" created="2012-11-25T17:48:50Z" id="10696065">Is that for just adding a new document or for reading also? 
</comment><comment author="imotov" created="2012-11-25T18:14:38Z" id="10696381">Yes, when you retrieve a document using [get](http://www.elasticsearch.org/guide/reference/api/get.html) operation, you have to specify routing if you used non-default routing for indexing. 
</comment><comment author="spinscale" created="2013-10-30T09:52:16Z" id="27376230">Closing this for now, as there are ways to implement similar functionality.

I highly recommend watching the following, which explains how to efficiently handle certain data flows in an elasticsearch cluster

http://www.elasticsearch.org/videos/big-data-search-and-analytics/ 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Request: Add empty shard to existing Index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2443</link><project id="" key="" /><description>I know everyone is about to tell me this is not a good idea because of having to split your data move it over blah blah blah, but that is not what I am looking for. I was watching the talk at Berlin buzz words by Shay Banon, and he was speaking about adding routing to a document so it will always end up on the same shard, there a single user wont need an index just for themselves. Now lets say I want to have a rule that every 1000 users I create a new shard, this way I get a new blank shard that i can start putting data into. and there is no migration required. If you want to think about the uses for something about this lets look at game design, when a user starts we are going to need there profile data maybe a log of the things they have done, if I am easily able to add more shards and when I need to add more machines to move the shards to that makes it easy and if the game gets less popular i can just remove nodes and scale donwn.
</description><key id="8650795">2443</key><summary>Request: Add empty shard to existing Index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wojons</reporter><labels /><created>2012-11-25T15:03:59Z</created><updated>2013-07-15T17:08:49Z</updated><resolved>2013-07-15T17:08:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-11-25T15:12:46Z" id="10694377">You can just use indices instead of shards and aliases instead of indices. Every 1000 users just create a new index and add it to the alias of all indices with user info. 
</comment><comment author="wojons" created="2012-11-25T15:20:00Z" id="10694458">That was the first thought that I was having but i was thinking that the price of a shard is less then the price of an index but I could be wrong. The other reason why I was suggesting this was for a multi-client logging system. It would be nice to have 1 index per week/month of logs and just add shards when i get 1000 new users instead of having 1 index per week of logs per 1000 users. I feel like tat code will get pretty hard to handle generating all the alieses for something like.
</comment><comment author="karmi" created="2012-11-25T16:13:10Z" id="10695036">&gt;  I feel like tat code will get pretty hard to handle generating all the alieses for something like.

@wojons, no, [not at all](https://github.com/karmi/tire/blob/master/test/integration/index_aliases_test.rb#L66-L120).

@imotov is right that the proper way how to look at it is to free your mind from fixating on the "index" as some static entity. Create indices with 1 shard when you're starting have little users. When you start to gain more users, just create an index with more number of shards, and add it to the alias.

That way it's a) opaque to your application (it just hits whatever virtual "index" you tell it), b) you can scale with the demand -- and not pick up some magical number of shards from the start...
</comment><comment author="wojons" created="2012-11-25T16:42:28Z" id="10695354">Okay that sounds like it could work i would like to know how much an alies cost in the system if i have +1 million alies on the system how does that effect my performance. Can i put an alies on an alies there for it makes managing them easier.? 
</comment><comment author="karmi" created="2012-11-25T16:54:21Z" id="10695467">&gt; if I have +1 million alies on the system how does that effect my performance (...)

Less then +1 million of indices...

&gt; Can i put an alies on an alies there for it makes managing them easier?

No, you can't point and alias to another alias. The API for alias management is very easy to work with.
</comment><comment author="wojons" created="2012-11-25T17:10:57Z" id="10695662">When you say less than 1 million indeces you mean a lot less or a little less I am going to guess a lot. I is there anyway the system can be changed to allow pointing one alias to another I looked at how easy the is to use I am more worried about as the system grows and it's time to change the alias mapping its going to take modifying maybe 3 to 5 alias per user in a short period of time. 
</comment><comment author="hazzadous" created="2012-11-25T18:11:53Z" id="10696355">To go back to your original suggestion about adding shards to an index, I'm not sure how the routing internals work but I should imagine its based on being able to hash against the number of shards there are in the index.  Adding a shard to an index would obviously invalidate all previous routing within that index.  You'd have to reindex all data in that index to a larger sharded index to repartition your data.

I wouldn't suggest adding aliases per user, just use routing values in your requests rather than adding implicit aliases for these.  You'd only be talking about having:
- an alias that points to all indicies with which you can query your entire dataset.  However, searches over this will hit #indices shards when using a routing value.
- one for the most recent index, to which you use for indexing operations
- another pointing to indicies from the last week, thereby reducing the number of shards you hit.  Given that you're indexing logging data it makes a lot of sense to structure it like this, and gives you lots of ways to optimize your operations over your data.  You could for instance reindex your historical data into an index with larger shard count...
</comment><comment author="wojons" created="2012-11-26T01:05:39Z" id="10701527">Okay that makes sense that I put a few aliases in place and just use routes
when working with an exact user. This does mean an extra database look up
depending on how i set it up. Now I was looking into having larger users
have more than one route. This way a users request can end up across a few
shards, i wanted to know if there is anyway to setup the route so i can
control that it's on different shards then it's friend. I am questing it
may just be a mod of the number of shards on the route.

Also i see the point on the issue of adding a new shard would not work very
well, is there anyway that can happen when optimizing an index let's look
at it the other way, I have some very old data its not getting accessed
very much and I don't mind it not being accessible for hours maybe even a
few days, just so I can lower the shard count while optimizing the index
and maybe even move it to my archive machine with spinning disks instead of
ssd
On Nov 25, 2012 10:12 AM, "hazzadous" notifications@github.com wrote:

&gt; To go back to your original suggestion about adding shards to an index,
&gt; I'm not sure how the routing internals work but I should imagine its based
&gt; on being able to hash against the number of shards there are in the index.
&gt; Adding a shard to an index would obviously invalidate all previous routing
&gt; within that index. You'd have to reindex all data in that index to a larger
&gt; sharded index to repartition you're data.
&gt; 
&gt; I wouldn't suggest adding aliases per user, just use routing values in
&gt; your requests rather than adding implicit aliases for these. You'd only be
&gt; talking about having:
&gt; - an alias that points to all indicies with which you can query your
&gt;   entire dataset. However, searches over this will hit #indices shards when
&gt;   using a routing value.
&gt; - one for the most recent index, to which you use for indexing
&gt;   operations
&gt; - another pointing to indicies from the last week, thereby reducing
&gt;   the number of shards you hit. Given that you're indexing logging data it
&gt;   makes a lot of sense to structure it like this, and gives you lots of ways
&gt;   to optimize your operations over your data. You could for instance reindex
&gt;   your historical data into an index with larger shard count...
&gt;   
&gt;   &#8212;
&gt;   Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2443#issuecomment-10696355.
</comment><comment author="spinscale" created="2013-07-15T17:08:49Z" id="20984394">closing this one as it is a bit stale and not an issue, but rather a helping thread. Pleae use the google group next time, as simply more eyes will have a look there...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The relevancy score in explanation should match the actual score in cust...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2442</link><project id="" key="" /><description>...om_filters_query

Fixes #2441
</description><key id="8650099">2442</key><summary>The relevancy score in explanation should match the actual score in cust...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-25T14:09:26Z</created><updated>2014-07-16T21:54:22Z</updated><resolved>2012-12-03T12:26:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>The relevancy score in explanation of custom_filters_query doesn&#8217;t match the actual score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2441</link><project id="" key="" /><description>Repro: https://gist.github.com/fa7219f4f7fb8d9435ad
</description><key id="8650043">2441</key><summary>The relevancy score in explanation of custom_filters_query doesn&#8217;t match the actual score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-25T14:04:14Z</created><updated>2012-11-27T18:16:15Z</updated><resolved>2012-11-27T18:16:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use specified interface address, despite available subinterfaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2440</link><project id="" key="" /><description>This fixes issue #2437
</description><key id="8637954">2440</key><summary>Use specified interface address, despite available subinterfaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">hazzadous</reporter><labels /><created>2012-11-24T23:03:16Z</created><updated>2014-10-21T21:41:06Z</updated><resolved>2014-10-10T10:55:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hazzadous" created="2012-11-24T23:20:16Z" id="10685062">Didn't quite know how to go about adding a test for this.  I've checked that we bind to:
- the first non loopback address
- an interfaces address, despite it being a parent to other interfaces
- a subinterfaces address

It feels like I must have missed a very simple part of the NetworkInterface and this should have all been much simpler.  Comments?
</comment><comment author="clintongormley" created="2014-08-22T07:49:50Z" id="53032729">@kimchy please could you take a look at this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simple ETag handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2439</link><project id="" key="" /><description>Hi Elasticsearch team! 

I hacked a simple ETag handling. It's not tested, but it should do.

See discussion https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/aEjjxQzUhdM

J&#246;rg
</description><key id="8636542">2439</key><summary>simple ETag handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>discuss</label></labels><created>2012-11-24T21:25:27Z</created><updated>2014-07-25T09:18:00Z</updated><resolved>2014-07-25T08:39:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2012-11-24T22:37:53Z" id="10684670">This is rather costly. Can there be a softer version, to be used in more idle indexes where no many updates are happening, where the etag value is associated with index updates (vs computed against the actual content)?
</comment><comment author="jprante" created="2012-11-24T22:42:11Z" id="10684726">Yes. I think one possibility is enabling ETags per REST action, I have to add it.
</comment><comment author="jprante" created="2012-11-25T00:20:14Z" id="10685565">Now, ETag caching can be enabled in the config only per REST action class,  e.g. `http.cache.etag.restsearchaction: true`

The header will look like

```
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 661
ETag: 4173176557
```
</comment><comment author="jprante" created="2012-11-25T00:31:12Z" id="10685659">Suppressing ETags on a per index or per type basis is another interesting thing, yes. This depends on the REST action. This needs some work in the RestUtils I guess. And I have to find out if ETag cacheable setting via cluster update API works.
</comment><comment author="jprante" created="2012-11-25T16:27:12Z" id="10695179">Oops.
</comment><comment author="clintongormley" created="2014-07-25T08:39:19Z" id="50122696">Closing this issue given that there has been no support for this, and it is easier to implement client side.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String field that contains floating numbers is parse as Long instead of String</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2438</link><project id="" key="" /><description>Hi guys,

One of the fields that is a string is parsed as Long type. Please see the error:

Failed to execute [org.elasticsearch.action.search.SearchRequest@846692c]
org.elasticsearch.search.SearchParseException: .... 
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:554)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:469)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NumberFormatException: For input string: "48.85341"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
    at java.lang.Long.parseLong(Long.java:419)
    at java.lang.Long.parseLong(Long.java:468)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.fieldQuery(LongFieldMapper.java:167)
    at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:131)
    at org.elasticsearch.index.query.MatchQueryParser.parse(MatchQueryParser.java:141)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:187)
    at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:82)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:187)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:265)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:245)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:542)

Thanks
</description><key id="8612600">2438</key><summary>String field that contains floating numbers is parse as Long instead of String</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JW200</reporter><labels /><created>2012-11-23T17:47:20Z</created><updated>2016-07-12T19:39:07Z</updated><resolved>2014-07-08T18:38:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-23T19:31:12Z" id="10668022">Can you create a repro for this failure?
</comment><comment author="clintongormley" created="2014-07-08T18:38:23Z" id="48381737">No more info after 2 years. Closing
</comment><comment author="MasseGuillaume" created="2016-07-12T19:39:06Z" id="232155961">I have the same issue
https://static.javadoc.io/org.elasticsearch/elasticsearch/2.3.4/org/elasticsearch/index/query/QueryStringQueryBuilder.html#field-java.lang.String-float-

it should be Long not Float

```
val builder = QueryBuilders.queryStringQuery(query)
builder.field("xyz", 10F)
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not bind to virtual interfaces if physical interface is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2437</link><project id="" key="" /><description>Currently when a physical interface is specified, host binding will bind to any of the physical or virtual addresses bound to an interface, which ever comes up first in the NetworkInterface.getInetAddresses() call in  org.elasticsearch.common.network.NetworkService.  This makes it impossible to explicitly specify the non-virtual address.  Any chance this could be changed?  I had a look but couldn't find a simple way of implementing this aside from going through subInterfaces and excluding the bound addresses (I'm not too conversant in Java).  However, happy to issue a pull request if desired.
</description><key id="8610277">2437</key><summary>Do not bind to virtual interfaces if physical interface is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hazzadous</reporter><labels><label>adoptme</label></labels><created>2012-11-23T16:24:45Z</created><updated>2014-10-10T10:55:07Z</updated><resolved>2014-10-10T10:55:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-23T19:30:13Z" id="10668000">Can you possibly gist your ifconfig options? Or execute elasticsearch with `monitor.network` set to `DEBUG` in teh logging and gist its output (it prints what Java sees as part of the configuration). Want to full understand your config before we suggest a solution.
</comment><comment author="hazzadous" created="2012-11-23T20:45:09Z" id="10669323">Sure: I have eth0 and eth0:failover setup.  The logging I get is: https://gist.github.com/4137185

Get addresses returns all addresses from all subinterfaces, in this case it happens to be the subinterface address that gets used.  This is with:

java version "1.6.0_24"
OpenJDK Runtime Environment (IcedTea6 1.11.5) (6b24-1.11.5-0ubuntu1~12.04.1)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
</comment><comment author="kimchy" created="2012-11-24T13:05:39Z" id="10677475">And what is the configuration you specify on `network.host`? 
(also, by the way, the java version you are using is very old).
</comment><comment author="hazzadous" created="2012-11-24T14:10:02Z" id="10677993">network.bind_host: 0.0.0.0
network.publish_host: _eth0:ipv4_

I've also updated to openjdk 1.7 with the same result.  I'll gist over a complete example, probably better/safer than drip feeding you parts of our config.
</comment><comment author="hazzadous" created="2012-11-24T14:59:21Z" id="10678443">This is probably easier to follow: https://gist.github.com/4139970

eth0 reports multiple addresses, the subinterface being the first ipv4 address listed and hence used for the bind.  I'd expect specifying _eth0:ipv4_ to use the non-virtual address, or at least having some way to make this explicit.
</comment><comment author="kimchy" created="2012-11-24T19:40:44Z" id="10682809">Its strange, since based on the name, the correct one, the `eth0` should be selected. But then, I wonder what ends up happening in the logic of `NetworkUtils.getFirstNonLoopbackAddress(ni, stackType)` in terms of which addresses it returns, and if it ends up returning null. Or listing all the inet addresses ends up listing the ones for the sub interfaces as well. Hard to tell cause I can't repro it easily.

Can create a custom version with much more verbose logging? Will you be able to give it a try?
</comment><comment author="hazzadous" created="2012-11-24T20:39:18Z" id="10683441">Just working through the code it appears that the correct interface is found, then passed to getFirstNonLoopbackAddress.  Unfortunately NetworkInterface.getInetAddresses returns all addresses including those attached to the interfaces subinterfaces.  Difficult to reproduce as its unclear what the ordering is on the returned list so sometimes the correct address is used.

I'll have a stab at fixing it, failing that I'll send over some more logging.
</comment><comment author="hazzadous" created="2012-11-29T22:57:34Z" id="10870515">I'm not proud of this solution but couldn't see anything more elegant.  Do you know if theres a better way?
</comment><comment author="hazzadous" created="2013-01-24T18:01:41Z" id="12664250">Is there anything more I can do on this, or is it a matter of waiting for review?
</comment><comment author="hazzadous" created="2013-06-27T01:08:12Z" id="20091213">Sorry to bring this one up again, but it would be nice to not have to maintain this in our own fork.  For our specific use case we have a failover ip which is a subinterface of eth0 that we point to in dns.  Thus we have eth0 listening on both its own ip and also this failover.  With the current code, specifying **eth0** as the bind address results on whatever is returned from getInetAddresses returns first.  For internode comm. thats obviously not great, and it ends up trying to bind to the failover occasionally.  With the pull request we look for the non-subinterface address.  Can we either close this or merge?
</comment><comment author="clintongormley" created="2014-10-10T10:51:01Z" id="58640267">Hi @hazzadous 

Apologies for the slow response here.  The problem with making progress on this is the difficulty in testing it.  In your PR, you comment that you weren't sure how to test it, and we're facing the same issue.  If you have subsequently had any inspiration about how to do that, we'd love to take this forward.
</comment><comment author="hazzadous" created="2014-10-10T10:55:07Z" id="58640653">Yeah to be honest if no one else is interested then perhaps we should just leave this.  We have a different config now anyhow so...  I'll just close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose Lucenes KeepWordTokenFilter in ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2436</link><project id="" key="" /><description>Lucene 4 comes with a handy KeepWordTokenFilter that is basically an inverse stop filter. ES should expose this filter by default.
</description><key id="8587112">2436</key><summary>Expose Lucenes KeepWordTokenFilter in ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2012-11-22T20:54:04Z</created><updated>2012-11-23T09:15:42Z</updated><resolved>2012-11-23T09:15:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-11-23T09:15:41Z" id="10653727">pushed to master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>match query supports multiple query strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2435</link><project id="" key="" /><description>We advocate using the `match` query for (almost) all fields, as it does the right thing based on the mapping of the field being queried.

However, if we want to query a non-full text field for multiple values, we resort to the `terms` query, which then requires having to explain why we use the `match` query instead of the `term` query for single values.

Would it be possible to change the `match` query (and derivatives) to support multiple query strings, eg:

```
{ match: { tag: ['foo',bar']}}
{ match: { num: [1,2,3]}}
```

or even

```
{match: { title: ['quick brown fox', 'jumpy fox overdosed on drugs'] }}
```

that way we could relegate the `term` and `terms` queries to the outer reaches of knowing-what-you're-doing.

@chrismale i was told to ping you about this one ;)
</description><key id="8586705">2435</key><summary>match query supports multiple query strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-11-22T20:29:34Z</created><updated>2013-01-16T16:37:19Z</updated><resolved>2013-01-16T16:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismale" created="2012-11-22T21:17:13Z" id="10645212">Very doable.  The only additional configuration option we need is what boolean operator to apply across strings.  For example, is 'foo', 'bar' translated into foo AND bar or foo OR bar.  We currently support configuring the operator used inside a string (so 'quick brown fox' becomes quick AND brown AND fox).
</comment><comment author="clintongormley" created="2012-11-23T09:35:06Z" id="10654148">Hmm, I'm having second thoughts on this - wondering if it overcomplicates our default query and thus interferes with understanding.  It all seems fine when you're dealing with terms: 

```
{ match: { tag: ['foo','bar','baz'] }}
```

But the `terms` query also supports `minimum_should_match`, which in the `match` query is already used for rewriting analyzed queries.

Then, how do you combine them? We could, as you suggest, just do a hardcoded `bool-should` clause with `minimum_should_match = 1` which will cover 90% of use cases. 

But then somebody will open an issue asking us to support other execution plans and configurable minimums...  Do we want to go down that rabbit hole?

Do we perhaps only enable the multi-query form for `not_analyzed` fields?  So you can't do:

```
{ match: { title: [ "quick brown fox", "jumping jack rabbit" ] }}
```

That way we can reuse the `minimum_should_match` and `operator` parameters that already exist in the `match` query.
Or is that just one more special case too many?

Thoughts?
</comment><comment author="clintongormley" created="2013-01-16T16:37:19Z" id="12327122">It looks like this change was never committed, which I think was a good thing. In retrospect, it just made everything too confusing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reassigning aliases to the newly created indexes using Aliases API (Java Client)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2434</link><project id="" key="" /><description>Hi folks, 
my question is about reassigning an alias to the newly created index in one step. 
It means, i don't want to first remove alias and add a new one, since it is not possible to send the request encapsulated in one request. IndicesAliasesRequest doesn't offer any method like reassign() or removeAndAddAlias(). The delay between remove and add requests can cause service interruptions. 
 Is there any workaround to solve this ? 
</description><key id="8579039">2434</key><summary>Reassigning aliases to the newly created indexes using Aliases API (Java Client)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bagdemir</reporter><labels /><created>2012-11-22T14:35:59Z</created><updated>2012-11-22T15:25:08Z</updated><resolved>2012-11-22T15:25:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2012-11-22T15:10:56Z" id="10637209">The alias API accept a list of add/remove actions which are executed atomically in one call. Java-wise you can do the following:

``` java
client.admin().indices().prepareAliases()
                .removeAlias("old_index", "my_alias")
                .addAlias("new_index", "my_alias")
                .execute().actionGet();
```
</comment><comment author="bagdemir" created="2012-11-22T15:24:48Z" id="10637592">oh thanks your reply. the issue can be closed. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Lucene's native encoding of numeric stored values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2433</link><project id="" key="" /><description>Currently ElasticSearch converts numeric values to byte arrays for storing.  However Lucene has native storage of numeric values which has less overhead.

We need to support backwards compatibility with indexes using the byte array encoding, therefore we will use the ElasticSearch index version as a way to identify those indexes that will be using the byte array encoding and those that will be using the native encoding.
</description><key id="8572482">2433</key><summary>Use Lucene's native encoding of numeric stored values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2012-11-22T09:57:11Z</created><updated>2013-02-26T09:32:15Z</updated><resolved>2013-02-26T09:32:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-26T09:32:15Z" id="14102883">Implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match query should fail when trying to provide several fields in its simplified form</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2432</link><project id="" key="" /><description>For example, this query should fail, because operator is being used in its simplified form:

```
curl "http://localhost:9200/test/orte/_search?pretty=1" -d '
{
  query: {
   "match": {
     "synonyme" : "das alte",
     "operator" : "or"
   }
  },
  "highlight" : {
    "fields" : {
        "synonyme" : {}
    }
  }
}
'
```

This issue was created because of #2431
</description><key id="8571282">2432</key><summary>Match query should fail when trying to provide several fields in its simplified form</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-22T09:13:03Z</created><updated>2012-11-22T09:23:53Z</updated><resolved>2012-11-22T09:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Highlight won't work with match/text query when operator is given.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2431</link><project id="" key="" /><description>The example shows, that with an "operator" given, the highlighting does not work anymore. This is on 0.20.0.RC1.

``` bash
curl -s -X DELETE "http://localhost:9200/test?pretty=1"
curl -s -X PUT    "http://localhost:9200/test?pretty=1" -d '
{
    "mappings": {
        "orte": {}
    }
}
'

curl -s -X POST "http://localhost:9200/test/orte?pretty=1" -d '
{
   "synonyme": "Das alte Land"
}
'
curl -s -X POST "http://localhost:9200/test/_refresh"
echo
echo "-----------highlight: OK --"
curl "http://localhost:9200/test/orte/_search?pretty=1" -d '
{
  query: {
   "match": {
     "synonyme" : "das alte"
   }
  },
  "highlight" : {
    "fields" : {
        "synonyme" : {}
    }
  }
}
'
echo
echo "-----------no highlight because of 'operator': NOT OK --"
curl "http://localhost:9200/test/orte/_search?pretty=1" -d '
{
  query: {
   "match": {
     "synonyme" : "das alte",
     "operator" : "or"
   }
  },
  "highlight" : {
    "fields" : {
        "synonyme" : {}
    }
  }
}
'
```
</description><key id="8570964">2431</key><summary>Highlight won't work with match/text query when operator is given.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">programmfabrik</reporter><labels /><created>2012-11-22T08:59:28Z</created><updated>2013-05-23T09:06:55Z</updated><resolved>2013-05-23T09:06:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-22T09:14:19Z" id="10627861">Hi, there is a problem in your query format when using the operator, see above for issue #2432 that I opened to properly fail because of that. Here is the proper structure and then it will work:

```
curl "http://localhost:9200/test/orte/_search?pretty=1" -d '
{
  query: {
   "match": {
     "synonyme" : {
        "query" : "das alte",
        "operator" : "or"
     }
   }
  },
  "highlight" : {
    "fields" : {
        "synonyme" : {}
    }
  }
}
'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High cpu spin in IO thread in http_server_worker, not sure what causes it.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2430</link><project id="" key="" /><description>Observed behavior: elasticsearch was not listening on any http port.

Not sure what the cause is, but I observed http requests to one elasticsearch node failling. Here's some debugging data I was able to gather 

top -H output (Threads, sorted by cpu usage)

```
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                               
18280 elastics  20   0 19.6g 651m 9248 R   97  4.1  93:34.72 java                                                   
10703 elastics  20   0 19.6g 651m 9248 S    0  4.1   0:00.00 java                                                   
10710 elastics  20   0 19.6g 651m 9248 S    0  4.1   0:02.62 java                                                   
10711 elastics  20   0 19.6g 651m 9248 S    0  4.1   0:22.96 java                                                   
10712 elastics  20   0 19.6g 651m 9248 S    0  4.1   0:22.95 java                                                   
```

pid/thread 18280 == 0x4768

Sent 'kill -QUIT' to the elasticsearch process

```
~# grep -A10 nid=0x4768 /etc/service/elasticsearch/log/current 
2012-11-21_23:11:43.16649 "elasticsearch[peon8480][http_server_worker][T#26]{New I/O  worker #90}" daemon prio=10 tid=0x0000030e1c2b7800 nid=0x4768 runnable [0x0000030fb9daf000]
2012-11-21_23:11:43.16650    java.lang.Thread.State: RUNNABLE
2012-11-21_23:11:43.16651       at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
2012-11-21_23:11:43.16651       at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:228)
2012-11-21_23:11:43.16652       at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:81)
2012-11-21_23:11:43.16653       at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
2012-11-21_23:11:43.16654       - locked &lt;0x000003101ce5a268&gt; (a sun.nio.ch.Util$2)
2012-11-21_23:11:43.16654       - locked &lt;0x000003101ce5a250&gt; (a java.util.Collections$UnmodifiableSet)
2012-11-21_23:11:43.16656       - locked &lt;0x000003101c7b4d48&gt; (a sun.nio.ch.EPollSelectorImpl)
2012-11-21_23:11:43.16657       at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
2012-11-21_23:11:43.16658       at org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:52)

```

strace data for a 5 second sample:

```
root@peon8480:~# timeout  5 strace -c -p 18280
Process 18280 attached - interrupt to quit
Process 18280 detached
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 34.26    0.000418           0     48339           epoll_wait
 34.18    0.000417           0     48339           read
 31.56    0.000385           0     48338           write
  0.00    0.000000           0        28           futex
------ ----------- ----------- --------- --------- ----------------
100.00    0.001220                145044           total
```

strace call info

```
# strace -p 18280 |&amp; head -10
Process 18280 attached - interrupt to quit
write(277, "\1", 1)                     = 1
epoll_wait(269, {{EPOLLIN, {u32=273, u64=12133744393198764305}}}, 8192, 10) = 1
read(273, "\1", 128)                    = 1
write(277, "\1", 1)                     = 1
epoll_wait(269, {{EPOLLIN, {u32=273, u64=12133744393198764305}}}, 8192, 10) = 1
read(273, "\1", 128)                    = 1
write(277, "\1", 1)                     = 1
epoll_wait(269, {{EPOLLIN, {u32=273, u64=12133744393198764305}}}, 8192, 10) = 1
read(273, "\1", 128)                    = 1
```

jstack timed out, so I had no hope of attaching jvisualvm to it to see what was going on.

Environmental info:

```
# /proc/18280/exe -version
java version "1.7.0_03"
OpenJDK Runtime Environment (IcedTea7 2.1.1pre) (7~u3-2.1.1~pre1-1ubuntu2)
OpenJDK 64-Bit Server VM (build 22.0-b10, mixed mode)

# uname -srm
Linux 2.6.32.45-grsec-2.2.2-r3 x86_64

# /home/elasticsearch/elasticsearch-0.19.9/bin/elasticsearch -v
ElasticSearch Version: 0.19.9, JVM: 22.0-b10
```

lsof data:

```
# lsof -nPi :9200
COMMAND   PID     USER   FD   TYPE   DEVICE SIZE/OFF NODE NAME
java    11940 logstash   31u  IPv4 13046130      0t0  TCP 127.0.0.1:42704-&gt;127.0.0.1:9200 (CLOSE_WAIT)

# lsof -nPp 10703 | grep LISTEN
java    10703 elasticsearch   29u  IPv4 12970639        0t0      TCP *:9300 (LISTEN)
```

tcpdump:

```
# tcpdump -nni any 'port 9200'
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes
23:20:11.013451 IP 127.0.0.1.48170 &gt; 127.0.0.1.9200: Flags [S], seq 2376793071, win 32792, options [mss 16396,sackOK,TS val 650110564 ecr 0,nop,wscale 7], length 0
23:20:11.013476 IP 127.0.0.1.9200 &gt; 127.0.0.1.48170: Flags [R.], seq 0, ack 2376793072, win 0, length 0
23:20:12.365484 IP 127.0.0.1.48171 &gt; 127.0.0.1.9200: Flags [S], seq 3795810129, win 32792, options [mss 16396,sackOK,TS val 650110902 ecr 0,nop,wscale 7], length 0
23:20:12.365508 IP 127.0.0.1.9200 &gt; 127.0.0.1.48171: Flags [R.], seq 0, ack 3795810130, win 0, length 0
23:20:13.717383 IP 127.0.0.1.48172 &gt; 127.0.0.1.9200: Flags [S], seq 2977786653, win 32792, options [mss 16396,sackOK,TS val 650111240 ecr 0,nop,wscale 7], length 0
23:20:13.717407 IP 127.0.0.1.9200 &gt; 127.0.0.1.48172: Flags [R.], seq 0, ack 2977786654, win 0, length 0
```
</description><key id="8562099">2430</key><summary>High cpu spin in IO thread in http_server_worker, not sure what causes it.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels /><created>2012-11-21T23:22:56Z</created><updated>2013-06-26T16:12:10Z</updated><resolved>2013-06-26T15:42:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2012-11-21T23:23:12Z" id="10618381">Saw this recently in the logs:

```
2012-11-21_21:37:26.84550 java.lang.NullPointerException
2012-11-21_21:37:26.84551       at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:75)
2012-11-21_21:37:26.84551       at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:198)
2012-11-21_21:37:26.84552       at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:70)
2012-11-21_21:37:26.84552       at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:165)
2012-11-21_21:37:26.84552       at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:115)
2012-11-21_21:37:26.84553       at org.elasticsearch.cluster.routing.RoutingService$1.execute(RoutingService.java:135)
2012-11-21_21:37:26.84553       at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:211)
```
</comment><comment author="jordansissel" created="2012-11-21T23:23:12Z" id="10618382">Saw this recently in the logs:

```
2012-11-21_21:37:26.84550 java.lang.NullPointerException
2012-11-21_21:37:26.84551       at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:75)
2012-11-21_21:37:26.84551       at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:198)
2012-11-21_21:37:26.84552       at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:70)
2012-11-21_21:37:26.84552       at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:165)
2012-11-21_21:37:26.84552       at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:115)
2012-11-21_21:37:26.84553       at org.elasticsearch.cluster.routing.RoutingService$1.execute(RoutingService.java:135)
2012-11-21_21:37:26.84553       at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:211)
```
</comment><comment author="kimchy" created="2012-11-23T09:11:02Z" id="10653624">Hey @jordansissel, which version of elasticsearch were you using? A similar problem (for the NullPointerException you get) was fixed in 0.19.10.

Regarding the high CPU problem, a new API was added to elasticsearch called hot threads, can you issue it if you see high CPU? Its: `curl localhost:9200/hot_threads`. Though I know that HTTP was problematic, maybe if you hit another node in the cluster, it will be able to execute it on the relevant node using the internal 9300 port.

Also, regarding the Java version, its `1.7.0_03`. I know netty has some problems with "earlier" 1.7 releases (because of some NIO bugs), can you try and use a later version?
</comment><comment author="spinscale" created="2013-06-26T15:42:37Z" id="20057094">Stale. Closing for now, but more than happy to reopen with more information.
</comment><comment author="jordansissel" created="2013-06-26T16:12:10Z" id="20060002">+1, I haven't seen this problem since filing so I'm not worried about it anymore. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot change MatchQuery behaviour with 0 terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2429</link><project id="" key="" /><description>A common problem that arises in query parsing is what to do when analysis strips all the terms from the input string.  For example, when the query is just "i" or even something more complex like "i AND you", stopword analysis will leave a clause without any terms.  

In this situation, both Lucene and ElasticSearch view the clause as an empty-set of terms which matches nothing.  However users often would prefer the clause to instead be a MatchAll Query, matching every document.  

Currently `MatchQuery` has hardcoded behaviour that uses `MatchNoDocsQuery` in this situation.  Instead we should allow users to provide a flag as part of a `match` query that gives them control over what to do when there is an empty-set clause.
</description><key id="8560256">2429</key><summary>Cannot change MatchQuery behaviour with 0 terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>enhancement</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-21T22:07:18Z</created><updated>2012-11-22T09:14:38Z</updated><resolved>2012-11-22T09:14:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismale" created="2012-11-21T22:18:57Z" id="10616700">Will add a flag `zero_term_docs` to the `match` query, which accepts two possible values, `none` which replicates the existing behaviour and will be the default and `all` which will create a `MatchAllDocsQuery` instead.
</comment><comment author="chrismale" created="2012-11-22T09:14:38Z" id="10627872">Added flag `zero_terms_query` in 2541847
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting logger levels using cluster update settings does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2428</link><project id="" key="" /><description /><key id="8541315">2428</key><summary>Setting logger levels using cluster update settings does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-21T12:44:36Z</created><updated>2012-11-21T12:45:06Z</updated><resolved>2012-11-21T12:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Using percolate API for new document classification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2427</link><project id="" key="" /><description>Hi,

This comes out of a conversationat the Barcelona training. (it was suggested that i should comment here with this)

The percolate API takes queries against an index and when a new document is added, it flags which queries in an index match against it (e.g. a simple boolean yes/no).

Would it be possible to extend percolate so that it could be used as a form of index classification e.g. I have x percolate queries registered in index A and y percolate queries in index B, tell me which index i match the new content on based on percolation.

I'm guessing this is a logical extension of some form of exposing the term frequency (or other) already in each index to classify a new document. The idea would be that you could work out what terms are 'good training classification data' for an index and then use that to figure out where a document should go?

Derry
</description><key id="8534661">2427</key><summary>Using percolate API for new document classification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">derryos</reporter><labels /><created>2012-11-21T08:17:48Z</created><updated>2014-07-08T18:34:31Z</updated><resolved>2014-07-08T18:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-27T20:29:31Z" id="23368027">The redesigned percolate api in master allows you to percolate against any index. In your case you could just create a count percolate request per index you want to percolate to and combine those requests in the multi percolate api. Also the percolate response includes to what index a match belongs to. I think the number of queries matching per percolate index is usable to classify your documents?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When script_score script returns NaN, entire request fails with a confusing error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2426</link><project id="" key="" /><description>Repro: https://gist.github.com/4122515
Error message:

``` json
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {IllegalArgumentException[docID must be &gt;= 0 and &lt; maxDoc=3 (got docID=2147483647)]}]",
    "status": 500
}
```
</description><key id="8528091">2426</key><summary>When script_score script returns NaN, entire request fails with a confusing error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Scripting</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2012-11-21T01:40:25Z</created><updated>2014-12-08T09:27:51Z</updated><resolved>2014-12-08T09:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nekulin" created="2013-03-16T06:49:12Z" id="15000523">how do you solved this problem?
</comment><comment author="clintongormley" created="2014-11-29T13:11:15Z" id="64951351">This is still an issue with `script_score` in the `function_score` query:

```
DELETE /test-idx

PUT /test-idx
{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  },
  "mappings": {
    "doc": {
      "properties": {
        "name": {
          "type": "string"
        }
      }
    }
  }
}

PUT /test-idx/doc/1
{"name":"foo"}

PUT /test-idx/doc/2
{"name":"bar"}

PUT /test-idx/doc/3
{"name":"baz"}

GET /test-idx/doc/_search
{
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "script_score": {
        "script": "Float.NaN"
      }
    }
  }
}
```

The above throws this exception:

```
"SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[QXq3EIF5RcqhjIVRReuKhQ][test-idx][0]: IllegalArgumentException[docID must be &gt;= 0 and &lt; maxDoc=3 (got docID=2147483647)]}]"
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse TokenStreams for not_analyzed Strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2425</link><project id="" key="" /><description>The TokenStream for not_analyzed Strings is very simple, emitting the value as a single term.  We can increase performance little by reusing the TokenStream (saving instantiation cost plus less gc).
</description><key id="8527210">2425</key><summary>Reuse TokenStreams for not_analyzed Strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2012-11-21T00:53:59Z</created><updated>2013-02-18T19:21:48Z</updated><resolved>2013-02-18T19:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-18T19:21:48Z" id="13738247">Closing this, we are already reusing the not_analyzed tokenstream in StringFieldMapper.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose Lucene's new Similarities per-field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2424</link><project id="" key="" /><description>Lucene 4.0 includes some large changes to the `Similarity` API, allowing simpler extension beyond the usual TF/IDF algorithm.  As part of this, new algorithms have been added including BM25.  Also as part of the changes, it is now possible to define a Similarity per field, giving even greater control over scoring.
### Configuring Similarity per Field

Defining the Similarity for a field is done via the `similarity` mapping property, as this example shows:

```
{
  "book" : {
    "properties" : {
      "title" : { "type" : "string", "similarity" : "BM25" }
    }
} 
```

The following Similarties are configured out-of -box:
- `default` - The Default TF/IDF algorithm used by Lucene in previous versions
- `BM25` - The BM25 algorithm.  See http://en.wikipedia.org/wiki/Okapi_BM25 for more details

Additionally, functionality for configuring the following Similarities is provided:
- `DFR`
- `IB`
### Configuring Similarity Properties

Most existing or custom Similarities have configuration options which can be configured.  This is done via index settings as shown below:

```
"similarity" : {
  "my_similarity : {
    "type" : "DFR",
    "basic_model" : "g",
    "after_effect" : "l",
    "normalization" : "h2",
    "normalization.h2.c" : "3.0"
  }
}
```

Here we configure the DFRSimilarity so it can be referenced as `my_similarity` in mappings.
### Default and Base Similarities

By default, ElasticSearch will use whatever Similarity is configured as `default`.  However, the Similarity functions `queryNorm()` and `coord()` are not per-field.  Consequently, for expert users wanting to change the implementation used for these two methods, while not changing the `default`, it is possible to configure a Similarity with the name `base`.  This Similarity will then be used for the two methods.
</description><key id="8525494">2424</key><summary>Expose Lucene's new Similarities per-field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2012-11-20T23:43:41Z</created><updated>2012-11-20T23:57:10Z</updated><resolved>2012-11-20T23:57:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismale" created="2012-11-20T23:57:10Z" id="10579979">Pushed in 9e2469e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term facets are disregarding filters specified by aliases when all_terms flag is set to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2423</link><project id="" key="" /><description>Repro: https://gist.github.com/4118328
</description><key id="8506774">2423</key><summary>Term facets are disregarding filters specified by aliases when all_terms flag is set to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2012-11-20T14:44:19Z</created><updated>2014-11-28T18:54:03Z</updated><resolved>2014-11-28T18:54:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-02T17:28:15Z" id="14331714">@imotov is this still valid? maybe you can provide a fix for this?
</comment><comment author="spinscale" created="2013-07-18T15:17:40Z" id="21190828">This is still valid, java test to reproduce below. From my limited understanding of the fielddata (which contains everything anyway), the `context.aliasFilter()` needs to be taken into account, but I'm not really sure, where this would be. Either in the `TermsFacetParser` or in each facet executor. Interestingly enough the totalCount is correct, but the entries are wrong.

```
    @Test
    public void allTermsFacetDoesNotIgnoreAliasesFilter() throws Exception {
        createIndex("test");
        ensureYellow();

        client().admin().indices().prepareAliases().addAlias("test", "test-alias", FilterBuilders.termFilter("tag", "blue")).get();

        client().prepareIndex("test", "doc", "1").setSource(jsonBuilder().startObject().field("tag", "blue").field("value", "one").endObject()).get();
        client().prepareIndex("test", "doc", "2").setSource(jsonBuilder().startObject().field("tag", "blue").field("value", "two").endObject()).get();
        client().prepareIndex("test", "doc", "3").setSource(jsonBuilder().startObject().field("tag", "green").field("value", "three").endObject()).get();
        client().prepareIndex("test", "doc", "4").setSource(jsonBuilder().startObject().field("tag", "green").field("value", "four").endObject()).get();
        client().admin().indices().prepareRefresh("test").get();

        SearchResponse searchResponse = client().prepareSearch("test-alias").setTypes("doc").setQuery(matchAllQuery()).addFacet(FacetBuilders.termsFacet("tags").allTerms(true).field("value")).get();
        assertHitCount(searchResponse, 2);
        TermsFacet termsFacet = searchResponse.getFacets().facet("tags");
        assertThat(termsFacet, is(notNullValue()));
        assertThat(termsFacet.getTotalCount(), is(2L)); // already correct
        assertThat(termsFacet.getEntries().size(), is(2)); // broken here...
        Iterator&lt;Entry&gt; iterator = termsFacet.iterator();
        while (iterator.hasNext()) {
            Entry entry = iterator.next();
            assertThat(entry.getTerm().string(), is(not("three")));
            assertThat(entry.getTerm().string(), is(not("four")));
        }
    }
```
</comment><comment author="nicolasgarnil" created="2013-10-30T15:29:03Z" id="27399493">+1
</comment><comment author="nkabbara" created="2013-11-02T19:52:24Z" id="27631000">+1
</comment><comment author="clintongormley" created="2014-07-08T18:31:09Z" id="48380741">The same bug is visible in aggregations.
</comment><comment author="clintongormley" created="2014-11-28T18:54:03Z" id="64921210">I don't think this is solvable, at least not efficiently.  It's the same reason that we don't exclude terms from multi-values fields where one term matches a filter but the other doesn't.

Closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mongodb   _id elasticsearch _id  sort error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2422</link><project id="" key="" /><description>mongodb  _id elasticsearch _id  sort error...
</description><key id="8497150">2422</key><summary>mongodb   _id elasticsearch _id  sort error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peterwillcn</reporter><labels /><created>2012-11-20T09:00:03Z</created><updated>2012-11-20T09:52:28Z</updated><resolved>2012-11-20T09:52:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="peterwillcn" created="2012-11-20T09:00:59Z" id="10546886">#&lt;Tire::Results::Collection:0x00000006710768 @response={"took"=&gt;4, "timed_out"=&gt;false, "_shards"=&gt;{"total"=&gt;5, "successful"=&gt;5, "failed"=&gt;0}, "hits"=&gt;{"total"=&gt;362, "max_score"=&gt;nil, "hits"=&gt;[{"_index"=&gt;"posts", "_type"=&gt;"post_mongo", "_id"=&gt;"28", "_score"=&gt;nil, "_source"=&gt;{"_id"=&gt;28, "content"=&gt;"&#27004;&#20027;&#35828;&#24471;&#22909;&#21834;&#65292;&#25105;&#39030;&#21834;", "created_at"=&gt;"2012-06-05T18:14:46+08:00", "title"=&gt;"&#27979;&#35797;post&#26631;&#39064;#1"}, "sort"=&gt;[nil]}, {"_index"=&gt;"posts", "_type"=&gt;"post_mongo", "_id"=&gt;"47", "_score"=&gt;nil, "_source"=&gt;{"_id"=&gt;47, "content"=&gt;"&#27004;&#20027;&#35828;&#24471;&#22909;&#21834;&#65292;&#25105;&#39030;&#21834;", "created_at"=&gt;"2012-06-05T18:14:47+08:00", "title"=&gt;"&#27979;&#35797;post&#26631;&#39064;#8"}, "sort"=&gt;[nil]},
</comment><comment author="peterwillcn" created="2012-11-20T09:01:16Z" id="10546896"> "sort"=&gt;[nil]
</comment><comment author="clintongormley" created="2012-11-20T09:52:28Z" id="10548308">You can't sort on the `_id` field unless you map it to be indexed:

```
 { "_id": { "index": "not_analyzed" }}
```

http://www.elasticsearch.org/guide/reference/mapping/id-field.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>@Required annotation has description for @Nullable one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2421</link><project id="" key="" /><description>Copy-paste issue. Would be nice to fix the description for @Required annotation.
</description><key id="8489524">2421</key><summary>@Required annotation has description for @Nullable one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dspasibenko</reporter><labels /><created>2012-11-20T01:10:14Z</created><updated>2013-10-07T15:42:42Z</updated><resolved>2013-10-07T15:42:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-07T15:42:42Z" id="25819476">hey

I actually just removed the annotation, so the documentation issue is gone now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix sporadically disappearing fields during concurrent dynamic mapping u...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2420</link><project id="" key="" /><description>...pdates
</description><key id="8487951">2420</key><summary>fix sporadically disappearing fields during concurrent dynamic mapping u...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-19T23:54:41Z</created><updated>2014-06-16T19:42:03Z</updated><resolved>2012-11-24T13:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-24T13:22:55Z" id="10677629">Pushed to 0.20 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch stops itself randomly.. No error visible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2419</link><project id="" key="" /><description>I installed the Debian package (0.19.11) on Ubuntu 12.04.1 LTS. Running with Oracle Java 7 had this same issue happening on OpenJDK 7

java version "1.7.0_09"
Java(TM) SE Runtime Environment (build 1.7.0_09-b05)
Java HotSpot(TM) 64-Bit Server VM (build 23.5-b02, mixed mode)

Basically after running for about 20 minutes elasticsearch will just stop running. I can't find a crash in the error log or even a shutdown sequence it's just not running anymore.

I am a little unsure what queries are being run. I am running it with Graylog2. But I don't think that is the issue as I turned off the Graylog daemons and even with nothing connected elasticsearch still terminated itself.

Graylog2 Server - 0.9.6p1 - Current Stable
Graylog2 Web Interface - 0.9.6p1 - Current Stable
</description><key id="8435383">2419</key><summary>Elasticsearch stops itself randomly.. No error visible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KellyLSB</reporter><labels /><created>2012-11-16T23:30:10Z</created><updated>2017-07-22T16:06:13Z</updated><resolved>2012-11-17T00:16:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-11-16T23:44:56Z" id="10466979">Check syslog and kern.log around the time when elasticsearch disappeared, do you see anything about oom-killer there?
</comment><comment author="KellyLSB" created="2012-11-16T23:53:48Z" id="10467177">Yes there is. I am running this on an Amazon EC2 Micro instance. This server only serves analytical and logging data and receives minimal web traffic.

```
[809124.687157] [ pid ]   uid  tgid total_vm      rss cpu oom_adj oom_score_adj name
[809124.687165] [  246]     0   246     4308       46   0       0             0 upstart-udev-br
[809124.687169] [  250]     0   250     5367       97   0     -17         -1000 udevd
[809124.687172] [  309]     0   309     5366       98   0     -17         -1000 udevd
[809124.687176] [  313]     0   313     5366       98   0     -17         -1000 udevd
[809124.687179] [  377]     0   377     1816      124   0       0             0 dhclient3
[809124.687183] [  382]     0   382     3797       47   0       0             0 upstart-socket-
[809124.687187] [  595]     0   595    12489      151   0     -17         -1000 sshd
[809124.687190] [  604]   102   604     5982       80   0       0             0 dbus-daemon
[809124.687194] [  621]   101   621    63429      103   0       0             0 rsyslogd
[809124.687197] [  674]     0   674     3626       42   0       0             0 getty
[809124.687201] [  683]     0   683     3626       40   0       0             0 getty
[809124.687204] [  685]     0   685   140793    31479   0       0             0 ruby
[809124.687208] [  689]     0   689     3626       42   0       0             0 getty
[809124.687211] [  691]     0   691     3626       42   0       0             0 getty
[809124.687215] [  693]     0   693     3626       41   0       0             0 getty
[809124.687218] [  702]   107   702    87952     6706   0       0             0 mongod
[809124.687222] [  704]     0   704     1082       37   0       0             0 acpid
[809124.687225] [  705]     0   705     4778       56   0       0             0 cron
[809124.687228] [  706]     0   706     4227       40   0       0             0 atd
[809124.687232] [  724]   103   724    46897      305   0       0             0 whoopsie
[809124.687235] [  742]     0   742    15706      256   0       0             0 nginx
[809124.687239] [  744]    33   744    15785      330   0       0             0 nginx
[809124.687243] [  745]    33   745    15785      330   0       0             0 nginx
[809124.687246] [  746]    33   746    15861      371   0       0             0 nginx
[809124.687250] [  747]    33   747    15785      330   0       0             0 nginx
[809124.687254] [  759]     0   759    14767      641   0       0             0 php5-fpm
[809124.687257] [  760]    33   760    14767      635   0       0             0 php5-fpm
[809124.687261] [  761]    33   761    14767      635   0       0             0 php5-fpm
[809124.687264] [  762]    33   762    14767      635   0       0             0 php5-fpm
[809124.687268] [  763]    33   763    14767      635   0       0             0 php5-fpm
[809124.687271] [  795]     0   795     3626       41   0       0             0 getty
[809124.687275] [ 1016]  1000  1016     6650      203   0       0             0 tmux
[809124.687279] [ 1017]  1000  1017     1100       25   0       0             0 sh
[809124.687282] [ 1021]  1000  1021     5429      639   0       0             0 bash
[809124.687285] [ 2267]   106  2267   474148    58629   0       0             0 java
[809124.687289] [ 3036]     0  3036   223097    38592   0       0             0 java
[809124.687292] Out of memory: Kill process 2267 (java) score 389 or sacrifice child
[809124.687308] Killed process 2267 (java) total-vm:1896592kB, anon-rss:234516kB, file-rss:0kB
```
</comment><comment author="imotov" created="2012-11-17T00:14:57Z" id="10467799">Basically, what happens here is system runs out of physical memory and kills the es out of self preservation. There are only two things you can do - reduce memory consumed by es or add more memory to the server (move to a larger instance in your case). Considering that micro instance has only 613M of memory, I would suggest the latter approach unless you have really small index. 
</comment><comment author="KellyLSB" created="2012-11-17T00:16:47Z" id="10467833">yah i just went ahead and boosted the memory. thanks.
</comment><comment author="johnfelipe" created="2015-05-15T01:06:00Z" id="102214675">I have same problem in VPS with 2 gb ram memory, please help
</comment><comment author="brianvoss" created="2015-06-25T19:41:03Z" id="115373112">I'm having the same problem on an r3.2xlarge instance with 60 GB of RAM.  

OS:  Ubuntu 14.04.2 LTS (GNU/Linux 3.13.0-48-generic x86_64).
I increased the heap size to 40GB, so that is definitely not the problem.  The only message I see in syslog is:

```
Jun 25 17:53:57 ip-172-31-14-128 kernel: [42761.897554] init: elasticsearch main process (3697) terminated with status 143
Jun 25 17:53:57 ip-172-31-14-128 kernel: [42761.897566] init: elasticsearch main process ended, respawning
Jun 25 17:55:21 ip-172-31-14-128 kernel: [42846.452398] init: elasticsearch main process (4003) terminated with status 143
```
</comment><comment author="nik9000" created="2015-06-25T19:47:51Z" id="115374634">That's elasticsearch being killed by a sigterm signal - the same one sent by `kill &lt;pid&gt;`

You may want to check if it was killed by the oomkiller - I don't remember if it tries a sigterm before the sigkill but it might and that'd look like this.

If it is the oomkiller this isn't an Elasticsearch bug - its you giving the process more memory than the machine can let it have. If.

References:
http://tldp.org/LDP/abs/html/exitcodes.html#EXITCODESREF
http://unixhelp.ed.ac.uk/CGI/man-cgi?signal+7
</comment><comment author="cnvo" created="2016-06-14T14:00:33Z" id="225889828">This might be old, but you might get an error such as this:

-bash: fork: Cannot allocate memory

...if you try starting a new instance.
Bottom like as stated, it's usually a memory issue when it randomly terminated.
</comment><comment author="keshav-marketingmindz" created="2016-09-28T05:18:57Z" id="250074077">i am having the same issue, i have 4gb ram and a 2 core cpu running elasticsearch 2.4.0 on ubuntu 14.04.
Its getting stopped without any error visible in the logs.
Can someone help me in this
</comment><comment author="nextofsearch" created="2016-10-30T10:46:13Z" id="257143896">@keshav-marketingmindz Did you solve your issue? I am in the same boat with you.
</comment><comment author="imotov" created="2016-10-31T20:25:45Z" id="257410382">@nextofsearch what do you see in syslog for the time when elasticsearch died?
</comment><comment author="keshav-marketingmindz" created="2016-11-02T06:33:15Z" id="257784041">@nextofsearch Yes i resolved it, i used two different servers both are centOS. One server for elasticsearch only and one for other such as php mysql and our website. This worked for me. Hope will work for you too. 
Thanks
</comment><comment author="wangwanchao" created="2017-07-02T13:26:15Z" id="312491831">i hava the same problem,then i try to modify the elasticsearch.conf&#65306;set.default.ES_HEAP_SIZE=256&#65292;it works! So i think the reason is the deficiency of memory.Hope it's helpful(&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#35273;&#24471;&#26159;&#26381;&#21153;&#22120;&#20869;&#23384;&#19981;&#36275;&#30340;&#21407;&#22240;&#65292;&#21407;&#26469;&#30340;ES_HEAP_SIZE&#35797;&#36807;1024&#12289;512&#65292;1024&#26080;&#27861;&#21551;&#21160;&#65292;512&#21551;&#21160;&#20197;&#21518;&#20250;&#31361;&#28982;&#23849;&#28291;&#65292;256&#21487;&#20197;&#27491;&#24120;&#36816;&#34892;)
STATUS | wrapper  | 2017/07/02 21:09:00 | Launching a JVM...
INFO   | jvm 4    | 2017/07/02 21:09:03 | WrapperManager: Initializing...
STATUS | wrapper  | 2017/07/02 21:09:11 | JVM received a signal SIGKILL (9).
STATUS | wrapper  | 2017/07/02 21:09:11 | JVM process is gone.
ERROR  | wrapper  | 2017/07/02 21:09:11 | JVM exited unexpectedly.
</comment><comment author="samthomson" created="2017-07-22T16:06:13Z" id="317193049">@brianvoss elastic recommend not going above 32 Gb https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html</comment></comments><attachments /><subtasks /><customfields /></item><item><title>empty routing.allocation.exclude.tag breaks all shard allocations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2418</link><project id="" key="" /><description>If [i|c].r.a.exclude.tag is set to "" at any level (template, index or cluster settings) it will cause all allocation of shards with that setting to break. If a index already exists and the setting is applied one the primaries will come up properly but replicas will refuse to start if the cluster is restarted.

```
"cluster.routing.allocation.exclude.tag" : ""
"index.routing.allocation.exclude.tag" : ""
```

Debug log shows messages like:

```
[2012-11-15 10:51:24,806][DEBUG][cluster.routing.allocation] [Frontend 1] [infuseds-2012.08.24][3] allocated on [[Storage 3][0gmkNk6HReiXV8udG7HBpQ][inet[/10.38.16.136:9300]]{tag=storage, master=false}], but can no longer be allocated on it, moving...
[2012-11-15 10:51:24,807][DEBUG][cluster.routing.allocation] [Frontend 1] [infuseds-2012.08.24][3] can't move
```

Problem started with 0.19.10 and persists in 0.19.11.

Tested with clean setup, allocation worked properly until exclude.tag was set, not sure if other exclude variables are effected by this.

Changing the exclude.tag to a value such as "fuuuuu" fixes the bug and everything works normally.
</description><key id="8419448">2418</key><summary>empty routing.allocation.exclude.tag breaks all shard allocations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JamesDooley</reporter><labels /><created>2012-11-16T13:32:01Z</created><updated>2012-11-22T12:29:30Z</updated><resolved>2012-11-16T16:56:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-11-16T13:33:57Z" id="10446915">I believe it was fixed by #2404
</comment><comment author="JamesDooley" created="2012-11-16T16:55:56Z" id="10453626">Ok, will test when 0.19.12 is released 
</comment><comment author="gakhov" created="2012-11-22T12:29:30Z" id="10632972">Have exactly the same issue in 0.19.11. This bug report saved me a lot of time!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to skip incompatible fields on multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2417</link><project id="" key="" /><description>I believe this is a typical use case, we allow a user to type a search string and then use it to search against a set of fields. Unfortunately, currently that doesn't allow to include integer fields (say, document ID) in this set. Following a simple example:

``` shell
$ curl -XGET http://localhost:9200/gallery/item/_search -d '{query:
  {multi_match: {fields: ["id", "name"], query: "test"}}
}'
```

which gives:
_{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[LJs6UPnTQ_u-Ws0SAna0zA][gallery][0]: SearchParseException[[gallery][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"id\", \"name\"], query: \"test\"}}}]]]; nested: NumberFormatException[For input string: \"test\"]; }{[LJs6UPnTQ_u-Ws0SAna0zA][gallery][1]: SearchParseException[[gallery][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\": {\"multi_match\": {\"fields\": [\"id\", \"name\"], query: \"test\"}}}]]]; nested: NumberFormatException[For input string: \"test\"]; }]","status":500}_

I did read the discussions in Google Groups on that matter and I agree that it's not really correct to simply ignore those errors. However, since it **is** a typical use case which is very commonly used  indeed (for instance, that's how Django Admin search_fields works), I believe we need to have a multi_match query option (off by default) which will allow such queries to complete.

Supposedly, that will be something like:

``` shell
$ curl -XGET http://localhost:9200/gallery/item/_search -d '{query:
  {multi_match: {fields: ["id", "name"], query: "test", "match_field_types": true}}
}'  # automatically remove 'id' from fields because of the incompatible type
```
</description><key id="8418767">2417</key><summary>Add option to skip incompatible fields on multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IlyaSemenov</reporter><labels /><created>2012-11-16T12:56:15Z</created><updated>2012-11-28T18:29:51Z</updated><resolved>2012-11-28T18:29:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-23T08:53:11Z" id="10653264">Agreed!, in 0.20, we have support for a lenient flag that supports it.
</comment><comment author="IlyaSemenov" created="2012-11-28T18:29:51Z" id="10814573">My bad, this is a duplicate of #2156, and the problem is indeed resolved in 0.20RC.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices filter parsed for indices to which it should not apply</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2416</link><project id="" key="" /><description>A used in the forum is trying to apply a nested filter to just one index, while querying more than one index, some of which don't have the nested mapping.

The query is complaining about a missing nested mapping, but on the indices to which the query should not apply.

Create two indices: `test_1` with a nested mapping, and `test_2` without a nested mapping:

```
curl -XPUT 'http://127.0.0.1:9200/test_1/?pretty=1'  -d '
{
   "mappings" : {
      "foo" : {
         "properties" : {
            "authors" : {
               "type" : "nested",
               "properties" : {
                  "name" : {
                     "type" : "string"
                  }
               }
            },
            "title" : {
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XPUT 'http://127.0.0.1:9200/test_2/?pretty=1'  -d '
{
   "mappings" : {
      "bar" : {
         "properties" : {
            "title" : {
               "type" : "string"
            }
         }
      }
   }
}
'
```

Index some data:

```
curl -XPOST 'http://127.0.0.1:9200/test_1/foo?pretty=1'  -d '
{
   "author" : {
      "name" : "john smith"
   },
   "title" : "test 1 doc"
}
'

curl -XPOST 'http://127.0.0.1:9200/test_2/bar?pretty=1'  -d '
{
   "title" : "test 2 doc"
}
'
```

Query `test_1` and `test_2` but limit the nested filter to just `test_1`:

```
curl -XGET 'http://127.0.0.1:9200/test_1%2Ctest_2/_search?pretty=1'  -d '
{
   "query" : {
      "filtered" : {
         "filter" : {
            "indices" : {
               "no_match_filter" : "none",
               "filter" : {
                  "nested" : {
                     "filter" : {
                        "term" : {
                           "author.name" : "john"
                        }
                     },
                     "path" : "author"
                  }
               },
               "indices" : [
                  "test_1"
               ]
            }
         },
         "query" : {
            "match" : {
               "title" : "test"
            }
         }
      }
   }
}
'
```

Throws this error:

```
SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[Yit05d94RgiUwMg9vzMOgw][test_1][1]: SearchParseException[[test_1][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
   "query" : {
      "filtered" : {
         "filter" : {
            "indices" : {
               "no_match_filter" : "none",
               "filter" : {
                  "nested" : {
                     "filter" : {
                        "term" : {
                           "author.name" : "john"
                        }
                     },
                     "path" : "author"
                  }
               },
               "indices" : [
                  "test_1"
               ]
            }
         },
         "query" : {
            "match" : {
               "title" : "test"
            }
         }
      }
   }
}
```
</description><key id="8413689">2416</key><summary>Indices filter parsed for indices to which it should not apply</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.Beta2</label></labels><created>2012-11-16T08:40:53Z</created><updated>2013-11-14T17:33:01Z</updated><resolved>2013-11-14T17:30:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrewclegg" created="2013-09-11T20:37:54Z" id="24274195">I've seen this behaviour too. I have a horrible feeling that it's causing a whole pile of extra work on our cluster. If someone's running a search that only covers two weeks of data, we really don't want to start dozens of extra threads for the rest of the year, even if ultimately they return no data.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse and validate mappings in index templates when they are put</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2415</link><project id="" key="" /><description>Currently, putting a template will succeed if the JSON is valid even if the mapping couldn't be parsed. An error will be thrown when the index will be created and try to use the mapping, but that may be too late.

An example for such an error is described in https://github.com/elasticsearch/elasticsearch/issues/2414

The probable solution is to validate the mapping defined in the template when putting it.
</description><key id="8381228">2415</key><summary>Parse and validate mappings in index templates when they are put</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>:Index Templates</label><label>enhancement</label></labels><created>2012-11-15T09:01:33Z</created><updated>2016-06-15T07:54:25Z</updated><resolved>2016-06-01T08:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-10T08:20:20Z" id="19185874">Hey,

I absolutely agree we should have better error handling in that case. Do you have any other samples except the date format where this happens in templates?
</comment><comment author="synhershko" created="2013-06-10T08:24:21Z" id="19186040">Not at the moment. I assume analyzer names and various configuration options?
</comment><comment author="clintongormley" created="2014-07-08T18:27:51Z" id="48380260">I think all of these are now handled correctly. Please open a new issue if you find another problem.
</comment><comment author="johtani" created="2014-11-28T15:26:59Z" id="64904682">related https://github.com/elasticsearch/elasticsearch/issues/8695
now, we have mappings validation when we create index. no validation when create template.
</comment><comment author="clintongormley" created="2015-10-16T15:36:20Z" id="148748827">@johtani An idea from #14160: When checking the provided index template for validity, store the template as the JSON generated from the real mappings, instead of storing the original string.
</comment><comment author="johtani" created="2015-10-23T12:41:11Z" id="150560557">@clint I see. Just now I don't change storing logic. I will try it.
</comment><comment author="johtani" created="2016-01-18T16:04:18Z" id="172570782">@clint I tried to change storing logic in #8802, then I would like to separate with #8802. 
I need more time for implementing to store the JSON generated from the real mappings. Now, I think the JSON generated from the mappings has properties witbout default values., however, it is some problem for storing template.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support built-in named date formats in index mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2414</link><project id="" key="" /><description>At the moment, having this in a mapping throws a parsing error:

"topic_date" : { "type" : "date", "format": "date_optional_time||yyyy-MM-dd'T'HH:mm" }
</description><key id="8381159">2414</key><summary>Support built-in named date formats in index mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2012-11-15T08:57:52Z</created><updated>2013-06-10T08:20:19Z</updated><resolved>2013-06-10T08:18:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-10T08:18:39Z" id="19185813">Looks like a duplicate of #2132 - if not, I may have gotten your feature request wrong and will reopen happily.

I am trying to tackle this issue via https://github.com/elasticsearch/elasticsearch/pull/3150 at the moment
</comment><comment author="synhershko" created="2013-06-10T08:20:19Z" id="19185873">Seems like it, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix handling of stop word _lang_ notation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2413</link><project id="" key="" /><description>Fixes #2412
</description><key id="8373321">2413</key><summary>Fix handling of stop word _lang_ notation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-14T23:48:50Z</created><updated>2014-07-16T21:54:23Z</updated><resolved>2012-11-23T08:59:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-23T08:59:57Z" id="10653401">Pushed to master (0.21)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent handling of stop word _lang_ notation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2412</link><project id="" key="" /><description>The [`_lang_` notation](http://www.elasticsearch.org/guide/reference/index-modules/analysis/stop-tokenfilter.html) is expanded into language specific stop word list if it's used in an array but ignored if it's used in a comma-separated stop word list. See https://gist.github.com/58cc207ae58ba3e5400c for reproduction. 
</description><key id="8373265">2412</key><summary>Inconsistent handling of stop word _lang_ notation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-14T23:46:13Z</created><updated>2012-11-23T08:54:18Z</updated><resolved>2012-11-23T08:54:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose Lucene's codec api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2411</link><project id="" key="" /><description>This issue adds the option to configure a `PostingsFormat` and assign it to a field in the mapping. This feature is very expert and in almost all cases Elasticsearch's defaults will suite your needs.
## Configuring a postingsformat per field

There're several default postings formats configured by default which can be used in your mapping:
- `pulsing` - A postings format that encodes the postinglist for terms with low document frequency in the term directory.
- `direct` - A codec that wraps the default postings format during write time, but loads the terms and postinglists into memory directly in memory during read time as raw arrays. This postings format is exceptional memory intensive, but can give a substantial increase in search performance.
- `memory` - A codec that loads and stores terms and postinglists in memory using a FST. Acts like a cached postingslist.
- `bloom_default` - Maintains a bloom filter for the indexed terms, which is stored to disk and builds on top of the `default` postings format. This postings format is useful for low document frequency terms and offers a fail fast for seeks to terms that don't exist. 
- `bloom_pulsing` - Similar to the `bloom_default` postings format, but builds on top of the `pulsing` postings format.
- `default` - The default postings format. The default if none is specified.

On all fields it possible to configure a `postings_format` attribute. Example mapping:

```
{
  "person" : {
     "properties" : {
         "second_person_id" : {"type" : "string", "postings_format" : "pulsing"}
     }
  }
}
```
## Configuring a custom postingsformat

It is possible the instantiate custom postingsformats. This can be specified via the index settings. 

```
{
   "codec" : {
      "postings_format" : {
         "my_format" : {
            "type" : "pulsing"
            "freq_cut_off" : "5"
         } 
      }
   }
}
```

In the above example the `freq_cut_off` is set the 5 (defaults to 1). This tells the pulsing postings format to inline the postinglist of terms with a document frequency lower or equal to 5 in the term dictionary.

Note: when we doc this, we need to properly doc and expose all the configuration options for all codecs.
</description><key id="8371223">2411</key><summary>Expose Lucene's codec api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2012-11-14T22:29:26Z</created><updated>2013-02-17T13:29:12Z</updated><resolved>2012-11-14T22:54:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow facet counts to return count for 'MISSING' term</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2410</link><project id="" key="" /><description>Hi there.

I am using the term_stats facets to sum the value of a field 'grouped by' a facet key. Not all records in the index have a value for this facet key.

The only problem is dealing with these 'empty' facet keys.

By default, I get a histogram of available facet keys and sum(count). However the question is how to work out sum(count) for those records without the facet key?

If I were using a simple counting (term) facet (as opposed to term_stats with a sum) I could use the 'missing' value that is returned. However, that does not help as I need the sum(count field) for the missing records.

Ideally it would be possible to:
- Ask ES to sum the count field for the records without that facet field and return as missing_total
- (Maybe cleaner) Signal that I would like ES to consider a virtual 'MISSING' facet that gets stats applied, ranked and returned if applicable.
</description><key id="8359092">2410</key><summary>Allow facet counts to return count for 'MISSING' term</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">electric-al</reporter><labels /><created>2012-11-14T16:27:17Z</created><updated>2014-01-22T11:43:34Z</updated><resolved>2014-01-22T11:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vrecan" created="2013-09-07T01:35:45Z" id="23979134">:+1:  This would make graphing data a lot easier.
</comment><comment author="macarthy" created="2013-10-10T13:48:21Z" id="26054845">+1 
</comment><comment author="jpountz" created="2014-01-22T11:43:34Z" id="33014467">Closing as this is now possible with the [`missing`](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-missing-aggregation.html) aggregation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using non-mapped fields in prefix queries shouldn't cause NullPointerExc...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2409</link><project id="" key="" /><description>...eption

Fixes #2408
</description><key id="8357017">2409</key><summary>Using non-mapped fields in prefix queries shouldn't cause NullPointerExc...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-14T15:28:04Z</created><updated>2014-07-16T21:54:24Z</updated><resolved>2012-11-14T17:36:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-14T17:36:35Z" id="10375553">pushed (master, 0.20, and 0.19)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException with prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2408</link><project id="" key="" /><description>I accidentally use boost syntax in a field of prefix query and got a NullPointerException.

When doing the same with match query i simply got an empty answer so i suppose the behavior is not expected.

Query and stack trace are in the following gist ... https://gist.github.com/4059732

ES version is 0.19.10

Regards.

Beno&#238;t
</description><key id="8348921">2408</key><summary>NullPointerException with prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benoit-intrw</reporter><labels><label>bug</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-14T09:31:03Z</created><updated>2012-11-14T17:35:25Z</updated><resolved>2012-11-14T17:35:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Retrieve content upto wordsAround  the search keyword</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2407</link><project id="" key="" /><description>Is there a way to highlight   only a part of a string field .
Suppose I have a field named Content and its very lengthy. 
for example I have 
Content  : After a contentious three-week patent trial between Apple and Samsung, jurors awarded Apple $1.05 billion and concluded that Samsung "willfully" infringed several Apple patents. The legal battle was significant for the normally clandestine company. Lawyers managed to get Apple talking in ways it never had, from telling emails between executives to weird and wonderful iPhone prototypes. Here are the juiciest revelations.

I want to search say "apple " in the content and I set wordsAround =5
then my query should return  "trial between Apple and Samsung" 
I found fragment_size can be set on highlighted fields but I didn't understand it fully .
is  fragment_size do the same ??
</description><key id="8348417">2407</key><summary>Retrieve content upto wordsAround  the search keyword</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">soumyamrs</reporter><labels /><created>2012-11-14T09:03:14Z</created><updated>2013-07-15T16:17:31Z</updated><resolved>2013-07-15T16:17:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-06T14:59:27Z" id="19051047">hey,

do you still have highlighting problems? have you tried the current release? Also, as this is merely an issue tracker, you might have more luck on the mailing list, as there are simply more eyes looking at your problem.

Either way you should provide always a sample of the search requests and the data your indexed in a way easy to recreate locally (most likely curl requests).
</comment><comment author="spinscale" created="2013-07-15T16:17:31Z" id="20980945">closing, happy to reopen if tested against a current version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date fields shouldn't be returned as longs by Get API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2406</link><project id="" key="" /><description /><key id="8333916">2406</key><summary>Date fields shouldn't be returned as longs by Get API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-13T19:57:58Z</created><updated>2014-07-16T21:54:24Z</updated><resolved>2012-11-13T20:37:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-13T20:37:49Z" id="10342517">Pushed to master and 0.20 branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>lucene 4: field visitors shouldn't return fields that were not present i...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2405</link><project id="" key="" /><description>...n the visited document
</description><key id="8310775">2405</key><summary>lucene 4: field visitors shouldn't return fields that were not present i...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-13T02:19:26Z</created><updated>2014-07-15T17:58:07Z</updated><resolved>2012-11-14T15:20:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-11-13T11:11:23Z" id="10322802">+1 Makes sense @imotov 
</comment><comment author="imotov" created="2012-11-14T15:20:23Z" id="10369697">Pushed to master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard Allocation: add index.routing.allocation.require.... and cluster.routing.allocation.require.... setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2404</link><project id="" key="" /><description>The new settings `index.routing.allocation.require....` and `cluster.routing.allocation.require....` allow to specify conditions, all of which have to be satisfied for a shard to be allocation on a given node. 

This commit introduces the following algorithm for determining if a shard can be allocated on a node. For allocation to be allowed the node has to satisfy ALL of the following conditions:
- if the `index.routing.allocation.require....` or `cluster.routing.allocation.require....`  lists are not empty, all filters must match the node
- if the `index.routing.allocation.include....` or `cluster.routing.allocation.include....`  lists are not empty, at list one filter must match the node
- if the `index.routing.allocation.exclude....` or `cluster.routing.allocation.exclude....`  lists are not empty, no filters must match the node

Due to a bug in the elasticsearch this commit introduces two potentially breaking changes. Prior to this commit, elasticsearch was looking only at the first setting in include and exclude groups. All other settings were essentially ignored. After this change all include and exclude settings will be considered. 

The second breaking change is caused by difference in treatment of space in `index.routing.allocation.exclude....` or `cluster.routing.allocation.exclude....` settings. Prior to this commit, space was matching any node, after this commit space will remove corresponding `index.routing.allocation.exclude....` or `cluster.routing.allocation.exclude....` setting from consideration.
</description><key id="8309238">2404</key><summary>Shard Allocation: add index.routing.allocation.require.... and cluster.routing.allocation.require.... setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>breaking</label><label>bug</label><label>enhancement</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-13T00:51:51Z</created><updated>2012-11-13T18:33:00Z</updated><resolved>2012-11-13T18:33:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Stats in Delete By Query API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2403</link><project id="" key="" /><description>How to get the stats for  deleted records using Delete By Query API?
</description><key id="8307585">2403</key><summary>Stats in Delete By Query API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">devilankur18</reporter><labels /><created>2012-11-12T23:33:30Z</created><updated>2013-06-28T11:36:52Z</updated><resolved>2013-06-28T11:36:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-06T15:01:51Z" id="19051198">what kind of stats do you mean?

Also, please rather use the google group instead of the issue tracker for these kind of requests, as there are simply more people out to help you (including some samples of what you did also helps the case a lot). Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deriving the REST status code from a failure can, very rarely, cause an infinite loop</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2402</link><project id="" key="" /><description /><key id="8292665">2402</key><summary>Deriving the REST status code from a failure can, very rarely, cause an infinite loop</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-12T15:37:50Z</created><updated>2012-11-12T16:09:41Z</updated><resolved>2012-11-12T16:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Dynamic template with match_mapping_type ignored when there's a match "*" after it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2401</link><project id="" key="" /><description>I have the following dynamic mappings set up in default-mapping.json:

``` javascript
{
    "_default_": {
        "_source": {
            "enabled": false
        },
        "_all": {
            "enabled": false
        },
        "dynamic_templates": [
            {
                "strings": {
                    "match": "*",
                    "match_mapping_type": "string",
                    "mapping": {
                        "type": "multi_field",
                        "fields": {
                            "{name}": {
                                "type": "string",
                                "index": "not_analyzed",
                                "omit_norms": true,
                                "omit_term_freq_and_positions": true
                            },
                            "lower": {
                                "type": "string",
                                "index": "analyzed",
                                "analyzer": "lowercase",
                                "omit_norms": true,
                                "omit_term_freq_and_positions": true
                            }
                        }
                    }
                }
            },
            {
                "everything": {
                    "match": "*",
                    "mapping": {
                        "omit_norms": true,
                        "omit_term_freq_and_positions": true
                    }
                }
            }
        ]
    }
}
```

If I understand this page correctly (at the very bottom):

http://www.elasticsearch.org/guide/reference/mapping/root-object-type.html

the first mapping that matches will be applied -- i.e. "everything" should only be applied to fields that don't match "strings".

However, this isn't what I see (in 0.19.8 anyway) -- no matter which order I put the mappings in, all fields have "everything" applied, including string fields.

(By the way, "lowercase" is a simple custom analayzer with a keyword tokenizer and lowercase token filter.)
</description><key id="8291111">2401</key><summary>Dynamic template with match_mapping_type ignored when there's a match "*" after it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">andrewclegg</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2012-11-12T14:42:33Z</created><updated>2016-05-31T10:13:23Z</updated><resolved>2016-05-31T10:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-11-12T15:03:56Z" id="10290649">Unfortunately, it seems the dynamic template list in org.elasticsearch.index.mapper.object.RootObjectMapper is derived from a map, fetched as Map&lt;String,Object&gt; from the given JSON source. So the keys in the map are not guaranteed to be ordered sequentially. I guess, internally, the entry "everything" is ordered before the entry "strings".

My suggestion is to add a positional attribute ("position") to the dynamic_templates entries so ES can order them more reliably according to the users preference. Patch wanted?
</comment><comment author="jprante" created="2012-11-12T15:13:57Z" id="10291014">Another cause may be that equals() and hashcode() methods in org.elasticsearch.index.mapper.object.DynamicTemplate do not work as expected.
</comment><comment author="andrewclegg" created="2012-11-12T15:32:46Z" id="10291748">It occurs to me that my original example is a bit bogus anyway -- since the omit_norms and omit_term_freq_and_positions are only valid for string types anyway. But the general point still stands...
</comment><comment author="kimchy" created="2012-11-13T14:12:05Z" id="10327282">Hi, this one is tricky... . The order is actually properly maintained of the dynamic templates, so thats not the problem (the array in the dynamic templates denotes the order, and we respect that).

The problem is with how we resolve dynamic templates, specifically, with `match_on_type`. When we encounter a `string` type, we first try and match on a dynamic template by name, _without_ the type. Then, if we don't match on it, we try and guess the type of the string value (it can be a date, an attachment, or numbers if numeric auto detection is turned on or something like that). If its not of any specialized non string type, only then we try and match on a dynamic template with the name and the `string` type as well.

The reason for this behavior is actually down to JSON and binary values. Because binary values in json are strings, trying to auto detect a date for example by trying to convert it to string ends up screwing up the internal parser binary value (I need to check if thats the case still). So we first need to try and match on name without actually knowing the type, and then match on the type...

What you see happens because in the initial match on name (without type), it ends up actually matching on the catch all `everything` one, and then its used. 

This one requires some thinking, not an easy one to solve...
</comment><comment author="haizaar" created="2013-08-08T12:25:08Z" id="22319732">Is there any plan to fix this? On recent ElasticSearch version it still happens. Is there any other way to provide specific dynamic mapping template for strings and another template for all other types?
</comment><comment author="InfinitiesLoop" created="2014-10-24T17:31:27Z" id="60421519">I've just run into this as well. I want strings within a subpath to be analyzed with a specific analyzer, and for all other types in the same subpath to be not_analyzed (but with some other changes for which I need a mapping defined -- for example, a set index_name). It seems because of this behavior I may need to put my strings into a different subpath. I was hoping to avoid making structure choices in my documents just so I can map it correctly.
</comment><comment author="clintongormley" created="2014-11-28T18:35:41Z" id="64920091">Wondering if an `unmatch_mapping_type` will help here?  Possibly combined with rules without `match_mapping_type` being placed below rules with a specified (or wildcard) `match_mapping_type`?
</comment><comment author="ppf2" created="2015-03-23T21:40:55Z" id="85208238">Recently came across this.  The following is the use case:

https://gist.github.com/ppf2/6da223f9517ddc0e9465

In this case, what appears to work (Test 2 in the gist) is if I add `"match_mapping_type": "*"` in addition to `"match": "*"` in the dynamic template mapping for the default/everything fields.
</comment><comment author="yanjunh" created="2015-03-26T04:29:51Z" id="86337227">This is a sweet workaround.  It appears working for me. thanks
</comment><comment author="clintongormley" created="2015-04-05T11:57:13Z" id="89756841">Given @ppf2 's workaround in https://github.com/elastic/elasticsearch/issues/2401#issuecomment-85208238 it seems that we just need to default `match_mapping_type` to `*`?
</comment><comment author="erikringsmuth" created="2015-04-08T16:51:43Z" id="90972832">+1

The workaround of adding `"match_mapping_type": "*"` to all fields works in the meantime.

``` js
PUT /_template/log_template
{
  "template": "log*",
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "timestamp": {
            "match": "@timestamp",
            "match_mapping_type": "*",
            "mapping": {
              "type": "date",
              "index": "not_analyzed",
              "doc_values": true
            }
          }
        },
        {
          "string_multifield": {
            "match": "*",
            "match_mapping_type": "string",
            "mapping": {
              "type": "string",
              "fields": {
                "raw": {
                  "type": "string",
                  "index": "not_analyzed",
                  "doc_values": true
                }
              }
            }
          }
        },
        {
          "catch_all": {
            "match": "*",
            "match_mapping_type": "*",
            "mapping": {
              "index": "not_analyzed",
              "doc_values": true
            }
          }
        }
      ]
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-04-10T13:58:21Z" id="91566774">@kimchy can you expand on what you mean here:

&gt; The reason for this behavior is actually down to JSON and binary values. Because binary values in json are strings, trying to auto detect a date for example by trying to convert it to string ends up screwing up the internal parser binary value (I need to check if thats the case still). So we first need to try and match on name without actually knowing the type, and then match on the type...

This patch:

```
         if (unmatch != null &amp;&amp; patternMatch(unmatch, name)) {
             return false;
         }
-        if (matchMappingType != null) {
-            if (dynamicType == null) {
-                return false;
-            }
-            if (!patternMatch(matchMappingType, dynamicType)) {
-                return false;
-            }
+        if (dynamicType == null) {
+            return false;
+        }
+        if (matchMappingType != null &amp;&amp; !patternMatch(matchMappingType, dynamicType)) {
+            return false;
         }
         return true;
     }
```

seems to work fine with binary strings, eg:

```
DELETE test

PUT test
{
  "mappings": {
    "_default_": {
      "_source": {
        "enabled": false
      },
      "dynamic_templates": [
        {
          "dates": {
            "match": "*",
            "match_mapping_type": "date",
            "mapping": {
              "type": "date",
              "format": "YYYY-mm-dd"
            }
          }
        },
        {
          "everything": {
            "match": "*",
            "mapping": {
              "type": "binary",
              "store": true
            }
          }
        }
      ]
    }
  }
}

PUT test/test/1
{
  "binary": "QUJDREVGR0hJSktMTU5PUFFSU1RVVldYWVoB",
  "date": "2014-01-01"
}

GET /test/test/_mapping
```

returns:

```
        "_source": {
           "enabled": false
        },
        "properties": {
           "binary": {
              "type": "binary",
              "store": true
           },
           "date": {
              "type": "date",
              "format": "YYYY-mm-dd"
           }
        }
```

and

```
GET test/test/_search?fields=*
```

returns:

```
        "fields": {
           "binary": [
              "QUJDREVGR0hJSktMTU5PUFFSU1RVVldYWVoB"
           ]
        }
```

And to update the original example, this seems to work correctly:

```
DELETE test

PUT test
{
  "mappings": {
    "_default_": {
      "dynamic_templates": [
        {
          "strings": {
            "match": "*",
            "match_mapping_type": "string",
            "mapping": {
              "type": "string",
              "fields": {
                "raw": {
                  "type": "string",
                  "index": "not_analyzed"
                }
              }
            }
          }
        },
        {
          "everything": {
            "match": "*",
            "mapping": {
              "type": "{dynamic_type}",
              "doc_values": true
            }
          }
        }
      ]
    }
  }
}

PUT test/test/1
{
  "string": "bar",
  "bool": true,
  "date": "2014-01-01",
  "int": 5
}

GET test/test/_mapping
```

returns:

```
        "properties": {
           "bool": {
              "type": "boolean",
              "doc_values": true
           },
           "date": {
              "type": "date",
              "doc_values": true,
              "format": "dateOptionalTime"
           },
           "int": {
              "type": "long",
              "doc_values": true
           },
           "string": {
              "type": "string",
              "fields": {
                 "raw": {
                    "type": "string",
                    "index": "not_analyzed"
                 }
              }
           }
```
</comment><comment author="clintongormley" created="2016-05-31T10:13:23Z" id="222647766">Closed by https://github.com/elastic/elasticsearch/pull/18638
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Support SpanMultiTermQueryWrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2400</link><project id="" key="" /><description>Elasticsearch does not support lucene's SpanMultiTermQueryWrapper. This lucene query allows users to form complicated queries such as wildcards or prefix queries embedded within span queries. These complex queries are regularly used in certain fields (e.g., legal research) and adding support for it would open ES up to a new field.

Unfortunately, adding support is not possible using a plugin because the plugin system offers no way to install new query types as plugins. All of the QueryBuilders are hard-coded in QueryBuilders.java.

This is not a bug, but a feature request to expose SpanMultiTermQueryWrapper to ES's query API.

Some additional information on this feature request can be found here:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/vHQh0ARaAHY
</description><key id="8289584">2400</key><summary>Query DSL: Support SpanMultiTermQueryWrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">speedplane</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2012-11-12T13:53:47Z</created><updated>2013-05-30T09:07:18Z</updated><resolved>2013-05-03T14:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-03T14:09:03Z" id="17395862">Closed by https://github.com/elasticsearch/elasticsearch/commit/e30aa6b22178c32f6bfe8dfa0d5d99e8a99a9833 (specified it wrong in the commit message)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to netty 3.5.10</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2399</link><project id="" key="" /><description /><key id="8284060">2399</key><summary>Upgrade to netty 3.5.10</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-12T09:39:05Z</created><updated>2012-11-12T09:39:37Z</updated><resolved>2012-11-12T09:39:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Inconsistent results of facets running with a nested query scope</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2398</link><project id="" key="" /><description>When a nested query is combined with a parent query and a facet is executed against the nested query scope, the facet results depend on the order of execution of nested and parent queries.  For example, if a bool query has two `must` clauses and the nested query is the first clause, the nested facets are filtered by the parents query in the last clause. If the nested query is the last clause, the nested facet runs against all nested document ignoring the parent query in the first clause. Example: https://gist.github.com/c505e3ce1f535d700f37
</description><key id="8273804">2398</key><summary>Inconsistent results of facets running with a nested query scope</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-11T17:47:53Z</created><updated>2013-02-06T10:16:16Z</updated><resolved>2013-02-06T10:16:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-06T10:16:16Z" id="13175060">This issue is fixed by the removal of `_scope` in #2606 

I updated your gist to demonstrate how to facet by nested facets with the use of `_scope`:
https://gist.github.com/martijnvg/4721628
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi Match: tie_breaker should allow for floating point value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2397</link><project id="" key="" /><description /><key id="8262035">2397</key><summary>Multi Match: tie_breaker should allow for floating point value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-10T13:59:22Z</created><updated>2012-11-10T14:10:16Z</updated><resolved>2012-11-10T13:59:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow highlighting on wildcard fields.. ie, comment_*</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2396</link><project id="" key="" /><description>Solr allows you to request highlighting on fields with a wildcard in the name. Ie, if I have fields:

  comment_1, comment_2, comment_3, comment_4, id_1, id_2, etc.. 

I can ask Solr to highlight on '_' fields, or on 'comment__' fields. Currently ElasticSearch forces me to create a list of all potential fields we want highlighting on, and pass that entire list in the query. Could you add wildcards to the highlight field list?
</description><key id="8250410">2396</key><summary>Allow highlighting on wildcard fields.. ie, comment_*</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">diranged</reporter><labels><label>feature</label><label>v0.20.2</label><label>v0.90.0.Beta1</label></labels><created>2012-11-09T20:04:23Z</created><updated>2013-09-17T21:25:03Z</updated><resolved>2012-12-26T23:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sreuter" created="2012-12-18T16:13:20Z" id="11493120">+1
</comment><comment author="kimchy" created="2012-12-26T22:54:19Z" id="11696235">Make sense, will push it shortly to 0.20 and master.
</comment><comment author="konklone" created="2013-09-17T21:25:03Z" id="24623785">Does this work with subfields? e.g. `person.*`?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms filter doesn't understand _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2395</link><project id="" key="" /><description>A `terms` query can query the `_id` field, as can a `term` filter, but not a `terms` filter:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1' 
curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1'  -d '
{
   "foo" : "bar"
}
'
curl -XPUT 'http://127.0.0.1:9200/test/test/2?pretty=1'  -d '
{
   "foo" : "baz"
}
'
```

`terms` query works:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "terms" : {
         "_id" : [
            1,
            2
         ]
      }
   }
}
'

# [Fri Nov  9 18:00:28 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : "bar"
#             },
#             "_score" : 0.35355338,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "test"
#          },
#          {
#             "_source" : {
#                "foo" : "baz"
#             },
#             "_score" : 0.35355338,
#             "_index" : "test",
#             "_id" : "2",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 0.35355338,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 0
# }
```

`terms` filter doesn't work:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "terms" : {
               "_id" : [
                  1,
                  2
               ]
            }
         }
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : null,
#       "total" : 0
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```

`term` filter works:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "term" : {
               "_id" : 1
            }
         }
      }
   }
}
'

# [Fri Nov  9 17:58:13 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : "bar"
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 1
# }
```
</description><key id="8245529">2395</key><summary>Terms filter doesn't understand _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.0.Beta1</label></labels><created>2012-11-09T17:02:34Z</created><updated>2013-02-18T22:03:28Z</updated><resolved>2013-02-18T22:03:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-18T22:03:28Z" id="13745633">@clintongormley this seems to be fixed in master I am closing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding a type with _source or _all enabled fails, when these are disabled in index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2394</link><project id="" key="" /><description>Full details here:

https://gist.github.com/4045563

This _may_ be related to these being disabled in an index template? (not sure)

Tested in 0.19.10 and 0.20.

Clarification of wording in title: adding the type doesn't fail, it is added ok, but these fields are still disabled.
</description><key id="8237664">2394</key><summary>Adding a type with _source or _all enabled fails, when these are disabled in index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels><label>bug</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-09T13:19:30Z</created><updated>2012-11-09T16:21:35Z</updated><resolved>2012-11-09T16:21:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-09T15:49:48Z" id="10233733">I see the problem, its effectively a bug in the process of applying the _default_ mapping, will push a fix soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add logging for environment paths on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2393</link><project id="" key="" /><description>This information can be really helpful for troubleshooting non-trivial configurations.
</description><key id="8227890">2393</key><summary>Add logging for environment paths on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-09T05:12:46Z</created><updated>2014-06-18T01:25:58Z</updated><resolved>2012-11-12T13:05:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-10T14:07:53Z" id="10255466">Its preferable to log it in one line, so it won't be too noisy. Also, we have `[]` to wrap values.
</comment><comment author="imotov" created="2012-11-10T16:54:48Z" id="10257127">Updated.
</comment><comment author="kimchy" created="2012-11-12T13:05:07Z" id="10286266">Pushed1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `index.routing.allocation.require....` and `cluster.routing.alloc&#8230; </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2392</link><project id="" key="" /><description>&#8230;ation.require....` settings

Fixes #2404
</description><key id="8227836">2392</key><summary>Add `index.routing.allocation.require....` and `cluster.routing.alloc&#8230; </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-11-09T05:06:35Z</created><updated>2014-06-16T23:29:35Z</updated><resolved>2012-11-21T13:58:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>gateway.expected_nodes counts non-data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2391</link><project id="" key="" /><description>I have a cluster with a large number of non-data nodes (logstash clients):
  "number_of_nodes" : 75,
  "number_of_data_nodes" : 6,

and this config:

gateway.expected_nodes: 6
gateway.recover_after_nodes: 4
gateway.recover_after_time: 15m

It seems like gateway.expected_nodes is counting _all_ nodes in the cluster.  If I restart a single data node, all it's shards go into recovery/reallocation.  When I bump expected_nodes to 100 and restart a single data node, things look more like I'd expect (a bunch of unassigned_shards pop up, and as the restarted data node recovers local indexes/shards, the count goes down).

Since this setting is about starting shard recovery, and non-data nodes don't matter there (afaik), shouldn't expected_nodes (and recover_after_nodes?) only count data nodes?
</description><key id="8226233">2391</key><summary>gateway.expected_nodes counts non-data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fetep</reporter><labels /><created>2012-11-09T02:40:00Z</created><updated>2013-10-30T09:46:32Z</updated><resolved>2013-10-30T09:46:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-11-09T12:51:54Z" id="10225472">The gateway.expected_nodes is counting data and master-eligible nodes. Depending on how you created logstash clients (as `node.data: false` or `node.client: true`) they may or may not be master-eligible.  Could you run the following command

```
curl -s "localhost:9200/_cluster/state?pretty=true&amp;filter_routing_table=true&amp;filter_metatdata=true&amp;filter_indices=true"
```

and check which attribute logstash clients have?

Meanwhile, if you want to consider only _data_ nodes for recovery, you can use `expected_data_nodes` and `recover_after_data_nodes` settings instead.
</comment><comment author="imotov" created="2012-11-09T15:04:48Z" id="10229916">Added a short description of these settings to the [Gateway](http://www.elasticsearch.org/guide/reference/modules/gateway/index.html) page. 
</comment><comment author="fetep" created="2012-11-09T15:21:45Z" id="10231531">The logstash clients are client:true and data:false.  expected_data_nodes seems like the setting I want, but I'm still a little confused about the best use of it.  I have expected_data_nodes set to 6 now, and when I restart a data node, a bunch of shards still get reassigned because when the node joins the cluster, it's still recovering local indices.
</comment><comment author="imotov" created="2012-11-09T15:34:05Z" id="10232858">The gateway settings are used only during initial recovery. Once cluster recovered these settings are no longer in effect until the next full cluster restart. If you want to temporary disable reassignment of shards while you are restarting a node, you can temporary set `cluster.routing.allocation.disable_allocation` to true using [Cluster Update Settings API](http://www.elasticsearch.org/guide/reference/api/admin-cluster-update-settings.html). Just don't forget to turn it back on afterwards. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Interpret .yaml files as YAML instead of JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2390</link><project id="" key="" /><description>This is a very minor change to allow the ElasticSearch config to have a ".yaml" extension. The current version interprets ".yaml" files as JSON.

One test is currently failing, though I don't think this is related to my change. Here's the surefire report,

---
## Test set: TestSuite

Tests run: 920, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1,537.94 sec &lt;&lt;&lt; FAILURE!
testSimpleRecovery(org.elasticsearch.test.integration.recovery.SimpleRecoveryTests)  Time elapsed: 73826 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError: 
Expected: &lt;false&gt;
     but: was &lt;true&gt;
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
        at org.elasticsearch.test.integration.recovery.SimpleRecoveryTests.testSimpleRecovery(SimpleRecoveryTests.java:101)
</description><key id="8224586">2390</key><summary>Interpret .yaml files as YAML instead of JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gatoatigrado</reporter><labels /><created>2012-11-09T00:53:32Z</created><updated>2014-07-16T21:54:27Z</updated><resolved>2012-11-12T13:06:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-12T13:06:54Z" id="10286320">Pushed to master and 0.20 branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace &#224; with a</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2389</link><project id="" key="" /><description>There are some encoding issues when the `&#224;` char is in `elasticsearch.yml`, encoding for this file **MUST** be in UTF-8. On windows systems, that cause some issues with default platform encoding.

It would be nice to avoid non ISO characters in the configuration file.
</description><key id="8211023">2389</key><summary>Replace &#224; with a</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-11-08T16:57:49Z</created><updated>2014-06-14T07:50:22Z</updated><resolved>2012-11-26T13:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-08T18:49:04Z" id="10199813">We can change it, but we read the files are UTF8, can you post the exception you got?
</comment><comment author="dadoonet" created="2012-11-09T08:22:05Z" id="10218341">I copy and paste the elasticsearch.yml content in a new file (encoded in cp1252 - default settings for eclipse under windows) and I got this exception.

```
elasticsearch-0.19.11\bin&gt;elasticsearch
{0.19.11}: Setup Failed ...
- SettingsException[Failed to load settings from [file:/E:/Apps/dev/elasticsearch/elasticsearch-0.19.11/config/elasticsearch.yml]]
        ReaderException[ special characters are not allowed]
```

It's not a big issue as the exception message is clear enough and with the right encoding (UTF-8) everything is fine. I just suggest to avoid special characters in config files.
</comment><comment author="s1monw" created="2012-11-09T08:58:57Z" id="10219150">Really the only way to make this reasonable is to hard fail if the input is not UTF-8. we need to prevent users from making mistakes as soon as possible. Encoding is crucial here and we should enforce UTF-8 and nothing else. @dadoonet I really don't get why eclipse still uses cp1252 so good that you figured that out here!

I just added a commit for this issue using the lucene's decodingReader which uses a CharsetDecoder that will fail on malformed input and unmappable characters. So if you pass non-utf8 you get a reasonable hard exception. I think we should make this the default for all resources we load like stopword lists / synonyms etc.
</comment><comment author="dadoonet" created="2012-11-09T10:10:31Z" id="10221107">&gt; Really the only way to make this reasonable is to hard fail if the input is not UTF-8.

Yes! And that's the way Elasticsearch does it right now. So that's fine for me.
With this pull request, I just suggest to remove the special character **&#224;** in the config file. I don't understand with your comment if you will merge it or not???

&gt; I really don't get why eclipse still uses cp1252 so good that you figured that out here!

Yes! It's a pain in the ass. Perhaps, it concerns only Eclipse on **french** Windows setup?

&gt; I think we should make this the default for all resources we load like stopword lists / synonyms etc.

+1.
</comment><comment author="brusic" created="2012-11-13T19:29:58Z" id="10339430">Fixed with this commit:
https://github.com/elasticsearch/elasticsearch/commit/91de48d2d396ca0868b7d0f6a6e8b2c5265793b9
</comment><comment author="kimchy" created="2012-11-13T19:31:02Z" id="10339471">@brusic Yea.., forgot to mention that I fixed it, at least it won't cause problems, though its recommended obviously to use UTF8.
</comment><comment author="brusic" created="2012-11-13T20:25:43Z" id="10342048">BTW, I had the same issue using default Intellij settings on Windows.
</comment><comment author="dadoonet" created="2012-11-26T13:31:41Z" id="10715336">Closed by https://github.com/elasticsearch/elasticsearch/commit/91de48d2d396ca0868b7d0f6a6e8b2c5265793b9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BM25 / BM25F Scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2388</link><project id="" key="" /><description>Now that Lucene 4 has this in core, it would be great to have it exposed in a simple way in the upcoming ES 0.21. I was thinking in the query DSL as "scoring" : "VSM / BM25 / BM25F / ... " ?
</description><key id="8205918">2388</key><summary>BM25 / BM25F Scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2012-11-08T14:35:46Z</created><updated>2014-10-03T12:14:43Z</updated><resolved>2013-02-27T16:27:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kshnurov" created="2012-11-08T14:40:39Z" id="10190023">+1 Really need it!
</comment><comment author="uboness" created="2012-11-08T14:52:05Z" id="10190417">Absolutely guys! it's certainly on the the near future roadmap. We're currently working on Lucene 4 integration and once we're done with that (done in the sense that we bring it to a state were we're sure L4 doesn't break es as it is) we'll start to gradually add L4 related features (e.g. support for the different similarities).
</comment><comment author="kshnurov" created="2012-11-08T14:58:09Z" id="10190660">When are you planning to release ES with Lucene 4.0?
I want to switch from Sphinx to ES and I really need some features of Lucene 4.0 to do it.
</comment><comment author="faliev" created="2013-03-06T01:25:22Z" id="14476933">Is it possible to use bm25f in 0.9 beta?
</comment><comment author="kimchy" created="2013-03-06T01:30:13Z" id="14477061">Yes, you can use that, check out: http://www.elasticsearch.org/guide/reference/index-modules/similarity.html.
</comment><comment author="faliev" created="2013-03-06T01:56:10Z" id="14477842">Thank you. I've seen new similarities introduced in 0.9, but I only saw mention of "BM25 similarity" not BM25F. 
</comment><comment author="s1monw" created="2013-03-06T12:13:51Z" id="14496294">You are right, we have a BM25 Similarity while BM25F would need some query support to be implemented. A similarity today doesn't really know about the fields in the query it only knows about the frequency and the document so either we extend the similarity to work across fields or add specialized queries to address this issue but at this point there is no BM25F support
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2387</link><project id="" key="" /><description>I linked the percolator functionality with the Highlighting API, so you could percolate and get highlights as well (not just an ack). This added functionality is completely optional, and doesn't introduce performance hit if not used. I also verified compatibility with queries which were stored using the old API (highlight section on the Query document is completely optional).

For that to work, I changed HighlightingPhase to use a static method for the actual process, and changed the Matches response to a container class (PercolationMatch) instead of a simple string.

At this point highlighting works per query, that is - you would need to store the highlighting settings per query. A feature I'm likely to add to this soon is to have the ability to define global highlighting settings (when percolating).

Assumptions / gotchas:
- Assumes one document per percolation operation. Lucene.ExistsCollector is used which makes it possible for using more than one doc while percolating, but the rest of the API doesn't show like that is supported.
- Various REST responses were changed
- Works only with string fields. When you think of it, it does make sense...
- Some cleanup may be required
</description><key id="8175702">2387</key><summary>Highlighting percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2012-11-07T16:14:28Z</created><updated>2014-06-15T04:18:29Z</updated><resolved>2013-04-04T10:07:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-10T13:57:40Z" id="10255390">Heya, thanks for the effort!. We are deep into the lucene_4 upgrade, once its done, we are going to go over this pull request.
</comment><comment author="synhershko" created="2013-04-04T10:07:51Z" id="15889183">This became too stale, see #2586 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Array of IDs as a parameter for MLT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2386</link><project id="" key="" /><description>Currently MLT accept only one ID. We could enhance the feature, and accept array of IDs, so it will be more-like-these. 
Related conversation: https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/GOzRXJmrb54
</description><key id="8174732">2386</key><summary>Array of IDs as a parameter for MLT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">strokine</reporter><labels /><created>2012-11-07T15:44:49Z</created><updated>2014-06-20T22:04:57Z</updated><resolved>2014-06-20T22:04:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2014-06-20T22:04:57Z" id="46731692">This has been implemented in More Like This Query https://github.com/elasticsearch/elasticsearch/commit/db991dc3a496dc8ba3ed1624ad65fc3c8bcbd5bc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The index of the next RestFilter must be incremented before the current filter starts processing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2385</link><project id="" key="" /><description>Otherwise, synchronous filters will not work. For example, the following filter would cause a `java.lang.StackOverflowError`:

```
public class SimpleRestFilter extends RestFilter {
    @Override
    public void process(RestRequest request, RestChannel channel, RestFilterChain filterChain) {
        filterChain.continueProcessing(request, channel);
    }
}
```
</description><key id="8172844">2385</key><summary>The index of the next RestFilter must be incremented before the current filter starts processing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2012-11-07T14:46:06Z</created><updated>2014-07-16T21:54:29Z</updated><resolved>2012-11-09T21:06:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-09T21:06:13Z" id="10244627">Pushed, cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to highlight full field values for multi-valued fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2384</link><project id="" key="" /><description>I've noticed that at some point in the recent past, ES support for highlighting multi-valued fields has improved considerably, in that now each value is returned in its own fragment. This is fantastic! However, I have noticed that it seems impossible to return multiple highlighted values from the same field in their entirety, in the case where there are multiple matching fields per document. When using the default options for highlight, ES will return a fragment for each value, but that fragment may be incomplete -- this is as expected. When setting `number_of_fragments` to zero, however, ES will return at most one fragment, corresponding to a single field value.

[This reduction](http://hastebin.com/raw/tawowayitu) demonstrates the behavior:

``` bash
curl -XDELETE "http://localhost:9200/test?pretty=true"
echo ""
curl -XPUT "http://localhost:9200/test?pretty=true"
echo ""

curl -XPUT "http://localhost:9200/test/doc/_mapping?pretty=true" -d '
{
  "doc": {
    "properties": {
      "tags": {
        "type": "string",
        "index":"analyzed",
        "term_vector":"with_positions_offsets"
      }
    }
  }
}
'
echo ""

curl -XPUT "http://localhost:9200/test/doc/1?pretty=true&amp;refresh=true" -d '
{
  "tags": [
    "this is a really long tag i would like to highlight",
    "here is another one that is very long and has the tag token near the end"
  ]
}
'
echo ""

curl -XGET "http://localhost:9200/test/doc/_search?pretty=true" -d '
{
  "query": {"query_string": {"query":"tag", "fields": ["tags"]}},
  "highlight": {"fields": {"tags": {}}},
  "fields": []
}
'
echo ""

curl -XGET "http://localhost:9200/test/doc/_search?pretty=true" -d '
{
  "query": {"query_string": {"query":"tag", "fields": ["tags"]}},
  "highlight": {"fields": {"tags": {"number_of_fragments": 0}}},
  "fields": []
}
'
echo ""

curl -XGET "http://localhost:9200/test/doc/_search?pretty=true" -d '
{
  "query": {"query_string": {"query":"tag", "fields": ["tags"]}},
  "highlight": {"fields": {"tags": {"fragment_size": 4096}}},
  "fields": []
}
'
echo ""
```

It would be great to get support for full-fragment multi-valued highlighting! Thank you!
</description><key id="8147920">2384</key><summary>Unable to highlight full field values for multi-valued fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-06T18:28:53Z</created><updated>2012-12-03T11:49:44Z</updated><resolved>2012-12-03T11:38:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-10T14:16:49Z" id="10255526">Heya, just to double check we won't miss this one, once we are post the lucene_4 upgrade, we will get back to this one. Just to make sure, ping us after lucene 4 is done.
</comment><comment author="outoftime" created="2012-11-10T15:19:23Z" id="10256115">Will do -- thanks!
</comment><comment author="martijnvg" created="2012-12-03T11:42:55Z" id="10949841">@outoftime In the next release setting `number_of_fragments` to `0` will highlight all matching field values as full fragment. 
</comment><comment author="outoftime" created="2012-12-03T11:49:44Z" id="10949987">Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to match non existant tags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2383</link><project id="" key="" /><description>Lets say I have a set of document which have a-z as keys. 
And I am firing this term query:
curl -X GET localhost:9200/test/test/_search?pretty=json -d '{"query":{"term":{"a":"abcd", "b":"wxyz"}}}'

This will return me all the documents for which the value of 'a' is abcd and the value of 'b' is wxyz.
By using should, I can at best get documents where both or at least one condition matches.
But, what if I want all the such documents in return for which (the value of 'a' is abcd and the value of 'b' is wxyz) or (the value of 'a' is abcd and tag 'b' doesn't exist) or (tag a doesn't exist and the value of 'b' is wxyz) or (tag 'a' doesn't exist and tag 'b' doesn't exist).

Is this possible.
I know this is not the right place to ask this questions. But I didn't to get the answer anywhere else.

Thanks In Advance
-Azitabh
</description><key id="8145411">2383</key><summary>How to match non existant tags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">azitabh</reporter><labels /><created>2012-11-06T17:05:45Z</created><updated>2012-11-07T06:51:59Z</updated><resolved>2012-11-07T06:51:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="azitabh" created="2012-11-07T06:51:59Z" id="10139364">Got the answer. 
Closing this now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Add largest thread pool count per thread pool stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2382</link><project id="" key="" /><description /><key id="8133231">2382</key><summary>Node Stats: Add largest thread pool count per thread pool stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.12</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-06T10:23:43Z</created><updated>2012-11-06T10:25:44Z</updated><resolved>2012-11-06T10:25:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>#2356 Break down the simple id cache size per type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2381</link><project id="" key="" /><description /><key id="8086653">2381</key><summary>#2356 Break down the simple id cache size per type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexliu68</reporter><labels><label>stalled</label></labels><created>2012-11-04T06:56:30Z</created><updated>2014-11-21T10:45:40Z</updated><resolved>2014-11-21T10:45:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-06T09:54:36Z" id="10104658">hi alex, thanks for your effort!, currently, the focus is to get the upgrade to lucene 4 done, once its in, we can then look at this feature.
</comment><comment author="martijnvg" created="2014-07-25T08:38:35Z" id="50122642">The id_cache has been removed a while back and parent/child relies now on field data. The id_cache keys in the stats apis still exist, but will be removed in 2.0. (see #5269)

We can show a break down in the field data stats by parent type. Right now these stats are bundled under `_parent` key.
</comment><comment author="clintongormley" created="2014-08-08T08:09:37Z" id="51574333">Probably don't need this if we move parent-child to use doc values.  Blocked by #6107 / #6511 
</comment><comment author="jpountz" created="2014-11-21T10:45:40Z" id="63954208">A more generic way to have more information about where memory goes would be to allow each memory user to return its own break down of memory usage. I opened https://github.com/elasticsearch/elasticsearch/issues/8589 for that purpose. This would allow not only to break down memory usage per type but also to know where memory for each type goes, etc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ignore indices not set when using multi search api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2380</link><project id="" key="" /><description /><key id="8054524">2380</key><summary>Ignore indices not set when using multi search api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.20.0</label></labels><created>2012-11-02T09:56:45Z</created><updated>2012-11-02T09:58:07Z</updated><resolved>2012-11-02T09:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolate query ignores the index name when querystring is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2379</link><project id="" key="" /><description>I wrote some code to index/percolate documents, and I have a unit test that creates an index (with a random name) then runs my indexations / search code, then deletes the index. This means that I have percolators with identical IDs in different indexes.

When I percolate my documents, I specify a query string for the percolators :

```
testindex-2b086e73/doc/a889db29-2409-4174-9425-818eb7293b57?percolate=SourceId:1
```

And then the result contains duplicate percolator IDs, as if the index name was ignored :

``` javascript
{
    "ok" : true,
    "_index" : "testindex-17eecc11",
    "_type" : "doc",
    "_id" : "824f26e4-76a6-4b66-9cb2-f3adc0103586",
    "_version" : 1,
    "matches" : ["1.789", "1.789", "1.789"]
}
```

But when I don't specify a query for the percolators (?percolate=*) the result only contains one ID as expected.

**Edit:**
I feel like it's an intended behavior in (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java#L345) is it? If yes, how do I filter percolators only on one documenttype/index?
</description><key id="8053904">2379</key><summary>Percolate query ignores the index name when querystring is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">alprema</reporter><labels /><created>2012-11-02T09:21:33Z</created><updated>2014-07-08T18:24:14Z</updated><resolved>2014-07-08T18:24:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alprema" created="2013-01-23T14:17:01Z" id="12597075">Any news?
</comment><comment author="joshbronson" created="2013-02-26T22:39:47Z" id="14144540">I'm seeing something similar when I use the Java percolate API.

http://www.elasticsearch.org/guide/reference/java-api/percolate.html

If I create identical percolators with id x in both index_a and index_b, then percolate a document via preparePercolate with a query that matches both, I will see id "x" twice in the matches, regardless of which index I pass to preparePercolate. If I remove one of the percolators, whether it's in index_a or index_b, I'll see only one response. I haven't tried without the query.

(I'm on 19.8.)

This is probably a bug, not an intended behavior.

The workaround for me was to add "term" : {"_type" : [index_name]} to the percolator query. 
</comment><comment author="alprema" created="2013-08-07T14:59:23Z" id="22257404">Do you require more information on how to reproduce or will the bug be set to "Won't fix" ?
</comment><comment author="martijnvg" created="2013-08-26T16:28:19Z" id="23275773">This happens by design. Within an index the combination of a type and id is unique. The workaround mentioned by @joshbronson is the way to go with the current percolator.

In the new percolator in master any index can contain percolate queries, so you can store your percolate queries in multiple indices and only percolate against the index you need to, this way you don't get 'duplicates'.
</comment><comment author="alprema" created="2013-10-22T14:02:21Z" id="26805139">Great, any idea when this will be shipped?
</comment><comment author="martijnvg" created="2013-10-29T16:13:07Z" id="27317169">The new percolator will be part of the 1.0-beta1 release. I think that this version will be released in the coming weeks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#1649 index level custom meta support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2378</link><project id="" key="" /><description /><key id="8053705">2378</key><summary>#1649 index level custom meta support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexliu68</reporter><labels><label>discuss</label></labels><created>2012-11-02T09:09:34Z</created><updated>2014-07-25T08:32:06Z</updated><resolved>2014-07-25T08:32:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T08:32:06Z" id="50121882">For index-level meta support, better to just store a document in the index.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.5.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2377</link><project id="" key="" /><description /><key id="8033110">2377</key><summary>Upgrade to Netty 3.5.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.11</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-11-01T15:15:38Z</created><updated>2012-11-01T15:16:00Z</updated><resolved>2012-11-01T15:16:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>multi_field mapping bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2376</link><project id="" key="" /><description>If I put mapping

``` json
{
  "test": {
    "properties": {
      "testfield": {
        "type": "multi_field",
        "fields": {
          "testfield": {
            "type": "string",
            "index": "no",
            "include_in_all": true
          },
          "fieldtest": {
            "type": "string",
            "index": "not_analyzed",
            "include_in_all": false
          }
        }
      }
    }
  }
}
```

it maps as defined.
But if I swap `testfield` and `fieldtest` in multi_field fields definition like this:

``` json
{
  "test": {
    "properties": {
      "testfield": {
        "type": "multi_field",
        "fields": {
          "fieldtest": {
            "type": "string",
            "index": "no",
            "include_in_all": true
          },
          "testfield": {
            "type": "string",
            "index": "not_analyzed",
            "include_in_all": false
          }
        }
      }
    }
  }
}
```

it maps the folowing (both fields have `"include_in_all": false`):

``` json
{
  "test": {
    "properties": {
      "testfield": {
        "type": "multi_field",
        "fields": {
          "testfield": {
            "type": "string",
            "index": "not_analyzed",
            "include_in_all": false
          },
          "fieldtest": {
            "type": "string",
            "index": "no",
            "include_in_all": false
          }
        }
      }
    }
  }
}
```

Is it ok?
</description><key id="8024500">2376</key><summary>multi_field mapping bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">printercu</reporter><labels /><created>2012-11-01T07:48:08Z</created><updated>2012-11-04T07:17:15Z</updated><resolved>2012-11-04T07:17:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-11-01T12:05:28Z" id="9978066">In multi_field mappers you can control "includeInAll" flag only on the default field ("testfield" in your case). On all other fields it's automatically set to false. 
</comment><comment author="imotov" created="2012-11-01T13:17:58Z" id="9979787">Added some clarifications to the documentation  site https://github.com/elasticsearch/elasticsearch.github.com/commit/9876e80bdc7d59db01407689018bb3bfc99b4650
</comment><comment author="printercu" created="2012-11-04T07:17:15Z" id="10048122">Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upsert (update API) does not create missing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2375</link><project id="" key="" /><description>Expected it to work pretty much like a standard index operation, where a missing index is auto created.
</description><key id="8017523">2375</key><summary>Upsert (update API) does not create missing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jrydberg</reporter><labels /><created>2012-10-31T22:17:57Z</created><updated>2012-11-02T16:34:45Z</updated><resolved>2012-11-02T16:34:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.19.11 does not support nested object query?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2374</link><project id="" key="" /><description> I follow the guide(http://www.elasticsearch.org/guide/reference/api/search/facets/index.html):
map&#65306;
{ 
    "type1" : { 
        "properties" : { 
            "obj1" : { 
                "type" : "nested" 
            } 
        } 
    } 
}

data
{ 
    "obj1" : [ 
        { 
            "name" : "blue", 
            "count" : 4 
        }, 
        { 
            "name" : "green", 
            "count" : 6 
        } 
    ] 
}

query&#65306;
{ 
    "query": { 
        "match_all": {} 
    }, 
    "facets": { 
        "facet1": { 
            "terms_stats": { 
                "key_field" : "name", 
                "value_field": "count" 
            }, 
            "nested": "obj1" 
        } 
    } 
}

error&#65306;
"error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[B5cyxUnMQC64JnNfhl16kw][articles][0]: SearchParseException[[articles][0]: query[ConstantScore(NotDeleted(_:_))],from[-1],size[-1]: Parse Failure [Failed to parse source [\n{ \n    \"query\": { \n        \"match_all\": {} \n    }, \n    \"facets\": { \n        \"facet1\": { \n            \"terms_stats\": { \n                \"key_field\" : \"name\", \n                \"value_field\": \"count\" \n            }, \n            \"nested\": \"obj1\" \n        } \n    } \n}]]]; nested: SearchParseException[[articles][0]: query[ConstantScore(NotDeleted(_:_))],from[-1],size[-1]: Parse Failure [facet nested path [obj1] is not nested]]; }{[B5cyxUnMQC64JnNfhl16kw][articles][4]: SearchParseException[[articles][4]: query[ConstantScore(NotDeleted(_:_))],from[-1],size[-1]:

And I met the same problem when using " Nested Query Facets ". My es version is 0.19.11&#65292;someone pass the above query in 0.19.10
</description><key id="7988565">2374</key><summary>0.19.11 does not support nested object query?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asoqa</reporter><labels /><created>2012-10-31T03:19:52Z</created><updated>2012-10-31T10:37:34Z</updated><resolved>2012-10-31T10:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-10-31T03:31:45Z" id="9931715">I just tried to reproduce the problem with both 0.19.11 and master and everything seems to work fine. Could you check this script https://gist.github.com/3984632 to see if you are doing anything differently?
</comment><comment author="asoqa" created="2012-10-31T09:39:17Z" id="9938276">thank you! please forgive stupid fault, I found the problem: 
your example: curl -XPUT 'http://localhost:9200/test-idx/type1/1'
But I input another wrong type and id
</comment><comment author="imotov" created="2012-10-31T10:37:34Z" id="9939942">No problem. I am glad I could help. Typically, when you have an issue like this one, it's better to start with posting it on [mailing list](http://www.elasticsearch.org/help/) and then, if it turns out to be an elasticsearch bug, create an issue here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: Allow the addition of custom metadata to an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2373</link><project id="" key="" /><description>It would be great if indices can have user-defined properties. Examples include version numbers, local creation parameters, etc...

Currently, custom settings are allowed at index creation time, but are not changeable due to MetaDataUpdateSettingsService rejecting any non-dynamic settings.
</description><key id="7979604">2373</key><summary>Feature Request: Allow the addition of custom metadata to an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels /><created>2012-10-30T19:51:31Z</created><updated>2012-10-31T18:22:48Z</updated><resolved>2012-10-31T18:22:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexliu68" created="2012-10-31T18:05:32Z" id="9955449">it's same issue as #1649

https://github.com/elasticsearch/elasticsearch/issues/1649
</comment><comment author="brusic" created="2012-10-31T18:22:48Z" id="9956104">Thanks Alex. I vaguely remember someone talking on the mailing list about such a request, but I was not able to find it. Closing this issue and will watching the other.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed issue2371 (incorrect behavior of path_match).</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2372</link><project id="" key="" /><description>This resolves Issue #2371 (_and_ Issue #1056).
</description><key id="7971531">2372</key><summary>Fixed issue2371 (incorrect behavior of path_match).</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">atdixon</reporter><labels /><created>2012-10-30T15:40:46Z</created><updated>2014-07-03T09:32:02Z</updated><resolved>2012-11-01T21:26:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T21:26:19Z" id="9996592">pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>path_match support in dynamic templates is incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2371</link><project id="" key="" /><description>Issue #1056 reports the same problem as an "ambiguity" but this is an actual defect.

patch_match patterns like "tags._" and "_.tags.*" are incorrectly matched and yield bad mappings when indexing documents. (A fix/pull request is on the way.)
</description><key id="7970697">2371</key><summary>path_match support in dynamic templates is incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">atdixon</reporter><labels><label>bug</label><label>v0.20.0</label></labels><created>2012-10-30T15:15:20Z</created><updated>2012-11-01T21:26:42Z</updated><resolved>2012-11-01T21:26:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T21:26:42Z" id="9996607">Pushed the fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warmup API should include routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2370</link><project id="" key="" /><description>Not sure if it matters, but should warmup requests take routing into account? I don't think they do currently as routing is a query string param, not part of the body of the request.
</description><key id="7966164">2370</key><summary>Warmup API should include routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-10-30T12:26:32Z</created><updated>2012-11-10T14:13:20Z</updated><resolved>2012-11-10T14:13:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T20:41:06Z" id="9995038">Yea, they don't. I don't see a reason why they should currently,
</comment><comment author="kimchy" created="2012-11-10T14:13:20Z" id="10255499">Closing this, I don't think this makes sense...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed geo_shape relation contains to within</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2369</link><project id="" key="" /><description>The 'contains' relation is incorrectly named as it implies that the indexed shape contains the query shape, when in fact it's the other way round.  'within' correctly implies that the indexed shape is within the query shape.
</description><key id="7963161">2369</key><summary>Changed geo_shape relation contains to within</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-10-30T09:58:30Z</created><updated>2014-07-16T21:54:30Z</updated><resolved>2012-11-01T21:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T21:26:58Z" id="9996616">Pushed!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.19.11 ConnectTransportException on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2368</link><project id="" key="" /><description>Hello,

When I look at my logs I have weird warnings like these:

http://ideone.com/WEWakn

Is this "normal" ?

Thanks
</description><key id="7962622">2368</key><summary>0.19.11 ConnectTransportException on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Silex</reporter><labels /><created>2012-10-30T09:31:06Z</created><updated>2012-11-01T13:12:35Z</updated><resolved>2012-11-01T12:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-10-30T13:55:07Z" id="9906285">No this is not "normal" and might indicate a problem with your setup. Most likely it's an issue with [network settings](http://www.elasticsearch.org/guide/reference/modules/network.html). As you can see in the log file, your node is listening on the local interface (127.0.0.1) while publishing external IP (192.168.0.106). Since it looks like a configuration problem, let's close this issue and continue the conversation [on the mailing list](http://www.elasticsearch.org/help/).
</comment><comment author="Silex" created="2012-11-01T12:58:15Z" id="9979262">You are right, I did change `network.bind_host` but not `network.publish_host`. I used `network.host` to fix this.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add getters to SearchSourceBuilder, to allow for encapsulation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2367</link><project id="" key="" /><description>At this point, there is no way to reuse existing behavior if you want to add custom behavior to it. In our case, we wanted to issue a search request but get processed data (vs getting actual search results). We found ourselves copying EVERYTHING.

This proposed changed will allow us to encapsulate and resuse at least some of the behavior. We will still need to duplicate quite a lot of code because of the way RequestBuilder and SourceBuilder classes are designed. If this can be opened for discussion, we would be happy to assist with that as well.
</description><key id="7961759">2367</key><summary>Add getters to SearchSourceBuilder, to allow for encapsulation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2012-10-30T08:47:36Z</created><updated>2014-07-16T21:54:31Z</updated><resolved>2013-05-12T12:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T20:57:47Z" id="9995662">I can add this, though really unsure why you need it. You treat SearchSourceBuilder as something you keep around, while you should treat the actual constructs of it as something that you keep around, or construct and compose, and then execute a search.
</comment><comment author="kimchy" created="2012-11-01T20:59:38Z" id="9995727">To be honest, unsure about this. Can you explain what you are after, without uppercases if possible.
</comment><comment author="synhershko" created="2012-11-01T22:06:14Z" id="9997874">YES :)

We want to reuse the search API for an operation which gets a standard search request (+ a few extra params), but outputs a processed response (without the actual results). The obvious solution would have been to derive from SearchRequest and reuse SearchRequestBuilder, SearchSourceBuilder and TransportSearchAction. For our case we would probably need a completely different Response class, but for simplicity sake let's assume we can derive from SearchResponse as well.

In our code, we use the same query construction routine for both standard search queries and this operation, so it only makes sense to reuse the query building and request sending. It will also be easier to keep up with changes made to the search API. This is the main motivation behind this.

The problem is with the classes implementing RequestBuilder and TransportAction being generic, so I cannot really reuse them. They only meant to server one, very specific purpose. We ended up writing all those classes ourselves, and copied some significant parts from the Search\* classes, but this feels quite wrong.

The only encapsulation I was able to do is in our MyTransportAction class - it uses a TransportSearchTypeAction to perform the search and then uses the results to produce the desired output. Encapsulating SearchRequestBuilder / SearchSourceBuilder is pretty much meaningless, because ToXContent uses inaccessible private fields to write the source as a whole. We had to pass the "encapsulated" SearchSourceBuilder around and use it directly for building the query. Not too bad, but could have been better...

The proposed change is definitely not the best solution, it is merely a stop gap, and only for some part of the problem. Bottom line, with the current class design there is no good way to build on top of existing functionality easily. It assumes any new extension is a completely new beast, and at least in our scenario that is not the case. Ideally, I should have been able to extend the search functionality by overriding a few methods.
</comment><comment author="synhershko" created="2012-11-06T14:00:31Z" id="10111331">Reading this again, this may not be one of my best written explanations. Should I try again?
</comment><comment author="synhershko" created="2013-05-12T12:07:21Z" id="17776911">Closing this for lack of interest, with a very warm recommendation for an API redesign which uses fewer classes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upsert should return fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2366</link><project id="" key="" /><description>Fixes #2362
</description><key id="7953353">2366</key><summary>Upsert should return fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-10-29T22:28:19Z</created><updated>2014-07-16T21:54:31Z</updated><resolved>2012-11-01T20:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T20:45:27Z" id="9995194">Pushed, cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleting a non-existent warmer shouldn't cause request to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2365</link><project id="" key="" /><description>Fixes #2363
</description><key id="7940567">2365</key><summary>Deleting a non-existent warmer shouldn't cause request to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-10-29T15:47:51Z</created><updated>2014-07-16T21:54:32Z</updated><resolved>2012-11-01T20:51:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T20:51:17Z" id="9995415">Pushed!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify plugin installation troubleshooting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2364</link><project id="" key="" /><description>Add check for write access to plugins directory before trying to install plugin. Support verbose mode that prints out intermediate exceptions. Don't install plugin is plugin directory already exists. Fixes #2357

This pull request should simplify troubleshooting of two difficult to troubleshoot issues that can occur during a plugin installation. The first issue occurs when a user doesn't have write access to the plugin directory and is currently difficult to troubleshoot due to confusing error message. The second issue occurs during a plugin upgrade, when the previous version of the plugin is already installed. If the new version contains jars that are different from the installed version, new jars are currently installed side-by-side with old jars, which creates installation that may or may not work depending on the order of plugin jars in the classpath. 
</description><key id="7925292">2364</key><summary>Simplify plugin installation troubleshooting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-10-29T01:31:55Z</created><updated>2014-06-19T12:34:48Z</updated><resolved>2012-11-01T20:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-01T20:48:07Z" id="9995300">Pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleting a non-existent warmer causes ES to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2363</link><project id="" key="" /><description>```
curl -XDELETE 'http://127.0.0.1:9200/foo/bar/_warmer/1?pretty=1' 

[2012-10-27 13:03:26,392][WARN ][cluster.service          ] [Oddball] failed to execute cluster state update, state:
version [7], source [delete_warmer [1]]
nodes: 
   [Oddball][CNyPGneKSIGNtS7MkcewyQ][inet[/192.168.5.20:9300]], local, master
routing_table:
-- index [test]
----shard_id [test][0]
--------[test][0], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][0], node[null], [R], s[UNASSIGNED]
----shard_id [test][1]
--------[test][1], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][1], node[null], [R], s[UNASSIGNED]
----shard_id [test][2]
--------[test][2], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][2], node[null], [R], s[UNASSIGNED]
----shard_id [test][3]
--------[test][3], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][3], node[null], [R], s[UNASSIGNED]
----shard_id [test][4]
--------[test][4], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][4], node[null], [R], s[UNASSIGNED]

-- index [foo]
----shard_id [foo][0]
--------[foo][0], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][0], node[null], [R], s[UNASSIGNED]
----shard_id [foo][1]
--------[foo][1], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][1], node[null], [R], s[UNASSIGNED]
----shard_id [foo][2]
--------[foo][2], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][2], node[null], [R], s[UNASSIGNED]
----shard_id [foo][3]
--------[foo][3], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][3], node[null], [R], s[UNASSIGNED]
----shard_id [foo][4]
--------[foo][4], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][4], node[null], [R], s[UNASSIGNED]

routing_nodes:
-----node_id[CNyPGneKSIGNtS7MkcewyQ][V]
--------[test][0], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][1], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][2], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][3], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[test][4], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][0], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][1], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][2], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][3], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
--------[foo][4], node[CNyPGneKSIGNtS7MkcewyQ], [P], s[STARTED]
---- unassigned
--------[test][0], node[null], [R], s[UNASSIGNED]
--------[test][1], node[null], [R], s[UNASSIGNED]
--------[test][2], node[null], [R], s[UNASSIGNED]
--------[test][3], node[null], [R], s[UNASSIGNED]
--------[test][4], node[null], [R], s[UNASSIGNED]
--------[foo][0], node[null], [R], s[UNASSIGNED]
--------[foo][1], node[null], [R], s[UNASSIGNED]
--------[foo][2], node[null], [R], s[UNASSIGNED]
--------[foo][3], node[null], [R], s[UNASSIGNED]
--------[foo][4], node[null], [R], s[UNASSIGNED]

org.elasticsearch.search.warmer.IndexWarmerMissingException: index_warmer [1] missing
    at org.elasticsearch.action.admin.indices.warmer.delete.TransportDeleteWarmerAction$1.execute(TransportDeleteWarmerAction.java:130)
    at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:223)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
```
</description><key id="7910249">2363</key><summary>Deleting a non-existent warmer causes ES to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.20.0</label></labels><created>2012-10-27T11:04:25Z</created><updated>2012-11-01T20:50:42Z</updated><resolved>2012-11-01T20:50:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-10-29T02:16:41Z" id="9854703">The issue occurs because TransportDeleteWarmerAction [throws an exception](/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java#L130) inside submitStateUpdateTask, and as a result [the count down latch](/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/warmer/delete/TransportDeleteWarmerAction.java#L155) in ProcessedClusterStateUpdateTask.clusterStateProcessed is never triggered and the REST request waits forever for a response. We could solve this issue by surrounding the body of the execute method in TransportDeleteWarmerAction with try catch block and releasing the latch if an exception is thrown. However, I think it would be more useful to add the clusterStateProcessingFailed method to the ProcessedClusterStateUpdateTask, that would be triggered in the event of cluster state update failure. Because failure can occur after the ClusterStateUpdateTask.execute method returns, (for example if one of the cluster state listeners throws an exception), it's still possible that the clusterStateProcessed would be never called even if ClusterStateUpdateTask.execute was successful. By adding clusterStateProcessingFailed method we would guarantee that the submitStateUpdateTask caller would get a response if update was successful (clusterStateProcessed) as well as if it failed (clusterStateProcessingFailed). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upsert doesn't return fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2362</link><project id="" key="" /><description>If a doc is upserted, instead of being updated, the `fields` parameter is ignored:

```
# DOC DOESN'T EXIST
curl -XPOST 'http://127.0.0.1:9200/es_test_1/type_1/1000/_update?pretty=1&amp;fields=_source'  -d '
{
   "script" : "ctx._source.extra=\"foo\"",
   "upsert" : {
      "bar" : "baz"
   }
}
'

# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "1000",
#    "_type" : "type_1",
#    "_version" : 1
# }


# DOC EXISTS
curl -XPOST 'http://127.0.0.1:9200/es_test_1/type_1/1000/_update?pretty=1&amp;fields=_source'  -d '
{
   "script" : "ctx._source.extra=\"foo\"",
   "upsert" : {
      "bar" : "baz"
   }
}
'

# {
#    "ok" : true,
#    "_index" : "es_test_1",
#    "_id" : "1000",
#    "get" : {
#       "_source" : {
#          "bar" : "baz",
#          "extra" : "foo"
#       },
#       "exists" : true
#    },
#    "_type" : "type_1",
#    "_version" : 2
# }
```
</description><key id="7910073">2362</key><summary>Upsert doesn't return fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-10-27T10:31:22Z</created><updated>2012-11-01T20:44:58Z</updated><resolved>2012-11-01T20:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>GeoShape intersects operation returns false positives</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2361</link><project id="" key="" /><description>The geoShape logic produces false positives in results returned from "intersects" queries and filters.  I presume (have not tested) that "contains" also produces false positives and "disjoint" can fail to return true matches.  This is because the queries/filters check only if the query shape and document shape have grid locations that overlap in the SpatialPrefixTree implementation, without actually comparing the query shape with the document shape.

In theory this bug can be worked around by increasing the depth of the SpatialPrefixTree (`maxLevels` in the constructor).  In practice both indexing shapes and querying shapes is a O(4^n) algorithm in both space and time, where n is the tree's depth, so highly accurate results and low-latency or limited RAM are mutually exclusive.  100% accurate querying at all resolutions requires an infinitely deep tree, under the current implementation, which is based solely on gridding.

One solution is to use the existing implementation to find candidate docs, and verify each of those docs by pulling back its geometry and comparing to the query using JTS.  This allows keeping spatial index trees relatively shallow and efficient.  I'm happy to implement this and share the pull request, unless you guys have other ideas.  I have questions:
- does such a patch go to the elasticsearch project, or upstream (Lucene?)
- documentation says we don't support storage of geometry fields, except in the `_source`. Where would I start to store the geometry (maybe as WKB) itself in the index (not just its tree nodes)?

Here's how to replicate the bug:

``` java
   private static final SpatialPrefixTree QUAD_PREFIX_TREE =
            new QuadPrefixTree(GeoShapeConstants.SPATIAL_CONTEXT,
            QuadPrefixTree.DEFAULT_MAX_LEVELS);
    strategy  = new TermQueryPrefixTreeStrategy(
            new FieldMapper.Names("shape"), QUAD_PREFIX_TREE, 0.0);

    IndexWriter writer = new IndexWriter(directory,
            new IndexWriterConfig(Version.LUCENE_36, new KeywordAnalyzer()));
    writer.addDocument(newDocument("5",
                newPolygon().point(-92.34, 37.56).point(-92.34, 37.55)
                .point(-92.32, 37.55).point(-92.32, 37.56).point(-92.34, 37.56).build()));

    Point point = new Point(-92.319, 37.54);
    Filter filter = STRATEGY.createIntersectsFilter(point);

    // BUG: the doc and query do not intersect, but this returns 1 hit
    indexSearcher.search(new MatchAllDocsQuery(), filter, 10);
```
</description><key id="7906879">2361</key><summary>GeoShape intersects operation returns false positives</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IamJeffG</reporter><labels><label>:Geo</label><label>adoptme</label><label>enhancement</label></labels><created>2012-10-27T00:15:58Z</created><updated>2015-04-21T19:03:55Z</updated><resolved>2015-04-21T19:03:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismale" created="2012-10-27T02:40:44Z" id="9831924">So the concern here is that the algorithms are not precise enough because they rely totally on the grid hash overlap? 

This is actually partially by design.  Iterating over a large set of documents and comparing them using JTS will heavily degrade performance.  The current behaviour is similar to the effect that stemming has on free text searches.  There are countless examples where stemming results in documents being matched that don't really line up with the user's intentions.  

With that said, we are considering bringing in an alternative algorithm based on Lucene's RecursivePrefixTreeStrategy which theoretically provides a little more precision at the edges of shapes, but it doesn't exceed the SpatialPrefixTree levels.

Have you done any benchmarking of the performance of using JTS? I have found even just the object creation is expensive.  If we can find a way to make the performance manageable, I can envisage maybe having an option to enable the precise filtering which is disabled by default.

&gt; does such a patch go to the elasticsearch project, or upstream (Lucene?)

ES uses its own version of the Lucene spatial code so we can explore the problem here and then consider contributing fixes upstream.  However due to the licensing of JTS, Lucene cannot directly use it.

&gt; documentation says we don't support storage of geometry fields, except in the _source. Where would I start to store the geometry (maybe as WKB) itself in the index (not just its tree nodes)?

Good question.  I have worked on a standalone WKT parser which we could use but WKB would probably be more compact.  Given a parser, we would need to change the Mapping code fro GeoShape to add an additional field which would index the parsed shape.  Something along these lines is already done in the GeoPoint codebase. 
</comment><comment author="IamJeffG" created="2012-10-29T17:41:49Z" id="9878198">Chris,
You make a good point that the false-positives are analogous to stemming in free text, but just as elasticsearch also supports exact matches (via non-analyzed term query) when desired, I'm proposing it be able to do the same for geo-shape.  it's true there will be a performance hit, possibly severe, so we should make this extra validation optional.

That said, I benchmark WKB deserialization to be extremely fast: 3% of GeoJSON deser time and  9% of WKT deser time, using the JTS libraries.  Once instantiated, geometry comparisons can be made 58% faster when using a JTS PreparedGeometry.

I'm going to attempt a patch; I'm new to the codebase but here's what I'm thinking:
At index time, GeoShapeFieldMapper needs to use a MultiFieldMapper (or similar), indexing both the prefix tree terms (as now), plus a BinaryMapper for the WKB, which has storage enabled.  It seems the FieldData and DocFieldData interfaces are used only at query time, and they would provide the stored WKB back to a GeoShapeFilter for polygon comparison, is this right?

Thanks for getting back!
</comment><comment author="chrismale" created="2012-10-29T21:11:25Z" id="9885617">&gt; You make a good point that the false-positives are analogous to stemming in free text, but just as elasticsearch also supports exact matches (via non-analyzed term query) when desired, I'm proposing it be able to do the same for geo-shape. it's true there will be a performance hit, possibly severe, so we should make this extra validation optional.

I appreciate the argument, just the severity concerns me.

&gt; That said, I benchmark WKB deserialization to be extremely fast: 3% of GeoJSON deser time and 9% of WKT deser time, using the JTS libraries. Once instantiated, geometry comparisons can be made 58% faster when using a JTS PreparedGeometry.

Are you able to benchmark the comparison of 10,000 Geometrys? Just to give me an idea of the severity.

&gt; I'm going to attempt a patch; I'm new to the codebase but here's what I'm thinking:
&gt; At index time, GeoShapeFieldMapper needs to use a MultiFieldMapper (or similar), indexing both the prefix tree terms (as now), plus a BinaryMapper for the WKB, which has storage enabled. It seems the FieldData and DocFieldData interfaces are used only at query time, and they would provide the stored WKB back to a GeoShapeFilter for polygon comparison, is this right?

I would take a look at GeoPointFieldMapper as an example.  It creates a number of different fields.  Equally, looking at GeoPolygonFilterParser will give an idea of how to use FieldData (the FilterParser uses a Filter which interacts with FieldData).
</comment><comment author="dsmiley" created="2012-10-31T19:17:41Z" id="9958226">Interesting conversation.

Lucene spatial's RecursivePrefixTreeStrategy (RPTS) would be great here.  It could be extended such that at the finder detail levels of intersection at the edges, it then consults the JTS shape for an exact hit.  This works well because this won't even happen for shapes that are clearly outside or clearly inside the query shape -- it's only at the intersecting edges (or very close to intersecting) where this would happen.  I don't have this requirement on my projects so I haven't bothered to do it.  RPTS will also find matches where the query shape is inside much larger indexed shapes -- a limitation to ES's spatial.
</comment><comment author="dsmiley" created="2012-10-31T19:44:59Z" id="9959253">I was thinking about this a bit more and I can see that with the combination of our grid system we can do even better when paired with JTS.  Instead of verifying intersection (or lack thereof) with JTS at the edges, it could be consulted only when the shapes come closed but indeterminate wether there is overlap.  If there is clear overlap (i.e. the indexed shape has grids inside and outside the query shape) we already know there is an intersection and need not consult JTS for those shapes.  I suspect an algorithm like this would improve massively on any other approaches, as this would forgoe the need to consult JTS in I think the majority of cases, and in those cases limit the # shapes to look at to a small #.
</comment><comment author="IamJeffG" created="2012-10-31T20:56:57Z" id="9961724">Thanks for the ideas David.  Sounds like a good plan for efficiency.  I've also noticed that in ES queries with very large spatial extent or indexing a doc with a very large spatial extent is quite infeasible due to ES's prefix trees indexing every single grid under the shape; the hack is to use a more shallow tree, but this is untenable when different docs have vastly different extents.  I wonder if RPTS would solve this too (but that's for another bug report).

Meantime, where can an overview of RPTS be found?  All I am finding with Google is the source code.
</comment><comment author="dsmiley" created="2012-10-31T21:34:48Z" id="9963005">The source code for the Lucene filter is where the search algorithm is, and that's what is pertinent here: http://svn.apache.org/viewvc/lucene/dev/trunk/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/RecursivePrefixTreeFilter.java?view=markup

There isn't much code to it, although it's complex.  Arguably the most complex aspect of the code is that it switches its algorithm from recursive decomposition to brute-force scan once the detail level gets high enough.

I've been putting a little thought into this filter and I can see it getting overhauled a bit so that it can do contains, within, disjoint, (not just intersects), plus also supporting a callback or related mechanism to handle ambiguous intersection, which is where you could consult JTS in a 2nd phase.

RPTS and ES's term query based search both inherit the same indexing approach; there's no difference there.  With the types of changes we're discussing here about using JTS as a fallback for high precision when there is ambiguity, it would not be necessary to index high-precision grids; I'd do course, which means more scalable indexing, with the trade-off of higher likelihood of consulting a serialized shape at search time.
</comment><comment author="chrismale" created="2012-10-31T23:07:08Z" id="9965579">Thanks for providing your insight David.

Couple of thoughts:
- We will definitely bring RecurisvePrefix into ES as an option, probably as part of 0.21.  It definitely has the potential to help improve the accuracy along the edges of shapes.
- I remain really unsure about consulting JTS, even if it's now with a smaller % of shapes.  I worry about the memory consumption and the impact on query performance.  I think we really need to do some benchmarking and get an idea of what impact this is going to have on search time.
</comment><comment author="IamJeffG" created="2012-11-01T06:16:46Z" id="9971935">I benchmarked intersects queries from a single reference shape (as a JTS PreparedGeometry for efficiency) against 10,000 other Geometrys.  My laptop can do this right around 15ms.  This is if Geometry instances are already instantiated.

To also include object instantiation, deserializing from WKB, the total time for 10,000 instantiations and comparisons is 35-45ms.

These are not stupid simple shapes either: I was using mostly MultiPolygons with mean 29 vertices per Geometry.
</comment><comment author="chrismale" created="2012-11-01T23:12:51Z" id="9999603">That sounds very manageable.  Do you know how much memory say, 100,000 Geometrys consumes?
</comment><comment author="IamJeffG" created="2012-11-27T17:34:42Z" id="10767834">I think the memory consumption of 100k Geometrys will vary wildly depending on the complexity of the shape.  In most cases, though, I don't think it's practical to expect to store or cache them all in memory.  There will probably be a disk hit to do the extra validation, but isn't there already a disk hit to read the indexed geohash tiles?  By chance can we pull back the WKB from the index at the same time?

Is it true that FieldData is the appropriate (or only?) method to access the BinaryFieldMapper data I store in the index?  I ask because http://www.elasticsearch.org/guide/reference/index-modules/cache.html says FieldData is used for sorting or caching, but doesn't mention filtering like I'm doing here.

If so, is the MultiValueByteFieldData an efficient way to retrieve the stored byte array from the index, or is it better to implement a custom GeoShapeFieldData, similar to the extant GeoPointFieldData? (and why?)
</comment><comment author="dsmiley" created="2012-11-27T18:22:02Z" id="10769859">RE "but isn't there already a disk hit to read the indexed geohash tiles?"

That's apples &amp; oranges. A geohash is a simple shape with a fairly compact representation (but could be improved).  The number of geohash tiles that will be examined has a roughly fixed upper bound given the distErrPct (shape approximation statistic).  The disk seek()'s are strictly increasing across the shape, and the geohashes will tend to be co-located and thus increasing the effectiveness of the OS's disk cache.  On the other hand, looking up shape data per-document is on the order of the number of affected documents (could be a small or big number), and it is a random disk seek per document that is unlikely to be co-located with nearby shapes.  That last point is solvable using sorted DocValues and a geohash centroid prefix to the bytes.
</comment><comment author="IamJeffG" created="2012-11-27T18:30:29Z" id="10770208">Makes perfect sense, thanks David.  Chris, can you confirm I'm on the right track around FieldData?  I'm following the GeoPolygonFilterParser, but I think what it does is overkill for our needs around this issue.
</comment><comment author="IamJeffG" created="2012-11-28T21:06:46Z" id="10821574">Followup after playing with the code a bit:
It seems that a FieldDataCache cannot read stored-but-not-indexed fields, such as are produced by the BinaryFieldMapper [1].  What's the best way around this?
I can hack around this and make further progress by using a StringFieldMapper for a base-64-encoded WKB, but I hate to have that extra overhead in both encoding time and storage space.

[1] http://www.elasticsearch.org/guide/reference/mapping/core-types.html "Binary types: The field is always stored and not indexed at all."

---

Update: the limitation that BinaryFieldMappers were not indexed has been removed by 549900a0824e2c856b7b7cedc1d447ea854aebf2.  However this merely defers the problem to instantiation of the Field instance:

```
java.lang.IllegalArgumentException: Fields with BytesRef values cannot be indexed
           at org.apache.lucene.document.Field.&lt;init&gt;(Field.java:222)
```
</comment><comment author="IamJeffG" created="2012-12-03T18:39:38Z" id="10965386">Sorry for all the stale commits up above.  Take a look at pull request 2460.  I have two doubts about the efficiency of its implementation, so please help me address these.  I'd prefer we don't merge 2460, but I'm happy to iterate based on our conversation and followup with a new pull request.
</comment><comment author="clintongormley" created="2014-07-08T18:21:53Z" id="48379434">According to https://github.com/elasticsearch/elasticsearch/issues/2803#issuecomment-35543599 this issue should be fixed by Lucene 4.7's [https://issues.apache.org/jira/browse/LUCENE-5408](SerializedDVStrategy).  What do we need to do to benefit from that?
</comment><comment author="IamJeffG" created="2014-07-08T22:38:12Z" id="48408371">From a long-ago discussion with @dsmiley, I believe this requires the following two changes in Elasticsearch:
1. Index time:  Mapping for geo_shape type objects should take boolean flag whether this field should support verified matches.  If so, we write the `indexableFields` from both the PrefixTreeStrategy _and_ the SerializedDVStrategy.  The latter serializes the doc's geometry in a BinaryDocValuesField (the DocValues should be disk-resident since values might be large).
2. A geo_shape query/filter, if "verified matches" are desired, will explicitly issue two filters/queries to Lucene:
   a) The PrefixTreeStrategy as we currently do, though perhaps with less precision (as a cachable filter if possible).
   b) In the same request, use the new SerializedDVStrategy to do exact polygon verification between the query shape and the DocValues of the remaining docs.

The SerializedDVStrategy itself has no index, it's O(N) for N documents: it should always be used in conjunction with another filter, specifically after the PrefixTreeStrategy.   Since it's slow, we'd also like to force it dead-last in the query tree; for example Solr has a mechanism called "PostFilter" to do this, I'm not sure what Elasticsearch provides.
</comment><comment author="dsmiley" created="2014-07-10T16:29:26Z" id="48628986">I think there are two approaches to do this that are philosophically a little different, but they don't have to be mutually exclusive:

(A) Augment geo_shape with this new capability.  In this concept, one mapping/type supports everything, with some configuration flags to turn unneeded things off.  Heck, it might eventually incorporate BBoxSpatialStrategy (newly committed yesterday) too.  One mapping/type but underlying it would be potentially multiple "SpatialStrategies" in use and multiple fields.  This would probably be the most user-friendly because the user is less exposed to some nuts &amp; bolts, but it hides some underlying things that may make it harder to configure each strategy a little differently.  There is an optimization easily done here too in which you can easily ensure that the underlying polygon is parsed once, since it's managed by one mapper (index time) and query.  

Ideally, in this approach, the GeoShapeFilterParser would be able to somehow modify the overall query to use FilteredQuery with QUERY_FIRST_FILTER_STRATEGY with the slow SerializedDVStrategy providing the (slow) filter.  If this can't be done or is too awkward, then it may have to settle for only wrapping the spatial PrefixTree (RPT) query instead of all other queries in play (e.g. keyword &amp; other filters), which means it'll definitely be less efficient.  If none of this is done then the user must provide a query with multiple spatial clauses (the RPT early and SerializedDV last), and if so I argue the one-mapper to rule them all abstraction breaks down.

(B) Enhance GeoShapeFieldMapper to support other SpatialStrategies (only one instance).  The user would then use two spatial fields, one for the RPT index, one for serialized geometry.  This is definitely an easier approach and should offer decent control by the user as to how to compose the query, but it requires the user to know what they are doing more.  And ideally ES has mechanisms to share a parsed Shape between fields on indexing and search so that both Mappers don't have to re-construct the Shapes from JSON.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ship Java client in separate JAR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2360</link><project id="" key="" /><description>It would be great if you ship Java client in its own small jar like elasticsearch-java. Current JAR with engine is way too big if you need just to talk to server.
</description><key id="7877423">2360</key><summary>Ship Java client in separate JAR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hsn10</reporter><labels><label>enhancement</label><label>feature</label></labels><created>2012-10-25T22:38:56Z</created><updated>2017-04-09T06:40:56Z</updated><resolved>2013-03-02T17:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-10-30T14:57:43Z" id="9908649">Have a look at @jprante https://github.com/jprante/elasticsearch-client repo
</comment><comment author="s1monw" created="2013-03-02T17:29:22Z" id="14331733">we likely not going to do this since we use this API internally too. yet if we do so in the future we can reopen
</comment><comment author="igal-getrailo" created="2014-04-29T16:38:40Z" id="41700067">I'm not sure why using that API internally would be a reason to not separate the jars.  

many libraries ship their code in multiple jars.  once you add the new jar to your classpath everything will work as before.

then a client application can add a jar of ~100kb instead of 11mb.

an even better approach would be to extract all the API into interfaces (which is probably already set up that way), and add a jar named elasticsearch-api-x.y.z.jar for applications that need to interact with ES without caring about implementation details.
</comment><comment author="MingleiLee" created="2017-04-09T05:55:43Z" id="292766080">I have a project contains 100 jars, about 50 of them are imported by
```
&lt;dependency&gt;
    &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;
    &lt;artifactId&gt;transport&lt;/artifactId&gt;
    &lt;version&gt;5.3.0&lt;/version&gt;
&lt;/dependency&gt;
```
I'm a little suprised about importing so many jars for only one functionality.</comment><comment author="dadoonet" created="2017-04-09T06:40:56Z" id="292767658">Have a look at: https://www.elastic.co/blog/to-shade-or-not-to-shade</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StringAndBytesText.hasString() may return false incorrectly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2359</link><project id="" key="" /><description>I encountered this with 0.19.10 client  when transforming the highlight map to a different form

``` java
        Map&lt;String, HighlightField&gt; fieldMap = hit.getHighlightFields();
        Map&lt;String, List&lt;String&gt;&gt; highlightMap = Maps.newHashMap();
        for (Map.Entry&lt;String, HighlightField&gt; entry : fieldMap.entrySet()) {
            String field = entry.getKey();
            HighlightField highlightField = entry.getValue();
            Text[] fragments = highlightField.getFragments();
            List&lt;String&gt; highlights = Lists.newArrayList();
            for (Text fragment : fragments) {
                if (fragment.hasString()) {
                    highlights.add(fragment.string());
                }
            }
            if (!highlights.isEmpty()) {
                highlightMap.put(field, highlights);
            }
        }

The method fragment.hasString() would always return false under normal 
operation however under a debugger it seemed to succeed. It seems there 
may be a race condition in the StringAndBytesText class where not calling 
string() prior would return false. The solution is likely to call string() if 
text == null in that method and then return text != null;
```
</description><key id="7866255">2359</key><summary>StringAndBytesText.hasString() may return false incorrectly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Omega359</reporter><labels /><created>2012-10-25T16:13:07Z</created><updated>2014-04-08T17:32:24Z</updated><resolved>2012-11-10T14:15:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-25T18:54:56Z" id="9789489">probably the debugger calls the toString method on it, which initializes the string. You can simply call the string method always, to get the string values. You can optimize if you want in case you have bytes, but it doesn't look like you do it. So just call string() on each fragment, without checking, it will automatically convert the bytes to string if needed.
</comment><comment author="Omega359" created="2012-10-25T19:09:32Z" id="9790024">Sure, that's what I've done. It's just that having a 'hasString' method implies according to normal coding conventions that you can call that to test prior to calling to retrieve the string :) Simple enough to work around but it can (and obviously did in my case) introduce some confusion. It's a method without a real purpose in this case.
</comment><comment author="kimchy" created="2012-11-10T14:15:14Z" id="10255517">It has a purpose to check if the method actually has a backing string, meaning calling string() will not requires converting from bytes to a string. Anyhow, closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_shape "contains" relation is backwards - should be "within"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2358</link><project id="" key="" /><description>I don't want to bikeshed, but I believe the "contains" relation in geo_shape queries and filters does the opposite of what "contains" means in GIS circles.  Rather, the relation actually performs a "within".

The confusion comes from the fact that the DSL is not explicit about how the query/filter should be read.  Compare:
- "Give me all results where the indexed document's shape contains this filter shape."
- "Give me all results where this filter shape contains the indexed document's shape."

The Elasticsearch DSL does have some precedent for the ordering, specifically that they should be read as in the first ordering (indexed documents are the subject, query terms are the sentence's object).  For example, the "has_child" filter's behavior "Gives all results where the indexed (parent) document has_(a)_child matching the query."

Observe that the commonly accepted spatial relation semantics (e.g. http://en.wikipedia.org/wiki/DE-9IM) assert that "A contains B" is equivalent to "B within A".

For the Elasticsearch DSL to be both self-consistent and correct, the [currently implemented behavior](http://www.elasticsearch.org/guide/reference/query-dsl/geo-shape-filter.html)

&gt; Finds those indexed shapes which are fully contained within the filter shape. Any shapes will additional area outside of the filter shape are excluded.

should instead be named "within".  In fact, you used the word "within" to define the "contains" relationship.  The relationship you describe is indeed "within".

By contrast, a true "contains" relationship would have been defined like so:

&gt; contains &#8211; Finds those indexed shapes that contain the filter shape.  The filter shape will not contain any points outside the shapes returned.

It would be great to rename this predicate now while 0.20 is brand new and you've still listed it as "experimental", before the wrong semantics catch on and confusion begins.
</description><key id="7845622">2358</key><summary>geo_shape "contains" relation is backwards - should be "within"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">IamJeffG</reporter><labels><label>:Geo</label><label>docs</label></labels><created>2012-10-25T00:05:10Z</created><updated>2015-12-11T23:15:57Z</updated><resolved>2015-12-11T23:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismale" created="2012-10-25T02:06:38Z" id="9763786">Makes sense.  I agree that we should be consistent and try to reduce confusion.  I'll prepare a fix.
</comment><comment author="clintongormley" created="2014-07-08T18:17:38Z" id="48378763">This has actually been fixed in the code, but we're currently missing documentation for `relation` and `strategy`
</comment><comment author="clintongormley" created="2015-09-19T17:19:50Z" id="141690485">@nknize could you add some docs for `relation` and `strategy` to the geoshape query please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"bin/plugin -install" Misleading error message when run without sudo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2357</link><project id="" key="" /><description>If "bin/plugin -install" is run without sudo when sudo is required then is reports that the "download failed", when in fact it is the installation that failed, not the download. This is very misleading and has wasted a few hours of my morning trying to debug network issues. This also seems to be related to issue 1432.
</description><key id="7836130">2357</key><summary>"bin/plugin -install" Misleading error message when run without sudo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">philwhln</reporter><labels /><created>2012-10-24T18:19:43Z</created><updated>2012-11-01T20:47:27Z</updated><resolved>2012-11-01T20:47:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Break down the simple id cache size per type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2356</link><project id="" key="" /><description>Currently the the the total size of the id cache is returned by the node stats api.  It would be nice if the id cache size is returned per type.
</description><key id="7824125">2356</key><summary>Break down the simple id cache size per type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label></labels><created>2012-10-24T11:05:43Z</created><updated>2014-04-03T07:40:53Z</updated><resolved>2014-04-03T07:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexliu68" created="2012-11-02T21:58:56Z" id="10031641">@martijnvg , I can implement this feature
</comment><comment author="martijnvg" created="2014-04-03T07:40:53Z" id="39421269">Closing, because the id cache is no more.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't use "filtered" in all DSL Guide filter examples</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2355</link><project id="" key="" /><description>In the DSL Guide for filters, almost all examples use "filtered" queries. This has a large implication to the underlying search run in Lucene and should be explicitly pointed out and only used in the "filtered" query example.

For example: http://www.elasticsearch.org/guide/reference/query-dsl/and-filter.html

Should be:

``` json
  {
        "query" : {
            "term" : { "name.first" : "shay" }
        },
        "filter" : {
            "and" : [
                {
                    "range" : { 
                        "postDate" : { 
                            "from" : "2010-03-01",
                            "to" : "2010-04-01"
                        }
                    }
                },
                {
                    "prefix" : { "name.second" : "ba" }
                }
            ]
        }
  }
```
</description><key id="7823128">2355</key><summary>Don't use "filtered" in all DSL Guide filter examples</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshdevins</reporter><labels /><created>2012-10-24T10:14:34Z</created><updated>2013-06-06T15:06:53Z</updated><resolved>2013-06-06T15:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-10-24T10:24:42Z" id="9734602">I don't follow. You _should_ use the `filtered` query rather than the `filter` parameter.  Otherwise you score all of your docs before filtering them.
</comment><comment author="clintongormley" created="2012-10-24T10:29:48Z" id="9734701">The `filter` parameter is specifically for filtering results AFTER facets have been calculated - that's the only time you should use it.
</comment><comment author="joshdevins" created="2012-10-24T16:01:27Z" id="9745127">The point is that the default documentation for filters is also doing a filtered query. This is misleading and makes it look like you should or always have to do a filtered query, which is just not true and depends heavily on the types of queries. There are times when it makes sense to execute the query, score and then filter after, such as when the query itself always returns a small subset of documents. Maybe @s1monw can explain better, we discussed in person last week.
</comment><comment author="clintongormley" created="2012-10-25T08:13:21Z" id="9769289">Fair enough, but given that most people copy and paste to learn, I'd say it is better to show them the general best practice, and to discuss other techniques elsewhere.  Just presenting a different way of doing it won't explain why or when you should use other techniques.
</comment><comment author="s1monw" created="2012-10-25T08:32:23Z" id="9769765">I think the general advice here is to put up a better documentation. I actually agree with josh in his example that if you use filter here IMO. @clintongormley the score assumption is not correct. What happens here is that if you use `filtered` query you execute the filter first and then do the query processing on a per document basis (we call it leap-frog approach or zigzag join) if you use `filter` instead  you first execute the query and for each hit in the query you ask the filter if it is ok to score it - if not get the next doc from the query. in general its about doing the cheaper execution first.  in this example you have a daterange + a prefix query which is way more expensive than the simple term query.
</comment><comment author="spinscale" created="2013-06-06T15:06:52Z" id="19051580">Moving this to the documentation repo https://github.com/elasticsearch/elasticsearch.github.com/issues/439
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sometimes MapperParsingException and sometimes not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2354</link><project id="" key="" /><description>Hi, 

On an empty 1 node cluster elasticsearch v0.19.10, I want to reproduce a MapperParsingException[object mapping for [my_type] tried to parse as object, but got EOF, has a concrete value been provided to it?]

But it's not so easy, here is the script I loop on:

```
curl -s -XDELETE http://localhost:9200/my_index/?pretty=true;

curl -s -XPUT 'http://localhost:9200/_bulk?pretty=true' --data-binary '
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":{"key":"value"}}
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":"value"}
'
```

Sometimes I have the Exception, and sometimes not : 

```
[root@dahu share]# curl -s -XDELETE http://localhost:9200/my_index/?pretty=true;

curl -s -XPUT 'http://localhost:9200/_bulk?pretty=true' --data-binary '
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":{"key":"value"}}
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":"value"}
'{
  "ok" : true,
  "acknowledged" : true
}[root@dahu share]#
[root@dahu share]# curl -s -XPUT 'http://localhost:9200/_bulk?pretty=true' --data-binary '
&gt; {"index":{"_index":"my_index","_type":"my_type"}}
&gt; {"obj":{"key":"value"}}
&gt; {"index":{"_index":"my_index","_type":"my_type"}}
&gt; {"obj":"value"}
&gt; '
{
  "took" : 221,
  "items" : [ {
    "create" : {
      "_index" : "my_index",
      "_type" : "my_type",
      "_id" : "QJz1_TNWT9yhf7Mu5cRlDw",
      "_version" : 1,
      "ok" : true
    }
  }, {
    "create" : {
      "_index" : "my_index",
      "_type" : "my_type",
      "_id" : "ibUrM7JzRaGfalbgaF-aTA",
      "error" : "MapperParsingException[object mapping for [my_type] tried to parse as object, but got EOF, has a concrete value been provided to it?]"
    }
  } ]
}[root@dahu share]#


[root@dahu share]# curl -s -XDELETE http://localhost:9200/my_index/?pretty=true;
curl -s -XPUT 'http://localhost:9200/_bulk?pretty=true' --data-binary '
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":{"key":"value"}}
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":"value"}
'{
  "ok" : true,
  "acknowledged" : true
}[root@dahu share]#
[root@dahu share]# curl -s -XPUT 'http://localhost:9200/_bulk?pretty=true' --data-binary '
&gt; {"index":{"_index":"my_index","_type":"my_type"}}
&gt; {"obj":{"key":"value"}}
&gt; {"index":{"_index":"my_index","_type":"my_type"}}
&gt; {"obj":"value"}
&gt; '
{
  "took" : 251,
  "items" : [ {
    "create" : {
      "_index" : "my_index",
      "_type" : "my_type",
      "_id" : "N2PS-hoPQv2mLN7wzkZFHg",
      "_version" : 1,
      "ok" : true
    }
  }, {
    "create" : {
      "_index" : "my_index",
      "_type" : "my_type",
      "_id" : "-G9gf7kYRTi3Jb2A4-OsVQ",
      "_version" : 1,
      "ok" : true
    }
  } ]
```

Quite strange...
</description><key id="7821304">2354</key><summary>Sometimes MapperParsingException and sometimes not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Filirom1</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2012-10-24T08:49:27Z</created><updated>2016-09-15T23:31:48Z</updated><resolved>2013-01-11T17:02:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-11-16T16:50:05Z" id="10453428">Your data is inconsistent.

`{"obj":{"key":"value"}}` tells Elasticsearch to interpret 'obj' as an object field. 

`{"obj":"value"}` tells Elasticsearch that 'obj' is not on object field, but a value field.

Elasticseach is smart, it can upgrade from a value field to an object field automatically, but not vice versa. Therefore, you receive the exception only if your concurrent bulk request with the object-style 'obj 'has been arrived in the mapping before the value-style 'obj' and downgrading is impossible.

So, take care of your data, decide what style the 'obj' field should have, and you'll be safe.
</comment><comment author="clintongormley" created="2012-11-16T17:58:43Z" id="10455834">@jprante Note that ES doesn't upgrade a value field to an object field. It just ignores the object
</comment><comment author="jprante" created="2012-11-16T20:01:17Z" id="10460035">@clintongormley ah, I see https://gist.github.com/4090274 there's no upgrade magic. Sad, I hoped so :)  Probably a missing feature?
</comment><comment author="clintongormley" created="2012-11-17T10:38:37Z" id="10472755">But how would you upgrade it? what can you reliably convert it to?
</comment><comment author="jprante" created="2012-11-17T17:04:00Z" id="10476177">One thought is promoting the  value-style mapping

```
{
  "test" : {
    "test" : {
      "properties" : {
        "obj" : {
          "type" : "string"
        }
      }
    }
  }
}
```

to the object-style mapping 

```
{
  "test" : {
    "test" : {
      "properties" : {
        "obj" : {
          "dynamic" : "true",
          "properties" : {
            "obj" : {
              "type" : "string"
            }
          }
        }
      }
    }
  }
}
```

so that `{ "obj" : { "key" : "value" }}` could be processed, giving the mapping

```
{
  "test" : {
    "test" : {
      "properties" : {
        "obj" : {
          "dynamic" : "true",
          "properties" : {
            "obj" : {
              "type" : "string"
            },
            "key" : {
              "type" : "string"
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="kimchy" created="2012-11-23T19:40:55Z" id="10668187">But then its tricky, what is "obj", is it a value or an object level mapping. We could potentially support it, but then its weird when it comes to search behavior...
</comment><comment author="clintongormley" created="2012-11-23T19:44:46Z" id="10668265">I'm not convinced that you can reliably guess the right thing to do - better to throw an error and make people fix their data. Otherwise it just leads to weird debugging sessions later on
</comment><comment author="VirgileD" created="2012-12-13T13:45:37Z" id="11334787">ok, so if it's not possible (that's fine with me) i'd like to ALWAYS have an error. because sometimes

```
curl -s -XDELETE http://localhost:9200/my_index/?pretty=true;

curl -s -XPUT 'http://localhost:9200/_bulk?pretty=true' --data-binary '
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":{"key":"value"}}
{"index":{"_index":"my_index","_type":"my_type"}}
{"obj":"value"}
```

succeed and sometimes it fails. I'd like it to always fail...
</comment><comment author="VirgileD" created="2013-01-10T10:45:02Z" id="12089385">is there some work around this? This inconsistency leads to very big problems here, because we can not predict what documents will be integrated...
</comment><comment author="imotov" created="2013-01-10T16:39:27Z" id="12105690">Index object as a string field doesn't throw any errors because field mapper is [trying to interpret](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/core/StringFieldMapper.java#L271) this object as a field boost construct. It sees an object and it expects something like this to follow:

```
{"obj":{"_value":"value", "_boost":2.0}} 
```

When mapper doesn't find the "_value" property it just ignores this field. What we could do is, perhaps, throw an exception if "_value" property is not present, or if it finds some unknown properties such as "key" in the example. I think the latter might be a nice solution for this issue. 
</comment><comment author="kimchy" created="2013-01-10T18:22:10Z" id="12110910">@imotov agreed, just fail if we get a field that we don't expect in the object notation of a string mapped field.
</comment><comment author="fdhenard" created="2014-03-18T20:12:45Z" id="37981786">It would be nice if the EOF error would say which field is causing the error
</comment><comment author="tmaiaroto" created="2014-04-25T03:23:47Z" id="41355768">+1 on more detail about the EOF and which field is causing the error. Data is dirty. Fact of life. So anything to help us out would be great.

...And on that note I also don't agree with clintongormley's theory of "make people fix their data." It's not always "their" data...And you can't predict what you might scrape from the internet AND the more schemaless databases and data sources we get the more we run into this issue. 

In fact, people have commonly seen this EOF error with Twitter data. So when you work with a very popular data source / API (even if it's "incorrect" or "bad practice" - I'm not arguing that dirty data sucks), it's quite easy to see this error.

You can set the `ignore_malformed` setting in your index mapping, but it doesn't seem to help this situation. This is what I would expect though. While we can't reasonably convert data types back and forth...We can choose to ignore them. Otherwise, we have this failure going on which stops initial indexing and it's just bad.

EVEN IF it was at the cost of skipping the entire document from being indexed...That would at least let us get past some of these errors which are typically edge cases to begin with (another reason why people "just can't clean their data").
</comment><comment author="ronsper" created="2014-09-18T17:08:23Z" id="56070778">A setting that would simply ignore these problem documents or a clear error message about what field is the actual problem would be so helpful. How it currently works is not helpful.
</comment><comment author="atott" created="2014-10-01T12:06:25Z" id="57453675">Maybe for someone it will be usefull to find data's inconsistent with this tool https://github.com/atott/es-mapping-validator.
</comment><comment author="ronsper" created="2014-10-01T20:04:20Z" id="57529000">I wish this tool was not needed, but since it is needed, this looks like a great way to find out more about the error. Thank you for pointing this out and creating this tool!
</comment><comment author="gjost" created="2016-09-15T23:31:48Z" id="247483284">It's maddening that Elasticsearch will not indicate **which field** is causing the problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolation shouldn't fail when the _size field is enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2353</link><project id="" key="" /><description>Fixes #2352
</description><key id="7795037">2353</key><summary>Percolation shouldn't fail when the _size field is enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-10-23T12:07:16Z</created><updated>2014-06-26T08:46:48Z</updated><resolved>2012-10-23T21:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-23T21:11:43Z" id="9718410">Pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException on percolation with _size enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2352</link><project id="" key="" /><description>This issue was introduced in v0.19.9.

Steps to reproduce:
1. create test index

```
curl -XPUT localhost:9200/test
```
1. update mapping

```
curl -XPUT 'http://localhost:9200/test/type1/_mapping' -d '
{
    "type1" : {
        "_size" : {
            "enabled" : true,
            "store" : "yes"
        }
    }
}'
```
1. register percolator query:

```
curl -XPUT localhost:9200/_percolator/test/kuku -d '{
    "query" : {
        "term" : {
            "field1" : "value1"
        }
    }
}'
```
1. percolate a document:

```
curl -XGET localhost:9200/test/type1/_percolate -d '{
    "doc" : {
        "field1" : "value1"
    }
}'
```

This leads to the following error:

```
2012-09-26 09:39:21,370][DEBUG][action.percolate         ] [Thor Girl] failed to execute [org.elasticsearch.action.percolate.PercolateRequest@e1935d6]
org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [_size]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:327)
    at org.elasticsearch.index.mapper.internal.SizeFieldMapper.postParse(SizeFieldMapper.java:122)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:502)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:438)
    at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:257)
    at org.elasticsearch.index.percolator.PercolatorService.percolate(PercolatorService.java:111)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:93)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:41)
    at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:176)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.mapper.internal.SizeFieldMapper.innerParseCreateField(SizeFieldMapper.java:140)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:171)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:314)
    ... 11 more
{"error":"MapperParsingException[Failed to parse [_size]]; nested: ","status":400}
```
</description><key id="7788787">2352</key><summary>MapperParsingException on percolation with _size enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fxh</reporter><labels><label>bug</label><label>v0.19.11</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-10-23T06:37:18Z</created><updated>2012-10-23T21:12:04Z</updated><resolved>2012-10-23T21:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-23T21:12:03Z" id="9718416">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add $JAVA_OPTS to the plugin manager, useful for those who have to use proxies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2351</link><project id="" key="" /><description>Summary:
Facebook machines have on direct access to the internet. Need to tell java that we should use proxies.

Test Plan:
export JAVA_OPTS="-Dhttp.proxyHost=&lt;ip&gt; -Dhttp.proxyPort=8080"
</description><key id="7784973">2351</key><summary>Add $JAVA_OPTS to the plugin manager, useful for those who have to use proxies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottmac</reporter><labels /><created>2012-10-23T01:06:21Z</created><updated>2014-07-09T19:59:23Z</updated><resolved>2012-10-23T21:55:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-23T21:55:51Z" id="9719774">Pushed to 0.19/0.20/master branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add highlighter type switch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2350</link><project id="" key="" /><description>Workaround for #2157
</description><key id="7775967">2350</key><summary>Add highlighter type switch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-10-22T18:57:14Z</created><updated>2014-07-13T06:29:59Z</updated><resolved>2012-10-23T00:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-23T00:29:18Z" id="9685963">Pushed!, added shortcuts called `fvh` and `plain` for the type.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: string mapping to automatically set omit_norms to true and index_options to docs when setting index to not_analyzed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2349</link><project id="" key="" /><description>When setting `index` to `not_analyzed` on a `string` field, it makes sense to automatically disable norms and only index docs without frequencies and positions, as they are mostly meaningless in this case.
</description><key id="7772476">2349</key><summary>Mapping: string mapping to automatically set omit_norms to true and index_options to docs when setting index to not_analyzed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-10-22T17:00:25Z</created><updated>2014-06-12T19:32:02Z</updated><resolved>2012-10-22T17:00:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ignore_missing should be true|false not none|missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2348</link><project id="" key="" /><description>Hiya

Surely it makes more sense for `ignore_missing` eg in #2273 and #2209  to accept `true` | `false` values rather than `none` or `missing`?
</description><key id="7767763">2348</key><summary>ignore_missing should be true|false not none|missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-10-22T14:27:28Z</created><updated>2012-10-22T15:31:27Z</updated><resolved>2012-10-22T15:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-10-22T14:31:18Z" id="9665757">Well, #2209 actually uses `ignore_indices` but i'd rename that to `ignore_missing_indices` and make it true | false
</comment><comment author="kimchy" created="2012-10-22T15:19:14Z" id="9667727">The idea is that we might have other values for it in the future, so we went with `ignore_indices` across the board. I fixed #2273 to have the correct parameter.
</comment><comment author="clintongormley" created="2012-10-22T15:31:27Z" id="9668242">Ah OK - thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong geo-distance calculations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2347</link><project id="" key="" /><description>Elasticsearch calculates wrong distance:

```
query:
script_fields : { distance : { script : "doc['location'].distanceInKm(59.924381779999976,30.35182490999999)"} } }

result:
"fields":{"location":{"lon":"30.3223002","lat":"59.94311459999999"}, "distance":3.8925021978692014

should be:
Geocoder::Calculations.distance_between([59.924381779999976,30.35182490999999],[59.94311459999999, 30.3223002], :units =&gt; :km)
2.6540883959769284
```
</description><key id="7758335">2347</key><summary>Wrong geo-distance calculations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kshnurov</reporter><labels /><created>2012-10-22T07:17:21Z</created><updated>2012-10-22T12:21:57Z</updated><resolved>2012-10-22T12:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-10-22T10:57:15Z" id="9659625">The `distanceInKm()` function calculates distance using "plane" distance type, which is faster but less precise than the "arc" type. To get a better precision, us `arcDistanceInKm()` function instead.
</comment><comment author="kshnurov" created="2012-10-22T12:10:09Z" id="9661312">Thank you! Haven't found anything about this method in docs.
</comment><comment author="imotov" created="2012-10-22T12:21:57Z" id="9661617">Working on it - https://github.com/elasticsearch/elasticsearch.github.com/pull/316
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Add `index_options` (deprecating omit_term_freq_and_positions)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2346</link><project id="" key="" /><description>Add `index_options` (applicable to `string` type) with values of:
- `docs`: only documents are indexed, term frequencies and positions are omitted.
- `freqs`: documents and term frequencies are index, positions are omitted.
- `positions`: documents, term frequencies, and positions are indexed.

Note, this deprecates the `omit_term_freq_and_positons` flag, which effectively means setting `index_options` to `docs`.
</description><key id="7754339">2346</key><summary>Mapping: Add `index_options` (deprecating omit_term_freq_and_positions)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-10-22T00:32:19Z</created><updated>2012-10-22T00:32:54Z</updated><resolved>2012-10-22T00:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-22T00:32:54Z" id="9649740">This has been pushed by 53f65d8ff27e0476c59fd9e7312d5d851aa74f99
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aliases API times out when actions end up with no actual change to aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2345</link><project id="" key="" /><description>For example, if an alias exists against an index, and the aliases call include removing and adding the same alias on the same index, it will timeout.
</description><key id="7743681">2345</key><summary>Aliases API times out when actions end up with no actual change to aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.11</label><label>v0.20.0.RC1</label></labels><created>2012-10-21T00:43:10Z</created><updated>2012-10-21T00:45:52Z</updated><resolved>2012-10-21T00:45:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Auto create index to support white/black lists of index patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2344</link><project id="" key="" /><description>Allow `action.auto_create_index` to be set with a comma delimited white or black list of index patterns that are allowed or disallowed to be automatically created.

For example, setting it to `+aaa*,-bbb*,+ccc*,+*`, means that an index that starts with `aaa` it will be automatically created, an index with `bbb` prefix will not be automatically created, `ccc` prefix will be automatically created, and anything else (`+*`) is allowed.
</description><key id="7743584">2344</key><summary>Auto create index to support white/black lists of index patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-10-21T00:24:36Z</created><updated>2012-10-21T00:27:29Z</updated><resolved>2012-10-21T00:27:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve implementation of SimpleIdCache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2343</link><project id="" key="" /><description>From doc I've read that 

With the current implementation, all _id values are loaded to memory (heap) in order to support fast lookups, so make sure there is enough mem for it.

Such implementation causes very wasteful memory usage and high IO when it's unnecessary.
So I hope that this implementation will be improved. It will be great if SimpleIdCache will obtain LRU behaviour. And if it will be filled by demand. I mean that if I have query with filer that should go through some filtered set of parent and child docs then only this doc ids appended to SimpleIdCache but not all.

My unsuccessful user experience described here  
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/IVLNd1SMlrs
</description><key id="7737408">2343</key><summary>Improve implementation of SimpleIdCache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serj-p</reporter><labels /><created>2012-10-20T13:01:23Z</created><updated>2017-03-22T14:50:00Z</updated><resolved>2013-10-30T09:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-20T20:54:14Z" id="9635889">Note that this is only applicable when using parent/child feature, to implement fast in memory joins. We might in the future have and "LRU" based behavior. Note, this easily scales "out", simply by adding more nodes and effectively adding more "memory" to the cluster.
</comment><comment author="cleg" created="2012-10-22T15:04:01Z" id="9667101">I'm really hit by this issue too, unfortunately. Adding more memory of course will do the trick, but memory on AWS is pretty expensive.
</comment><comment author="rmihael" created="2012-10-22T22:28:33Z" id="9683259">I agree, having to load that huge bulk of data to memory can easily make parent-child feature virtually useless from budget standpoint. Doubling and tripling number of nodes just to support it is painful.
</comment><comment author="spinscale" created="2013-06-10T15:15:04Z" id="19205147">@serj-p did https://github.com/elasticsearch/elasticsearch/issues/3028 solve or improve your problems with the id cache (it is included in 0.90.1)?
</comment><comment author="spinscale" created="2013-10-30T09:42:09Z" id="27375722">Closing due to inactivity. Happy to reopen with more data we can work on. Also we have id cache stats in the current release which you should monitor.
</comment><comment author="serj-p" created="2017-03-22T14:50:00Z" id="288422842">We managed to get same code running again and I confirm that on 2.4.1 it works fine.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting elasticsearch connect error on continuous uploads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2342</link><project id="" key="" /><description>I have a script that reads lines one by one from a csv file and index the same to elasticsearch in the same order. My elasticsearch host is the same machine on which the script is running. Everything else works fine except for a few rows when I suddenly start getting the following error:

```
W, [2012-10-09T14:46:00.899876 #11567]  WARN -- : Cannot assign requested address connect(2)
D, [2012-10-09T14:46:00.900037 #11567] DEBUG -- : ["/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/net/http.rb:644:in `initialize'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/net/http.rb:644:in `open'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/net/http.rb:644:in `block in connect'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/timeout.rb:44:in `timeout'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/timeout.rb:89:in `timeout'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/net/http.rb:644:in `connect'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/net/http.rb:637:in `do_start'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/net/http.rb:626:in `start'",
"/home/azitabh/.rvm/gems/ruby-1.9.2-p320/gems/rest-client-1.6.7/lib/restclient/request.rb:172:in `transmit'",
"/home/azitabh/.rvm/gems/ruby-1.9.2-p320/gems/rest-client-1.6.7/lib/restclient/request.rb:64:in `execute'",
"/home/azitabh/.rvm/gems/ruby-1.9.2-p320/gems/tire-0.4.2/lib/tire/http/client.rb:11:in `get'",
"/home/azitabh/.rvm/gems/ruby-1.9.2-p320/gems/tire-0.4.2/lib/tire/search.rb:94:in `perform'",
"/home/azitabh/.rvm/gems/ruby-1.9.2-p320/gems/tire-0.4.2/lib/tire/search.rb:20:in `results'",
"models/test.rb:31:in `get_details'",
"models/test.rb:56:in `block in index_test'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/csv.rb:1768:in `each'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/csv.rb:1202:in `block in foreach'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/csv.rb:1340:in `open'",
"/home/azitabh/.rvm/rubies/ruby-1.9.2-p320/lib/ruby/1.9.1/csv.rb:1201:in `foreach'",
"models/test.rb:40:in `index_test'",
"models/test.rb:85:in `&lt;main&gt;'"]
```

And the errors are not related to the values at those rows in the csv. I get these errors at different locations at different times.

There is another error "WAIT_TIMEOUT" which I get some time. Couldn't put the trace here as I didn't get that error this time.

I am coding in ruby and using "Tire" gem to talk to elasticsearch. I don't feel these are responsible in any way though.

JAVA was using 8% of my system's memory at the time I got this error. This is much below the assigned value ES_MIN_MEM=2g.

Thanks in advance -Azitabh
</description><key id="7726868">2342</key><summary>Getting elasticsearch connect error on continuous uploads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">azitabh</reporter><labels /><created>2012-10-19T19:46:48Z</created><updated>2012-10-22T05:53:57Z</updated><resolved>2012-10-21T20:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-20T20:56:30Z" id="9635913">Hey, see if you are not running our of available sockets because it keeps on opening and closing sockets? I think Tire can work with other HTTP implementations, have you tried to use other ones possibly?
</comment><comment author="azitabh" created="2012-10-21T08:53:53Z" id="9640851">I don't think Tire provides option of using http keep alive or something similar...
inviting @karmi for insights
</comment><comment author="karmi" created="2012-10-21T09:06:19Z" id="9640918">@azitabh The [Curb client](https://github.com/karmi/tire/blob/master/lib/tire/http/clients/curb.rb) has support for keep-alive;&#160;benchmark: https://gist.github.com/1204159

Also, much better to submit Tire related issues at the Tire repository...
</comment><comment author="azitabh" created="2012-10-21T17:16:52Z" id="9645033">@karmi : Thanks for the response.
I am currently using version 0.4.2 of Tire. This version has curb as it's only http clint. So, i can't be using some other clint. Now, does it use keep alive by default or do I need to configure this somehow(didn't find anything on this on web)?
There were nearly 28000 open sockets in waiting state around the time I got the error.

```
azitabh@azitabh:/data/work$ netstat -a | grep 9200 | grep "TIME_WAIT" | wc -l
27847
```

9200 is the port on which elasticsearch is running.
Is this too big a number?

And, I didn't initially think that the issue I am facing is related to Tire. That's why I put it here.
</comment><comment author="karmi" created="2012-10-21T18:42:02Z" id="9645997">@azitabh No, RestClient is the default client (as you see in the backtrace).

You have to specifically configure Tire to use Curb:

``` ruby
require 'curb'
require 'tire/http/clients/curb'
Tire.configure do
    client Tire::HTTP::Client::Curb
  end
```

 Curb uses keep-alive by default. Watch the `total_opened` entry in the http://localhost:9200/_cluster/nodes/stats?clear&amp;http API while your script is running.
</comment><comment author="azitabh" created="2012-10-21T20:31:29Z" id="9647217">Thanks @karmi for help.
Using curb worked perfectly for me and I didn't get any error. I am closing this issue now.

Just one more question. I have read your comment at https://gist.github.com/1205241 which says:

```
# In general, Curb seems to be more then two times faster the RestClient, in some cases it's three
# to five times faster. I wonder if keep-alive has anything to do with it, but it probably does.
#
```

Then, why is it that rest client is still being used as the default http clint?
I am just wondering if switching permanently to curb might lead me to some other problem.
</comment><comment author="karmi" created="2012-10-22T05:53:57Z" id="9653266">RestClient is the default, because it's the most commonly used HTTP client in Ruby, and is more portable: doesn't need extra libraries (`libcurl`) and/or native extensions as is the case with Curb...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Broken links on the Snowball Analyzer page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2341</link><project id="" key="" /><description>The stop word links in this paragraph are broken/404
The stopwords parameter can be used to provide stopwords for the languages that has no defaults, or to simply replace the default set with your custom list. A default set of stopwords for many of these languages is available from for instance _here_ and _here_.

page
http://www.elasticsearch.org/guide/reference/index-modules/analysis/snowball-analyzer.html
</description><key id="7722741">2341</key><summary>Broken links on the Snowball Analyzer page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">concept47</reporter><labels /><created>2012-10-19T17:34:14Z</created><updated>2013-05-27T16:51:27Z</updated><resolved>2013-05-27T16:51:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-27T16:51:27Z" id="18506962">looks good to me now. Please report these kind of problem to the https://github.com/elasticsearch/elasticsearch.github.com repo, where we maintain the documentation.

Thanks for reporting!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>With cache turned off I'm getting the following error: nested: ClassCastException[org.apache.lucene.util.FixedBitSet cannot be cast to org.elasticsearch.common.lucene.docset.FixedBitDocSet]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2340</link><project id="" key="" /><description>While I develop I wanted to do be sure that nothing is cached so I turned off cache by placing:

```
index.cache.filter.type: none
```

as I found in: http://elasticsearch-users.115913.n3.nabble.com/Disable-cache-td3825105.html

I started getting following error:

```
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[_TbZZ8pnTjCqa6SuzvttqQ][sentences][0]: QueryPhaseExecutionException[[sentences][0]: query[filtered(ConstantScore(NotDeleted(+authority_name:Cons. Stato +regulations.norm_number:[163 TO 163])))-&gt;org.elasticsearch.index.search.nested.NonNestedDocsFilter@a2a5984b],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.apache.lucene.util.FixedBitSet cannot be cast to org.elasticsearch.common.lucene.docset.FixedBitDocSet]; }{[_TbZZ8pnTjCqa6SuzvttqQ][sentences][2]: QueryPhaseExecutionException[[sentences][2]: query[filtered(ConstantScore(NotDeleted(+authority_name:Cons. Stato +regulations.norm_number:[163 TO 163])))-&gt;org.elasticsearch.index.search.nested.NonNestedDocsFilter@a2a5984b],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.apache.lucene.util.FixedBitSet cannot be cast to org.elasticsearch.common.lucene.docset.FixedBitDocSet]; }{[_TbZZ8pnTjCqa6SuzvttqQ][sentences][3]: QueryPhaseExecutionException[[sentences][3]: query[filtered(ConstantScore(NotDeleted(+authority_name:Cons. Stato +regulations.norm_number:[163 TO 163])))-&gt;org.elasticsearch.index.search.nested.NonNestedDocsFilter@a2a5984b],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ClassCastException[org.apache.lucene.util.FixedBitSet cannot be cast to org.elasticsearch.common.lucene.docset.FixedBitDocSet]; }]
```

When executing the following query:

```
curl http://localhost:9200/sentences/_search -d '
{
    "query": {
        "filtered": {
            "query": {
                "match_all": { }   
            },
            "filter": {
                "and": [
                    { "term": { "authority_name": "Cons. Stato" } },
                    { "term": { "regulations.norm_number": 163 } }
                ]
            }
        }
    },
    "facets": {
        "regulation_name": {
            "terms": { "field": "name" },
            "facet_filter": [ { "term": { "norm_number": 163 } } ],
            "nested": "regulations"
        }
    }
}'
```

The error is gone after cancelling the cache line from the config. 

I don't know internals of ES so I cannot say if nested facet_filter should work or not with cache
turned off but I think the error is a little bit misleading and I lost a little bit of time searching for solution. 

I spoke on irc about that and someone asked me to open a ticket so here it is ;)
</description><key id="7714630">2340</key><summary>With cache turned off I'm getting the following error: nested: ClassCastException[org.apache.lucene.util.FixedBitSet cannot be cast to org.elasticsearch.common.lucene.docset.FixedBitDocSet]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">troszok</reporter><labels><label>bug</label><label>v0.19.11</label><label>v0.20.0</label><label>v0.90.0.Beta1</label></labels><created>2012-10-19T12:47:51Z</created><updated>2012-10-25T16:38:07Z</updated><resolved>2012-10-22T18:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-22T17:31:37Z" id="9672865">Can you post the exception you see in the logs? Alternatively, can you create a full gist recreation (I can't recreate it here).
</comment><comment author="kimchy" created="2012-10-22T18:03:56Z" id="9674093">ok, I think I managed to find the problem, will push a fix soon.

btw, its highly recommended to keep the filter cache, why do you want to disable it. If you want to restrain it from using too much form the heap allocated to ES, you can set it (by default, it will take 20% of the heap).
</comment><comment author="troszok" created="2012-10-25T16:38:07Z" id="9784613">Thanks for fast feedback. Usually I have the cache turned on - it's only when I was debugging my more complex search queries I wanted to be sure that unexpected result doesn't come from caching. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FieldDataCache API allows getting the wrong cache type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2339</link><project id="" key="" /><description>Hi,

The FieldDataCache's cache method allows you to specify the type of cache you would to have for the requested field

``` Java
FieldData cache(FieldDataType type, IndexReader reader, String fieldName) throws IOException;
```

The current implementation creates the cache request upon the first request and returns. Subsequent request for cache for the field will return the stored version _regardless_ of requested type.

Ideally, I would allow having multiple field data types associated with the same field. If this is not desirable, I would at least throw an exception / raise an error if the requested type can not be served. A third option would be to evict the stored cache and create a new one. This will potentially cause an unintended performance hit if used by an uninformed user.

All of the above work for me.

Cheers,
Boaz
</description><key id="7714500">2339</key><summary>FieldDataCache API allows getting the wrong cache type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2012-10-19T12:41:49Z</created><updated>2013-05-27T18:03:22Z</updated><resolved>2013-05-27T18:03:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Adding Collation Key Analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2338</link><project id="" key="" /><description>Hi,

this patch adds the Lucene CollationKeyAnalyzer for locale-sensitive sorting by using java.text.Collator. I know that ICUCollationKey is much preferable https://issues.apache.org/jira/browse/LUCENE-1719 but it could be useful for non-ICU environments, and where performance is not very important. See also my pull request for java.text.Collator based terms facet sorting. It would be nice being able to align hit sorting and terms facet sorting.

Test is included for demonstrating a collation-key based sort.

Cheers, J&#246;rg
</description><key id="7700872">2338</key><summary>Adding Collation Key Analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>discuss</label></labels><created>2012-10-18T22:10:36Z</created><updated>2015-02-02T08:23:33Z</updated><resolved>2014-07-25T08:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T08:30:22Z" id="50121716">Closing in favour of #6917 
</comment><comment author="GeetNair" created="2015-02-02T05:55:45Z" id="72409676">I am new to elasticsearch and I would like to know the steps required to  use the ICU plugin as a library i.e. we call some API in that plugin which converts our field into required format for sorting.

the problem is that we are trying to use the doc_values = true option in mapping but this cannot be used for string fields having analyzer.So if we need to use ICU plugin then we cannot use doc_value option.

Also will you please provide me stepwise guid from how to install and what needs to be done 
</comment><comment author="jprante" created="2015-02-02T08:23:16Z" id="72420683">@GeetNair this pull request was about aggregations with (non-ICU) collation-based sort. Your question is about the  ICU plugin. Please open an issue there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding collator-based sorting for terms facet entries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2337</link><project id="" key="" /><description>Hi, 

here is a non-ICU feature extension for the standard ES terms facets, which may be useful in linguistic applications. 

Right now it is not possible to sort names in facet entries according to locale settings. So I have added a subpackage org.elasticsearch.search.facet.terms.comparator with a set of comparator classes. One comparator is based on java.text.Collator.  Some new parameters can be given to a facet query: "locale", "reverse", "rules", "strength", and "decomp".

Test case is included in TermsFacetCollationTests.java for german and french sorting, also a rule-based demo.

Cheers, J&#246;rg
</description><key id="7691630">2337</key><summary>Adding collator-based sorting for terms facet entries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>discuss</label></labels><created>2012-10-18T16:22:28Z</created><updated>2014-07-18T09:43:45Z</updated><resolved>2014-07-18T09:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:01:56Z" id="48376663">Related to #2078 
</comment><comment author="jpountz" created="2014-07-18T09:43:45Z" id="49413375">Closing in favor of https://github.com/elasticsearch/elasticsearch/issues/6917
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API for Percolate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2336</link><project id="" key="" /><description>Currently, percolation is not possible in bulk. It is in combination with index, but we would like to use percolate only for matching. Indexing is done in a later step.

Would be great if bulk percolation would be possible.
</description><key id="7688125">2336</key><summary>Bulk API for Percolate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels /><created>2012-10-18T14:35:41Z</created><updated>2013-08-12T16:44:22Z</updated><resolved>2013-08-12T16:44:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tdeconin" created="2012-10-18T14:43:03Z" id="9566901">+1

Using bulk percolation without indexing could seriously speed up things. The matches returned by the percolation determine which index will be used for the actual indexing of the document. So this must remain a separate step in our use case.
</comment><comment author="kimchy" created="2012-10-20T21:00:36Z" id="9635960">Agreed, the ability to "bulk" percolate is very useful. There are some other aspects to percolate API that requires work, like the ability to execute it in distributed mode without hacking multiple indices. The plain is (post other more critical features, like the Lucene 4.0 upgrade) is to revamp the percolate infrastructure and then, part of it, will be adding a "bulk percolate" feature.
</comment><comment author="martijnvg" created="2013-08-12T16:44:21Z" id="22507077">Added multi percolate api via #3488, which allows to bulk percolate requests
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Yellow State: JsonParseException[Numeric value (9223372036854775807) out of range of int] </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2335</link><project id="" key="" /><description>I post this here since i isn't an tire problem.
https://github.com/karmi/tire/issues/474
</description><key id="7681071">2335</key><summary>Yellow State: JsonParseException[Numeric value (9223372036854775807) out of range of int] </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">danielschlegel</reporter><labels /><created>2012-10-18T09:26:59Z</created><updated>2013-06-07T07:20:36Z</updated><resolved>2013-06-07T07:20:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-20T21:03:48Z" id="9635986">A failure like that (the parse json failure) will not cause yellow state. The failure is pretty evident (can't parse that number as its out of range).
</comment><comment author="danielschlegel" created="2012-10-20T21:19:37Z" id="9636121">Yes your right, the json failure doesn't lead into yellow state. The Problem is that the Parsing error occurs when cluster is in yellow state and it disappeared as soon as the cluster was green. There should not be an error in yellow state!?
</comment><comment author="kimchy" created="2012-10-21T19:20:51Z" id="9646381">@danielschlegel the failure disappearing does not relate to the cluster becoming green, I would say its coincidental.
</comment><comment author="spinscale" created="2013-06-06T15:09:53Z" id="19051791">@danielschlegel does this still happen? Can you provide more information so we can try to reproduce and debug?
</comment><comment author="danielschlegel" created="2013-06-07T07:20:29Z" id="19092601">@spinscale no this never happend again, but we updated version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update src/main/java/org/elasticsearch/search/facet/AbstractFacetBuilder...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2334</link><project id="" key="" /><description>Improve handling of global in AbstractFacetBuilder.global(boolean).

Currently argument is ignored and global scope is always enabled.
</description><key id="7654469">2334</key><summary>Update src/main/java/org/elasticsearch/search/facet/AbstractFacetBuilder...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">georgkoester</reporter><labels /><created>2012-10-17T12:40:23Z</created><updated>2014-07-16T21:54:35Z</updated><resolved>2013-07-15T16:22:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-10T15:18:58Z" id="19205422">This looks fixed in the current 0.90 and master branch (though different than your proposed fix) - in the `FacetBuilder` class. I think we can close this one, do you agree?

Sorry for the late feedback!
</comment><comment author="spinscale" created="2013-07-15T16:22:23Z" id="20981284">closing this stale one, happy reopen with more information provided, of what is missing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#2332 support CustomScoreQuery highlighting and expose QueryBuilder on CustomScoreQuery via Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2333</link><project id="" key="" /><description>This PR moves some of the "hacked" query extraction into a custom SpanTermExtractor that can extract those queries even if they are nested in a BQ or similar composite queries. This also exposes queryBuilder support on CustomScoreQueryBuilder via the Java API - this fixes #2332
</description><key id="7649226">2333</key><summary>#2332 support CustomScoreQuery highlighting and expose QueryBuilder on CustomScoreQuery via Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2012-10-17T08:25:30Z</created><updated>2014-06-30T11:08:14Z</updated><resolved>2012-10-19T12:53:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-19T12:53:59Z" id="9599866">Pushed the change, looks great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting fails for ConstantScoreQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2332</link><project id="" key="" /><description>a user ran into this issue on the mailinglist: https://groups.google.com/d/topic/elasticsearch/ozfn5xhpH-g/discussion

We currently can't highlight constant score query if it is nested in another query. The Lucene 3.6 release opened up the WeightedSpanTermExtractor to extract from "unknown" queries, we can possibly move our logic in HighlightPhase in there now.
</description><key id="7648705">2332</key><summary>Highlighting fails for ConstantScoreQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.19.11</label><label>v0.20.0.RC1</label></labels><created>2012-10-17T07:58:04Z</created><updated>2012-10-19T19:34:49Z</updated><resolved>2012-10-19T19:34:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-10-19T19:34:49Z" id="9615698">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ElasticSearch Cloud Azure plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2331</link><project id="" key="" /><description>Develop an ElasticSearch Cloud plugin for Windows Azure to support the discovery of ElasticSearch nodes. Similar reasoning to that of AWS EC2 whereas the discovery of nodes via multicast is not possible on Azure either.
</description><key id="7646199">2331</key><summary>Add ElasticSearch Cloud Azure plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">Yoomax</reporter><labels /><created>2012-10-17T04:24:14Z</created><updated>2014-02-01T07:42:41Z</updated><resolved>2014-02-01T07:42:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-01T07:42:41Z" id="33865876">Heya,

We have released https://github.com/elasticsearch/elasticsearch-cloud-azure

:-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to filter out the null datatype by using elastica in elastic search ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2330</link><project id="" key="" /><description>Hello :

I have a big problem as following :
How to filter out the null datatype by using elastica in elastic search ?
Hope someone can help me to solve it....

Thanks in advance~
Jack
</description><key id="7618888">2330</key><summary>How to filter out the null datatype by using elastica in elastic search ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jclocal</reporter><labels /><created>2012-10-16T09:47:00Z</created><updated>2013-06-10T15:19:32Z</updated><resolved>2013-06-10T15:19:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-10-17T19:22:34Z" id="9540273">Hi Jack,

This is not the right place to ask questions. Please consider reading: http://www.elasticsearch.org/help/

I think you should close this issue as it's not really a bug or a feature request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting index_analyzer incorrectly sets analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2329</link><project id="" key="" /><description>Setting just `index_analyzer` incorrectly sets `analyzer`:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "foo" : {
               "index_analyzer" : "whitespace",
               "type" : "string"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_mapping?pretty=1' 

# {
#    "test" : {
#       "properties" : {
#          "foo" : {
#             "type" : "string",
#             "analyzer" : "whitespace"
#          }
#       }
#    }
# }
```
</description><key id="7589048">2329</key><summary>Setting index_analyzer incorrectly sets analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.19.11</label><label>v0.20.0.RC1</label></labels><created>2012-10-15T10:56:47Z</created><updated>2012-10-16T13:21:51Z</updated><resolved>2012-10-16T13:21:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-16T13:08:44Z" id="9483888">The logic here was that if you set the `index_analyzer` then you would almost always want to set the `search_analyzer` as well, so we do it automatically. But then, it was before we had the `analyzer` setting, now, its confusing, agreed. Will fix it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When searching over multiple indices, don't throw IndexMissingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2328</link><project id="" key="" /><description>When specifying one index only, it makes sense to throw this. When querying on multiple indexes, this should be thrown only if none of the indexes were found. Details on non-existing indices can be embedded in the response, letting the consumer decide if this result is desired or not.

At least have the option of having this configurable.

This is very important to have for scenarios like the "log file indexing" scenario @kimchy talks about
</description><key id="7587226">2328</key><summary>When searching over multiple indices, don't throw IndexMissingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2012-10-15T09:27:01Z</created><updated>2012-10-16T13:04:34Z</updated><resolved>2012-10-15T12:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bryangreen" created="2012-10-15T11:37:46Z" id="9442013">Is this related to #1578?

Agreed, would be a good addition.

On 10/15/2012 03:27, Itamar Syn-Hershko wrote:

&gt; When specifying one index only, it makes sense to throw this. When 
&gt; querying on multiple indexes, this should be thrown only if none of 
&gt; the indexes were found. Details on non-existing indices can be 
&gt; embedded in the response, letting the consumer decide if this result 
&gt; is desired or not.
&gt; 
&gt; At least have the option of having this configurable.
&gt; 
&gt; This is very important to have for scenarios like the "log file 
&gt; indexing" scenario @kimchy https://github.com/kimchy talks about
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elasticsearch/elasticsearch/issues/2328.
</comment><comment author="awick" created="2012-10-15T11:44:03Z" id="9442124">I think this is a duplicate of https://github.com/elasticsearch/elasticsearch/issues/2167 and being fixed in 0.20
</comment><comment author="kimchy" created="2012-10-15T12:58:17Z" id="9443848">Yea, the `ignore_indices` option in the upcoming 0.20 covers this.
</comment><comment author="synhershko" created="2012-10-15T13:33:29Z" id="9444838">Awesome, thanks. Would it report back the missing indexes?
</comment><comment author="kimchy" created="2012-10-16T13:04:34Z" id="9483783">@synhershko no, it will simply ignore ones that are missing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hot_threads output should be JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2327</link><project id="" key="" /><description>Please could you wrap the `hot_threads` output in a JSON structure, as the client APIs all expect to receive JSON rather than plain text
</description><key id="7585653">2327</key><summary>hot_threads output should be JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-10-15T08:02:19Z</created><updated>2014-07-03T19:27:37Z</updated><resolved>2014-07-03T19:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-09T12:02:16Z" id="22390059">Bummer. Do you have a good idea keep it readable for humans and represent an easy to parse multi line JSON string?
</comment><comment author="clintongormley" created="2014-07-03T19:27:37Z" id="47974018">Clients now handle plain text responses too.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Grouping prototype implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2326</link><project id="" key="" /><description>Hi!

I am trying to implement grouping feature and want to contribute to es. This is a proof-of-concept prototype to start conversation :), it is not meant to be pulled.

What do i mean by grouping?
Lets say i have 4 Documents:
{ "id" : 1, "name": "Doc1", "field" : "A" }
{ "id" : 2, "name": "Doc2", "field" : "A" }
{ "id" : 3, "name": "Doc3", "field" : "B" }
{ "id" : 4, "name": "Doc4", "field" : "B" }
I want to execute search that is something like this:
{ "query": { "match_all" : {}, "group", "field" }
With result:
{ "field" : {
    "A" : { "hits" : { "id" : 1, "_source" : { "id" : 1, "name": "Doc1", "field" : "A" } },
                           { "id" : 2, "_source" : { "id" : 2, "name": "Doc2", "field" : "A" } }
             },
    "B" : { "hits" : { "id" : 3, "_source" : { "id" : 3, "name": "Doc3", "field" : "B" } },
                           { "id" : 4, "_source" : { "id" : 4, "name": "Doc4", "field" : "B" } }
            }
}

Implementation.
First i looked at lucene-grouping module, but later decided to make a simple (slower?) one-pass solution based on Facets. Basically, in aggregator i collect docs in maps, and later - sort them and merge into one map.

The code is a work-in-progress for now, because i was unable to fit nicely it into current hierarchy (SearchPhaseController operates only with flat ScoreDocs array, or i have to reimplement whole Search stack from sratch).
Any thought?

Thank!
</description><key id="7573181">2326</key><summary>Grouping prototype implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">artemredkin</reporter><labels /><created>2012-10-14T12:48:21Z</created><updated>2014-06-26T00:22:50Z</updated><resolved>2014-05-12T12:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-12T12:00:30Z" id="42823397">Closed in favour of #6124 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>automatically fill routing parameters based on querystring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2325</link><project id="" key="" /><description>Currently field routing will route a record to a row during the indexing phase. However, when we are querying it, it expects the user to provide the routing= parameter, ignoring the field appearing in the query. if no routing parameter, the request will reach all shards.

Please improve the field routing feature to automatically detect the routing number from the query such  that if the field value appeared in the query, automatically take that value as  the routing key.

Example:

we have a index "videos" defined with routing on "account_id" field:
    "videos": {
    "_routing": {
    "required": true,
    "path": "account_id"
    },
    "properties": {
    "account_id": {
    "type": "integer",
    "ignore_malformed": false
    },
    "text": {
    "type": "string",
    "analyzer": "simple"
    },
    ...

The two queries given different performance (measured through apache bench):
     - http://localhost:9200/videos/_search?q=account_id:1001560%20and%20text:vampire
     - http://localhost:9200/videos/_search?q=account_id:1001560%20and%20text:vampire&amp;routing=1001560

Make it such that if account_id field appears in query, it automatically assign it as routing (so that no need to update it at application level. )

Thank you.

request based on the discussion from the mailing list:

Hi Yuhan

&gt; The existing code consists various queries with account_id provided.
&gt; ex:
&gt; http://localhost:9200/videos/_search?q=account_id:123+AND+text:test
&gt; 
&gt; So, will elasticsearch smartly reach just 1 shard on this query? or
&gt; will it still reach all shards?

Ah right. Not sure, but I think the answer is probably no.  It won't do
that automatically. You'll still need to provide the routing value.

Maybe it could be supported.  Perhaps open an issue.

clint
</description><key id="7558332">2325</key><summary>automatically fill routing parameters based on querystring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yuhanonescreen</reporter><labels /><created>2012-10-13T01:41:32Z</created><updated>2014-07-08T18:01:03Z</updated><resolved>2014-07-08T18:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:01:03Z" id="48376546">Hi @yuhanonescreen 

The search request is routed to the shards that need to handle the request before the query is parsed. In fact, we're discussing removing support for the `_routing` path in #6730. On top of that, multi-index, multi-type queries with potentially many different analyzer configurations would make this automatic routing a nightmare to implement and debug.

If you know what field should be used for routing, why not extract it in your application and use it as a routing value to the query?  Seems a lot simpler than trying to get it right within something as complex as the query dsl.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for Issue #2322</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2324</link><project id="" key="" /><description>Fixing the transposed lat and lon values in various geo filter builders.
</description><key id="7554813">2324</key><summary>Fix for Issue #2322</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gorbachev</reporter><labels /><created>2012-10-12T21:34:19Z</created><updated>2014-07-16T21:54:36Z</updated><resolved>2012-10-12T22:48:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gorbachev" created="2012-10-12T22:48:27Z" id="9395942">Closing, not an issue. See comments on issue #2322
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for Issue #2322</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2323</link><project id="" key="" /><description>Fixing the transposed lat and lon values in various geo filter builders.
</description><key id="7554673">2323</key><summary>Fix for Issue #2322</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gorbachev</reporter><labels /><created>2012-10-12T21:27:43Z</created><updated>2014-07-16T21:54:37Z</updated><resolved>2012-10-12T21:34:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gorbachev" created="2012-10-12T21:34:02Z" id="9391975">Re-submitting from another branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Various Geo Filter Builder classes are transposing lat and lon values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2322</link><project id="" key="" /><description>From GeoDistanceFilterBuilder.doXContent() line 124:

```
        builder.startArray(name).value(lon).value(lat).endArray();
```

This doesn't seem right.

Using the builder as follows:

```
        SearchRequestBuilder request = client.prepareSearch("index")
                .setSearchType(SearchType.QUERY_AND_FETCH)
                .setQuery(matchAllQuery())
                .setFilter(geoDistanceFilter("geopoint_field")
                        .distance(25.0, DistanceUnit.KILOMETERS)
                        .lat(20.0)
                        .lon(30.0)
                        .geoDistance(GeoDistance.ARC))
                .setFrom(0)
                .setSize(10);
```

creates a query:

{
  "from" : 0,
  "size" : 10,
  "query" : {
    "match_all" : { }
  },
  "filter" : {
    "geo_distance" : {
      "geopoint_field" : [ 30.0, 20.0 ],
      "distance" : "300.0km",
      "distance_type" : "arc"
    }
  }
}

It should be "geopoint_field" : [20.0, 30.0]
</description><key id="7554654">2322</key><summary>Various Geo Filter Builder classes are transposing lat and lon values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gorbachev</reporter><labels /><created>2012-10-12T21:26:56Z</created><updated>2012-10-12T22:47:57Z</updated><resolved>2012-10-12T22:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jacobevans" created="2012-10-12T22:13:21Z" id="9394704">From ES documentation. (http://www.elasticsearch.org/guide/reference/mapping/geo-point-type.html)

```
Lat Lon as Array

Format in [lon, lat], note, the order of lon/lat here in order to conform with GeoJSON.
```

It's counter-intuitive, but intentional.  Pretty sure there's no plan to change this behavior...actually talked to @kimchy about it today.
</comment><comment author="chrismale" created="2012-10-12T22:44:58Z" id="9395874">The use of [lon, lat] is becoming standard in Point / Shape definitions since lon, lat is comparable to x, y which we use for cartesian coordinates.  GeoJSON (http://www.geojson.org/) and WKT (http://en.wikipedia.org/wiki/Well-known_text) both format points this way.
</comment><comment author="gorbachev" created="2012-10-12T22:47:57Z" id="9395931">Thanks chris and jacob. Missed that on the docs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>System.err.println causes NullPointerException when using ElasticSearch as a java library</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2321</link><project id="" key="" /><description>After upgrade of Elastic Search java library from version 0.19.4 to 0.19.9 there are random NullPointerExceptions when using System.err.
They are caused by code in org.elasticsearch.common.compress.snappy.xerial.XerialSnappy.
Best way to repair this would be to patch snappy-java library and remove broken code from XerialSnappy.
If it's impossible then maybe at least replace System.setErr(null) to System.setErr(nullPrintStream) where nullPrintStream is an instance of PrintStream that does nothing when invoked from Snappy?
</description><key id="7538081">2321</key><summary>System.err.println causes NullPointerException when using ElasticSearch as a java library</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Avatah</reporter><labels /><created>2012-10-12T09:37:15Z</created><updated>2013-06-24T12:59:19Z</updated><resolved>2013-06-24T12:59:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-20T20:08:47Z" id="9635456">Can you share the stack trace you get? I don't get this when snappy fails to load.
</comment><comment author="devilankur18" created="2012-10-27T05:36:50Z" id="9832849">I am also facing this issue. Here is my stack. Check if it is useful

2012-10-27 11:01:01,860 DEBUG: com.github.joschi.jadconfig.JadConfig - Trimmed parameter value twilio_sender
2012-10-27 11:01:01,860 DEBUG: com.github.joschi.jadconfig.JadConfig - Validating parameter twilio_sender
2012-10-27 11:01:01,861 DEBUG: com.github.joschi.jadconfig.JadConfig - Validating parameter twilio_sender with value 
2012-10-27 11:01:01,861 DEBUG: com.github.joschi.jadconfig.JadConfig - Converting parameter value twilio_sender: 
2012-10-27 11:01:01,861 DEBUG: com.github.joschi.jadconfig.JadConfig - Trying to find converter class class com.github.joschi.jadconfig.converters.NoConverter for type class java.lang.String
2012-10-27 11:01:01,862 DEBUG: com.github.joschi.jadconfig.JadConfig - Loaded converter class for type class java.lang.String: com.github.joschi.jadconfig.converters.StringConverter@6766afb3
2012-10-27 11:01:01,862 DEBUG: com.github.joschi.jadconfig.JadConfig - Setting parameter twilio_sender to 
2012-10-27 11:01:01,866 DEBUG: com.github.joschi.jadconfig.JadConfig - Invoking validator method public void org.graylog2.Configuration.validate() throws com.github.joschi.jadconfig.ValidationException in org.graylog2.Configuration@4c5e176f
2012-10-27 11:01:02,237 INFO : org.elasticsearch.node - [graylog2-server] {0.19.9}[32313]: initializing ...
2012-10-27 11:01:02,933 INFO : org.elasticsearch.plugins - [graylog2-server] loaded [], sites []
2012-10-27 11:01:02,941 DEBUG: org.elasticsearch.common.compress.lzf - using [UnsafeChunkDecoder] decoder
2012-10-27 11:01:02,942 DEBUG: org.elasticsearch.common.compress - failed to load xerial snappy-java
java.lang.NoClassDefFoundError: org/xerial/snappy/Snappy
    at org.elasticsearch.common.compress.snappy.xerial.XerialSnappy.&lt;clinit&gt;(XerialSnappy.java:41)
    at org.elasticsearch.common.compress.CompressorFactory.&lt;clinit&gt;(CompressorFactory.java:58)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:121)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)
    at org.graylog2.indexer.EmbeddedElasticSearchClient.&lt;init&gt;(EmbeddedElasticSearchClient.java:88)
    at org.graylog2.Core.initialize(Core.java:148)
    at org.graylog2.Main.main(Main.java:121)
Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.Snappy
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
    ... 8 more
2012-10-27 11:01:03,283 TRACE: org.elasticsearch.monitor.sigar - failed to load sigar
java.lang.ClassNotFoundException: org.hyperic.sigar.Sigar
    at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
    at org.elasticsearch.monitor.MonitorModule.configure(MonitorModule.java:80)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:201)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:82)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:130)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:152)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)
    at org.graylog2.indexer.EmbeddedElasticSearchClient.&lt;init&gt;(EmbeddedElasticSearchClient.java:88)
    at org.graylog2.Core.initialize(Core.java:148)
    at org.graylog2.Main.main(Main.java:121)
2012-10-27 11:01:03,604 DEBUG: org.elasticsearch.threadpool - [graylog2-server] creating thread_pool [generic], type [cached], keep_alive [30s]
2012-10-27 11:01:03,609 DEBUG: org.elasticsearch.threadpool - [graylog2-server] creating thread_pool [index], type [cached], keep_alive [5m]
2012-10-27 11:01:03,609 DEBUG: org.elasticsearch.threadpool - [graylog2-server] creating thread_pool [bulk], type [cached], keep_alive [5m]
2012-10-27 11:01:03,609 DEBUG: org.elasticsearch.threadpool - [graylog2-
</comment><comment author="kimchy" created="2012-10-27T22:59:48Z" id="9840831">@devilankur18 those are valid output messages, thats DEBUG output noting that snappy did not load. Can you check 0.19.11? I did not get the relevant stack trace, but if an NullPointerException happened, it shouldn't happen now.
</comment><comment author="mkpandey" created="2013-02-25T04:52:27Z" id="14025773">I am using elasticsearch-0.19.10 and find the Error when submit the jsp form 
my java class code:
queryFilters.add(FilterBuilders.prefixFilter("Education", getZeducation()));
            log.debug(FilterBuilders.prefixFilter("Education", getZeducation()));
            queryFilters.add(FilterBuilders.prefixFilter("ResumeHeadline", getZresumeHeadline()));
            FilterBuilder aggFilter = FilterBuilders.andFilter(queryFilters);
            SearchResponse response = client.prepareSearch("resume_index")
                      .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                         .setQuery(QueryBuilders.prefixQuery("Name",getZname()))
                          .setFilter(aggFilter)
Error:

failed to load xerial snappy-java
java.lang.NoClassDefFoundError: org/xerial/snappy/Snappy
    at org.elasticsearch.common.compress.snappy.xerial.XerialSnappy.&lt;clinit&gt;(XerialSnappy.java:41)
    at org.elasticsearch.common.compress.CompressorFactory.&lt;clinit&gt;(CompressorFactory.java:58)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:161)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:117)
    at com.SaralHiring.Search.execute(Search.java:57)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.opensymphony.xwork2.DefaultActionInvocation.invokeAction(DefaultActionInvocation.java:452)
    at com.opensymphony.xwork2.DefaultActionInvocation.invokeActionOnly(DefaultActionInvocation.java:291)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:254)
    at com.opensymphony.xwork2.interceptor.DefaultWorkflowInterceptor.doIntercept(DefaultWorkflowInterceptor.java:176)
    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:98)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.validator.ValidationInterceptor.doIntercept(ValidationInterceptor.java:263)
    at org.apache.struts2.interceptor.validation.AnnotationValidationInterceptor.doIntercept(AnnotationValidationInterceptor.java:68)
    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:98)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ConversionErrorInterceptor.intercept(ConversionErrorInterceptor.java:133)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ParametersInterceptor.doIntercept(ParametersInterceptor.java:207)
    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:98)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ParametersInterceptor.doIntercept(ParametersInterceptor.java:207)
    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:98)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.StaticParametersInterceptor.intercept(StaticParametersInterceptor.java:190)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at org.apache.struts2.interceptor.MultiselectInterceptor.intercept(MultiselectInterceptor.java:75)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at org.apache.struts2.interceptor.CheckboxInterceptor.intercept(CheckboxInterceptor.java:94)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at org.apache.struts2.interceptor.FileUploadInterceptor.intercept(FileUploadInterceptor.java:243)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ModelDrivenInterceptor.intercept(ModelDrivenInterceptor.java:100)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ScopedModelDrivenInterceptor.intercept(ScopedModelDrivenInterceptor.java:141)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at org.apache.struts2.interceptor.debugging.DebuggingInterceptor.intercept(DebuggingInterceptor.java:267)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ChainingInterceptor.intercept(ChainingInterceptor.java:142)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.PrepareInterceptor.doIntercept(PrepareInterceptor.java:166)
    at com.opensymphony.xwork2.interceptor.MethodFilterInterceptor.intercept(MethodFilterInterceptor.java:98)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.I18nInterceptor.intercept(I18nInterceptor.java:176)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at org.apache.struts2.interceptor.ServletConfigInterceptor.intercept(ServletConfigInterceptor.java:164)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.AliasInterceptor.intercept(AliasInterceptor.java:190)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at com.opensymphony.xwork2.interceptor.ExceptionMappingInterceptor.intercept(ExceptionMappingInterceptor.java:187)
    at com.opensymphony.xwork2.DefaultActionInvocation.invoke(DefaultActionInvocation.java:248)
    at org.apache.struts2.impl.StrutsActionProxy.execute(StrutsActionProxy.java:52)
</comment><comment author="michaelklishin" created="2013-02-25T09:10:26Z" id="14031548">The reason is in the very first line of the stack trace:

```
java.lang.NoClassDefFoundError: org/xerial/snappy/Snappy
```

This may be due to a [known problem with Snappy on OS X with JDK 7](https://github.com/xerial/snappy-java/issues/6). [The workaround](https://github.com/ptaoussanis/carmine/issues/5#issuecomment-6450607) is to extract the native
library and copy it somewhere on the classpath, e.g. `./lib`. 
</comment><comment author="spinscale" created="2013-05-27T16:57:50Z" id="18507184">Hey,

do you still hit this issue with 0.90? (you shouldnt as we do not use snappy anymore)
If not, can you close this issue or alternatively can someone provide more information (with the current version) so we can find out, what needs to be fixed?
</comment><comment author="spinscale" created="2013-06-24T12:59:19Z" id="19905105">Closing. Please reopen, if you still find this to be valid in current versions. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lintian errors on deb package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2320</link><project id="" key="" /><description>When I run lintian to check the package I'm getting some errors.
This is the lintian command: lintian -EvI --color=auto elasticsearch-0.19.10.deb

And these are the errors:
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
E: elasticsearch: arch-dependent-file-in-usr-share usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
E: elasticsearch: unstripped-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so
E: elasticsearch: arch-dependent-file-in-usr-share usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so
E: elasticsearch: unstripped-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so
E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so
E: elasticsearch: arch-dependent-file-in-usr-share usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so
E: elasticsearch: unstripped-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so
E: elasticsearch: missing-dependency-on-libc needed by usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so and 2 others
E: elasticsearch: control-file-has-bad-permissions conffiles 0755 != 0644
E: elasticsearch: no-copyright-file

Can someone fix that?
Thanks.
</description><key id="7536718">2320</key><summary>Lintian errors on deb package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">voyagersm</reporter><labels><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2012-10-12T08:31:15Z</created><updated>2013-06-07T11:53:42Z</updated><resolved>2013-06-07T11:53:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-23T15:03:20Z" id="18349163">Hey,

thanks for reporting.

Regarding the arch dependent file. We will not start shipping an own version of elasticsearch for each binary type, so maybe we can disable certain warnings (the way RPM does it) or we will have to live with it. We are using these library files only if we are on the right architecture anyway.

Regarding the copyright file, it might make sense to copy the included copyright file to the correct destination.

Not sure about the control file issue, but maybe I can take a look at it as well.

Do you agree?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_id field's path setting dose not work some time, especially when we create an index and do come indexing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2319</link><project id="" key="" /><description>I set the _id's path as:
"_id":{
        "path":"id"
},
I generate an id in my own program. But ES sometimes also make its own _id value.
Especially  first time to index the date while index is not exist, all the _id id made by ES.
</description><key id="7535845">2319</key><summary>_id field's path setting dose not work some time, especially when we create an index and do come indexing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dongxurr123</reporter><labels /><created>2012-10-12T07:38:31Z</created><updated>2014-07-08T17:18:07Z</updated><resolved>2014-07-08T17:18:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T17:18:07Z" id="48370631">I think this has been solved already with the new Ack mechanism. If this is still a problem, please open a new issue with a recreation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index aliases and delete operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2318</link><project id="" key="" /><description>Suppose we use the "usersdata flow" exposed in kimchy's "Big Data, Search and Analytics" presentation to index users documents in a big oversharded index with routing+filtering aliases. Some users content are moved to their own index when becoming too big.

From the client perspective we don't know if we are manipulating a virtual index via an alias or a real index dedicated to the user and the code should be the same. So if an user is deleted and we want to remove its content the client code do something like curl -XDELETE localhost:9200/$INDEX_NAME. The power of aliasing being that the client is agnostic of knowing if it deals with a real index or an aliased one.

But with the current behavior if $INDEX_NAME is an alias the whole big oversharded index will be deleted and all users document are lost!

IMHO it is really dangerous. I was expecting a simple delete index if the $INDEX_NAME is a real index under the hood and maybe a delete by query (even if it is expensive) or at least a failure or something like that maybe configurable via a parameter if $INDEX_NAME is an alias linking to the big oversharded index.

What do you think?
</description><key id="7535133">2318</key><summary>Index aliases and delete operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">Paikan</reporter><labels><label>:Aliases</label><label>enhancement</label><label>Pretty Bloody Important</label><label>v6.0.0</label></labels><created>2012-10-12T06:45:59Z</created><updated>2017-06-16T15:46:02Z</updated><resolved>2017-06-16T15:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-03-03T14:25:38Z" id="14347575">Hey any update on this one? @imotov are you unhappy with your proposed fix?
</comment><comment author="s1monw" created="2013-03-07T19:26:23Z" id="14580774">I thought about this a bit and I tend to agree with @Paikan that this is pretty dangerous. Maybe we can add an option that allows you to specify an "on_delete" operation on the alias that is executed on a delete and if it's not present we just fail to delete? Any thoughts?
</comment><comment author="kimchy" created="2013-07-30T19:52:00Z" id="21817783">We can reject an alias with a registered filter when trying to delete it. Another potential solution is that if the delete is done on an alias with a filter, we delete by query the user content, and not delete the index (that would make the implementation much more complex though...).

@imotov regarding the change, I think it makes sense to have a method on the metadata that resolves jsut aliases out of the list of indices, and do checks on it where applicable. Or somethign like the lines. Otherwise, the concreteAndXXX might get out of hand in terms of options.
</comment><comment author="jprante" created="2013-07-30T20:14:13Z" id="21819235">I think the solution of @imotov to reject aliases with an ElasticsearchIllegalArgumentException is not correct. Instead, a name of an index alias should be silently skipped to maintain the idempotency of the REST DELETE operations. I suggest the following semantics:

`curl -XDELETE 'http://localhost:9200/{name}'`  = deletes {name} if and only if {name} is a concrete index,  otherwise it does nothing and it never returns an error

`curl -XDELETE 'http://localhost:9200/_alias/{name}'`  = deletes {name} if and only if {name} is an index alias, otherwise it does nothing and it never returns an error
</comment><comment author="kimchy" created="2013-07-30T20:21:22Z" id="21819715">@jprante I might be missing something, but if it is rejected, what makes it not idempotent?
</comment><comment author="Paikan" created="2013-07-30T20:27:44Z" id="21820159">I think the best approach would be a delete by query because it is what the client expect at the end
</comment><comment author="Paikan" created="2013-07-30T20:37:36Z" id="21820842">The @s1monw approach is also quite flexible by default we reject the delete operation on alias with a registered filter but when creating the alias we could set a boolean in the alias to specify we want to delete data if deleting an aliased index.
</comment><comment author="jprante" created="2013-07-30T20:49:00Z" id="21821655">@kimchy the question is if names are mixed in one REST command, should `curl -XDELETE 'http://localhost:9200/concreteindex,aliasindexname'` succeed or fail? I think it shall succeed but 'aliasindexname' is ignored.
</comment><comment author="kimchy" created="2013-07-30T20:50:30Z" id="21821784">@jprante understood, and thats one option, but rejecting it will not break the REST delete contract. I don't think they should succeed because you loose information on what you intentionally asked ES to do.
</comment><comment author="Paikan" created="2013-08-25T11:16:23Z" id="23225887">Till this issue has not been addressed properly why not rejecting a delete on a filtered alias to avoid data loss. Maybe a message explaining that an explicit delete by query order should be use in that case could be returned?
</comment><comment author="Paikan" created="2014-05-26T19:06:43Z" id="44211495">As discussed with @s1monw ping this one as I think it is still a critical issue that can lead to data loss.
</comment><comment author="Paikan" created="2014-11-13T11:31:50Z" id="62877752">@jpountz as discussed this morning this is a ping :)
</comment><comment author="javanna" created="2014-11-13T14:53:36Z" id="62901120">Hey @Paikan don't worry this is on our radar, we do want to fix it, just a matter of time ;)
</comment><comment author="Paikan" created="2014-11-13T22:44:49Z" id="62980543">Hey @javanna perfect then thanks for the update!
</comment><comment author="javanna" created="2015-06-08T12:02:49Z" id="109967149">I just labelled back for discussion, in #11496 we started discussing again what we should do with delete against aliases. Seems like the best approach would be to reject delete against any alias (regardless of whether it's filtered or not). Also, when we expand wildcards we should not expand to aliases only for this api. The latter is trickier to be honest, we don't currently support this and in order to support it we should probably add yet another "indices options", which are already too many. I would love to simplify those indices options and the code around it before adding other ones to the mix.
</comment><comment author="clintongormley" created="2015-09-19T17:17:32Z" id="141690394">++ for not allowing aliases in the DELETE /index api
</comment><comment author="sundarv85" created="2016-03-23T10:13:57Z" id="200282178">+1
</comment><comment author="compasses" created="2016-10-30T08:39:40Z" id="257138974">agree with @jprante , and want to know when will it be fixed.
</comment><comment author="clintongormley" created="2016-12-09T10:35:22Z" id="265983368">Discussed in FixItFriday: expand wildcards to aliases and concrete indices.  If any aliases are present, throw an exception</comment><comment author="BrickXu" created="2017-02-14T06:53:34Z" id="279623216">+1
In my production cluster, I set an alias for a collection of indices and make the query easily, but somebody delete this alias and indices were lost, it is really dangerous if elasticsearch do nothing for alias.</comment><comment author="neogeo" created="2017-03-03T15:08:13Z" id="283978211">At the very least, the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html#deleting) should be updated to mention this unexpected behavior. It looks like a lot of people find this out by  trial and error. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Using _default_ mapping type with _source excludes (or array based config) can cause the array to grow indefinitely </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2317</link><project id="" key="" /><description>Here is an example for mapping, where if new fields are being introduced, it causes the excludes array in _source to keep on growing with the same value.

```
curl -XPUT localhost:9200/test -d '{
"mappings" : {
 "_default_" : {
     "date_detection" : false,
     "_source" : { "excludes" : ["*smart_typed*"] },
     "dynamic_templates" : [
       {
       "numberIndexing":{
         "match" :"smart_typed_number_*",
         "mapping":{
           "type":"double"
         }
       }
     },
     {
       "geoIndexing" : {
         "match" : "smart_typed_location",
         "mapping" : {
           "type" : "geo_point"
         }
       }
     },
     {
       "exactString" : {
         "match" : "*",
         "match_mapping_type" : "string",
         "mapping" : {
           "type" : "multi_field",
           "fields" : {
             "{name}" : {"type": "string", "index" : "analyzed"},
             "exact" : {"type": "string", "index" : "not_analyzed"}
           }
         }
       }
     }
     ]
 }
}
}'
```
</description><key id="7532355">2317</key><summary>Mapping: Using _default_ mapping type with _source excludes (or array based config) can cause the array to grow indefinitely </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.11</label><label>v0.20.0.RC1</label></labels><created>2012-10-12T01:59:55Z</created><updated>2012-10-12T02:02:22Z</updated><resolved>2012-10-12T02:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Better handling of missing index failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2316</link><project id="" key="" /><description>We see the following during normal operation, when trying to add documents. Auto-create indexes is set to true.

[2012-10-10 14:00:11,259][INFO ][cluster.metadata         ] [Iron Man 2020] [el-2011-11-21-0000] creating index, cause [auto(index api)], shards [1]/[0], mappings [thread]
[2012-10-10 14:00:11,294][DEBUG][action.admin.indices.stats] [Iron Man 2020] [el-2011-11-21-0000][0], node[LxU0GSjRQtmZM0h8vBxuBg], [P], s[INITIALIZING]: Failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@41201158]
org.elasticsearch.transport.RemoteTransportException: [Crimson Cowl][inet[/192.168.1.11:9301]][indices/stats/s]
Caused by: org.elasticsearch.indices.IndexMissingException: [el-2011-11-21-0000] missing
    at org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:244)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:144)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:53)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
[2012-10-10 14:00:11,295][DEBUG][action.admin.indices.status] [Iron Man 2020] [el-2011-11-21-0000][0], node[LxU0GSjRQtmZM0h8vBxuBg], [P], s[INITIALIZING]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@1b3f459d]
org.elasticsearch.transport.RemoteTransportException: [Crimson Cowl][inet[/192.168.1.11:9301]][indices/status/s]
Caused by: org.elasticsearch.indices.IndexMissingException: [el-2011-11-21-0000] missing
    at org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:244)
    at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:152)
    at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:59)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
</description><key id="7516030">2316</key><summary>Better handling of missing index failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2012-10-11T15:12:03Z</created><updated>2013-08-01T22:38:13Z</updated><resolved>2013-08-01T22:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-10-11T19:23:29Z" id="9354291">It seems that you are executing /_stats (indices stats) requests. The auto index creation is only applicable for the index api. In the upcoming 0.20.0 version there will be the `ignore_indices` (#2209) request param that will help you prevent these errors.
</comment><comment author="synhershko" created="2012-10-11T21:04:39Z" id="9357877">I might have had the ES Paramedic thing on, yes, but how could this have triggered this alert? I didn't delete any index, so I can't see how there could be a request for a non-existing index.
</comment><comment author="javanna" created="2013-08-01T22:31:36Z" id="21974744">@synhershko Are you still experiencing this problem? Did the ignore_indices parameter help perhaps?
</comment><comment author="synhershko" created="2013-08-01T22:38:13Z" id="21975105">Maybe, haven't experienced this since. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verify that source object is ended properly and does not contain any trailing tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2315</link><project id="" key="" /><description>...iling tokens (this might corrupt client result parsing).

Invalidate document when the source containins trailing tokens:

Example (indexing):

'{}}' =&gt; invalid sice it contains an extra '}' after the source object is exited.
</description><key id="7482473">2315</key><summary>Verify that source object is ended properly and does not contain any trailing tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lindstromhenrik</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>bug</label></labels><created>2012-10-10T13:28:58Z</created><updated>2015-06-07T18:08:26Z</updated><resolved>2015-05-29T10:21:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-14T09:00:59Z" id="48878191">I [updated](https://github.com/s1monw/elasticsearch/commit/4dca0960be56c0c5a8cc80c021c1bf6636b91241) this PR a bit with the addition this is only applied to indices that are created with ES v 1.3. 
</comment><comment author="bleskes" created="2014-07-14T09:49:47Z" id="48882123">To be completely honest, I don't think the document mapper can decide that when it's done parsing it is also the end of the underlying stream. It should also not mutate it with reading nextToken on it beyond the object scope. Imho it's the responsibility of the caller like the TransportIndexAction to validate the trailing content
</comment><comment author="clintongormley" created="2014-08-22T07:06:11Z" id="53029669">@s1monw @bleskes what are the next steps here?
</comment><comment author="clintongormley" created="2014-10-16T19:31:22Z" id="59416891">@bleskes can you think of a single place where we can perform this check for all REST requests?
</comment><comment author="bleskes" created="2014-10-18T12:08:26Z" id="59610650">A quick fix would be to do the check in this PR does but only if the parser was created locally, like here: https://github.com/episerver/elasticsearch/blob/ThrowMapperParserExceptionIfSourceIsNotEndedProperly/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java#L518  

A more general solution is to see if it makes sense (not sure) to add a flag to the close method of XContentParser to verify that the underlying stream was fully consumed.

Btw - we should also consider accepting trailing empty spaces and new lines - I'm not sure whether the current check is good or not for that. As a different change, I would trim those down when copying over the bytes in SourceToParse
</comment><comment author="clintongormley" created="2014-10-20T11:38:26Z" id="59734277">This is certainly something I would like to see fixed, but we need to fix it in a clean and generic way, as per @bleskes' comments. 

@lindstromhenrik would you be interested in taking this further?  It may take a few iterations.
</comment><comment author="s1monw" created="2014-11-21T10:37:30Z" id="63953371">@bleskes lets talk about this at some point - we should move forward here 
</comment><comment author="javanna" created="2015-03-28T08:00:49Z" id="87188740">We discussed this issue and PR, it is indeed a nice to have. We would want to expand it a bit probably, e.g. make sure we consume the entire document we are going to index, and add some more tests with different invalid jsons.

We have some concerns around backwards compatibility though. Even if we apply this validation at index time only (so we don't break searches for invalid documents that might have been previously stored), the additional validation might cause problems anyway because there is no way to fix invalid jsons other than deleting/reindexing them. Once this feature is in, we might rely on it in the future (e.g. assuming stored json is always valid) but that would be wrong as we will never be sure that there are no invalid jsons in the index.

That said, I am personally for moving forward with this, by looking at it as just a best effort to reject invalid jsons. Some might still get through or might be in the index from before, so it's something that we should never rely on in future features. Marking as adoptme since this is a good starting point but some more work needs to be done on it.
</comment><comment author="rmuir" created="2015-04-08T19:10:42Z" id="91007342">So the summary is, because ES did the wrong thing before, it must always do the wrong thing forever? Doesn't sound right...
</comment><comment author="rjernst" created="2015-04-08T19:38:41Z" id="91014313">This should be simple to solve backcompat.  Ignore the check on indexes created before 2.0 (although I think this is silly and we should just always enforce the check).
</comment><comment author="s1monw" created="2015-04-08T20:05:19Z" id="91020242">+1 @rjernst 
</comment><comment author="rjernst" created="2015-05-29T10:21:16Z" id="106766516">I've revived this and created a new PR: #11414
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Importing a renamed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2314</link><project id="" key="" /><description>If I copy the directory that contains an index to a new directory name, then restart the cluster, I see the message:

```
[INFO ][gateway.local.state.meta ] [Sytsevich, Aleksei] dangled index directory name is [new_name], state name is [old_name], renaming to directory name
```

This, to me, implies that the state name is being changed to the directory name.  In other words, I should have a new index called `new_name`.

Actually, it just ignores the index because we already have `old_name`.

If I edit the state file to change the state name, then I get a new index with the edited name.

So first: the log message is inaccurate. It should be "renaming to state name".
But second, I think the better behaviour would be to update the state name to the directory name.  For one, this would make it easy to physically rename an index.
</description><key id="7480497">2314</key><summary>Importing a renamed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-10-10T12:41:58Z</created><updated>2014-04-30T07:48:12Z</updated><resolved>2012-11-10T17:01:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-11-10T17:01:17Z" id="10257193">In version 0.20, this works, so closing
</comment><comment author="thetuxracer" created="2014-04-30T07:37:16Z" id="41768591">Hello, I am still getting this message. Pastebin: http://pastebin.com/NdbfDzZU

Steps to reproduce:

```
1. Stop ES. 
2. Enter into data directory: elasticsearch/graylog2/nodes/0/indices
3. mv graylog_14 graylog_14_new
4. Start ES.
```

Cluster health becomes yellow. The old index and new index are both shown in the elasticsearch-head plugin. If I stop ES, delete this folder called graylog_14, and start ES again, everything goes green.

Thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException on a running node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2313</link><project id="" key="" /><description>When connecting using the Java API and doing a mass import. All documents made it in as far as I can tell, but I noticed the following error in the console (using 0.9.10):

[2012-10-10 13:58:50,451][INFO ][cluster.metadata         ] [Iron Man 2020] [el-2012-10-09-0000] creating index, cause [auto(index api)], shards [1]/[0], mappings [thread]
[2012-10-10 13:58:50,619][INFO ][cluster.metadata         ] [Iron Man 2020] [el-2012-10-09-0000] update_mapping [thread](dynamic)
[2012-10-10 13:58:50,938][INFO ][cluster.service          ] [Iron Man 2020] removed {[Rosenberg, Marsha][GlUDrCclSweRcWq2hNjbtw][inet[/192.168.1.11:9302]]{client=true, data=false},}, reason: zen-disco-node_failed([Rosenberg, Marsha][GlUDrCclSweRcWq2hNjbtw][inet[/192.168.1.11:9302]]{client=true, data=false}), reason transport disconnected (with verified connect)
[2012-10-10 13:58:50,939][DEBUG][action.admin.cluster.node.stats] [Iron Man 2020] failed to execute on node [GlUDrCclSweRcWq2hNjbtw]
org.elasticsearch.transport.NodeDisconnectedException: [Rosenberg, Marsha][inet[/192.168.1.11:9302]][cluster/nodes/stats/n] disconnected
[2012-10-10 14:00:03,466][INFO ][cluster.service          ] [Iron Man 2020] added {[Modred the Mystic][D1OyVHOBTGex3kQ29PU6ow][inet[/192.168.1.11:9302]]{client=true, data=false},}, reason: zen-disco-receive(join from node[[Modred the Mystic][D1OyVHOBTGex3kQ29PU6ow][inet[/192.168.1.11:9302]]{client=true, data=false}])
[2012-10-10 14:00:03,509][DEBUG][action.admin.cluster.node.stats] [Iron Man 2020] failed to execute on node [D1OyVHOBTGex3kQ29PU6ow]
org.elasticsearch.transport.RemoteTransportException: [Modred the Mystic][inet[/192.168.1.11:9302]][cluster/nodes/stats/n]
Caused by: java.lang.NullPointerException
    at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:66)
    at org.elasticsearch.action.admin.cluster.node.stats.NodeStats.writeTo(NodeStats.java:290)
    at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:139)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:76)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:67)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:276)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
</description><key id="7479374">2313</key><summary>NullPointerException on a running node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2012-10-10T12:11:11Z</created><updated>2013-05-27T16:30:02Z</updated><resolved>2013-05-27T16:30:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-27T16:24:19Z" id="18505973">Hey,

is this still an issue for you in the current 0.90 release? Can you reproduce it? If so, can you create a gist, so we can fix it?

Otherwise I'd like to close it.
</comment><comment author="synhershko" created="2013-05-27T16:30:02Z" id="18506191">Hasn't happened in a while, closing it for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow control over if we load settings from System Properties when using TransportClient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2312</link><project id="" key="" /><description>I had problems when trying to use the TransportClient inside a java application plugin - that now uses an internal ES node.  They are setting all the ES settings via the System Properties, so my settings get overridden.  I am now just allowing a bit more control over how the settings get loaded.
</description><key id="7455359">2312</key><summary>Allow control over if we load settings from System Properties when using TransportClient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NickPadilla</reporter><labels /><created>2012-10-09T17:09:31Z</created><updated>2014-07-02T22:19:03Z</updated><resolved>2012-12-17T04:35:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="NickPadilla" created="2012-10-09T17:11:25Z" id="9269934">Here is a link to some more discussion over this topic in the google group :

https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/jRxu0ZzaKo8
</comment><comment author="NickPadilla" created="2012-10-22T14:35:46Z" id="9665956">Hello All - any idea when this fix might make it in a release?  Just a general timeline would be helpful.  I need to figure out how/when to move this fix into production.  It will either mean building a custom jar with the latest fully released codebase or waiting for this fix to make it in.  Thanks for the great product!
</comment><comment author="NickPadilla" created="2012-10-26T16:46:11Z" id="9819222">@kimchy Hey Shay, any way I could get a timeline on when we could expect this "fix" to be in a release?  I don't want o give my client a custom jar and then they update it and it breaks their stuff because this isn't in it.  Please advise.  Thanks!
</comment><comment author="NickPadilla" created="2012-11-26T16:05:42Z" id="10721066">@imotov  Hey Igor, is there any way to get an update as to how long this update might take to make it in trunk - or a release?  I was hoping it could be in trunk before December, but that is looking unlikely.  If there is anything you need from me please let me know.
</comment><comment author="NickPadilla" created="2012-12-17T04:35:23Z" id="11430326">After testing the latest release of ES, I have found that the setting provided does work as expected.  However, it looks like the property is 

config.ignore_system_properties

Thanks a million guys! 
Nick
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to replace standard actions via Java plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2311</link><project id="" key="" /><description>The plugin module is loaded before the actions module and therefore plugins are loaded before registering the default actions. If a custom plugin registers a custom action in place of the default ones , the custom action is removed when the action module is loaded.

For more details see:

https://groups.google.com/d/topic/elasticsearch/OFNkDUcnJGA/discussion
</description><key id="7454473">2311</key><summary>Unable to replace standard actions via Java plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andreasch</reporter><labels /><created>2012-10-09T16:38:37Z</created><updated>2012-10-24T17:09:03Z</updated><resolved>2012-10-20T21:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wires" created="2012-10-15T18:33:40Z" id="9456224">This can probably be fixed by adding a check to `ActionModule.registerAction(...)` in the ES source

``` java
        // don't override already existing actions
        if(actions.containsKey(action.name()))
            return;
```

but I wonder if this is actually what you want;  thought I needed this, but then I found the `org.elasticsearch.index.indexing.IndexingOperationListener` that allows you to hook into indexing operations.
</comment><comment author="andreasch" created="2012-10-18T05:58:57Z" id="9554064">That was my solution. I introduced a property for allowing/disallowing standard action overrides. If overriding is enabled, the action is added only if it does not already exist. The IndexingOperationListener would get me most of what I needed but I also needed to override another action (PutMappingAction) and there is no listener for that one.
</comment><comment author="kimchy" created="2012-10-20T21:04:36Z" id="9635992">The idea is not to allow one to override the default set of actions in elasticsearch, this is not what the extension aspect was meant to do.
</comment><comment author="andreasch" created="2012-10-20T21:14:43Z" id="9636073">Is there any other way to intercept a request and change it or execute something else before executing the default action? 
</comment><comment author="wires" created="2012-10-20T22:09:24Z" id="9637375">I've intercepted the calls at shard level using a `IndexingOperationListener`, see [this](https://github.com/0x01/degraphmalizer/blob/030be1c7ae35759f1a6e4bbfbde0ea2684b06b37/src/main/java/org/elasticsearch/plugin/degraphmalizer/DegraphmalizerListener.java) commit. I'm not sure if the document can be modified here, but it seems so, Kimchy?

Furthermore, J&#246;rg commented on the [forum](https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/fsTVZg3Hyr8)

&gt; Thanks for sharing this. Afaik the indexing operation listener's job is to catch write/update events inside of a Lucene index within an ES shard (which is not the scope of a whole index, only a part of it), just as the translog works. Pretty much low-level stuff. I'm not sure if shard level operations are the best place for adding denormalized data to documents, I would prefer the node client level, where you have access to the field mapping and the cluster state.

So maybe he misunderstood me, because I don't want to add the attributes there, I just want to drop some attributes. I am interested in catching every document going into ES and then later, asynchronously, update the document using a regular get/put (and use MVCC to make sure it stays consistent).

But I would certainly be interested how to achieve what J&#246;rg is talking about, having access to the field mappings, but still catch all documents (in particular ones that are pushed using rivers). Any hints?

thnx,
Jelle
</comment><comment author="kimchy" created="2012-10-21T19:23:34Z" id="9646426">I am not a fan of extensions into elasticsearch that munges the data, or change "default" behavior outside of the features that are provided. You don't extend MySQL to drop some attributes, you do that on your application layer.
</comment><comment author="wires" created="2012-10-22T11:57:26Z" id="9661014">I can certainly follow that reasoning, but unlike DBMS, in ES there are plugins like rivers which allow elasticsearch to "autonomously" fill itself (which is great!).

Yet is is not possible to place a function in-between a river and the indexing. Such processing is thus moved _into_ the river: the couch river, for instance, has script filters. But, at least from my perspective, it makes sense to use this by other rivers as well. Can this not be generalised into "document modification" plugins that sit just before the indexing?
</comment><comment author="kimchy" created="2012-10-22T23:27:31Z" id="9684692">@0x01 to be honest, if you end up needing to have additional functionality that the rivers does not produce, personally, I would write it outside of ES and just do what the river does. For example, with couch, its pretty simple to listen on the changes feed writing it in your favorite language, and munging the data however you want before indexing it to ES. I personally find this type of solution more manageable, then having to write a plugin "into" elasticsearch.
</comment><comment author="wires" created="2012-10-24T17:09:03Z" id="9747636">I see.

Well, we get our data from various sources:
- couchdb, activemq, rss
- things pushing to solr (using solr-mock plugin)
- things pushing to ES api

For us the rivers and the mock-solr plugin provide all the functionality we need, in terms of fetching/receiving the documents.

But what I'm looking for is a central place to process these documents.

What we've already build comes down to two things:
1. extract graph information from incoming documents, preferably dropping that information before it hits indexing
2. define special "denormalized" fields, which are (asynchronously) added to the document, based on a traversal of the graph.

All configuration is pushed by the user, in a similar way as River configuration is send to /_river/, so there is little magic going on inside the plugin (it's all in de configuration).

As an alternative we could use something like the proposed changes feed (https://github.com/elasticsearch/elasticsearch/issues/1242) but then we cannot modify the documents. (We want to modify the documents to remove some fields that will be stored in the graph).

So at least for our situation it makes a lot of sense to run this "inside" ES, as we get a lot of functionality, essentially for free.

Some others have also expressed interest in such a plugin (mrvisser, jprante), so maybe we should move this discussion to the mailing list?

bye,
Jelle.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move all generated files during tests to maven target dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2310</link><project id="" key="" /><description>Hi there,

By now when launching tests, some dir are created under `/data` or `/work` dirs.
IMHO, it's best to use `/target` dir to hold test dirs.

So, here a proposal for change:
- all tests creates data files under `/target/es/data` and work files under `/target/es/work`
- `src/test/resources/es-test.properties` is created and contain `path.data` and `path.work` settings
- `pom.xml` is modified to parse test resources and modify `${project.build.directory}` to maven target dir
- some TestNG tests are modified to use theses settings
- some _main()_ tests (stress tests) are modified to use theses settings

HTH
David.
</description><key id="7419042">2310</key><summary>Move all generated files during tests to maven target dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-10-08T12:46:38Z</created><updated>2014-07-16T21:54:37Z</updated><resolved>2013-02-25T16:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-10-18T17:24:11Z" id="9572963">+1

I think it can be easily done. Maven's pom.xml can be extended by the clean plugin. So, for example, repeated executions of `mvn clean test` get a fresh setup each iteration. 

```
           &lt;plugin&gt;
                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt;
                &lt;version&gt;2.5&lt;/version&gt;
                &lt;configuration&gt;
                    &lt;filesets&gt;
                        &lt;fileset&gt;
                            &lt;directory&gt;work&lt;/directory&gt;
                        &lt;/fileset&gt;
                        &lt;fileset&gt;
                            &lt;directory&gt;data&lt;/directory&gt;
                        &lt;/fileset&gt;
                    &lt;/filesets&gt;
                &lt;/configuration&gt;
            &lt;/plugin&gt;
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-index document GET</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2309</link><project id="" key="" /><description>As described in this post https://groups.google.com/d/msg/elasticsearch/lhoG0gE9Kvo/ez3XXIlsAx0J there is one bit missing from the index-per-user model which gets in the way of making things transparent to the application.

That 'bit' is that, given a document type and ID, you have to know which index the document lives in in order to be able to GET it.

So when you have documents sitting in your general all-users index and you add a user-specific index, you can't easily know which index to talk to in order to retrieve a document.

What about some mechanism for doing a GET from a list of indices? In other words, try index 1, if the document doesn't exist, try index 2, etc

Perhaps it is a `read_priority` setting in an alias? Something like:

```
curl -XPOST 'http://127.0.0.1:9200/_aliases?pretty=1'  -d '
{
   "actions" : [
      {
         "add" : {
            "read_priority" : 1,
            "index" : "foo_v1",
            "default" : 1,
            "alias" : "foo"
         }
      },
      {
         "add" : {
            "read_priority" : 2,
            "index" : "all_users",
            "filter" : {
               "term" : {
                  "user_id" : "foo"
               }
            },
            "routing" : "foo",
            "alias" : "foo"
         }
      }
   ]
}
'
```

I've also added a `default` param which indicates that, when creating new documents, the index `foo_v1` should be used (as opposed to just failing with "can't write to multiple indices")
</description><key id="7415454">2309</key><summary>Multi-index document GET</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-10-08T09:37:11Z</created><updated>2014-05-20T09:34:04Z</updated><resolved>2014-05-20T09:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-20T09:34:04Z" id="43604813">Just search on the document _id 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve threaklocal to avoid a probable memory leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2308</link><project id="" key="" /><description>when I run elasticsearch in tomcat7 using embedded mode,tomcat print lots of warning log
please clear threadlocal as possible as eleasticsearch can do 

&#21313;&#26376; 08, 2012 9:58:59 &#19978;&#21320; org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [] created a ThreadLocal with key of type [org.elasticsearch.index.mapper.core.NumberFieldMapper$1](value [org.elasticsearch.index.mapper.core.NumberFieldMapper$1@61b6dd]) and a value of type [org.apache.lucene.analysis.NumericTokenStream](value [%28numeric,valSize=64,precisionStep=4%29]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
&#21313;&#26376; 08, 2012 9:58:59 &#19978;&#21320; org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [] created a ThreadLocal with key of type [org.elasticsearch.index.mapper.DocumentMapper$1](value [org.elasticsearch.index.mapper.DocumentMapper$1@a15735]) and a value of type [org.elasticsearch.index.mapper.ParseContext](value [org.elasticsearch.index.mapper.ParseContext@df48c4]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
&#21313;&#26376; 08, 2012 9:58:59 &#19978;&#21320; org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [] created a ThreadLocal with key of type [org.elasticsearch.index.mapper.core.NumberFieldMapper$1](value [org.elasticsearch.index.mapper.core.NumberFieldMapper$1@1ecbf74]) and a value of type [org.apache.lucene.analysis.NumericTokenStream](value [%28numeric,valSize=64,precisionStep=4%29]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
&#21313;&#26376; 08, 2012 9:58:59 &#19978;&#21320; org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [] created a ThreadLocal with key of type [org.elasticsearch.index.mapper.core.NumberFieldMapper$1](value [org.elasticsearch.index.mapper.core.NumberFieldMapper$1@1bb2525]) and a value of type [org.apache.lucene.analysis.NumericTokenStream](value [%28numeric,valSize=64,precisionStep=4%29]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
&#21313;&#26376; 08, 2012 9:58:59 &#19978;&#21320; org.apache.catalina.loader.WebappClassLoader checkThreadLocalMapForLeaks
SEVERE: The web application [] created a ThreadLocal with key of type [org.elasticsearch.index.mapper.core.NumberFieldMapper$1](value [org.elasticsearch.index.mapper.core.NumberFieldMapper$1@d89df]) and a value of type [org.apache.lucene.analysis.NumericTokenStream](value [%28numeric,valSize=64,precisionStep=4%29]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
</description><key id="7411142">2308</key><summary>improve threaklocal to avoid a probable memory leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">quaff</reporter><labels><label>discuss</label></labels><created>2012-10-08T03:14:25Z</created><updated>2014-07-10T11:42:13Z</updated><resolved>2014-07-10T11:42:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T09:40:46Z" id="27375645">do you still see this with elasticsearch 0.90? If so, do you also see this leak with the above classes?
</comment><comment author="quaff" created="2013-10-31T00:15:38Z" id="27451520">org.elasticsearch.index.mapper.core.StringFieldMapper$1
org.elasticsearch.index.mapper.core.StringFieldMapper.StringTokenStream

org.elasticsearch.index.mapper.internal.UidFieldMapper$1
org.elasticsearch.common.lucene.uid.UidField

org.elasticsearch.index.mapper.core.NumberFieldMapper$2
org.apache.lucene.analysis.NumericTokenStream

org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom$1
org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom

org.elasticsearch.common.util.concurrent.jsr166e.Striped64.ThreadHashCode
org.elasticsearch.common.util.concurrent.jsr166e.Striped64.HashCode
</comment><comment author="clintongormley" created="2014-07-08T17:13:11Z" id="48370037">Is this related to #283?
</comment><comment author="quaff" created="2014-07-10T01:09:45Z" id="48554699">yes

2014-07-09 1:13 GMT+08:00 Clinton Gormley notifications@github.com:

&gt; Is this related to #283
&gt; https://github.com/elasticsearch/elasticsearch/issues/283?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/2308#issuecomment-48370037
&gt; .
</comment><comment author="clintongormley" created="2014-07-10T11:42:13Z" id="48594047">Closing in favour of #283 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a toString() method to MultiSearchResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2307</link><project id="" key="" /><description>Heya,

For debugging purpose, I would like to add a toString() method to the MultiSearchResponse class.

Thanks
David.
</description><key id="7395409">2307</key><summary>Add a toString() method to MultiSearchResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-10-06T07:53:11Z</created><updated>2014-07-16T21:54:38Z</updated><resolved>2012-11-19T12:51:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-19T12:51:47Z" id="10512321">Pushed to master and 0.20, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms filter: Add `or` and `or_nocache` execution modes </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2306</link><project id="" key="" /><description /><key id="7382869">2306</key><summary>Terms filter: Add `or` and `or_nocache` execution modes </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-10-05T16:21:06Z</created><updated>2012-10-05T16:22:08Z</updated><resolved>2012-10-05T16:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>exposing some more Lucene analysis, mostly light and minimal stemmers, and common gram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2305</link><project id="" key="" /><description /><key id="7371560">2305</key><summary>exposing some more Lucene analysis, mostly light and minimal stemmers, and common gram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-10-05T07:26:48Z</created><updated>2014-07-04T10:39:55Z</updated><resolved>2014-07-04T10:39:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-05T14:29:31Z" id="9177319">The `StemmerTokenFilterFactory` has the stemmers in place, the thought process was not to have specific ones, but simply allow to configure a stemmer with the relevant stemming. Did you miss that, or do you see value in having those built in ones as top level filters?
</comment><comment author="jprante" created="2012-10-05T14:51:12Z" id="9178011">Doh. I know I must have overlooked something... sorry.

Are there plans to expose the normalizer token filters (such as org.apache.lucene.analysis.de.GermanNormalizationFilter) or the new common gram filter in org.apache.lucene.analysis.commongrams  (which is in Lucene 4)?  I might open separate issues if it is convenient.
</comment><comment author="rmuir" created="2014-07-04T10:39:55Z" id="48029923">I reviewed the commit, sorry I didn't notice this one earlier.

All the analyzers here are now exposed (some were missing: they are fixed in #5935). 

Additionally we now have a test in elasticsearch to alert us when lucene adds new ones so we can think about what to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get _mapping on a empty index with no mappings passed when created returns 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2304</link><project id="" key="" /><description>Given an empty index with no mappings passed when created returns 404 when requesting _mapping. Should the "empty" mapping be considered "empty" or "non existant"?

PUT localhost:9200/myindex

GET localhost:9200/myindex/_mapping 
=&gt; returns the empty mapping:
{
    "myindex": {}
}
with status 404.

Should this be considered as a bug or by design? 

This pull request resolves the issue by returning 200 if the index has no mappings and no types passed to the _mapping-call. 
</description><key id="7353893">2304</key><summary>Get _mapping on a empty index with no mappings passed when created returns 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lindstromhenrik</reporter><labels /><created>2012-10-04T15:22:23Z</created><updated>2014-06-18T17:10:53Z</updated><resolved>2012-10-04T18:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-04T18:08:19Z" id="9151146">Makes a lot of sense, pushed to master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch throws Index out of bounds Exception every second.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2303</link><project id="" key="" /><description>Version 19.8:

[2012-10-04 15:38:38,555][WARN ][index.merge.scheduler    ] [Raye, Frankie] [senti_index][10] failed to merge
java.lang.IndexOutOfBoundsException: Index: 105, Size: 22
        at java.util.ArrayList.rangeCheck(ArrayList.java:604)
        at java.util.ArrayList.get(ArrayList.java:382)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:255)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:244)
        at org.apache.lucene.index.TermBuffer.read(TermBuffer.java:86)
        at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:133)
        at org.apache.lucene.index.SegmentMergeInfo.next(SegmentMergeInfo.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:501)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:428)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:108)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4256)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3901)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:388)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:456)
[2012-10-04 15:38:39,560][WARN ][index.merge.scheduler    ] [Raye, Frankie] [senti_index][10] failed to merge
java.lang.IndexOutOfBoundsException: Index: 105, Size: 22
        at java.util.ArrayList.rangeCheck(ArrayList.java:604)
        at java.util.ArrayList.get(ArrayList.java:382)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:255)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:244)
        at org.apache.lucene.index.TermBuffer.read(TermBuffer.java:86)
        at org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:133)
        at org.apache.lucene.index.SegmentMergeInfo.next(SegmentMergeInfo.java:72)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:501)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:428)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:108)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4256)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3901)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:388)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:456)

...then dies after short time without any warning.
</description><key id="7350823">2303</key><summary>ElasticSearch throws Index out of bounds Exception every second.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rzulf</reporter><labels /><created>2012-10-04T13:41:36Z</created><updated>2013-05-23T14:53:41Z</updated><resolved>2013-05-23T14:53:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SakulK" created="2012-10-04T14:54:44Z" id="9144121">I have the same problem
</comment><comment author="kimchy" created="2012-10-04T16:20:12Z" id="9147380">Which version are you using?
</comment><comment author="Rzulf" created="2012-10-04T17:28:57Z" id="9149762">Version 0.19.8
</comment><comment author="kimchy" created="2012-10-04T18:01:02Z" id="9150885">Can you also check the logs of the nodes and see if there was any error happening before you started to get this failures?
</comment><comment author="Rzulf" created="2012-10-04T19:23:24Z" id="9153777">I have noticed something like this in logs. In general ES works, but throws loads of Exceptions into log and is unstable, about few hours of uptime, then dies without any error.

[2012-09-18 16:30:18,316][WARN ][index.shard.service      ] [Cethlann] [senti_index][10] Failed to perform scheduled engine refresh
org.elasticsearch.index.engine.RefreshFailedEngineException: [senti_index][10] Refresh failed
        at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:779)
        at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:440)
        at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:765)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.lucene.index.CorruptIndexException: docs out of order (9 &lt;= 9 ) (out: org.apache.lucene.store.FSDirectory$FSIndexOutput@7d851e56)
        at org.apache.lucene.index.FormatPostingsDocsWriter.addDoc(FormatPostingsDocsWriter.java:84)
        at org.apache.lucene.index.FreqProxTermsWriter.appendPostings(FreqProxTermsWriter.java:232)
        at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:120)
        at org.apache.lucene.index.TermsHash.flush(TermsHash.java:113)
        at org.apache.lucene.index.DocInverter.flush(DocInverter.java:70)
        at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:60)
        at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:581)
        at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3580)
        at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3545)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:450)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:399)
        at org.apache.lucene.index.DirectoryReader.doOpenFromWriter(DirectoryReader.java:413)
        at org.apache.lucene.index.DirectoryReader.doOpenIfChanged(DirectoryReader.java:432)
        at org.apache.lucene.index.DirectoryReader.doOpenIfChanged(DirectoryReader.java:375)
        at org.apache.lucene.index.IndexReader.openIfChanged(IndexReader.java:508)
        at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:109)
        at org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:57)
        at org.apache.lucene.search.ReferenceManager.maybeRefresh(ReferenceManager.java:137)
        at org.elasticsearch.index.engine.robin.RobinEngine.refresh(RobinEngine.java:760)
</comment><comment author="kimchy" created="2012-10-04T19:37:08Z" id="9154197">@Rzulf anything interesting before this log? I am looking for possibly an OutOfMemory failure, or running out of file descriptors. Also, are you using the (default) local gateway, or a shared gateway (like s3)?
</comment><comment author="Rzulf" created="2012-10-05T13:36:35Z" id="9175461">I am using local gateway. Hosting serwer in general was pretty unstable lately. In ES logs there were "too many open files" as well as "out of memory". 
</comment><comment author="kimchy" created="2012-10-05T14:31:57Z" id="9177401">@Rzulf can you make sure that you have enough memory and you set properly the max open file descs? You can check the process max files desc config using the nodes info API (with the process flag).

In general, you shouldn't get into this situation of having problems with the index if this failure happens. Can you gist the errors for the OOM ones and the "too many open files" ones? But if you get those, it means that your server is not properly configured and it will be unstable until you resolve it.
</comment><comment author="spinscale" created="2013-05-23T14:53:41Z" id="18348469">Closing this issue for now. If you encounter these problems without running into OOM or 'too many open files' again, please reopen this issue and attach relevent log files. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete by query hangs when performed on an alias with filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2302</link><project id="" key="" /><description /><key id="7335686">2302</key><summary>Delete by query hangs when performed on an alias with filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.11</label></labels><created>2012-10-03T21:46:43Z</created><updated>2013-08-14T21:43:05Z</updated><resolved>2012-10-20T22:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Official nodejs client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2301</link><project id="" key="" /><description>Is there an official module for nodejs - or one that you recommend? Using it with curl seems to be a bit raw.
</description><key id="7323904">2301</key><summary>Official nodejs client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavpursche</reporter><labels /><created>2012-10-03T14:47:04Z</created><updated>2012-10-04T19:00:29Z</updated><resolved>2012-10-03T17:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2012-10-03T17:24:49Z" id="9114581">Ram Viswanadha announced this week on the mailing list a Node.js client:

https://github.com/ramv/node-elastical
</comment><comment author="gustavpursche" created="2012-10-03T17:26:32Z" id="9114660">Thanks a lot!
</comment><comment author="gustavpursche" created="2012-10-04T19:00:29Z" id="9152989">Again, thank you. The module works great and your searchengine is incredibly fast.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug passing params to custom_filters_score script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2300</link><project id="" key="" /><description>Trying to pass `params` to a script in a `custom_filters_score` query fails:

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "query" : {
      "custom_filters_score" : {
         "query" : {
            "match_all" : {}
         },
         "filters" : [
            {
               "params" : {
                  "foo" : 1
               },
               "script" : "foo",
               "filter" : {
                  "match_all" : {}
               }
            }
         ]
      }
   }
}
'
```

with this error:

```
 QueryParsingException[[test] [custom_filters_score] missing 'filter' in filters array element]
```

The same query but without parameters works:

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "query" : {
      "custom_filters_score" : {
         "query" : {
            "match_all" : {}
         },
         "filters" : [
            {
               "script" : "1",
               "filter" : {
                  "match_all" : {}
               }
            }
         ]
      }
   }
}
'
```
</description><key id="7297651">2300</key><summary>Bug passing params to custom_filters_score script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2012-10-02T16:14:23Z</created><updated>2014-02-21T15:39:39Z</updated><resolved>2014-02-21T15:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-03T16:22:02Z" id="9112316">The `params` is used for all the scripts, and is provided on the same level as `query` and `filters`. Do you need it per filter as well?
</comment><comment author="clintongormley" created="2012-10-16T17:27:08Z" id="9498870">I think it makes sense to do so. 
</comment><comment author="kimchy" created="2012-10-17T03:28:28Z" id="9514896">Yea, the idea is to not having to repeat it for each script, since the typical use case is to use similar scripts with same parameters based on different filters.
</comment><comment author="clintongormley" created="2012-10-17T09:33:14Z" id="9521244">But we can support it at both levels?

Everywhere else, the params are passed at the same level as the script,
so not supporting it at the script level is confusing.  Also it is quite
likely that there are use cases where different scripts with different
params are used.

For instance I might have two scripts with mode: total
1) geo-distance
2) recency
</comment><comment author="spinscale" created="2014-02-21T15:39:39Z" id="35741466">closing in favor of function score
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to netty 3.5.8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2299</link><project id="" key="" /><description /><key id="7241778">2299</key><summary>Upgrade to netty 3.5.8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-29T23:24:25Z</created><updated>2012-09-29T23:24:51Z</updated><resolved>2012-09-29T23:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Synonyms not working with terms of varying # of words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2298</link><project id="" key="" /><description>Not sure if there is a bug or if something is misconfigured, but I have an index setup like so:

```
{
  "dev-events" : {
    "settings" : {
      "index.number_of_replicas" : "1",
      "index.version.created" : "190999",
      "index.number_of_shards" : "5",

      "index.analysis.filter.sb_synonym.ignore_case" : "true",
      "index.analysis.filter.sb_synonym.type" : "synonym",
      "index.analysis.filter.sb_synonym.synonyms_path" : "synonyms.txt",

      "index.analysis.analyzer.sb_analyzer.type" : "custom",
      "index.analysis.analyzer.sb_analyzer.tokenizer" : "standard",

      "index.analysis.analyzer.sb_analyzer.filter.0" : "sb_stop",
      "index.analysis.analyzer.sb_analyzer.filter.1" : "sb_synonym",
      "index.analysis.analyzer.sb_analyzer.filter.2" : "standard",
      "index.analysis.analyzer.sb_analyzer.filter.3" : "stop",
      "index.analysis.analyzer.sb_analyzer.filter.4" : "snowball",
      "index.analysis.analyzer.sb_analyzer.filter.5" : "lowercase",

      "index.analysis.analyzer.sb_keyword.type" : "custom",
      "index.analysis.analyzer.sb_keyword.filter.0" : "lowercase",
      "index.analysis.analyzer.sb_keyword.tokenizer" : "keyword",

      "index.analysis.filter.sb_stop.ignore_case" : "true",
      "index.analysis.filter.sb_stop.enable_position_increments" : "true",
      "index.analysis.filter.sb_stop.type" : "stop"
      "index.analysis.filter.sb_stop.stopwords_path" : "stopwords.txt",
    }
  }
}
```

The important part is the synonym stuff, but I've included everything just in case. There's a synonyms file with just this line:

```
test =&gt; Some Thing Here, Another Item
```

And a query like so:

```
{
  "fields" : [ "name" ],
  "query" : {
    "query_string" : 
    {
      "query" : "test",
      "analyzer" : "sb_analyzer",
      "fields" : [ "name" ],
      "auto_generate_phrase_queries" : true
    }
  },
  "sort" : [
    {
      "localDateTime" : { "order" : "asc" }
    }
  ],
}
```

The problem I'm having is that because the replacements have a different number of words (3 and 2, respectively), it will only return the items that match the term with the most words, "Some Thing Here" in this case, but if I have a synonym like this:

```
test =&gt; Some Thing Here, Another Item There
```

It will work as I expect, returning items that the terms "Some Thing Here", or "Another Item There". I'm not sure if I'm missing something or if there's some limitation/bug in ElasticSearch regarding synonyms with differing # of words in their replacements.
</description><key id="7233602">2298</key><summary>Synonyms not working with terms of varying # of words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikeyoon</reporter><labels /><created>2012-09-29T00:38:15Z</created><updated>2014-08-15T01:34:40Z</updated><resolved>2014-07-08T17:11:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-09-29T10:10:00Z" id="9002619">Multi-word synonyms can mess around with the token positions in your
strings, which can give poor results combined with phrase queries.
However, with the information that you provided, my test queries still work.

Please can you gist a full curl recreation so that we can see exactly what
you are doing.

Clint
</comment><comment author="mikeyoon" created="2012-10-02T19:19:16Z" id="9083268">Here's a gist of a full reproduction. It includes an analyze and 3 queries, 1 for the synonym, and 1 each for the two terms it should be looking for.

https://gist.github.com/3822664
</comment><comment author="clintongormley" created="2014-07-08T17:11:00Z" id="48369748">Hi @mikeyoon 

Sorry it's taken "a while" to look at this again :)  Essentially, phrase queries with synonyms of differing length are problematic, and there's nothing to do about it currently.  Have a read of this blog about why it is problematic: http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html

If you want synonyms of mixed length, then you need to do it at index time and replace them with the shorter alias, eg:

```
"New England Patriots,Washington Nationals =&gt; foo"
```

That would replace either of those sequences with `foo`, which could then be used in a phrase query.  Of course, you wouldn't be able to search for just `New England` then.  Messy but all we have right now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child / has_parent filters can throw NPE when either child or parent documents aren't indexed yet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2297</link><project id="" key="" /><description /><key id="7215332">2297</key><summary>has_child / has_parent filters can throw NPE when either child or parent documents aren't indexed yet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-28T14:24:15Z</created><updated>2012-09-28T15:09:19Z</updated><resolved>2012-09-28T15:09:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>query_string on multiple fields with "*" fails in 0.19.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2296</link><project id="" key="" /><description>Sample:

```
{
  "from": 0,
  "size": 50,
  "query": {
    "bool": {
      "must": [
        {
          "query_string": {
            "query": "*",
            "fields": [
              "description",
              "synonyms"
            ]
          }
        }
      ]
    }
  }
}
```
</description><key id="7211914">2296</key><summary>query_string on multiple fields with "*" fails in 0.19.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-28T12:00:13Z</created><updated>2012-09-28T12:00:50Z</updated><resolved>2012-09-28T12:00:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>tiny error explain api documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2295</link><project id="" key="" /><description>There's a tiny error regarding the explain api:
http://www.elasticsearch.org/guide/reference/api/explain.html

instead of 
curl -XGET http://localhost:9200/twitter/tweet/1/_explain -d '{
        "term" : { "message" : "search" }
}'

it should say
curl -XGET http://localhost:9200/twitter/tweet/1/_explain -d '{
   "query {
        "term" : { "message" : "search" }
   }
}'

cheers,
chris
</description><key id="7208369">2295</key><summary>tiny error explain api documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">achselschweisz</reporter><labels /><created>2012-09-28T09:18:39Z</created><updated>2012-09-28T10:40:06Z</updated><resolved>2012-09-28T10:40:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-09-28T10:40:06Z" id="8970402">Thanks. Fixed it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>threadedOperation has private access in ShardReplicationOperationRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2294</link><project id="" key="" /><description>To get trunk compiled, I had to let threadedOperation be protected instead of private in ShardReplicationOperationRequest.

$ java -version
java version "1.7.0_07"
OpenJDK Runtime Environment (IcedTea7 2.3.2) (7u7-2.3.2a-0ubuntu0.12.04.1)
OpenJDK 64-Bit Server VM (build 23.2-b09, mixed mode)
</description><key id="7207477">2294</key><summary>threadedOperation has private access in ShardReplicationOperationRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">decitre</reporter><labels /><created>2012-09-28T08:30:49Z</created><updated>2012-10-01T12:11:31Z</updated><resolved>2012-10-01T12:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-10-01T12:10:49Z" id="9029200">Seems to be failing with 1.7 compiler, but passes with 1.6. Will fix it...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not possible to execute scripts on fields which name contain @ through _update api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2293</link><project id="" key="" /><description>I'm running version 0.19.9

curl -XGET 'http://xxx:9200/_cluster/health?pretty=true'
{
  "cluster_name" : "elasticsearch",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 85,
  "active_shards" : 85,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 85
}

This works:
curl -XPUT xxx:9200/test/test/1 -d '{"counter" : 1, "tags" : ["fubar1"]}'
curl -XPOST 'xxx:9200/test/test/1/_update' -d '{"script" : "ctx._source.tags += tag","params": {"tag":"fubar2"}}'

This does not work:
curl -XPUT xxx:9200/test/test/2 -d '{"counter" : 1, "@tags" : ["fubar1"]}'
curl -XPOST 'xxx:9200/test/test/2/_update' -d '{"script" : "ctx._source.@tags += tag","params": {"tag":"fubar2"}}'

{"error":"ElasticSearchIllegalArgumentException[failed to execute script]; nested: ArrayIndexOutOfBoundsException[0]; ","status":400}

Posted in mailing list:
https://groups.google.com/d/topic/elasticsearch/SZgbpMj00lk/discussion
</description><key id="7178224">2293</key><summary>Not possible to execute scripts on fields which name contain @ through _update api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smetj</reporter><labels /><created>2012-09-27T10:10:05Z</created><updated>2013-04-05T18:17:39Z</updated><resolved>2013-04-05T18:17:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-04-05T18:17:39Z" id="15971958">Duplicate of #2852 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trove licensing issue?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2292</link><project id="" key="" /><description>Hi,

I would just like to raise one question: is the LGPL 2.1 compatible with Apache?

[Trove links to LGPL 2.1](http://trove.starlight-systems.com/license) now see [this point](http://www.apache.org/legal/3party.html#transition-note):
    _"The LGPL v2.1 is ineligible from being a Category B license (a category that includes the MPL, CPL, EPL, and CDDL) primarily due to the restrictions it places on larger works, violating the third license criterion. Therefore, LGPL v2.1-licensed works must not be included in Apache products, although they may be listed as system requirements or distributed elsewhere as optional works."_

At least the different license should be mentioned in the Notice.txt to be clear.

I also read that Rob Eden (maintainer+author of trove) cannot change the license - [see here](https://issues.apache.org/jira/browse/MAHOUT-121?focusedCommentId=12895284&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12895284)

Should the mahout collections used instead?

The same applies to [JTS](http://www.vividsolutions.com/jts/jtshome.htm) and [JNA](https://github.com/twall/jna/blob/master/LICENSE)
</description><key id="7174619">2292</key><summary>Trove licensing issue?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2012-09-27T06:50:46Z</created><updated>2012-09-28T10:52:53Z</updated><resolved>2012-09-28T10:52:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-27T14:13:01Z" id="8937375">I don't see it a licensing issue? Yea, we should add the fact that we use it to the notice (its missing there), but you can use LGPL in Apache Licensed projects. JNA and JTS are optional and elasticsearch can run without them.
</comment><comment author="karussell" created="2012-09-27T15:07:41Z" id="8939212">Ok, sorry. I've probably not fully understand the part on the apache site: "due to the restrictions it places on larger works, violating the third license criterion." 

Do you understand it :) ?
</comment><comment author="kimchy" created="2012-09-28T09:30:58Z" id="8969022">@karussell thats for the Apache Foundation, not the Apache license.
</comment><comment author="karussell" created="2012-09-28T10:52:53Z" id="8970621">Ok, I thought the 'larger works' a bit confusing as I didn't found it in the original LGPL-text. So, I thought there might be a possibility of an issue if one creates 'a larger work' from or up on LGPL-projects ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix some ClassCastExceptions from MVEL scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2291</link><project id="" key="" /><description>Fix some class-cast issues while interacting with MVEL. Related to discussion at https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/prHbqX20PtM
</description><key id="7172862">2291</key><summary>Fix some ClassCastExceptions from MVEL scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kjiwa</reporter><labels><label>discuss</label></labels><created>2012-09-27T04:32:32Z</created><updated>2014-07-25T08:25:28Z</updated><resolved>2014-07-25T08:25:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-27T14:14:36Z" id="8937424">This fix wil cause considerable more memory overhead. We need to figure out why mvel if failing here, can you try your script with javascript?
</comment><comment author="kjiwa" created="2012-09-27T16:10:10Z" id="8941706">MVEL is failing because it tries to cast the long[] into an Object[] in ArrayAccessorNest.getValue() (see http://svn.codehaus.org/mvel/tags/2.0.12-SNAPSHOT-070609/src/main/java/org/mvel2/optimizers/impl/refl/nodes/ArrayAccessorNest.java as an example). I determined this by turning off the shade plugin and setting a breakpoint for ClassCastExceptions.

I'm not too familiar with Javascript scripts in elasticsearch but I'll give it a try. If you have an example to start with that'll be great.
</comment><comment author="kjiwa" created="2012-09-27T18:25:38Z" id="8948050">Just to clarify what is happening...

The LongDocFieldData object is dereferenced into a long[] via LongDocFieldData.getValues(...).

The long[] gets passed along through to ArrayAccessorNest.getValue(Object ctx, Object elCtx, VariableResolverFactory vars) as the "ctx" variable. This is the first cast (long[] --&gt; Object).

Within ArrayAccessorNest.getValue(...), ctx is further cast into an Object[] and this line is what causes the ClassCastException.

This simple example illustrates the path:

```
public class LongArrayToObjectArray {
  public static void main(String[] args) {
    Object[] O = (Object[]) (Object) new long[0];
  }
}
```

Running this program will result in the same ClassCastException behaviour.
</comment><comment author="clintongormley" created="2014-07-25T08:25:28Z" id="50121302">MVEL is now deprecated and will be removed in 1.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ICU collation factory should use ULocale, not Locale</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2290</link><project id="" key="" /><description>Basically if you use Locale, the parsing will be limited to what happens to be available on the system JRE.  This is a subset of whats available in ICU, and system-dependent (e.g. certain JRE downloads on windows don't include chinese/japanese for space reasons).

So the parsing for ICU collator should be done with ULocale... a one character fix to attack the bug :)

See https://groups.google.com/group/elasticsearch/msg/1b1c08f147a708da? for an example of a user that ran into this.
</description><key id="7153898">2290</key><summary>ICU collation factory should use ULocale, not Locale</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2012-09-26T15:51:46Z</created><updated>2012-09-27T08:39:39Z</updated><resolved>2012-09-27T08:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-09-26T15:52:34Z" id="8894772">Good catch robert, thanks for raising this! 
</comment><comment author="martijnvg" created="2012-09-27T08:39:39Z" id="8929657">Thanks Robert! 
Changes pushed and released a new version of the elasticsearch-analysis-icu plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Configuration option to set custom ShardsAllocator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2289</link><project id="" key="" /><description>Hi,

please add a configuration option to be able to configure the shardsAllocator in org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocatorModule

Robert
</description><key id="7145009">2289</key><summary>Configuration option to set custom ShardsAllocator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snazy</reporter><labels /><created>2012-09-26T09:21:24Z</created><updated>2013-10-30T09:33:43Z</updated><resolved>2013-10-30T09:33:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="snazy" created="2012-09-26T09:21:55Z" id="8883868">For 0.19.10 (if possible), too :-)
</comment><comment author="imotov" created="2012-09-26T11:47:44Z" id="8887135">I think shardsAllocator is configurable using setShardsAllocator method since 0.19.0. All you have to do is implement PreProcessModule and replace default shards allocator with your own shards allocator when ShardsAllocatorModule is getting processed. It may look something like this:

``` java
public class MyShardsAllocatorModule extends AbstractModule implements PreProcessModule {

    private final Settings settings;

    public MyShardsAllocatorModule(Settings settings) {
        this.settings = settings;
    }

    @Override
    protected void configure() {
        // Configure your module
    }

    @Override
    public void processModule(Module module) {
        if (module instanceof ShardsAllocatorModule &amp;&amp; 
                settings.getAsBoolean("my.shards.allocator.enabled", false)) {
            ((ShardsAllocatorModule) module).setShardsAllocator(MyShardsAllocator.class);
        }
    }
}
```
</comment><comment author="snazy" created="2012-09-26T15:39:40Z" id="8894321">Hi - yes - seems to work (at least the coding) :-)
Thanks!
I'll try my stuff tomorrow as a module.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field data caches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2288</link><project id="" key="" /><description>The sizes of the field caches per index are exported with the node stats
</description><key id="7124380">2288</key><summary>Field data caches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreasSinz</reporter><labels /><created>2012-09-25T15:06:20Z</created><updated>2014-07-16T21:54:40Z</updated><resolved>2013-07-17T12:35:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gustavobmaia" created="2012-09-25T22:46:25Z" id="8873074">Cool.
</comment><comment author="AndreasSinz" created="2012-09-28T08:15:49Z" id="8967536">This addition prints out the field name and the size of its cache:

fieldDataCaches:
  index1 : {
      "name": 300,
      "birthday": 456,
      "gender": 150
  }
</comment><comment author="chendo" created="2012-10-10T03:49:36Z" id="9289297">+1
</comment><comment author="spinscale" created="2013-05-29T16:17:06Z" id="18628103">@AndreasSinz can we close this, as we are exporting field data stats with 0.90.0 now or are you missing something?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add constructor IndexRequest(String index, String type) and fix javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2287</link><project id="" key="" /><description>When using Bulk insertion, we need to add IndexRequest to the bulk.
But, constructors does not allow not to provide the id.

``` java
IndexRequest irq = new IndexRequest("index", "type", "id");
```

But, as we can do it for prepareIndex(), it could be nice to have a constructor without the id:

``` java
// PrepareIndex
client.prepareIndex("index", "type").setSource("{}")
  .execute().actionGet();

// Bulk
client.prepareBulk()
  .add(new IndexRequest("index", "type").source("{}"))
  .execute().actionGet();
```

Prior to this pull request, if we don't want to set the ID and let Elasticsearch generate it, we have to use one of these forms:

``` java
new IndexRequest().index("index").type("type");
new IndexRequest("index").type("type");
```

BTW, I fixed the javadoc documentation for `IndexRequest(String index)` as ID is not mandatory.
</description><key id="7118971">2287</key><summary>Add constructor IndexRequest(String index, String type) and fix javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-09-25T11:30:44Z</created><updated>2014-07-16T21:54:41Z</updated><resolved>2012-11-19T12:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-11-19T12:54:22Z" id="10512389">Pushed to maser and 0.20, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some mapping conflicts do not throw errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2286</link><project id="" key="" /><description>For instance, mapping a field as type `nested` then trying to change it to type `object` should throw a conflict error, but this change gets silently ignored.

Not sure what other mapping conflicts exist which are also being ignored.

Map field to type `nested`:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "foo" : {
               "type" : "nested",
               "properties" : {
                  "bar" : {
                     "type" : "string"
                  }
               }
            }
         }
      }
   }
}
'

# {
#    "ok" : true,
#    "acknowledged" : true
# }
```

Try to change to type `object`:

```
curl -XPUT 'http://127.0.0.1:9200/test/test/_mapping?pretty=1'  -d '
{
   "test" : {
      "properties" : {
         "foo" : {
            "type" : "object",
            "properties" : {
               "bar" : {
                  "type" : "string"
               }
            }
         }
      }
   }
}
'

# {
#    "ok" : true,
#    "acknowledged" : true
# }
```

Mapping hasn't changed, but no conflict error was thrown above:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_mapping?pretty=1' 

# {
#    "test" : {
#       "properties" : {
#          "foo" : {
#             "type" : "nested",
#             "properties" : {
#                "bar" : {
#                   "type" : "string"
#                }
#             }
#          }
#       }
#    }
# }
```
</description><key id="7101001">2286</key><summary>Some mapping conflicts do not throw errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-09-24T18:18:56Z</created><updated>2013-10-08T16:06:28Z</updated><resolved>2013-10-08T16:06:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-09T11:53:35Z" id="22389692">This looks fixed to me. Getting back an error message with the current 0.90.3

```
MergeMappingException[Merge failed with failures {[object mapping [foo] can't be changed from nested to non-nested]}]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk Delete item when broadcast to all shard (lack of routing) might not be applied correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2285</link><project id="" key="" /><description>See more info at #2282. 
</description><key id="7087975">2285</key><summary>Bulk Delete item when broadcast to all shard (lack of routing) might not be applied correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-24T11:57:56Z</created><updated>2012-09-24T12:04:17Z</updated><resolved>2012-09-24T12:04:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fail on sorting in empty result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2284</link><project id="" key="" /><description>Hi, I had used elasticsearch in my ruby on rails app through tire. It works fine in development mode, but yesterday I add sorting and my test is fail with error

Parse Failure [No mapping found for [published_at] in order to sort on]

After reading docs I found answer -  add "ignoring unmapped fields" and it works. But I really dont understand why sorting without "ignoring unmapped fields" must fail on empty results. Can you fix that?
</description><key id="7087969">2284</key><summary>Fail on sorting in empty result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jalkoby</reporter><labels /><created>2012-09-24T11:57:25Z</created><updated>2013-06-07T08:31:18Z</updated><resolved>2013-06-07T08:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T08:24:36Z" id="19094796">Hey,

can you provide samples maybe, what kind of queries you executed on what kind of data? I guess you executed a search in several indices and/or over several types, but I want to be sure before elaborating further (in that case it is complex for us to handle, and we still return an error at the moment).
</comment><comment author="jalkoby" created="2013-06-07T08:31:18Z" id="19095051">Sorry, I forgot about this issue. I've already solved it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom_filters_score with score_mode first doesn't return explanation correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2283</link><project id="" key="" /><description>If score_mode is set to "first", explanation details inludes only details of boost filter.
When other score_mode is used, details contains details of both query and boost filter(s).
</description><key id="7082209">2283</key><summary>custom_filters_score with score_mode first doesn't return explanation correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IoriH</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-24T08:23:48Z</created><updated>2012-09-24T12:08:04Z</updated><resolved>2012-09-24T12:08:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-09-24T11:17:34Z" id="8815210">Nice catch! Indeed the explain of the query isn't included when score_mode=first is used. I'll add it.
</comment><comment author="martijnvg" created="2012-09-24T12:08:03Z" id="8816600">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possible race condition when routing is required but not specified f...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2282</link><project id="" key="" /><description>...or a delete operation

This small pull request fixes a possible race condition in the TransportShardBulkAction. This race condition can be triggered by a bulk delete operation on an index with required routing when several primary shards are located on the node that executes the operation. It sounds complicated, but it can be easily reproduced by placing content of AliasRoutingTests.testRequiredRoutingMappingWithAlias or 
SimpleRoutingTests.testRequiredRoutingMapping into an infinite loop. On my machine it typically fails after only a few iterations.

When an index is marked as routing required but routing is not specified on a delete operation, TransportBulkAction broadcasts such delete operation to all shards using [the same delete request object](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java#L191).

When TransportShardBulkAction receives this request on a primary shard, it tries to delete the record and [updates the version field in the request](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java#L210). The problem occurs because the version can be updated even if the record on the shard was not actually deleted. It looks like this behavior is desired and was introduced by #1341. When several primary shards are located on the same node, the DeleteRequest object is shared among all shards and the version changes on one shard affect all shards of the index on the same node. As a result the delete operation sometimes fails with VersionConflictEngineException.

While this pull request fixes the problem, it might still make sense to not share the DeleteRequest among shards. What do you think?
</description><key id="7075407">2282</key><summary>Fix possible race condition when routing is required but not specified f...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-09-23T22:11:01Z</created><updated>2014-06-18T17:06:30Z</updated><resolved>2012-09-24T11:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-24T09:16:17Z" id="8811487">Nice catch, I think in cases of distributing the delete request, the better solution would be to copy over the delete request? Maybe have a constructor for delete request, that accepts another delete request, and use that when deciding to broadcast it?
</comment><comment author="kimchy" created="2012-09-24T11:58:24Z" id="8816393">I am pushing the fix with a copy of the delete request, see #2285.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for fetching pre-indexed GeoShapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2281</link><project id="" key="" /><description>Adds a ShapeFetchService which supports retrieving Shapes from another index.  The ShapeFetchService is injected into GeoShapeQueryParser/FilterParser which expose new syntax for referencing a pre-indexed Shape rather than having to provide the Shape coordinates.
</description><key id="7065535">2281</key><summary>Add support for fetching pre-indexed GeoShapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-09-23T03:42:34Z</created><updated>2014-07-16T21:54:41Z</updated><resolved>2012-09-23T19:32:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-23T19:32:17Z" id="8801877">Pushed, cleaned it up a bit as well: 6e66f45f58db2361540b1952b2e50d3983c3bc85.
</comment><comment author="chrismale" created="2012-09-23T23:24:01Z" id="8804156">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: Add header token allowing to more easily identify when illegal content is being sent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2280</link><project id="" key="" /><description /><key id="7054017">2280</key><summary>Transport: Add header token allowing to more easily identify when illegal content is being sent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-09-21T21:55:33Z</created><updated>2012-09-21T21:56:35Z</updated><resolved>2012-09-21T21:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Trimmed down config for low resources environments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2279</link><project id="" key="" /><description>Hello,

It'd be nice if there was an example configuration for low resources systems.

E.g, number_of_shards 1, replica 0, thread_pool all set to 1, ES_HEAP set to 128M, etc.

Also maybe some tricks like norms/freqs disabled for the fields that don't need it?
</description><key id="7044160">2279</key><summary>Trimmed down config for low resources environments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Silex</reporter><labels /><created>2012-09-21T15:40:13Z</created><updated>2013-06-30T21:36:21Z</updated><resolved>2013-06-06T15:19:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-06T15:19:40Z" id="19052488">it is kinda hard to us to define a 'low resource' configuration without knowing system and/or sacrificing lots of functionality (and that maybe unneeded). Some system might be low on ram, other low on cores (if it is high on cores there is less need to reduce the threadpool for example) or I/O performance, so we cannot really create a generic configuration for this.

Do you think, it makes sense to create additional documentation about this for the documentation repository in such a case?
</comment><comment author="Silex" created="2013-06-30T21:36:21Z" id="20255951">When you have to run ES on a tight resource server it's hard to guess from the documentation which tweaks really have an impact on resources. Fortunately for me there was IRC where people could help a lot, the situation I was facing was running ES on a server with only 512Mb of RAM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shingle Min/Max values seem reversed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2278</link><project id="" key="" /><description>It appears that commit https://github.com/elasticsearch/elasticsearch/commit/e66bd4359a5d5e03a04a4a8a6bb26f25fe107f24 has left the min_shingle_size and max_shingle_size parameters reversed.

I now need to set the "min_shingle_size" parameter to set the maximum shingle size.
</description><key id="7043573">2278</key><summary>Shingle Min/Max values seem reversed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gjstockham</reporter><labels /><created>2012-09-21T15:17:40Z</created><updated>2012-09-24T11:22:19Z</updated><resolved>2012-09-24T11:22:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-21T17:02:44Z" id="8773971">Yea, its a bug, but the mentioned commit fixes it, the fix will be part of upcoming 0.19.10 (its already in the 0.19 branch).
</comment><comment author="gjstockham" created="2012-09-24T11:22:19Z" id="8815461">Ah, thanks, will close the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Norms field is ignored when doing a match_all query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2277</link><project id="" key="" /><description>A `match_all` query should take the value stored in the norms field into account, but it ignores it, even when the `norms_field` is specified in the query:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_boost" : {
            "null_value" : 1,
            "name" : "boost"
         }
      }
   }
}
'

curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1'  -d '
{
   "text" : "foo",
   "boost" : 1
}
'

curl -XPUT 'http://127.0.0.1:9200/test/test/2?pretty=1'  -d '
{
   "text" : "foo",
   "boost" : 2
}
'

curl -XPOST 'http://127.0.0.1:9200/test/_refresh?pretty=1' 
```

The `match_all` gets translated to a constant score query, ignoring the norms field:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "match_all" : {
         "norms_field" : "boost"
      }
   },
   "explain" : 1
}
'

# [Fri Sep 21 16:48:10 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "boost" : 1,
#                "text" : "foo"
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_shard" : 2,
#             "_id" : "1",
#             "_node" : "BdOHDT8FT6yMecrZDzpkHA",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScore(NotDeleted(cache(_type:test))), product of:"
#             }
#          },
#          {
#             "_source" : {
#                "boost" : 2,
#                "text" : "foo"
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_shard" : 3,
#             "_id" : "2",
#             "_node" : "BdOHDT8FT6yMecrZDzpkHA",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScore(NotDeleted(cache(_type:test))), product of:"
#             }
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```

The norms field is taken into account on other queris:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "text" : {
         "text" : "foo"
      }
   },
   "explain" : 1
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "boost" : 2,
#                "text" : "foo"
#             },
#             "_score" : 0.61370564,
#             "_index" : "test",
#             "_shard" : 3,
#             "_id" : "2",
#             "_node" : "BdOHDT8FT6yMecrZDzpkHA",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 0.61370564,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "tf(termFreq(text:foo)=1)"
#                   },
#                   {
#                      "value" : 0.30685282,
#                      "description" : "idf(docFreq=1, maxDocs=1)"
#                   },
#                   {
#                      "value" : 2,
#                      "description" : "fieldNorm(field=text, doc=0)"
#                   }
#                ],
#                "description" : "fieldWeight(text:foo in 0), product of:"
#             }
#          },
#          {
#             "_source" : {
#                "boost" : 1,
#                "text" : "foo"
#             },
#             "_score" : 0.30685282,
#             "_index" : "test",
#             "_shard" : 2,
#             "_id" : "1",
#             "_node" : "BdOHDT8FT6yMecrZDzpkHA",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 0.30685282,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "tf(termFreq(text:foo)=1)"
#                   },
#                   {
#                      "value" : 0.30685282,
#                      "description" : "idf(docFreq=1, maxDocs=1)"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "fieldNorm(field=text, doc=0)"
#                   }
#                ],
#                "description" : "fieldWeight(text:foo in 0), product of:"
#             }
#          }
#       ],
#       "max_score" : 0.61370564,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```
</description><key id="7042890">2277</key><summary>Norms field is ignored when doing a match_all query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-09-21T14:52:28Z</created><updated>2012-11-10T17:02:18Z</updated><resolved>2012-11-10T17:02:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-09-21T14:59:11Z" id="8767385">Hi Clinton,

I'd think this is expected behavior? if you want this special behavior can't you just sort by _boost?
</comment><comment author="ayr-ton" created="2012-09-21T15:00:03Z" id="8767404">It happens to me, I'm trying to boost like this:
https://gist.github.com/3761823
</comment><comment author="clintongormley" created="2012-09-21T15:08:00Z" id="8767659">@s1monw i'd say that's a reasonable solution, but probably not the original intended behaviour, otherwise why would there be a `norms_field` parameter to `match_all`? 

In fact the docs indicate that the expected behaviour is that the norms field should be taken into account:
http://www.elasticsearch.org/guide/reference/query-dsl/match-all-query.html
</comment><comment author="s1monw" created="2012-09-21T15:12:18Z" id="8767801">yeah good point. Lets fix it. For now people can work around this with the sort option.
</comment><comment author="ayr-ton" created="2012-09-21T15:29:26Z" id="8768335">@s1monw Do you think that it can be fixed soon? :-(
</comment><comment author="clintongormley" created="2012-09-21T15:32:56Z" id="8768459">@ayrlabs @s1monw has provided you with a work around which you can use now, which is to sort on the boost field
</comment><comment author="ayr-ton" created="2012-09-21T15:53:51Z" id="8769155">@clintongormley Thank you very much for the support.
</comment><comment author="s1monw" created="2012-09-21T16:32:05Z" id="8770259">wait, did you actually try to search for:

```
{
   "query" : {
      "match_all" : {
         "norms_field" : "text"
      }
   },
   "explain" : 1
}
```

? 
</comment><comment author="ayr-ton" created="2012-09-21T16:48:44Z" id="8773596">@s1monw It works but the priority is the same than the normal query, with no boost.
</comment><comment author="kimchy" created="2012-09-21T16:49:45Z" id="8773635">I see where the problem is, I will push a fix soon and update here....
</comment><comment author="clintongormley" created="2012-10-19T17:34:49Z" id="9609400">The norms field is still being ignored in `match_all`:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "match_all" : {
         "norms_field" : "foo"
      }
   }
}
'

# [Fri Oct 19 19:32:55 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : 1
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "pyneKEpVSveqU87Jfq0R4g",
#             "_type" : "test"
#          },
#          {
#             "_source" : {
#                "foo" : 2
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "GSpVGCaOTLWpcn_FE-9QEQ",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 5
# }
```

Note: the score is 1 for both docs, but it should be using the value in `foo`
</comment><comment author="ayr-ton" created="2012-10-27T14:18:53Z" id="9835781">Same here
</comment><comment author="clintongormley" created="2012-11-10T17:02:18Z" id="10257205">This functionality is being removed in the lucene 4 version of ES
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added MultiPolygon parsing and serialization support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2276</link><project id="" key="" /><description>Adds support for GeoJSON's multipolygon type using JTS's MultiPolygon.
</description><key id="7038851">2276</key><summary>Added MultiPolygon parsing and serialization support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-09-21T11:42:01Z</created><updated>2014-06-29T22:01:49Z</updated><resolved>2012-09-21T12:12:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-21T12:12:36Z" id="8762642">Pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `_id` path should not allow arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2275</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_id" : {
            "path" : "foo.bar"
         }
      }
   }
}
'
```

This works correctly:

```
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "foo" : {
      "bar" : 1,
      "baz" : "xx"
   }
}
'

# [Fri Sep 21 11:14:21 2012] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "test",
#    "_version" : 1
# }
```

This should throw an error, not set the `_id` to `[`:

```
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "foo" : {
      "bar" : [
         2
      ],
      "baz" : "xx"
   }
}
'

# [Fri Sep 21 11:14:23 2012] Response:
# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "[",
#    "_type" : "test",
#    "_version" : 1
# }
```
</description><key id="7036460">2275</key><summary>The `_id` path should not allow arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2012-09-21T09:18:56Z</created><updated>2013-03-01T12:13:45Z</updated><resolved>2013-03-01T12:13:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>"The system cannot find the path specified." when running bat file.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2274</link><project id="" key="" /><description>Windows 7
I also set %JAVA_HOME% to C:\Java\bin to avoid Program Files (Space) issue.
</description><key id="7029265">2274</key><summary>"The system cannot find the path specified." when running bat file.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LarryEitel</reporter><labels /><created>2012-09-21T00:47:06Z</created><updated>2016-10-25T02:47:36Z</updated><resolved>2012-09-24T09:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2012-09-21T14:51:16Z" id="8767113">Hi,

%JAVA_HOME% should probably point to "C:\Java" in your case.

Cheers,
Uri
</comment><comment author="LarryEitel" created="2012-09-22T13:58:10Z" id="8789002">Thank you uboness for your suggestion. Still no joy.

My current config has:

C:\Program Files\Java\jdk1.7.0
and
C:\Program Files\Java\jre7

I tried the following:

set JAVA_HOME=C:\jre7\bin
set JAVA_HOME=C:\jre7

set JAVA_HOME=C:/jre7/bin
set JAVA_HOME=C:/jre7

set JAVA_HOME=/jre7/bin
set JAVA_HOME=/jre7

I also have a jdk1.7.0 directory but I am assuming elastic looks for runtime?

I am anxious to fire up elastic!! :)
</comment><comment author="uboness" created="2012-09-22T20:59:38Z" id="8792729">so basically, the JAVA_HOME should point to the parent directory of the bin. In the examples above, it should point to:

"C:\Program Files\Java\jdk1.7.0"
or
"C:\Program Files\Java\jre7"

If you look at the elasticsearch.bat file. You can insert an echo just before the line where the program is executed and echo the whole line (it might show you where things go wrong):

``` bat
echo "%JAVA_HOME%\bin\java" %JAVA_OPTS% %ES_JAVA_OPTS% %ES_PARAMS% %* -cp "%ES_CLASSPATH%" "org.elasticsearch.bootstrap.ElasticSearch"
```

after executing it, make sure that the paths (as they're echoed) indeed exist. More specifically, that one of the following exists:

 "C:\ProgramFiles\Java\jdk1.7.0\bin\java.exe"
or
"C:\ProgramFiles\Java\jre7\bin\java.exe"
</comment><comment author="LarryEitel" created="2012-09-23T00:49:51Z" id="8794275">Working!!! Thank you for stating the obvious. :)
</comment><comment author="phancongphuoc" created="2014-04-05T17:46:26Z" id="39645456">Thanks too, :)
</comment><comment author="spangul" created="2014-11-06T18:58:45Z" id="62031443">Thanks a ton. 
</comment><comment author="dzolnjan" created="2015-02-24T14:03:48Z" id="75760942">If you got your elastic root folder prefixed with '!' you'll get same error.
E:!elasticsearch-1.4.4\elasticsearch-1.4.4\bin     &lt;--- wont work
</comment><comment author="jaureguif" created="2015-12-28T22:28:21Z" id="167669648">thanks a bunch!
</comment><comment author="Monepi" created="2016-01-05T07:52:26Z" id="168927629">Awesome!
</comment><comment author="BelieveC" created="2016-03-27T09:52:04Z" id="202029326">Still getting "System cannot find specified path" error in windows 10 even after doing above stuff..
JAVA_HOME = C:\Program Files (x86)\Java\jre1.8.0_77
</comment><comment author="mayankdevops" created="2016-04-19T13:42:45Z" id="211927836">Thanks this helped !!
</comment><comment author="nycsri" created="2016-09-19T00:49:14Z" id="247895155">thanks !!!
</comment><comment author="sanchhoker" created="2016-10-25T02:47:36Z" id="255921632">Thanks!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add types exists api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2273</link><project id="" key="" /><description>The types exists api checks whether one or more types exists in one or more indices.
## Example usage

curl -XHEAD 'localhost:9200/twitter/tweet'
## Options
- `index` - One or more indices. Either specified as query string parameter or in the uri path.
- `type` - One or more types. Either specified as query string parameter or in the uri path.
- `ignore_indices` -  Determines what type of indices to exclude from a request. The option can have the following values: `none` or `missing`.

This issue originates from #2263
</description><key id="7016936">2273</key><summary>Add types exists api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-09-20T16:08:51Z</created><updated>2012-10-22T15:17:56Z</updated><resolved>2012-09-21T09:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow passing additional parameters with query request.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2272</link><project id="" key="" /><description>A small addition to ElasticSearch allowing to pass additional, not parsed parameters along with the query, for example like the following:

```
{
 "from" : 0,
 "size" : 60,
 "query" : {
  "term" : { 
   "name" : "value" 
  }
 },
 "additional" : { 
  "token" : "nabsy26dsqnbu1276", 
  "userId" : "12345" 
 }
}
```
</description><key id="7009743">2272</key><summary>Allow passing additional parameters with query request.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">gr0</reporter><labels /><created>2012-09-20T11:58:14Z</created><updated>2014-09-07T11:35:23Z</updated><resolved>2014-09-07T11:35:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gr0" created="2012-09-24T21:45:08Z" id="8836068">If one would think about if the functionality is useful please consider the following:
- in a shared, multi-tenant ES cluster with an authorization proxy in front of the cluster, this can be used to pass information about who is making the query for authorization purposes.
- identification of query sources, types, etc., which can be very useful when troubleshooting problematic queries or problematic clients. We have an ES cluster with 500+ clients hitting it. Not all clients are behaving well, and by having each client specify their identifier in the request we can identify problematic ones.
</comment><comment author="kimchy" created="2012-09-24T22:17:05Z" id="8837026">Yea, agreed that this is beneficial, though, the problem with this is that its limited to the search API. A more generic infrastructure for this makes more sense, and we are working on building one.
</comment><comment author="otisg" created="2012-09-24T22:26:15Z" id="8837331">+1 for the feature and +1 for going beyond search.
@kimchy is there an existing issue for this functionality?
</comment><comment author="kimchy" created="2012-09-25T10:40:51Z" id="8849937">@otisg no issue yet, I am still in the "playing around" phase to really see how best to implement this.
</comment><comment author="kimchy" created="2014-09-07T11:35:23Z" id="54744255">this is implemented using the header infra we have in ES already, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support fetching GeoShape from another index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2271</link><project id="" key="" /><description>Currently we require users to pass in the coordinates of Shapes they want to search with, which can be quite onerous with large and complex polygons.  We should allow users to specify the ID, type and index where the Shape they wish to use can be retrieved from.   This allows Shapes to be pre-loaded into an index from other sources.

This change will involve adding a simple ShapeFetchingService which will be responsible for retrieving  and parsing the Shapes.

Additionally, the GeoShapeQueryParser and GeoShapeFilterParser will support a "named_shape" JSON object with four fields:
- "id": ID of the Shape
- "type": Index type where the Shape is indexed
- "index": Name of the index where the Shape is
- "shape_field": Name of the field that the Shape itself is indexed in

Future issues will cover providing ways to pre-load the Shapes from common sources.
</description><key id="7009726">2271</key><summary>Support fetching GeoShape from another index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>enhancement</label></labels><created>2012-09-20T11:57:24Z</created><updated>2013-06-26T15:35:11Z</updated><resolved>2013-06-26T15:35:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-09-21T14:58:11Z" id="8767346">Very nice
</comment><comment author="spinscale" created="2013-06-26T15:35:11Z" id="20056073">Bug left open. Implemented, see http://www.elasticsearch.org/guide/reference/query-dsl/geo-shape-filter/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgraded to Spatial4j 0.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2270</link><project id="" key="" /><description>Upgrade to Spatial4j 0.3.  

The predominant changes are in two places: 
- All Shapes in Spatial4j now require a SpatialContext during construction, so the single instance used has been isolated for reuse.  
- The SpatialPrefixTree API has changed a little as part of new interaction with Spatial4j and as part of some bugs that were identified during the Spatial4j testing.
</description><key id="7007822">2270</key><summary>Upgraded to Spatial4j 0.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-09-20T10:30:47Z</created><updated>2014-07-03T15:23:01Z</updated><resolved>2012-09-20T22:09:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-20T22:09:50Z" id="8747472">Pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for MultiPolygon Shapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2269</link><project id="" key="" /><description>The initial GeoShape code did not include support for MultiPolygons in serialization and parsing.  However they are supported by Spatial4j and JTS.  Adding this is necessary so that country shapes which are often MultiPolygons, can be used.
</description><key id="7007521">2269</key><summary>Add support for MultiPolygon Shapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>enhancement</label></labels><created>2012-09-20T10:21:17Z</created><updated>2013-07-15T17:17:37Z</updated><resolved>2013-07-15T17:17:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-14T14:27:04Z" id="14904954">Is this anything different to #2276 or can this be closed as well?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Spatial4j 0.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2268</link><project id="" key="" /><description>Spatial4j 0.3 was released a couple of weeks ago.  It includes a number of API changes (which shouldn't effect ES users) but most importantly it adds support for Polygons that cross the dateline.  Additional validation of Shapes is also included.
</description><key id="7007438">2268</key><summary>Upgrade to Spatial4j 0.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-09-20T10:18:38Z</created><updated>2012-09-20T22:10:03Z</updated><resolved>2012-09-20T22:10:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-20T22:10:03Z" id="8747479">Pushed in #2270.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Race condition if rapidly creating, deleting then recreating an index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2267</link><project id="" key="" /><description>I recently ran into this issue where the sequence of operations [create index, delete index, create index] would cause the latter create index operation to return OK, but the index was not properly created.

Some of the shards was missing a lot of expected files and snapshotting, refreshing and recovering was not possible. It was, however, possible to index some documents into the index in this state.

See https://gist.github.com/27279f6ad2a00924fc92 for the log output from the cluster.
</description><key id="7006912">2267</key><summary>Race condition if rapidly creating, deleting then recreating an index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2012-09-20T10:02:37Z</created><updated>2014-07-08T16:43:38Z</updated><resolved>2014-07-08T16:43:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T16:43:38Z" id="48365924">Testing this out in master all seems to work OK.  Please reopen if you're still seeng issues.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation error about threadpool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2266</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/reference/modules/threadpool.html

It say the parameter is named `queue_size`, but the example shows `queue`.
</description><key id="7005786">2266</key><summary>Documentation error about threadpool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Silex</reporter><labels /><created>2012-09-20T09:27:33Z</created><updated>2013-10-30T09:36:48Z</updated><resolved>2013-10-30T09:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gustavobmaia" created="2012-09-25T22:51:32Z" id="8873180">You can use both, See the code in  org.elasticsearch.threadpool.ThreadPool

```
SizeValue capacity = settings.getAsSize("capacity", settings.getAsSize("queue", settings.getAsSize("queue_size", defaultSettings.getAsSize("queue", defaultSettings.getAsSize("queue_size", null)))));

```
</comment><comment author="Silex" created="2012-11-01T13:13:39Z" id="9979684">Shouldn't the doc be consistent and push one way only, deprecating the others?
</comment><comment author="spinscale" created="2013-10-30T09:36:48Z" id="27375432">Definately. Using `queue_size` everywhere now... 

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a mechanism for allowing indexing when versions mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2265</link><project id="" key="" /><description>When using versioning, there is currently no way to force a document to be indexed when the same or newer version already exists. This would be useful when, for instance, ElasticSearch contains data replicated from RDMS, and there has been a transaction rollback.

One option would be a mode in which ElasticSearch does index the older document, but still returns the VersionConflictEngineException as a warning to the user.

See also https://github.com/elasticsearch/elasticsearch/issues/2166
</description><key id="6983826">2265</key><summary>Provide a mechanism for allowing indexing when versions mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snellm</reporter><labels /><created>2012-09-19T15:01:52Z</created><updated>2014-07-08T16:38:02Z</updated><resolved>2014-07-08T16:38:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T16:38:02Z" id="48365200">The new `force` version type allows you to override versions in Elasticsearch. http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add id cache size to nodes stats api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2264</link><project id="" key="" /><description>Also add bloom cache size to nodes stats _rest_ api.
</description><key id="6978704">2264</key><summary>Add id cache size to nodes stats api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-09-19T11:00:40Z</created><updated>2012-09-19T13:58:20Z</updated><resolved>2012-09-19T13:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rest API: Add `HEAD` support for `/{index}/{type}` to quickly check if a index&amp;type exists or not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2263</link><project id="" key="" /><description>Like the existing commands but will check if a type exists on a given index. 

Pulling the mapping is too expensive.

This will be helpful for sorting across indices to see if a given attribute exists in all indices.
</description><key id="6962472">2263</key><summary>Rest API: Add `HEAD` support for `/{index}/{type}` to quickly check if a index&amp;type exists or not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels /><created>2012-09-18T19:27:28Z</created><updated>2012-09-21T09:27:49Z</updated><resolved>2012-09-21T09:27:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-09-21T09:27:49Z" id="8759475">Added in #2273
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query boost skipped for text/match query with queries of more than one word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2262</link><project id="" key="" /><description>When doing a text/match query where the string has one word, the boosts get applied and show up in the explains text. However, when doing a text/match query where the string has more than one word, the bosts don't appear to get applied and definitely don't show up in the explains text.

We found the problem with 0.17.0, but I've verified that it's also a problem in 0.19.8 and 0.19.9, too.

This bash script can reproduce the problem. Sure seems like it's a bug. Any ideas?

```
#!/bin/bash

# Create the index and mapping.
curl -XPUT 'http://localhost:9200/status_index/?pretty=1'  -d '
{
   "mappings" : {
      "status" : {
         "properties" : {
            "id" : {
               "type" : "integer"
            },
            "summary": {
               "type": "string",
               "analyzer" : "snowball"
            },
            "msg" : {
               "type" : "string",
               "analyzer" : "snowball"
            }
         }
      }
   }
}
'
echo ""

# Response.
# {
#   "ok" : true,
#   "acknowledged" : true
# }


sleep 1s


# Add some things to the index.
curl -XPOST 'http://localhost:9200/status_index/status?pretty=1'  -d '
{
   "id" : 1,
   "summary": "first post",
   "msg" : "first status message posted."
}
'
echo ""

# Response.
# {
#   "ok" : true,
#   "_index" : "status_index",
#   "_type" : "status",
#   "_id" : "7tvZFNjzSPuN1ULbewp-cA",
#   "_version" : 1
# }


curl -XPOST 'http://localhost:9200/status_index/status?pretty=1'  -d '
{
   "id" : 2,
   "summary": "second post",
   "msg" : "second status message posted."
}
'
echo ""

# Response.
# {
#   "ok" : true,
#   "_index" : "status_index",
#   "_type" : "status",
#   "_id" : "bpDLEMHzRGO61OIXBsV3Rg",
#   "_version" : 1
# }


curl -XPOST 'http://localhost:9200/status_index/_refresh?pretty=1'
echo ""

# Response.
# {
#   "ok" : true,
#   "_shards" : {
#     "total" : 10,
#     "successful" : 5,
#     "failed" : 0
#   }
# }


# Text query with a boost and a single word. This works fine.
curl -XGET 'http://localhost:9200/status_index/_search?pretty=1'  -d '
{
   "fields" : [],
   "query" : {
      "bool" : {
         "should" : [
            {"text": {
               "summary": {
                  "query": "posted",
                  "boost": 4.0
               }
            }},
            {"text": {
               "msg": {
                  "query": "posted",
                  "boost": 2.0
               }
            }}
         ]
      }
   },
   "explain": true
}
'
echo ""

# Response.
# {
#   "took" : 173,
#   "timed_out" : false,
#   "_shards" : {
#     "total" : 5,
#     "successful" : 5,
#     "failed" : 0
#   },
#   "hits" : {
#     "total" : 2,
#     "max_score" : 0.24015032,
#     "hits" : [ {
#       "_shard" : 0,
#       "_node" : "dRqpy_aITq-Qg7GTu3PRag",
#       "_index" : "status_index",
#       "_type" : "status",
#       "_id" : "bpDLEMHzRGO61OIXBsV3Rg",
#       "_score" : 0.24015032,
#       "_explanation" : {
#         "value" : 0.24015033,
#         "description" : "sum of:",
#         "details" : [ {
#           "value" : 0.17153595,
#           "description" : "weight(summary:post^4.0 in 0), product of:",
#           "details" : [ {
#             "value" : 0.89442724,
#             "description" : "queryWeight(summary:post^4.0), product of:",
#             "details" : [ {
#               "value" : 4.0,
#               "description" : "boost"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.7287103,
#               "description" : "queryNorm"
#             } ]
#           }, {
#             "value" : 0.19178301,
#             "description" : "fieldWeight(summary:post in 0), product of:",
#             "details" : [ {
#               "value" : 1.0,
#               "description" : "tf(termFreq(summary:post)=1)"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.625,
#               "description" : "fieldNorm(field=summary, doc=0)"
#             } ]
#           } ]
#         }, {
#           "value" : 0.06861438,
#           "description" : "weight(msg:post^2.0 in 0), product of:",
#           "details" : [ {
#             "value" : 0.44721362,
#             "description" : "queryWeight(msg:post^2.0), product of:",
#             "details" : [ {
#               "value" : 2.0,
#               "description" : "boost"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.7287103,
#               "description" : "queryNorm"
#             } ]
#           }, {
#             "value" : 0.15342641,
#             "description" : "fieldWeight(msg:post in 0), product of:",
#             "details" : [ {
#               "value" : 1.0,
#               "description" : "tf(termFreq(msg:post)=1)"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.5,
#               "description" : "fieldNorm(field=msg, doc=0)"
#             } ]
#           } ]
#         } ]
#       }
#     }, {
#       "_shard" : 4,
#       "_node" : "dRqpy_aITq-Qg7GTu3PRag",
#       "_index" : "status_index",
#       "_type" : "status",
#       "_id" : "7tvZFNjzSPuN1ULbewp-cA",
#       "_score" : 0.24015032,
#       "_explanation" : {
#         "value" : 0.24015033,
#         "description" : "sum of:",
#         "details" : [ {
#           "value" : 0.17153595,
#           "description" : "weight(summary:post^4.0 in 0), product of:",
#           "details" : [ {
#             "value" : 0.89442724,
#             "description" : "queryWeight(summary:post^4.0), product of:",
#             "details" : [ {
#               "value" : 4.0,
#               "description" : "boost"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.7287103,
#               "description" : "queryNorm"
#             } ]
#           }, {
#             "value" : 0.19178301,
#             "description" : "fieldWeight(summary:post in 0), product of:",
#             "details" : [ {
#               "value" : 1.0,
#               "description" : "tf(termFreq(summary:post)=1)"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.625,
#               "description" : "fieldNorm(field=summary, doc=0)"
#             } ]
#           } ]
#         }, {
#           "value" : 0.06861438,
#           "description" : "weight(msg:post^2.0 in 0), product of:",
#           "details" : [ {
#             "value" : 0.44721362,
#             "description" : "queryWeight(msg:post^2.0), product of:",
#             "details" : [ {
#               "value" : 2.0,
#               "description" : "boost"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.7287103,
#               "description" : "queryNorm"
#             } ]
#           }, {
#             "value" : 0.15342641,
#             "description" : "fieldWeight(msg:post in 0), product of:",
#             "details" : [ {
#               "value" : 1.0,
#               "description" : "tf(termFreq(msg:post)=1)"
#             }, {
#               "value" : 0.30685282,
#               "description" : "idf(docFreq=1, maxDocs=1)"
#             }, {
#               "value" : 0.5,
#               "description" : "fieldNorm(field=msg, doc=0)"
#             } ]
#           } ]
#         } ]
#       }
#     } ]
#   }
# }


# Text query with a boost and two words word. This doesn't work
# The boosts don't seem to be applied.
curl -XGET 'http://localhost:9200/status_index/_search?pretty=1'  -d '
{
   "fields" : [],
   "query" : {
      "bool" : {
         "should" : [
            {"text": {
               "summary": {
                  "query": "message posted",
                  "boost": 4.0
               }
            }},
            {"text": {
               "msg": {
                  "query": "message posted",
                  "boost": 2.0
               }
            }}
         ]
      }
   },
   "explain": true
}
'
echo ""

# Response.
# {
#   "took" : 14,
#   "timed_out" : false,
#   "_shards" : {
#     "total" : 5,
#     "successful" : 5,
#     "failed" : 0
#   },
#   "hits" : {
#     "total" : 2,
#     "max_score" : 0.0716136,
#     "hits" : [ {
#       "_shard" : 0,
#       "_node" : "dRqpy_aITq-Qg7GTu3PRag",
#       "_index" : "status_index",
#       "_type" : "status",
#       "_id" : "bpDLEMHzRGO61OIXBsV3Rg",
#       "_score" : 0.0716136,
#       "_explanation" : {
#         "value" : 0.0716136,
#         "description" : "sum of:",
#         "details" : [ {
#           "value" : 0.027543694,
#           "description" : "product of:",
#           "details" : [ {
#             "value" : 0.055087388,
#             "description" : "sum of:",
#             "details" : [ {
#               "value" : 0.055087388,
#               "description" : "weight(summary:post in 0), product of:",
#               "details" : [ {
#                 "value" : 0.2872381,
#                 "description" : "queryWeight(summary:post), product of:",
#                 "details" : [ {
#                   "value" : 0.30685282,
#                   "description" : "idf(docFreq=1, maxDocs=1)"
#                 }, {
#                   "value" : 0.9360777,
#                   "description" : "queryNorm"
#                 } ]
#               }, {
#                 "value" : 0.19178301,
#                 "description" : "fieldWeight(summary:post in 0), product of:",
#                 "details" : [ {
#                   "value" : 1.0,
#                   "description" : "tf(termFreq(summary:post)=1)"
#                 }, {
#                   "value" : 0.30685282,
#                   "description" : "idf(docFreq=1, maxDocs=1)"
#                 }, {
#                   "value" : 0.625,
#                   "description" : "fieldNorm(field=summary, doc=0)"
#                 } ]
#               } ]
#             } ]
#           }, {
#             "value" : 0.5,
#             "description" : "coord(1/2)"
#           } ]
#         }, {
#           "value" : 0.04406991,
#           "description" : "sum of:",
#           "details" : [ {
#             "value" : 0.022034954,
#             "description" : "weight(msg:messag in 0), product of:",
#             "details" : [ {
#               "value" : 0.14361905,
#               "description" : "queryWeight(msg:messag), product of:",
#               "details" : [ {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.46803886,
#                 "description" : "queryNorm"
#               } ]
#             }, {
#               "value" : 0.15342641,
#               "description" : "fieldWeight(msg:messag in 0), product of:",
#               "details" : [ {
#                 "value" : 1.0,
#                 "description" : "tf(termFreq(msg:messag)=1)"
#               }, {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.5,
#                 "description" : "fieldNorm(field=msg, doc=0)"
#               } ]
#             } ]
#           }, {
#             "value" : 0.022034954,
#             "description" : "weight(msg:post in 0), product of:",
#             "details" : [ {
#               "value" : 0.14361905,
#               "description" : "queryWeight(msg:post), product of:",
#               "details" : [ {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.46803886,
#                 "description" : "queryNorm"
#               } ]
#             }, {
#               "value" : 0.15342641,
#               "description" : "fieldWeight(msg:post in 0), product of:",
#               "details" : [ {
#                 "value" : 1.0,
#                 "description" : "tf(termFreq(msg:post)=1)"
#               }, {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.5,
#                 "description" : "fieldNorm(field=msg, doc=0)"
#               } ]
#             } ]
#           } ]
#         } ]
#       }
#     }, {
#       "_shard" : 4,
#       "_node" : "dRqpy_aITq-Qg7GTu3PRag",
#       "_index" : "status_index",
#       "_type" : "status",
#       "_id" : "7tvZFNjzSPuN1ULbewp-cA",
#       "_score" : 0.0716136,
#       "_explanation" : {
#         "value" : 0.0716136,
#         "description" : "sum of:",
#         "details" : [ {
#           "value" : 0.027543694,
#           "description" : "product of:",
#           "details" : [ {
#             "value" : 0.055087388,
#             "description" : "sum of:",
#             "details" : [ {
#               "value" : 0.055087388,
#               "description" : "weight(summary:post in 0), product of:",
#               "details" : [ {
#                 "value" : 0.2872381,
#                 "description" : "queryWeight(summary:post), product of:",
#                 "details" : [ {
#                   "value" : 0.30685282,
#                   "description" : "idf(docFreq=1, maxDocs=1)"
#                 }, {
#                   "value" : 0.9360777,
#                   "description" : "queryNorm"
#                 } ]
#               }, {
#                 "value" : 0.19178301,
#                 "description" : "fieldWeight(summary:post in 0), product of:",
#                 "details" : [ {
#                   "value" : 1.0,
#                   "description" : "tf(termFreq(summary:post)=1)"
#                 }, {
#                   "value" : 0.30685282,
#                   "description" : "idf(docFreq=1, maxDocs=1)"
#                 }, {
#                   "value" : 0.625,
#                   "description" : "fieldNorm(field=summary, doc=0)"
#                 } ]
#               } ]
#             } ]
#           }, {
#             "value" : 0.5,
#             "description" : "coord(1/2)"
#           } ]
#         }, {
#           "value" : 0.04406991,
#           "description" : "sum of:",
#             "description" : "weight(msg:messag in 0), product of:",
#             "details" : [ {
#               "value" : 0.14361905,
#               "description" : "queryWeight(msg:messag), product of:",
#               "details" : [ {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.46803886,
#                 "description" : "queryNorm"
#               } ]
#             }, {
#               "value" : 0.15342641,
#               "description" : "fieldWeight(msg:messag in 0), product of:",
#               "details" : [ {
#                 "value" : 1.0,
#                 "description" : "tf(termFreq(msg:messag)=1)"
#               }, {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.5,
#                 "description" : "fieldNorm(field=msg, doc=0)"
#               } ]
#             } ]
#           }, {
#             "value" : 0.022034954,
#             "description" : "weight(msg:post in 0), product of:",
#             "details" : [ {
#               "value" : 0.14361905,
#               "description" : "queryWeight(msg:post), product of:",
#               "details" : [ {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.46803886,
#                 "description" : "queryNorm"
#               } ]
#             }, {
#               "value" : 0.15342641,
#               "description" : "fieldWeight(msg:post in 0), product of:",
#               "details" : [ {
#                 "value" : 1.0,
#                 "description" : "tf(termFreq(msg:post)=1)"
#               }, {
#                 "value" : 0.30685282,
#                 "description" : "idf(docFreq=1, maxDocs=1)"
#               }, {
#                 "value" : 0.5,
#                 "description" : "fieldNorm(field=msg, doc=0)"
#               } ]
#             } ]
#           } ]
#         } ]
#       }
#     } ]
#   }
# }


# Cleanup.
curl -XDELETE 'http://localhost:9200/status_index/?pretty=1'

# Response.
# {
#   "ok" : true,
#   "acknowledged" : true
# }
```
</description><key id="6961117">2262</key><summary>query boost skipped for text/match query with queries of more than one word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">willkg</reporter><labels /><created>2012-09-18T18:38:04Z</created><updated>2012-12-12T16:32:13Z</updated><resolved>2012-12-12T16:32:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="willkg" created="2012-12-12T00:16:18Z" id="11271177">I checked ElasticSearch 0.19.11 and still see this issue.

Any thoughts?
</comment><comment author="martijnvg" created="2012-12-12T11:01:52Z" id="11284760">@willkg The boost does get applied to the match queries in the last search request. See my re-creation:
https://gist.github.com/4266907

Inside ES the match queries in both search requests get resolved to different Lucene queries. In the first request the Lucene term query is chosen, which in the case is the most efficient query to choose. In the second request the Lucene boolean query (with two should clauses) is chosen, this query is choses b/c there are two tokens in the query field.

The boost set on a term query has a different score as result than when a boost is set on a boolean query. Unfortunately setting a boost on a boolean query can't be seen in the explain as boost (boosts on a boolean query get pushed in the clauses in a normalised form).

In general what is important to remember when setting a boost on a query it doesn't mean the score needs to get x times higher then on a document that doesn't match that specified query. You're specifying that a document that matches a query is x times more important than documents that don't match that query. In your case both documents match with the match query that has query time boost of 4, so they're equal important. 
</comment><comment author="willkg" created="2012-12-12T16:32:13Z" id="11296395">Ahhh... ok. That makes a lot more sense. It is weird and confusing that the boost doesn't show up in the explains text--makes it really difficult to trust that the explains is telling me everything that's being used to calculate the score.

Anyhow, I really appreciate you walking me through this. Looks good to me--closing it out. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add better error handling for has_child, has_parent and top_children. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2261</link><project id="" key="" /><description>If has_parent, has_child or top_children are executed incorrectly then throw a better exception instead of the NPE that is currently being thrown. This happens now when one of these queries / filters are used in the count api.
</description><key id="6949212">2261</key><summary>Add better error handling for has_child, has_parent and top_children. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-18T11:16:49Z</created><updated>2012-09-18T11:27:15Z</updated><resolved>2012-09-18T11:27:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.19</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2260</link><project id="" key="" /><description /><key id="6944510">2260</key><summary>0.19</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">milesli</reporter><labels /><created>2012-09-18T08:16:11Z</created><updated>2014-06-12T21:09:12Z</updated><resolved>2013-10-30T09:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="milesli" created="2012-09-18T08:31:03Z" id="8645204">Filter  transmission in recovery.
When a replica is in recovery, it is no need to perform replica.
</comment><comment author="kimchy" created="2012-09-18T13:32:55Z" id="8654815">Not sure what the pull request changes..., it includes a lot of commits. Can you clean it up and explain maybe what you are trying to fix?
</comment><comment author="milesli" created="2012-09-19T08:33:09Z" id="8684214">When source node in recovery, synchronize new request between source node and target node with sending translog. so no need to forward new requests to target node, it exhausts cpu,bandwidth. 
For example&#65292; After a node joins in the cluster, it spend 5 hours to recover data from source node,  meanwhile, it receives a lot of new requests forwarded from source node, but these new requests immediately will be overwritten by translog from source node.
so i want to  filter these new requests in source node.
</comment><comment author="kimchy" created="2012-09-19T09:44:49Z" id="8685784">Can you please fix your pull request to only include your changes, so I can have a look? If I understand correctly, then changes do need to be transmitted to the target node, to make sure things are in sync, they are not executed unless the target node is in a state that it can receive them.
</comment><comment author="milesli" created="2012-09-20T05:27:45Z" id="8717178">My pull request only include this change.  there are really 6 changed files!
</comment><comment author="spinscale" created="2013-07-15T16:52:44Z" id="20983341">hey, your PR is against the 0.19 branch and not against master. It would be great if you could either rebase and force push this one or create new pull request, if you still think this is a useful feature.

Sorry for getting back to you late! Thanks!
</comment><comment author="spinscale" created="2013-10-30T09:33:04Z" id="27375232">closing this, too hard to extract.. I am happy to have a closer look, if you provide a patch or a new PR. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include bloom &amp; id cache size in node stats api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2259</link><project id="" key="" /><description /><key id="6923745">2259</key><summary>Include bloom &amp; id cache size in node stats api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-09-17T15:56:33Z</created><updated>2012-10-03T17:21:33Z</updated><resolved>2012-10-03T17:21:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2012-10-02T23:51:33Z" id="9091442">Duplicate of #2264?
</comment><comment author="martijnvg" created="2012-10-03T17:21:33Z" id="9114470">Yep.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable allocation: New indices allocation not to be disabled by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2258</link><project id="" key="" /><description>When setting `cluster.routing.allocation.disable_allocation`, it causes new indices primary shards to not be allocated. By default, new indices created should allow to, at the very least, allocate primary shards so they become operations. A new setting, `cluster.routing.allocation.disable_new_allocation`, allows to also disable "new" allocations.
</description><key id="6919844">2258</key><summary>Disable allocation: New indices allocation not to be disabled by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-17T14:00:05Z</created><updated>2012-09-17T14:01:01Z</updated><resolved>2012-09-17T14:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Default logical name analyzer isn't being applied to mappings and can't be referenced explicitly in a field mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2257</link><project id="" key="" /><description>this effects 0.19.9

index settings:

```
curl -XPUT http://localhost:9200/my_index -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {              
                "partial" : {
                    "filter" : [
                        "standard",
                        "lowercase",
                        "stop",
                        "asciifolding",
                        "partial"
                    ],
                    "tokenizer" : "standard",
                    "type": "custom"
                },
                "default" : {
                    "filter" : [
                        "standard",
                        "lowercase",
                        "stop",
                        "asciifolding",
                        "minimal_stemmer"
                    ],
                    "tokenizer" : "standard",
                    "type": "custom"
                }
            },
            "filter" : {
                "minimal_stemmer" : {&#732;
                    "type" : "stemmer",
                    "name" : "minimal_english"
                },
                "partial" : {
                    "type" : "nGram",
                    "min_gram" : 2,
                    "max_gram" : 10
                }
            }
        }
    }
}
'
```

example mapping

```
curl -XPUT http://localhost:9200/my_index/my_type/_mapping -d '
{
    "my_type" : {
        "properties" : {
            "my_field" : {
                "type" : "string",
                "index_analyzer" : "partial",
                "search_analyzer" : "default"
            }
        }
    }
}
'
```

get mapping

```
curl -XGET http://localhost:9200/my_index/my_type/_mapping?pretty=1
{
  "my_type" : {   
    "properties" : {
      "my_field" : {
        "type" : "string",
        "analyzer" : "partial"
      }
    }
  }
}
```

note, only the partial analyzer is recognised and is applied for both indexing and searching.

however, if we use the logical names default_index and default_search, then they are applied to the mapping globally as well as being able to be referenced on fields explicitly:

create index

```
curl -XPUT http://localhost:9200/my_index -d '
{
    "index" : {
        "analysis" : {
            "analyzer" : {              
                "partial" : {
                    "filter" : [
                        "standard",
                        "lowercase",
                        "stop",
                        "asciifolding",
                        "partial"
                    ],
                    "tokenizer" : "standard",
                    "type": "custom"
                },
                "default_index" : {
                    "filter" : [
                        "standard",
                        "lowercase",
                        "stop",
                        "asciifolding",
                        "minimal_stemmer"
                    ],
                    "tokenizer" : "standard",
                    "type": "custom"
                },
                "default_search" : {
                    "filter" : [
                        "standard",
                        "lowercase",
                        "stop",
                        "asciifolding",
                        "minimal_stemmer"
                    ],
                    "tokenizer" : "standard",
                    "type": "custom"
                }
            },
            "filter" : {
                "minimal_stemmer" : {&#732;
                    "type" : "stemmer",
                    "name" : "minimal_english"
                },
                "partial" : {
                    "type" : "nGram",
                    "min_gram" : 2,
                    "max_gram" : 10
                }
            }
        }
    }
}
'
```

put mapping

```
curl -XPUT http://localhost:9200/my_index/my_type/_mapping -d '
{
    "my_type" : {
        "properties" : {
            "my_field" : {
                "type" : "string",
                "index_analyzer" : "partial",
                "search_analyzer" : "default_search"
            }
        }
    }
}
'
```

get mapping

```
curl -XGET http://localhost:9200/my_index/my_type/_mapping?pretty=1

{
  "my_type" : {
    "index_analyzer" : "default_index",
    "search_analyzer" : "default_search",   
    "properties" : {
      "my_field" : {
        "type" : "string",
        "index_analyzer" : "partial",
        "search_analyzer" : "default_search"
      }
    }
  }
}
```

default index and search analyzers are applied to the mapping globally, as well as usable from a field.
</description><key id="6908411">2257</key><summary>Default logical name analyzer isn't being applied to mappings and can't be referenced explicitly in a field mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rayward</reporter><labels /><created>2012-09-17T00:41:26Z</created><updated>2014-07-08T16:34:57Z</updated><resolved>2014-07-08T16:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-17T14:37:38Z" id="8616828">I don't really understand what the problem is?
</comment><comment author="rayward" created="2012-09-17T23:08:40Z" id="8634278">The documentation for default analyzers (http://www.elasticsearch.org/guide/reference/index-modules/analysis/) mentions that an analyzer that is given the name 'default' will be used as the default for indexing and search analysis.

However when I override the index_analyzer on a field, it's being set as just the overall 'analyzer', causing it to also affect searching. If I explicitly tell that field to use 'default' on the search_analyzer it also doesn't work. Neither does setting an alias on the default analyzer work either.

It appears that only the logical names default_index and default_search work.
</comment><comment author="clintongormley" created="2014-07-08T16:34:56Z" id="48364792">This looks like it has been fixed in 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Reroute API: Allow to specify allocation commands</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2256</link><project id="" key="" /><description>The reroute command allows to explcitiyly execute a cluster reroute allocation command including specific commands. For example, a shard can be moved from one node to another explicitly, an allocation can be canceled, or an unassigned shard can be explicitly allocated on a specific node.

Here is a short example of how a simple reroute API call:

```
curl -XPOST 'localhost:9200/_cluster/reroute' -d '{
    "commands" : [
        {"move" : {"index" : "test", "shard" : 0, "from_node" : "node1", "to_node" : "node2"}},
        {"allocate" : {"index" : "test", "shard" : 1, "node" : "node3"}}
    ]
}'
```

An importnat aspect to remember is the fact that once when an allocation occurs, the cluster will aim at rebalancing its state back to an even state. For example, if the allocation includes moving a shard from `node1` to `node2`, in an "even" state, then another shard will be moved from `node2` to `node1` to even things out.

The cluster can be set to disable allocations, which means that only the explicitl allocations will be performed. Obviously, only once all commands has been applied, the cluster will aim to be rebalance its state.

Anohter option is to run the commands in "dry_run" (as a URI flag, or in the request body). This will cause the commands to apply to the current cluster state, and reutrn the resulting cluster after the comamnds (and rebalancing) has been applied.

The commands supporterd are:
- `move`: Move a started shard from one node to anotehr node. Accepts `index` and `shard` for index name and shard number, `from_node` for the node to move the shard "from", and `to_node` for the node to move the shard to.
- `cancel`: Cancel allocation of a shard (or recovery). Accepts `index` and `shard` for index name and shar number, and `node` for the node to cancel the shard allocation on. It also accepts `allow_primary` flag to explciitly specify that it is allowed to cancel allocation for a primary shard.
- `allocate`: Allocate an unassigned shard to a node. Accepts the `index` and `shard` for index name and shard number, and `node` to allocate the shard to. It also accepts `allow_primary` flag to explciitly specify that it is allowed to explciitly allocate a primary shard (might result in data loss).
</description><key id="6898191">2256</key><summary>Cluster Reroute API: Allow to specify allocation commands</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-15T20:55:48Z</created><updated>2012-09-17T19:12:12Z</updated><resolved>2012-09-15T20:56:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pierrre" created="2012-09-16T14:02:08Z" id="8595050">We need a drag and drop interface for https://github.com/Aconex/elasticsearch-head :D
</comment><comment author="dadoonet" created="2012-09-17T19:12:12Z" id="8627111">@pierrre you should ask for it here: https://github.com/Aconex/elasticsearch-head/issues
cc @mobz
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard failures when searching multiple indices with a type filter and sort field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2255</link><project id="" key="" /><description>When executing a search with both a type filter and a sort across multiple indices any shards that do not contain the target type mapping fail. While these failures can be masked by using ignore_unmapped, it seems as if the type filter should make the flag unnecessary.

For example, let there be an index aaa with a type mapping Y and another index bbb without mapping Y. A search across all indices for all records with type Y (i.e. GET /_all/Y) and sorted by a field F in Y will fail on all shards containing data for index bbb. The failed shards all throw SearchParseExceptions because there was no mapping Y found for the field F (the sort field).

This failure arises specifically because the sort field is resolved (for type information, etc) before the type filter (or any other filter) is applied. After the filters are applied there are no matching records and so no sort is needed. There does not appear to be any downside to deferring this field resolution until after the number of matched records is known to be greater than zero.

Simple test case for scenario above:

```
curl -XPUT 'localhost:9200/aaa' -d '{
    "settings" : {
        "number_of_shards" : 5
    },
    "mappings" : {
        "Y" : {
            "properties" : {
                "F" : { "type" : "integer" }
            }
        }
    }
}'

curl -XPUT 'localhost:9200/bbb' -d '{
    "settings" : {
        "number_of_shards" : 5
    }
}'

curl -XPUT 'localhost:9200/aaa/Y/1' -d '{
    "F" : 1
}'

curl -XPOST 'localhost:9200/_all/Y/_search' -d '{
     "sort" : [
        { "F" : {"order" : "asc"} }
    ]
}'
```
</description><key id="6883551">2255</key><summary>Shard failures when searching multiple indices with a type filter and sort field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ddecola</reporter><labels /><created>2012-09-14T18:30:15Z</created><updated>2014-08-01T13:30:39Z</updated><resolved>2014-08-01T13:30:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T08:19:41Z" id="50120877">We should support `ignore_missing` for cases where the field doesn't exist in the mapping as well.
</comment><comment author="clintongormley" created="2014-07-25T08:20:52Z" id="50120956">Also see https://github.com/elasticsearch/elasticsearch/issues/2801
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add has_parent query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2254</link><project id="" key="" /><description>The `has_parent` query works the same as the `has_parent` filter, by automatically wrapping the filter with a constant_score. It has the same syntax as the `has_parent` filter.
</description><key id="6876664">2254</key><summary>Add has_parent query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-14T14:14:04Z</created><updated>2012-09-14T15:34:55Z</updated><resolved>2012-09-14T15:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>When configuring mmapfs it is not used since 0.19.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2253</link><project id="" key="" /><description /><key id="6874275">2253</key><summary>When configuring mmapfs it is not used since 0.19.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.10</label></labels><created>2012-09-14T12:30:25Z</created><updated>2012-09-14T12:30:48Z</updated><resolved>2012-09-14T12:30:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improved `has_child` filter / query performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2252</link><project id="" key="" /><description>Relates to #2251
</description><key id="6848148">2252</key><summary>Improved `has_child` filter / query performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-09-13T14:09:55Z</created><updated>2015-05-18T23:35:26Z</updated><resolved>2012-09-14T14:02:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-14T14:02:23Z" id="8561887">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve `has_child` filter / query performance </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2251</link><project id="" key="" /><description>Added a new `has_child` filter implementation, that works `_uid` based instead of bitset based. This implementation is about ~2 till ~6 times (depending on the query) faster than the already existing bitset based implementation.
</description><key id="6847547">2251</key><summary>Improve `has_child` filter / query performance </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-13T13:46:19Z</created><updated>2012-09-14T14:02:30Z</updated><resolved>2012-09-14T14:02:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-14T14:02:30Z" id="8561893">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed mlockall doesn't warn</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2250</link><project id="" key="" /><description>I've set `$ES_HEAP` to 2048MB, added `bootstrap.mlockall: true` to my config, and `ulimit -l` is set to 1024.

I can start elasticsearch, and there is no warning in the logs to inform me that the mlockall failed.
</description><key id="6844811">2250</key><summary>Failed mlockall doesn't warn</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-09-13T11:35:42Z</created><updated>2013-10-18T12:57:55Z</updated><resolved>2013-10-18T12:57:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T12:57:55Z" id="26593333">Seems to be working now, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>setting "index.store.type: mmapfs" still uses NIOFS backend</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2249</link><project id="" key="" /><description>XMMapFSDirectory class extends wrong directory
</description><key id="6838781">2249</key><summary>setting "index.store.type: mmapfs" still uses NIOFS backend</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nyaxt</reporter><labels /><created>2012-09-13T04:59:42Z</created><updated>2014-06-29T16:03:35Z</updated><resolved>2012-09-14T12:32:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-09-13T08:44:37Z" id="8521714">@voodoowill mmap is not flawed at all. Windows x64 should use mmap, so the issue is only due to Win32. See Uwe Schindler notes:

http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html

https://issues.apache.org/jira/browse/LUCENE-753?focusedCommentId=12986445&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12986445

My personal comments to mmap: http://jprante.github.com/applications/2012/07/26/Mmap-with-Lucene
</comment><comment author="jprante" created="2012-09-13T13:03:06Z" id="8527325">voodoowill, "mmap" in Java is hidden in MappedByteBuffer and sun.misc.Unsafe - (implementation dependent stuff in the JVM). In Lucene, the only usage is mapping the index files to virtual memory. So "mmap" is just an add-on to NIO.
The reason for keeping NIO is not every JVM and not every OS (e.g. FreeBSD or Mac OS X) support the unmap() function, indicated by the org.apache.lucene.store.MMapDirectory.UNMAP_SUPPORTED flag.
AFAIK most Lucene committers publish their NIO/mmap tests (and regressions) in their Apache Jira ticket system.
</comment><comment author="kimchy" created="2012-09-14T12:32:22Z" id="8559621">@nyaxt I fixed this bug in #2253, missed your pull request, but great catch!. Will be part of 0.19.10.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to add a 0.19.9 node into 0.19.4 cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2248</link><project id="" key="" /><description>Brought up a 0.19.9 node to test with and it refused to join the cluster. 

Error was:

`java.lang.NoSuchMethodError: org.elasticsearch.common.compress.lzf.ChunkEncoder.&lt;init&gt;(I)`

Full backtrace:
https://gist.github.com/3703863
</description><key id="6836026">2248</key><summary>Unable to add a 0.19.9 node into 0.19.4 cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottmac</reporter><labels /><created>2012-09-13T00:38:48Z</created><updated>2012-09-18T09:05:42Z</updated><resolved>2012-09-14T20:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-09-13T03:01:15Z" id="8517007">Same issue with 0.19.8 and 0.19.9 nodes. It's look like a breaking change...
I don't think it can be fixed.
I had to upgrade my java client to 0.19.9.
</comment><comment author="scottmac" created="2012-09-13T03:13:10Z" id="8517153">We have 30 nodes in our cluster, upgrading them all at once is something we try to avoid. Doing one at a time over a period of time is ideal.

I chatted to Shay last week and he said this should have worked.
</comment><comment author="dadoonet" created="2012-09-13T03:24:37Z" id="8517273">Hmmm. Strange. I will rerun my test with 0.19.9 node with 0.19.8 java client and report here in the next days.
</comment><comment author="scottmac" created="2012-09-14T18:46:26Z" id="8571696">I went digging more and looked at the ChunkEncoder.class file, it appears completely different when compile it ourselves.

From our binaries:

```
public class org.elasticsearch.common.compress.lzf.ChunkEncoder {
  public org.elasticsearch.common.compress.lzf.ChunkEncoder(int, org.elasticsearch.common.compress.lzf.BufferRecycler);
  public void close();
  public org.elasticsearch.common.compress.lzf.LZFChunk encodeChunk(byte[], int, int);
  public void encodeAndWriteChunk(byte[], int, int, java.io.OutputStream) throws java.io.IOException;
}
```

From the pre-built binaries:

```
public final class org.elasticsearch.common.compress.lzf.ChunkEncoder {
  public org.elasticsearch.common.compress.lzf.ChunkEncoder(int);
  public static org.elasticsearch.common.compress.lzf.ChunkEncoder nonAllocatingEncoder(int);
  public void close();
  public org.elasticsearch.common.compress.lzf.LZFChunk encodeChunk(byte[], int, int);
  public void encodeAndWriteChunk(byte[], int, int, java.io.OutputStream) throws java.io.IOException;
  protected int tryCompress(byte[], int, int, byte[], int);
}
```
</comment><comment author="scottmac" created="2012-09-14T20:51:23Z" id="8575246">Looks like in 231ea6bea70453d1cbf5b261abfa1f94a65eaf42 when this was abstracted out, some identical class names were added from an external package and the old ones removed.

Fixed this with a:
mvn clean
</comment><comment author="kimchy" created="2012-09-18T09:05:42Z" id="8646146">@scottmac yea..., whenever I build a version, I make sure to use `mvn clean`, it has bitten some users several times. Any reason not to use the "official" releases?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtered query + type filter behaves differently than querying type directly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2247</link><project id="" key="" /><description>There are couple of issues behind the behavior I'm seeing.
1. `filtered` query type + a `type` filter can return different results than a query directed to the type via the URL, if custom analyzers are used, mainly because of:
2. the default analyzer for fields in an index, if not specified, depends on the order in which types were created

Reproducing the issue:

Let's create an index with a custom analyzer:

```
curl -XPUT http://localhost:9200/test -d '
{
  "settings" : {
    "index.analysis.filter.typeahead_ngram.max_gram" : "10",
    "index.analysis.filter.typeahead_ngram.min_gram" : "1",
    "index.analysis.filter.typeahead_ngram.type" : "edgeNGram",
    "index.analysis.filter.typeahead_ngram.side" : "front",
    "index.analysis.analyzer.name_prefix.tokenizer" : "standard",
    "index.analysis.analyzer.name_prefix.filter.2" : "typeahead_ngram",
    "index.analysis.analyzer.name_prefix.filter.0" : "standard",
    "index.analysis.analyzer.name_prefix.filter.1" : "lowercase",
    "index.analysis.analyzer.name_prefix.type" : "custom"
  }
}'
```

Now create a couple of types, the order will matter:

```
curl -XPUT http://localhost:9200/test/document/_mapping -d '
{
  "document": {
    "properties": {
      "title" : {
        "type" : "string",
        "analyzer" : "standard",
        "store" : "yes"
      }
    }
  }
}'

curl -XPUT http://localhost:9200/test/user/_mapping -d '
{
  "user" : {
    "properties" : {
      "title" : {
        "type" : "string",
        "analyzer" : "name_prefix",
        "store" : "yes"
      }
    }
  }
}'
```

Insert some data:

```
curl -XPUT http://localhost:9200/test/user/1 -d '{"title": "Will Jobs"}'
curl -XPUT http://localhost:9200/test/user/2 -d '{"title": "Steve Jobs"}'
```

Now do a filtered query and limit the type:

```
curl -XGET http://localhost:9200/test/_search?pretty=1 -d '
{
  "query": {
    "filtered" : {
      "query": {
          "text": {
            "title": {
              "operator": "and",
              "query": "will jobs"
            }
          }
      },
      "filter": {
        "type": {
          "value": "user"
        }
      }
    }
  },
  "fields": [
    "title"
  ]
}'
```

Output:

```
{
  "took" : 40,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.19178301,
    "hits" : [ {
      "_index" : "test",
      "_type" : "user",
      "_id" : "1",
      "_score" : 0.19178301,
      "fields" : {
        "title" : "Will Jobs"
      }
    }, {
      "_index" : "test",
      "_type" : "user",
      "_id" : "3",
      "_score" : 0.19178301,
      "fields" : {
        "title" : "Steve Jobs"
      }
    } ]
  }
}
```

Now do a query restricted by type in the URL:

```
curl -XGET http://localhost:9200/test/user/_search?pretty=1 -d '
{
 "query": {
    "text": {
      "title": {
        "operator": "and",
        "query": "will jobs"
      }
    }
  },
  "fields": [
    "title"
  ]
}'
```

Results:

```
{
  "took" : 81,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.5424442,
    "hits" : [ {
      "_index" : "test",
      "_type" : "user",
      "_id" : "1",
      "_score" : 0.5424442,
      "fields" : {
        "title" : "Will Jobs"
      }
    } ]
  }
}
```

What happened? Looking at what happened with the analyzers tells the story:

```
curl -XGET 'http://localhost:9200/test/_analyze?pretty=1&amp;field=title' -d 'will jobs'
{
  "tokens" : [ {
    "token" : "jobs",
    "start_offset" : 5,
    "end_offset" : 9,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 2
  } ]
}
```

So the first query that did a `text` query on the `title` field got processed with the `standard` analyzer that is used by the field `document.title`. Since "will" is a stop word, it got removed in the analysis. We search only for the term "jobs" and get two results.

What about if the type and field are specified?

```
curl -XGET 'http://localhost:9200/test/_analyze?pretty=1&amp;field=user.title' -d 'will jobs'
{
  "tokens" : [ {
    "token" : "w",
    "start_offset" : 0,
    "end_offset" : 1,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "wi",
    "start_offset" : 0,
    "end_offset" : 2,
    "type" : "word",
    "position" : 2
  }, {
    "token" : "wil",
    "start_offset" : 0,
    "end_offset" : 3,
    "type" : "word",
    "position" : 3
  }, {
    "token" : "will",
    "start_offset" : 0,
    "end_offset" : 4,
    "type" : "word",
    "position" : 4
  }, {
    "token" : "j",
    "start_offset" : 5,
    "end_offset" : 6,
    "type" : "word",
    "position" : 5
  }, {
    "token" : "jo",
    "start_offset" : 5,
    "end_offset" : 7,
    "type" : "word",
    "position" : 6
  }, {
    "token" : "job",
    "start_offset" : 5,
    "end_offset" : 8,
    "type" : "word",
    "position" : 7
  }, {
    "token" : "jobs",
    "start_offset" : 5,
    "end_offset" : 9,
    "type" : "word",
    "position" : 8
  } ]
}
```

Now we're using the `name_prefix` filter.

In the first query, the query is not being analyzed using the analyzer for `user.title` even though both the analyzer can be inferred from the index, type, and field from the query.

Another problem is that if the creation of the two types are reversed, things behave very differently! It appears the first type created sets the default analyzer for its field names. So if the `user` type is created before `document`, running the first example of the analyze API shows it's using the `name_prefix` analyzer instead of `standard`.
</description><key id="6830822">2247</key><summary>Filtered query + type filter behaves differently than querying type directly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels><label>discuss</label></labels><created>2012-09-12T20:30:51Z</created><updated>2014-07-25T08:14:11Z</updated><resolved>2014-07-25T08:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-12T22:19:21Z" id="8512186">We can only infer the actual `title` field to use if between the two types if you use type in the "url", if its in a filtered query, we can't infer it, so we simply pick one of those and use its analysis settings.
</comment><comment author="jmwilson" created="2012-09-13T01:34:04Z" id="8515834">Is there a way to work around this without multiple requests? Doing something like

```
curl -XGET http://localhost:9200/test/document,user/_search ...
```

is also dependent on the order the types are listed in the URL. Getting the correct analyzer when searching across types and fields is pretty critical for search quality in a lot cases.
</comment><comment author="ghost" created="2012-09-13T06:44:59Z" id="8519575">The feature needed?

http://www.javadocexamples.com/org/apache/lucene/analysis/org.apache.lucene.analysis.PerFieldAnalyzerWrapper.html
</comment><comment author="ghost" created="2012-09-13T07:19:29Z" id="8520116">Maybe I should see if I can find the REST parser code for ES and not assume its capacity for triggered modifications on "occurrenceOf" multiple field constraints in sub-queries... sorry for the swirl.
</comment><comment author="kimchy" created="2012-09-13T12:13:52Z" id="8526276">@jmwilson the way that it might work is to auto generate several queries, each one parsed only with "one" type, and take those analysis considerations that differ per field into account. So, ES will automatically take your query, and create a boolean query out of it, with each should element with your main query parsed "as if" it is running against each type.
</comment><comment author="clintongormley" created="2014-07-25T08:14:11Z" id="50120443">Closed in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reassign Shard Feature Request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2246</link><project id="" key="" /><description>It would be useful to provide a mechanism to reassign shards in a cluster.

== Scenario:
- 10 nodes
- 1 Index with 5 primary shards and 1 replica shard each.

== Initial State:
[Shards Distribution]
Node 1 =&gt; Primary Shard 0
Node 2 =&gt; Primary Shard 1
Node 3 =&gt; Primary Shard 2
Node 4 =&gt; Primary Shard 3
Node 5 =&gt; Primary Shard 4
Node 6 =&gt; Replica Shard 0
Node 7 =&gt; Replica Shard 1
Node 8 =&gt; Replica Shard 2
Node 9 =&gt; Replica Shard 3
Node 10 =&gt; Replica Shard 4

== Shutdown 2 nodes containing the same shard:
[Shards Distribution]
Node 1 =&gt; offline
Node 2 =&gt; Primary Shard 1
Node 3 =&gt; Primary Shard 2
Node 4 =&gt; Primary Shard 3
Node 5 =&gt; Primary Shard 4
Node 6 =&gt; offline
Node 7 =&gt; Replica Shard 1
Node 8 =&gt; Replica Shard 2
Node 9 =&gt; Replica Shard 3
Node 10 =&gt; Replica Shard 4

As the nodes were put offline at the same time, the "primary shard 0" and "replica shard 0" changed its status to UNASSIGNED

== REQUEST
If we add new nodes, and we know that the nodes offline (node 1 and node 6) wont be recovered, we should be able to reassign the UNASSIGNED shards to new nodes.
</description><key id="6828185">2246</key><summary>Reassign Shard Feature Request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">warptrosse</reporter><labels /><created>2012-09-12T18:53:39Z</created><updated>2014-07-08T16:24:32Z</updated><resolved>2014-07-08T16:24:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-13T12:16:12Z" id="8526335">There should be an option, agreed. Actually, it is being worked on as we speak (part of the internal support for it has already been committed). Note though, if you do manage to get node1 or node6 online, or even just another node but with the data directory (for example, if its on EBS on amazon), then it will be automatically recovered.
</comment><comment author="clintongormley" created="2014-07-08T16:24:32Z" id="48363400">This is now available via the `reroute` API: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-reroute.html#cluster-reroute
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bigdesk 'index.html' issue on 0.19.9 ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2245</link><project id="" key="" /><description>Not sure if this is a user error, but after installing bigdesk on elasticsearch 0.19.9, I got a 404 from /_plugin/bigdesk:

```
 % curl -D- http://localhost:9200/_plugin/bigdesk
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0

```

But hitting /index.html worked:

```
% curl -sD- http://localhost:9200/_plugin/bigdesk/index.html | grep bigdesk
    &lt;title&gt;bigdesk&lt;/title&gt;
```

Not sure if this is a regression, but I do remember hitting '/_plugin/bigdesk' before working in 0.19.8
</description><key id="6827976">2245</key><summary>bigdesk 'index.html' issue on 0.19.9 ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels /><created>2012-09-12T18:45:45Z</created><updated>2013-06-10T15:22:04Z</updated><resolved>2013-06-10T15:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-09-12T18:49:24Z" id="8505555">Just add a / at the end of your url : http://localhost:9200/_plugin/bigdesk/
</comment><comment author="spinscale" created="2013-06-10T15:22:04Z" id="19205655">closing as it is fixed in the current release - and even tested
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added `has_parent` filter (#2243)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2244</link><project id="" key="" /><description /><key id="6821248">2244</key><summary>Added `has_parent` filter (#2243)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-09-12T15:12:47Z</created><updated>2015-05-18T23:35:27Z</updated><resolved>2012-09-13T12:09:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-13T12:09:43Z" id="8526179">Pushed!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `has_parent` filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2243</link><project id="" key="" /><description>The `has_parent` filter accepts a query and a parent type. The query is executed in the parent document space, which is specified by the parent type. This filter return child documents which associated parents have matched. For the rest `has_parent` filter has the same options and works in the same manner as the `has_child` filter. 

This is an experimental filter.
## Filter example

```
{
    "has_parent" : {
        "parent_type" : "blog"
        "query" : {
            "term" : {
                "tag" : "something"
            }
        }
    }
}  
```

The `parent_type` field name can also be abbreviated to `type`.
## Memory considerations

With the current implementation, all _id values are loaded to memory (heap) in order to support fast lookups, so make sure there is enough mem for it.

This issue originates from issue #792
</description><key id="6821152">2243</key><summary>Add `has_parent` filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-12T15:09:19Z</created><updated>2012-09-13T12:10:00Z</updated><resolved>2012-09-13T12:10:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-13T12:10:00Z" id="8526190">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed shard while its being the source of relocation can cause inconsistent cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2242</link><project id="" key="" /><description /><key id="6817712">2242</key><summary>failed shard while its being the source of relocation can cause inconsistent cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-12T13:11:20Z</created><updated>2012-09-12T13:14:40Z</updated><resolved>2012-09-12T13:14:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Field boosts not used in multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2241</link><project id="" key="" /><description /><key id="6813654">2241</key><summary>Field boosts not used in multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-12T09:48:19Z</created><updated>2012-09-12T09:48:24Z</updated><resolved>2012-09-12T09:48:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to netty 3.5.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2240</link><project id="" key="" /><description /><key id="6709621">2240</key><summary>Upgrade to netty 3.5.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-07T09:12:22Z</created><updated>2012-09-07T09:16:39Z</updated><resolved>2012-09-07T09:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-07T09:16:39Z" id="8360233">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes/ajax cors [TEST]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2239</link><project id="" key="" /><description>Let's fix the Ajax again :)
</description><key id="6688369">2239</key><summary>Fixes/ajax cors [TEST]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2012-09-06T13:53:27Z</created><updated>2014-06-27T05:44:00Z</updated><resolved>2012-09-06T14:25:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-06T14:25:39Z" id="8333794">Will use the other pull request :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added proper headers for cross-origin resource sharing (CORS) with Ajax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2238</link><project id="" key="" /><description>Previously, when responding to Ajax requests, elasticsearch did not send proper headers for
cross-origin resource sharing (CORS) -- see issues #828, #2186.

With this commit, Ajax requests should be working. Example:

```
jQuery.ajax({
  url: "http://localhost:9200/_search",
  type: "POST",
  contentType: 'application/json; charset=UTF-8',
  success: function(data) { console.log(data) }
});
```

See:
- http://www.nczonline.net/blog/2010/05/25/cross-domain-ajax-with-cross-origin-resource-sharing/
- http://www.w3.org/TR/cors/#access-control-allow-headers-response-header

Closes #2186, fixes #828
</description><key id="6682048">2238</key><summary>Added proper headers for cross-origin resource sharing (CORS) with Ajax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2012-09-06T08:21:04Z</created><updated>2014-06-13T08:32:19Z</updated><resolved>2012-09-06T17:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2012-09-06T17:27:30Z" id="8340382">Closed by elasticsearch/elasticsearch@f659cad8d6cd0821a054bb02f56262cac196b606
</comment><comment author="karussell" created="2013-02-26T16:18:55Z" id="14123426">Shouldn't there a more generic possibility to enable or disable this? 

Or in general: modifying the header should be easier e.g. to challenge a WWW-Authentication, cache control, etc

See also https://github.com/Asquera/elasticsearch-http-basic/issues/3
</comment><comment author="spinscale" created="2013-02-26T16:21:47Z" id="14123590">See #2540
</comment><comment author="karussell" created="2013-02-26T17:37:07Z" id="14128122">Thanks!

Hmmh, it looks like only certain headers are allowed which is a blocking issue for us when trying to implement http basic authorization and CORS.
</comment><comment author="karussell" created="2013-02-27T11:39:46Z" id="14169675">it looks like it is in 0.20 already configurable ... 

```
      if (transport.settings().getAsBoolean("http.cors.enabled", true)) {
            // Add support for cross-origin Ajax requests (CORS)
            resp.addHeader("Access-Control-Allow-Origin", transport.settings().get("http.cors.allow-origin", "*"));
            if (request.getMethod() == HttpMethod.OPTIONS) {
                // Allow Ajax requests based on the CORS "preflight" request
                resp.addHeader("Access-Control-Max-Age", transport.settings().getAsInt("http.cors.max-age", 1728000));
                resp.addHeader("Access-Control-Allow-Methods", transport.settings().get("http.cors.allow-methods", "OPTIONS, HEAD, GET, POST, PUT, DELETE"));
                resp.addHeader("Access-Control-Allow-Headers", transport.settings().get("http.cors.allow-headers", "X-Requested-With, Content-Type, Content-Length"));
            }
        }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document maximum size for bulk indexing over HTTP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2237</link><project id="" key="" /><description>There seems to be a maximum record count for bulk indexing of about 100k records over HTTP. It does not seem to be documented.
</description><key id="6675648">2237</key><summary>Document maximum size for bulk indexing over HTTP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2012-09-05T23:15:00Z</created><updated>2017-05-19T09:54:31Z</updated><resolved>2012-09-06T01:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2012-09-06T00:06:49Z" id="8318304">Nevermind, found it here as a max http request size: http.max_content_length setting
</comment><comment author="Downchuck" created="2012-09-06T00:33:33Z" id="8318772">I've increased the setting but still get the error from Netty: HTTP content length exceeded 104857600 bytes.
This is because: maxContentLength[2.9gb] set to high value, resetting it to [100mb]
Netty expects an integer and checks against Integer.MAX_VALUE, or 2^31-1.

So, basically, 2gb is the maximum size. ES does not process an HTTP request until it completes: for those working with large files, it makes quite a bit more sense to use a streaming API or otherwise run a river on the server. In an example of 2m records at about 2g uncompressed, it took longer to upload through netty than to index: indexing does not start until the http request has completed (which is reasonable).
</comment><comment author="kimchy" created="2012-09-06T14:35:57Z" id="8334207">Yea, it represents the whole request in memory (thats how async IO works, and the fact that it needs to break the bulk request to each shard). You should make sure to break down your indexing into smaller bulk sizes.
</comment><comment author="ghost" created="2012-09-06T16:04:01Z" id="8337470">from netty docs on http://docs.jboss.org/netty/3.2/api/org/jboss/netty/handler/codec/http/HttpMessageDecoder.html

Name    Meaning
maxInitialLineLength    The maximum length of the initial line (e.g. "GET / HTTP/1.0" or "HTTP/1.0 200 OK") If the length of the initial line exceeds this value, a TooLongFrameException will be raised.
maxHeaderSize   The maximum length of all headers. If the sum of the length of each header exceeds this value, a TooLongFrameException will be raised.
maxChunkSize    The maximum length of the content or each chunk. If the content length (or the length of each chunk) exceeds this value, the content or chunk will be split into multiple HttpChunks whose length is maxChunkSize at maximum.

As you can see, just like tomcat, netty supports chunking&#8230;. so async IO may represent the whole request in memory, but not at the Netty level, eh?  Famous bug in tomcat early v 6 around chunking&#8230;..

And since request can be sent in chunks also, there is no system async io that knows if the entire request is complete, it only knows if all tcp segments are received and correct.

will

On Sep 6, 2012, at 10:36 AM, Shay Banon wrote:

&gt; Yea, it represents the whole request in memory (thats how async IO works, and the fact that it needs to break the bulk request to each shard). You should make sure to break down your indexing into smaller bulk sizes.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="kimchy" created="2012-09-06T18:26:30Z" id="8342253">Http chunking, or any chunking in general does not solve "boundaries" problems when it comes to bulk requests. Not saying that its not impossible to try and support chunking with bulk (find boundaries), and then sending those chunks to shards, and keep on doing it and send back the responses in chunks. Its quite complicated though. What you need to do now is simply break the bulk requests yourself to chunks.
</comment><comment author="donzkie" created="2017-05-19T09:54:31Z" id="302660952">I have the same issue, we are indexing and saving documents using **_bulk** endpoint, we knew that the maximum threshold for HTTP request payload is up to 10MB so we chunks our bulk saving up to 8MB but still we got the same exception - below is the sample exception we got from our code:

[POST] URL [https://&lt;RestAPI&gt;/_bulk] 
 RESPONSE_BODY [{"Message":"Request size exceeded 10485760 bytes"}] 
 REQUEST PARAM SIZE [8298283 Bytes] 
 REQUEST_PARAM [{"index":{"_index":"indexname here","_type":"item","_id":"https://..."}}]
 
Removed other information (company information), but as you can see it was throwing an exception that we exceeded the 10MB threshold but request payload that we sent is just around 8MB.

By the way we are using AWS hosted Elasticserch. Hoping to hear from the expert here. Thanks.

Ador
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexMetaData.Builder.Index(String name) isn't renaming this.index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2236</link><project id="" key="" /><description>This code in package org.elasticsearch.cluster.metadata.IndexMetaData.Builder

``` java
public Builder index(String name) {
            this.index = index;
            return this;
        }
```

should be:

``` java
public Builder index(String name) {
            this.index = name;
            return this;
        }
```
</description><key id="6671800">2236</key><summary>IndexMetaData.Builder.Index(String name) isn't renaming this.index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trilokacharya</reporter><labels /><created>2012-09-05T20:14:16Z</created><updated>2012-09-06T14:30:10Z</updated><resolved>2012-09-06T14:30:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Hanging during bulk inserts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2235</link><project id="" key="" /><description>I have a cluster that we have one index / day, 7 nodes and 7 shards 1 replica.  Currently we insert using bulk operations of ~10k docs.  Each doc is very small (&lt;1k).  I've been getting inserts requests hanging across all nodes, and it looks as though the bulk threads are getting blocked on a single node, causing all inserts to hang.  When this happens the node that appears to be gummed up has no IO, and no CPU utilization and still responds to _cluster health requests.

I created a fork and uploaded the jstack for the process in question as it is large:
https://github.com/bradvoth/elasticsearch/blob/master/jstacks/jstack.lockedinBulk
</description><key id="6667525">2235</key><summary>Hanging during bulk inserts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">bradvoth</reporter><labels /><created>2012-09-05T17:24:39Z</created><updated>2013-12-02T09:12:34Z</updated><resolved>2013-12-02T09:12:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2012-09-05T23:11:52Z" id="8317254">I've also been hitting an occasional hang while doing bulk operations of about 10k docs on a data set of about 100m. I've not diagnosed whether this is an issue with curl or ES or the dynamic between the two.
</comment><comment author="Downchuck" created="2012-09-05T23:28:04Z" id="8317573">Do check that you have increased your ulimit settings, as that can easily lead to issues.
</comment><comment author="bradvoth" created="2012-09-06T14:02:24Z" id="8333072">Ulimits are all set properly and no hint of any messages in the logs with default log settings.
</comment><comment author="ghost" created="2012-09-06T15:55:42Z" id="8337129">If you restart all your ES components with the jmx on, there will be a system bean that can lookup any thread deadlocks.

will
On Sep 6, 2012, at 10:02 AM, bradvoth wrote:

&gt; Ulimits are all set properly and no hint of any messages in the logs with default log settings.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="awick" created="2012-09-10T20:09:24Z" id="8436596">Looks like I'm having a similar issue https://github.com/elasticsearch/elasticsearch/issues/2224
</comment><comment author="spinscale" created="2013-10-30T09:20:15Z" id="27374586">hey,

does this still happen with elasticsearch 0.90 - my assumption would be that you might be running out of threads, because earlier versions of elasticsearch were unbounded regarding the creation of new threads by default.

Just guessing here a lot. Anything we can assist in finding this issue, in case it still happens for you?
</comment><comment author="spinscale" created="2013-12-02T09:12:34Z" id="29603209">Closing this due to lack of activity. Very happy to have another look with more information!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed no valid missing index type error.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2234</link><project id="" key="" /><description /><key id="6667409">2234</key><summary>Fixed no valid missing index type error.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-09-05T17:19:36Z</created><updated>2015-05-18T23:41:16Z</updated><resolved>2012-09-06T08:14:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Search on one set of fields and highlight another</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2233</link><project id="" key="" /><description>I have a schema that contains two text fields - A and B. There is one document, which looks like this:

```
A: abc cde zzz
B: xxx zzz
```

When I perform a query "A:abc B:zzz" with highlight option on field A ElasticSearch returns:

```
highlighted A: &lt;em&gt;abc&lt;/em&gt; cde &lt;em&gt;zzz&lt;/em&gt;
B: xxx zzz
```

Is it right that "zzz" is highlighted in field A when I was searching for it in other field? Is there a way to disable this?
</description><key id="6634005">2233</key><summary>Search on one set of fields and highlight another</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SakulK</reporter><labels /><created>2012-09-04T13:03:47Z</created><updated>2013-10-18T10:07:23Z</updated><resolved>2013-10-18T10:07:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Rzulf" created="2012-09-07T10:16:56Z" id="8361472">bump!
</comment><comment author="martijnvg" created="2012-09-10T09:46:37Z" id="8418679">The `require_field_match` option should prevent "zzz" from being highlighted. The option is described at:http://www.elasticsearch.org/guide/reference/api/search/highlighting.html
</comment><comment author="Simek" created="2012-09-10T15:14:35Z" id="8426043">We tried to play with this option, but after setting it to true there were no highlights at all.
</comment><comment author="martijnvg" created="2012-09-10T15:47:56Z" id="8427340">Can you create a gist? (Index a doc and then use highlighting)
</comment><comment author="SakulK" created="2012-09-11T08:05:58Z" id="8450076">https://gist.github.com/3696800

Adding require_field_match results in nothing being highlighted at all. As for the usage of two query_strings it is required in our application because we sometimes need to write something like "zzz OR abc OR qqq" etc.
</comment><comment author="martijnvg" created="2012-09-11T16:22:45Z" id="8463820">The reason why `require_field_match` didn't work in your gist, is because of the fact that you're querying on the `_all` field. Highlighting on field A and querying on the _all field isn't a field match. If you also include the A field in your query string query the `require_field_match` does work. I tested it out:
https://gist.github.com/3699628
</comment><comment author="SakulK" created="2012-09-11T16:30:41Z" id="8463925">Ok but I guess querying on both "_all" and "A" will have a negative effect on the performance of our query right? Or is ES so smart that this shouldn't be the case? :)
</comment><comment author="martijnvg" created="2012-09-12T12:06:58Z" id="8491291">The query will be more expensive. You can also decide to not use the `_all` field and specify the fields you want to use in the query yourself.
</comment><comment author="javanna" created="2013-10-18T10:07:23Z" id="26584737">Closing this one. The way to go is the `require_field_match` option, as @martijnvg said.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes level API: Allow to specify `data:true` to only execute the APIs on data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2232</link><project id="" key="" /><description>Allos to more easily just execute the APIs on just data nodes using the "attribute" notation, for example: `/_nodes/data:true/_stats`.
</description><key id="6622993">2232</key><summary>Nodes level API: Allow to specify `data:true` to only execute the APIs on data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-03T21:57:56Z</created><updated>2012-09-03T22:03:11Z</updated><resolved>2012-09-03T22:03:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added update by query api. #2230</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2231</link><project id="" key="" /><description /><key id="6619425">2231</key><summary>Added update by query api. #2230</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:CRUD</label><label>stalled</label></labels><created>2012-09-03T16:40:55Z</created><updated>2014-11-21T10:35:34Z</updated><resolved>2014-11-21T10:35:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2012-10-22T18:30:30Z" id="9675002">Is it planned to push this one soon?
</comment><comment author="devilankur18" created="2012-11-19T01:00:40Z" id="10498500">Please release this one. This is in big demand 

https://github.com/elasticsearch/elasticsearch/issues/1607
</comment><comment author="ofavre" created="2013-02-13T16:08:21Z" id="13501708">I've packaged it as a plugin: [yakaz/elasticsearch-action-updatebyquery](http://github.com/yakaz/elasticsearch-action-updatebyquery/).
Have fun.
</comment><comment author="ofavre" created="2013-03-01T15:18:53Z" id="14293993">The plugin is now ported to ElasticSearch 0.90.0.Beta1!
</comment><comment author="ofavre" created="2013-03-06T17:23:57Z" id="14513728">I've added some more fields in the context.
See the corresponding [README section](https://github.com/yakaz/elasticsearch-action-updatebyquery#context-variables) and [the commit](https://github.com/yakaz/elasticsearch-action-updatebyquery/commit/fd7896765f17b9a12581a6806dd32f289daf7a97).

This feature would be nice as part of the Update API too, should I make a pull request, or are you planning on integrating Update By Query API and merging it a bit code-wise with the regular Update API?
</comment><comment author="clintongormley" created="2014-08-08T07:57:23Z" id="51573435">Blocked by #6914 
</comment><comment author="tobigue" created="2014-08-13T12:00:04Z" id="52039110">+1 for an update by query possibility
</comment><comment author="jpountz" created="2014-11-21T10:35:34Z" id="63953144">As we want to implement this feature by using a task management infrastructure, I am closing this pull request. Discussion can still continue on #2230.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update by query API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2230</link><project id="" key="" /><description>The update by query API allows all documents that with the query to be updated with a script.  This feature is experimental.
The update by query works a bit different than the delete by query. The update by query api translates the documents that match into bulk index / delete requests. After the bulk limit has been reached, the bulk requests created thus far will be executed. After the bulk requests have been executed the next batch of requests will be prepared and executed. This behavior continues until all documents that matched the query have been processed. The bulk size can be configured with the _action.updatebyquery.bulk_size_ option in the elasticsearch configuration. For example:
`action.updatebyquery.bulk_size=2500`
# Example usage

Index an example document:
curl -XPUT 'localhost:9200/twitter/tweet/1' -d '

```
{
   "text" : {
      "message" : "you know for search"
    }
}
```

Execute the following update by query command:
curl -XPOST 'localhost:9200/twitter/_update_by_query' -d '

```
{
    "query" : {
        "term" : {
            "message" : "you"
        }
    },
    "script" : "ctx._source.field1 += 1"
}
```

This will yield the following response:

```
{
  "ok" : true,
  "took" : 9,
  "total" : 1,
  "updated" : 1,
  "indices" : [ {
    "twitter" : { }
  } ]
}
```

By default no bulk item responses are included in the response. If there are bulk item responses included in the response, the bulk response items are grouped by index and shard. This can be controlled by the `response` option.
# Options:

Additional general options in request body:
- `lang`:  The script language.
- `params`: The script parameters.
## Query string options:
- `replication`:  The replication type for the delete/index operation (sync or async).
- `consistency`: The write consistency of the index/delete operation.
- `response`:  What bulk response items to include into the update by query response. This can be set to the following: `none`, `failed` and `all`. Defaults to none. Warning: `all` can result in out of memory errors when the query results in many hits.
- `routing` : Sets the routing that will be used to route the document to the relevant shard.
- `timeout` : Timeout waiting for a shard to become available.

This is issue originates from #1607
</description><key id="6619380">2230</key><summary>Update by query API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Reindex API</label><label>feature</label></labels><created>2012-09-03T16:37:38Z</created><updated>2016-06-17T12:06:10Z</updated><resolved>2016-03-29T18:21:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mdojwa" created="2012-12-08T09:37:33Z" id="11156898">Hi, when will it be released ?
</comment><comment author="martijnvg" created="2012-12-10T09:46:42Z" id="11187036">Hi @mdojwa Not sure when this will be included into ES. The implementations works fine, but there is one thing that we are missing. An update by query request can take a long time to complete. It would be very help ful to cancel a running update by query request. This isn't implemented yet. Perhaps in the future we might have a process api, where one can see the current running requests and via this api requests can be cancelled.
</comment><comment author="ofavre" created="2013-02-13T16:07:54Z" id="13501678">I've packaged it (pull request #2231) as a plugin: [yakaz/elasticsearch-action-updatebyquery](http://github.com/yakaz/elasticsearch-action-updatebyquery/).
Have fun.
</comment><comment author="mdojwa" created="2013-02-13T20:03:47Z" id="13515065">Hi, thanks for this one :)
</comment><comment author="ofavre" created="2013-03-01T15:18:53Z" id="14293995">The plugin is now ported to ElasticSearch 0.90.0.Beta1!
</comment><comment author="neogenix" created="2013-03-04T02:50:55Z" id="14361855">+1
</comment><comment author="Vineeth-Mohan" created="2013-03-24T17:20:58Z" id="15363992">+1 to this feature. @ofavre - Just one question here. Will all the updates be atomic or sequential ? As in would there be a situation where say half of the documents in the query was updated and the rest was not in case of process crash or restart.
</comment><comment author="ofavre" created="2013-03-24T18:01:38Z" id="15364919">I've just packaged the code, I didn't write it. But the pull request description states that it treats document in batch, hence there definitively can be cases where the documents are help updated.
For small changes, maybe ensuring that all the documents fit inside one batch can help. The batch size is controller by `action.updatebyquery.bulk_size`.
</comment><comment author="Vineeth-Mohan" created="2013-03-25T02:15:19Z" id="15374972">@ofavre  - Thanks Oliver. That answers my question perfectly.
</comment><comment author="parth-j-gandhi" created="2014-06-03T18:47:19Z" id="45004640">This is a much needed feature for my current project which indexes hundreds of thousands of docs and updates them on a regular basis. Is this feature available via elasticsearch-py python client?
</comment><comment author="ofavre" created="2014-06-04T08:15:36Z" id="45062929">There is no chance that this unofficial (or not yet official) feature contributed through a plugin is exposed through a mainstream client library.
And unless this client library lets you write arbitrary calls easily, you're good to go with urllib and the like.
</comment><comment author="1st" created="2014-06-25T10:44:39Z" id="47085965">Hello guys!

As I understand, I can insert JSON data into index, query it, but can't update this JSON record? For example, I need to change structure of all JSON documents in one given collection - and I need to add extra field "is_moderated = true/false". Is it possible to do in current implementation of Elastic Search?

If its not possible now - please help me to find workaround for this common task. Thanks!
</comment><comment author="ofavre" created="2014-06-27T09:21:38Z" id="47323943">You can use the [Update API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-update.html) to update any **single** document.
To update **several** documents at once, the good news is the pull request associated to this issue has been packaged as a plugin : [yakaz/elasticsearch-action-updatebyquery](https://github.com/yakaz/elasticsearch-action-updatebyquery/).
Using it you can run any script you like and modify the document structure, and it will be re-indexed.
</comment><comment author="1st" created="2014-06-27T09:27:45Z" id="47324411">@ofavre as I understand, "update" will reindex full document, not the part of it. Is it correct? I mean, if I have changed only one field in document, does this check all other fields to recreate index, or only affect one field that I changed? Thanks.
</comment><comment author="mdojwa" created="2014-06-27T09:50:05Z" id="47326127">Hi,

Update works like Insert new and delete old. So updating a document
reindexes the whole document.

Best regards.
Marcin Dojwa

2014-06-27 11:28 GMT+02:00 Anton Danilchenko notifications@github.com:

&gt; @ofavre https://github.com/ofavre as I understand, "update" will
&gt; reindex full document, not the part of it. Is it correct? I mean, if I have
&gt; changed only one field in document, does this check all other fields to
&gt; recreate index, or only affect one field that I changed? Thanks.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/2230#issuecomment-47324411
&gt; .
</comment><comment author="1st" created="2014-06-27T10:03:24Z" id="47327416">@mdojwa @ofavre Guys, in SQL databases, and in MongoDB as I know - rendered only changed field. Do you have any plans to implement "partial reindex of document"? It can save a lot of processor usage on these operations.
</comment><comment author="ofavre" created="2014-06-27T10:34:23Z" id="47329668">@1st Lucene does not work this way, hence neither does ElasticSearch.

Updating a document takes its old structure, applies modifications (either using a doc for the Update API, or a script for both Update and Update By Query), indexes, by adding the new document and deleting the old one under the hood.
This is transparent to the user as far as ElasticSearch is concerned.

I understand that this may not be interesting for huge documents, or documents with many nested docs, but that's the way it is in a Lucene world.
</comment><comment author="clintongormley" created="2014-07-18T07:50:49Z" id="49404503">Blocked by #6914
</comment><comment author="niemyjski" created="2014-08-18T18:19:32Z" id="52532912">Any updates on when this might be included?
</comment><comment author="shadow000fire" created="2014-08-20T17:43:21Z" id="52814033">+1
</comment><comment author="pentium10" created="2014-10-17T14:06:26Z" id="59517498">So to understand, this is packaged in the plugin and not available in the standard elasticsearch release? Can somebody confirm this.
</comment><comment author="niemyjski" created="2014-10-17T14:47:56Z" id="59523646">good luck getting a response...
</comment><comment author="shadow000fire" created="2014-10-17T15:01:03Z" id="59525742">Yes, confirmed.  Update by query is currently not a feature of
elasticsearch.  You can get it by installing this plugin.  I believe there
is an open issue to implement this in elasticsearch itself, but I'm not
sure of the issue #.

On Fri, Oct 17, 2014 at 10:48 AM, Blake Niemyjski notifications@github.com
wrote:

&gt; good luck getting a response...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/2230#issuecomment-59523646
&gt; .
</comment><comment author="pentium10" created="2014-10-17T15:05:57Z" id="59526452">I posted a new question on SO, that discusses on margin of this problem: 
[Applying &#8220;tag&#8221; to millions of documents, using bulk/update methods](http://stackoverflow.com/q/26427819/243782)
</comment><comment author="climberwoodi" created="2014-12-10T09:35:20Z" id="66425493">+1
</comment><comment author="leibale" created="2015-01-01T14:34:50Z" id="68487941">+1
</comment><comment author="mrdanadams" created="2015-01-14T02:01:37Z" id="69857572">+1
</comment><comment author="niemyjski" created="2015-01-20T22:16:33Z" id="70745638">+1
</comment><comment author="ejsmith" created="2015-01-20T22:17:09Z" id="70745750">+1
</comment><comment author="pemontto" created="2015-01-27T02:24:33Z" id="71579392">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard Allocation: `index.routing.allocation....` settings do not "remove" the setting on empty string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2229</link><project id="" key="" /><description>For example, `index.routing.allocation.include._name` set to an empty string will not match any node, thus not do anything. It should simply ignore it. For now, one can set it to `*` which will match "all".
</description><key id="6617313">2229</key><summary>Shard Allocation: `index.routing.allocation....` settings do not "remove" the setting on empty string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-03T14:44:00Z</created><updated>2012-09-03T14:44:31Z</updated><resolved>2012-09-03T14:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to mvel 2.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2228</link><project id="" key="" /><description /><key id="6604918">2228</key><summary>Upgrade to mvel 2.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-02T19:28:48Z</created><updated>2012-09-02T19:29:06Z</updated><resolved>2012-09-02T19:29:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update Settings API: Allow body request to be wrapped with `settings` element to conform with other APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2227</link><project id="" key="" /><description>See #2225.
</description><key id="6603425">2227</key><summary>Update Settings API: Allow body request to be wrapped with `settings` element to conform with other APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-09-02T15:39:17Z</created><updated>2012-09-02T15:40:05Z</updated><resolved>2012-09-02T15:40:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix reversed ShingleFilter constructor arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2226</link><project id="" key="" /><description>The `ShingleTokenFilterFactory` in ElasticSearch reverses the `minShingleSize` and `maxShingleSize` arguments to the underlying Lucene `ShingleFilter`

For reference, here is a script that reproduces the issue clearly (tested against 0.19.9): https://gist.github.com/3558149
</description><key id="6587705">2226</key><summary>Fix reversed ShingleFilter constructor arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kennknowles</reporter><labels /><created>2012-08-31T20:00:02Z</created><updated>2014-06-15T17:06:56Z</updated><resolved>2012-08-31T20:25:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-31T20:25:25Z" id="8204012">Applied to both 0.19 and master, cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyzer added via Update Settings (on temporarily closed index) not found by Put Mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2225</link><project id="" key="" /><description>Hello, I am using version 0.19.8 but I do not see a mention of this in the 0.19.9 release notes so I presume it remains true.

I have two gists which I believe are intended to be equivalent, but they are not:
- (Succeeds) Create an index with a custom analyzer, and refer to in mapping: https://gist.github.com/3555567
- (Fails) Create an index, close it, add a custom analyzer, open it, and then refer to it in a mapping: https://gist.github.com/3555574

The failure is `{"error":"MapperParsingException[Analyzer [my_analyzer] not found for field [custom_field]]","status":400}`

By the way, the only place I found documented that _close and _open are necessary is http://elasticsearch-users.115913.n3.nabble.com/Adding-analyzers-during-runtime-td3412592.html
</description><key id="6583586">2225</key><summary>Analyzer added via Update Settings (on temporarily closed index) not found by Put Mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kennknowles</reporter><labels /><created>2012-08-31T16:46:27Z</created><updated>2012-09-02T15:39:35Z</updated><resolved>2012-09-02T15:39:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-02T15:39:35Z" id="8223026">The problem is that the format of the update settings API body is not correct, it includes the `settings` top level element, where it shouldn't. Once you remove it, things will work. 

To be honest, I agree its confusing, I will open an issue to support also top level `settings` element: #2227.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Under moderate write only load a single shard goes crazy with high number of open file descriptors and multi hour flush time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2224</link><project id="" key="" /><description>I experience almost nightly a single shard that hangs and causes the rest of the shards in the index to hang and all bulk inserts to hang.  This hang lasts for multiple hours, and usually fixes it self when my daily script runs that optimizes the PREVIOUS daily index.

My application uses daily indexes, is http bulk inserting when this happens with NO reads, and the shard it happens to is usually not on a node that the application is connected to via http.

Last night it happen again and I collected hopefully some useful data.
- lsof shows 28k open file descriptors, all the other nodes have around 1.2k
- Over 30k files in the sessions-120831/3/index directory.
- Over 5600 segments in that directory
- Total size of all files is 21 gigabytes
- The hang usually happens around 4 hours AFTER the index is created.  I'm not sure if that is how long it takes before some autoflush happens or what
- Only thing in that nodes log is when the index is created at 0:00 GMT

[2012-08-30 20:00:00,221][DEBUG][index.gateway            ] [moloches-m10a] [sessions-120831][3] starting recovery from local ...
[2012-08-30 20:00:00,227][DEBUG][index.gateway            ] [moloches-m10a] [sessions-120831][3] recovery completed from local, took [6ms]
    index    : files           [0] with total_size [0b], took[0s]
             : recovered_files [0] with total_size [0b]
             : reusing_files   [0] with total_size [0b]
    start    : took [6ms], check_index [0s]
    translog : number_of_operations [0], took [0s]
- Similar for master node just has 
  [2012-08-30 20:00:00,096][INFO ][cluster.metadata         ] [moloches-m02a] [sessions-120831] creating index, cause [auto(bulk api)], shards [10]/[0], mappings [session]
- Hot threads for that bad node shows
  30.0% (150ms out of 500ms) cpu usage by thread 'elasticsearch[moloches-m10a][bulk][T#986]'
   10/10 snapshots sharing following 9 elements
     org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:513)
     org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)
     org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)
     org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
     java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
     java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
     java.lang.Thread.run(Unknown Source)
  
  30.0% (150ms out of 500ms) cpu usage by thread 'elasticsearch[moloches-m10a][bulk][T#796]'
   10/10 snapshots sharing following 9 elements
     org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:513)
     org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)
     org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)
     org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
     java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
     java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
     java.lang.Thread.run(Unknown Source)
  
  10.0% (50ms out of 500ms) cpu usage by thread 'elasticsearch[moloches-m10a][bulk][T#857]'
   10/10 snapshots sharing following 9 elements
     org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:513)
     org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)
     org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)
     org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
     org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
     java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
     java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
     java.lang.Thread.run(Unknown Source)
- If I try and do a flush or any other operation it says that a flush is already in progress on that node
- If I try and shutdown that node it just ignores it (would be useful to log if a node is ignoring a shutdown request and why)
</description><key id="6581562">2224</key><summary>Under moderate write only load a single shard goes crazy with high number of open file descriptors and multi hour flush time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2012-08-31T15:19:32Z</created><updated>2013-03-11T19:46:31Z</updated><resolved>2013-03-11T19:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-31T18:55:19Z" id="8201711">How many nodes are you running? Is there a shard being stuck in recovery by any chance from the mentioned shard 3 of the specified index? Can you issue a `jstack` on the node and gist the response?
</comment><comment author="awick" created="2012-09-08T11:30:27Z" id="8386737">15 (64G) machines running 30 nodes (22G Elastic Search Memory)

Right now i was using 10 shards (with "index.routing.allocation.total_shards_per_node" :  1) if I increase the number of shards used the problem doesn't seem to happen as much.  Which I guess is an ok solution, but still seems like there is some deadlock issue since I'm not doing too much.

I'm pretty sure not iops or cpu related since these are decent machines only running elasticsearch.

If I try and flush while this hang is happening I get
"reason" : "BroadcastShardOperationFailedException[[sessions-120908][2] ]; nested: RemoteTransportException[[moloches-m15b][inet[/X.X.X.X:9301]][indices/flush/s]]; nested: FlushNotAllowedEngineException[[sessions-120908][2] Already flushing...]; "

I got a jstack while the hang was happening. https://gist.github.com/3673632

Tons of entries like

"elasticsearch[moloches-m15b][bulk][T#3597]" daemon prio=10 tid=0x00002aaad5aa0800 nid=0x4b94 waiting for monitor entry [0x0000000043fbd000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3562)
        - waiting to lock &lt;0x000000034c25d708&gt; (a org.apache.lucene.index.XIndexWriter)
        at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3552)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2067)
        at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)
        - locked &lt;0x000000030e0a9a40&gt; (a java.lang.Object)
        at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
        - &lt;0x000000054f85c4e0&gt; (a java.util.concurrent.locks.ReentrantLock$NonfairSync)

"elasticsearch[moloches-m15b][bulk][T#3561]" daemon prio=10 tid=0x00002aaad4256000 nid=0x4a53 in Object.wait() [0x00000000476d6000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.lucene.index.IndexWriter$FlushControl.waitUpdate(IndexWriter.java:4884)
        - locked &lt;0x00000003110100e8&gt; (a org.apache.lucene.index.IndexWriter$FlushControl)
        at org.apache.lucene.index.IndexWriter$FlushControl.waitUpdate(IndexWriter.java:4878)
        - locked &lt;0x00000003110100e8&gt; (a org.apache.lucene.index.IndexWriter$FlushControl)
        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:751)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2060)
        at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)
        - locked &lt;0x000000030e0a7ac0&gt; (a java.lang.Object)
        at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
</comment><comment author="dadoonet" created="2012-09-08T12:10:21Z" id="8386972">Just a question outside the scope of your concern : why do you launch 2 nodes per machine ?

IMHO, the "best" setup is to have one node per machine with one single shard (or replica) per node.
This node should have about half of the total physical memory. Also a single node will open less files than two.
</comment><comment author="awick" created="2012-09-08T12:35:32Z" id="8387148">My app is high write, extremely low read (with mullti second query times normal), and most importantly high facet using.  That last part is key, it means that elastic needs the memory, not the OS disk cache.  I run multiple nodes since I want to use UseCompressedOops, since I assume (hopefully correctly) that will leave memory memory for facets.
</comment><comment author="awick" created="2012-09-08T17:08:29Z" id="8391543">So after several hours it still didn't recover.

I did a curl -XPOST http://localhost:9201/_cluster/nodes/_local/_shutdown

which stopped the HTTP interface but didn't stop the process

[2012-09-08 12:54:38,524][INFO ][cluster.service          ] [moloches-m15b] removed {[moloches-m15a][K9NUdozvShuhrnaC8axN8w][inet[/X.X.X.X:9300]]{max_local_storage_nodes=2},}, reason: zen-disco-receive(from master [[moloches-m03a][22ar_oX6S4qvv37_KIuEHQ][inet[/X.X.X.X:9300]]{max_local_storage_nodes=2}])
[2012-09-08 12:54:46,664][INFO ][action.admin.cluster.node.shutdown] [moloches-m15b] shutting down in [200ms]
[2012-09-08 12:54:46,867][INFO ][action.admin.cluster.node.shutdown] [moloches-m15b] initiating requested shutdown...
[2012-09-08 12:54:46,868][INFO ][node                     ] [moloches-m15b] {0.19.9}[11618]: stopping ...
[2012-09-08 12:56:24,702][INFO ][node                     ] [moloches-m15b] {0.19.9}[11618]: closing ...
[2012-09-08 12:56:24,737][DEBUG][discovery.zen.fd         ] [moloches-m15b] [master] stopping fault detection against master [[moloches-m03a][22ar_oX6S4qvv37_KIuEHQ][inet[/X.X.X.X:9300]]{max_local_storage_nodes=2}], reason [zen disco stop]
[2012-09-08 12:56:25,718][DEBUG][action.bulk              ] [moloches-m15b] [sessions-120908][2] failed to execute bulk item (index) index {[sessions-120908][session][120908-UxuT1bVU_J1O24xuOSgwy3s1], source[{"fp":1347095588,"lp":1347095595,"a1":2894203953,"g1":"USA","p1":39596,"a2":3305523277,"g2":"TUN","p2":11301,"pr":17,"pa":8,"by":2640,"db":2576,"ps":[2648320844911980,2648320896463782,2648320897664843,2648320962516726,2648320965823569,2648320966451606,2648321009095386,2648321009962153],"ta":[2,4,25675],"fs":[38538]}]}
org.elasticsearch.index.engine.IndexFailedEngineException: [sessions-120908][2] Index failed for [session#120908-UxuT1bVU_J1O24xuOSgwy3s1]
    at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:506)
    at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:321)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:158)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.lucene.store.AlreadyClosedException: this Analyzer is closed
    at org.apache.lucene.analysis.Analyzer.getPreviousTokenStream(Analyzer.java:93)
    at org.apache.lucene.analysis.snowball.SnowballAnalyzer.reusableTokenStream(SnowballAnalyzer.java:109)
    at org.elasticsearch.index.analysis.NamedAnalyzer.reusableTokenStream(NamedAnalyzer.java:79)
    at org.elasticsearch.index.analysis.FieldNameAnalyzer.reusableTokenStream(FieldNameAnalyzer.java:60)
    at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:132)
    at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:276)
    at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:766)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2060)
    at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)
    at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:493)
    ... 7 more
[2012-09-08 12:56:25,725][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException
    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1768)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.start(AbstractNioWorker.java:184)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:330)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:313)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:35)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:637)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:504)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:47)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:659)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:578)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:712)
    at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:679)
    at org.elasticsearch.common.netty.channel.AbstractChannel.write(AbstractChannel.java:246)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:104)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler$1.onFailure(TransportShardReplicationOperationAction.java:232)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$3.onClose(TransportShardReplicationOperationAction.java:497)
    at org.elasticsearch.cluster.service.InternalClusterService.add(InternalClusterService.java:180)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.retry(TransportShardReplicationOperationAction.java:485)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:538)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)

I eventually kill -9 the process, and restart it.  Which causes it to hang right away.  I stop the process, remove the shard off the disk, copy a empty version on top, and now everything is fine.
</comment><comment author="awick" created="2012-09-09T11:28:51Z" id="8402781">Spoke to soon about raising the shards helping.  I went to 30 shards and this morning woke up to another flush hang.  Different machine/shard number.

curl http://localhost:9200/_flush\?pretty
{
  "ok" : true,
  "_shards" : {
    "total" : 366,
    "successful" : 365,
    "failed" : 1,
    "failures" : [ {
      "index" : "sessions-120909",
      "shard" : 23,
      "reason" : "BroadcastShardOperationFailedException[[sessions-120909][23] ]; nested: FlushNotAllowedEngineException[[sessions-120909][23] Already flushing...]; "
    } ]
  }
}
</comment><comment author="awick" created="2012-09-09T12:59:50Z" id="8403422">Here is the status for the shard that is hung.  Merge section looks bad - 6 current merges for 12.2 hours (the shard was created a little less then 13 hours ago)

23: [
{
routing: {
state: STARTED
primary: true
node: P9HExwI2TqOYRszOsTeGmQ
relocating_node: null
shard: 23
index: sessions-120909
}
state: STARTED
index: {
size: 9.8gb
size_in_bytes: 10612211214
}
translog: {
id: 1347148801443
operations: 25759
}
docs: {
num_docs: 7410594
max_doc: 7410594
deleted_docs: 0
}
merges: {
current: 6
current_docs: 54
current_size: 149.4kb
current_size_in_bytes: 153024
total: 49880
total_time: 12.2h
total_time_in_millis: 44246930
total_docs: 45288021
total_size: 64.4gb
total_size_in_bytes: 69236703248
}
refresh: {
total: 394937
total_time: 5.1h
total_time_in_millis: 18469193
}
flush: {
total: 1288
total_time: 2.7m
total_time_in_millis: 167848
}

Here is similar output for a shard that isn't having the issue.  The merge section looks fine.
22: [
{
routing: {
state: STARTED
primary: true
node: 2GcOSrjdQm25k37jpUVXCA
relocating_node: null
shard: 22
index: sessions-120909
}
state: STARTED
index: {
size: 9.8gb
size_in_bytes: 10568233662
}
translog: {
id: 1347148801525
operations: 90
}
docs: {
num_docs: 7414334
max_doc: 7414334
deleted_docs: 0
}
merges: {
current: 0
current_docs: 0
current_size: 0b
current_size_in_bytes: 0
total: 50044
total_time: 31m
total_time_in_millis: 1863813
total_docs: 45742651
total_size: 65.1gb
total_size_in_bytes: 69905301434
}
refresh: {
total: 399587
total_time: 33.9m
total_time_in_millis: 2039999
}
flush: {
total: 1307
total_time: 2.5m
total_time_in_millis: 155000
}
}
]
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use the java args to enable HTTP proxying etc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2223</link><project id="" key="" /><description>The plugin downloader uses HTTP(S) to download new plugins, but does not include support for environments where HTTP proxies are required.

This patch simply enables the use of the standard JAVA_ARGS environment variable when the java command is called in the script.

Thus you may enable HTTP proxying as so prior to running the plugin script:

export JAVA_ARGS=-Dhttp.proxyHost=host -Dhttp.proxyPort=3128 -Dhttps.proxyHost=host -Dhttps.proxyPort=3128
</description><key id="6581265">2223</key><summary>Use the java args to enable HTTP proxying etc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrisalexander</reporter><labels /><created>2012-08-31T15:08:00Z</created><updated>2014-07-07T06:33:32Z</updated><resolved>2013-06-26T13:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-26T13:57:18Z" id="20048851">Called differently (JAVA_OPTS), but included as of https://github.com/elasticsearch/elasticsearch/commit/cf7ebfcebf50ad596bf5d6f373da94f01500daf9
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added a global ignore_malformed index setting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2222</link><project id="" key="" /><description>Add the following option:
index.mapping.ignore_malformed=[true|false]
Setting this to true will ignore malformed values for nummeric based fields (such as long, date and ip fields).

Relates to issue #2220
</description><key id="6580248">2222</key><summary>Added a global ignore_malformed index setting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-31T14:31:09Z</created><updated>2015-05-18T23:41:17Z</updated><resolved>2012-08-31T20:13:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-31T20:13:53Z" id="8203704">Pushed to master (0.20)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw MapperParserException if trying to parse value as object.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2221</link><project id="" key="" /><description>When a value type is passed to the object parser it will call nextToken() until it finds an END_OBJECT token (an END_OBJECT for another object). When nested in an array the serializeArray call never exits (the extra nextToken() in parse forwards the stream out of the array).

Example:

```
{
  "object": {
    "array":[
    {
      "object": { "value": "value" }
    },
    {
      "object":"value"
    }
    ]
  },
  "value":"value"
}
```

This fix makes the object parser throw a MapperParsingException if a value type is passed to it.
</description><key id="6580078">2221</key><summary>Throw MapperParserException if trying to parse value as object.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lindstromhenrik</reporter><labels /><created>2012-08-31T14:25:25Z</created><updated>2014-07-16T21:54:48Z</updated><resolved>2012-09-05T09:43:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-05T09:43:03Z" id="8292726">Pushed to master (0.20) and 0.19 branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: `index.mapping.ignore_malformed` global index setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2220</link><project id="" key="" /><description>Add `index.mapping.ignore_malformed` index level setting that will automatically set it to all mappings (unless explicitly set).
## Original Request

Discussion is here: https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/Q4AL6rh0c3c
It would be nice to have a setting for object type fields with dynamic properties to ignore malformed values, i.e store them without raising an exception.
</description><key id="6570952">2220</key><summary>Mapping: `index.mapping.ignore_malformed` global index setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rauanmayemir</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-08-31T04:31:02Z</created><updated>2012-11-05T17:18:22Z</updated><resolved>2012-08-31T20:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-31T20:15:34Z" id="8203761">Pushed @martijnvg changes.
</comment><comment author="rauanmayemir" created="2012-09-02T12:50:11Z" id="8221616">I didn't get it.
Ignore malformed - means it will ignore the values (i.e not store them) or it will store it and ignore that it is malformed?
</comment><comment author="kimchy" created="2012-09-02T15:23:26Z" id="8222874">they will be "stored" as the json is stored. They will not be indexed though so you can't search them.
</comment><comment author="rauanmayemir" created="2012-09-02T15:25:10Z" id="8222897">got it. so I can't also facet over them?
</comment><comment author="kimchy" created="2012-09-02T15:59:24Z" id="8223199">only over the ones that ended up being indexed, not the "malformed" values.
</comment><comment author="eldilibra" created="2012-11-05T16:36:46Z" id="10077367">@kimchy Has this been brought into the latest stable release? (0.19.11 at time of comment) Also, am I correct in thinking that simply adding index.mapping.ignore_malformed to my elasticsearch.yml will allow me to take advantage of it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard Relocation / initialization / repair progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2219</link><project id="" key="" /><description>I have a cluster that is scaling into the 10s-100sTB of data and any time I take a node down for repair (hardware, os, etc) I'm always waiting and wondering how long till we're green again.  Any thoughts on what it would take to add progress indicators of some sort to the API for these processes?
</description><key id="6562842">2219</key><summary>Shard Relocation / initialization / repair progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bradvoth</reporter><labels /><created>2012-08-30T19:40:35Z</created><updated>2014-07-08T16:22:26Z</updated><resolved>2014-07-08T16:22:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bradvoth" created="2012-08-30T20:05:56Z" id="8172529">Additionally it would be nice to see queued or planned actions.  We currently, because of the size of our shards, only allow 2 relocations to happen at once, but typically a re-balancing takes moving more than just 2 shards, so it would be nice to understand what the remaining steps would be.
</comment><comment author="clintongormley" created="2014-07-08T16:22:26Z" id="48363091">Recovery can now be monitored with the `cat-recovery` API: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-recovery.html#cat-recovery

Pending tasks can also be seen with the `cat-pending-tasks` API: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-pending-tasks.html#cat-pending-tasks althought this won't include future relocations as these are decided only when there are threads available to act on the decisions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange problem with searching across multiple types of one index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2218</link><project id="" key="" /><description>I have two elasticsearch indexes and many types in them.
And there's some strange behaviour.
My index "fts" has the folowing types: 'category', 'product', 'blog_entry', 'comment', 'forum'.
I am searching for some keyword and I know that some documents of 'comment' type contains needed keyword or phrase.
If I do request like:
curl -XGET "http://127.0.0.1:9200/fts/comment/_search" -d "{my_query}" 
OR
curl -XGET "http://127.0.0.1:9200/fts/category,comment/_search" -d "{my_query}"
I recieve expected documents.

But if I do:
curl -XGET "http://127.0.0.1:9200/fts/category,product,comment/_search" -d "{my_query}"
OR
curl -XGET "http://127.0.0.1:9200/fts/_all/_search" -d "{my_query}"
OR EVEN
curl -XGET "http://127.0.0.1:9200/fts/_search" -d "{my_query}"
I recieve: {"took":32,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

Also, such problem appeared recently and suddenly, when I was using elasticsearch 0.19.2. I decided to upgrade it to 0.19.9, but it didn't help.
Also, such strange behaviour is taking place on our production server, but on local system everything works as expected.
</description><key id="6556620">2218</key><summary>Strange problem with searching across multiple types of one index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">unikoid</reporter><labels /><created>2012-08-30T15:37:25Z</created><updated>2014-07-08T16:13:38Z</updated><resolved>2014-07-08T16:13:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="unikoid" created="2012-08-31T14:46:08Z" id="8194259">1 minute ago I tryed to delete index completely and reindex all my data from db again. It helps. But I'm afraid that an issue can appear again soon.
</comment><comment author="kimchy" created="2012-09-05T09:45:55Z" id="8292806">If you can get a recreation, it would help to try and solve this. What you do should work, i.e. searching across all types. One possible reason for this is maybe some types share the same field name, but with different field type? For example, you have an age field in two types, where in one its a string, and in the other its numeric? In this case, without specifying the type, ES will pick one of those to build the query.
</comment><comment author="unikoid" created="2012-09-07T06:26:37Z" id="8357121">No, there are no types that share the same field name, but with different field type. I will try to provide a testcase, but now I can't provide all our data, it's hundreds of thousands of docs.
</comment><comment author="steeve" created="2012-10-30T18:09:04Z" id="9916662">Same issue here :(

`type1,type2` works as expected, but `type2,type1` returns no results.

Note that each type has it's own analyzer (multilang).
</comment><comment author="kimchy" created="2012-11-01T21:30:28Z" id="9996726">When you do "cross" type search, the first field that we find for the type we use its mapping definition to construct the query. There has been requests for a different behavior, using boolean logic between fields of different types, but for now, you can simply search on all types, and have your query be type specific on the same field (i.e. `type1.field1`, `type2.field1`).
</comment><comment author="bclozel" created="2013-02-19T14:02:10Z" id="13773632">Got caught by this one.
Had a different mapping configuration on types (with same field name).

Getting different results for `type1,type2` and `type2,type1` feels wrong, but it makes sense once you figure it out.

Fixing mappings should do the trick.
</comment><comment author="jbarata" created="2013-03-07T15:36:59Z" id="14567310">Hi, 
I think I'm having the same problem.

We have a simple search page (with an input field) where a user can search for anything in our 2 types of docs (client and invoice). 
A `cliente` doc is like `{"name":"john"}` and an invoice doc is like `{"number":"123", "client":{"name":"john"}}`

The output of the search, beside de docs, consists of a grand total and byType totals.

If the search consists of only the text to be found, for eg. `john`, than all the docs are shown and all the totals are OK.
If the search is like `name:john` than the results are NOT OK (it returns the only client doc or the invoice doc, depending on the types order that goes to ElasticSearch); also the totals are incorrect.

To reproduce the situation execute the following curls against your ElasticSearch:

```
Create the index `testindex`
curl -XPUT 'http://127.0.0.1:9602/testindex'

Create a type `client` where docs have a root field `name`
curl -XPUT 'http://127.0.0.1:9602/testindex/client/1' -d '{"name":"john"}'

Create a type `invoice` where docs have a root field `number` and an object `client` with a `name` field
curl -XPUT 'http://127.0.0.1:9602/testindex/invoice/1' -d '{"number":"123", "client":{"name":"john"}}'



Now, if one searches for "name:john" in the `client` type we get the `client` doc
curl -XGET 'http://127.0.0.1:9602/testindex/client/_search?q=name:john'

if the search is made in the `invoice` type we get the `invoice` doc
curl -XGET 'http://127.0.0.1:9602/testindex/invoice/_search?q=name:john'

if the search is made in both types with `client` before `invoice` we get only the `client` doc
curl -XGET 'http://127.0.0.1:9602/testindex/client,invoice/_search?q=name:john'

if we swap the types order (`invoice` before `client`) we get only the `invoice` doc
curl -XGET 'http://127.0.0.1:9602/testindex/invoice,client/_search?q=name:john'
```

I would expect that the searches made on both types would return both docs but that's not happening.

I guess it may have to do with the mappings.

Is there a way to overcome this problem? (without manually extract the fields from the user query and build a multi-match query as suggested in https://groups.google.com/forum/#!topic/elasticsearch/-RZtZykZq5o) 

Thanks in advance!
</comment><comment author="dadoonet" created="2013-03-07T20:23:11Z" id="14583894">@jbarata you can search on all types:

``` sh
curl -XGET 'http://localhost:9200/testindex/_search?q=john&amp;pretty'
```

produces:

``` javascript
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.5945348,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "client",
      "_id" : "1",
      "_score" : 0.5945348, "_source" : {"name":"john"}
    }, {
      "_index" : "testindex",
      "_type" : "invoice",
      "_id" : "1",
      "_score" : 0.37158427, "_source" : {"number":"123", "client":{"name":"john"}}
    } ]
  }
}
```

Here is a full recreation: https://gist.github.com/dadoonet/5111167
</comment><comment author="jbarata" created="2013-03-07T22:26:13Z" id="14590412">@dadoonet 
thank you very much for taking the time to test this.

You might have not noticed in the beginning of my post but i have done that test with success (when the query has no field filter everything works fine)

&gt; If the search consists of only the text to be found, for eg. john, than all the docs are shown and all the totals are OK.

The problem is when the query has a field filter that exists in different levels of the JSONs, as is the `name` field in my example.

cheers!
</comment><comment author="jbarata" created="2013-03-12T00:50:36Z" id="14752567">Also, @kimchy solution would do just fine for now but it seems not to work (at least in this case)

&gt; you can simply search on all types, and have your query be type specific on the same field (i.e. type1.field1, type2.field1).

If I use `client.name` as in
`curl -XGET 'http://127.0.0.1:9602/testindex/client,invoice/_search?q=client.name:john'`
I'll get the `invoice` doc no matter the order of the `invoice` and `client` types. 
With `client.name` I will only get the client doc if the search is made only on the `client`type (I would be happy if the result was only the client doc whatever the types order is). 

If I use `invoice.client.name` then it will always return the invoice as expected

Is there a problem with the subpath to the field being equal in both types?
Is there a way to force field full path by configuration or query parameter (searched a lot but could not find a way to do this)

Thanks
</comment><comment author="jbarata" created="2013-03-12T22:00:54Z" id="14807266">Hi again!
just to let you guys now that I managed to put it returning the correct results if I "promote" the types to be separated indexes and ignore the types altogether, i.e having one type per index.

Do you think there will be any performance problems with this approach?
Thanks
</comment><comment author="kimchy" created="2013-03-12T22:03:13Z" id="14807506">@jbarata don't think you will notice a performance difference, and yea, the reason why it happens is that ES needs to decide on what to search, either `name` or `customer.name`, and thats picked based on the first type match
</comment><comment author="jbarata" created="2013-03-12T22:15:03Z" id="14808717">Ok @kimchy :+1: 

If I find any problems implementing this approach I'll report it here so it helps others in any way :)

Thanks a lot for your help.
</comment><comment author="clintongormley" created="2013-03-13T13:00:34Z" id="14839603">I'm wondering if we should make the field "chooser" more predictable, eg choosing the field that starts with a type name before a field that doesn't, so `customer.name` should match type `customer`, field `name` before it matches type `invoice`, field `customer.name`.
</comment><comment author="clintongormley" created="2014-07-08T16:13:38Z" id="48361854">Closed in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deleting all indexes via _all stopped working between 0.19.8 and 0.19.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2217</link><project id="" key="" /><description>After upgrading to `0.19.9`, [Elastisch](http://clojureelasticsearch.info) test suite that resets indexes by specifying index name as `_all` now fails. ElasticSearch responds with 404s:

```
curl -XDELETE 'http://localhost:9200/_all?pretty=true'
{
  "error" : "IndexMissingException[[_all] missing]",
  "status" : 404
}
```

I am not sure if this change is intentional but looks like `_all` should not be treated as a regular index name. `action.disable_delete_all_indices` is not set in my config.
</description><key id="6552449">2217</key><summary>Deleting all indexes via _all stopped working between 0.19.8 and 0.19.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">michaelklishin</reporter><labels /><created>2012-08-30T12:55:25Z</created><updated>2013-05-16T14:56:01Z</updated><resolved>2012-08-30T13:45:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaelklishin" created="2012-08-30T13:01:19Z" id="8158531">This also affects Delete via Query when index name is specified as `_all`.
</comment><comment author="Paikan" created="2012-08-30T13:42:36Z" id="8159671">I think it has been fixed in master branch check issue #2205
</comment><comment author="michaelklishin" created="2012-08-30T13:45:42Z" id="8159775">Yes, looks like a duplicate of #2205. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[FIX] Added checking for Java on the system in `bin/elasticsearch`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2216</link><project id="" key="" /><description>When Java wasn't installed/available in the system, a confusing "permission denied" error
was diplayed to users:

```
user@server:/home/ubuntu/elasticsearch-0.19.9# ./bin/elasticsearch -f
./bin/elasticsearch: 122: exec: : Permission denied
```

This patch fixes the issue by checking for:

a) Java script existence based on the $JAVA environment variable,
b) executability of the Java script

The following [Vagrant](http://vagrantup.com) configuration was used to set up a clean Linux environment:

``` ruby
Vagrant::Config.run do |config|
  config.vm.box = "precise64"

  config.vm.provision :shell, :inline =&gt; &lt;&lt;-BOOTSTRAP
    sudo apt-get install unzip -y
    sudo apt-get install vim -y
    sudo apt-get install curl -y

    wget https://github.com/downloads/elasticsearch/elasticsearch/elasticsearch-0.19.9.zip
    unzip elasticsearch-0.19.9.zip
  BOOTSTRAP
end
```
</description><key id="6547447">2216</key><summary>[FIX] Added checking for Java on the system in `bin/elasticsearch`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2012-08-30T08:00:56Z</created><updated>2014-06-16T23:46:22Z</updated><resolved>2012-09-06T17:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2012-09-06T17:23:31Z" id="8340276">Closed by elasticsearch/elasticsearch@c834bca43c3bf882eb8a95c20e9aa82886e4c789
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>every node for itself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2215</link><project id="" key="" /><description>Setup : 

```
5 similar nodes : 

btrainer-1.182  (192.168.1.182) (Current Master before incident) 
btrainer-1.186 (192.168.1.186)
btrainer-1.136  (192.168.1.136)
btrainer-13.137 (192.168.13.137)
btrainer-1.138  (192.168.1.138)
```

ES Configs : (version : 0.19.8)

```
cluster.name: btrainer
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: [ "192.168.1.182:10300", "192.168.1.186:10300", "192.168.1.136:10300", "192.168.13.137:10300", "192.168.1.138:10300" ]
http.port: 10200
index.number_of_replicas: 4
transport.tcp.port: 10300
```

Java Options : 

```
-Des-foreground=yes 
-Des.path.home=/elasticsearch 
-Xms4096m 
-Xmx20480m 
-Djline.enabled=true 
-XX:+UseParNewGC 
-XX:+UseConcMarkSweepGC 
-XX:+CMSParallelRemarkEnabled 
-XX:SurvivorRatio=8 
-XX:MaxTenuringThreshold=1 
-XX:CMSInitiatingOccupancyFraction=75 
-XX:+UseCMSInitiatingOccupancyOnly 
-cp /elasticsearch/lib/*:/elasticsearch/lib/sigar/* 
org.elasticsearch.bootstrap.ElasticSearch
```

Problem :

This problem repeats itself every 5-12 hours period. When everything running smoothly (cluster is green) 1 node goes down and everynode creates its own cluster (not 1/4 split, 1/1/1/1/1 split). The sample problem happened exactly at 22:06, we have a job checking cluster state every minute. This cluster mainly used for training so we have heavy traffic spikes on both reads and writes when jobs are triggered (also some continious small reads). 

1) What happened to btrainer-1.138 ?
2) Even if 1 node (btrainer-1.138) behaves irrationally why didn't the cluster split by 1/4; why did other nodes lose the master btrainer-1.182 ?

Logs :

you can check the logs from the nodes : https://gist.github.com/3510448
</description><key id="6523984">2215</key><summary>every node for itself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">frail</reporter><labels /><created>2012-08-29T11:00:22Z</created><updated>2013-06-07T16:51:51Z</updated><resolved>2013-06-07T16:51:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T16:15:41Z" id="19116935">Hey, does this still happen to you or have you found the issue?

Your gist is not available anymore to take a closer look.
</comment><comment author="frail" created="2013-06-07T16:51:46Z" id="19118995">we found out that on busy cluster data nodes can have long garbage collections, thus we created separate master nodes (1 master for each data node on same machine as data node but with different port) for leader selection purposes only to avoid any gc from splitting our cluster. 

so yes, we found out the issue is about long gc, and we avoid it with master only nodes. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support XML as a content type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2214</link><project id="" key="" /><description>YML is now supported as a content type; the Jackson library supports XML.

The YML patch sets an easy precedent to support XML input:
https://github.com/elasticsearch/elasticsearch/commit/dbe2f53a0082bd5c90e8b4d1b8e6eb3cae91e5e8

While optimized indexes should not use XML, various steps on the indexing process would certainly benefit from the ease of use of simply inserting XML documents into the index. And this support is already in the Jackson dependency.

Generally, XML attributes are prefixed with "@" and mixed nesting results in array fields with some elements containing a key of "#text" (representing the current depth). These practices have matured.
</description><key id="6519845">2214</key><summary>Support XML as a content type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels><label>discuss</label></labels><created>2012-08-29T06:49:09Z</created><updated>2014-07-25T08:12:53Z</updated><resolved>2014-07-25T08:12:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T08:12:53Z" id="50120326">this is available as a community plugin: https://github.com/jprante/elasticsearch-xml
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Permit full accuracy of results when limiting the size of a facet query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2213</link><project id="" key="" /><description>Fixed issue #1832

The current method used to implement a size limit on a facet query favors performance over accuracy. This is a result of the size limit being applied per shard before aggregation of the results.

I modified class org.elasticsearch.search.facet.terms.strings.TermsStringOrdinalsFacetCollector to check for a new configuration parameter and if enabled, override this behavior to return all of the facets in the sorted order on a per shard basis. The merge facet process will still apply the size limit to return the top N facets based on the counts.

The result of this is that the client receives full accuracy with a size limited facet query.
</description><key id="6517375">2213</key><summary>Permit full accuracy of results when limiting the size of a facet query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tzhou</reporter><labels><label>discuss</label></labels><created>2012-08-29T02:31:59Z</created><updated>2014-07-25T08:11:56Z</updated><resolved>2014-07-25T08:11:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-09-04T14:03:29Z" id="8263849">I think that a possible better solution is to expose a "shard_size" parameter in the facet request, so one can ask for 10 terms, but each shard will send back "shard_size" response.
</comment><comment author="tlaukkanen" created="2013-07-30T06:31:08Z" id="21772098">@kimchy that wouldn't provide full accuracy either unless you used your full document count as shard_size? I'd vote for same approach as Solr has which @jaysonminard mentioned on issue #1832:

&gt; Solr provides accuracy here with a 2nd call to shards to calculate counts found on share A but so far missing from B,C,...Z. And vis versa.
</comment><comment author="clintongormley" created="2014-07-25T08:11:56Z" id="50120229">Closed in favour of #1832
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Queries fail when value of field is 'IN'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2212</link><project id="" key="" /><description>When indexing US States using their two character representation, we discovered that we cannot find addresses in Indiana (IN).  I have replicated the issue using the following curl commands:

```
curl -XPUT localhost:9200/address/1 -d '{"state" : "IN"}'
curl -v 'http://localhost:9200/address/_search' -d '{"query":{"text":{"state":"IN"}}}}'
curl -v 'http://localhost:9200/address/_search' -d '{"query":{"term" : { "state" : "IN" }}}'
```
</description><key id="6477895">2212</key><summary>Queries fail when value of field is 'IN'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkroll</reporter><labels /><created>2012-08-27T16:26:26Z</created><updated>2013-06-26T15:21:53Z</updated><resolved>2013-06-26T15:21:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2012-08-27T17:41:41Z" id="8063804">Be careful "in" is a stopword maybe you should consider changing analyzer
for the state field?

On Mon, Aug 27, 2012 at 6:26 PM, Rich Kroll notifications@github.comwrote:

&gt; When indexing US States using their two character representation, we
&gt; discovered that we cannot find addresses in Indiana (IN). I have replicated
&gt; the issue using the following curl commands:
&gt; 
&gt; curl -XPUT localhost:9200/address/1 -d '{"state" : "IN"}'
&gt; curl -v 'http://localhost:9200/address/_search' -d '{"query":{"text":{"state":"IN"}}}}'
&gt; curl -v 'http://localhost:9200/address/_search' -d '{"query":{"term" : { "state" : "IN" }}}'
&gt; 
&gt;  &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2212.

## 

Benjamin DEVEZE
</comment><comment author="spinscale" created="2013-06-26T15:21:53Z" id="20055032">Hey,

@Paikan is right, use the analyze API in order to understand how data gets indexed

```
curl -X POST 'localhost:9200/_analyze?analyzer=standard' -d 'IN'
{"tokens":[]}
```

You may want to set the state field to `not_analyzed` at all in your mapping (or analyzed as `keyword`), and to a term query/filter on it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added fields option to explain api. #2203</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2211</link><project id="" key="" /><description /><key id="6472430">2211</key><summary>Added fields option to explain api. #2203</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-27T12:32:39Z</created><updated>2015-05-18T23:35:32Z</updated><resolved>2012-08-31T20:22:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-31T20:22:02Z" id="8203915">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ignore_indices option pull request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2210</link><project id="" key="" /><description /><key id="6470945">2210</key><summary>ignore_indices option pull request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-27T10:53:04Z</created><updated>2015-05-18T23:41:18Z</updated><resolved>2012-08-27T12:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-27T12:46:31Z" id="8054648">Pushed, cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ignore_indices option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2209</link><project id="" key="" /><description>The `ignore_indices` determines what type of indices to exclude from a request. The option can have the following values:
- `none`: No indices / aliases will be excluded from a request.
- `missing`: Indices / aliases that are missing will be excluded from a request.

The `ignore_indices` option applies on the following APIs: 
- count
- search
- multi search
- clear cache
- flush
- gateway snapshot
- optimize
- refresh
- segments
- stats
- status
- validate query

This issue is related to #2167
</description><key id="6470872">2209</key><summary>Add ignore_indices option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-08-27T10:48:04Z</created><updated>2013-02-10T12:04:41Z</updated><resolved>2012-08-27T12:46:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-27T12:46:43Z" id="8054653">Pushed.
</comment><comment author="mahdeto" created="2013-02-10T10:31:14Z" id="13347010">worth adding a "missing_some" option, which can ignore some indices in an index list but fails if none of them exists (specially helpful in case of daily logs where some days have no logs).
</comment><comment author="synhershko" created="2013-02-10T11:07:17Z" id="13347348">This is exactly what "missing" does, it will fail if none exist
</comment><comment author="mahdeto" created="2013-02-10T11:55:51Z" id="13347842">not really. on 20.4 it returns results equivalent of querying with no indices (all indices)

you can reproduce as within the following gist:
https://gist.github.com/mahdeto/4749348
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Heap size overflow and dump when updating replicate and running facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2208</link><project id="" key="" /><description>I decided to run a terms facet search on 20M records about a minute after sending an update to increase replication from zero to one. It lead to ES running out of HEAP space and dumping an hprof.  The out of memory errors were completely reasonable given the limited RAM. It did seem to stall replication on the secondary node.
</description><key id="6464457">2208</key><summary>Heap size overflow and dump when updating replicate and running facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2012-08-27T00:46:49Z</created><updated>2014-07-08T16:07:03Z</updated><resolved>2014-07-08T16:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2012-08-27T00:47:36Z" id="8044955">May be related: org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: transport content length received [1.8gb] exceeded [906.6mb]
</comment><comment author="clintongormley" created="2014-07-08T16:07:03Z" id="48360840">Terms facet can be very expensive because it loads all the field values into memory (esp expensive with high cardinality string fields).  Hopefully the field data circuit breaker would prevent such things happening in recent versions.

Please reopen if you see similar problems.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>- Explain API open engine searcher open / close issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2207</link><project id="" key="" /><description /><key id="6462464">2207</key><summary>- Explain API open engine searcher open / close issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-26T22:28:28Z</created><updated>2015-05-18T23:41:20Z</updated><resolved>2012-08-27T09:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-27T09:20:21Z" id="8050792">Pushed, cheers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain api opens 2 engine searchers, but closes only 1 engine searcher, causing resource leak</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2206</link><project id="" key="" /><description /><key id="6462336">2206</key><summary>Explain api opens 2 engine searchers, but closes only 1 engine searcher, causing resource leak</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-08-26T22:21:25Z</created><updated>2012-08-27T09:21:42Z</updated><resolved>2012-08-27T09:21:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.19.9 no longer supports /_all/_status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2205</link><project id="" key="" /><description>curl http://localhost:9200/_all/_status
{"error":"IndexMissingException[[_all] missing]","status":404}

Works in 0.19.8

Looks like /_status does work, is /_all/_status no longer supported on purpose?  http://www.elasticsearch.org/guide/reference/api/admin-indices-status.html  kind of says it should still work, but not clear.
</description><key id="6445259">2205</key><summary>0.19.9 no longer supports /_all/_status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-08-24T21:55:09Z</created><updated>2012-08-25T11:18:50Z</updated><resolved>2012-08-25T11:18:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-25T11:18:16Z" id="8022347">Yea, its a bug (though I personally like the non `_all` version). Will fix it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed issue #2197</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2204</link><project id="" key="" /><description /><key id="6437798">2204</key><summary>Fixed issue #2197</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-24T16:04:04Z</created><updated>2014-07-16T21:54:51Z</updated><resolved>2012-08-24T21:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-24T21:51:20Z" id="8015891">Pushed, thanks!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add fields options to explain api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2203</link><project id="" key="" /><description>The fields option allows to return the document (or part of it) the explain has been computed for.
`fields`: Specify what fields or part of the _source should be added to the explain response.
</description><key id="6434636">2203</key><summary>Add fields options to explain api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-08-24T14:20:52Z</created><updated>2012-08-31T20:22:31Z</updated><resolved>2012-08-31T20:22:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-31T20:22:31Z" id="8203929">Applied.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local node master listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2202</link><project id="" key="" /><description>- Fixed an issue where dynamic update to minimum_master_nodes settings would not take immediate effect
- Added LocalNodeMasterListener support to the ClusterService. Enables listening to when the local node becomes/stopped being a master
</description><key id="6421394">2202</key><summary>Local node master listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2012-08-23T22:26:54Z</created><updated>2014-07-16T21:54:51Z</updated><resolved>2012-08-24T00:26:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-24T00:26:51Z" id="7988823">Push, cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk UDP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2201</link><project id="" key="" /><description>A Bulk UDP service is a service listening over UDP for bulk format requests. The idea is to provide a low latency UDP service that allows to easily index data that is not of critical nature.

The Bulk UDP service is disabled by default, but can be enabled by setting `bulk.udp.enabled` to `true`.

The bulk UDP service performs intenral bulk aggregation of the data and then flushes it based on several parametres:
- `bulk.udp.bulk_actions`: The number of actions to flush a bulk after, defaults to `1000`.
- `bulk.udp.bulk_size`: The size of the current bulk request to flush the request once exceeded, defaults to `5mb`.
- `bulk.udp.flush_interval`: An interval after which the current request is flushed, regarldess of the above limits. Defaults to `5s`.
- `bulk.udp.concurrent_requests`: The number on max in flight bulk requests allowed. Defaults to `4`.

The network settings allowed are:
- `bulk.udp.host`: The host to bind to, defualts to `network.host` which defaults to any.
- `bulk.udp.port`: The port to use, defaults to `9700-9800`.
- `bulk.udp.receive_buffer_size`: The receive buffer size, defaults to `10mb`.

Here is an example of how it can be used:

```
&gt; cat bulk.txt
{ "index" : { "_index" : "test", "_type" : "type1" } }
{ "field1" : "value1" }
{ "index" : { "_index" : "test", "_type" : "type1" } }
{ "field1" : "value1" }

&gt; cat bulk.txt | nc -w 0 -u localhost 9700
```
</description><key id="6418388">2201</key><summary>Bulk UDP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-08-23T20:16:44Z</created><updated>2014-02-27T19:57:07Z</updated><resolved>2012-08-23T20:18:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-09-05T15:14:49Z" id="8302302">as the udp is not reliable,so does that mean the data maybe losed for some reason? is that right?
@kimchy 
anyway,this feature is very suitable for indexing logging data,cheers :) 
</comment><comment author="kimchy" created="2012-09-06T14:28:37Z" id="8333931">Yes, UDP comes with its downsides, but sometimes its acceptable.
</comment><comment author="vbichkovsky" created="2012-10-04T12:22:48Z" id="9139401">Hello!
I tried to import 100 documents from a text file with "cat ...|nc ..." (as mentioned in documentation), but only small portion of these was imported. In the log I see org.elasticsearch.ElasticSearchParseException and sometimes org.elasticsearch.common.jackson.core.JsonParseException. JSON data is valid, separated by \n. The error occurs only when I try to import several documents at once, like 6 of them. Sending each document (+meta) in a separate datagram works. Elasticsearch version is 0.19.10, I can provide example document if necessary. Is it a well-known limitation or an issue?
</comment><comment author="kimchy" created="2012-10-04T16:33:40Z" id="9147844">@vbichkovsky it should work, can you provide an example?
</comment><comment author="vbichkovsky" created="2012-10-04T17:35:42Z" id="9149978">The file: http://pastie.org/4909910
Log output: http://pastie.org/4909902

Commands I typed:
curl -XDELETE http://localhost:9292/tests
cat data-file | nc -w 0 -u localhost 9797

I'm using a virtual box (debian squeeze) set up via Vagrant. Ports 9200 and 9700 are forwarded to 9292 and 9797. Elasticsearch was installed using .deb package downloaded from the site.
</comment><comment author="kimchy" created="2012-10-04T18:37:07Z" id="9152136">I can see that the message sent is broken down into two, one in the size of `1024` bytes, and then the rest. The parsing fails on the second chunk of the message. We set properly the receive buffer size (as far as I can see) for the UDP socket, so my first guess is that nc has a send buffer size of 1024 maybe? If not, then the other option is that the receive buffer that we set is not properly set for some reason. Still need to chase it down if its on the nc level or not, a simple way is to write a udp client with proper settings and see if it fails then, will try and do it in the coming days, but if you can try on your end it would help speed things up.
</comment><comment author="vbichkovsky" created="2012-10-05T09:56:45Z" id="9170695">You were right, it is netcat.
I wrote a small Ruby script (http://pastie.org/4913960) to check what is sent by netcat, and, indeed, first packet is 1024 bytes, then comes the rest.
Unfortunately, there is no option in netcat to increase it, here is related thread: http://unix.derkeiler.com/Mailing-Lists/FreeBSD/questions/2010-02/msg00747.html
Btw, if I have questions about Elasticsearch, where is the best place to ask them?
Are you going to release a book about it? :)
</comment><comment author="kimchy" created="2012-10-05T14:25:55Z" id="9177188">@vbichkovsky luck guess on my end regarding netcat, thanks for checking it out!, Questions on ES are best asked on the ES google group. We lurk there quite a bit, or on IRC (but sometimes, you might need to ping us when we are online...).
Book is planned for sure!, working on _starting_ to get the process going on one.
</comment><comment author="tianchu" created="2014-02-27T19:57:07Z" id="36284085">Hi @kimchy, I'm currently using 0.90.10. To enable this udp bulk api, I only need to add bulk.udp.enabled: true in the elasticsearch.yml file? Just wanna double check, since I'm not seeing bulk.udp.enabled in the elasticsearch.yml file shipped with ES installation. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Local node master listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2200</link><project id="" key="" /><description>- Fixed an issue where dynamic update to minimum_master_nodes settings would not take immediate effect
- Added LocalNodeMasterListener support to the ClusterService. Enables listening to when the local node becomes/stopped being a master
</description><key id="6417630">2200</key><summary>Local node master listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2012-08-23T19:45:18Z</created><updated>2014-06-21T22:22:36Z</updated><resolved>2012-08-23T22:25:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>- Add ignore_indices option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2199</link><project id="" key="" /><description>Related to #2167
</description><key id="6405928">2199</key><summary>- Add ignore_indices option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-23T12:02:23Z</created><updated>2015-05-18T23:35:34Z</updated><resolved>2012-08-27T10:58:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Reduce index.shard.recovery.concurrent_streams from 5 to 3 to reduce the load when doing recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2198</link><project id="" key="" /><description>the `index.shard.recovery.concurrent_streams` controls how many streams are opened from a recovery source to a recovery target to transfer index files. Reduce it from `5` to `3` to reduce the load when doing recovery (for example, due to relocation).

Note, recent changes in network buffering will mean that recovery will progress considerably faster, so this change will not affect recovery times.
</description><key id="6404746">2198</key><summary>Reduce index.shard.recovery.concurrent_streams from 5 to 3 to reduce the load when doing recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-23T11:09:28Z</created><updated>2012-08-23T12:31:35Z</updated><resolved>2012-08-23T12:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-23T12:31:35Z" id="7967983">implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexMissingException when calling _mlt on client node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2197</link><project id="" key="" /><description>I think I've found a bug with ElasticSearch with _mlt and node clients.

We're running 0.19.8.

We have two data nodes and each app server has its own ES instance running in node client mode. Searches and normal document GETs work fine, however when running _mlt on a client node, I get an IndexMissingException. The exact same query works fine on the data node. I suspect _mlt is only checking the local _mlt

To sum it up:

On data node:
$ curl http://localhost:9200/foo/bar/1 # Works
$ curl http://localhost:9200/foo/bar/1/_mlt # Works

On client node:
$ curl http://localhost:9200/foo/bar/1 # Works
$ curl http://localhost:9200/foo/bar/1/_mlt # Fails with IndexMissingException

Full gist showing curl verbose output at https://gist.github.com/3431254

Originally raised on group at https://groups.google.com/forum/#!starred/elasticsearch/iFGSXu1e77Q
</description><key id="6395750">2197</key><summary>IndexMissingException when calling _mlt on client node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">madlep</reporter><labels><label>bug</label><label>v0.19.10</label><label>v0.20.0.RC1</label></labels><created>2012-08-23T02:06:30Z</created><updated>2012-08-25T03:58:32Z</updated><resolved>2012-08-24T21:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-08-24T15:50:08Z" id="8005405">I was able to reproduce this issue as well. The issue is that the mlt request should be executed on the client node, but it should be redirected to data node that actually has the index available locally.
</comment><comment author="kimchy" created="2012-08-24T21:52:00Z" id="8015904">Fixed by @martijnvg.
</comment><comment author="chendo" created="2012-08-25T03:58:32Z" id="8019893">Thanks for the quick turnaround!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to netty 3.5.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2196</link><project id="" key="" /><description /><key id="6393822">2196</key><summary>Upgrade to netty 3.5.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-22T23:29:09Z</created><updated>2012-08-22T23:31:13Z</updated><resolved>2012-08-22T23:31:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Local node master listener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2195</link><project id="" key="" /><description>- Fixed an issue when the minimum_master_nodes is updated dynamically it takes immediate effect on the cluster state.
- Added LocalNodeMasterListener which enables listening to when the local node becomes a master or stops being one 
</description><key id="6388237">2195</key><summary>Local node master listener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2012-08-22T20:37:23Z</created><updated>2014-07-16T21:54:53Z</updated><resolved>2013-05-29T16:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-29T16:06:38Z" id="18627367">Closing as this is in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MinimumNumberShouldMatch inconcistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2194</link><project id="" key="" /><description>I ran into this while using the API - the same parameter is named differently throughout the API. We currently have 
- "minimum_number_should_match" and the api method minimumNumberShouldMatch(string) 
- "minimum_should_match" and the api method minimumShouldMatch(String)
- "minimum_match" and the api method minimumMatch(int)

yet the last one has a slightly different semantic since it is more explicit, I still want to discuss to make this consistent since they all have the same purpose and having 3 different names is confusing. I suggest do use the "minimumShouldMatch" version consistently and deprecate the other methods in 0.19 we certainly need support minimum_number_should_match as JSON keys for compatibility

comments welcome
</description><key id="6371305">2194</key><summary>MinimumNumberShouldMatch inconcistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-22T07:55:35Z</created><updated>2012-08-22T12:15:15Z</updated><resolved>2012-08-22T12:15:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-22T11:01:32Z" id="7930866">I agree, even more, for example, we can add support for the custom syntax in the `bool` query, which we don't currently.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enabled the option of configuring plugin types in the settings. This wil...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2193</link><project id="" key="" /><description>...l also help in tests when testing plugin related functionality
</description><key id="6355647">2193</key><summary>Enabled the option of configuring plugin types in the settings. This wil...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2012-08-21T16:43:02Z</created><updated>2014-07-16T21:54:53Z</updated><resolved>2012-08-21T21:01:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-21T21:01:57Z" id="7916047">Pushed, cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect application of boost in SpanNotQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2192</link><project id="" key="" /><description>Boost is only getting applied when it is equals to -1. Shouldn't it be the inverse?

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/SpanNotQueryBuilder.java#L65
</description><key id="6340126">2192</key><summary>Incorrect application of boost in SpanNotQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels><label>bug</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-21T00:57:02Z</created><updated>2012-08-21T11:08:36Z</updated><resolved>2012-08-21T11:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-21T11:08:07Z" id="7897868">Nice catch, fixing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2191</link><project id="" key="" /><description>#2184
</description><key id="6329226">2191</key><summary>Explain api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-20T16:08:37Z</created><updated>2015-05-18T23:41:21Z</updated><resolved>2012-08-21T12:19:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-21T12:19:06Z" id="7899188">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support YAML as content type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2190</link><project id="" key="" /><description>Allow to both index yaml request (less interesting...), but also return REST responses in YAML (a bit more readable than json). For REST API, either specify the `Content-Type` header, or just pass `format` as a URI parameter.
</description><key id="6328497">2190</key><summary>Support YAML as content type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-08-20T15:39:48Z</created><updated>2012-08-20T15:50:46Z</updated><resolved>2012-08-20T15:50:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-20T15:50:46Z" id="7873734">Implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>All Field: Automatically detect when field level boosting is used, and optimize when its not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2189</link><project id="" key="" /><description>We can optimize the case where no field level boosting is done where we can execute a more performant term queries.
</description><key id="6324795">2189</key><summary>All Field: Automatically detect when field level boosting is used, and optimize when its not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-20T13:07:06Z</created><updated>2012-08-20T13:07:36Z</updated><resolved>2012-08-20T13:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>mapping BigDecimal to double or string in XContentBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2188</link><project id="" key="" /><description>A pull request on demand for supporting BigDecimal in XContentBuilder
https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/H74o1mNZXPk%5B1-25%5D
</description><key id="6324473">2188</key><summary>mapping BigDecimal to double or string in XContentBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-08-20T12:48:08Z</created><updated>2014-06-17T16:08:14Z</updated><resolved>2012-08-21T12:24:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-21T12:24:31Z" id="7899289">added, cheers!
</comment><comment author="djpentz" created="2012-10-08T09:37:15Z" id="9220615">I looked in the source for 0.19.9 and this code is still not there in XContentBuilder.field(). When will it be released?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Object type not correctly guessed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2187</link><project id="" key="" /><description>When I try to index data with mixed types, I get this error:

```
$ curl -XPUT 'http://localhost:9200/test/fooo/1' -d '{"mixed": [1, 2, "trois", "quatre"]}'
{"error":"MapperParsingException[Failed to parse [mixed]]; nested: NumberFormatException[For input string: \"trois\"]; ","status":400}
```

But if I had first explicitly set a string mapping type, it would have worked fine :

```
$ curl -XPUT 'http://localhost:9200/test/fooo/_mapping' -d '{ "fooo" : { "properties" : { "mixed" : {"type" : "string"} } }}'
{"ok":true,"acknowledged":true}
$ curl -XPUT 'http://localhost:9200/test/fooo/1' -d '{"mixed": [1, 2, "trois", "quatre"]}'
{"ok":true,"_index":"test","_type":"fooo","_id":"1","_version":1}
```

Shouldn't the first try have guessed the string type mapping ?
</description><key id="6319980">2187</key><summary>Object type not correctly guessed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">A21z</reporter><labels /><created>2012-08-20T07:55:02Z</created><updated>2014-07-08T16:03:36Z</updated><resolved>2014-07-08T16:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-21T12:36:22Z" id="7899538">its tricky :), we guess the type based on the first value (we pull parse the json), so guessing it based on subsequent values in an array is problematic. It would certainly give better user experience though, just tricky on the implementation side.
</comment><comment author="clintongormley" created="2014-07-08T16:03:36Z" id="48360378">There is no easy, correct and performant automatic solution to this.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `Access-Control-Allow-Headers` for browsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2186</link><project id="" key="" /><description>When accessing elasticsearch from JavaScript, the request fails if it contains a `Content-Type` header. To reproduce the issue, open a browser console on this page and paste (assuming elasticsearch is running on `localhost:9200`):

```
jQuery.ajax({url: "http://localhost:9200/_search", type: "POST", contentType: 'application/json; charset=UTF-8', success: function(data) { console.log(data) }})
```

The current code adds a [`Access-Control-Allow-Origin`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java#L77) header for browsers, but not a [`Access-Control-Allow-Headers`](http://www.w3.org/TR/cors/#access-control-allow-headers-response-header).
</description><key id="6298991">2186</key><summary>Add `Access-Control-Allow-Headers` for browsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2012-08-17T21:03:31Z</created><updated>2012-09-06T15:21:10Z</updated><resolved>2012-09-06T15:21:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geo Bounding box returning no results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2185</link><project id="" key="" /><description>I've created the following mapping and followed several examples on using geo_bounding_box, including those in the book but none of them return any results when filtering using this condition:

Mapping: https://gist.github.com/3378435

Query:

```
curl -XGET 'http://127.0.0.1:9200/carlsberg/places/_count' -d '{"filtered":{"query":{"match_all":{}},"filter":{"geo_bounding_box":{"Place.location":{"top_left":"40.73,-74.1","bottom_right":"40.717,-73.99"}}}}}'
```

Results:

```
{"count":0,"_shards":{"total":5,"successful":5,"failed":0}}
```

A plain "_search" has the following Place.location which can be found in between the bounding box:

```
location: {lat: "40.12", lon: "-71.34"}
```

I've basically searched everywhere and my last resort was to create this ticket. Tested against versions 0.19.0 and 0.19.8
</description><key id="6288082">2185</key><summary>Geo Bounding box returning no results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lorenzo</reporter><labels /><created>2012-08-17T12:26:38Z</created><updated>2012-08-17T12:35:41Z</updated><resolved>2012-08-17T12:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lorenzo" created="2012-08-17T12:35:41Z" id="7816644">Deleting temporarily... I'm validating again that all examples I used are actually contained in the bounding box
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2184</link><project id="" key="" /><description>The explain api computes a score explanation for a query and a specific document. This can give useful feedback weither a document matches or didn't match a specific query.
## Usage

Full query example:

```
curl -XGET http://localhost:9200/twitter/tweet/1/_explain -d '
{
        "term" : { "message" : "search" }
}
'
```

This will yield the following result:

```
{
  "ok" : true,
  "matches" : true,
  "explanation" : {
    "value" : 0.15342641,
    "description" : "fieldWeight(message:search in 0), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "tf(termFreq(message:search)=1)"
    }, {
      "value" : 0.30685282,
      "description" : "idf(docFreq=1, maxDocs=1)"
    }, {
      "value" : 0.5,
      "description" : "fieldNorm(field=message, doc=0)"
    } ]
  }
}
```

There is also a simpler way of specifying the query via the `q` parameter.
The specified q parameter value is then parsed as if the `query_string` query was used.
Example usage of the `q` parameter in the explain api:

```
curl -XGET http://localhost:9200/twitter/tweet/1/_explain?q=message:search
```

This will yield the same result as the previous request.
## All parameters
- `routing` - Controls the routing in the case the routing was used during indexing.
- `parent` - Same effect as setting the `routing` parameter.
- `preference` - Controls on which shard the explain is executed.
- `source` - Allows the data of the request to be put in the query string of the url.
- `q` - The query string (maps to the query_string query).
- `df`\- The default field to use when no field prefix is defined within the query. Defaults to _all field.
- `analyzer` - The analyzer name to be used when analyzing the query string. Defaults to the analyzer of the _all field.
- `analyze_wildcard` - Should wildcard and prefix queries be analyzed or not. Defaults to false.
- `lowercase_expanded_terms` - Should terms be automatically lowercased or not. Defaults to true.
- `lenient` - If set to true will cause format based failures (like providing text to a numeric field) to be ignored.  Defaults to false.
- `default_operator` - The default operator to be used, can be AND or OR. Defaults to OR.
</description><key id="6287308">2184</key><summary>Explain API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-17T11:30:16Z</created><updated>2013-08-14T15:15:21Z</updated><resolved>2012-08-21T12:19:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-21T12:19:36Z" id="7899195">Pushed on #2184.
</comment><comment author="jonshea" created="2012-08-22T17:33:36Z" id="7942303">Amazing. Thank you!
</comment><comment author="gpstathis" created="2012-08-24T02:46:13Z" id="7990727">Nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyzed wildcard always uses OR operator on split terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2183</link><project id="" key="" /><description>Query string queries ignore the default operator on terms split from an analyzed prefix query.

``` sh
curl -XGET 'localhost:9200/test/doc/_search?pretty=true' -d '{
  "query": {
    "query_string": {
      "query": "apples-oranges*",
      "default_operator": "and",
      "analyze_wildcard": true
    }
  }
}'
```

The above will search for apples OR oranges, even though the default operator is set to AND.

This gist will reproduce the bug, showing that the above query will match a document with just "apples" or just "oranges":
https://gist.github.com/3375594
</description><key id="6280826">2183</key><summary>Analyzed wildcard always uses OR operator on split terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">dylanahsmith</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2012-08-17T03:17:43Z</created><updated>2016-04-13T13:43:51Z</updated><resolved>2016-04-13T13:43:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dylanahsmith" created="2012-08-17T04:11:20Z" id="7807961">The problem can be seen in [line 533 of MapperQueryParser.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/apache/lucene/queryParser/MapperQueryParser.java#L533) which creates a boolean query out of the analyzed terms using a hardcoded `BooleanClause.Occur.SHOULD`.  The occur should be changed to MUST when the default_operator is AND.
</comment><comment author="dylanahsmith" created="2012-08-18T05:41:43Z" id="7842018">A workaround for this issue is to use `"minimum_should_match": "100%"`
</comment><comment author="kimchy" created="2012-08-21T12:44:08Z" id="7899735">I am not too sure what the default value should be, I see cases where actually using `should` regardless of what you set in the operator make sense..., trying to think whats best to do here..., possibly another flag?
</comment><comment author="clintongormley" created="2014-07-25T07:15:31Z" id="50115739">I think that stacked tokens (tokens in the same position) should be OR'ed, while tokens in different positions should be AND'ed.
</comment><comment author="clintongormley" created="2014-07-25T08:09:25Z" id="50120041">In fact the wildcard should only apply to the term (or terms) in the last position.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Org. Elasticsearch. ElasticSearchInterruptedException  custom-filters-score-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2182</link><project id="" key="" /><description>Hello, I use in the application of custom - filters - score - query achieve business requirements, similarity calculation. But I found, two machines running for a period of time, of which one will search performance fell sharply, 
Obviously see IO, speaking, reading and writing down, finally the following anomaly, but found that stop off one of the machine and returned to normal. 

The 2012-08-16 10:46:37, 048 ERROR filter. ExceptionFilter - [DUBBO] Got unchecked and undeclare service method invoke exception: null, DUBBO version: 2.0.14, current host: 172.22.28.88 
Org. Elasticsearch. ElasticSearchInterruptedException 
At org. Elasticsearch. Action. Support. AdapterActionFuture. ActionGet (AdapterActionFuture. Java: 47) 

Unfortunately I don't have specific anomaly information, only org. Elasticsearch. ElasticSearchInterruptedException. Please instruct me. 

2  \*  &#65288; 16 core 24 g physical machine &#65289;
</description><key id="6280806">2182</key><summary>Org. Elasticsearch. ElasticSearchInterruptedException  custom-filters-score-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">iskytek</reporter><labels /><created>2012-08-17T03:15:20Z</created><updated>2013-07-22T12:05:59Z</updated><resolved>2013-07-22T12:05:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-26T15:26:39Z" id="20055398">Hey,

can you please provide more information, in case this problem still exists with a current version of elasticsearch?
- What elasticsearch version are you using
- Provide a full stack trace, when the problem occurs (take a look at the logfiles)
- Provide a full possibility for us to recreate your issue using curl, see http://www.elasticsearch.org/help/

Thanks for your help!
</comment><comment author="spinscale" created="2013-07-22T12:05:58Z" id="21339584">Closing. Happy to reopen when more information is provided and this still happens with the current 0.90 release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count calls for q=*:* do not do a match_all (very slow, 2000x slower)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2181</link><project id="" key="" /><description>We just upgraded from 0.19.3 to 0.19.8 and noticed that our application started behaving erratically. The root cause seems to be that count calls like `/_count?q=*:*` perform terribly slow. The same call without the q parameter returns instantly.

See example below (and it has nothing to do with caching)

``` shell
folke@server01:~$ time curl -XGET 'http://server02:9200/shard12258-3/_count'
{"count":753780,"_shards":{"total":1,"successful":1,"failed":0}}
real    0m0.012s
user    0m0.000s
sys 0m0.000s


folke@server01:~$ time curl -XGET 'http://server02:9200/shard12258-3/_count?q=*:*'
{"count":753785,"_shards":{"total":1,"successful":1,"failed":0}}
real    0m25.903s
user    0m0.000s
sys 0m0.000s
```
</description><key id="6275561">2181</key><summary>Count calls for q=*:* do not do a match_all (very slow, 2000x slower)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels /><created>2012-08-16T21:17:20Z</created><updated>2013-03-02T06:51:30Z</updated><resolved>2013-03-02T06:51:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="folke" created="2012-08-16T21:24:29Z" id="7800039">After further investigating this issue, the same seems to be true for `_search`
</comment><comment author="folke" created="2012-08-16T21:26:49Z" id="7800094">Probably related to #413
</comment><comment author="s1monw" created="2012-08-16T22:15:55Z" id="7801402">can you tell which query parser you are using on the count?
</comment><comment author="folke" created="2012-08-17T06:39:04Z" id="7810913">@s1monw what do you mean? Didn't specify a query parser anywhere. Just did a:

``` shell
curl -XGET 'http://server02:9200/shard12258-3/_count?q=*:*'
```
</comment><comment author="folke" created="2012-08-17T06:40:44Z" id="7810935">In the meantime, we have seen that this issue occurs for any query / filter that uses `*:*` somewhere in the parse tree. Using `*` instead fixes this.
</comment><comment author="kimchy" created="2012-08-18T15:57:32Z" id="7845642">@folke yea, this is already fixed and will be part of upcoming 0.19.9, just using `*` is a valid workaround up until then.
</comment><comment author="s1monw" created="2013-03-02T06:51:30Z" id="14324187">according to @kimchy this is fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.5.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2180</link><project id="" key="" /><description /><key id="6272645">2180</key><summary>Upgrade to Netty 3.5.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-16T19:15:04Z</created><updated>2012-08-16T19:15:33Z</updated><resolved>2012-08-16T19:15:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>use term query instead of a specialized SpanTermQuery on _all field if positions are omitted </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2179</link><project id="" key="" /><description>This PR uses a TermQuery instead of AllQuery if positions are omitted. See #2178 for details
</description><key id="6264667">2179</key><summary>use term query instead of a specialized SpanTermQuery on _all field if positions are omitted </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2012-08-16T13:43:30Z</created><updated>2014-06-18T14:03:47Z</updated><resolved>2012-08-16T17:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-16T17:43:43Z" id="7793084">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use TermQuery In _all field if positions are omitted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2178</link><project id="" key="" /><description>Currently the _all field is always using a specialized SpanTermQuery as a leave query. This might cause unexpected scoring effects and might cost performance if we don't have positions / payloads. We should default to TermQuery if positions are omitted.
</description><key id="6263663">2178</key><summary>Use TermQuery In _all field if positions are omitted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2012-08-16T12:55:41Z</created><updated>2012-12-27T14:25:16Z</updated><resolved>2012-12-27T14:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-08-16T13:45:43Z" id="7785690">I simply added a condition in the AllFieldMapper that returns a TermQuery instead of the all query - I think this is a fair game here in terms of BWcompat etc. Anyway I think this field should never ever use spans. This might be a "this is awesome for the beginners" feature but really this is not scalable and damn slow. I'd vote for dropping the AllQuery entirely. But this is a different discussion.
</comment><comment author="martijnvg" created="2012-08-16T14:00:40Z" id="7786128">I wonder why the _all field needs per term boosting. This is what seems to happen in the AllTokenStream class.
</comment><comment author="martijnvg" created="2012-08-16T14:05:49Z" id="7786296">I guess this happens to preserve the boost from the original field. 
</comment><comment author="kimchy" created="2012-08-16T17:39:00Z" id="7792954">The pull request makes sense, I think that we can also introduce a flag that controls if we even do a special thing when it comes to storing boost values related to specific fields in the _all field. The `_all` field is really useful for many cases, obviously it comes with an overhead, but note, you can completely disable it as well. I'll merge this pull request.
</comment><comment author="kimchy" created="2012-08-16T17:45:17Z" id="7793140">Pushed the pull request, but I think to complete that (and also have something can potentially backport to 0.19), is to have a flag on the _all field called `field_boost` (defaults to `true`), that will control if specific boost values are going to be stored as payloads, and which type of term query is going to be generated. I am open to the fact that this flag default value can be `true` in master/0.20.
</comment><comment author="s1monw" created="2012-08-16T18:39:33Z" id="7794713">First, thanks for pulling this so quickly. I totally agree a _all field is handy and can simplify a lot of things. I also agree that we should make this "field-boosting" optional and I strongly recommend to make "field_boost" default to _false_. A SpanQuery is ~ an order of magnitude slower compared to a straight term query. I'd also argue some other defaults like proximity infos. We should really think about this though - I mean the overhead of prox pointers in the posting list is significant and I doubt that lots of users benefit from it ie. use positional queries. In any case I'd like to add a _big fat_ warning to the documentation that not omitting positions can lead to significant performance impact and make a big portion of the index size. I will backport that change to 0.19 and add the "field_boost" in another issue. 
</comment><comment author="clintongormley" created="2012-08-16T19:33:01Z" id="7796564">I'd be interested to know how many power users of ES use the `_all` field. My guess would be: few.  In other words the people who are likely to use it are beginners, so its functionality should probably be geared at them (ie easy start to using ES), rather than at the power user.

I'm not arguing for or against any of the above changes, because i'm not entirely sure what difference they will make.  Just saying that the changes should probably be guided by the targeted user.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom-filters-score-query  Org. Elasticsearch. ElasticSearchInterruptedException </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2177</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/reference/query-dsl/custom-filters-score-query.html 

Hello, I use in the application of custom - filters - score - query achieve business requirements, similarity calculation. But I found, two machines running for a period of time, of which one will search performance fell sharply, 
Obviously see IO, speaking, reading and writing down, finally the following anomaly, but found that stop off one of the machine and returned to normal. 

The 2012-08-16 10:46:37, 048 ERROR filter. ExceptionFilter - [DUBBO] Got unchecked and undeclare service method invoke exception: null, DUBBO version: 2.0.14, current host: 172.22.28.88 
Org. Elasticsearch. ElasticSearchInterruptedException 
At org. Elasticsearch. Action. Support. AdapterActionFuture. ActionGet (AdapterActionFuture. Java: 47) 

Unfortunately I don't have specific anomaly information, only org. Elasticsearch. ElasticSearchInterruptedException. Please instruct me. 

2  \*  &#65288; 16 core 24 g physical machine &#65289;
</description><key id="6257975">2177</key><summary>custom-filters-score-query  Org. Elasticsearch. ElasticSearchInterruptedException </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iskytek</reporter><labels /><created>2012-08-16T07:00:27Z</created><updated>2013-06-26T15:24:18Z</updated><resolved>2013-06-26T15:24:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-26T15:24:18Z" id="20055204">Closing. Duplicate of #2182 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is there any plan to upgrade to lucene 4.x?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2176</link><project id="" key="" /><description>Lucene 4.0 beta is released, it provides lots of exciting features, so, is there any plan to upgrade  to lucene 4.x? thanks!
</description><key id="6233742">2176</key><summary>Is there any plan to upgrade to lucene 4.x?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mailsurfie</reporter><labels /><created>2012-08-15T05:05:12Z</created><updated>2013-05-23T14:36:41Z</updated><resolved>2013-05-23T14:36:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-20T09:56:03Z" id="7865026">Agreed, it provides a lot of exciting features. The plan is to get to a state where when 4.0 goes GA, there will be an elasticsearch version that uses it.
</comment><comment author="chendo" created="2012-10-10T06:36:25Z" id="9291509">I'm mostly interested in the lower memory usage :) +1
</comment><comment author="sandys" created="2012-10-12T15:46:45Z" id="9380768">Lucene 4.0 has been released. Any plans of porting ES to 4.0 ? I'm really keen on using ES+Lucene 4.0 in a new project.
</comment><comment author="n0rthwood" created="2012-10-13T21:12:23Z" id="9410129">Yeah, was searching for elasticsearch + Lucene 4 plan. and see this, looking foward to see the ES + L4 plan out 
</comment><comment author="AnAppAMonth" created="2012-10-14T09:13:09Z" id="9417648">The long long long wait for Lucene 4 is finally over! Now just need to wait for ES support so that I can use it:)
</comment><comment author="mailsurfie" created="2012-10-16T02:57:43Z" id="9471562">really expect the new ES with Lucene 4 supported.
</comment><comment author="stanxii" created="2012-10-18T04:35:07Z" id="9553064">Hope ES support Lucene 4.x too!
</comment><comment author="jippi" created="2013-01-10T23:00:38Z" id="12123469">What's the status on Lucene 4 ? :) 
</comment><comment author="drewr" created="2013-01-10T23:05:15Z" id="12123646">@Jippi `master`, which will be `0.21.0`, is all Lucene 4.  Try it out and give us feedback! :+1: 
</comment><comment author="jippi" created="2013-01-10T23:10:13Z" id="12123821">I will ! :) When do you think it will be release worthy for deb and normal production usage ? :)
</comment><comment author="kimchy" created="2013-01-10T23:11:46Z" id="12123880">@Jippi we don't have concrete date yet, all tests pass in master with Lucene 4, and we already exposed most of the planned features. We still have some ironing out to do. We might end up starting to release at least a formal Beta release of `0.21.0` just so people can play with it.
</comment><comment author="jippi" created="2013-01-10T23:15:10Z" id="12124001">Alright, I will have my eyeballs on the blog for a beta I can mess around with :) Thanks for the quick replies, and thanks for an awesome piece of software!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle facet terms int overflow better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2175</link><project id="" key="" /><description>Facet terms are int based and can overflow with a large number of documents.

Would be nice if the merge step either didn't overflow and returned Integer.MAX_VALUE or if longs were used.
</description><key id="6222151">2175</key><summary>Handle facet terms int overflow better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels><label>discuss</label></labels><created>2012-08-14T17:39:52Z</created><updated>2014-07-25T08:07:07Z</updated><resolved>2014-07-25T08:07:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="awick" created="2012-08-14T17:40:33Z" id="7733691">Possible pull request that stops overflow since converting from int to long looked harder.  Would be happy to do the work if you could point me in the right direction

https://github.com/elasticsearch/elasticsearch/pull/2115
</comment><comment author="awick" created="2013-07-08T12:48:04Z" id="20602723">Oops, didn't mean to close the issue, only the pull request.

Still would be nice if longs were used for the aggregation step.  I would be fine with a int still used per shard.
</comment><comment author="clintongormley" created="2014-07-25T08:07:07Z" id="50119832">Facets are deprecated. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support proxies on plugin command line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2174</link><project id="" key="" /><description>Currently have to edit the plugin script to support proxies, since the -Dhttps.proxyHost option is passed to the program instead of java
</description><key id="6222097">2174</key><summary>Support proxies on plugin command line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2012-08-14T17:37:33Z</created><updated>2013-03-11T19:47:23Z</updated><resolved>2013-03-11T19:47:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="awick" created="2012-08-14T17:38:14Z" id="7733611">Possible pull https://github.com/elasticsearch/elasticsearch/pull/2170
</comment><comment author="awick" created="2012-08-31T15:23:27Z" id="8195440">Another possible solution https://github.com/elasticsearch/elasticsearch/pull/2223
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed scoped faceting issue that occurs when nested queries are used.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2173</link><project id="" key="" /><description /><key id="6219629">2173</key><summary>Fixed scoped faceting issue that occurs when nested queries are used.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-14T15:52:50Z</created><updated>2015-05-18T23:35:45Z</updated><resolved>2012-08-27T10:59:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>upstart script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2172</link><project id="" key="" /><description>This patch replaces the init.d script with an upstart script:

https://github.com/elasticsearch/elasticsearch/pull/2171

it probably needs review.
</description><key id="6218388">2172</key><summary>upstart script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">igalic</reporter><labels><label>:Packaging</label></labels><created>2012-08-14T15:06:32Z</created><updated>2014-12-05T10:32:09Z</updated><resolved>2014-12-05T10:32:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="igalic" created="2012-08-14T15:07:30Z" id="7728832">Accepting this will also trigger a much needed cleanup of 

http://www.elasticsearch.org/tutorials/2010/07/02/setting-up-elasticsearch-on-debian.html

Which I am also happy to provide.
</comment><comment author="QuinnyPig" created="2013-02-20T19:05:53Z" id="13849954">Still no upstart support for the packages; Debian packages are complex enough with wrapping them in gradle, so I've got no useful feedback on how to add them, but this could use a poke.
</comment><comment author="spinscale" created="2014-07-18T11:20:02Z" id="49420416">As Debian (and thus Ubuntu) decided to switch to systemd, how much sense does it make to do this nowadays in your opinion?
</comment><comment author="jpountz" created="2014-12-05T10:32:09Z" id="65772274">Closing for the reasons mentionned on #2171
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>replace init.d with upstart scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2171</link><project id="" key="" /><description>This patch replaces init.d with upstart.

It probably needs review.
</description><key id="6218348">2171</key><summary>replace init.d with upstart scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">igalic</reporter><labels><label>:Packaging</label></labels><created>2012-08-14T15:04:42Z</created><updated>2014-11-21T18:36:24Z</updated><resolved>2014-11-21T14:29:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="victor73" created="2012-12-06T05:58:47Z" id="11074125">Definitely would like to see the elasticsearch package support upstart as well since it would probably make it easier for other systems that depend on it to declare that they should be started after elasticsearch has been fired up. However, Debian systems do not use upstart by default, so removal of the init.d script entirely might not be a good idea. Perhaps the debconf supports having both scripts in the package and only deploying the correct one at package install time...
</comment><comment author="mattweber" created="2012-12-06T06:49:51Z" id="11074851">See [elasticsearch-servicewrapper](https://github.com/elasticsearch/elasticsearch-servicewrapper) which supports upstart.
</comment><comment author="kimchy" created="2012-12-07T17:13:57Z" id="11137985">@victor73 sounds good, how can we make sure which one to install at install time?
</comment><comment author="drewr" created="2012-12-07T18:01:05Z" id="11139830">We also need to leave an init.d script for large clusters which cannot tolerate the IO incurred by automated, supervised process restarts.  Although I agree that for Ubuntu the default should be upstart.

I'm taking this into account in my work on improving our support across platforms.  For now though, Since Debian and Ubuntu both still support init.d, I think we should leave the deb alone so it will still work on both.  We'll be publishing officially supported run scripts for most popular supervision suites at some point.
</comment><comment author="victor73" created="2012-12-11T02:55:37Z" id="11229074">@kimchy So it looks like the dh_installinit utility supports both init.d AND upstart. From `man dh_installinit`:

&gt; dh_installinit is a debhelper program that is responsible for installing upstart job files or init scripts with
&gt; associated defaults files into package build directories, and in the former case providing compatibility handling
&gt; for non-upstart systems

The utility has the --upstart-only option to install the upstart script for upstart systems (Ubuntu). So, it seems that there could be an if condition that would check for the presence of /etc/init, or maybe check if upstart is installed via dpkg:

```
dpkg-query -s upstart | grep -Eq "Status:.*install.*ok"
```

If upstart is installed, then one could call dh_installinit with the --upstart-only option. If not, then it would be callled just as it is now (I'm assuming dh_installinit is already in use).
</comment><comment author="clintongormley" created="2014-07-08T15:55:05Z" id="48359125">Looks like upstart scripts are now provided in the relevant packages.  Closing this.
</comment><comment author="clintongormley" created="2014-07-08T15:56:47Z" id="48359367">Apparently they don't - reopening.
</comment><comment author="dakrone" created="2014-11-21T10:32:50Z" id="63952884">@drewr @electrical @joehillen do you guys have any ideas how this plays with upstart versus sysvinit versus systemd (now that systemd is a thing)? Ideally it seems like this would be something that a chef or puppet module configures, but I don't know what the best one to put in the .deb file is
</comment><comment author="electrical" created="2014-11-21T10:37:13Z" id="63953328">Upstart will be disappearing in debian/ubuntu at some point since they will be moving to SystemD. [0]
Also CentOS / RHEL 7 is using systemd.
I wonder because of this if it makes sense to maintain an upstart config? ( I'm not supporting it in the Puppet module btw )

[0] http://www.markshuttleworth.com/archives/1316
</comment><comment author="igalic" created="2014-11-21T13:45:28Z" id="63971352">yeah, i agree we can probably close this.
</comment><comment author="dakrone" created="2014-11-21T14:29:12Z" id="63976667">Okay, thanks, closing this
</comment><comment author="victor73" created="2014-11-21T15:11:40Z" id="63982869">Well, that's disappointing. Closing the ticket because debian/ubuntu will move to SystemD "at some point" ? What about all the systems out there that are using upstart right now and for years to come?
</comment><comment author="joehillen" created="2014-11-21T18:36:24Z" id="64016139">I'm inclined to agree. Ubuntu isn't going to start using [systemd](http://www.freedesktop.org/wiki/Software/systemd/#spelling) in their LTS release until 16.04 at the earliest, which is what any sane server admin will be waiting for.

However, Debian still uses SysVinit, so in order to support Upstart on Ubuntu and SysVinit on Debian, we would need 2 debian packaging configs. (I could be wrong. I'm not very experienced with packaging.)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support proxies on plugin command line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2170</link><project id="" key="" /><description>Look for any -D options and pass them to java so that
https.proxyHost and https.proxyPort can be used
</description><key id="6217699">2170</key><summary>Support proxies on plugin command line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2012-08-14T14:41:05Z</created><updated>2014-06-28T20:09:53Z</updated><resolved>2013-03-11T19:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="artgreen" created="2012-10-03T20:23:59Z" id="9120745">This change is critical for the installation of plugins when the server is located in a secured data center (i.e., no outbound Internet access, only proxy access).  Support of proxy servers is becoming a critical feature for enterprise applications.  I was able to use this modified script to install a plugin from behind a proxy. 
</comment><comment author="Savar" created="2013-03-04T06:47:40Z" id="14366214">I would like to be able to set the proxy also but maybe with either extra parameter (not all -D parameter should be passed) and/or respecting http_proxy and https_proxy env variables?!

ARGH: I see it is possible to use the JAVA_OPTS variable right now.. please ignore
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support GeoShape searching and indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2169</link><project id="" key="" /><description>A new spatial module was added to Lucene 4 this year which added support for searching not just for points, but for general shapes including rectangles and polygons.  Some of this code can be ported to ES and improved further, so that users can index and search for shapes.
### Shape representation

To efficiently represent shapes in the index, Shapes are converted into a series of hashes representing grid squares using implementations of a PrefixTree.  The tree aspect comes from the fact that the PrefixTree represents the planet with multiple grid layers each representing a different level of precision.  For example, the PrefixTree implemenetation QuadPrefixTree represents the Earth with four grid squares, labelled A, B, C and D, at its first level of precision.  Move one level of precision further and there is now 16 grid squares, AA, AB, AC, AD, BA... and so on.  By having this multiple layers of precision, we can create the most efficient representation of a shape which balances number of terms vs. precision

As mentioned, there are multiple PrefixTree implemenetations.  Two are provided:
- GeohashPrefixTree - As its name suggests, uses geohashes which have more grid squares per layer
- QuadPrefixTree - As mentioned, represents the Earth as a quadtree and uses A, B, C and D for hashes.
### Search Logic

Put simply, searches for shapes are implemented by converting the query shape into grid hashes as well, and then querying for those documents which have one or many (or none) of the query shape hashes.  This allows us to efficiently find those shapes which intersect or are disjoint of.  Other relationships between Shapes such as contains, as discussed below, also build upon this logic.
### GeoShapeFieldMapper

Indexing Shapes is handled through the GeoShapeFieldMapper (known as `geo_shape`).  The Mapper can be configured with the following options:
- `tree` - Name of the PrefixTree implementation to be used.  Currently `geohash` and `quadtree` are supported.  Defaults to `geohash`.
- `tree_levels` - Maximum number of levels to be used in the PrefixTree.  This can be used to control how precise the representation of Shapes should be.  Defaults to the default value of the chosen PrefixTree implementation.
- `distance_error_pct` - Used as a guide to the PrefixTree for how precise its representation should be.  Defaults to 0.025 (2.5%) however 0 is also supported.

Shapes are defined in index requests using the GeoJSON format as follows:

```
{
    "location" : {
        "type" : "point",
        "coordinates" : [45.0, -45.0]
    }
}
```

Currently the GeoJSON shape types `MultiPolygon`, `MultiLineString` and `GeometryCollection` are unsupported.  The additional shape type `envelope` has been added which accepts two coordinates representing the upper left and bottom right of the shape respectively.  Support for `MultiPolygon` will be added in the future.

Also note that currently Polygons cannot cross the dateline.  This is in the process of being fixed.

Due to the complexity of the shape representation, the shape for a document can only be retrieved through the document's `source`.
### GeoShapeFilterParser &amp; GeoShapeQueryParser

Filtering and querying Shapes is supported through the GeoShapeFilterParser and GeoShapeQueryParser, both denoted `geo_shape`.  Both accept, in addition to the usual caching parameters for filtering and boost for querying:
- `shape` - The GeoJSON representation of the query shape, following the same syntax as used in the Mapper
- `relation` - The kind of filter / query to be executed, as represented by a relationship between the query and indexed shapes.  Currently supported values are `intersects`, `disjoint` and `contains`.  See more information below.

The following is an example of this syntax:

```
{
    "location" : {
        "shape" : {
            "type" : "envelope",
            "coordinates" : [ 
                [-45.0, 45.0], [45.0, -45.0] 
             ]
        },
        "relation" : "contains"
    }
}
```
#### Shape Relations

Currently three shape relationships are supported:
- `intersects` - Finds those indexed shapes which intersect with the query shape.  Intersection occurs when the two shapes have at least one shared grid hash.  Because of current limitations of the algorithm, very large indexed shapes are not deemed to intersect with very small query shapes.  However, smaller indexed shapes will intersect with larger query shapes
- `disjoint` - Finds those indexed shapes which are disjoint to the query shape.  This means the the indexed shapes and query shape must have no shared grid hashes.
- `contains` - Finds those indexed shapes which are fully contained within the query shape.  Unlike `intersects`, this means that all of the indexed shape must be present in the query shape.  Any shapes which have additional area outside of the query shape are excluded.  This relationship is still experimental and due to its use of polygons, the query shape cannot be within 0.5 of the dateline.
### Future improvements

Future improvements for this feature are:
- Currently queries created by the GeoShapeQueryParser are wrapped in a ConstantScoreQuery.  With integration into ES's in memory caching, it would be possible to cache the Shapes and calculate more useful scores based on distance or overlap percetange.
- Multipolygon support
- Handling of polygons which cross the dateline
- Support for adding and using named shapes.  This would mean that users wouldn't have to define the coordinates for their shapes everytime, they would only need to configure a shape once with a name and would then be able to reference it.
</description><key id="6208276">2169</key><summary>Support GeoShape searching and indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-08-14T04:40:13Z</created><updated>2014-06-25T18:19:00Z</updated><resolved>2012-08-16T13:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-16T13:56:19Z" id="7786010">Pushed in #2164.
</comment><comment author="nfmelendez" created="2012-08-16T18:11:45Z" id="7793909">Nice!
</comment><comment author="clintongormley" created="2012-08-16T18:57:23Z" id="7795257">That is WICKED!
</comment><comment author="jasongilman" created="2012-08-17T01:03:49Z" id="7804433">This is awesome.  Is there support for geodetic data such a polygons on a ellipsoidal/spherical earth model?  
</comment><comment author="jdunck" created="2012-08-17T02:43:59Z" id="7805654">I'm pretty familiar with GIS stuff and one thing that strikes me as a potential problem is complex queries shapes -- the query shape GeoJSON representation could be quite large.  Perhaps it should be possible to register shapes with ES ahead of time so that they don't have to be sent each time, and instead could be referred to with handles:

/register_shape?shape=&lt;geojson&gt;
-&gt; {'shape_handle': &lt;guid&gt;}

Then 
{
    "location" : {
        "shape" : {
            "type" : "geometry",
            "shape_handle": &lt;guid&gt;
        },
        "relation" : "contains"
    }
}

This would be useful where the query shape is commonly combined with other filter parameters, or where the same criteria needs to be relayed to many nodes to satisfy the query.

Since this is in master now, is it reasonable to expect this in 0.19.x or 0.20.x?
</comment><comment author="chrismale" created="2012-08-17T04:29:31Z" id="7808166">@jdunck I'm actually working on that very feature at the moment.  I'm hoping to incorporate a ShapeService concept where Shapes can be pre-loaded, registered and retrieved.  I'm also working on WKT support to reduce the verbosity of large shapes.
</comment><comment author="jdunck" created="2012-08-17T04:48:15Z" id="7808382">Cool.  If you have a to-do list and don't mind mentoring (I'm rusty but
experienced with Java, totally inexperienced with the ES codebase), I'm
interested in helping.

On Thu, Aug 16, 2012 at 9:29 PM, chrismale notifications@github.com wrote:

&gt; @jdunck https://github.com/jdunck I'm actually working on that very
&gt; feature at the moment. I'm hoping to incorporate a ShapeService concept
&gt; where Shapes can be pre-loaded, registered and retrieved. I'm also working
&gt; on WKT support to reduce the verbosity of large shapes.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2169#issuecomment-7808166.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggestion: make man page or better CLI help</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2168</link><project id="" key="" /><description>Currently, all I get from running `elasticsearch -h` on a Homebrew install is:

&lt;pre&gt;
Usage: /usr/local/bin/elasticsearch [-f] [-h] [-p pidfile]
&lt;/pre&gt;


`-f` was not obvious to me. We should add a basic man page that at least covers:

```
OPTIONS:
  -f: start in the foreground
  -p &lt;filename&gt;: log the pid to a file (useful to kill it later)
```
</description><key id="6206629">2168</key><summary>Suggestion: make man page or better CLI help</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bluemont</reporter><labels><label>:Packaging</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2012-08-14T01:44:35Z</created><updated>2014-12-02T12:29:56Z</updated><resolved>2014-12-02T12:29:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kedarmhaswade" created="2014-03-13T16:34:14Z" id="37554663">+1. Also, when I do bin/elasticsearch --help it just hangs there. This is with 1.0.1 (Version: 1.0.1, Build: 5c03844/2014-02-25T15:52:53Z, JVM: 1.7.0_45) on a Mac.
</comment><comment author="t-lo" created="2014-12-01T14:27:48Z" id="65071378">@kedarmhaswade Any long option parameter w/o a value (`--param val` with `val` missing, e.g. `--version` or `--help`) will send the script into a neverending busy-loop.
</comment><comment author="t-lo" created="2014-12-02T12:21:38Z" id="65223261">Ping @clintongormley - this issue was addressed in #8729 (which has been merged) :)
</comment><comment author="clintongormley" created="2014-12-02T12:29:56Z" id="65224018">Splendid - thanks @t-lo. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When specifying multiple indexes to search, don't return IndexMissingException if at least one index exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2167</link><project id="" key="" /><description>For applications using daily indexes it would be useful if elasticsearch didn't return an error if at least one of the specified indexes existed.  Currently before each query I need to check what indexes exist incase a daily index was removed outside my application .   If the current behavior is required, then maybe add a flag for new behavior?

Reproduce:
curl http://localhost:9200/sessions-120813/session/1 -d '{"test": 1}'
curl 'http://localhost:9200/sessions-120812,sessions-120813/session/_search?query=test:1'
{"error":"IndexMissingException[[sessions-120812] missing]","status":404}

Desired response:
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"sessions-120813","_type":"session","_id":"1","_score":1.0, "_source" : {"test": 1}}]}}
</description><key id="6196568">2167</key><summary>When specifying multiple indexes to search, don't return IndexMissingException if at least one index exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2012-08-13T17:41:21Z</created><updated>2012-08-27T12:47:12Z</updated><resolved>2012-08-27T12:47:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="folke" created="2012-08-16T21:39:31Z" id="7800468">Having similar issues and still returning results would be great. Is actually similar with how a `index` operation on a non existing index creates the index on the fly...
</comment><comment author="kimchy" created="2012-08-21T13:53:48Z" id="7901495">We can definitely support it. Possibly through a flag (but then also apply it to other APIs, not just search).
</comment><comment author="kimchy" created="2012-08-27T12:47:12Z" id="8054664">Implemented and will be available in upcoming 0.20 using the `ignore_indices` parameter.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.gc_deletes=0s does not behave as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2166</link><project id="" key="" /><description>If a document is deleted, indexing a new document with the same id within approx 60 seconds of the delete fails with a VersionConflictEngineException.

The scenario where it's necessary to delete and immediately reindex a document is when external versioning is in place and we need to force indexing of an older version of a document - there doesn't appear to be any other way to do this besides deleting the "newer" version before indexing the "older".
</description><key id="6186287">2166</key><summary>index.gc_deletes=0s does not behave as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snellm</reporter><labels /><created>2012-08-13T09:38:39Z</created><updated>2014-07-08T16:39:02Z</updated><resolved>2014-07-08T16:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="snellm" created="2012-08-13T10:59:49Z" id="7688762">We discovered the "index.gc_deletes" setting from trawling the forums (this should be documented on the Delete API page?). Although setting this to 0s helps, it appears VersionConflictEngineException still occur for a short period after the delete.

Perhaps it would be better for delete to honour the gc_deletes setting only if passed a version number to delete? Or is there a better way to deal with the version conflict scenario mentioned above?
</comment><comment author="kimchy" created="2012-08-13T16:11:31Z" id="7696703">Can you gist a recreation, I know when it might happen, just want something to work with. This is, btw, by design, but it might make sense to allow for your case as well...
</comment><comment author="snellm" created="2012-08-14T10:07:02Z" id="7721593">Will do. Looking at our case, what we think we'd like (from highest to lowest preference) is:
- A flag on IndexRequest to force indexing regardless of version conflicts
- Make delete with version behave differently from delete without version:
  - delete without a version being passed should not remember the version (ie allow immediate indexing with a different version)
  - delete with version should remember the version for the time period defined by "index.gc_deletes" (ie current behaviour)
- Make it so that if "index.gc_deletes" is set to zero, the version is not remembered at all (currently appears to still be remembered for a small time period)
</comment><comment author="snellm" created="2012-08-14T17:07:27Z" id="7732629">Here is a gist that recreates the issue: https://gist.github.com/3350884

Output for me is as below - if I uncomment the one second sleep, the VersionConflictEngineException does not occur.

```
Create a local node with index.gc_deletes set to zero seconds
Indexing a document with external versioning and a version of 2
Deleting the document
Indexing a document with external versioning and a version of 1
Exception in thread "main" org.elasticsearch.index.engine.VersionConflictEngineException: [foo][1] [bar][12345]: version conflict, current [3], provided [1]
    at org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:541)
    at org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:486)
    at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:323)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:206)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
```
</comment><comment author="randunel" created="2013-12-23T15:39:15Z" id="31125309">Any updates on this issue? I seem to be experiencing similar issues with version 1.0 beta1
</comment><comment author="clintongormley" created="2014-07-08T16:38:55Z" id="48365310">The new force version type allows you to override versions in Elasticsearch. http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>haystack django integration </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2165</link><project id="" key="" /><description>haystack a django integration for elasticsearch not mentioned in  
http://www.elasticsearch.org/guide/appendix/clients.html
</description><key id="6182726">2165</key><summary>haystack django integration </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">japrogramer</reporter><labels /><created>2012-08-13T04:13:58Z</created><updated>2012-08-19T21:58:08Z</updated><resolved>2012-08-19T21:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2012-08-13T06:42:00Z" id="7684387">Hey this isn't the right place for this issue. I think you should close it and open an issue or a pull request in https://github.com/elasticsearch/elasticsearch.github.com which is the repository for the ES documentation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoShape indexing and query support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2164</link><project id="" key="" /><description>Based on the spatial module in Lucene trunk, this PR adds support for indexing and querying arbitrary shapes (Points, Rectangles, Polygons).  Since both documents and queries can be shapes, the notion of a relationship between shapes is included so users can query for shapes that intersect the query shape, are disjoint of, or are contained within.  Also included is more extensive support for parsing GeoJSON shapes.

The underlying logic is implemented using two projects, Shape4j which provides Shape implemetations and handles much of the spatial logic, and JTS, which is used by Shape4j for Polygons and by code in the PR to implement the contains relationship.
</description><key id="6181452">2164</key><summary>GeoShape indexing and query support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-08-13T01:09:05Z</created><updated>2014-06-19T12:21:02Z</updated><resolved>2012-08-13T12:47:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="travisbot" created="2012-08-13T01:44:02Z" id="7682057">This pull request [fails](http://travis-ci.org/elasticsearch/elasticsearch/builds/2104380) (merged 37aaa00c into b979dfa0).
</comment><comment author="travisbot" created="2012-08-13T12:31:15Z" id="7690352">This pull request [fails](http://travis-ci.org/elasticsearch/elasticsearch/builds/2107299) (merged 9e2347eb into ad0e916f).
</comment><comment author="travisbot" created="2012-08-13T12:31:17Z" id="7690354">This pull request [fails](http://travis-ci.org/elasticsearch/elasticsearch/builds/2107299) (merged 9e2347eb into ad0e916f).
</comment><comment author="kimchy" created="2012-08-13T12:47:35Z" id="7690680">Pushed. Lets create an issue corresponding to this change with full docs on how it works.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enabling the programatic registration of plugins in NodeBuilder and expo...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2163</link><project id="" key="" /><description>...sing it in the AbstractNodeTests
</description><key id="6180827">2163</key><summary>Enabling the programatic registration of plugins in NodeBuilder and expo...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2012-08-12T23:30:34Z</created><updated>2014-07-16T21:54:56Z</updated><resolved>2013-05-28T10:06:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="travisbot" created="2012-08-13T00:19:57Z" id="7681347">This pull request [fails](http://travis-ci.org/elasticsearch/elasticsearch/builds/2103969) (merged af59ab5f into b979dfa0).
</comment><comment author="travisbot" created="2012-08-13T00:19:59Z" id="7681348">This pull request [fails](http://travis-ci.org/elasticsearch/elasticsearch/builds/2103969) (merged af59ab5f into b979dfa0).
</comment><comment author="spinscale" created="2013-05-28T10:06:49Z" id="18541513">This is already included. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit multi search using from and size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2162</link><project id="" key="" /><description>Currently, size and from can be used to limit the amount of search results within each individual queryin a multi search request.

It would be nice if we can set a from and size that limits the total amount of results returned for the multi search query.
</description><key id="6175913">2162</key><summary>Limit multi search using from and size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels><label>discuss</label></labels><created>2012-08-12T09:18:42Z</created><updated>2014-07-25T08:06:24Z</updated><resolved>2014-07-25T08:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="darylrobbins" created="2013-11-26T00:18:09Z" id="29256427">+1 for this enhancement. It's proving quite messy to try and do this after the fact.

Our use case is as follows:
- First query returns up to say 20 featured items (which are duplicates of the results in the second query), which appear first in the result set
- Second query returns all non-featured and featured results grouped by category
</comment><comment author="clintongormley" created="2014-07-25T08:06:24Z" id="50119768">Given that multi-search happens in parallel, Elasticsearch would have to run the second query after the first.  This can be done client side instead.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When _id field is stored and is present in the source, it is returned twice in the list of fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2161</link><project id="" key="" /><description>Repro: https://gist.github.com/3309993
Related discussion on the mailing list:  https://groups.google.com/d/topic/elasticsearch/SVN8bpzKk7k/discussion
</description><key id="6151023">2161</key><summary>When _id field is stored and is present in the source, it is returned twice in the list of fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>adoptme</label></labels><created>2012-08-10T13:49:12Z</created><updated>2014-11-28T18:26:37Z</updated><resolved>2014-11-28T18:26:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-13T11:10:42Z" id="7688946">There isn't really a reason to store the id field at all, the internal `_uid` field is always stored, and loaded, probably make sense to actually simply ignore any store settings for `_id` in this case.
</comment><comment author="clintongormley" created="2014-11-28T18:26:37Z" id="64919531">Closing in favour of #8143
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose omitPositions via FieldMapping to omit positions only if frequencies are required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2160</link><project id="" key="" /><description>Lucene has the ability to omit positions only and keep frequencies. This is one of the most common usecases when folks don't need proximity information. Yet, if you need freqs for scoring you might pay a huge price for indexing and merging positions like wasting CPU, Memory and diskspace. This PR exposes omit_positions just like omit_norms etc.
</description><key id="6150469">2160</key><summary>Expose omitPositions via FieldMapping to omit positions only if frequencies are required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2012-08-10T13:22:43Z</created><updated>2014-07-16T21:54:56Z</updated><resolved>2012-08-13T15:31:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-08-10T22:11:13Z" id="7660332">I updated the commit to actually remove the _omit_term_freq_and_positions_ flag entirely (except of a some BW compat use in TypeMappers) in favor of IndexOptions. IndexOptions are exposed directly via the mapping api and can be used to define the options on the field. the options here _docs_, _freqs_, and _positions_ where _freqs_ implies docs &amp; frequencies and _positions_ implies docs &amp; frequencies and position information. 
I kept the _omit_term_freq_and_positions_ option on the parsing side to handle BW compat gracefully. Yet, if somebody defines both _omit_term_freq_and_positions_ and _index_options_ the result is undefined ie. whatever comes first wins. We need to document that propperly and I wanna add an entire test collection to check if all defaults are set right etc. This doesn't seem to be tested throughout the codebase so I will add some more tests later or maybe in a different PR
</comment><comment author="kimchy" created="2012-08-13T15:31:23Z" id="7695351">Pushed to master. An issue tagged with 0.20 and explaining the new `index_options` that we can close and for it to be on the release notes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add lenient option to match &amp; multi_match queries. #2156</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2159</link><project id="" key="" /><description /><key id="6135445">2159</key><summary>Add lenient option to match &amp; multi_match queries. #2156</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-09T19:30:31Z</created><updated>2015-05-18T23:35:47Z</updated><resolved>2012-08-09T19:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-09T19:57:15Z" id="7625152">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add lenient option to match &amp; multi_match queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2158</link><project id="" key="" /><description /><key id="6135264">2158</key><summary>Add lenient option to match &amp; multi_match queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-09T19:22:09Z</created><updated>2014-06-29T05:26:51Z</updated><resolved>2012-08-09T19:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>No highlighting for phrases with stop words when term vector with positions offsets is enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2157</link><project id="" key="" /><description>If a record contains a phrase with stop words in the middle  (`foo and bar`, for example) and the search expression contains a matching phrase, highlighter produces different results that depend on how the field was indexed. 
- If the field was indexed with `"term_vector": "with_positions_offsets"` elasticsearch doesn't highlight anything
- if the field was indexed with `"term_vector": "with_positions_offsets"` and the field's analyzer has `"enable_position_increments": false`, elasticsearch highlights the entire phrase: `foo and bar`. 
- if the field was indexed without "term_vector" enabled elasticsearch highlights words `foo` and `bar` separately.

Full repro can be found here: https://gist.github.com/3280061

Solr seems to be behaving the same way: see https://gist.github.com/3279879#file_response_fvh.txt  and [SOLR-3724](https://issues.apache.org/jira/browse/SOLR-3724)

A possible workaround for us would be to add a flag on the highlighter query and globally that would disable fast vector highlighter. FVH is relatively new portion of lucene and this is already the second time when I wish I had a way to temporarily disable it without reindexing my data. (The first time was [LUCENE-3719](https://issues.apache.org/jira/browse/LUCENE-3719).) I could submit a pull request if you think that would be a useful feature.
</description><key id="6128596">2157</key><summary>No highlighting for phrases with stop words when term vector with positions offsets is enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2012-08-09T14:50:30Z</created><updated>2016-11-24T18:13:24Z</updated><resolved>2016-11-24T18:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-09T18:51:52Z" id="7623282">Yea, I think adding a "type" for highlighting will make sense, as long as we throw a proper failure when trying to use "fvh" and no term vectors exists.
</comment><comment author="clintongormley" created="2014-12-24T15:36:00Z" id="68059004">Weirdly, even though it is no longer possible to disable position increments in the stop filter, the FVH still doesn't highlight phrases with stopwords.
</comment><comment author="ajhalani" created="2015-09-02T21:31:03Z" id="137250553">Any chance there has been any update on this?? The default "english" analyzer exposed this issue so easily. 
</comment><comment author="aih" created="2015-09-13T07:30:43Z" id="139848957">+1 I need to use the fvh filter to combine the highlights from searching two fields (one using the English analyzer and one using whitespace, to capture exact matches). However, highlighting fails for phrases with stopwords.
</comment><comment author="nik9000" created="2015-09-13T11:36:33Z" id="139862674">Give the [experimental highlighter](https://github.com/wikimedia/search-highlighter) a shot. It works reasonably well as a replacement for the fvh and _should_ support this though I don't remember testing it explicitly. It [looks](https://en.wikipedia.org/w/index.php?search=to+be+or+not+to+be&amp;title=Special%3ASearch&amp;go=Go&amp;fulltext=1) like this works, though it might be working because of the more whitespace like analyzer in use over there in addition to the english one.

I believe I'll be working a ton more on highlighters in the coming months and I'll make sure this is on the list of things we fix.
</comment><comment author="aih" created="2015-09-14T06:25:09Z" id="139973618">Thank you, @nik9000! I tried the experimental highlighter and it had the same behavior as fvh (ignoring stopwords in phrase searches, and therefore missing matches where the stopword is in the middle-- e.g. "motion to adjourn"). I also noticed that the experimental highlighter doesn't support whole-field fragments (number_of_fragments = 0).
**However**, your comment solved my problem. Indeed, it looks like the Wikipedia search matches phrases because it uses a whitespace-like analyzer. So, I took out the stopwords from my English analyzer, reindexed, and now I've got phrase highlighting with stopwords. Would still be nice to have the option to match stopwords with the highlighter only, but this works for now.
</comment><comment author="clintongormley" created="2016-11-06T09:22:24Z" id="258666812">Updated for 5.0.  FVH still doesn't handle phrase queries with stop words unless slop is included:

```
PUT /test-idx
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "standard_stop": {
            "type": "standard",
            "stopwords": "_english_"
          }
        }
      }
    }
  },
  "mappings": {
    "rec": {
      "properties": {
        "message": {
          "type": "text",
          "analyzer": "standard",
          "term_vector": "with_positions_offsets",
          "fields": {
            "stop": {
              "type": "text",
              "analyzer": "standard_stop",
              "term_vector": "with_positions_offsets"
            }
          }
        }
      }
    }
  }
}

PUT /test-idx/rec/1
{
  "message": "foo and bar and something else"
}

GET /test-idx/_search
{
  "highlight": {
    "fields": {
      "message": {},
      "message.stop": {}
    }
  },
  "query": {
    "query_string": {
      "fields": ["message","message.stop"],
      "query": "\"foo and bar\""
    }
  }
}

```

FVH works when slop is enabled:

```
GET /test-idx/_search
{
  "highlight": {
    "fields": {
      "message": {},
      "message.stop": {}
    }
  },
  "query": {
    "query_string": {
      "fields": ["message","message.stop"],
      "query": "\"foo and bar\"~4"
    }
  }
}
```
</comment><comment author="yoav2" created="2016-11-10T13:12:57Z" id="259686470">Is there any solution for this?. highlighting is not working when I index with English Analyzer, and search for a phrase with stop words.
</comment><comment author="clintongormley" created="2016-11-10T13:45:47Z" id="259693354">@jimczi The FVH doesn't handle phrase matches when slop is involved - is this a bug in our implementation or in Lucene?
</comment><comment author="yoav2" created="2016-11-10T14:15:52Z" id="259700263">How can I make the stop words highlighted? (using English Analyzer and With_Positions_Offsets) in phrase matches query?
</comment><comment author="jimczi" created="2016-11-10T16:33:08Z" id="259738203">The problem is in Lucene:
https://issues.apache.org/jira/browse/LUCENE-7551
The FVH does not handle gaps in phrase query. I'll work on this.
</comment><comment author="dadoonet" created="2016-11-10T18:07:00Z" id="259763175">@jimczi apparently you opened the JIRA issue twice :)
Do you think this is related to https://issues.apache.org/jira/browse/LUCENE-7541 as well?
</comment><comment author="jimczi" created="2016-11-10T20:32:07Z" id="259799349">Thanks @dadoonet, this is not related. This one is about gaps in phrase query and the other one is about repeated tokens in the query. The multi tagging system of the FVH is at the token level and it will not be easy to fix that.
</comment><comment author="clintongormley" created="2016-11-24T18:13:23Z" id="262827650">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add lenient option to match &amp; multi_match queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2156</link><project id="" key="" /><description>The lenient option will control format based failure to be ignored (just as in `query_string` query).
</description><key id="6127033">2156</key><summary>Add lenient option to match &amp; multi_match queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-08-09T13:43:11Z</created><updated>2012-08-09T19:57:27Z</updated><resolved>2012-08-09T19:57:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-09T19:57:27Z" id="7625160">Pushed the pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API: Update through an alias with routing configured on it fail to use the routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2155</link><project id="" key="" /><description /><key id="6126277">2155</key><summary>Update API: Update through an alias with routing configured on it fail to use the routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-09T13:07:53Z</created><updated>2012-08-09T13:15:00Z</updated><resolved>2012-08-09T13:15:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve JMX security for elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2154</link><project id="" key="" /><description>Hi,

I would like to secure JMX for elasticsearch.

For example I tried this in /etc/default/elasticsearch :

{code}
ES_JAVA_OPTS="${ES_JAVA_OPTS} -Dcom.sun.management.jmxremote.authenticate=true
ES_JAVA_OPTS="${ES_JAVA_OPTS} -Dcom.sun.management.jmxremote.password.file=/etc/elasticsearch/jmxremote.password
ES_JAVA_OPTS="${ES_JAVA_OPTS} -Dcom.sun.management.jmxremote.access.file=/etc/elasticsearch/jmxremote.access
{code}

It will be possible in the next version ?

Best regards
</description><key id="6125412">2154</key><summary>Improve JMX security for elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">florent26</reporter><labels /><created>2012-08-09T12:17:18Z</created><updated>2013-07-17T12:28:51Z</updated><resolved>2013-07-17T12:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cwsusa" created="2012-09-24T15:45:29Z" id="8823436">I'm pretty sure you need to specify the port if you're using authentication. And there are peculiarities to windows, what's your platform? AND, there is a security hole in Java 6. I believe the current security baseline for Sun JDK is 6.33, you can find the security baseline by drilling down on release notes.

Here's the page on jmx, which says to spec the port if you're using any other properties....   JMX is not controlled by elastic search, although there may be beans in it for es.

http://docs.oracle.com/javase/6/docs/technotes/guides/management/agent.html
</comment><comment author="mrflip" created="2013-04-05T19:55:09Z" id="15977133">Note that as per #2728 in recent (0.90+) versions JMX is not handled by ES; you should add the flags yourself. For example (change 1.2.3.4 and 9400 as desired):

```
   java ...  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=1.2.3.4 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9400 ...
```
</comment><comment author="cwsusa" created="2013-04-06T01:00:50Z" id="15988295">People need this spelled out for them?  This community has a long way to go &#8230;.

From: Philip (flip) Kromer [mailto:notifications@github.com] 
Sent: Friday, April 05, 2013 3:56 PM
To: elasticsearch/elasticsearch
Cc: cwsusa
Subject: Re: [elasticsearch] Improve JMX security for elasticsearch (#2154)

Note that as per #2728 https://github.com/elasticsearch/elasticsearch/issues/2728  in recent (0.90+) versions JMX is not handled by ES; you should add the flags yourself. For example (change 1.2.3.4 and 9400 as desired):

   java ...  -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Djava.rmi.server.hostname=1.2.3.4 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9400 ...

&#8212;
Reply to this email directly or view it on GitHub https://github.com/elasticsearch/elasticsearch/issues/2154#issuecomment-15977133 .  https://github.com/notifications/beacon/KPp08FeMnIZrWh_9VoJyQMFfnIenXZgK0M-lWht3wb397cAeeL9RH_Y9sAhFw0lZ.gif 
</comment><comment author="spinscale" created="2013-07-17T12:28:51Z" id="21109282">closing, as this one is stale, we disabled JMX for most of the elasticsearch information. You still can enable it if you need to though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2153</link><project id="" key="" /><description>## Multi match query

The `multi_match` query builds further on top of the `match` query by allowing multiple fields to be specified. The idea here is to allow to more easily build a concise match type query over multiple fields instead of using a relative more expressive query by using multiple match queries within a `bool` query.

The structure of the query is a bit different. Instead of a nested json object defining the query field, there is a top json level field for defining the query fields. Example:

```
{
  "multi_match" : {
    "query" : "this is a test",
    "fields" : [ "subject", "message" ]
  }
}
```

The `multi_match` query creates ether a `bool` or a `dis_max`top level query. Each field is a query clause in this top level query. The query clause contains the actual query (the specified 'type' defines what query this will be). Each query clause is basically a `should` clause.
## Options

All options that apply on the `match` query also apply on the `multi_match` query. The 'match' query options apply only on the individual clauses inside the top level query.
- `fields` - Fields to be used in the query.
- `use_dis_max` - Boolean indicating to either create a `dis_max` query or a `bool` query. Defaults to `true`.
- `tie_breaker` - Multiplier value to balance the scores between lower and higher scoring fields. Only applicable when `use_dis_max` is set to true. Defaults to `0.0`.

The query accepts all the options that a regular `match` query accepts.
</description><key id="6124792">2153</key><summary>Multi match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-09T11:38:59Z</created><updated>2012-08-09T13:22:55Z</updated><resolved>2012-08-09T13:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-09T13:22:54Z" id="7613795">Implemented in #2151.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Map call no longer automatically creates index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2152</link><project id="" key="" /><description>issuing the tweet mapping call from the docs:

http://www.elasticsearch.org/guide/reference/api/admin-indices-put-mapping.html

results in a 404

```
{"error":"IndexMissingException[[twitter] missing]","status":404}
```

I remember in earlier versions it would just create the index like it does when first processing a new doc. 
</description><key id="6120407">2152</key><summary>Map call no longer automatically creates index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>discuss</label></labels><created>2012-08-09T06:29:23Z</created><updated>2014-07-25T08:05:08Z</updated><resolved>2014-07-25T08:05:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T08:05:08Z" id="50119661">Given that it's been 2 years and nobody seems to mind too much, closing this ticket
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added support for multi match query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2151</link><project id="" key="" /><description /><key id="6115642">2151</key><summary>Added support for multi match query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-08T23:00:48Z</created><updated>2014-06-17T05:11:37Z</updated><resolved>2012-08-09T10:16:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-09T10:16:22Z" id="7610385">pushed to master and 0.19, great!. Lets create an issue that has the docs for this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename text query to match query (text query still works, with variants)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2150</link><project id="" key="" /><description>The `text` query should be the "go to" query when wanting to match on a field. It already supports numerics and such (thus text is a bit misleading), and properly handles analysis cases for string queries.

So, all options of `text` should be renamed to `match`, but still support `text`.
</description><key id="6099607">2150</key><summary>Rename text query to match query (text query still works, with variants)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-08T10:21:34Z</created><updated>2012-08-09T12:34:42Z</updated><resolved>2012-08-08T12:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chrismale" created="2012-08-08T10:26:04Z" id="7580132">One thing I just wonder about this, coming from Lucene, is that 'match' suggests to me that the query isn't scored, that it just does the matching part of querying.
</comment><comment author="kimchy" created="2012-08-08T13:11:29Z" id="7583037">@chrismale didn't think about that, for me match does not mean not being scored (specifically also the fact that its a query). Open for better names :)
</comment><comment author="uboness" created="2012-08-08T13:16:37Z" id="7583140">How about "standard"? It indicates that this is the "go to" query type and doesn't hint anything about scoring
</comment><comment author="chrismale" created="2012-08-08T13:20:32Z" id="7583242">Yeah I quite like 'standard'
</comment><comment author="kimchy" created="2012-08-08T13:24:02Z" id="7583319">The problem with things like `standard`, `default`, or `simple` is that they don't really indicate what they do. `match` on the other hand does.
</comment><comment author="chrismale" created="2012-08-08T13:28:05Z" id="7583423">I would definitely shy away from 'simple' since it doesn't sound simple and something simpler could come along.  But as you say, this is a type query so people know what it does - it queries documents so I think standard or default wouldn't confuse.
</comment><comment author="uboness" created="2012-08-08T13:28:05Z" id="7583424">Does it really? I'd say all queries do some sort of matching and indeed since this query does so much, no name can really be descriptive enough... I don't like simple, because what makes it simple and the others complex? And I don't like default, as it's not the default and for me it also indicates that it's actually a pointer to another type which is marked as the default....
</comment><comment author="clintongormley" created="2012-08-09T11:45:01Z" id="7611908">I like `match` because:
- it's short, which makes it easier to type than `simple`, `default`, `standard`, 
- it's expressive "match this"
- it doesn't downplay its abilities like `simple` would
- I read it in the English sense, not the Lucene sense, so for me there is no implication of whether it would score or not. It just "does the right thing"
</comment><comment author="uboness" created="2012-08-09T12:34:42Z" id="7612785">And this is exactly why we need the input from  non Lucene people :), thanks clint, makes sense.

On Thursday, August 9, 2012 at 1:45 PM, Clinton Gormley wrote:

&gt; I like match because:
&gt; it's short, which makes it easier to type than simple, default, standard,  
&gt; it's expressive "match this"
&gt; it doesn't downplay its abilities like simple would  
&gt; I read it in the English sense, not the Lucene sense, so for me there is no implication of whether it would score or not. It just "does the right thing"
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub (https://github.com/elasticsearch/elasticsearch/issues/2150#issuecomment-7611908).  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>URI Request that returns just the _source, without metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2149</link><project id="" key="" /><description>'http://localhost:9200/twitter/tweet/_search?q=user:kimchy' returns:

``` json
{
    "_shards":{
        "total" : 5,
        "successful" : 5,
        "failed" : 0
    },
    "hits":{
        "total" : 1,
        "hits" : [
            {
                "_index" : "twitter",
                "_type" : "tweet",
                "_id" : "1", 
                "_source" : {
                    "user" : "kimchy",
                    "postDate" : "2009-11-15T14:12:12",
                    "message" : "trying out Elastic Search"
                }
            }
        ]
    }
}
```

But sometimes it would be more useful to get a plain "dump" of the _source data instead:

``` json
{
    ...
    "hits":{
        "total" : 1,
        "hits" : [
            {
                "user" : "kimchy",
                "postDate" : "2009-11-15T14:12:12",
                "message" : "trying out Elastic Search"
            }
        ]
    }
}
```
</description><key id="6092796">2149</key><summary>URI Request that returns just the _source, without metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels><label>discuss</label></labels><created>2012-08-08T00:42:37Z</created><updated>2014-08-22T11:03:56Z</updated><resolved>2014-08-22T11:03:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xstevens" created="2013-06-20T20:35:27Z" id="19781340">This would be really useful to have. In my case I'm trying to do HTTP response caching but "took" in the results obviously can change on each query even though the results are the same.
</comment><comment author="spinscale" created="2013-06-24T07:49:08Z" id="19893274">Hey,

you can do this with the current elasticsearch release for single documents (but not for searches)

```
curl -X PUT localhost:9200/foo/bar/1 -d '{ "name":"foo", "f":"a" }'
{"ok":true,"_index":"foo","_type":"bar","_id":"1","_version":2}                                                                                                                                          

curl localhost:9200/foo/bar/1/_source
{ "name":"foo", "f":"a" }
```

@xstevens If you really need to this for searches, putting a varnish proxy (or something similar) front makes more sense.

@ejain Can you tell what the big difference of only having the source compared to having the source including the metadata is anyway in a search response? Maybe I didnt get your request completely right.
</comment><comment author="xstevens" created="2013-06-24T17:18:05Z" id="19921433">Well this wasn't really my request but it would work for what I want. I'm looking to remove the "took" variable from search results because that's what blows out an HTTP response cache. What I mean by that is, I end up with an entry per took="response time" even though the rest of the data stays the same.
</comment><comment author="spinscale" created="2013-06-24T21:54:48Z" id="19938888">Hey,

I am still not sure, if these are the right approaches to the problem, as I am still unsure about the problem. Maybe you can elaborate on what you want to do. If you simply want to cache the response, is it really important, if the `took` value is included in the response? I mean, does it matter? If an old took value is sent, because the search response is cached, what does this mean for you? Is that bad?

I am not sure, how your caching is working either. Is that configurable? Or do you simply cache the result of a certain  request with a certain body? Maybe you can use the `X-Unique-Id` header for this (can be specified in the request and is included in the response as well), but I cannot really tell, until I understand your caching strategy (and why you are so focused on some fields :-)
</comment><comment author="xstevens" created="2013-06-24T22:57:06Z" id="19942053">I'm just trying to do basic HTTP response caching with no knowledge that's it is even ElasticSearch that I'm talking to. I'm using Apache HttpClient caching that comes built-in. The reason why the "took" field is a problem is because the caching mechanism is checking on whether the payload (search result in this case) has changed in the background. So it's invalidating the cache more often than it needs to. I can work around this of course by doing my own caching, but I was going to try to avoid that since HttpClient has some other nice checks around Cache-Control headers, etc. for services that give that kind of feedback.
</comment><comment author="xstevens" created="2013-06-24T22:58:41Z" id="19942134">As far as how HttpClient is detecting a payload change I believe their impelmentation is using SHA256(payload).
</comment><comment author="ejain" created="2013-06-25T18:15:16Z" id="19996015">My use case is that I need to let users download their documents in bulk; this would be a lot more efficient if I didn't have to parse the response and strip out elasticsearch-specific properties.
</comment><comment author="dpkirchner" created="2013-10-14T19:33:59Z" id="26282258">@spinscale What version introduced _source? I get "No handler found for uri /index/type/NNN/_source" on 0.26.

This feature would be really useful for me as well (I'd like to be able to download documents in bulk and then update them in bulk without having to do surgery).
</comment><comment author="brusic" created="2013-10-14T21:08:46Z" id="26288958">@therealdpk Judging by the commit/issue, the feature will be available in elasticsearch 1.0. Someone please correct me if I am wrong, but I am curious as well and I do not see it in the 0.90 branch.

https://github.com/elasticsearch/elasticsearch/issues/3301
</comment><comment author="spinscale" created="2013-10-15T07:39:57Z" id="26314654">@therealdpk it was introduced in 0.90.1

@brusic the issue you referred to is for more fine grained access control to the source without changing the data structure layout when requesting the data (which can happen in few cases)

https://github.com/elasticsearch/elasticsearch/blob/0.90/src/main/java/org/elasticsearch/rest/action/get/RestGetSourceAction.java
</comment><comment author="brusic" created="2013-10-15T17:59:46Z" id="26357471">Sorry for the misinformation. I assumed the _source param would be part of the normal RestGetAction.
</comment><comment author="karmi" created="2013-10-23T15:31:26Z" id="26915127">Just a correction, the correct header is `X-Opaque-Id`, not `X-Unique-Id`:

``` bash
curl -i -H "X-Opaque-Id: foobar" localhost:9200/_search | grep foobar
```
</comment><comment author="abhijitiitr" created="2013-12-15T10:18:06Z" id="30601643">Is this feature implemented in the latest beta version?
Shouldn't the _source only option be a part of _search &amp; _msearch similar to _get &amp; _mget.
</comment><comment author="clintongormley" created="2014-07-25T07:59:04Z" id="50119169">Given that this isn't a common use case, and can be solved easily on the application side (by extracting the hits only and sha'ing just those), we've decided against making any changes here.
</comment><comment author="kuseman" created="2014-08-13T14:11:27Z" id="52052987">Could this be opened again and reconsidered?

Solving this on the application side is not an option for us because then it's too late.
We have certain queries that only request small amount of data from each document, then in the whole 80-90% of the response is just metadata and is garbage to us and slows down the response times.

Being able to exclude the meta data would be awesome.
</comment><comment author="brusic" created="2014-08-18T18:55:39Z" id="52537985">Take a look at J&#246;rg's plugin: https://github.com/jprante/elasticsearch-arrayformat
</comment><comment author="kuseman" created="2014-08-19T07:09:41Z" id="52597730">Added https://github.com/elasticsearch/elasticsearch/pull/7330
</comment><comment author="clintongormley" created="2014-08-22T11:03:56Z" id="53048285">We're keen to provide a more generic solution to this problem, so I'm going to close this issue in favour of #7401 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>shardFailures in SearchResponse showing from previous query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2148</link><project id="" key="" /><description>Steps to repro issue with the shardFailures array showing msgs from a previous (not current) query:

1) create something
curl -XPUT localhost:9200/acme/blog/1111 -d '{"message":"foo"}'

2) execute this query - it should succeed
curl -XGET localhost:9200/acme/blog/_search -d '{"query":{"field":{"message":"foo"}}}'

3) this is an invalid query and is expected to fail
curl -XGET localhost:9200/acme/blog/_search -d '{"foobar":{"message":"foo"}}'

4) now rerun the query from step 2. This query succeeds, and returns the hits, but also shows the shardFailures from the failed query in step 3
curl -XGET localhost:9200/acme/blog/_search -d '{"query":{"field":{"message":"foo"}}}'

Here is the output I get from step 4)  :

{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0,"failures":[{"index":"acme","shard":1,"status":400,"reason":"SearchParseException[[acme][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"foobar\":{\"message\":\"foo\"}}]]]; nested: SearchParseException[[acme][1]: from[-1],size[-1]: Parse Failure [No parser for element [foobar]]]; "},{"index":"acme","shard":0,"status":400,"reason":"SearchParseException[[acme][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"foobar\":{\"message\":\"foo\"}}]]]; nested: SearchParseException[[acme][0]: from[-1],size[-1]: Parse Failure [No parser for element [foobar]]]; "},{"index":"acme","shard":4,"status":400,"reason":"SearchParseException[[acme][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"foobar\":{\"message\":\"foo\"}}]]]; nested: SearchParseException[[acme][4]: from[-1],size[-1]: Parse Failure [No parser for element [foobar]]]; "}]},"hits":{"total":1,"max_score":1.6931472,"hits":[{"_index":"acme","_type":"blog","_id":"1111","_score":1.6931472, "_source" : {"message" : "foo" }}]}}
</description><key id="6091838">2148</key><summary>shardFailures in SearchResponse showing from previous query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lmader</reporter><labels /><created>2012-08-07T23:31:38Z</created><updated>2013-11-02T08:44:42Z</updated><resolved>2013-11-02T08:44:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-08T09:50:29Z" id="7579500">This has been fixed in 0.19, doubled check but you can double check as well.
</comment><comment author="spinscale" created="2013-10-30T09:06:08Z" id="27373841">@lmader can you confirm if this is fixed for you or not?
</comment><comment author="lmader" created="2013-11-01T20:06:30Z" id="27596859">I just re-tested this on elasticsearch 0.90.5 and I can confirm that this
is fixed.  I think we noticed that it was fixed in 0.19.x as well.

On Wed, Oct 30, 2013 at 2:06 AM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; @lmader https://github.com/lmader can you confirm if this is fixed for
&gt; you or not?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2148#issuecomment-27373841
&gt; .
</comment><comment author="spinscale" created="2013-11-02T08:44:42Z" id="27618125">Thanks a lot for your feedback! Closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed timezone parsing when input starts with '+'sign.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2147</link><project id="" key="" /><description /><key id="6075535">2147</key><summary>Fixed timezone parsing when input starts with '+'sign.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-07T13:03:47Z</created><updated>2015-05-18T23:35:48Z</updated><resolved>2012-08-07T20:56:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-07T20:56:53Z" id="7566566">pushed to 0.19 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow custom type_table to be passed to Lucene's WordDelimiterFilter() constructor, instead of just the default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2146</link><project id="" key="" /><description>fixes #2145
</description><key id="6065771">2146</key><summary>Allow custom type_table to be passed to Lucene's WordDelimiterFilter() constructor, instead of just the default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jacobevans</reporter><labels /><created>2012-08-07T00:08:58Z</created><updated>2014-06-23T14:19:56Z</updated><resolved>2012-08-07T22:20:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-07T22:20:00Z" id="7568791">Pushed to 0.19 and master, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>word_delimiter token filter does not honor "type_table" option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2145</link><project id="" key="" /><description>So, `elasticsearch/src/main/java/org/elasticsearch/index/analysis/WordDelimiterTokeFilterFactory.java` has a bunch of logic for parsing the type_table / type_table_path option for word_delimiter, and intersecting it with `WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE` (`parseTypes()`) on line 114.

However, the instance variable that that function populates (line 66) is charTypeTable, which is never used!

in create(), on line 93, the second parameter, which defines the character type table for WordDelimiterFilter is simply:

```
WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE
```

I believe it should simply be: `charTypeTable` since lines 63-67 read as follows:

```
    if (charTypeTableValues == null) {
        this.charTypeTable = WordDelimiterIterator.DEFAULT_WORD_DELIM_TABLE;
    } else {
        this.charTypeTable = parseTypes(charTypeTableValues);
    }
```

I'll make this change, and open a pull request, but I don't have a java env set up to test this at the moment, so I can't be 100% sure.
</description><key id="6065504">2145</key><summary>word_delimiter token filter does not honor "type_table" option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jacobevans</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-06T23:46:35Z</created><updated>2012-08-07T22:20:40Z</updated><resolved>2012-08-07T22:20:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-07T22:20:40Z" id="7568808">Implemented in pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Search Client library fails when dealing with small score values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2144</link><project id="" key="" /><description>repro: set score script to 0.00000000000000001, execute query, get results, and then try to do getHits() on the result and see it break.

Work around: if your score script will produce really small values, add 1.0 to your score script.
</description><key id="6059898">2144</key><summary>Elastic Search Client library fails when dealing with small score values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">holdenk</reporter><labels /><created>2012-08-06T19:18:54Z</created><updated>2014-07-08T15:41:13Z</updated><resolved>2014-07-08T15:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T13:54:32Z" id="20519956">I could not reproduce this with 0.90. Can you provide a test case? Thanks!
</comment><comment author="clintongormley" created="2014-07-08T15:41:13Z" id="48356424">No further info after 2 years. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed top children query bug reported in issue #2140</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2143</link><project id="" key="" /><description /><key id="6047838">2143</key><summary>Fixed top children query bug reported in issue #2140</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-06T11:24:21Z</created><updated>2015-05-18T23:35:52Z</updated><resolved>2012-08-06T20:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gustavobmaia" created="2012-08-06T17:25:02Z" id="7529644">Hi,
In the coming version of the elasticsearch it already available?
</comment><comment author="kimchy" created="2012-08-06T20:03:28Z" id="7534075">@gustavobbamaia it will be available in upcoming 0.19.9.
</comment><comment author="kimchy" created="2012-08-06T20:25:12Z" id="7534643">pushed to 0.19 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[FEATURE] More helpful error msg if plugin download dir is not writable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2142</link><project id="" key="" /><description>Currently, trying to `plugin -install` leads to 

```
Failed to install ...., reason: failed to download
```

The obvious problem was that the appropriate target directory was not writable by the exec'ing user; so it'd be helpful to have an error message that covers and explains this case.

Definition of "done": Have an error message like 

```
Failed to install ...., reason: directory "/foo/bar/baz" not writable by the user
```
</description><key id="6047506">2142</key><summary>[FEATURE] More helpful error msg if plugin download dir is not writable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">afoeder</reporter><labels /><created>2012-08-06T11:03:49Z</created><updated>2013-07-05T10:25:57Z</updated><resolved>2013-07-05T10:25:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T10:25:57Z" id="20511746">Looks like it has been solved in the meantime

```
bin/plugin -install lukas-vlcek/bigdesk
-&gt; Installing lukas-vlcek/bigdesk...

Failed to install lukas-vlcek/bigdesk, reason: plugin directory /path/to/elasticsearch-0.90.2/plugins is read only
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Histogram facet fails with "pre_zone" : "+02:00"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2141</link><project id="" key="" /><description>"pre_zone" : "-02:00" works fine, as does "02:00", but "+02:00" fails with a NumberFormatException:

Execution exception [[SearchPhaseExecutionException: Failed to execute phase [query_fetch], total failure; shardFailures {[vdmcVg94Qw-kgMeAy9Ibtg][u07qih0a27][0]: RemoteTransportException[[Abominable Snowman][inet[/10.29.147.216:9300]][search/phase/query+fetch]]; nested: SearchParseException[[u07qih0a27][0]: query[ConstantScore(NotDeleted(_:_))],from[0],size[10],sort[&lt;custom:"timestamp": org.elasticsearch.index.field.data.longs.LongFieldDataType$1@1a91abc&gt;!]: Parse Failure [Failed to parse source [{"from":0,"size":10,"query":{"match_all":{}},"sort":[{"timestamp":{"order":"desc"}}],"facets":{"test":{"date_histogram":{"key_field":"timestamp","value_field":"timestamp","interval":"month","pre_zone":"+02:00","pre_zone_adjust_large_interval":true}}}]]; nested: NumberFormatException[For input string: "+02"]; }]]
</description><key id="6039146">2141</key><summary>Date Histogram facet fails with "pre_zone" : "+02:00"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels><label>bug</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-05T19:31:27Z</created><updated>2012-08-07T20:57:41Z</updated><resolved>2012-08-07T20:57:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-07T20:57:40Z" id="7566590">Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem Parents &amp; Children</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2140</link><project id="" key="" /><description>I had one small problem when doing a search using TopChildrenQuery and realized that when I have more than 51 children to the same document, my search has not returned any results. Digging deeper I found comparison what might be wrong in class QueryPhase.

I posted a code to simulate this problem in gist.

https://gist.github.com/3140681
</description><key id="6032077">2140</key><summary>Problem Parents &amp; Children</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels /><created>2012-08-04T19:02:05Z</created><updated>2014-06-30T14:27:12Z</updated><resolved>2012-08-06T20:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2012-08-06T11:26:05Z" id="7521789">Good catch finding this error! Your fix doesn't solve the underlying issue. When the query re-executed the TCQ needs to be cleared; this didn't happen. See #2143 for details. 
</comment><comment author="kimchy" created="2012-08-06T20:26:12Z" id="7534675">Closing this as its fixed in #2140.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding wrong port exception for misguided HTTP requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2139</link><project id="" key="" /><description>Hi kimchy,

I think it is useful to add a special wrong port handling to netty transport, so people who accidentally connect with a HTTP client to port 9300 will receive a small diagnostic message.

Cheers,

J&#246;rg
</description><key id="6027037">2139</key><summary>adding wrong port exception for misguided HTTP requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jprante</reporter><labels><label>enhancement</label></labels><created>2012-08-04T01:03:22Z</created><updated>2015-03-17T16:57:11Z</updated><resolved>2015-03-17T16:57:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gjdev" created="2012-08-07T17:59:39Z" id="7561445">This pull-request will deny documents having one of the lengths you test against. That's going to be very unfortunate in a production system that actually indexes a document of that size...
</comment><comment author="jprante" created="2012-08-07T19:29:16Z" id="7564008">The four numbers exceed 1 GB. Practically, nobody is likely to send requests to ES with &gt; 1GB in size. Beside the inconvenience (it would take quite a long time for the transport and the memory usage would be very inefficient high), it should be noticed that ES nodes have heap sizes lower than 1 GB by default (see line " ES_MAX_MEM=1g" in bin/elasticsearch.in.sh), and such big messages would be rejected anyway, with the scary non-friendly message. 

But in theory, if ES_MAX_MEM is increased, you are right, four message sizes would be no longer possible. 

A fix for this situation could only be done by changing the netty transport protocol. This would be an incompatible change to former versions of ES so I hesitate. My suggestion for such an incompatible change would be marking each netty transport protocol message by a magic header ID before the message length. Example four bytes of 'E', 'S', '\x0', '\x1' where 'ES' means 'elasticsearch' and \x00\x01 is the protocol version in binary form. 

I can work on such a pull request for a 'magic marker' if required.
</comment><comment author="kimchy" created="2012-08-07T22:14:22Z" id="7568655">@jprante thats what I was thinking of adding for some time now, if we have a magic marker on the transport level, it will improve our resiliency for failures. Requires some thinking, but we can do it on in upcoming 0.20.
</comment><comment author="dakrone" created="2014-11-21T10:23:08Z" id="63951828">@spinscale assigning this to you, can you take a look at this?
</comment><comment author="dakrone" created="2015-03-14T22:03:34Z" id="80740359">@spinscale ping on this request?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve recovery time when processing large mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2138</link><project id="" key="" /><description>Effectively, because its recovery time, we can perform some bulk "add" operations where applicable.
</description><key id="6017335">2138</key><summary>Improve recovery time when processing large mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-03T15:39:33Z</created><updated>2012-08-03T15:40:59Z</updated><resolved>2012-08-03T15:40:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>TooLongFrameException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2137</link><project id="" key="" /><description>While debugging pipeline stalls in logstash, I found this in elasticsearch's output:

```
[2012-08-01 18:27:13,778][WARN ][transport.netty          ] [Penance] Exception caught on netty layer [[id: 0x5f315cd6, /127.0.0.1:51889 =&gt; /127.0.0.1:9300]]org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: transport content length received [1.4gb] exceeded [910.1mb]        
    at org.elasticsearch.transport.netty.MessageChannelHandler.callDecode(MessageChannelHandler.java:143)        
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:101)        
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)        
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:563)        
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)        
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)        
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:563)        
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)        
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)        
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)        
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:91)         
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:373)        
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:247)        
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)          
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
    at java.lang.Thread.run(Thread.java:619)
```

My config is:

```
% grep '^[A-z]' config/elasticsearch.yml 
network.bind_host: 127.0.0.1
network.host: 127.0.0.1
transport.tcp.compress: true
http.host: 127.0.0.1
discovery.zen.ping.multicast.enabled: false
index.store.compress.stored: true
index.store.compress.tv: true
```

On a hunch, I'm going to disable transport compression to see if this makes the problem go away. I'll try to reproduce if it happens again.
</description><key id="5994116">2137</key><summary>TooLongFrameException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels /><created>2012-08-02T14:57:24Z</created><updated>2014-07-08T15:40:17Z</updated><resolved>2014-07-08T15:40:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2012-08-02T15:11:39Z" id="7458027">Just to be clear on what I observed:
- logstash was writing to elasticsearch happily
- at some point hours ago, that writing stopped. No new documents
- on investigation, saw ElasticSearch logs as shown above
- also saw these on the logstash side: `[dh-mail-2012.58.02][2] [2] shardIt, [1] active : Timeout waiting for [1m], request: index {[dh-mail-2012.58.02][mail][g9NMfU1sQhOUZ5Q4ZYrsIQ]`

I found reasonable correlation between indexing problems and these frame errors.
</comment><comment author="kimchy" created="2012-08-03T06:45:33Z" id="7476912">Is there a chance that a bulk indexing request actually ended up being ~1.4gb in size?
</comment><comment author="jordansissel" created="2012-08-03T06:50:32Z" id="7476965">it seems unlikely. The logstash events/documents were simple mail logs (maybe 1kb in size), and I was using the non-bulk index api from java.
</comment><comment author="drewp" created="2012-09-24T06:42:49Z" id="8808632">I got this just now while testing logstash and ES. Tiny dataset (du says 2.8MB on disk); ElasticSearch Version: 0.19.9, JVM: 16.3-b01, logstash 1.1.1. My trace is slightly different:

[2012-09-23 23:20:29,573][WARN ][transport.netty          ] [Ringo Kid] Exception caught on netty layer [[id: 0x6d09a8df, /127.0.0.1:37207 =&gt; /127.0.0.1:11033]]
org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: transport content length received [1.3gb] exceeded [917.8mb]
    at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:31)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:422)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:793)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:94)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:390)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:261)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)

Config was this:

path:
  conf: conf/elastic
  logs: log
  data: db/elasticsearch

cluster.name: ffglog
http.port: 11034
transport.tcp.port: 11033
discovery.zen.ping.multicast.enabled: true
</comment><comment author="kimchy" created="2012-09-24T07:56:19Z" id="8809808">Are you connecting using http on the tcp transport?
</comment><comment author="jonshea" created="2012-09-24T16:34:57Z" id="8825303">I just ran into this same issue, and, sure enough, I was sending http requests to the tcp transport port (9300), instead of to the http transport port (9200).
</comment><comment author="kimchy" created="2012-09-24T17:49:39Z" id="8827744">The next version (0.20) will have a better log message in this case, so it will be simpler to figure this out.
</comment><comment author="castorm" created="2012-10-18T09:30:45Z" id="9558404">I'm getting the same Exception by using bulkRequest from a client node which is unicast connected to a remote cluster.

The size mentioned in the Exception is not reasonable, I'm sendind log traces too, it's not possible to gather 1gb of data in a period of 10 seconds.

ElasticSearch version: 0.19.10

Regards.
</comment><comment author="castorm" created="2012-10-18T09:55:12Z" id="9559047">I just saw the reason of the message here: http://elasticsearch-users.115913.n3.nabble.com/what-to-do-with-ES-td4021167.html

That thread points to a port misunderstanding, but I don't think it was my case because I'm publishing through a client node not a TransporClient, so the only port I'm configuring is the one to connect through unicast to the cluster.
</comment><comment author="kimchy" created="2012-10-20T21:02:16Z" id="9635978">If you are using the `TransportClient` and pointing it to port `9200` (the HTTP one) you will get such failure.
If you are not making sure you control your single bulk request size, then you might get this failure as well.
</comment><comment author="stephenpatten" created="2013-05-30T04:24:09Z" id="18660847">Just got the same message;

2013-05-29 21:00:38,367][WARN ][http.netty               ] [Zodiak] Caught exception while handling client http traffic, closing co
nection [id: 0x59f04be1, /127.0.0.1:3005 =&gt; /127.0.0.1:9200]
rg.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: HTTP content length exceeded 104857600 bytes.
       at org.elasticsearch.common.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:167)
       at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
       at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
line.java:791)
       at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
       at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
       at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
       at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
       at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
       at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
line.java:791)
       at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
       at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
       at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
       at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
       at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
       at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
       at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
       at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
       at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
       at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
       at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
       at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
       at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
       at java.lang.Thread.run(Unknown Source)

I am following this example from http://www.fullscale.co/blog/2013/02/19/Using_ElasticSearch_To_Find_Best_Time_To_Ask_Questions_On_StackOverflow.html. Currently, the post.xml file is around 48.9 GB.

Thank you,
Stephen
</comment><comment author="s1monw" created="2013-05-30T05:59:43Z" id="18662993">Stephen,  you are postin a 48GB file to Elasticsearch? I think you should split it up in small chunks (maybe start with 10MB) and try again. The max frame of 1GB is already very large. You have to consider that we have to keep this frame around in memory?
</comment><comment author="stephenpatten" created="2013-05-30T16:04:05Z" id="18690292">This is with the bulk API... Anyways I didn't write the code (perl), that posts it to the server,  http://www.fullscale.co/blog/2013/02/19/Using_ElasticSearch_To_Find_Best_Time_To_Ask_Questions_On_StackOverflow.html#comment-913888942. 

It looks to me like it should work, no?
</comment><comment author="mattweber" created="2013-05-30T16:18:51Z" id="18691217">@stephenpatten the bulk size is too large (typo in python script).  Set it to a smaller size so it breaks up the posts.xml into small chunks before submitting to ES.
</comment><comment author="seti123" created="2014-02-08T20:36:27Z" id="34555322">+1 Version 0.90.10

org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: HTTP content length exceeded 104857600 bytes.

Update: I increased size in /etc/elasticsearch/elasticsearch.yml and it works again

&lt;pre&gt;
    # Set a custom allowed content length:
    # 
    http.max_content_length: 500mb
&lt;/pre&gt;
</comment><comment author="clintongormley" created="2014-07-08T15:40:17Z" id="48356276">Closed in favour of #2902 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_timestamp, _ttl not updating on using put mapping API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2136</link><project id="" key="" /><description>See this Groups thread for an example of this issue using _ttl:

https://groups.google.com/forum/?fromgroups#!searchin/elasticsearch/_timestamp/elasticsearch/v5G7C8beClc/mTf3hbJsqU8J

I'm trying to do something similar with _timestamp, change the value from the default (disabled) and although the response is OK-acknowledged, nothing actually changes on the index.

I can always create a new index and migrate the data, or just drop the current index and recreate, but it would be nice to be able to change these values without doing that, at least for new documents coming in.

There may be other mapping parameters that also do not update.
</description><key id="5991731">2136</key><summary>_timestamp, _ttl not updating on using put mapping API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">elentar</reporter><labels /><created>2012-08-02T13:11:24Z</created><updated>2013-04-02T09:00:46Z</updated><resolved>2013-04-02T06:37:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-22T10:54:49Z" id="15290865">This problem still exists in current 0.90 release. It also applies to other mapping parameters like _size.

I think there is a problem in the merging process when a mapping is updated with an existing mapping. I need to verify first, that this is not some undocumented feature to make sure you actually cannot change these values, but I dont think so.
</comment><comment author="spinscale" created="2013-03-22T13:54:19Z" id="15297623">I created a small fix for the TTLFieldMapper, which allows you to enable and disable the ttl field via the PUT Mapping API. I dont see a problem, why this should not work.

The reason for this problem is, that the field mapper do not make use of the Mapper.merge() method, thus making it impossible to change after initial creation.

I also see similar problems for TimestampFieldMapper, SizeFieldMapper (there is a comment in the merge() method), IndexFieldMapper, BoostFieldMapper.

I will create a PR for this, and if it is accepted, I'll fix the others.
</comment><comment author="spinscale" created="2013-03-25T11:12:06Z" id="15387537">After a small discussion, it is clear that we need a 'tri state' fieldmapper, which can be 'true', 'false' and 'unset' in order to cover this use-case (you would disable TTL again with my patch, if you simply changed the default timeout, which is a bad solution). I will try to create this as soon as possible.
</comment><comment author="spinscale" created="2013-04-02T09:00:46Z" id="15763993">Just a side note, this pull request only caters for fixing this issue with the TTL field, I will now fix the other mappers (including timestamp) in a separate PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Overwrite ignore_above parameter when mapping is updated. Issue #2121</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2135</link><project id="" key="" /><description /><key id="5990789">2135</key><summary>Overwrite ignore_above parameter when mapping is updated. Issue #2121</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-02T12:20:10Z</created><updated>2015-05-18T23:35:52Z</updated><resolved>2012-08-03T07:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-03T07:39:24Z" id="7477592">Pushed to 0.19 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Nodes hot_threads API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2134</link><project id="" key="" /><description>A new API allowing to get the current hot threads on each node in the cluster. Endpoints are `/_nodes/hot_threads`, and `/_nodes/{nodesIds}/hot_threads`.

The output is plain text with a breakdown of each node top hot threads. Parameters allows are: 
- `threads`: number of hot threads to provide, defaults to 3.
- `interval`: the interval to do the second sampling of threads. Defaults to `500ms`.
- `type`: The type to sample, defaults to `cpu`, but supports `wait` and `block` to see hot threads that are in wait or block state.
</description><key id="5990344">2134</key><summary>Cluster Nodes hot_threads API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-08-02T11:52:13Z</created><updated>2012-08-02T11:52:40Z</updated><resolved>2012-08-02T11:52:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added ignore_malformed mapping parameter. Issue #2120</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2133</link><project id="" key="" /><description /><key id="5989689">2133</key><summary>Added ignore_malformed mapping parameter. Issue #2120</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-08-02T11:05:06Z</created><updated>2015-05-18T23:35:53Z</updated><resolved>2012-08-03T07:52:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-03T07:52:45Z" id="7477791">Pushed to 0.19 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't define multiple predefined date formats using `||`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2132</link><project id="" key="" /><description>The following mapping:

```
'index': {
    'properties': {
        'date_created': {
            'type': 'date',
            'index': 'not_analyzed',
            'format: 'date_time||date_time_no_millis',
        },
    },
}
```

Gives an error:

`ElasticSearchException[Illegal pattern component: t]; nested: IllegalArgumentException[Illegal pattern component: t];`

I tried patching as follows, however didn't manage to get it working since `forPattern` wasn't called as expected:

```
diff --git a/src/main/java/org/elasticsearch/common/joda/Joda.java b/src/main/java/org/elasticsearch/common/joda/Joda.java
index a4f3427..e8d6e53 100644
--- a/src/main/java/org/elasticsearch/common/joda/Joda.java
+++ b/src/main/java/org/elasticsearch/common/joda/Joda.java
@@ -31,11 +31,11 @@ import org.joda.time.format.*;
  */
 public class Joda {

-    /**
-     * Parses a joda based pattern, including some named ones (similar to the built in Joda ISO ones).
-     */
-    public static FormatDateTimeFormatter forPattern(String input) {
+    private  static DateTimeFormatter formatterForPattern(String input) {
         DateTimeFormatter formatter;
         if ("basicDate".equals(input) || "basic_date".equals(input)) {
             formatter = ISODateTimeFormat.basicDate();
         } else if ("basicDateTime".equals(input) || "basic_date_time".equals(input)) {
@@ -76,9 +76,10 @@ public class Joda {
             formatter = ISODateTimeFormat.dateHourMinuteSecondMillis();
         } else if ("dateOptionalTime".equals(input) || "date_optional_time".equals(input)) {
             // in this case, we have a separate parser and printer since the dataOptionalTimeParser can't print
-            return new FormatDateTimeFormatter(input,
-                    ISODateTimeFormat.dateOptionalTimeParser().withZone(DateTimeZone.UTC),
-                    ISODateTimeFormat.dateTime().withZone(DateTimeZone.UTC));
+            DateTimeFormatterBuilder builder = new DateTimeFormatterBuilder()
+                    .append(ISODateTimeFormat.dateTime().withZone(DateTimeZone.UTC).getPrinter(),
+                            ISODateTimeFormat.dateOptionalTimeParser().withZone(DateTimeZone.UTC).getParser());
+            formatter = builder.toFormatter();
         } else if ("dateTime".equals(input) || "date_time".equals(input)) {
             formatter = ISODateTimeFormat.dateTime();
         } else if ("dateTimeNoMillis".equals(input) || "date_time_no_millis".equals(input)) {
@@ -125,14 +126,27 @@ public class Joda {
                 formatter = DateTimeFormat.forPattern(input);
             } else {
                 DateTimeParser[] parsers = new DateTimeParser[formats.length];
+                DateTimePrinter printer = null;
                 for (int i = 0; i &lt; formats.length; i++) {
-                    parsers[i] = DateTimeFormat.forPattern(formats[i]).withZone(DateTimeZone.UTC).getParser();
+                    DateTimeFormatter f = formatterForPattern(formats[i]);
+                    parsers[i] = f.getParser();
+                    if (i == 0) {
+                        printer = f.getPrinter();
+                    }
                 }
                 DateTimeFormatterBuilder builder = new DateTimeFormatterBuilder()
-                        .append(DateTimeFormat.forPattern(formats[0]).withZone(DateTimeZone.UTC).getPrinter(), parsers);
+                        .append(printer, parsers);
                 formatter = builder.toFormatter();
             }
         }
+        return formatter;
+    }
+
+    /**
+     * Parses a joda based pattern, including some named ones (similar to the built in Joda ISO ones).
+     */
+    public static FormatDateTimeFormatter forPattern(String input) {
+        DateTimeFormatter formatter = formatterForPattern(input);
         return new FormatDateTimeFormatter(input, formatter.withZone(DateTimeZone.UTC));
     }
```

I'm happy to submit a full patch to make it work, but need a hint on where this exception is actually triggered.
</description><key id="5982687">2132</key><summary>Can't define multiple predefined date formats using `||`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thomasst</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2012-08-02T00:58:58Z</created><updated>2017-01-24T05:04:09Z</updated><resolved>2013-06-13T13:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="philfreo" created="2012-09-05T23:44:41Z" id="8317912">+1
</comment><comment author="kimchy" created="2012-09-06T14:34:43Z" id="8334168">Yea, the `||` format is only supported for "explicit" formats, like `yyyy||yyyy-MM`, not for the "built in" patterns. we can try and support it for built in patters, but for now you can explicitly specify the formats.
</comment><comment author="thomasst" created="2012-09-06T17:11:49Z" id="8339896">@kimchy Any clues on what's missing in the patch?
</comment><comment author="ppearcy" created="2013-06-06T18:25:36Z" id="19064570">Yeah, it would be very convenient to mix actual formats and Joda descriptions. Reverse engineering what the Joda format strings should be is doable, but inconvenient. 

Thanks!
</comment><comment author="Hyunsuk-Baek" created="2017-01-24T05:04:09Z" id="274710770">gg</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ImmutableSettings#remove(String) does not follow the Builder pattern</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2131</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java#L394

Currently the remove(String) method returns the value of the key that is removed. Should return "this" instead.

I see no dependencies on the return value in the existing code. Change is too simple to merit a pull request.
</description><key id="5982134">2131</key><summary>ImmutableSettings#remove(String) does not follow the Builder pattern</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels /><created>2012-08-02T00:08:47Z</created><updated>2013-03-21T19:37:51Z</updated><resolved>2013-03-21T19:37:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-02T06:47:10Z" id="7448299">I thought that the return value here would be important. Also, as a side note, why do you need it?
</comment><comment author="brusic" created="2012-08-02T16:53:23Z" id="7461238">I should have noted that this issue is not important to myself. Merely wanted to point out where the builder pattern could be enforced. Please feel free to close and ignore!

My original use case was reading in the elasticsearch.yml file on the client side in order to utilize the custom analyzers. Remove would have allowed me to remove anything not pertinent to analysis (! index.analysis). In the end I used injectors+modules instead of created an embedded node, so I should no longer have issues with other values set in my elasticsearch.yml file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add 1ms sleep after update to prevent test from failing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2130</link><project id="" key="" /><description>This test fails half of the time on my laptop with _ttl still equal to 3600000 during get. This sleep ensures that ttl is reduced by at least 1ms. 
</description><key id="5958138">2130</key><summary>Add 1ms sleep after update to prevent test from failing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-08-01T01:44:29Z</created><updated>2014-07-16T21:55:01Z</updated><resolved>2012-08-02T12:24:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-01T13:17:08Z" id="7426969">I think that simply fixing the test to have `lessThanOrEqualTo(3600000L)` would be better?
</comment><comment author="imotov" created="2012-08-01T14:01:25Z" id="7428046">Sure, I just thought that one of the goals of the test was to make sure that the _ttl clock is still "ticking". 
</comment><comment author="kimchy" created="2012-08-02T06:40:29Z" id="7448213">@imotov might be, but then the test would probably need to be different, since `currentTimeInMillis` does not guarantee 1 millisecond resolution. 
</comment><comment author="imotov" created="2012-08-02T12:24:50Z" id="7453790">Makes sense. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#2116 Expose all ShingleFilter settings via ShingleTokenFilterFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2129</link><project id="" key="" /><description>Exposing all relevant settings for ShingleFilter via ShingleTokenFilterFactory &amp; added basic testcases for checking the exposed settings. (issue-id: #2116)
</description><key id="5951995">2129</key><summary>#2116 Expose all ShingleFilter settings via ShingleTokenFilterFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2012-07-31T19:43:05Z</created><updated>2014-07-16T21:55:02Z</updated><resolved>2012-08-01T13:22:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-01T13:22:03Z" id="7427101">Pushed to 0.19 and master branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose ShingleFilter configuration via Settings API #2116</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2128</link><project id="" key="" /><description>I added missing configurations to ShingleTokenFilterFactory and added a testcase to check the basic settings. I will go ahead and update documentation next. (issue ref: #2116 )
</description><key id="5947759">2128</key><summary>Expose ShingleFilter configuration via Settings API #2116</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2012-07-31T16:38:05Z</created><updated>2014-07-16T21:55:02Z</updated><resolved>2012-07-31T19:41:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>there is some problem about my elasticsearch cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2127</link><project id="" key="" /><description>Hi all:
We use elasticsearch for a long time, and got some problems by using.
# When we started the es server about 3 days+-,the master and salve throw many same exception as :

master: 10.0.8.38:19400
salve:
10.0.8.39:19400,10.0.8.39:19401,
10.0.8.40:19400,10.0.8.40:19401,
#10.0.8.41:19400,10.0.8.41:19401

the master exception is:

[2012-07-30 02:02:14,423][WARN ][transport ] [es-sas2-38] Received response for a request that has timed out, sent [46133ms] ago, timed out [16133ms] ago, action [discovery/zen/fd/ping], node [[es-sas2-39][sU1HvQM5Tpq-G_LX91WE0w][inet[/10.0.8.39:19401]]], id [599797878]

[2012-07-30 02:11:21,561][WARN ][transport ] [es-sas2-38] Received response for a request that has timed out, sent [71786ms] ago, timed out [24ms] ago, action [discovery/zen/fd/ping], node [[es-sas2-40][BJFnMhn0RySIH8edEe0rNw][inet[/10.0.8.40:19401]]], id [600806756]

some time about 10.0.8.41

the salve exception is all of this:

[2012-07-30 05:52:28,881][WARN ][transport ] [es-sas1-39] Received response for a request that has timed out, sent [76426ms] ago, timed out [46426ms] ago, action [discovery/zen/fd/masterPing], node [[es-sas2-38][2RiFKkrARjmP-H4H03W92w][inet[/10.0.8.38:19401]]], id [847491]

The salves cant&#8217;t find the master,and the master can&#8217;t find all salves !! omg&#8230;

But,when i restart the servers,it will all be find;but when 3 or 4 days later,the problem still exist !!

I use Http to read data, and use Tcp to write data(sure closed)&#12290;

When you seen,please help us,and reply my email,Thanks so much!!
</description><key id="5935146">2127</key><summary>there is some problem about my elasticsearch cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">elvisjian</reporter><labels /><created>2012-07-31T03:53:25Z</created><updated>2013-07-05T10:12:23Z</updated><resolved>2013-07-05T10:12:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T10:12:23Z" id="20510578">Hey,

I hope you solved your problem. Please use the mailinglist with problems like this the next time, as there will be many more people looking at your issue, so the possibility of getting help is rising a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue #2121 Added limit parameter for string type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2126</link><project id="" key="" /><description /><key id="5929833">2126</key><summary>Issue #2121 Added limit parameter for string type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-07-30T21:17:34Z</created><updated>2014-07-16T21:55:03Z</updated><resolved>2012-08-02T12:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Increase default recovery chunk size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2125</link><project id="" key="" /><description>Increase the default recovery chunk sizes (when transferring files and translog) from `100k` to `512k` (which gets further compressed).
</description><key id="5927305">2125</key><summary>Increase default recovery chunk size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-30T19:40:11Z</created><updated>2012-07-30T19:48:55Z</updated><resolved>2012-07-30T19:48:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Transport/Http: Remove explicit setting of send/receive buffer, and improve netty receive buffer predictor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2124</link><project id="" key="" /><description>First, don't explicitly set the send / receive buffers of the sockets, i.e. remove the default values for `network.tcp.send_buffer_size` and `network.tcp.receive_buffer_size` and let the OS control that unless explicitly set.

Second, netty uses a "user space" receive buffer, with adaptive size from 1k to 64k. Allow to control that using `transport.netty.receive_predictor_size` (for transport) and `http.netty.receive_predictor_size` (for http) (min / max can be set instead of size, though fixed size is preferable). Also, change that default value to `512k`, as `64k` is too small.
</description><key id="5927198">2124</key><summary>Transport/Http: Remove explicit setting of send/receive buffer, and improve netty receive buffer predictor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-30T19:36:05Z</created><updated>2012-07-30T19:37:52Z</updated><resolved>2012-07-30T19:37:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support for stem_exclusion in custom analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2123</link><project id="" key="" /><description>Currently, only the language analyzers support the stem_exclusion property, which means there is no way to create exclusions for custom analyzers.

The solution would be to have the SnowballFilter directly support stem_exclusion or expose the KeywordMarkerFilter so that it can be added manually to an analyzer.
</description><key id="5922653">2123</key><summary>Support for stem_exclusion in custom analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels /><created>2012-07-30T17:04:41Z</created><updated>2012-08-15T16:04:37Z</updated><resolved>2012-08-14T23:37:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2012-08-14T23:37:27Z" id="7744081">After trying to implement the feature myself, I noticed that ElasticSearch already exposes the StemmerOverrideFilter (not mentioned in the docs), which makes adding stem exclusions possible:

```
index :
    analysis :
        filter:
            protwods:
                type: stemmer_override
                rules_path : analysis/protwords.txt
```
</comment><comment author="s1monw" created="2012-08-15T10:16:13Z" id="7752585">hey brusic, can you add this to the documentation? This would be awesome!
</comment><comment author="brusic" created="2012-08-15T16:04:37Z" id="7760078">I fully intended to document these filters on the site. I first need to consult the Lucene list for the differences between KeywordMarkerFilter and StemmerOverrideFilter (I think I understand, just wanted to double-check). For those that think ElasticSearch's documentation is lacking has apparently never seen Lucene's.

For those curious: a full list of supported analyzers, tokenizers, and filters can be found in the AnalysisModule.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Add jvm buffer pools stats (when available, for java 7 and above)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2122</link><project id="" key="" /><description /><key id="5896631">2122</key><summary>Node Stats: Add jvm buffer pools stats (when available, for java 7 and above)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-28T22:42:54Z</created><updated>2012-07-28T22:49:27Z</updated><resolved>2012-07-28T22:49:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping: String type to allow for a "ignore_above" size parameter, above which it will ignore the content</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2121</link><project id="" key="" /><description>Can he handy for `not_analyzed` fields, in generic multi_mapping types, where it doesn't really make sense to have large text treated as not analyzed.
</description><key id="5883480">2121</key><summary>Mapping: String type to allow for a "ignore_above" size parameter, above which it will ignore the content</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-27T19:12:52Z</created><updated>2012-07-31T11:01:04Z</updated><resolved>2012-07-31T11:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-31T11:01:04Z" id="7395198">Implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Numeric/Date types to have a flag to ignore wrongly formatted values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2120</link><project id="" key="" /><description>Add `ignore_malformed` flag to all numeric / date types.
</description><key id="5883441">2120</key><summary>Mapping: Numeric/Date types to have a flag to ignore wrongly formatted values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-27T19:10:50Z</created><updated>2012-08-03T07:53:35Z</updated><resolved>2012-08-03T07:53:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-08-03T07:53:09Z" id="7477796">Pushed changes to 0.19 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.5.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2119</link><project id="" key="" /><description /><key id="5880731">2119</key><summary>Upgrade to Netty 3.5.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-27T17:01:01Z</created><updated>2012-07-27T17:01:33Z</updated><resolved>2012-07-27T17:01:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add Explicit Multi &amp; PhraseQuery support to REST &amp; Java API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2118</link><project id="" key="" /><description>currently I don't see explicit support for MultiPhraseQuery in ElasticSearch. It would be interesting to be able to combine PhraseQuery with fuzzy terms especially with Lucene 4.0 coming up.
</description><key id="5827359">2118</key><summary>Add Explicit Multi &amp; PhraseQuery support to REST &amp; Java API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>discuss</label><label>enhancement</label></labels><created>2012-07-25T10:39:30Z</created><updated>2015-09-22T09:23:01Z</updated><resolved>2015-09-22T09:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T14:07:14Z" id="135033163">@s1monw Do you mean we should have a dedicated "phrase" query in addition to the match phrase query?
</comment><comment author="clintongormley" created="2015-09-19T17:11:56Z" id="141689497">@s1monw do you still want this?
</comment><comment author="clintongormley" created="2015-09-22T09:23:01Z" id="142224352">Chatted to @s1monw who said no, so closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>split brain condition after second network disconnect - even with minimum_master_nodes set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2117</link><project id="" key="" /><description>## Summary:

Split brain can occur on the second network disconnect of a node, when the minimum_master_nodes is configured correctly(n/2+1).  The split brain occurs if the nodeId(UUID) of the disconnected node is such that the disconnected node picks itself as the next logical master while pinging the other nodes(NodeFaultDetection).  The split brain only occurs on the second time that the node is disconnected/isolated.
## Detail:

Using ZenDiscovery, Node Id's are randomly generated(A UUID):  ZenDiscovery:169.

When the node is disconnected/isolated it the ElectMasterService uses an ordered list of the Nodes (Ordered by nodeId) to determine a new potential master.  It picks the first of the ordered list: ElectMasterService:95

Because the nodeId's are random, it's possible for the disconnected/isolated node to be first in the ordered list, electing itself as a possible master.

The first time network is disconnected, the minimum_master_nodes property is honored and the disconnected/isolated node goes into a "ping" mode, where it simply tries to ping for other nodes.  Once the network is re-connected, the node re-joins the cluster successfully.

The Second time the network is disconnected, the minimum_master_nodes intent is not honored.  The disconnected/isolated node fails to realise that it's not connected to the remaining node in the 3 node cluster and elects itself as master, still thinking it's connected.

It feels like there is a failure in the transition between MasterFaultDetection and NodeFaultDetection, because it works the first time!

The fault only occurs if the nodeId is ordered such that the disconnected node picks itself as the master while isolated.  If the nodeId's are ordered such that it picks one of the other 2 nodes to be potential master then the isolated node honors the minimum_master_nodes intent every time.

Because the nodeId's are randomly(UUID) generated, the probability of this occuring drops as the number of nodes in the cluster goes up.  For our 3 node cluster it's ~50% (with one node detected as gone, it's up to the ordering of the remaining two nodeId's)

Note, While we were trying track this down we found that the cluster.service TRACE level logging (which outputs the cluster state) does not list the nodes in election order.  IE, the first node in that printed list is not necessarily going to elected as master by the isolated node.
## Detail Steps to reproduce:

Because the ordering of the nodeId's is random(UUID) we were having trouble getting a consitantly reproducable test case.  To fix the ordering, we made a patch to ZenDiscovery to allow us to optionally configure a nodeId.  This allowed us to set the nodeId of the disconnected/isolated node to guarantee it's ordering, allowing us to consistently reproduce.

We've tested this scenario on the 0.19.4, 0.19.7, 0.19.8 distributions and see the error when the nodeId's were ordered just right.

We also tested this scenario on the current git master with the supplied patch.

In this scenario, node3 will the be the node we disconnect/isolate.  So we start the nodes up in numerical order to ensure node3 doesn't _start_ as master.
1. Configure nodes with attached configs (one is provided for each node)
2. Start up nodes 1 and 2.  After they are attached and one is master, start node 3
3. Create a blank index with default shard/replica(5/1) settings
4. Pull network cable from node 3
5. Node 3 detects master has gone (MasterFaultDetection)
6. Node 3 elects itself as master (Because the nodeId's are ordered just right)
7. Node 3 detects the remaining node has gone, enters ZenDiscovery minimum_master_nodes mode, prints a message indicating not enough nodes
8. Node 3 goes into a ping state looking for nodes
9. At this point, node 1 and node 2 report a valid cluster, they know about each other but not about node 3.
10. Reconnect network to node 3
11. Node 3 rejoins the cluster correctly, seeing that there is already a master in the cluster.

At this point, everything is working as expected.
1.  Pull network cable from node 3 again
2.  Node 3 detects master has gone (MasterFaultDetection)
3.  Node 3 elects as itself as master (Because the nodeId's are ordered just right)
4.  Node 3 now fails to detect that the remaining node in the cluster is not accessible.  It starts throwing a number of Netty NoRouteToHostExceptions about the remaining node.
5.  According to node 3, cluster health is yellow and cluster state shows 2 data nodes
6.  Reconnect network to node 3
7.  Node 3 appears to connect to the node that it thinks it's still connected to.  (can see that via the cluster state api).  The other nodes log nothing and do not show the disconnected node as connected in any way.
8.  Node 3 at this point accepts indexing and search requests, a classic split brain.

Here's a gist with the patch to ZenDiscovery and the 3 node configs.

https://gist.github.com/3174651
</description><key id="5824122">2117</key><summary>split brain condition after second network disconnect - even with minimum_master_nodes set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">owenbutler</reporter><labels><label>bug</label></labels><created>2012-07-25T06:52:36Z</created><updated>2014-06-16T02:09:38Z</updated><resolved>2014-06-16T02:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-30T20:22:45Z" id="7380813">Thanks for the detailed explanation, do you have the logs for the 3 nodes around, would love to have a look at them.
</comment><comment author="owenbutler" created="2012-08-01T04:59:40Z" id="7418413">Hi Shay,

Logs for the three nodes here:

https://gist.github.com/3223822

The disconnected/isolated node is the top file, log named "splitbrain-isolatednode.log".

Timestamps of note (the clocks of the 3 nodes are within a second of eachother):

14:42:49 -&gt; first network disconnect
14:44:30 -&gt; Reconnect
14:45:13 -&gt; Second network disconnect
14:46:11 -&gt; Split brain begins (the isolated node still thinks it's connected to one of the others at this point)
14:47:41 -&gt; After second reconnect the isolated node now just sees one node.

There's a few index status request errors logged because we used elasticsearch-head to check status on the isolated node.
</comment><comment author="praveenbm5" created="2012-08-28T23:34:45Z" id="8110914">Looks like we are facing a similar issue...

[2012-08-28 06:54:20,729][INFO ][discovery.zen            ] [TES3] master_left [[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2012-08-28 06:54:20,741][INFO ][cluster.service          ] [TES3] master {new [TES3][DIhQQmanQOOgo1qVFz8tSA][inet[/178.238.237.240:9300]], previous [TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]}, removed {[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]],}, reason: zen-disco-master_failed ([TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]])
[2012-08-28 06:54:50,707][INFO ][cluster.service          ] [TES3] added {[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]],}, reason: zen-disco-receive(join from node[[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]])
[2012-08-28 07:06:03,238][INFO ][cluster.service          ] [TES3] removed {[TES2][kT7r8dFxSt6bjVKUAvdcdg][inet[/178.238.237.239:9300]],}, reason: zen-disco-node_failed([TES2][kT7r8dFxSt6bjVKUAvdcdg][inet[/178.238.237.239:9300]]), reason failed to ping, tried [3] times, each with maximum [30s] timeout
[2012-08-28 07:06:03,278][WARN ][discovery.zen            ] [TES3] not enough master nodes, current nodes: {[TES3][DIhQQmanQOOgo1qVFz8tSA][inet[/178.238.237.240:9300]],}
[2012-08-28 07:06:03,279][INFO ][cluster.service          ] [TES3] removed {[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]],}, reason: zen-disco-node_failed([TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]), reason failed to ping, tried [3] times, each with maximum [30s] timeout
[2012-08-28 07:06:33,290][INFO ][cluster.service          ] [TES3] detected_master [TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]], added {[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]],[TES2][kT7r8dFxSt6bjVKUAvdcdg][inet[/178.238.237.239:9300]],}, reason: zen-disco-receive(from master [[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]])
[2012-08-28 08:36:45,100][INFO ][discovery.zen            ] [TES3] master_left [[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2012-08-28 08:36:45,112][INFO ][cluster.service          ] [TES3] master {new [TES3][DIhQQmanQOOgo1qVFz8tSA][inet[/178.238.237.240:9300]], previous [TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]]}, removed {[TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]],}, reason: zen-disco-master_failed ([TES1][-G0NH7iwRQevY3La-zlxaA][inet[/178.238.237.241:9300]])
</comment><comment author="tallpsmith" created="2012-12-18T20:39:04Z" id="11504042">@kimchy et al have you had a chance to try to reproduce this with the steps outlined above?  I see a fair number of Split Brain style issues appearing on the mailing lists, and likely a smaller cluster size for them is exacerbating this issue?
</comment><comment author="tallpsmith" created="2013-03-21T03:37:53Z" id="15217851">@kimchy  @s1monw  anyone able to comment on whether they've even tried the steps above ?  I still think people are vulnerable to this condition.

It is a pain to setup the test I know, but trust me, once you see it happen, it's bad.  
</comment><comment author="brusic" created="2013-03-21T18:26:42Z" id="15256812">I have been watching this thread for a while since I have encountered the issue as well (running 0.20RC1). I tend to avoid adding a +1 comment on GitHub, but I wanted to lend my support to this issue.

Another related problem this week: https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/erpa7mMT5DM

My hopes is that the new field cache changes will alleviate memory pressure, causing fewer garbage collection (which might have been our issue as well).
</comment><comment author="s1monw" created="2013-03-21T20:42:52Z" id="15264107">@tallpsmith thanks for pinging me on this one again. I started looking into this today but it might take until next week to get some results / comments on this. I haven't had this issue on my radar so it's great that you pinged again!
</comment><comment author="synhershko" created="2013-04-04T16:08:20Z" id="15906808">radar blip
</comment><comment author="s1monw" created="2013-04-08T21:02:53Z" id="16078340">hey folks,
sorry to come back to this so late. I have worked out a setup where I can reproduce this issue. Unfortunately, this situation can in-fact occur with zen discovery at this point. We are working on a fix for this issue which might take a bit until we have something that can bring a solid solution for this. 
</comment><comment author="jprante" created="2013-05-08T18:25:55Z" id="17624471">Maybe it should be possible to add an alternative (selectable) Leader Election algorithm? For example, Chang-Roberts or Hirschfeld-Sinclair?
</comment><comment author="brusic" created="2013-05-23T17:32:46Z" id="18358983">Our live cluster experienced the split-brain scenario once again after a node was unresponsive during garbage collection. Plenty of logs if wanted.
</comment><comment author="synhershko" created="2013-05-23T17:51:23Z" id="18360110">Ivan, how do you connect it to the cluster again - simple restart to that node?
</comment><comment author="brusic" created="2013-05-23T18:14:56Z" id="18361562">Yes. I didn't have time to identify the problematic node, so I restarted the node I thought was having issues (I choose correctly, I guess I should have played PowerBall after all). One shard (on a different node) was stuck in a RECOVERING state, which I fixed by reducing the number of replicas from 2 to 1 and then increasing the number again.

Simon mentioned that issue is with zen discovery, so I am assuming switching from Multicast to Unicast will not alleviate the problem.
</comment><comment author="brusic" created="2013-07-11T01:25:56Z" id="20784789">Have I mentioned how serious this problem is? Our production cluster has pretty much gone away. It is like a game of whackomole trying to kill instances that think they are the master.
</comment><comment author="fasher" created="2013-07-11T07:37:38Z" id="20795284">I agree, this issue is critical please give this priority.
We gone to a single master node in our production to avoid split brains we had in the past that corrupted our index.
</comment><comment author="bitsofinfo" created="2014-01-02T15:35:30Z" id="31459821">Any update on a timeline for a fix for this?
</comment><comment author="XANi" created="2014-05-28T12:03:46Z" id="44397441">Any progress on that?
</comment><comment author="kimchy" created="2014-06-16T02:09:38Z" id="46135764">this is the same issue as #2488, so closing this in favor of the other one. Please comment if you think its different.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ShingleTokenFilterFactory doesn't expose all relevant settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2116</link><project id="" key="" /><description>Currently you are only able to set "max_shingle_size" &amp; "output_unigrams". For efficient use of this filter we should expose "min_shingle_size", "output_unigrams_if_no_shingles" &amp; "token_separator" as well.
</description><key id="5824084">2116</key><summary>ShingleTokenFilterFactory doesn't expose all relevant settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-25T06:49:22Z</created><updated>2012-08-01T13:34:24Z</updated><resolved>2012-08-01T13:34:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2012-07-31T20:00:36Z" id="7409727">added documentation update PR see https://github.com/elasticsearch/elasticsearch.github.com/pull/217
</comment><comment author="s1monw" created="2012-08-01T13:34:24Z" id="7427394">superseded by #2129
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle facet terms int overflow better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2115</link><project id="" key="" /><description>Facet terms are int based and can overflow with a large number of documents.
Instead of overflowing return Integer.MAX_VALUE.
This fix assumes overflow only happens during the reduce phase.
</description><key id="5816507">2115</key><summary>Handle facet terms int overflow better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2012-07-24T20:55:55Z</created><updated>2014-06-20T12:16:52Z</updated><resolved>2013-03-11T19:47:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow setting ttl at index level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2114</link><project id="" key="" /><description>Allow setting ttl to index, so the whole index gets deleted when expired
</description><key id="5808360">2114</key><summary>Allow setting ttl at index level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">tsouza</reporter><labels><label>discuss</label></labels><created>2012-07-24T17:28:53Z</created><updated>2015-12-29T19:01:39Z</updated><resolved>2014-07-25T07:49:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timbunce" created="2014-02-26T10:59:35Z" id="36113620">+1 for this, or something like it.
</comment><comment author="philfreo" created="2014-03-01T00:59:08Z" id="36410409">+1 this would especially be useful for Marvel
</comment><comment author="svenmeys" created="2014-03-14T13:55:05Z" id="37648927">+1 Definately a keeper. Also for marvel
</comment><comment author="michelem09" created="2014-03-19T08:08:05Z" id="38026074">+1 especially for Marvel
</comment><comment author="kimchy" created="2014-03-19T08:20:24Z" id="38026706">my thoughts are that we will allow to register custom operations to be executed against an index, after a specific time has passed. Deleting an index is one thing, another is optimizing it, changing its allocation attributed, unloading bloom filters, ... .

Side note, have you looked at curator? Its an external tool that can execute those actions.
</comment><comment author="glongman" created="2014-07-08T14:42:49Z" id="48345736">+1 to "register custom actions"
</comment><comment author="clintongormley" created="2014-07-25T07:49:36Z" id="50118453">Given that the Curator tool now exists (https://github.com/elasticsearch/curator) and is easy to use, closing this issue.
</comment><comment author="gdeconto" created="2015-03-02T21:00:33Z" id="76821405">+1 especially for marvel
</comment><comment author="yehosef" created="2015-03-05T12:15:52Z" id="77354172">Since it seems most of the people needing this are mentioning marvel - I'll add a comment.  If I'm using a product that is monitoring the health of my search servers, especially one that you pay for for production, it shouldn't require me to install and configure another tool to keep it from crashing the server.  The default config of marvel will eventually consume all the disk space. Marvel is making the log files, I should configure the clean up through there.  If you want to install and use curator with marvel, that's fine.  But we should have an interface to manage how long we want to keep archives or settings to automatically delete when the remaining space gets to (...)
</comment><comment author="chrisinmtown" created="2015-07-06T19:12:06Z" id="118966663">Suggest reopening this issue.  I agree strongly with @yehosef bcos it's surprising to find Marvel steadily filling the disk.  Suggest default setting should be expiration &amp; auto deletion after 7 days (not infinity). 
</comment><comment author="glongman" created="2015-07-06T19:59:42Z" id="118979071">+1
</comment><comment author="blackside" created="2015-12-01T12:55:39Z" id="160961580">+1
</comment><comment author="photofinale" created="2015-12-17T01:17:16Z" id="165306845">+1
</comment><comment author="casieowen" created="2015-12-29T19:01:39Z" id="167853786">+1.  Please!  Yes, curator can do it, but it's way more efficient to set a ttl on the index at time of creation!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support files with no extension in config mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2113</link><project id="" key="" /><description>When config mappings directory contains a file without an extension, index creation fails with 

java.lang.StringIndexOutOfBoundsException: String index out of range: -1

To reproduce:
- create file test (with no extension) in directory config/mappings/_default
- create a new index
- index creation fails

Since an extension doesn't play any role in the mapping parsing, it might make sense to support files with no extensions or provide a better error message.

Found in https://groups.google.com/d/topic/elasticsearch/tcfhIpxaRKQ/discussion
</description><key id="5808193">2113</key><summary>Support files with no extension in config mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-07-24T17:21:35Z</created><updated>2014-07-16T21:55:04Z</updated><resolved>2012-07-30T20:31:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-30T20:31:08Z" id="7381070">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk mappings import</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2112</link><project id="" key="" /><description>At the moment, I can `GET /index/_mapping` and get a list of all type mappings for a given index.

It would be nice if I could `PUT /index/_mapping` with the same format to bulk update mappings.

I'm aware I can also specify a "mappings" key at index creation time, but hosted services such as [Bonsai](//bonsai.io) don't allow me to create and delete indices, so having this "symmetric" API call would be very handy.
</description><key id="5800308">2112</key><summary>Bulk mappings import</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickstenning</reporter><labels><label>adoptme</label></labels><created>2012-07-24T11:26:29Z</created><updated>2015-09-19T17:11:11Z</updated><resolved>2015-09-19T17:11:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:34:53Z" id="48355434">Actually, this should be available not just for mappings but also for settings, warmers and aliases, as a counterpart to: #4069 
</comment><comment author="clintongormley" created="2015-09-19T17:11:11Z" id="141689205">Given that this issue has had no further comments in the last 4 years, I'm going to close it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add regular expression query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2111</link><project id="" key="" /><description>Add support for Lucene's RegexQuery to ElasticSearch.

RegexQuery is really slow on Lucene 3.x, we are planning to use this query only with Percolate API
</description><key id="5775582">2111</key><summary>Add regular expression query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">maxcom</reporter><labels /><created>2012-07-23T10:02:54Z</created><updated>2014-07-07T17:17:56Z</updated><resolved>2013-05-27T07:18:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="maxcom" created="2012-07-23T10:04:33Z" id="7174465">Sample query:

```
curl -XPOST http://localhost:9200/twitter/tweet/_search?pretty=true -d '{ "query" : { "regex" : { "user" : "[a-z]imchy" }}}'
```
</comment><comment author="maxcom" created="2012-08-15T10:44:18Z" id="7752977">Added two commits: change 'wildcard' parameter name to 'regex' and integration test
</comment><comment author="maxcom" created="2012-08-15T11:22:09Z" id="7753510">Added website documentation: elasticsearch/elasticsearch.github.com#225
</comment><comment author="maxcom" created="2012-08-28T12:10:12Z" id="8089324">@s1monw do I need to do something more to get this patch merged?
</comment><comment author="eskatos" created="2012-10-02T09:17:08Z" id="9064121">What about including RegexFilterBuilder as the counterpart of RegexQueryBuilder?
</comment><comment author="spinscale" created="2013-05-27T07:16:19Z" id="18485493">@maxcom as elasticsearch now features a regexp query and a regexp filter, can you close this issue or is there something missing?

I am also in the process of updating the documentation as well.
</comment><comment author="maxcom" created="2013-05-27T07:18:13Z" id="18485552">thank you, regexp query works fine
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 3.6.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2110</link><project id="" key="" /><description /><key id="5768483">2110</key><summary>Upgrade to Lucene 3.6.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-22T20:28:20Z</created><updated>2012-07-22T20:28:45Z</updated><resolved>2012-07-22T20:28:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add regex filter on terms_stats facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2109</link><project id="" key="" /><description>Relative to #2063, here come the pull request...
BTW, documentation update proposal is here : https://github.com/elasticsearch/elasticsearch.github.com/pull/212
</description><key id="5747402">2109</key><summary>Add regex filter on terms_stats facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-07-20T22:04:05Z</created><updated>2014-06-13T13:26:55Z</updated><resolved>2013-09-14T13:02:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-08-01T21:33:10Z" id="7440730">Heya @elasticsearch team,
Do you think you will accept and merge this pull request?
If so, in which version could it be merged? 0.19 or 0.20 or ... ?

Thanks
</comment><comment author="dadoonet" created="2012-09-06T19:11:34Z" id="8343763">@kimchy Sorry to nag Shay, but do you think you can accept this pull request as part of the 0.20 ? Thanks
</comment><comment author="dadoonet" created="2013-09-14T07:37:15Z" id="24438745">@uboness I think I can close this one in favor of #3300, right?
Or should I rebase my PR on 0.90 to support regex filter sooner?
</comment><comment author="uboness" created="2013-09-14T13:02:05Z" id="24444887">@dadoonet yeah... closing this one. we only fix bugs in the facets... new features will be implemented as part of the aggregations
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Manipulatable pipelines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2108</link><project id="" key="" /><description>This concerns #664 . This pull request enables a plugin to implement any additional functionality on the transport handlers by itself by providing a different PipelineFactories.

A proof-of-concept plugin can be found here:
https://github.com/Asquera/elasticsearch-ssl-transport-plugin

The plugin is much lighter on the side of configurability, where #2105 does a much better job.

As a side-effect of the change, I also had to un-minify netty, because it was a huge pain to compile the plugin properly with SslHandler etc. living in a different (non-relocated) package.

Implementing SSL as a plugin would allow the plugin to be iterated independently of elasticsearch master, which, after thinking alot about what you can do after enabling SSL (different authentication strategies, etc).
</description><key id="5739300">2108</key><summary>Manipulatable pipelines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2012-07-20T15:08:44Z</created><updated>2014-06-14T18:09:29Z</updated><resolved>2012-07-22T19:00:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-21T21:25:39Z" id="7156814">See the comment I made on #2105. I don't really see a reason why extensions points should be written as netty pipeline handlers. SSL is a feature that should be in ES, not as a plugin, and on top of that, being able to "plug" custom logic should go on the transport layer, not on the netty layer, so it will be cross transport implementation.
</comment><comment author="skade" created="2012-07-22T19:00:05Z" id="7164436">This sounds reasonable, as introducing your own handler is really an odd edge case. Thank you for clarifying.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hyphen search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2107</link><project id="" key="" /><description>question posted in google group instead
</description><key id="5737691">2107</key><summary>hyphen search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cxodje</reporter><labels /><created>2012-07-20T13:59:49Z</created><updated>2012-07-20T14:12:46Z</updated><resolved>2012-07-20T14:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-07-20T14:00:44Z" id="7130773">Please don't post questions as issues.  The mailing list is the place for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problem Parents &amp; Children</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2106</link><project id="" key="" /><description>I had one small problem when doing a search using TopChildrenQuery and realized that when I have more than 51 children to the same document, my search has not returned any results. Digging deeper I found comparison what might be wrong in class QueryPhase.

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; if (topDocs.totalHits &lt;= numDocs) {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; break;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }

I changed this comparison for

&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; if (topDocsPhase instanceof TopChildrenQuery) {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; if (topDocsPhase.numHits () &lt;= numDocs) {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; break;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; else {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; if (topDocs.totalHits &lt;= numDocs) {
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; break;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; }

But i don't know if is the best solution.

I posted a code to simulate this problem in gist.

https://gist.github.com/3140681
</description><key id="5715944">2106</key><summary>Problem Parents &amp; Children</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gustavobmaia</reporter><labels /><created>2012-07-19T16:05:37Z</created><updated>2012-08-08T01:54:58Z</updated><resolved>2012-08-08T01:54:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add SSL support to Netty transport layer for Client/Node-to-Node communication</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2105</link><project id="" key="" /><description>This pull request modifies the Netty transport layer in order to support SSL for client/node to node communication within an elasticsearch cluster.
## Configuration

SSL encryption is activated with a new configuration entry _transport.tcp.compress_. If set to true, the keystore and trustore of the node must also be configured:

&lt;pre&gt;
# Enable SSL/TLS encryption for all communication between nodes:
#
# transport.tcp.ssl: true

# Settings for SSL/TLS encryption
#
# transport.tcp.ssl.keystore: /path/to/the/keystore
# transport.tcp.ssl.keystore_password: password
# transport.tcp.ssl.keystore_algorithm: SunX509
#
# transport.tcp.ssl.truststore: /path/to/the/truststore
# transport.tcp.ssl.truststore_password: password
# transport.tcp.ssl.truststore_algorithm: PKIX
&lt;/pre&gt;


If keystores settings are not set in configuration file, it looks for JVM options (-Djavax.net.ssl.keyStore=..., -Djavax.net.ssl.trustStore=...)

Before starting the cluster, make sure that all nodes recognize each other (by importing the certificates in other nodes' trustores)

A _generate.sh_ script can be used to create keystores for testing:

&lt;pre&gt;$./generate.sh 3     // will create 3 keystores and 3 certificates&lt;/pre&gt;

## Transport Client

SSL can be configured for a TransportClient with the Java API:

&lt;pre&gt;
TransportClient client = new TransportClient(settingsBuilder()
                .put("client.transport.nodes_sampler_interval", "30s")
                .put("transport.tcp.ssl", true)
                .put("transport.tcp.ssl.keystore", "/opt/certificates/esnode1.jks")
                .put("transport.tcp.ssl.keystore_password", "mypassword")
                .put("transport.tcp.ssl.truststore", "/opt/certificates/esnode1.jks")
                .put("transport.tcp.ssl.truststore_password", "mypassword")
                .put("client.transport.sniff", false).build());
        
        client.addTransportAddress(new InetSocketTransportAddress("localhost", 9300));
&lt;/pre&gt;

## Use cases
1.  If a node tries to join a cluster but is not identified (certificate is unknown): discovery/recovery will fail, the node will start but won't join the cluster. The error message in log file is "org.elasticsearch.transport.SSLTransportException: SSL / TLS handshake failed, closing the channel" on cluster side.
2.  If a non-SSL node tries to join a cluster configured for SSL: discovery/recovery will fail, the node will start but won't join the cluster. The error message in log file is "StreamCorruptedException: invalid data length".
3.  If a transport client tries to connect to a cluster but is not identified: a NoNodeAvailableException is thrown.
## Perf

One should pay attention to performance :/

NettyEchoBenchmark:

&lt;pre&gt;
Warming up...
Warmed up
Ran 50000, TPS 9069.47215672048
Ran 50000, TPS 8939.746111210441
Ran 50000, TPS 10322.047894302228
Ran 50000, TPS 9238.728750923872
Ran 50000, TPS 10181.225819588679
Ran 50000, TPS 13319.126265316996
Ran 50000, TPS 9823.18271119843
Ran 50000, TPS 9529.25481227368
Ran 50000, TPS 9072.763563781527
Ran [500000] iterations, payload [100]: took [51], TPS: 9803.921568627451
&lt;/pre&gt;


NettyEchoSSLBenchmark:

&lt;pre&gt;
Warming up...
Handshake completed on server side
Handshake completed on client side
Warmed up
Ran 50000, TPS 5730.002292000916
Ran 50000, TPS 6375.924509053813
Ran 50000, TPS 5571.651437486071
Ran 50000, TPS 6209.6373571783415
Ran 50000, TPS 5787.0370370370365
Ran 50000, TPS 5855.486590935707
Ran 50000, TPS 5726.720879624328
Ran 50000, TPS 5262.050094716901
Ran 50000, TPS 5755.726948313572
Ran [500000] iterations, payload [100]: took [87], TPS: 5747.126436781609
&lt;/pre&gt;


Code review and comments are welcome! :)
</description><key id="5698140">2105</key><summary>Add SSL support to Netty transport layer for Client/Node-to-Node communication</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels /><created>2012-07-18T19:56:27Z</created><updated>2014-07-09T10:51:34Z</updated><resolved>2012-07-23T16:44:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-07-19T22:32:05Z" id="7117298">Very nice. I love the idea.
</comment><comment author="kimchy" created="2012-07-21T21:23:31Z" id="7156801">Thanks for providing this pull request. The story around SSL on the transport layer is a bit more complicated though. If we add this, we need to think about allowing for cases where node to node communication does not use SSL, while clients do. 

The overhead of node to node communication over SSL is _huge_, I don't really think people understand that ... . The value security wise is close to 0, to be honest, if you have proper firewall and other security measures in place.
</comment><comment author="tlrx" created="2012-07-23T16:44:18Z" id="7183462">Thanks for the feedback. I understand the complexity that represents node to node communication over SSL. This pull request was a quick and easy way to get SSL within a cluster, as requested by many users.
</comment><comment author="thejohnfreeman" created="2013-06-25T21:07:51Z" id="20007464">This seemed like the most promising proposal, but it was rejected. What does it take to get SSL into ES? Is it enough to have separate options for requiring SSL for connections initiated inside the node versus outside the node? What are your requirements, @kimchy ?
</comment><comment author="mbarrien" created="2013-07-12T06:32:50Z" id="20860674">Quick note: Even though security value may be 0, having node-to-node communication be encrypted may still be required. Our specific case is that to store HIPAA information in Amazon and still be considered in compliance for auditing purposes, _all_ patient data must be encrypted when sent over the wire. It's not strictly required by pure HIPAA regulations, but a HIPAA related contract (BAA) signed with Amazon does require it. Because this pull request was rejected, Elasticsearch fails to meet this requirement and thus can't be used.

HIPAA won't be the last place we'll see this requirement (I suspect a lot of government related Amazon work will be similarly covered). And since it's a bunch of legalese, no amount of pointing out how little actual security is gained or how firewalls and port isolation are enough will suffice to pass the lawyers and auditors scrutiny.

Please consider resurrecting this pull request!
</comment><comment author="robottaway" created="2013-08-03T17:55:11Z" id="22058665">@mbarrien in the same boat here. Firewall rules alone won't cut it. We've been audited by expensive HIPAA experts and I can tell you the actual price of _not_ having SSL transport layer, and it's definitely not 0. 

@kimchy is the pull request going to hurt anyone if they actually implement it? At this point I may just have to clone ES and roll my own solution even if it's ugly. BTW the overhead looks completely acceptable for our setup if the figures @tlrx provided are in the ballpark.
</comment><comment author="fatlotus" created="2013-08-04T16:15:47Z" id="22074219">@robottaway Yeah, I'd second that &#8212; seems silly to throw away @tlrx's efforts just because we think it was difficult to do.
</comment><comment author="danielberlinger" created="2014-01-07T14:54:00Z" id="31743364">@kimchy I'd like to add a vote for this feature (despite the age of the issue). It would be incredibly helpful from a HIPPA/HITECH compliance perspective.
</comment><comment author="MartinHatas" created="2014-03-10T11:00:37Z" id="37171157">+1000 for this feature
</comment><comment author="anandsomani" created="2014-03-26T04:54:34Z" id="38649902">+1000 for this one!
</comment><comment author="Mrc0113" created="2014-05-04T04:01:31Z" id="42123510">+100000....Also looking to have node to node encryption for HIPAA information in Amazon
</comment><comment author="robottaway" created="2014-05-07T16:00:41Z" id="42446271">We have a repo containing the SSL enabled version of the 1.1.0 code here:

https://gitlab.devero.com/public/projects

there is also a SSL and QOS enabled RabbitMQ river plugin. On my radar to
cut releases in our Nexus so they can simply be dowloaded but for now you
have to "mvn package" to get the built product.

We plan to keep SSL branches as we move up versions of Elasticsearch or
until SSL transport becomes part if the project proper.

-rob

On Saturday, May 3, 2014, Marc DiPasquale notifications@github.com wrote:

&gt; +100000....Also looking to have node to node encryption for HIPAA
&gt; information in Amazon
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/2105#issuecomment-42123510
&gt; .
</comment><comment author="RobbieHer" created="2014-05-16T17:37:37Z" id="43358626">@robottaway :+1:  

Also, could you please confirm if the v1.1.0_ssl_work branch contains functionality to SSL protect inter-node communications on the transport layer (port 9300)? If so, could you please share some configuration settings, java client API usage and expected performance metrics? 

Thanks!
</comment><comment author="robottaway" created="2014-05-17T17:11:02Z" id="43415545">@RobbieHer encryption on 9300 is what this branch of ours provides. If you want TLS/SSL on the 9200 you will likely setup NGINX, Apache or another proxy in front which will allow setting up encryption. Note if you are going to use the Java API in your app you will need to configure encryption for 9300 there also since you cannot have both secured and unsecured.
</comment><comment author="RobbieHer" created="2014-05-19T17:42:07Z" id="43534139">@robottaway Thanks! 

Any idea on when this change can/will make it into the official ElasticSearch branch?
</comment><comment author="robottaway" created="2014-05-19T19:19:45Z" id="43544691">You are very welcome! I would hope this could get into the official branch(es) some day! Past conversations being an indicator it doesn't seem likely though. It def is a bit of work to have to patch every branch we want to use, and it certainly keeps us from upgrading as quickly as we could.
</comment><comment author="skurfuerst" created="2014-07-09T10:51:34Z" id="48455449">:+1:  for this feature! Checking it out right now, it would be _awesome_ if this could be implemented in ES or an "official" module.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>multiple filters in facet_filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2104</link><project id="" key="" /><description>using multiple filters nested in an "and" filter for 'facet_filter' results in a SearchPhaseExecutionException. The error is: Parse Failure [No facet type found for [numeric_range]]

``` json
{
  "fields": [
    "*"
  ],
  "facets": {
    "facet_name": {
      "statistical": {
        "fields": [
          "twto"
        ],
        "facet_filter": {
          "and": [
            {
              "exists": {
                "field": "twto"
              }
            },
            {
              "numeric_range": {
                "ts": {
                  "lt": 1309471199999
                }
              }
            }
          ]
        }
      }
    }
  },
  "query": {
    "match_all": {}
  },
  "from": 0,
  "size": 0
}
```

It doesn't matter what type of facet or what type of filters i'm using.

using the same expression as 'filter' the query works fine.

``` json
{
  "fields": [
    "*"
  ],
  "filter": {
    "and": [
      {
        "exists": {
          "field": "twto"
        }
      },
      {
        "numeric_range": {
          "ts": {
            "lt": 1341093600001,
            "gte": 1309471199999
          }
        }
      }
    ]
  },
  "from": 0,
  "size": 0
}
```
</description><key id="5658396">2104</key><summary>multiple filters in facet_filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">quodt</reporter><labels /><created>2012-07-17T08:15:02Z</created><updated>2012-07-17T13:12:41Z</updated><resolved>2012-07-17T13:12:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="quodt" created="2012-07-17T13:12:41Z" id="7034635">my fault.

facet_filter must not be located inside the type but next to it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Initial implementation of ResourceWatcherService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2103</link><project id="" key="" /><description>This pull request creates a node-level resource watcher service that allows services to register files and directories to be watched for changes. The first user of this service is Script Service that uses this service to check for changes in script files.
</description><key id="5647564">2103</key><summary>Initial implementation of ResourceWatcherService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-07-16T19:22:44Z</created><updated>2017-03-06T19:08:56Z</updated><resolved>2013-11-04T15:22:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2012-07-24T23:22:29Z" id="7236534">Excellent feature! If this pull request can make it into master, I would be able to implement reloadable query side synonyms as defined in this issue: https://github.com/elasticsearch/elasticsearch/issues/1956
</comment><comment author="medcl" created="2012-10-14T07:24:45Z" id="9416957">+1
</comment><comment author="imotov" created="2012-11-21T14:59:12Z" id="10599946">Updated to resolve conflicts with the current master
</comment><comment author="mahdeto" created="2013-02-10T10:37:08Z" id="13347071">+1
</comment><comment author="imotov" created="2013-06-18T18:06:03Z" id="19629407">Updated to resolve conflicts with the current master
</comment><comment author="javanna" created="2013-11-04T15:22:14Z" id="27692850">This one got merged
</comment><comment author="koskapan" created="2017-03-06T14:17:19Z" id="284407533">how to use this feature on analysis/synonyms.txt?</comment><comment author="imotov" created="2017-03-06T19:08:56Z" id="284499065">@koskapan, please see the linked issue #1956</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support date math on range facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2102</link><project id="" key="" /><description>eg:

```
curl -XGET 'http://127.0.0.1:9200/_search?pretty=1&amp;search_type=count'  -d '
{
   "query" : {
      "match_all" : {}
   },
   "facets" : {
      "created" : {
         "range" : {
            "ranges" : [
               {
                  "to" : "now-7d"
               },
               {
                  "to" : "now-14d"
               }
            ],
            "field" : "created"
         }
      }
   }
}
'
```
</description><key id="5645251">2102</key><summary>Support date math on range facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-07-16T17:48:10Z</created><updated>2014-01-22T11:37:26Z</updated><resolved>2014-01-22T11:37:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kluikens" created="2012-07-21T07:09:51Z" id="7150709">Is this not solved by changeset f997315 for #1708?
</comment><comment author="clintongormley" created="2012-07-21T08:45:19Z" id="7151244">No.  I tried it - it throws an error
</comment><comment author="kluikens" created="2012-07-22T09:31:24Z" id="7159929">You're right. `RangeFacet` makes [assumptions](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/facet/range/RangeFacet.java#L48) about the range values being numeric. 
</comment><comment author="jpountz" created="2014-01-22T11:37:26Z" id="33014153">Closing: [date range aggregations support date math](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-daterange-aggregation.html) to define ranges
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NodeStats comment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2101</link><project id="" key="" /><description>Looks like a copy/paste error from NodeInfo.
</description><key id="5645142">2101</key><summary>Fix NodeStats comment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2012-07-16T17:42:52Z</created><updated>2014-07-16T21:55:07Z</updated><resolved>2012-07-21T21:05:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-21T21:05:07Z" id="7156701">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting breaks when using Fuzzy Like This Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2100</link><project id="" key="" /><description>Also posted this in the google group, but appears to be an issue according to people in the IRC Chat.

We are switching from using Suery String to Fuzzy Like This Query. Doing so breaks the highlighting. Here is the query, result and mapping: https://gist.github.com/3097781

We are using Elasticsearch 0.19.8
</description><key id="5573197">2100</key><summary>Highlighting breaks when using Fuzzy Like This Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kundo</reporter><labels /><created>2012-07-12T14:37:05Z</created><updated>2013-06-28T11:30:09Z</updated><resolved>2013-06-28T11:30:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Haensel" created="2013-01-29T14:34:08Z" id="12837023">Any news on this? I am having the same problem using ElasticSearch 0.20.1. (update: and 0.20.4)
</comment><comment author="spinscale" created="2013-05-22T15:18:17Z" id="18285628">Hey,

can you retry with 0.90 and tell if it works? The following snippet works for me, but maybe I misinterpreted your configuration gist

```
curl -X DELETE localhost:9200/foo
curl -X PUT localhost:9200/foo

curl -X PUT localhost:9200/foo/bar/_mapping -d '{
    "properties" : {
    "title": {
        "boost": 2.5,
        "type": "string",
        "term_vector": "with_positions_offsets"
    }

    }   
}'

curl -X PUT 'localhost:9200/foo/bar/1?refresh=true' -d '{ "title" : "this is not a very long text, but maybe it is long enough in order to get the highlighting up and running. i18n. this is not a very long text, but maybe it is long enough in order to get the highlighting up and running " }'

curl -XGET 'http://localhost:9200/foo/bar/_search?pretty=true' -d '{"highlight": {"pre_tags": ["&lt;span class=highlighted&gt;"], "fields": {"title": {"fragment_size": 40, "number_of_fragments": 1}}, "post_tags": ["&lt;/span&gt;"]}, "query": {"fuzzy_like_this": {"like_text": "i18n", "fields": ["title"]}}}}'
```
</comment><comment author="spinscale" created="2013-06-28T11:30:09Z" id="20183225">closing for now, but happy to reopen if more information is provided
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Parser caching does not take types into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2099</link><project id="" key="" /><description>The QueryParserSettings class that is [used as a key](https://github.com/elasticsearch/elasticsearch/blob/3bf55a0858c17ac48208210dd71a84a6fde35dbf/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java#L202) in the parser cache doesn't contain any information about types that the cached query was parsed on. As a result some type-specific cached queries might be reused with wrong types. 

The problem can be reproduced with _id:1234 queries:  https://gist.github.com/b36137fb228eb54db8ce
</description><key id="5554846">2099</key><summary>Query Parser caching does not take types into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-07-11T18:21:53Z</created><updated>2014-07-16T21:55:08Z</updated><resolved>2012-07-12T09:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-12T09:31:30Z" id="6930745">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OutOfMemory Error -&gt; corrupted translog entry in shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2098</link><project id="" key="" /><description>we did some load testing on our elasticsearchcluster (30 nodes, 100M documents, up to 50.000 req/s) and received OutOfMemory errors on some nodes:

```
[2012-07-11 09:11:00,588][WARN ][transport.netty          ] [b22] Exception caught on netty layer [[id: 0x65459c6f, /10.66.1.16:58763 =&gt; /10.66.1.22:9300]]
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:657)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:943)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1336)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:427)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:342)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.doExecute(TransportShardReplicationOperationAction.java:108)
        at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:135)
        at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:117)
        at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:67)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:219)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$OperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:201)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:315)
        at org.elasticsearch.transport.netty.MessageChannelHandler.process(MessageChannelHandler.java:216)
        at org.elasticsearch.transport.netty.MessageChannelHandler.callDecode(MessageChannelHandler.java:141)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:93)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:792)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:94)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:372)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:246)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:38)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
```

after the restart of the nodes their shards were corrupted:

```
[2012-07-11 10:50:58,827][WARN ][indices.cluster          ] [b22] [myindex][7] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [myindex][7] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:204)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [payload]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:325)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:585)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:449)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:437)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:311)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:624)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:199)
    ... 4 more
Caused by: org.elasticsearch.common.jackson.JsonParseException: Unexpected end-of-input in VALUE_STRING
 at [Source: [B@18cc3b50; line: 1, column: 12639]
    at org.elasticsearch.common.jackson.JsonParser._constructError(JsonParser.java:1433)
    at org.elasticsearch.common.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)
    at org.elasticsearch.common.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:454)
    at org.elasticsearch.common.jackson.impl.JsonParserMinimalBase._reportInvalidEOF(JsonParserMinimalBase.java:448)
    at org.elasticsearch.common.jackson.impl.JsonParserBase.loadMoreGuaranteed(JsonParserBase.java:426)
    at org.elasticsearch.common.jackson.impl.Utf8StreamParser._finishString2(Utf8StreamParser.java:1924)
    at org.elasticsearch.common.jackson.impl.Utf8StreamParser._finishString(Utf8StreamParser.java:1905)
    at org.elasticsearch.common.jackson.impl.Utf8StreamParser.getText(Utf8StreamParser.java:276)
    at org.elasticsearch.common.xcontent.json.JsonXContentParser.text(JsonXContentParser.java:83)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.textOrNull(AbstractXContentParser.java:106)
    at org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateField(StringFieldMapper.java:242)
    at org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateField(StringFieldMapper.java:43)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:312)
    ... 11 more
[2012-07-11 10:50:58,830][WARN ][cluster.action.shard     ] [b22] sending failed shard for [myindex][7], node[Rn4HB7QKSMO7SqzodOXkdA], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[myindex][7] failed to recover shard]; nested: MapperParsingException[Failed to parse [payload]]; nested: JsonParseException[Unexpected end-of-input in VALUE_STRING
 at [Source: [B@18cc3b50; line: 1, column: 12639]]; ]]
```

```
b12.ls.af: [2012-07-11 10:51:00,046][WARN ][cluster.action.shard     ] [b12] sending failed shard for [myindex][13], node[JhZZ8DH6QPyskmigFaCtlw], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[myindex][13] failed to recover shard]; nested: ElasticSearchParseException[Failed to derive xcontent from (offset=0, length=6560): [84, 97, 69, 77, 50, 43, 53, 66, 99, 69, 50, 75, 48, 107, 116, 97, 54, 55, 106, 98, 82, 70, 107, 56, 115, 85, 112, 49, 88, 109, 57, 43, 78, 108, 115, 101, 101, 120, 68, 102, 81, 117, 117, 116, 86, 75, 84, 54, 111, 75, 119, 115, 116, 89, 120, 90, 120, 79, 69, 87, 43, 67, 122, 50, 90, 69, 120, 90, 83, 53, 121, 66, 57, 102, 114, 50, 70, 57, 80, 52, 77, 71, 100, 105, 88, 97, 114, 72, 66, 77, 109, 116, 71, 55, 67, 48, 43, 47, 86, 55, 112, 109, 66, 86, 68, 118, 76, 67, 109, 102, 114, 113, 47, 103, 72, 115, 89, 117, 79, 118, 82, 88, 113, 84, 107, 73, 119, 84, 85, 48, 112, 117, 75, 80, 110, 80, 100, 111, 102, 99, 83, 84, 117, 50, 83, 119, 53, 81, 88, 112, 116, 119, 116, 57, 112, 83, 67, 106, 73, 55, 107, 54, 65, 113, 115, 122, 82, 86, 53, 87, 81, 67, 49, 54, 82, 121, 57, 99, 76, 100, 43, 67, 121, 101, 102, 72, 47, 43, 98, 51, 75, 67, 71, 100, 72, 110, 89, 55, 88, 43, 48, 84, 52, 118, 72, 109, 51, 121, 112, 112, 110, 75, 99, 104, 50, 120, 73, 120, 80, 88, 106, 43, 97, 69, 108, 90, 112, 75, 76, 77, 113, 52, 49, 85, 50, 69, 68, 117, 71, 97, 80, 75, 103, 90, 57, 72, 74, 116, 121, 74, 85, 57, 57, 105, 105, 112, 79, 114, 80, 52, 111, 85, 73, 90, 85, 79, 102, 66, 117, 113, 98, 98, 112, 107, 84, 85, 71, 51, 116, 73, 112, 86, 121, 72, 82, 118, 110, 85, 114, 72, 102, 47, 75, 115, 88, 122, 74, 102, 43, 122, 97, 99, 113, 56, 53, 110, 81, 86, 90, 100, 114, 68, 107, 118, 105, 118, 101, 49, 105, 50, 86, 47, 74, 122, 120, 81, 83, 72, 105, 75, 81, 114, 113, 112, 79, 105, 47, 76, 118, 116, 97, 67, 106, 89, 112, 105, 109, 51, 110, 89, 112, 52, 72, 74, 108, 90, 70, 115, 70, 52, 107, 97, 79, 54, 101, 65, 117, 57, 50, 57, 104, 43, 101, 108, 55, 77, 90, 122, 67, 105, 100, 67, 80, 72, 106, 87, 100, 68, 56, 105, 67, 84, 104, 122, 53, 74, 83, 117, 110, 121, 90, 86, 113, 99, 54, 116, 84, 65, 89, 75, 122, 106, 66, 120, 102, 78, 116, 115, 97, 111, 110, 81, 114, 85, 98, 86, 107, 117, 116, 77, 101, 101, 109, 118, 110, 51, 85, 43, 80, 48, 110, 107, 48, 102, 55, 121, 79, 43, 76, 78, 110, 113, 78, 81, 117, 48, 85, 75, 66, 72, 115, 71, 105, 82, 65, 65, 88, 88, 81, 117, 68, 114, 54, 110, 71, 118, 51, 100, 118, 76, 109, 113, 110, 73, 76, 98, 106, 52, 55, 72, 83, 56, 75, 83, 101, 50, 65, 76, 53, 89, 48, 71, 82, 111, 102, 56, 77, 54, 119, 122, 71, 85, 71, 69, 76, 97, 88, 80, 117, 75, 83, 77, 71, 76, 89, 97, 79, 120, 55, 114, 82, 98, 52, 54, 103, 115, 112, 90, 102, 47, 106, 115, 101, 66, 97, 109, 105, 75, 85, 80, 88, 111, 89, 52, 47, 56, 109, 118, 71, 84, 109, 115, 65, 53, 104, 80, 56, 99, 81, 90, 97, 101, 118, 88, 56, 108, 51, 85, 84, 88, 43, 82, 80, 103, 69, 53, 77, 43, 54, 65, 107, 72, 75, 101, 78, 43, 47, 50, 86, 84, 52, 106, 90, 77, 105, 71, 112, 113, 108, 86, 77, 57, 70, 85, 117, 72, 89, 70, 67, 113, 112, 97, 49, 109, 106, 114, 109, 79, 78, 121, 70, 55, 54, 82, 54, 54, 76, 111, 75, 122, 43, 110, 81, 55, 117, 82, 69, 99, 75, 70, 53, 55, 75, 65, 67, 118, 99, 109, 53, 84, 121, 121, 43, 118, 120, 117, 108, 72, 69, 78, 121, 108, 116, 52, 81, 117, 47, 80, 110, 119, 107, 100, 70, 54, 76, 77, 90, 90, 67, 98, 98, 50, 118, 113, 79, 68, 76, 66, 107, 66, 110, 71, 79, 113, 106, 98, 51, 112, 104, 78, 104, 66, 55, 43, 57, 74, 98, 103, 97, 108, 117, 73, 69, 86, 113, 51, 108, 85, 105, 68, 69, 48, 122, 112, 101, 90, 89, 85, 72, 103, 116, 81, 119, 110, 86, 73, 98, 100, 49, 43, 66, 50, 51, 121, 109, 100, 121, 105, 105, 73, 56, 72, 53, 54, 66, 113, 81, 54, 89, 71, 79, 49, 67, 51, 56, 99, 108, 121, 122, 115, 102, 66, 66, 104, 57, 54, 73, 48, 66, 82, 71, 105, 112, 70, 82, 67, 85, 76, 87, 65, 71, 51, 108, 98, 71, 100, 109, 111, 107, 49, 74, 47, 55, 118, 65, 86, 109, 117, 117, 50, 107, 122, 68, 103, 104, 121, 82, 84, 90, 80, 54, 47, 106, 99, 54, 121, 98, 48, 48, 88, 113, 66, 52, 98, 72, 53, 51, 74, 55, 55, 47, 111, 67, 107, 85, 67, 106, 98, 53, 90, 47, 43, 115, 76, 117, 118, 97, 107, 53, 97, 70, 52, 99, 110, 47, 78, 65, 89, 121, 89, 84, 109, 57, 51, 72, 90, 75, 84, 78, 100, 74, 117, 121, 47, 68, 80, 106, 75, 47, 122, 71, 57, 74, 67, 84, 112, 110, 72, 74, 102, 118, 55, 52, 65, 103, 69, 56, 101, 68, 105, 48, 51, 56, 83, 50, 85, 72, 70, 74, 111, 52, 98, 43, 80, 88, 115, 72, 105, 108, 109, 112, 56, 78, 102, 81, 118, 55, 108, 87, 68, 86, 118, 66, 71, 67, 77, 101, 57, 55, 101, 52, 82, 66, 80, 66, 116, 48, 77, 119, 103, 82, 112, 52, 71, 65, 56, 66, 54, 99, 86, 120, 88, 113, 120, 77, 74, 69, 82, 90, 112, 118, 51, 80, 85, 82, 99, 110, 108, 120, 72, 69, 88, 57, 80, 122, 115, 53, 86, 121, 49, 119, 90, 48, 114, 81, 85, 97, 83, 47, 55, 69, 81, 109, 121, 116, 108, 114, 117, 75, 83, 53, 90, 73, 71, 54, 114, 122, 88, 120, 50, 110, 49, 120, 50, 75, 111, 109, 103, 80, 56, 111, 78, 69, 51, 67, 119, 53, 109, 54, 106, 122, 70, 115, 66, 52, 87, 69, 49, 109, 66, 81, 54, 116, 120, 109, 77, 116, 47, 117, 120, 85, 107, 85, 50, 56, 57, 72, 67, 68, 114, 88, 68, 105, 76, 75, 82, 89, 99, 113, 111, 109, 111, 102, 98, 73, 52, 97, 85, 57, 82, 102, 116, 57, 83, 120, 75, 74, 55, 47, 56, 49, 51, 79, 81, 78, 43, 78, 53, 73, 113, 68, 104, 97, 113, 98, 56, 101, 114, 105, 69, 57, 97, 99, 102, 72, 81, 48, 53, 113, 78, 75, 49, 119, 120, 50, 84, 99, 54, 82, 48, 52, 67, 119, 103, 88, 104, 110, 104, 102, 68, 52, 98, 78, 49, 49, 104, 89, 43, 67, 112, 122, 118, 90, 117, 90, 90, 115, 121, 122, 101, 107, 75, 85, 43, 90, 69, 83, 106, 47, 106, 90, 86, 55, 71, 56, 54, 82, 114, 90, 105, 99, 55, 71, 103, 106, 70, 53, 118, 69, 108, 76, 69, 52, 87, 56, 84, 55, 104, 111, 67, 70, 87, 51, 77, 122, 54, 102, 69, 86, 117, 54, 79, 71, 107, 49, 102, 122, 70, 84, 54, 103, 84, 116, 102, 109, 111, 100, 73, 86, 97, 67, 71, 66, 118, 48, 47, 101, 120, 119, 117, 57, 43, 102, 50, 67, 107, 78, 65, 57, 47, 74, 121, 74, 55, 67, 89, 89, 68, 98, 110, 77, 102, 114, 116, 99, 68, 84, 83, 87, 71, 72, 102, 78, 85, 103, 113, 101, 106, 106, 86, 119, 68, 89, 119, 81, 82, 101, 106, 89, 97, 87, 99, 120, 79, 98, 72, 74, 51, 50, 48, 109, 112, 76, 113, 47, 66, 112, 122, 68, 117, 52, 99, 74, 84, 80, 51, 87, 75, 67, 90, 97, 97, 84, 108, 43, 79, 49, 109, 111, 89, 99, 49, 108, 118, 49, 122, 117, 55, 48, 118, 78, 122, 90, 74, 79, 90, 50, 80, 108, 72, 43, 106, 101, 99, 50, 85, 101, 103, 116, 50, 74, 66, 77, 49, 109, 77, 85, 114, 73, 50, 74, 56, 72, 89, 66, 104, 90, 49, 122, 66, 69, 118, 97, 84, 114, 88, 87, 86, 56, 81, 82, 102, 98, 77, 117, 56, 104, 47, 122, 69, 87, 103, 89, 90, 84, 89, 85, 119, 55, 56, 48, 105, 86, 112, 43, 49, 109, 114, 111, 86, 114, 112, 75, 104, 57, 66, 99, 104, 86, 67, 116, 43, 101, 84, 77, 53, 122, 100, 55, 104, 117, 110, 66, 79, 43, 75, 81, 116, 116, 70, 98, 112, 43, 80, 81, 120, 98, 67, 110, 53, 107, 66, 85, 43, 80, 85, 109, 69, 114, 89, 69, 103, 83, 54, 77, 83, 47, 82, 82, 82, 80, 90, 72, 116, 82, 51, 89, 112, 109, 104, 103, 89, 100, 114, 56, 116, 114, 106, 121, 109, 79, 110, 79, 54, 57, 56, 115, 118, 77, 114, 81, 80, 102, 99, 84, 48, 53, 77, 50, 89, 67, 99, 75, 51, 69, 72, 111, 99, 85, 76, 79, 75, 83, 100, 74, 110, 113, 73, 48, 103, 69, 74, 101, 47, 105, 48, 110, 108, 106, 54, 109, 71, 65, 51, 48, 72, 97, 70, 75, 50, 102, 106, 104, 119, 79, 114, 86, 85, 90, 71, 51, 73, 54, 116, 119, 117, 74, 120, 48, 98, 65, 119, 104, 114, 103, 68, 87, 114, 56, 68, 108, 76, 43, 49, 73, 118, 89, 75, 48, 47, 80, 107, 66, 109, 90, 90, 51, 49, 88, 84, 89, 108, 98, 81, 109, 55, 85, 110, 84, 83, 51, 54, 99, 104, 106, 69, 106, 119, 113, 76, 43, 81, 90, 118, 73, 98, 111, 66, 49, 103, 79, 48, 70, 104, 118, 50, 73, 80, 79, 76, 67, 49, 76, 116, 101, 72, 74, 43, 97, 110, 78, 72, 90, 79, 54, 79, 118, 90, 105, 122, 116, 115, 52, 98, 121, 87, 70, 119, 86, 53, 117, 79, 104, 57, 117, 65, 87, 103, 116, 48, 67, 114, 119, 108, 116, 54, 106, 78, 73, 122, 76, 76, 107, 67, 57, 76, 77, 73, 80, 56, 72, 86, 67, 87, 99, 71, 118, 83, 109, 113, 80, 102, 105, 102, 51, 68, 90, 112, 100, 66, 117, 49, 79, 66, 68, 120, 47, 76, 103, 120, 97, 99, 104, 74, 104, 50, 74, 57, 99, 69, 121, 68, 67, 56, 83, 52, 113, 48, 74, 90, 110, 53, 109, 104, 112, 87, 77, 103, 99, 117, 119, 67, 99, 103, 73, 97, 101, 101, 77, 49, 102, 113, 97, 68, 47, 69, 116, 112, 118, 68, 55, 68, 90, 57, 83, 103, 105, 53, 52, 82, 53, 105, 98, 55, 101, 99, 66, 82, 83, 100, 118, 74, 85, 121, 84, 67, 116, 121, 89, 77, 76, 80, 78, 84, 87, 81, 76, 106, 83, 49, 111, 68, 76, 52, 109, 103, 81, 53, 76, 107, 73, 118, 117, 83, 82, 109, 47, 65, 57, 78, 47, 89, 99, 109, 105, 73, 105, 102, 70, 84, 109, 117, 122, 103, 48, 70, 102, 85, 98, 79, 109, 87, 81, 84, 54, 76, 80, 87, 72, 121, 100, 104, 90, 99, 99, 122, 118, 82, 47, 121, 116, 116, 122, 56, 104, 112, 120, 117, 79, 118, 105, 110, 81, 119, 84, 89, 87, 68, 112, 116, 82, 76, 114, 110, 99, 89, 77, 82, 110, 109, 68, 47, 88, 68, 52, 47, 83, 111, 86, 116, 82, 43, 101, 109, 119, 107, 71, 74, 52, 83, 78, 121, 89, 54, 98, 116, 79, 100, 55, 69, 52, 56, 67, 70, 43, 121, 105, 103, 120, 114, 55, 73, 68, 113, 73, 86, 115, 48, 55, 115, 76, 102, 81, 83, 100, 74, 108, 112, 116, 101, 66, 101, 85, 112, 73, 83, 66, 69, 57, 89, 66, 102, 107, 56, 43, 57, 51, 113, 87, 55, 81, 53, 99, 89, 65, 97, 107, 75, 84, 67, 49, 113, 50, 53, 81, 89, 82, 71, 82, 105, 57, 65, 103, 51, 73, 71, 73, 51, 70, 67, 48, 86, 50, 98, 111, 104, 115, 79, 84, 89, 73, 72, 119, 74, 102, 67, 68, 74, 99, 65, 70, 81, 97, 100, 117, 48, 57, 104, 72, 85, 82, 114, 89, 82, 65, 72, 51, 98, 107, 82, 89, 84, 69, 77, 108, 66, 102, 74, 109, 68, 116, 118, 114, 85, 115, 82, 89, 74, 70, 111, 51, 56, 87, 80, 69, 110, 114, 57, 120, 77, 52, 110, 73, 75, 101, 82, 104, 101, 49, 71, 84, 89, 43, 84, 43, 100, 101, 99, 49, 119, 121, 90, 66, 50, 117, 57, 50, 118, 57, 72, 98, 47, 79, 117, 106, 114, 49, 105, 89, 85, 57, 78, 80, 88, 116, 57, 73, 107, 117, 56, 101, 50, 67, 51, 49, 66, 110, 90, 66, 56, 110, 80, 43, 67, 81, 118, 43, 73, 116, 78, 113, 72, 43, 101, 54, 80, 88, 47, 65, 47, 56, 109, 54, 102, 121, 54, 112, 51, 81, 113, 71, 73, 85, 69, 99, 47, 119, 114, 50, 83, 88, 90, 49, 89, 98, 111, 71, 49, 116, 72, 47, 98, 107, 121, 101, 79, 108, 47, 72, 111, 83, 75, 107, 68, 81, 120, 116, 103, 101, 53, 118, 89, 52, 79, 56, 85, 119, 73, 98, 67, 47, 102, 106, 89, 43, 53, 54, 65, 54, 47, 90, 80, 70, 116, 112, 106, 108, 120, 85, 113, 110, 70, 80, 108, 85, 67, 119, 114, 50, 99, 74, 55, 113, 117, 106, 75, 116, 50, 54, 114, 56, 48, 106, 71, 80, 75, 105, 72, 87, 120, 106, 68, 78, 73, 69, 115, 82, 122, 87, 105, 106, 55, 57, 81, 116, 109, 97, 113, 98, 52, 74, 83, 80, 81, 49, 68, 111, 112, 102, 106, 54, 53, 89, 82, 98, 77, 69, 89, 100, 100, 111, 100, 56, 68, 99, 97, 117, 51, 78, 81, 107, 100, 113, 52, 87, 122, 116, 86, 121, 109, 86, 121, 122, 69, 103, 115, 66, 79, 106, 70, 110, 69, 77, 51, 97, 122, 67, 98, 79, 99, 111, 86, 65, 53, 79, 68, 89, 65, 76, 51, 111, 99, 79, 67, 105, 69, 108, 116, 85, 54, 89, 113, 81, 110, 103, 57, 82, 102, 84, 73, 119, 53, 82, 70, 105, 79, 120, 82, 67, 81, 70, 85, 99, 75, 117, 65, 83, 53, 72, 57, 70, 114, 112, 104, 47, 82, 103, 88, 120, 121, 75, 43, 90, 87, 101, 109, 120, 89, 89, 117, 111, 43, 121, 119, 110, 82, 120, 113, 43, 66, 56, 77, 86, 49, 78, 79, 72, 50, 120, 43, 103, 104, 48, 116, 68, 104, 84, 53, 102, 85, 78, 89, 86, 110, 66, 85, 109, 110, 102, 120, 116, 108, 114, 56, 74, 68, 102, 55, 87, 75, 73, 85, 114, 112, 118, 77, 50, 114, 57, 99, 121, 43, 116, 101, 89, 90, 100, 103, 89, 55, 76, 115, 118, 82, 76, 66, 111, 57, 108, 66, 107, 102, 67, 122, 54, 75, 121, 84, 54, 70, 98, 122, 82, 78, 65, 82, 83, 51, 113, 90, 55, 76, 48, 49, 97, 52, 54, 81, 87, 65, 66, 111, 65, 56, 52, 105, 83, 89, 101, 51, 108, 78, 89, 66, 109, 107, 51, 115, 50, 52, 66, 47, 66, 114, 101, 102, 113, 88, 85, 80, 49, 47, 87, 86, 83, 83, 97, 52, 88, 76, 43, 50, 107, 74, 72, 103, 111, 66, 99, 65, 77, 80, 82, 80, 101, 86, 72, 67, 85, 76, 121, 111, 105, 116, 121, 90, 100, 50, 97, 101, 55, 52, 104, 53, 85, 72, 76, 87, 104, 86, 81, 103, 79, 106, 79, 55, 100, 102, 88, 53, 66, 75, 115, 88, 109, 43, 116, 112, 49, 88, 103, 56, 48, 118, 120, 65, 90, 84, 106, 112, 86, 85, 52, 112, 119, 109, 89, 85, 47, 82, 110, 66, 82, 113, 82, 43, 47, 90, 114, 55, 101, 111, 82, 117, 85, 100, 56, 53, 88, 118, 51, 117, 110, 51, 73, 67, 98, 65, 51, 109, 107, 97, 115, 81, 80, 108, 72, 72, 113, 110, 49, 98, 48, 88, 49, 73, 106, 53, 47, 79, 120, 122, 104, 109, 73, 97, 106, 78, 85, 102, 73, 87, 54, 66, 54, 121, 77, 50, 65, 78, 85, 75, 74, 110, 55, 86, 112, 116, 111, 113, 116, 69, 99, 69, 55, 120, 72, 81, 90, 54, 57, 47, 102, 97, 75, 71, 111, 82, 68, 110, 55, 107, 70, 66, 49, 47, 81, 80, 72, 80, 54, 115, 69, 102, 51, 74, 75, 102, 118, 56, 102, 120, 109, 84, 117, 76, 89, 118, 74, 115, 100, 111, 108, 79, 76, 83, 105, 52, 73, 49, 87, 76, 105, 67, 84, 111, 102, 73, 108, 109, 48, 71, 117, 121, 88, 87, 47, 80, 82, 54, 74, 73, 50, 88, 114, 105, 101, 85, 69, 52, 74, 97, 101, 109, 78, 99, 72, 106, 79, 56, 116, 116, 83, 53, 117, 109, 73, 90, 67, 83, 51, 104, 75, 88, 122, 121, 81, 106, 49, 88, 77, 104, 55, 48, 112, 107, 67, 47, 114, 66, 100, 110, 118, 48, 51, 117, 88, 76, 86, 52, 121, 81, 98, 110, 57, 120, 101, 48, 77, 98, 116, 110, 107, 65, 66, 67, 100, 110, 49, 105, 87, 115, 73, 109, 76, 99, 68, 43, 72, 105, 65, 52, 75, 87, 106, 76, 100, 57, 106, 107, 78, 43, 102, 56, 106, 67, 110, 71, 49, 48, 105, 82, 72, 98, 110, 56, 52, 107, 122, 78, 85, 120, 108, 67, 97, 116, 43, 65, 74, 80, 105, 66, 121, 56, 67, 56, 68, 54, 83, 81, 52, 102, 51, 67, 82, 89, 82, 102, 72, 66, 85, 102, 112, 68, 100, 105, 74, 79, 107, 100, 88, 89, 120, 88, 101, 43, 109, 120, 74, 118, 109, 88, 111, 84, 113, 87, 113, 56, 43, 43, 120, 99, 76, 47, 67, 97, 107, 108, 111, 89, 75, 78, 54, 48, 86, 114, 80, 50, 81, 98, 50, 52, 97, 82, 89, 53, 81, 87, 90, 84, 89, 103, 49, 84, 113, 87, 66, 117, 120, 112, 108, 87, 55, 66, 114, 68, 104, 103, 110, 53, 85, 114, 52, 101, 113, 55, 84, 98, 53, 69, 54, 53, 54, 105, 97, 98, 67, 106, 103, 43, 70, 70, 65, 50, 43, 118, 73, 116, 117, 54, 89, 55, 66, 114, 69, 68, 116, 120, 50, 57, 97, 112, 70, 81, 121, 82, 80, 110, 113, 77, 79, 121, 66, 57, 99, 48, 88, 101, 88, 65, 89, 119, 80, 83, 69, 73, 99, 77, 49, 48, 70, 105, 74, 119, 100, 85, 75, 98, 109, 116, 67, 108, 108, 49, 74, 106, 101, 80, 106, 112, 105, 51, 103, 56, 72, 72, 99, 68, 102, 78, 79, 68, 100, 69, 82, 80, 82, 67, 89, 57, 88, 80, 55, 69, 121, 84, 68, 90, 110, 105, 88, 79, 84, 105, 87, 50, 69, 114, 54, 43, 67, 67, 122, 79, 107, 43, 122, 83, 76, 87, 43, 74, 108, 43, 48, 89, 101, 111, 102, 70, 77, 54, 110, 73, 52, 49, 111, 97, 54, 113, 106, 52, 67, 121, 87, 108, 117, 107, 118, 66, 102, 77, 99, 103, 84, 82, 48, 75, 86, 122, 116, 67, 110, 80, 119, 78, 53, 112, 114, 53, 109, 56, 110, 111, 54, 98, 115, 117, 51, 54, 77, 119, 66, 98, 106, 109, 88, 84, 97, 78, 76, 120, 77, 107, 110, 120, 104, 116, 65, 89, 120, 84, 110, 83, 122, 67, 109, 88, 79, 83, 107, 55, 117, 106, 107, 87, 99, 120, 54, 43, 116, 84, 103, 65, 84, 112, 52, 103, 85, 55, 90, 105, 55, 55, 78, 104, 71, 89, 75, 66, 102, 122, 74, 79, 67, 122, 55, 56, 77, 116, 90, 114, 72, 53, 68, 43, 101, 68, 57, 73, 65, 99, 98, 68, 81, 105, 77, 120, 76, 109, 43, 116, 97, 54, 98, 74, 57, 119, 119, 67, 102, 74, 67, 71, 111, 109, 50, 77, 99, 113, 118, 77, 109, 72, 101, 67, 86, 106, 121, 56, 48, 56, 100, 108, 73, 56, 102, 117, 68, 101, 74, 84, 82, 71, 102, 48, 50, 107, 75, 108, 84, 53, 56, 43, 111, 109, 89, 55, 67, 113, 43, 67, 67, 101, 83, 107, 51, 47, 65, 79, 83, 51, 83, 87, 78, 51, 116, 86, 74, 113, 116, 114, 114, 70, 86, 106, 82, 72, 97, 107, 48, 113, 110, 104, 85, 57, 74, 112, 47, 82, 99, 112, 43, 80, 48, 118, 101, 97, 104, 118, 89, 87, 76, 122, 102, 118, 99, 87, 67, 104, 49, 50, 82, 118, 43, 103, 43, 115, 121, 116, 66, 84, 78, 99, 72, 107, 113, 68, 114, 97, 115, 115, 76, 112, 101, 68, 105, 105, 122, 99, 43, 57, 103, 90, 79, 66, 57, 67, 55, 66, 43, 85, 52, 116, 105, 108, 69, 83, 81, 57, 97, 118, 106, 107, 103, 118, 98, 70, 43, 118, 87, 115, 118, 51, 74, 77, 112, 50, 101, 85, 57, 47, 57, 108, 84, 118, 66, 86, 88, 89, 107, 47, 48, 72, 120, 88, 68, 103, 57, 81, 112, 67, 87, 83, 122, 65, 67, 80, 110, 52, 104, 76, 106, 99, 115, 53, 51, 82, 105, 83, 65, 84, 87, 65, 100, 88, 68, 83, 54, 55, 118, 84, 116, 82, 112, 103, 71, 79, 56, 121, 115, 72, 101, 120, 86, 121, 81, 112, 88, 43, 77, 105, 117, 75, 114, 50, 116, 77, 112, 43, 70, 117, 48, 113, 48, 104, 121, 97, 57, 122, 112, 70, 83, 111, 86, 73, 69, 86, 110, 67, 49, 81, 47, 43, 100, 50, 97, 68, 119, 119, 77, 56, 108, 115, 69, 100, 49, 120, 76, 75, 68, 43, 76, 98, 112, 47, 55, 118, 66, 81, 85, 120, 47, 67, 54, 104, 104, 74, 121, 115, 48, 80, 77, 66, 105, 50, 105, 43, 82, 106, 117, 105, 90, 78, 78, 75, 55, 99, 50, 99, 71, 51, 89, 119, 81, 119, 47, 77, 43, 101, 89, 99, 84, 89, 118, 69, 77, 77, 117, 116, 68, 54, 100, 49, 116, 76, 66, 103, 48, 49, 43, 51, 90, 66, 86, 69, 99, 97, 99, 47, 85, 56, 66, 108, 50, 70, 69, 116, 72, 65, 54, 79, 103, 111, 113, 120, 102, 48, 97, 43, 108, 82, 100, 90, 108, 78, 50, 85, 73, 115, 82, 76, 89, 69, 119, 52, 98, 83, 102, 111, 56, 72, 87, 79, 119, 69, 86, 118, 113, 110, 67, 65, 47, 51, 100, 68, 75, 102, 89, 121, 112, 110, 82, 50, 81, 117, 99, 76, 98, 108, 77, 67, 74, 85, 117, 69, 113, 54, 118, 68, 51, 113, 78, 83, 102, 69, 70, 77, 47, 89, 113, 48, 117, 49, 73, 119, 47, 48, 81, 57, 68, 115, 102, 97, 55, 78, 99, 80, 55, 82, 88, 71, 75, 81, 88, 78, 66, 102, 78, 97, 51, 65, 82, 55, 119, 54, 121, 56, 69, 79, 116, 116, 53, 105, 68, 49, 82, 89, 105, 90, 100, 117, 117, 53, 55, 106, 68, 84, 113, 79, 102, 102, 84, 51, 79, 101, 105, 74, 117, 117, 108, 102, 68, 89, 81, 87, 119, 83, 105, 101, 113, 75, 57, 102, 53, 72, 105, 65, 51, 81, 76, 89, 49, 103, 114, 57, 117, 99, 84, 49, 88, 89, 79, 112, 73, 109, 112, 120, 115, 86, 115, 120, 115, 48, 112, 104, 66, 54, 55, 50, 87, 118, 73, 49, 97, 116, 110, 79, 69, 97, 47, 102, 70, 78, 72, 54, 107, 83, 107, 87, 81, 73, 115, 43, 99, 50, 110, 54, 84, 87, 53, 121, 113, 66, 89, 98, 114, 73, 109, 75, 82, 86, 121, 100, 77, 77, 117, 81, 73, 117, 69, 71, 71, 79, 67, 48, 112, 77, 98, 77, 117, 66, 84, 79, 68, 74, 90, 73, 75, 106, 100, 108, 82, 72, 72, 66, 76, 50, 122, 84, 54, 77, 111, 70, 97, 117, 67, 116, 74, 104, 111, 104, 119, 106, 84, 75, 69, 50, 73, 75, 118, 74, 115, 86, 52, 104, 90, 117, 55, 100, 100, 104, 50, 116, 122, 74, 99, 78, 74, 112, 112, 54, 89, 86, 68, 52, 75, 120, 116, 54, 121, 113, 77, 50, 80, 98, 76, 116, 118, 115, 76, 115, 100, 84, 78, 71, 84, 87, 122, 112, 56, 89, 75, 119, 43, 111, 57, 55, 57, 104, 77, 107, 55, 49, 88, 85, 101, 86, 76, 52, 54, 81, 103, 112, 51, 81, 47, 52, 79, 115, 69, 48, 69, 78, 104, 48, 102, 117, 84, 49, 103, 71, 72, 69, 103, 57, 86, 74, 50, 102, 77, 55, 101, 102, 119, 66, 89, 48, 80, 82, 104, 101, 114, 116, 75, 111, 103, 69, 56, 81, 110, 103, 99, 73, 111, 78, 54, 110, 98, 56, 115, 56, 89, 74, 57, 47, 113, 54, 99, 117, 87, 71, 57, 56, 49, 116, 50, 99, 54, 65, 99, 80, 79, 106, 121, 97, 88, 110, 109, 111, 111, 76, 78, 106, 117, 54, 101, 121, 120, 113, 100, 103, 49, 101, 57, 43, 104, 101, 53, 106, 57, 98, 51, 79, 104, 101, 81, 102, 120, 109, 108, 116, 102, 111, 53, 103, 103, 116, 50, 43, 108, 111, 47, 70, 108, 106, 120, 79, 81, 105, 113, 113, 57, 105, 43, 99, 85, 100, 107, 55, 49, 47, 120, 68, 65, 82, 50, 52, 71, 49, 108, 69, 117, 50, 89, 69, 110, 102, 78, 76, 105, 72, 50, 65, 70, 114, 98, 52, 47, 78, 52, 72, 103, 70, 68, 65, 105, 99, 87, 68, 104, 76, 86, 90, 76, 48, 79, 118, 73, 86, 43, 48, 51, 48, 81, 49, 82, 85, 51, 115, 90, 71, 80, 119, 43, 84, 82, 85, 82, 85, 57, 108, 48, 108, 106, 103, 99, 51, 66, 104, 48, 67, 43, 65, 50, 80, 48, 99, 100, 104, 115, 56, 117, 75, 77, 99, 75, 88, 47, 74, 47, 118, 112, 82, 98, 57, 116, 89, 78, 109, 55, 101, 85, 84, 50, 54, 77, 81, 74, 83, 86, 88, 101, 101, 71, 66, 85, 86, 84, 83, 109, 74, 112, 87, 112, 110, 103, 72, 87, 79, 82, 84, 50, 83, 80, 110, 80, 121, 114, 48, 122, 81, 98, 112, 54, 105, 122, 68, 90, 86, 114, 116, 73, 90, 43, 76, 72, 87, 110, 50, 97, 53, 116, 70, 56, 70, 106, 116, 107, 75, 109, 53, 98, 51, 47, 55, 77, 81, 54, 55, 52, 89, 67, 117, 102, 48, 69, 50, 72, 80, 108, 72, 80, 88, 77, 75, 110, 57, 114, 104, 53, 88, 109, 90, 115, 119, 65, 105, 80, 112, 86, 102, 86, 121, 71, 47, 88, 109, 74, 83, 66, 112, 118, 102, 47, 80, 81, 71, 70, 76, 75, 100, 74, 115, 103, 54, 105, 47, 102, 78, 114, 121, 108, 90, 85, 109, 103, 54, 98, 111, 115, 102, 114, 86, 90, 77, 101, 109, 98, 43, 76, 85, 83, 50, 114, 74, 109, 117, 110, 114, 78, 105, 81, 74, 100, 88, 87, 80, 101, 97, 67, 50, 73, 121, 57, 105, 67, 122, 57, 85, 57, 90, 118, 57, 120, 112, 100, 102, 76, 54, 86, 67, 90, 120, 105, 65, 68, 55, 78, 105, 56, 86, 51, 65, 47, 90, 89, 71, 115, 114, 90, 51, 86, 72, 74, 79, 69, 122, 57, 68, 85, 79, 119, 65, 50, 43, 85, 112, 100, 100, 54, 82, 101, 88, 118, 50, 56, 50, 102, 87, 100, 88, 47, 115, 43, 114, 50, 104, 97, 111, 54, 75, 89, 113, 81, 98, 67, 72, 84, 76, 71, 88, 114, 113, 99, 120, 88, 100, 116, 85, 51, 80, 100, 76, 85, 67, 77, 80, 118, 56, 116, 104, 88, 66, 82, 55, 105, 74, 100, 72, 98, 83, 53, 108, 77, 76, 109, 115, 49, 111, 83, 115, 47, 80, 109, 85, 100, 85, 111, 113, 48, 97, 56, 90, 106, 48, 118, 98, 88, 89, 84, 82, 48, 100, 108, 72, 84, 48, 52, 113, 103, 66, 99, 81, 88, 81, 101, 103, 120, 117, 119, 97, 81, 88, 115, 103, 49, 78, 99, 50, 51, 85, 106, 115, 51, 122, 114, 112, 84, 102, 51, 118, 114, 57, 69, 43, 103, 75, 109, 70, 65, 57, 105, 48, 89, 70, 54, 70, 114, 72, 75, 97, 81, 122, 114, 100, 88, 82, 99, 106, 111, 82, 120, 73, 121, 81, 108, 81, 119, 67, 110, 103, 76, 55, 117, 118, 110, 76, 108, 87, 85, 72, 73, 69, 99, 82, 88, 121, 78, 82, 120, 97, 75, 66, 43, 107, 82, 111, 97, 50, 108, 76, 57, 73, 98, 47, 47, 88, 82, 120, 88, 72, 49, 119, 70, 50, 50, 87, 56, 55, 97, 87, 48, 78, 102, 122, 112, 74, 112, 101, 85, 107, 111, 82, 47, 47, 122, 116, 81, 87, 79, 43, 51, 49, 122, 55, 47, 83, 84, 90, 102, 112, 74, 85, 47, 119, 73, 102, 84, 120, 51, 81, 50, 114, 121, 107, 78, 83, 105, 79, 103, 88, 112, 65, 49, 83, 87, 80, 119, 112, 89, 67, 121, 89, 87, 100, 103, 48, 111, 90, 51, 90, 49, 116, 48, 79, 87, 99, 57, 101, 68, 51, 100, 69, 106, 56, 121, 106, 100, 121, 69, 107, 115, 115, 79, 115, 79, 117, 71, 111, 105, 110, 89, 101, 50, 87, 52, 54, 107, 68, 99, 54, 43, 83, 81, 101, 83, 79, 51, 68, 101, 75, 84, 89, 74, 56, 66, 116, 108, 103, 49, 117, 56, 103, 122, 119, 102, 116, 111, 78, 71, 68, 119, 77, 88, 71, 66, 52, 72, 104, 121, 111, 55, 105, 121, 79, 119, 118, 68, 73, 76, 67, 47, 107, 77, 97, 102, 112, 76, 43, 75, 121, 122, 100, 86, 101, 57, 98, 116, 99, 98, 112, 87, 43, 57, 109, 54, 54, 110, 98, 77, 78, 54, 69, 77, 84, 87, 87, 99, 98, 57, 53, 75, 90, 116, 79, 113, 117, 87, 120, 120, 101, 74, 85, 83, 52, 66, 49, 89, 81, 115, 52, 48, 79, 79, 43, 87, 70, 54, 98, 53, 105, 112, 83, 107, 84, 109, 52, 69, 120, 101, 101, 105, 110, 111, 113, 110, 78, 73, 104, 85, 65, 57, 87, 121, 100, 105, 78, 76, 43, 54, 77, 87, 108, 97, 87, 105, 85, 49, 73, 110, 121, 118, 118, 48, 47, 70, 47, 105, 66, 109, 54, 101, 87, 50, 52, 65, 81, 83, 79, 101, 66, 73, 67, 71, 56, 79, 69, 74, 119, 88, 67, 119, 122, 49, 66, 121, 114, 79, 68, 50, 76, 90, 105, 71, 104, 53, 73, 110, 50, 67, 116, 72, 55, 43, 47, 122, 75, 89, 98, 68, 116, 80, 118, 48, 82, 75, 109, 51, 116, 84, 98, 77, 84, 122, 109, 120, 98, 112, 56, 108, 43, 55, 47, 110, 122, 80, 47, 65, 88, 57, 107, 69, 43, 50, 67, 70, 81, 115, 110, 89, 55, 43, 110, 97, 101, 82, 99, 69, 75, 101, 122, 108, 103, 121, 99, 113, 101, 106, 50, 97, 51, 67, 87, 122, 53, 68, 114, 84, 99, 116, 48, 65, 122, 106, 106, 112, 51, 122, 52, 80, 100, 73, 82, 99, 70, 98, 113, 98, 65, 115, 101, 83, 89, 86, 120, 68, 51, 65, 69, 68, 69, 98, 87, 109, 115, 122, 115, 107, 110, 71, 88, 47, 85, 70, 108, 50, 43, 55, 100, 56, 69, 121, 114, 73, 66, 48, 106, 90, 106, 53, 75, 48, 86, 50, 120, 103, 111, 78, 114, 114, 104, 103, 113, 98, 110, 56, 56, 87, 109, 55, 47, 98, 52, 80, 108, 82, 114, 74, 117, 100, 78, 72, 98, 119, 43, 70, 73, 56, 70, 82, 73, 103, 112, 97, 50, 77, 56, 71, 80, 57, 114, 51, 68, 103, 121, 90, 102, 80, 112, 87, 79, 115, 50, 74, 80, 69, 84, 108, 122, 75, 118, 69, 84, 54, 80, 106, 116, 50, 76, 115, 72, 66, 68, 65, 80, 89, 106, 53, 101, 72, 66, 115, 114, 89, 110, 120, 83, 97, 50, 81, 118, 120, 106, 109, 81, 83, 122, 86, 43, 43, 88, 49, 89, 88, 78, 113, 104, 86, 55, 56, 121, 65, 50, 51, 90, 56, 118, 49, 119, 52, 108, 77, 56, 104, 54, 66, 112, 67, 47, 77, 101, 81, 117, 83, 79, 48, 68, 122, 69, 75, 104, 67, 115, 55, 67, 99, 104, 67, 75, 115, 108, 52, 54, 69, 99, 85, 69, 111, 67, 116, 68, 103, 110, 121, 66, 70, 88, 106, 84, 77, 66, 54, 118, 51, 57, 50, 105, 87, 75, 101, 54, 99, 67, 76, 114, 122, 119, 76, 55, 118, 73, 113, 88, 56, 110, 67, 77, 66, 109, 73, 113, 73, 84, 100, 102, 101, 47, 106, 114, 79, 104, 56, 66, 102, 47, 107, 47, 48, 52, 108, 111, 88, 110, 87, 106, 116, 114, 48, 107, 66, 105, 72, 80, 120, 53, 76, 113, 49, 121, 109, 108, 53, 67, 103, 83, 66, 102, 48, 73, 84, 55, 98, 72, 88, 56, 65, 122, 115, 55, 69, 71, 117, 69, 67, 72, 99, 119, 88, 73, 56, 65, 47, 72, 78, 99, 76, 73, 75, 65, 84, 53, 43, 57, 116, 110, 66, 90, 105, 86, 53, 101, 76, 68, 117, 50, 56, 113, 116, 57, 85, 109, 80, 110, 89, 105, 117, 68, 104, 53, 120, 75, 104, 99, 104, 110, 83, 112, 52, 80, 67, 87, 115, 116, 102, 80, 122, 100, 83, 111, 97, 102, 67, 87, 112, 83, 83, 75, 70, 71, 66, 71, 55, 72, 108, 68, 71, 110, 69, 109, 106, 78, 109, 111, 114, 98, 76, 51, 119, 99, 121, 112, 76, 88, 75, 52, 83, 52, 56, 69, 43, 84, 57, 83, 119, 77, 104, 87, 86, 101, 79, 82, 102, 68, 118, 74, 109, 107, 76, 98, 121, 115, 80, 67, 82, 57, 72, 79, 97, 115, 110, 82, 70, 74, 110, 80, 78, 67, 47, 79, 121, 110, 83, 71, 98, 111, 105, 103, 55, 101, 117, 83, 68, 100, 117, 69, 69, 49, 97, 71, 78, 87, 110, 103, 69, 71, 72, 52, 73, 98, 110, 88, 115, 114, 114, 87, 113, 50, 89, 112, 113, 113, 66, 118, 98, 111, 43, 52, 52, 121, 68, 78, 102, 104, 116, 101, 107, 121, 116, 67, 82, 70, 101, 67, 67, 76, 107, 75, 100, 68, 90, 81, 48, 103, 122, 71, 119, 65, 108, 80, 81, 86, 81, 74, 48, 99, 97, 49, 74, 68, 51, 81, 103, 65, 68, 111, 115, 105, 100, 90, 97, 78, 70, 122, 79, 47, 100, 56, 66, 57, 71, 113, 88, 88, 68, 43, 84, 74, 49, 77, 47, 72, 118, 72, 114, 82, 122, 50, 52, 83, 85, 75, 66, 72, 86, 43, 50, 101, 121, 106, 116, 114, 102, 108, 117, 99, 103, 66, 116, 65, 49, 106, 79, 121, 115, 104, 51, 120, 78, 118, 73, 67, 88, 79, 57, 69, 120, 71, 103, 109, 103, 51, 90, 113, 43, 116, 52, 48, 115, 50, 109, 112, 66, 107, 50, 69, 100, 121, 65, 106, 54, 48, 81, 69, 43, 53, 75, 53, 69, 56, 48, 103, 122, 77, 50, 73, 57, 85, 74, 113, 99, 108, 84, 51, 116, 54, 68, 117, 114, 49, 52, 57, 121, 104, 66, 83, 70, 48, 90, 68, 85, 117, 116, 65, 104, 120, 68, 53, 76, 78, 66, 53, 73, 121, 65, 69, 78, 110, 101, 104, 117, 119, 84, 67, 73, 89, 81, 49, 87, 47, 109, 67, 54, 104, 108, 98, 99, 82, 55, 116, 48, 53, 103, 87, 69, 70, 50, 112, 121, 55, 74, 90, 54, 87, 76, 87, 81, 90, 76, 111, 118, 72, 77, 113, 66, 99, 117, 119, 97, 55, 52, 89, 56, 82, 117, 119, 54, 78, 54, 87, 53, 56, 83, 43, 106, 98, 108, 65, 86, 108, 110, 109, 105, 57, 118, 101, 107, 76, 78, 48, 48, 107, 89, 77, 112, 66, 74, 98, 89, 107, 86, 111, 51, 88, 68, 34, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 108, 0, 0, 0, 0, 0, -74, -98, -70, 0, 0, 0, 5, 105, 110, 100, 101, 120, 2, 0, 0, 0, 0, 13, -8, 71, 88, 0, 0, 1, 12, 110, 101, 120, 116, 97, 117, 100, 105, 101, 110, 99, 101, 0, 2, 6, 99, 111, 111, 107, 105, 101, 1, 0, 3, 7, 56, 51, 52, 51, 50, 54, 52, 0, 0, 1, 0, 4, 13, 49, 51, 52, 49, 57, 57, 49, 55, 57, 48, 50, 52, 56, -1, -1, -1, -1, -1, -1, -1, -1, -123, 56, 123, 34, 112, 97, 121, 108, 111, 97, 100, 34, 58, 32, 34, 51, 80, 56, 115, 117, 79, 77, 108, 81, 106, 76, 47, 100, 56, 108, 70, 50, 119, 86, 57, 116, 110, 54, 52, 114, 120, 80, 54, 86, 90, 65, 56, 74, 66, 48, 118, 70, 106, 70, 88, 120, 112, 107, 80, 68, 53, 49, 98, 103, 98, 111, 98, 56, 120, 72, 116, 49, 117, 47, 117, 77, 83, 101, 53, 112, 107, 120, 122, 90, 54, 111, 54, 48, 69, 101, 53, 87, 78, 73, 109, 101, 98, 122, 79, 111, 79, 97, 71, 116, 66, 117, 119, 47, 48, 106, 85, 48, 50, 47, 66, 111, 106, 103, 69, 110, 105, 90, 121, 53, 114, 66, 118, 116, 99, 116, 70, 110, 105, 115, 56, 117, 51, 56, 82, 116, 98, 89, 47, 118, 117, 72, 90, 68, 75, 86, 53, 52, 103, 54, 53, 74, 68, 83, 87, 74, 113, 121, 55, 88, 89, 50, 88, 121, 88, 115, 90, 118, 87, 119, 68, 106, 75, 49, 102, 122, 105, 119, 82, 43, 103, 50, 83, 67, 52, 120, 107, 106, 114, 52, 100, 108, 80, 107, 90, 105, 99, 117, 107, 78, 67, 110, 108, 78, 68, 79, 52, 72, 54, 68, 116, 47, 104, 111, 69, 71, 85, 100, 110, 118, 56, 108, 52, 47, 79, 90, 100, 81, 57, 121, 86, 117, 106, 77, 102, 118, 85, 54, 109, 104, 67, 57, 50, 113, 116, 71, 83, 109, 86, 50, 104, 49, 115, 116, 82, 116, 120, 57, 115, 80, 83, 102, 69, 99, 90, 53, 73, 106, 78, 53, 73, 51, 47, 117, 84, 103, 56, 48, 120, 54, 73, 115, 108, 81, 120, 76, 70, 101, 55, 71, 81, 120, 54, 107, 103, 43, 113, 57, 112, 53, 116, 88, 106, 89, 77, 53, 121, 57, 77, 55, 102, 78, 107, 53, 65, 82, 53, 74, 85, 97, 121, 87, 54, 115, 90, 55, 65, 43, 116, 67, 49, 117, 98, 105, 110, 98, 104, 111, 55, 119, 103, 77, 101, 57, 53, 109, 54, 48, 84, 87, 50, 88, 109, 115]]; ]]
'''
```
</description><key id="5543884">2098</key><summary>OutOfMemory Error -&gt; corrupted translog entry in shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jodok</reporter><labels /><created>2012-07-11T08:59:50Z</created><updated>2012-07-11T23:39:47Z</updated><resolved>2012-07-11T17:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-11T13:32:39Z" id="6906497">Its basically a translog entry that is corrupted, it can be fixed by removing the translog, but, we can also ignore this case as we can detect that its corrupt. Btw, did you have replicas when you indexed the data?
</comment><comment author="jodok" created="2012-07-11T13:35:59Z" id="6906583">we had no replicas while indexing the data. it would be great, if elasticsearch ignores corrupt translog data (and not just retry over and over). a hint, which file is corrupted might help as well. we "fixed" it by deleting the index and creating it again - which is of course not optimal.
</comment><comment author="kimchy" created="2012-07-11T23:39:47Z" id="6923197">If you had replicas, then it would have tried the other replica to recover from. I pushed a fix in any case. Btw, you wouldn't have needed to delete the index, just remove the translog (or trim it a bit).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add replace operation type #2096</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2097</link><project id="" key="" /><description /><key id="5536857">2097</key><summary>add replace operation type #2096</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-07-10T22:54:32Z</created><updated>2014-09-08T09:27:08Z</updated><resolved>2014-07-08T12:20:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2014-07-08T12:20:27Z" id="48328607">same end as #2096 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace operation type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2096</link><project id="" key="" /><description>It would be nice to have a "replace" op_type. It would be similar to "create" op_type except that it will fail if the document to replace does not already exist in the index.
</description><key id="5536845">2096</key><summary>Replace operation type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-07-10T22:53:49Z</created><updated>2014-07-08T20:26:19Z</updated><resolved>2014-07-08T12:18:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-15T16:49:27Z" id="20983130">Hey, do you still think this is a useful addition? Anything that cannot be covered with the Update API? (Didnt dig too deep in your PR at https://github.com/elasticsearch/elasticsearch/pull/2097 I have to admit)
</comment><comment author="Paikan" created="2013-07-30T08:27:19Z" id="21776390">@spinscale I think an index request with an op_type set to replace is a bit different than what is covered by the update API used with a "doc" update because the doc will be merged into the existing indexed doc and won't replace it.

The idea here was a bit to mimic memcache in having a create and replace operation.

If no one think that new op_type is useful I am happy to help you cleaning old issues by closing this one :)

Otherwise I am willing to update the PR to fit with the updated ES code.
</comment><comment author="Paikan" created="2014-07-08T12:18:32Z" id="48328433">As interests have been limited for this one I am closing it to help @clintongormley closing old issues :)
</comment><comment author="clintongormley" created="2014-07-08T20:26:19Z" id="48395301">:D
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighters (both kinds) break when phrase query has stop words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2095</link><project id="" key="" /><description>When a phrase query with a stop word in it is used, the highlighting breaks in different ways, depending on whether the plain or term vector highlighter is used.

The plain highlighter does strange things to the result - in my example, it split the excerpt in two and lists them in reverse order.

The term vector simply returns no highlight fragment at all. It seems like it knows enough to match the phrase, but when going to highlight it just subtracts the stop-word from the pattern it's using to find the excerpt.

I have a recreation of both examples here:
https://gist.github.com/3083891

This is similar to #1986, but I'm focusing just on the end result, not on suggesting a way to switch which highlighter is used.
</description><key id="5525172">2095</key><summary>Highlighters (both kinds) break when phrase query has stop words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konklone</reporter><labels /><created>2012-07-10T15:07:27Z</created><updated>2016-11-10T13:12:52Z</updated><resolved>2014-07-08T15:33:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="konklone" created="2013-04-27T01:33:03Z" id="17108070">So #2157 seems related to this. Is the solution here to use `"enable_position_increments": "false"` on the analyzer for this field? What are the ramifications for doing this?

The net effect of this bug is that many common searches, e.g. "freedom of information" can't include highlighted excerpts with the results.
</comment><comment author="ajhalani" created="2014-01-13T21:39:41Z" id="32214367">+1, able to replicate issue.
</comment><comment author="konklone" created="2014-01-13T21:48:40Z" id="32215288">Thanks, @ajhalani. It's been filed for 18 months, and is still a problem. Can someone from the elasticsearch team at least ring in with a suggested workaround?
</comment><comment author="ajhalani" created="2014-02-20T16:48:47Z" id="35642209">I don't think there's been any fix for this. Using plain highlighter is terribly slow for big documents, so scaling is a issue. 
The workaround I found is to use a custom analyzer which doesn't remove stopwords. Thus term vector highlighter works fine and we are able to retain its advantages. 
</comment><comment author="konklone" created="2014-02-20T16:52:04Z" id="35642592">Thanks for the suggestion - could you post or link to your custom analyzer declaration?
</comment><comment author="ajhalani" created="2014-02-20T17:40:31Z" id="35647935">```
            standard_no_stop:
                tokenizer: standard
                filter: [standard, lowercase]
```
</comment><comment author="konklone" created="2014-02-20T18:21:08Z" id="35652198">Sorry, one more question - if I omit a stop words filter, should I expect a user search for "and" to take  a huge amount of time and load? Seems like a serious downside.
</comment><comment author="ajhalani" created="2014-02-20T20:09:41Z" id="35663397">Glad to answer any questions I can. 
There's a small performance hit , it depends lot on if the query has stop words in it or not or if it's a phrase search. Roughly it was ~10-15% and slightly more for phrase (quoted) search.  There is also an advantage though, earlier searching for "a or b" used to match  "a and b"  or   "a for b" etc. With new analzyer, it matches exactly.  I guess you will have to test it on your data to find exact performance impact. 
</comment><comment author="clintongormley" created="2014-07-08T15:33:24Z" id="48355163">Tested this out and both the plain and FVH highlighters do the right thing with the `match_phrase` query in master.  Closing.  Please reopen if this is still an issue
</comment><comment author="yoav2" created="2016-11-10T13:12:52Z" id="259686445">Is there any solution for this?. highlighting is not working when I index with English Analyzer, and search for a phrase with stop words.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid NPE when passing a bool filter with should, must or not keys omitted.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2094</link><project id="" key="" /><description>I was getting NPEs on a seemingly valid JSON:
"filtered":{
        "query":{
            "match_all":{}
        },
        "filter":{
            "and":{
                "filters":[
                    {
                            "should":{
                                "term":{
                                    "category":"Laptops"
                                }
                            }
                    }
                ]
            }
        }
</description><key id="5518483">2094</key><summary>Avoid NPE when passing a bool filter with should, must or not keys omitted.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andriusj</reporter><labels /><created>2012-07-10T09:00:52Z</created><updated>2014-07-16T21:55:09Z</updated><resolved>2012-07-14T11:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-14T11:20:12Z" id="6982163">Pushed, thanks!. There was another check that was needed basically verifying that a filter was actually provided.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce Text abstraction, allowing for improved representation of strings, apply to HighlightedField (breaks backward for Java API from String to Text)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2093</link><project id="" key="" /><description>By introducing the `Text` abstraction, we can keep (long) text fields in their UTF8 bytes format, and no need to convert them to a string when serializing it back to Json for example.

The first place we can apply this is to highlighted text, which can be long.. . This does breaks backward comp. for people using the Java API where the `HighlightField` now has a `Text` as its content, and not `String`.
</description><key id="5510886">2093</key><summary>Introduce Text abstraction, allowing for improved representation of strings, apply to HighlightedField (breaks backward for Java API from String to Text)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-09T22:46:59Z</created><updated>2012-07-09T22:47:50Z</updated><resolved>2012-07-09T22:47:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Stored binary fields to change internal representation (break Java API if used)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2092</link><project id="" key="" /><description>This is a breaking change for the Java API, where for stored binary fields, the type is now `BytesReference` and not `byte[]`.
</description><key id="5500445">2092</key><summary>Stored binary fields to change internal representation (break Java API if used)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-09T14:53:06Z</created><updated>2012-07-09T14:53:27Z</updated><resolved>2012-07-09T14:53:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>CompressorFactory forces inclusion of lucene jars to TransportClient applications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2091</link><project id="" key="" /><description>It seems there is a problem with applications using ES but only the TransportClient. TransportClient should not depend on Lucene jars AFAIK.

But now such applications will bark with 

```
java.util.concurrent.ExecutionException: java.lang.NoClassDefFoundError: org/apache/lucene/store/IndexOutput
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.xbib.importer.ImportService.waitFor(ImportService.java:124)
        at org.xbib.importer.ImportService.run(ImportService.java:82)
        at org.xbib.tools.medline.ElasticsearchMedlineIndexer.main(ElasticsearchMedlineIndexer.java:110)
Caused by: java.lang.NoClassDefFoundError: org/apache/lucene/store/IndexOutput
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
        at org.elasticsearch.common.compress.CompressorFactory.&lt;clinit&gt;(CompressorFactory.java:36)
        at org.elasticsearch.transport.support.TransportStreams.buildRequest(TransportStreams.java:109)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:453)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:185)
        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:63)
        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:100)
        at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:214)
        at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:97)
        at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:141)
        at org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:328)
        at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:128)
        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53) [...]
```

Is this by accident or do I need to always include Lucene jars in my ES client applications?
</description><key id="5486493">2091</key><summary>CompressorFactory forces inclusion of lucene jars to TransportClient applications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-07-08T12:57:11Z</created><updated>2016-03-03T22:25:27Z</updated><resolved>2012-07-22T09:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-09T14:34:09Z" id="6848112">Well, it wasn't intentional, but its getting hard to try and separate the need for Lucene core jar with elasticsearch. For example, a search hit can have an explanation associated with it, which is a Lucene object (but does not fails when the lucene jar do not exists, and you don't ask for explanation).

The way elasticsearch is built is that it tries not to require Lucene jars with the transport client, though with the project structure, its not enforceable module wise, and trying to do that is going to deeply complicate things.

I need to think about it a bit, and see if the compression part should be solved for the transport client case. If we do solve it, its going to complicate the codebase, and its a question if its worth the complication.
</comment><comment author="tallpsmith" created="2012-07-11T21:17:49Z" id="6920056">We're currently relying on using the TransportClient within an app that still uses significant portions of a much older version of Lucene just to work around this issue.  Hate to see it regress! we'd be screwed.

Could the Lucene library be shaded like all the other ones to avoid this?
</comment><comment author="jprante" created="2012-07-11T21:42:31Z" id="6920697">Maybe it's worth to think about modularizing the ES code a little bit, splitting out a separate TransportClient build.

On the server side, where most of the plugins live, I'd love to see all ES dependencies unshaded, because it will be convenient for plugin authors to reuse transitive dependencies (of course lucene, but also guava, jackson, joda,...)

Meanwhile on the transport client side, shading the ES dependencies could help integrating ES into legacy Java apps that depend on older versions of Lucene or other software without conflicts, as noted by tallpsmith.

Instead of shading, maven offers an alternative for building uberjars by using the maven assembly plugin, so the ES server could be built with all dependencies in one single jar with the command `mvn assembly:assembly DdescriptorRef=jar-with-dependencies`. See http://maven.apache.org/plugins/maven-assembly-plugin/descriptor-refs.html#jar-with-dependencies
</comment><comment author="kimchy" created="2012-07-21T21:17:30Z" id="7156766">The shading part is intentional, the shaded libraries we use in elasticsearch are for all intent and purpose part of elasticsearch, the version used is tied closely into what elasticsearch exposes and how it uses the library based on the internals of how the library works (and that changes between versions), netty and guava are great examples.

Lucene on the other hand never really felt like something that we should shade. We can possibly shade it as well, since lucene changes enough between versions (like 3.5-&gt;3.6) that you can't really different versions with ES, but on the other hand it does increases the elasticsearch jar size.

The aim was not to require Lucene if you just use the TransportClient. The overhead of maintaining it as a different module is not worth it though, I prefer that we invest time in actually progressing elasticsearch than complicating the codebase just for that. This, as has been noted, has changed and because our compression module can now also compress Lucene input/output content, and it is also same compression abstraction used in the networking layer (its very similar, compressing lucene data and compression streams), Lucene is now needed when you use just the TransportClient.

Btw, I have no problem with actually providing several jars of elasticsearch, one with lucene not shaded, and one with Lucene shaded. Not sure how to do it with maven though. I don't want to provide a version that does not shade netty/jackson for example, because of the deep intimiate usage elasticsearch has with them (for example, using the upcoming bufferring improvement with any previous version of netty except for the current one will actually use more memory compared to using considerably less).
</comment><comment author="jprante" created="2012-07-22T09:17:42Z" id="7159875">I agree, the time is more valuable to move Elasticsearch forward instead of complicating the codebase.

Because this issue is not important enough I think I close the issue.

Just one thought. The CompressorFactory and the Compressor interface is used from several other places in the ES codebase, but it is also dependent on external Lucene IndexInput and IndexOutput. This dependency could be delegated back to the places where CompressorFactory/Compressor is used. Wrapper classes like CompressedIndexInput and CompressedIndexOutput could look up dynamically what compression codecs are available so a factory is no longer needed in the Lucene parts of ES. A redesigned codec interface for compression could help to remove the dependencies from the CompressorFactory. The compressor codecs like bzip2, lzf, snappy could become even plugins. I saw such a codec interface design here: https://github.com/nearinfinity/lucene-compression

Since I'm too busy to get this managed now, I'm sorry I can just offer my humble thoughts. 
</comment><comment author="tallpsmith" created="2012-07-24T03:00:48Z" id="7197596">I'd like to keep this open, and I'd like to volunteer to help with the Maven side of things to try to bring up an additional classified artifact that has the Lucene things shaded.  

If I was able to keep the existing packaging as is, and provide an additional classified (Maven-speak) jars with the Lucene shaded, would Shay be open to a Pull Request? 
</comment><comment author="jprante" created="2012-07-24T08:41:34Z" id="7201646">He said he has no problem with it, so go for it :)
</comment><comment author="tfreitas" created="2012-10-24T01:22:58Z" id="9724624">closed? or open?
</comment><comment author="jprante" created="2012-10-24T18:05:27Z" id="9749817">Still pending... in the meantime, I have prepared two projects https://github.com/jprante/elasticsearch-client (without any lucene or netty dependency) and https://github.com/jprante/elasticsearch-server (re-using the client codebase) as a showcase to address the modularization challenge.  
It's a preview and not complete, I want to include all the core plugins as maven modules. It should compile, and integration tests run sucessful. There are very few API changes. You can also play with maven dependencies, shading or not shading, packaging, maven classifiers etc. I did the task mainly because I wanted to create a developer path with a netty websocket ES ingest client and a netty websocket transport plugin, which must use a non-shaded netty dependency.  
Well, by intention, it is not announced or documented yet. Shay's team is busy right now with Lucene 4 so I think I wait after 0.21 release. Hopefully, after the Lucene 4 move, the ES core team will pick up the idea of codebase modularization.
</comment><comment author="vmassol" created="2013-09-01T18:27:32Z" id="23630194">This is also a problem for me and I also didn't understand why ES java client API required Lucene. My problem is that I stuck on Lucene 4.0.0 on my project and ES 0.90.3 client API requires Lucene 4.4.0... A real pain.

It would have been really nice to separate the org.elasticsearch:elasticsearch jar into 2: one for the client API and another one for the server APIs.

See http://jira.xwiki.org/browse/XWIKI-8911?focusedCommentId=77140&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-77140

Thanks
</comment><comment author="kimchy" created="2013-09-02T15:25:09Z" id="23666243">We found ourself repeating code in several places on the client level between ES codebase and lucene codebase. We are simply going to require the Lucene jars when using the native Java client, the code duplication and maintenance overhead/headache is not worth it.

There are a few clients based on the HTTP API that you can use in various programming languages on the JVM.
</comment><comment author="vmassol" created="2013-09-02T22:04:08Z" id="23680029">ok, too bad I have to use yet another external library... (like Jest). You should reconsider the need for Lucene on the client API side at some point to make it extra simple/easy to use that API. Thanks.
</comment><comment author="tallpsmith" created="2013-09-02T23:02:32Z" id="23682481">Shay, is there any reason that the Lucene jar couldn't just be shaded like
a bunch of the other ones ?  Wouldn't that just make life simple ?  Am I
missing something here? Not sure why Lucene is not shaded but others like
netty etc are.

On 3 September 2013 08:04, Vincent Massol notifications@github.com wrote:

&gt; ok, too bad I have to use yet another external library... (like Jest). You
&gt; should reconsider the need for Lucene on the client API side at some point
&gt; to make it extra simple/easy to use that API. Thanks.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2091#issuecomment-23680029
&gt; .
</comment><comment author="Hronom" created="2016-03-03T22:25:27Z" id="191994277">Any progress on this?
Still have a pain with lucene...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better ExplainableSearchScript interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2090</link><project id="" key="" /><description>ExplainableSearchScript is a life saver to debug native script, but it doesn't display accurately the explanation of the computation. The explanation of the sub query of the custom scorer should be placed in the hierarchy by the custom scorer, since it is the one using it, or not. Hence the suggested patch.
</description><key id="5473210">2090</key><summary>Better ExplainableSearchScript interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-07-06T20:20:56Z</created><updated>2014-07-16T21:55:09Z</updated><resolved>2012-07-09T14:39:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-09T14:39:41Z" id="6848287">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Confirming use of 'path' and 'index_name' in objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2089</link><project id="" key="" /><description>Hiya

I'm using `index_name` and `path` in what feels like a hacky way, and just want to confirm that the way it works is intended, and won't change in the future.

I'm using a `UID` object to reference other objects stored in other types in ElasticSearch. That `UID` has an `index`, `type` and `id` attribute.  I'd like to be able to access those as `uid.index`, `uid.type` etc, even thought the path may be:

```
{ employees: { managers: { uid: { index: 'foo' .....}}}}
```

 I'm using a mapping like this:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "baz" : {
               "type" : "object",
               "properties" : {
                  "foo" : {
                     "type" : "object",
                     "properties" : {
                        "uid" : {
                           "path" : "just_name",
                           "type" : "object",
                           "properties" : {
                              "index" : {
                                 "index_name" : "uid.index",
                                 "type" : "string"
                              }
                           }
                        }
                     }
                  }
               }
            }
         }
      }
   }
}
'
```

Note the `index_name: "uid.index"` and `path: "just_name"`.

This mapping makes these fields queryable:
- index
- uid.index
- baz.foo.uid.index

That suits my purposes, but I'd like to make sure that it isn't going to change :)

```
curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1'  -d '
{
   "baz" : {
      "foo" : {
         "uid" : {
            "index" : "bar"
         }
      }
   }
}
'
```
# index = 1

```
curl -XGET 'http://127.0.0.1:9200/test/test/_count?pretty=1'  -d '
{
   "text" : {
      "index" : "bar"
   }
}
'
```
# uid.index = 1

```
curl -XGET 'http://127.0.0.1:9200/test/test/_count?pretty=1'  -d '
{
   "text" : {
      "uid.index" : "bar"
   }
}
'
```
# foo.uid.index = 0

```
curl -XGET 'http://127.0.0.1:9200/test/test/_count?pretty=1'  -d '
{
   "text" : {
      "foo.uid.index" : "bar"
   }
}
'
```
# baz.foo.uid.index = 1

```
curl -XGET 'http://127.0.0.1:9200/test/test/_count?pretty=1'  -d '
{
   "text" : {
      "baz.foo.uid.index" : "bar"
   }
}
'
```
</description><key id="5468569">2089</key><summary>Confirming use of 'path' and 'index_name' in objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-07-06T16:19:41Z</created><updated>2013-10-18T13:28:14Z</updated><resolved>2013-10-18T13:28:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T13:28:14Z" id="26595263">Makes sense, I used the same tricks too with `"path":"just_name"` and `index_name` in a previous project.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException[Wrong value for index [yes] for field [...]]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2088</link><project id="" key="" /><description>I get the following error `MapperParsingException[Wrong value for index [yes] for field [date]]`, when I try to _put_ the following mapping into my totally new and empty index (created with `curl -XPUT http://localhost:9200/myindex/`).

```
curl -XPUT http://localhost:9200/myindex/mytype/_mapping -d ' {
  "mytype": {
    "properties": {
      "date": {
        "type": "long",
        "store": "yes",
        "index": "yes",
        "omit_norm": "yes",
        "omit_term_freq_and_positions": "yes"
      }   
    }   
  }
}'
```

Setting `index` to `no` _works_.

In Java using Lucene 3.6.0 I can do the following:

``` java
NumericField f = new NumericField("date", Field.Store.Yes, true);
f.setOmitNorms(true);
f.setOmitTermFreqAndPositions(true);
f.setLongValue(1L);

Document d = new Document();
d.add(f);
```

From this (and the documentation [here](http://www.elasticsearch.org/guide/reference/mapping/core-types.html), which doesn't mention that it's illegal what I'm trying to do) I assume what I'm trying to do is _doable_.

(I realize that elasticsearch has a _date_ type, but I _must_ use the mapping mentioned above.)
</description><key id="5463409">2088</key><summary>MapperParsingException[Wrong value for index [yes] for field [...]]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kohanyirobert</reporter><labels /><created>2012-07-06T11:39:19Z</created><updated>2013-06-14T08:28:11Z</updated><resolved>2012-07-06T12:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-07-06T12:20:19Z" id="6803554">&gt; ```
&gt; curl -XPUT http://localhost:9200/myindex/mytype/_mapping -d ' {
&gt;   "mytype": {
&gt;     "properties": {
&gt;       "date": {
&gt;         "type": "long",
&gt;         "store": "yes",
&gt;         "index": "yes",
&gt;         "omit_norm": "yes",
&gt;         "omit_term_freq_and_positions": "yes"
&gt;       }   
&gt;     }   
&gt;   }
&gt; }'
&gt; ```
&gt; 
&gt; Setting `index` to `no` _works_.
&gt; 
&gt; In Java using Lucene 3.6.0 I can do the following:
&gt; 
&gt; ``` java
&gt; NumericField f = new NumericField("date", Field.Store.Yes, true);
&gt; f.setOmitNorms(true);
&gt; f.setOmitTermFreqAndPositions(true);
&gt; f.setLongValue(1L);
&gt; 
&gt; Document d = new Document();
&gt; d.add(f);
&gt; ```
&gt; 
&gt; From this (and the documentation
&gt; [here](http://www.elasticsearch.org/guide/reference/mapping/core-types.html), which doesn't mention that it's illegal what I'm trying to do) I assume what I'm trying to do is _doable_.

The allowed values for "index" are "no", "not_analyzed" and "analyzed"

&gt; (I realize that elasticsearch has a _date_ type, but I _must_ use the
&gt; mapping mentioned above.)

Why? Given that that is pretty much exactly what the 'date' field maps
to.

Also, why are you storing the field? Each stored field means another
disk seek, which is slow.  By default the _source field is stored,
meaning that you can retrieve the whole doc almost as quickly as you can
retrieve a single stored field.

clint
</comment><comment author="kohanyirobert" created="2012-07-06T12:29:18Z" id="6803712">&gt; The allowed values for "index" are "no", "not_analyzed" and "analyzed"

Okay, you're right, thanks for the heads up. Reading ES's documentation on _numeric_ fields I somehow assumed that an _index_ mapping's value can only be _no_ or _yes_, because the possible values wasn't listed beside mapping field's documentation.

&gt; Also, why are you storing the field?

Did you ever hear about _legacy code_? :)

Thanks for the help.
</comment><comment author="kimchy" created="2012-07-09T19:52:54Z" id="6857584">@kohanyirobert in Lucene, there isn't really a meaning for analysis on numeric fields, so in ES, `analyzed` and `not_analyzed` is the same for numeric fields, meaning, "indexed", and `no` means not indexing it.
</comment><comment author="kohanyirobert" created="2012-07-10T04:10:35Z" id="6866520">@kimchy Yeah, I've already managed to figure this out. What lead me astray was that the legacy code I'm _porting_ to ES created Lucene _numeric_ fields using a constructor which takes a boolean argument named _index_. From this (and from the documentation) it was evident for me that a numeric field can be indexed (_yes_) and not indexed (_no_).
</comment><comment author="patelatharva" created="2012-10-30T11:47:39Z" id="9902712">So the conclusion is we can only use 'analyzed','not_analyzed' or 'no' with index for date and numeric fields; and shouldn't use 'yes', right?! (if it is so then why the documentation is not reflecting the same! : ( )
</comment><comment author="Mpdreamz" created="2012-12-29T18:24:57Z" id="11756080">Core type documentation does indeed mention `yes`, `no` as the only two valid options for date, numeric and boolean `index` values.

http://www.elasticsearch.org/guide/reference/mapping/core-types.html

and

http://www.elasticsearch.org/guide/reference/mapping/type-field.html could make a better mention of possible values as well.
</comment><comment author="pgplus1628" created="2013-03-30T17:50:30Z" id="15678670">i think the document of http://www.elasticsearch.org/guide/reference/mapping/core-types.html on boolean and long types has a mistake. "yes" should be "not_analyzed" or "analyzed".
</comment><comment author="cutalion" created="2013-04-10T16:00:24Z" id="16183643">@zorksylar, you're so right!
</comment><comment author="eranid" created="2013-06-14T08:28:11Z" id="19445076">@clintongormley you suggest not setting store to true, but the documentation for number and boolean suggest it should be set to "yes" if index is set to "no". It further says otherwise there is nothing we can do with it.
Should it not be in the "_source" anyway, even if both are set to "no"?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed TermsFacet.Entry ordering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2087</link><project id="" key="" /><description>If ComparatorType.COUNT is used Entries with same count will now be sorted ascending and not descending.
</description><key id="5462443">2087</key><summary>fixed TermsFacet.Entry ordering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dawi</reporter><labels><label>discuss</label></labels><created>2012-07-06T10:24:57Z</created><updated>2014-07-18T09:44:09Z</updated><resolved>2014-07-18T09:44:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:29:20Z" id="48354355">Relates to #2078
</comment><comment author="jpountz" created="2014-07-18T09:42:55Z" id="49413313">Closing in favor of https://github.com/elasticsearch/elasticsearch/issues/6917
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread Pool: Allow to configure the queue_type for fixed thread pool (linked/array), defaults to array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2086</link><project id="" key="" /><description /><key id="5461922">2086</key><summary>Thread Pool: Allow to configure the queue_type for fixed thread pool (linked/array), defaults to array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-06T09:48:11Z</created><updated>2012-08-07T11:54:08Z</updated><resolved>2012-07-06T09:48:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Local Gateway: old global state files are not properly deleted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2085</link><project id="" key="" /><description /><key id="5454808">2085</key><summary>Local Gateway: old global state files are not properly deleted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-05T22:19:17Z</created><updated>2012-07-05T22:19:48Z</updated><resolved>2012-07-05T22:19:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.5.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2084</link><project id="" key="" /><description /><key id="5453498">2084</key><summary>Upgrade to Netty 3.5.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-05T21:08:36Z</created><updated>2012-07-05T21:09:54Z</updated><resolved>2012-07-05T21:09:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Explanable native script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2083</link><project id="" key="" /><description>I have added an interface to be implemented by native scripts to be able to explain the score
</description><key id="5448898">2083</key><summary>Explanable native script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-07-05T17:29:09Z</created><updated>2014-07-16T21:55:10Z</updated><resolved>2012-07-05T23:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-05T23:04:32Z" id="6793309">Looks good!. Pushed to 0.19 and master, changed the name to ExplainableSearchScript, and had it extend SearchScript.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent _parent field query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2082</link><project id="" key="" /><description>There is a small inconsistency in the _parent field query. 

With term query, only **parent_id** is required, but with e.g. wildcard query, the format **parent_type#parent_id** is required, as described in http://www.elasticsearch.org/guide/reference/api/delete.html

Demonstration: a curl-based parent/child example script can be found here: https://gist.github.com/3047912

Alternative - term query

```
curl -XGET 'http://localhost:9200/test/library/_search?pretty' -d '{
      "query": {
            "term" : {
                  "_parent" : "1"
             }
      }
}'
```

Alternative - wildcard query

```
curl -XGET 'http://localhost:9200/test/library/_search?pretty' -d '{
      "query": {
            "wildcard" : {
                  "_parent" : "title#1"
             }
      }
}'
```

Both yield same result.

Suggestion: there should be no special format or query types allowed for _parent queries to avoid confusion. Possibly every _parent query should just be forced to become a term query. The docs at http://www.elasticsearch.org/guide/reference/mapping/parent-field.html need a query example. http://www.elasticsearch.org/guide/reference/api/delete.html should be cleaned up.
</description><key id="5430727">2082</key><summary>Inconsistent _parent field query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">jprante</reporter><labels /><created>2012-07-04T16:13:05Z</created><updated>2015-08-26T14:11:34Z</updated><resolved>2015-08-26T14:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-01-06T15:18:00Z" id="68877956">Better late than never... I don't exactly recall how the support for `_parent` field query was in 2012, but at the moment both formats (with and without parent type) are supported for both the term and wildcard queries.

The reason why there is a special type syntax is that in the case that there are multiple parent types, one can pick the parent type to filter by. 
</comment><comment author="martijnvg" created="2015-08-26T14:11:34Z" id="135035039">this is no longer inconsistent 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compression: Support snappy as a compression option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2081</link><project id="" key="" /><description>Add support for `snappy` as a compression option (on top of `lzf`), using the `snappy-java` library. Setting `compress.default.type` to `snappy` will change all compression to `snappy`. Note, anything compressed with `lzf` can still be decompressed and compressed.
</description><key id="5429833">2081</key><summary>Compression: Support snappy as a compression option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.9</label><label>v0.20.0.RC1</label></labels><created>2012-07-04T15:13:45Z</created><updated>2012-07-04T15:14:57Z</updated><resolved>2012-07-04T15:14:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add initial implementation of script_source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2080</link><project id="" key="" /><description>What do you think about this solution for the external source problem? It basically allows users to replace source in the query using a script. The replaced source is then used for highlighting, in script fields, etc. Here is a small demo: https://gist.github.com/3040566
</description><key id="5410323">2080</key><summary>Add initial implementation of script_source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-07-03T16:03:43Z</created><updated>2014-06-13T09:12:22Z</updated><resolved>2012-10-23T13:29:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="asoqa" created="2012-10-24T03:28:24Z" id="9726565">i want a solution to integrate elasticsearch with hbase
</comment><comment author="gelmajjouti" created="2013-02-01T09:30:25Z" id="12986800">What happened to this idea? Looks like it could be very useful to query an external database for source.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Faceting _all index with named type causes exceptions on non matching index/types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2079</link><project id="" key="" /><description>If I have index/types of /session1206/session, /session1207/session, /users/user and I do a search of /_all/session/_search I get exceptions in the users index, when it doesn't even match the type I'm searching on.

curl -XPUT "http://localhost:9200/sessions1206/session/1" -d '{"lp":1}'
curl -XPUT "http://localhost:9200/sessions1207/session/1" -d '{"lp":2}'
curl -XPUT "http://localhost:9200/users/user/1" -d '{"id":"andy"}'

curl "http://localhost:9200/_all/session/_search" -d '{"facets":{"histo":{"histogram":{"field":"lp","interval":60,"size":1440}}}}'

I still get the correct results, however the exceptions are worrying.

[2012-07-02 10:42:07,973][DEBUG][action.search.type       ] [Tana Nile] [users][2], node[E5meS5ZxT36UQRYvifPwkg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4eb98fe1]
org.elasticsearch.search.SearchParseException: [users][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"facets":{"histo":{"histogram":{"field":"lp","interval":60,"size":1440}}}}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:557)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:469)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:204)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:280)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:212)
    at org.elasticsearch.search.action.SearchServiceTransportAction$2.handleException(SearchServiceTransportAction.java:160)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:304)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:295)
    at org.elasticsearch.transport.netty.MessageChannelHandler.process(MessageChannelHandler.java:247)
    at org.elasticsearch.transport.netty.MessageChannelHandler.callDecode(MessageChannelHandler.java:154)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:103)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:563)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:91)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:373)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:247)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [histo]: No mapping found for field [lp]
    at org.elasticsearch.search.facet.histogram.unbounded.CountHistogramFacetCollector.&lt;init&gt;(CountHistogramFacetCollector.java:63)
    at org.elasticsearch.search.facet.histogram.HistogramFacetProcessor.parse(HistogramFacetProcessor.java:138)
    at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:93)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:545)
    ... 27 more

[2012-07-02 10:42:07,973][DEBUG][action.search.type       ] [Tana Nile] [users][0], node[MK4leTfGTQG4iIbkhhW8iw], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4eb98fe1]
org.elasticsearch.transport.RemoteTransportException: [Beautiful Dreamer][inet[/10.181.195.113:9301]][search/phase/query]
Caused by: org.elasticsearch.search.SearchParseException: [users][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"facets":{"histo":{"histogram":{"field":"lp","interval":60,"size":1440}}}}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:557)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:469)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:497)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:486)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:390)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [histo]: No mapping found for field [lp]
    at org.elasticsearch.search.facet.histogram.unbounded.CountHistogramFacetCollector.&lt;init&gt;(CountHistogramFacetCollector.java:63)
    at org.elasticsearch.search.facet.histogram.HistogramFacetProcessor.parse(HistogramFacetProcessor.java:138)
    at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:93)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:545)
    ... 8 more
</description><key id="5387630">2079</key><summary>Faceting _all index with named type causes exceptions on non matching index/types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2012-07-02T14:48:21Z</created><updated>2014-07-08T15:27:59Z</updated><resolved>2014-07-08T15:27:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:27:59Z" id="48354104">This has been fixed in aggregations. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facet Ordering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2078</link><project id="" key="" /><description>It would be great if ES would support more TermsFacet.ComparatorTypes as well as using them in a compound manner.

Example:

```
termsFacetBuilder.order(TermsFacet.ComparatorType.COUNT, 
                        TermsFacet.ComparatorType.TERM_LOWERCASE, 
                        TermsFacet.ComparatorType.TERM)
```

Explanation:

I have the following terms:
- a
- aa
- aa
- aA
- aA
- ab
- ab
- aB
- aB

The following facet search

```
termsFacetBuilder.size(4).order(TermsFacet.ComparatorType.COUNT)
```

returns 
- ab (2)
- aa (2)
- aB (2)
- aA (2)

What I want to achieve is a result like this:
- aA (2)
- aa (2)
- aB (2)
- ab (2)

And I think the only way to achieve this result is to have more ComparatorTypes and the possibility to chain them.

Manual sorting is not really an option, because if I use the size(x) method, I already get results that I don't really want.
</description><key id="5382048">2078</key><summary>Terms Facet Ordering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dawi</reporter><labels><label>discuss</label></labels><created>2012-07-02T08:53:06Z</created><updated>2014-07-18T09:40:21Z</updated><resolved>2014-07-18T09:40:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="edwardsmit" created="2012-10-12T09:30:33Z" id="9371016">+1 I have a requirement to have facets sorted in a case-insensitive manner, which I now have to do myself on the returned facets.
</comment><comment author="DominicWatson" created="2013-04-25T14:57:28Z" id="17012015">+1 Or even defaulting to term order as a tiebreaker when the counts are the same would be good.
</comment><comment author="igorv88" created="2013-06-18T09:57:54Z" id="19601560">+1
</comment><comment author="clintongormley" created="2013-06-18T09:59:06Z" id="19601613">cc @uboness 
</comment><comment author="CodingFabian" created="2013-09-29T20:48:39Z" id="25328873">any chance to get a short term improvement?
Either do a BC break and change the default sorting, or add a constant allowing for case insensitive sorting?
</comment><comment author="matthiasfeist" created="2014-05-15T12:02:40Z" id="43200978">+1
</comment><comment author="jpountz" created="2014-07-18T09:40:21Z" id="49413087">Given that facets are now deprecated, I'm closing in favor or #6917
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>(full) recovery memory based indices with local gateway, don't fail them</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2077</link><project id="" key="" /><description /><key id="5377365">2077</key><summary>(full) recovery memory based indices with local gateway, don't fail them</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-07-01T22:27:23Z</created><updated>2012-07-01T22:31:16Z</updated><resolved>2012-07-01T22:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ElasticSearchIllegalStateException: stream marked as compressed, but no compressor found on transport layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2076</link><project id="" key="" /><description>This happens because the message is empty, but the networking buffer still holds data for the next message, we should not check for a compressor in this case.
</description><key id="5375696">2076</key><summary>ElasticSearchIllegalStateException: stream marked as compressed, but no compressor found on transport layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-07-01T17:52:19Z</created><updated>2012-07-01T18:26:12Z</updated><resolved>2012-07-01T18:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Boolean fields do not accept 'F' as false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2075</link><project id="" key="" /><description>According to the docs on http://www.elasticsearch.org/guide/reference/mapping/core-types.html a `boolean` field should accept `F` as false, but it doesn't:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "x" : {
               "type" : "boolean"
            }
         }
      }
   },
   "settings" : {
      "number_of_shards" : 1
   }
}
'
curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1&amp;refresh=true'  -d '
{
   "x" : "F"
}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "facets" : {
      "x" : {
         "terms" : {
            "field" : "x"
         }
      }
   },
   "size" : 0
}
'

# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "facets" : {
#       "x" : {
#          "other" : 0,
#          "terms" : [
#             {
#                "count" : 1,
#                "term" : "T"
#             }
#          ],
#          "missing" : 0,
#          "_type" : "terms",
#          "total" : 1
#       }
#    },
#    "took" : 2
# }
```
</description><key id="5375286">2075</key><summary>Boolean fields do not accept 'F' as false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2012-07-01T16:41:34Z</created><updated>2014-07-08T15:24:25Z</updated><resolved>2014-07-08T15:24:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:24:25Z" id="48353408">Actually, I misread the docs. It says that it stores `T` and `F` internally but not that those values are accepted as `true` or `false`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support wildcard and +/- notation for multi index APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2074</link><project id="" key="" /><description>Support wildcards for multi index APIS, for example: `test-2012-*`. Also, support `+` and `-` as notations to include and exclude indices / aliases, for example: `+test-2012-*,-test-2012-01-01`.

Note, the wildcard notation applies to aliases when applicable, "expanding" them to match other aliases, and on indices, to expand them to match to several indices.
</description><key id="5375145">2074</key><summary>Support wildcard and +/- notation for multi index APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-07-01T16:15:45Z</created><updated>2012-07-01T16:16:14Z</updated><resolved>2012-07-01T16:16:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>elasticsearch high cpu usage </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2073</link><project id="" key="" /><description>Hi,
i have elasticsearch installed on two clustered  ec2 large nodes 
today i found that elasticsearch process consume almost all cpu available on both nodes 
my elasticsearch version 0.19.2
i'm using openjdk-6-jre version 6b22-1.10.6-0ubuntu1

jstack on node 2 =&gt; 

root@ga2:/usr/lib/jvm/java-6-sun# bin/jstack  -F 26556
Attaching to process ID 26556, please wait...
Debugger attached successfully.
Server compiler detected.
JVM version is 20.1-b02
Deadlock Detection:

No deadlocks found.

Thread 2692: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=198 (Compiled frame)
- java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(java.util.concurrent.SynchronousQueue$TransferStack$SNode, boolean, long) @bci=174, line=424 (Compiled frame)
- java.util.concurrent.SynchronousQueue$TransferStack.transfer(java.lang.Object, boolean, long) @bci=102, line=323 (Interpreted frame)
- java.util.concurrent.SynchronousQueue.poll(long, java.util.concurrent.TimeUnit) @bci=11, line=874 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=62, line=945 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 2089: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=198 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(org.elasticsearch.common.util
  .concurrent.jsr166y.LinkedTransferQueue$Node, org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQue
  ue$Node, java.lang.Object, boolean, long) @bci=180, line=702 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(java.lang.Object, boolean, int, long) @bci=286, line=615 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(long, java.util.concurrent.TimeUnit) @bci=9, line=1117 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=62, line=945 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 2088: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=198 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue$Node, org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue$Node, java.lang.Object, boolean, long) @bci=180, line=702 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(java.lang.Object, boolean, int, long) @bci=286, line=615 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(long, java.util.concurrent.TimeUnit) @bci=9, line=1117 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=62, line=945 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 30638: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=198 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue$Node, org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue$Node, java.lang.Object, boolean, long) @bci=180, line=702 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(java.lang.Object, boolean, int, long) @bci=286, line=615 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(long, java.util.concurrent.TimeUnit) @bci=9, line=1117 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=62, line=945 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 30610: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 30553: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=198 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.awaitMatch(org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue$Node, org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue$Node, java.lang.Object, boolean, long) @bci=180, line=702 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.xfer(java.lang.Object, boolean, int, long) @bci=286, line=615 (Compiled frame)
- org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue.poll(long, java.util.concurrent.TimeUnit
  ) @bci=9, line=1117 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=62, line=945 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 30552: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 26557: (state = BLOCKED)

Thread 27616: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=158 (Interpreted frame)
- java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=811 (Interpreted frame)
- java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(int) @bci=55, line=969 (Interpreted frame)
- java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(int) @bci=24, line=1281 (Interpreted frame)
- java.util.concurrent.CountDownLatch.await() @bci=5, line=207 (Interpreted frame)
- org.elasticsearch.bootstrap.Bootstrap$3.run() @bci=3, line=222 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27615: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Interpreted frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Interpreted frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Interpreted frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink$Boss.run() @bci=23, line=232 (Interpreted frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27614: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=158 (Interpreted frame)
- java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=1987 (Interpreted frame)
- java.util.concurrent.LinkedBlockingQueue.take() @bci=29, line=399 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=78, line=947 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27501: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27500: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27499: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27498: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted fra
  me)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27497: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27496: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27494: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27487: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Compiled frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Compiled frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.SelectorUtil.select(java.nio.channels.Selector) @bci=4, line=33 (Compiled frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=57, line=157 (Compiled frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27398: (state = IN_NATIVE)
- sun.nio.ch.EPollArrayWrapper.epollWait(long, int, long, int) @bci=0 (Compiled frame; information may be imprecise)
- sun.nio.ch.EPollArrayWrapper.poll(long) @bci=18, line=210 (Compiled frame)
- sun.nio.ch.EPollSelectorImpl.doSelect(long) @bci=28, line=65 (Interpreted frame)
- sun.nio.ch.SelectorImpl.lockAndDoSelect(long) @bci=37, line=69 (Interpreted frame)
- sun.nio.ch.SelectorImpl.select(long) @bci=30, line=80 (Interpreted frame)
- org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink$Boss.run() @bci=23, line=232 (Interpreted frame)
- org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=102 (Interpreted frame)
- org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.runTask(java.lang.Runnable) @bci=59, line=886 (Interpreted frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=28, line=908 (Interpreted frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 27390: (state = BLOCKED)
- java.lang.Thread.sleep(long) @bci=0 (Interpreted frame)
- org.elasticsearch.indices.ttl.IndicesTTLService$PurgerThread.run() @bci=60, line=132 (Interpreted frame)

Thread 27232: (state = BLOCKED)
- sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
- java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=198 (Compiled frame)
- java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(long) @bci=68, line=2025 (Compiled frame)
- java.util.concurrent.DelayQueue.take() @bci=57, line=164 (Compiled frame)
- java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take() @bci=4, line=609 (Compiled frame)
- java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take() @bci=1, line=602 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor.getTask() @bci=78, line=947 (Compiled frame)
- java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=18, line=907 (Compiled frame)
- java.lang.Thread.run() @bci=11, line=662 (Interpreted frame)

Thread 26806: (state = BLOCKED)
- java.lang.Thread.sleep(long) @bci=0 (Interpreted frame)
- org.elasticsearch.threadpool.ThreadPool$EstimatedTimeThread.run() @bci=18, line=356 (Interpreted frame)

Thread 26579: (state = BLOCKED)

Thread 26578: (state = BLOCKED)

Thread 26565: (state = BLOCKED)
- java.lang.Object.wait(long) @bci=0 (Interpreted frame)
- java.lang.ref.ReferenceQueue.remove(long) @bci=44, line=118 (Interpreted frame)
- java.lang.ref.ReferenceQueue.remove() @bci=2, line=134 (Interpreted frame)
- java.lang.ref.Finalizer$FinalizerThread.run() @bci=3, line=159 (Interpreted frame)

Thread 26564: (state = BLOCKED)
- java.lang.Object.wait(long) @bci=0 (Interpreted frame)
- java.lang.Object.wait() @bci=2, line=485 (Interpreted frame)
- java.lang.ref.Reference$ReferenceHandler.run() @bci=46, line=116 (Interpreted frame)
</description><key id="5373984">2073</key><summary>elasticsearch high cpu usage </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EslamElHusseiny</reporter><labels /><created>2012-07-01T12:25:37Z</created><updated>2012-07-02T21:08:09Z</updated><resolved>2012-07-02T21:08:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mindreframer" created="2012-07-01T13:24:34Z" id="6693438">this could be due to #leapocalypse
check you logs for: 
 "Clock: inserting leap second 23:59:60 UTC" 
fixed with: 

```
 sudo /etc/init.d/ntp stop; sudo date -s "`date`"; sudo /etc/init.d/ntp start
```

cheers!
</comment><comment author="EslamElHusseiny" created="2012-07-01T13:55:07Z" id="6693650">thanks this solved the issue
but i have no idea what is this all about
just for the sake of knowledge can you describe what did you mean 
</comment><comment author="mindreframer" created="2012-07-01T13:56:56Z" id="6693663">search twitter for  #leapocalypse ))) you'll see, what i mean 
</comment><comment author="konklone" created="2012-07-02T18:42:34Z" id="6716506">@EslamElHusseiny, can you close the issue if it's resolved?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exists API can send response body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2072</link><project id="" key="" /><description>If I do a HEAD query like:

```
curl -XHEAD 'http://127.0.0.1:9200/test/test/2'
```

If (eg) `test` doesn't exist, it returns an error in the response body.

This is not allowed in the HTTP spec (http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html)

```
The HEAD method is identical to GET except that the server MUST NOT return a message-body in the response.
```

While some HTTP clients (eg `curl`) will handle this incorrect response, other clients will just not read the body, causing subsequent requests to hang.

Rather send the error message in a header.
</description><key id="5366533">2072</key><summary>Exists API can send response body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-06-30T09:34:34Z</created><updated>2012-07-01T22:04:56Z</updated><resolved>2012-07-01T22:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-06-30T09:42:11Z" id="6684132">For instance:

```
curl -XPUT 'http://127.0.0.1:9200/t1/' 
curl -XPUT 'http://127.0.0.1:9200/t2/' 
curl -XPOST 'http://127.0.0.1:9200/_aliases?pretty=1'  -d '
{
   "actions" : [
      {
         "add" : {
            "index" : "t2",
            "alias" : "test"
         }
      }
   ]
}
'
curl -XPOST 'http://127.0.0.1:9200/_aliases?pretty=1'  -d '
{
   "actions" : [
      {
         "add" : {
            "index" : "t1",
            "alias" : "test"
         }
      },
      {
         "add" : {
            "index" : "t2",
            "alias" : "test"
         }
      }
   ]
}
'

curl -XHEAD 'http://127.0.0.1:9200/test/test/2?pretty=1'
{
  "error" : "ElasticSearchIllegalArgumentException[Alias [test] has more than one index associated with it [[t2, t1]], can't execute a single index op]",
  "status" : 400
}

curl -XGET 'http://127.0.0.1:9200/?pretty=1' 
```

While the above doesn't hang with `curl`, it hangs with one of my backends.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store Compression: integer overflow causes failed reads (index is safe)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2071</link><project id="" key="" /><description>This applied to large segments (&gt;4gb)
</description><key id="5363926">2071</key><summary>Store Compression: integer overflow causes failed reads (index is safe)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-06-29T23:34:48Z</created><updated>2012-06-29T23:35:43Z</updated><resolved>2012-06-29T23:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update API: Update without script, provide an update doc and recursively update the original document with the new doc. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2070</link><project id="" key="" /><description>```
curl -XPUT localhost:9200/test/type1/1 -d '{
    "counter" : 1,
    "tags" : ["red"],
    "name" : {
        "first" : "Shay"
    }
}'

# add a field
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "doc" : { "text" : "some text" }
}'

# recursively update
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "doc" : {
        "name" : {
            "last" : "Banon"
        }
    }
}'

curl -XGET localhost:9200/test/type1/1
```
</description><key id="5356490">2070</key><summary>Update API: Update without script, provide an update doc and recursively update the original document with the new doc. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2012-06-29T16:24:25Z</created><updated>2013-03-08T04:58:50Z</updated><resolved>2013-03-08T04:58:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-06-29T18:01:26Z" id="6670473">I was thinking about a list of actions (but never got around to formulating the syntax). Something like:

```
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    actions: [
        { set:      { 'foo.bar.baz' : 1 }},
        { merge:    { 'foo.bar.xyz' : { one: { two: 3 }}}},
        { delete:   'abc.def' },
        { push:     { tags: ['val_1', 'val_2']}},
        { remove:   { tags: ['old_val']}}
        { set:      { timestamp: '_now_' }} 
    ]
}'
```

etc
</comment><comment author="mattweber" created="2012-06-29T18:49:32Z" id="6673437">I think most of that can be done with a script.  The new update doc (d6bc17fee5) covers set and merge.  I imagine we can use a HTTP DELETE with _update to handle deleting of fields using the new update doc if one doesn't want to use a script...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip non-persisted indices during recovery from gateway</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2069</link><project id="" key="" /><description>Proof of concept thus no tests yet.

Observed behavior with a memory index and local gateway is that on a full cluster restart (try 1 shard, 0 replicas, restart the node) the recovery fails.
It will not even correctly restore the index structure, because loading the shard(s) fails.

This patch does not attempt to load the shards for non-persistent index stores.
</description><key id="5351601">2069</key><summary>Skip non-persisted indices during recovery from gateway</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kroepke</reporter><labels /><created>2012-06-29T14:26:24Z</created><updated>2014-07-16T21:55:11Z</updated><resolved>2012-07-01T22:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-01T22:15:17Z" id="6697538">This change is a bit problematic, since it skips recovery also in case of shared gateway, where we can recover into non persistent store. I can add a similar change to the local gateway recovery mechanism. I will close this and open an issue for this.
</comment><comment author="kroepke" created="2012-07-02T10:58:10Z" id="6705342">Ok. With some guidance I'm happy to implement the necessary changes if they make sense. Just let me know :)
</comment><comment author="kimchy" created="2012-07-02T20:58:05Z" id="6719582">Actually, already implemented :). Its in 0.19.8.
</comment><comment author="kroepke" created="2012-07-02T21:02:23Z" id="6719679">That was easy ;)

On Jul 2, 2012, at 10:58 PM, Shay Banonreply@reply.github.com wrote:

&gt; Actually, already implemented :). Its in 0.19.8.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/2069#issuecomment-6719582
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.19.6 OSGi Environment: Could not initialize class org.elasticsearch.common.util.concurrent.EsExecutors$ExecutorScalingQueue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2068</link><project id="" key="" /><description>My aim is to start a server instance using an OSGi bundle.  I have packed my own bundle and write some startup code in BundleActivator and embedded the elasticsearch and related jars into the bundle.

It seemed that I was hitting an infinite loop:

2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [generic], type [cached], keep_alive [30s]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [index], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [bulk], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [get], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [search], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [percolate], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [generic], type [cached], keep_alive [30s]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [index], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [bulk], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [get], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [search], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [percolate], type [cached], keep_alive [5m]
2012-06-29 00:35:16 threadpool [DEBUG] [Sweetface] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]

Further debugging, I see the exception in my debugger.

```
Could not initialize class org.elasticsearch.common.util.concurrent.EsExecutors$ExecutorScalingQueue
```

In addition, I also found that sun.misc.Unsafe class not found maybe the root cause.

Any clues?
</description><key id="5342722">2068</key><summary>0.19.6 OSGi Environment: Could not initialize class org.elasticsearch.common.util.concurrent.EsExecutors$ExecutorScalingQueue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kmtong</reporter><labels /><created>2012-06-29T03:44:30Z</created><updated>2013-05-24T12:46:49Z</updated><resolved>2013-05-24T12:46:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kmtong" created="2012-06-29T06:53:01Z" id="6653367">Bingo!  I found the cupid.  Really, I have to add a startup parameter for sun.misc package as mentioned in 

http://stackoverflow.com/questions/4462154/how-to-access-the-internal-sun-security-class-from-an-osgi-bundle

After I added the JVM parameter:

```
-Dorg.osgi.framework.system.packages.extra=sun.misc
```

and the server starts!

I think it is good for not relying on the internal classes, right?
</comment><comment author="kimchy" created="2012-06-29T18:35:21Z" id="6672737">@kmtong many libraries use Unsafe, or end up having better perf when using Unsafe (see LZF compression lib for example), the one we do is include the jsr166y and e since we rely on their concurrency utilities, and they in turn use Unsafe (those are the concurrency libs of Java 7 and upcoming Java 8).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto import dangling indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2067</link><project id="" key="" /><description>Dangling indices happen when a node that has several indices stored locally, joins a cluster and those local indices do not exists in the cluster metadata. This usually does not happen, especially not with proper gateway.recover_after_nodes flag, but still, users can by mistake get into this state.

A new setting `gateway.local.auto_import_dangled` setting, with possible values of `no` (never import dangling indices, but also delay the delation of them), `yes` (import dangling indices), and `closed` (import dangling indices, but in closed state). The default value is `yes`.
</description><key id="5334521">2067</key><summary>Auto import dangling indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-06-28T22:02:36Z</created><updated>2017-02-27T04:55:51Z</updated><resolved>2012-06-28T23:01:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-08-11T02:25:01Z" id="7663162">mark
</comment><comment author="synhershko" created="2012-10-03T10:25:51Z" id="9101739">What is the intended behavior?

We are seeing the following: running a cluster of 2 data nodes, and a Java application which uses an instance of a Client as a non-data node, we index some documents into several different indices, and shutdown the application.

Then we issue a delete all indexes request: curl -XDELETE 'http://localhost:9200/', and can see the command was acknowledged and performed.

The next time we run the application, we see the following - all indexes being recreated on one of the data nodes, even tho the Java app was hosting a non-data node, and they were explicitly marked for deletion on the data node:

[2012-10-03 12:17:41,131][INFO ][cluster.service          ] [Firearm] added {[Landslide][pT7UUb00TNWYBTeluR6mcw][inet[/192.168.1.8:9302]],}, reason: zen-disco-receive(join from node[[Landslide][pT7UUb00TNWYBTeluR6mcw][inet[/192.168.1.8:9302]]])
[2012-10-03 12:17:41,320][INFO ][gateway.local.state.meta ] [Firearm] auto importing dangled indices [2008-01-01-1200/OPEN][2010-10-29-12-00/OPEN][2005-01-01-0000/OPEN][2006-01-01-0000/OPEN][2009-01-01-0000/OPEN][2002-01-01-1200/OPEN][2004-01-01-0000/OPEN][2011-08-08-12-00/OPEN][2007-09-02-12-00/OPEN][2004-08-28-12-00/OPEN][2009-08-09-12-00/OPEN][2005-01-01-1200/OPEN][foo/OPEN][2010-01-01-0000/OPEN][2001-01-01-1200/OPEN][2002-01-01-0000/OPEN][2011-12-12-12-00/OPEN][2010-10-28-12-00/OPEN][2001-01-01-0000/OPEN][2007-01-01-1200/OPEN][2002-12-09-12-00/OPEN][2010-01-01-1200/OPEN][2004-01-01-1200/OPEN][2012-01-12-12-00/OPEN][2011-01-01-0000/OPEN][2010-11-23-12-00/OPEN][2011-08-07-12-00/OPEN][2011-01-01-1200/OPEN][2012-01-13-12-00/OPEN][2012-01-01-1200/OPEN][2012-01-01-0000/OPEN][2007-09-08-12-00/OPEN][2009-01-01-1200/OPEN][el-2005-01-01-0000/OPEN][2007-01-01-0000/OPEN][2008-01-01-0000/OPEN][2009-08-15-12-00/OPEN][2010-11-22-12-00/OPEN][2003-01-01-1200/OPEN][2004-08-22-12-00/OPEN][2006-01-01-1200/OPEN][2003-01-01-0000/OPEN][2011-12-13-12-00/OPEN] from [[Landslide][pT7UUb00TNWYBTeluR6mcw][inet[/192.168.1.8:9302]]]

It's either I'm missing something, or it is an issue with both a non-data node storing data it shouldn't and no tombstones are kept.
</comment><comment author="gvkrf" created="2017-02-27T04:44:03Z" id="282626960">Hi kimchy,

I would like to make Auto import dangling indices to "no", can you please help me out here how to do it and which file i need to change. Can you tell me the file location.</comment><comment author="tvernum" created="2017-02-27T04:55:51Z" id="282628072">@gvkrf Please do not use GitHub issues to ask questions about how to use Elasticsearch.

Please head to our discussion site http://discuss.elastic.co/ and ask your question there.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API endpoint for closing a scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2066</link><project id="" key="" /><description>Per our discussion, would be nice to have an explicit endpoint for closing a scroll in the case that the client wishes to terminate iteration before all results have been retrieved.
</description><key id="5334361">2066</key><summary>API endpoint for closing a scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels /><created>2012-06-28T21:54:33Z</created><updated>2014-02-21T15:20:15Z</updated><resolved>2014-02-21T15:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-06-29T07:52:35Z" id="6654136">Agreed ++
</comment><comment author="spinscale" created="2014-02-21T15:20:15Z" id="35739393">Implemented via clear scroll API http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-search-type.html#clear-scroll
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dangling index handling might still remove the state files for the dangling index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2065</link><project id="" key="" /><description>We try and not delete indices automatically when a node joins the cluster and has some local indices that do not exist in the cluster metadata state. But, we might still delete the state files for them, which is obviously a bug...
</description><key id="5320373">2065</key><summary>dangling index handling might still remove the state files for the dangling index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-06-28T11:32:25Z</created><updated>2012-06-28T11:32:54Z</updated><resolved>2012-06-28T11:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>getSourceAsString() for doc inserted as SMILE fails, auto convert to JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2064</link><project id="" key="" /><description>In a unit test, I have a document containing one field "val" which is a list of floats:

{ Float.MIN_VALUE, Float.MIN_VALUE - 1, Float.MAX_VALUE, -Float.MIN_VALUE }

The JSON representation of this is:

{"val":[1.4E-45,-1.0,3.4028235E38,-1.4E-45]}

getSourceAsString() on the corresponding GetResponse object returns this successfully when I inserted the document using  XContentFactory.jsonBuilder(), but when I inserted the document using XContentFactory.smileBuilder(), I got:

java.lang.ArrayIndexOutOfBoundsException: 87
    at org.elasticsearch.common.Unicode.UTF8toUTF16(Unicode.java:190)
    at org.elasticsearch.common.Unicode.unsafeFromBytesAsUtf16(Unicode.java:106)
    at org.elasticsearch.common.Unicode.fromBytes(Unicode.java:80)
    at org.elasticsearch.index.get.GetResult.sourceAsString(GetResult.java:203)
    at org.elasticsearch.action.get.GetResponse.sourceAsString(GetResponse.java:150)
...

Will attach a demonstration case when I have a chance.
</description><key id="5319559">2064</key><summary>getSourceAsString() for doc inserted as SMILE fails, auto convert to JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrewclegg</reporter><labels><label>enhancement</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-06-28T10:33:05Z</created><updated>2012-06-28T12:18:06Z</updated><resolved>2012-06-28T12:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-28T10:46:55Z" id="6626669">Yea, that will happen..., wondering what should be done here, obviously, smile can't be converted to string, maybe we should do auto conversion to json and then return it as string?
</comment><comment author="andrewclegg" created="2012-06-28T10:56:41Z" id="6626817">Aaah okay. Won't need a demo case if it's known behaviour :-)

Auto conversion would work (if it's not super heavyweight?), but even some sort of NotSupportedException that told you you couldn't do this with SMILE docs would be useful.
</comment><comment author="kimchy" created="2012-06-28T10:58:41Z" id="6626837">I think I will add auto conversion, in any case, you are usually better off using the bytes and not the string itself in your app.
</comment><comment author="andrewclegg" created="2012-06-28T11:00:09Z" id="6626855">We're not actually even using the source, this was just in a defunct test that died when I switched to SMILE. Thanks for quick feedback!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add regex filter on terms_stats facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2063</link><project id="" key="" /><description>As discussed with @clintongormley in the mailing list : https://groups.google.com/forum/?hl=fr&amp;fromgroups#!topic/elasticsearch/E0Y0Ew7NlxE

In short :
I would like to be able to restrict results only on terms of a given length in a terms_stats facet. 

More details :
Use case Gist is here : https://gist.github.com/3007168

Let&#8217;s say I have a document like this :

``` json
{
  "id":"212468504455168001",
  "location":{
    "lat":46.20715933,
    "lon":6.14494212
  },
  "lat":46.20715933,
  "lng":6.14494212,
  "path":"1022232003013110123223112",
  "created_at":1339491413
}
```

I defined first an analyzer and a mapping for field path:

``` json
      "analyzer":{
        "myanalyzer":{
          "type":"custom",
          "tokenizer":"mytokenizer"
        }
      },
      "tokenizer":{
        "mytokenizer":{
          "type":"edgeNGram",
          "min_gram":"1",
          "max_gram":"25",
          "side":"front"
        }
      }
```

Then I defined a mapping for field path:

``` json
        "path":{"type":"string", "analyzer":"myanalyzer"}
```

As you can see, I apply a EdgeNGram on path field. So my path is broken in 25 tokens like : 1,10,102,1022,...,1022232003013110123223112

My need is now to compute facets on this field as a key, but only for tokens with a size of x (x depends to the user).

So if x is 4, I want to compute on the first 4 characters of my path.

My facet is :

``` json
    "path_lat":{
      "terms_stats":{
        "key_field":"path",
        "value_field":"lat",
        "size":0
      },
      "facet_filter":{
        "geo_bounding_box":{
          "location":{
            "top_left":{ "lat":84.4740645845916,  "lon":-179.999999 },
            "bottom_right":{ "lat":-75.67219739055291, "lon":179.999999 }
          }
        }
      }
    }
```

My concern is that I get facets for the 25 tokens for each document.

``` json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 3,
    "failed" : 2,
    "failures" : [ {
      "index" : "dpitestcase",
      "shard" : 3,
      "status" : 500,
      "reason" : "No active shards"
    }, {
      "index" : "dpitestcase",
      "shard" : 4,
      "status" : 500,
      "reason" : "No active shards"
    } ]
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "facets" : {
    "path_lat" : {
      "_type" : "terms_stats",
      "missing" : 0,
      "terms" : [ {
        "term" : "1022232003013110123223112",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223200301311012322311",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222320030131101232231",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022232003013110123223",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223200301311012322",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222320030131101232",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022232003013110123",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223200301311012",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222320030131101",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022232003013110",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223200301311",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222320030131",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022232003013",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223200301",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222320030",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022232003",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223200",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222320",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022232",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102223",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10222",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1022",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "102",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "10",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      }, {
        "term" : "1",
        "count" : 1,
        "total_count" : 1,
        "min" : 46.20715933,
        "max" : 46.20715933,
        "total" : 46.20715933,
        "mean" : 46.20715933
      } ]
    }
  }
}
```

But, I only need to compute facet on the edgeNGram with a length of 4.
</description><key id="5317558">2063</key><summary>Add regex filter on terms_stats facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-06-28T08:11:35Z</created><updated>2012-09-03T07:30:42Z</updated><resolved>2012-09-03T07:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="scharrier" created="2012-06-28T14:14:37Z" id="6630780">Exactly the same problem for me.
</comment><comment author="sgruhier" created="2012-07-02T07:12:32Z" id="6701832">+1
</comment><comment author="scharrier" created="2012-07-04T13:10:50Z" id="6759969">This can be absolutely dramatic, in terms of performances : actually I need 10 facets, and I get more than 8000 facets computed that I have to filter in PHP.

Juste the requesting and json decoding take more than 6s, where ES needs 85ms to calculate... _ouch_
</comment><comment author="dadoonet" created="2012-07-19T22:25:14Z" id="7117162">@kimchy : I started to implement this feature. Could you please have a look at my commit and tell me if I am following the right way?
https://github.com/dadoonet/elasticsearch/commit/2e2b5f6e226a068cd3b4e94b424a48e77424537f
BTW, I did not find test cases for TermsStats facets. Does it exist ?
</comment><comment author="dadoonet" created="2012-07-20T20:01:26Z" id="7139814">Forget my last comment. It was late and I didn't see the SimpleFacetsTests class! :-(
</comment><comment author="dadoonet" created="2012-09-03T07:30:42Z" id="8231277">I close this issue as the Pull Request #2109 now exists.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting index to no ends up using by default the "keyword" analyzer on it, which means one can't highlight on it when searching on _all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2062</link><project id="" key="" /><description /><key id="5304378">2062</key><summary>Setting index to no ends up using by default the "keyword" analyzer on it, which means one can't highlight on it when searching on _all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.8</label><label>v0.20.0.RC1</label></labels><created>2012-06-27T19:05:35Z</created><updated>2012-06-27T19:16:11Z</updated><resolved>2012-06-27T19:16:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Validate query bug fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2061</link><project id="" key="" /><description>Found a couple of bugs in the /validate/query request. The query explanations were not propagated between shards (fixed in de52f91) and because current SearchContext wasn't set, some query parsers were failing while trying to access SearchContext (fixed in be74b31).
</description><key id="5304351">2061</key><summary>Validate query bug fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-06-27T19:03:25Z</created><updated>2014-07-16T21:55:11Z</updated><resolved>2012-06-27T19:24:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-27T19:24:55Z" id="6613060">cheers, pushed to 0.19 branch and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make it easy to install on Ubuntu servers via a PPA</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2060</link><project id="" key="" /><description>If you google for [install elasticsearch ubuntu](http://www.google.com/search?client=safari&amp;rls=en&amp;q=install+elasticsearch+ubuntu&amp;ie=UTF-8&amp;oe=UTF-8) you'll get many (outdated) guides.

One thing is that it is no longer possible to install Oracle's Java as a package, and Elasticsearch can work well with openjdk.

Another thing is that this series of manual steps from a tarball is just so 1999. A package that would pull the dependencies, install the service and run it immediately would make it much easier.
</description><key id="5295171">2060</key><summary>Make it easy to install on Ubuntu servers via a PPA</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markoa</reporter><labels /><created>2012-06-27T11:49:20Z</created><updated>2013-08-05T15:12:20Z</updated><resolved>2013-05-24T12:39:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-27T12:05:50Z" id="6601465">btw, I thought you knew, but there is a new deb package that you can download.
</comment><comment author="markoa" created="2012-06-27T12:07:57Z" id="6601498">I saw it, but I don't know on which versions of Ubuntu it is guaranteed to work?

On Wednesday, June 27, 2012 at 14:05 , Shay Banon wrote:

&gt; btw, I thought you knew, but there is a new deb package that you can download.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/2060#issuecomment-6601465
</comment><comment author="kimchy" created="2012-06-27T12:18:39Z" id="6601680">should work on 10.04 and above, nothing specific to the ubuntu version there..., though newer versions of ubuntu, specifically 12.04 will have a newer default version of Java, which is better.
</comment><comment author="deanmalmgren" created="2013-07-27T17:09:51Z" id="21668845">I know there is a deb package (and an rpm) available on the downloads page, but it would definitely make things considerably easier to install elasticsearch if there were a PPA (or equivalent for RPMs). My current approach is to use a rather [unwieldy puppet module](https://github.com/deanmalmgren/puppet-elasticsearch) to install the latest version. It would be vastly simpler -- for me and others -- if you could just install elasticsearch with the more user-friendly package managers (apt-get, yum, etc). 

I admittedly don't know much about preparing a PPA (or equivalent for yum-based package management). Are there reasons for not doing it? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deb: Allow configuring max open files and max locked memory limits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2059</link><project id="" key="" /><description>The debian init script currently sets the max open files limit to 65535, and there's no option to set the max locked memory size limit. This patch makes both configurable.
</description><key id="5294163">2059</key><summary>deb: Allow configuring max open files and max locked memory limits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akheron</reporter><labels /><created>2012-06-27T10:32:57Z</created><updated>2014-07-05T21:32:05Z</updated><resolved>2012-06-27T11:11:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-27T11:11:32Z" id="6600570">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use bloom filter when flushing (applying deletes)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2058</link><project id="" key="" /><description>This is a hack into Lucene, effectively replacing the class that does delete buffering with ours, and adding the bloom filter filtering. This will improve updates or deletes flushing. Need to change the scripts to include elasticsearch jar as the first one in the classpath again.
</description><key id="5274852">2058</key><summary>Use bloom filter when flushing (applying deletes)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.7</label><label>v0.20.0.RC1</label></labels><created>2012-06-26T14:44:46Z</created><updated>2012-06-26T14:45:39Z</updated><resolved>2012-06-26T14:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Plugin info REST endpoint in node info</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2057</link><project id="" key="" /><description>Another version of pull request #2007, now integrated into node info mechanism.

One minor flaw is that the "plugins" message in the logs appears twice. I am not sure how to initialize the PluginsService correctly without getting the message printed.
</description><key id="5268763">2057</key><summary>Plugin info REST endpoint in node info</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jprante</reporter><labels /><created>2012-06-26T10:01:24Z</created><updated>2014-06-14T20:32:58Z</updated><resolved>2013-07-15T16:33:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-28T23:11:55Z" id="6644418">Can't the `NodeService` be injected with the `PluginService` instance? It should be available for it.
Also, the RestNodeInfo in `handleRequest` is missing setting the plugins flag there
</comment><comment author="jprante" created="2012-06-29T11:39:26Z" id="6657664">Ah, thanks for the hint. I updated the pull request.
</comment><comment author="brusic" created="2012-10-25T18:46:06Z" id="9789192">Would be great to have this feature. Was about to write it myself.
</comment><comment author="spinscale" created="2013-05-29T16:13:16Z" id="18627827">@jprante, can we close this, since we have `http://localhost:9200/_nodes?plugin=true` or is there anything missing?
</comment><comment author="spinscale" created="2013-07-15T16:33:27Z" id="20982044">closing this one, I think we"re good here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for zero queue size in the search thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2056</link><project id="" key="" /><description>See https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/LEmObqCvncM
</description><key id="5262789">2056</key><summary>Add support for zero queue size in the search thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-06-26T00:45:03Z</created><updated>2014-07-16T21:55:13Z</updated><resolved>2012-06-26T11:03:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-26T11:03:32Z" id="6572766">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stored Compression: failure to fetch document in certain cases (read failure, index compression works)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2055</link><project id="" key="" /><description /><key id="5262235">2055</key><summary>Stored Compression: failure to fetch document in certain cases (read failure, index compression works)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.7</label><label>v0.20.0.RC1</label></labels><created>2012-06-25T23:54:03Z</created><updated>2012-06-25T23:54:26Z</updated><resolved>2012-06-25T23:54:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Delete Mapping API should wait for deletion on all nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2054</link><project id="" key="" /><description>As discussed in the mailing list:

&gt; The delete mapping API does not wait currently until the mapping has been deleted on all the other nodes (while others do, btw). I will fix it...

This issue is relative to https://github.com/dadoonet/spring-elasticsearch/issues/13
</description><key id="5258300">2054</key><summary>Delete Mapping API should wait for deletion on all nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-06-25T20:22:24Z</created><updated>2012-06-28T05:43:07Z</updated><resolved>2012-06-28T05:43:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="iskytek" created="2012-06-27T11:35:08Z" id="6600969">very good.
When fix ? Which version fix ? 
</comment><comment author="kimchy" created="2012-06-27T20:00:40Z" id="6613901">Already fixed in 0.19.7. Check it out?
</comment><comment author="dadoonet" created="2012-06-27T20:12:23Z" id="6614185">@kimchy Thanks Shay. I can not find the 0.19.7 version in sonatype repo : https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch/

Did I miss something ?
</comment><comment author="kimchy" created="2012-06-27T23:00:03Z" id="6617883">@dadoonet should be there now...
</comment><comment author="dadoonet" created="2012-06-27T23:24:00Z" id="6618245">Thanks. I can get it now. I'm just waiting to @iskytek to confirm that it works as expected now (I can't reproduce the issue myself). I will close this issue after its answer.
</comment><comment author="iskytek" created="2012-06-28T01:49:17Z" id="6620115">This is the issue of  spring-elasticsearch  , or elasticsearch issue &#65311;
</comment><comment author="iskytek" created="2012-06-28T02:59:23Z" id="6620870">We have been tested to normal, you can be closed .
thank you .
but, delete mapping, needs to wait for a long time, oh .
</comment><comment author="iskytek" created="2012-06-28T03:04:32Z" id="6620917">Custom similarity calculation, seems to be a issue. Look back I sorted out, and then stick out, let you look at. 

```
 BoolQueryBuilder bq= boolQuery();  
.....
.....

SearchResponse  searchResponse = client.prepareSearch("index").setQuery(bq).addSort("_score", SortOrder.DESC).setFrom(0).setSize(10).setExplain(true).execute().actionGet();

searchResponse.hits().getMaxScore()   and   searchResponse.getHits().getAt(0).explanation().getValue()  not equal  &#12290;   why ?
```
</comment><comment author="dadoonet" created="2012-06-28T05:42:57Z" id="6622298">Please don't use an existing issue to ask for help and follow the guidelines http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>deb package fails to start elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2053</link><project id="" key="" /><description>The ability to use `es.default` prefix broke the deb package that uses it now.
</description><key id="5257801">2053</key><summary>deb package fails to start elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.7</label><label>v0.20.0.RC1</label></labels><created>2012-06-25T19:58:55Z</created><updated>2012-06-25T19:59:35Z</updated><resolved>2012-06-25T19:59:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-25T19:59:35Z" id="6557972">Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"index_analyzer" and "search_analyzer" should override "analyzer"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2052</link><project id="" key="" /><description>Given the following settings for a field:

```
{"index_analyzer": "foo", "analyzer": "bar"}
```

Both the index analyzer and search analyzer will be set to "bar". Better behavior would be for the more-specific settings to override the more-general one.
</description><key id="5257440">2052</key><summary>"index_analyzer" and "search_analyzer" should override "analyzer"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels><label>enhancement</label><label>v0.19.7</label><label>v0.20.0.RC1</label></labels><created>2012-06-25T19:39:49Z</created><updated>2012-06-25T20:05:39Z</updated><resolved>2012-06-25T20:05:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>elasticsearch fails to start due to verification error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2051</link><project id="" key="" /><description>Nasty one, does not happen to all, and basically require renaming some files to loading them will work properly....
</description><key id="5250638">2051</key><summary>elasticsearch fails to start due to verification error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.6</label><label>v0.20.0.RC1</label></labels><created>2012-06-25T14:56:55Z</created><updated>2012-06-25T15:00:08Z</updated><resolved>2012-06-25T15:00:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Request - Published hashes of elasticsearch download artifacts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2050</link><project id="" key="" /><description>Hi,

I'm writing an elasticsearch juju charm (see https://juju.ubuntu.com).  During the charm review process I was asked if there were published hashes for the elasticsearch tarball.  I was not able to find anything like that in the download area.  Specifically the reviewer said:

"charms that are part of the charmstore need to cryptographically
verify downloads.  Can you please look to see if there're published
hashes of the elasticsearch tarball that can be used?"

Would it be possible to provide that ?  It seems its a pre-requisite for charms that can be published in the juju charm store.

Another nice to have would be a way to download the latest version without knowing what the version number is, maybe with a symlinked /latest in the url. Otherwise it becomes quite a chore to update the default version number in the charm given the rapid release cycles practiced here.

Thanks!
Luis 
</description><key id="5250414">2050</key><summary>Request - Published hashes of elasticsearch download artifacts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">kaaloo</reporter><labels /><created>2012-06-25T14:47:14Z</created><updated>2013-05-27T18:49:11Z</updated><resolved>2013-05-27T18:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-25T15:53:28Z" id="6551195">Agreed, hashes would be nice. There isn't a way to associate it with a github download, in what form do you want them? For homebrew, we simply compute them ourself when publishing the artifact.

The symlink thingy does not seem to be possible with github pages, but need to look into it more...
</comment><comment author="spinscale" created="2013-05-27T16:28:46Z" id="18506146">@kaaloo we are providing SHA1 hashes in the meantime. Is this sufficient for you?

if so, please close this issue, otherwise feel free to tell us, what else is needed and we see what we can do.
</comment><comment author="kaaloo" created="2013-05-27T18:49:11Z" id="18510687">Thank you for your input on this longstanding issue.  I am no longer involved in development of the elasticsearch juju charm but I have updated the Launchpad issue with this information so the juju charms team is aware of this update.

https://bugs.launchpad.net/charms/+bug/822979

Thanks again,
Luis
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store Compression: Term Vector Vector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2049</link><project id="" key="" /><description>Allow to compress the main term vector file. The config is: `index.store.compress.tv` and defaults to `false` (can be set to `true`).
</description><key id="5232067">2049</key><summary>Store Compression: Term Vector Vector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-23T21:10:39Z</created><updated>2012-06-23T21:11:10Z</updated><resolved>2012-06-23T21:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Return 503 when threadpool limit is reached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2048</link><project id="" key="" /><description>See https://groups.google.com/d/msg/elasticsearch/fxxJG6iSVrM/3fynSV7xPyYJ
</description><key id="5222882">2048</key><summary>Return 503 when threadpool limit is reached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-22T20:04:56Z</created><updated>2012-06-23T15:26:59Z</updated><resolved>2012-06-23T15:26:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rest API: Add `HEAD` support for `/{index}/{type}/{id}` to quickly check if doc exists or not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2047</link><project id="" key="" /><description /><key id="5220004">2047</key><summary>Rest API: Add `HEAD` support for `/{index}/{type}/{id}` to quickly check if doc exists or not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-22T17:32:20Z</created><updated>2012-06-22T17:32:45Z</updated><resolved>2012-06-22T17:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Failure to recover properly on node(s) restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2046</link><project id="" key="" /><description>When a node restarts, it might be canceling one recovery of a shard id only to get another one in the next cycle. We should detect this case and handle it properly.

This is a fix to the annoying message seen by users: suspect illegal state: trying to move shard from primary mode to replica mode.
</description><key id="5217875">2046</key><summary>Failure to recover properly on node(s) restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-22T15:46:22Z</created><updated>2012-06-23T13:57:41Z</updated><resolved>2012-06-23T13:57:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-23T13:57:41Z" id="6525150">Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST: HEAD should work for documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2045</link><project id="" key="" /><description>Currently, `HEAD` on documents (/index/type/id) returns a 400 Bad Request: No handler found for uri [/index/type/1] and method [HEAD]

HEAD works fine on / and /index, and it would be more consistent if it also worked on documents.
</description><key id="5208264">2045</key><summary>REST: HEAD should work for documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chendo</reporter><labels /><created>2012-06-22T04:23:59Z</created><updated>2012-06-22T18:13:12Z</updated><resolved>2012-06-22T18:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-22T18:13:12Z" id="6514606">Fixed in #2047.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: Be able to specify logger.yml path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2044</link><project id="" key="" /><description>I'd like to be able to specify the `logger.yml` path when booting up elasticsearch, similar to the `-Des.config=&lt;path&gt;` option.

Use case: booting up a ES for testing purposes, want to be able to specify a custom logger.yml without having to edit the system-wide one.
</description><key id="5207385">2044</key><summary>Feature request: Be able to specify logger.yml path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">chendo</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2012-06-22T02:33:08Z</created><updated>2015-06-26T09:19:58Z</updated><resolved>2015-06-26T09:19:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="neowulf33" created="2014-08-22T07:43:42Z" id="53032290">Adding a pull request: [https://github.com/elasticsearch/elasticsearch/pull/7395]
</comment><comment author="javanna" created="2015-06-26T09:19:56Z" id="115600901">We've had a long discussion about this in our FixItFriday session. Our feeling is that we're trying to remove complication from the code base. Having one way to do things just makes it simpler to understand and debug. So for now, we're going to close this issue. We may revisit this in the future if there is popular demand and the change makes life easier for the majority of users.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>{"error":"InvalidTypeNameException[mapping type name [_bulk] can't start with '_']","status":400}</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2043</link><project id="" key="" /><description>Hi,
I installed Elastic search and tire on my development box and also on our staging box. 
Indexing/import works fine on my development machine. I have the same setup on the Staging machine + jenkins and other things. When I try to index data either from ActiveRecord or from a plain array I get
{"error":"InvalidTypeNameException[mapping type name [_bulk] can't start with '_']","status":400}
I also tried to do it from curl
curl -s -XPOST http://localhost:9200/articles/_bulk

Any ideas what could be the problem?

thanks!
mina
</description><key id="5207070">2043</key><summary>{"error":"InvalidTypeNameException[mapping type name [_bulk] can't start with '_']","status":400}</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdoroudi</reporter><labels /><created>2012-06-22T01:48:22Z</created><updated>2012-06-23T15:58:08Z</updated><resolved>2012-06-22T17:59:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-22T18:20:43Z" id="6514797">Does it work then? I tested it again and seems to work...
</comment><comment author="mdoroudi" created="2012-06-22T18:41:19Z" id="6515257">It doesn't work, I closed this and posted it in Tire, since it seams to be a Tire problem. 
Actually Curl works fine, but since I get InvalidTypeNameException and that's an elasticsearch error, I thought it could be elastic search problem, but now i"m pretty sure it's Tire.
</comment><comment author="kimchy" created="2012-06-23T15:58:08Z" id="6525920">cool, thanks for the update.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve cluster resiliency to disconnected sub clusters + fix a shard allocation bug with quick rolling restarts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2042</link><project id="" key="" /><description>Two main changes:
1. Improve cluster resiliency to disconnected sub clusters. If a node pings a master and that node is no longer registered with the master, improve the rejoin process of that node to the cluster. Also, if a master receives a message from another master, pick one to force to rejoin the cluster (based on cluster state versioning).
2. On quick rolling restart, without waiting for shard allocation, the shard allocation logic can mess up its counts, causing for strange logic in allocating shards, or validation failures on routing table allocation.
</description><key id="5206978">2042</key><summary>Improve cluster resiliency to disconnected sub clusters + fix a shard allocation bug with quick rolling restarts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-22T01:36:24Z</created><updated>2012-06-22T10:32:51Z</updated><resolved>2012-06-22T01:38:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-22T01:38:28Z" id="6498844">Fixed by #cc3fab45ffcc6d8208a35bcdc1bb9d8f7f7da7d8.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store Throttling (node level and/or index level) with options on merge or all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2041</link><project id="" key="" /><description>Allow to configure store throttling (only applied on file system based storage), which allows to control the maximum bytes per sec written to the file system. It can be configured to only apply while merging, or on all output operations. The setting can eb set on the node level (in which case the throttling is done _across_ all shards allocated on the node), or index level, in which case it only applied to that index.

The node level settings are `indices.store.throttle.type` to set the type, with values of `none`, `merge` and `all` (defaults to `none`). And, also, `indices.store.throttle.max_bytes_per_sec` (defaults to `0`), which can be set to something like `1mb`.

The index level settings is `index.store.throttle.type` for the type, with values of `node`, `none`, `merge`, and `all`. Defaults to `node` which will use the "shared" throttling on the node level. And, `index.store.throttle.max_bytes_per_sec` (defaults to `0`).
</description><key id="5201388">2041</key><summary>Store Throttling (node level and/or index level) with options on merge or all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-21T19:19:53Z</created><updated>2012-06-21T19:20:30Z</updated><resolved>2012-06-21T19:20:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Limit faceting to top N hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2040</link><project id="" key="" /><description>See
https://groups.google.com/d/msg/elasticsearch/OZsc5ofNhag/9JfwSy-CEMwJ where Shay says "...with the design of the facets, if shouldn't be hard to implement..."
</description><key id="5200801">2040</key><summary>Limit faceting to top N hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">otisg</reporter><labels><label>feature</label></labels><created>2012-06-21T18:50:04Z</created><updated>2015-05-12T14:29:04Z</updated><resolved>2015-05-12T14:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2012-06-29T14:04:47Z" id="6660281">+1, I also need this feature

Can anyone point me to the class(es) or way to implement this? Thx
</comment><comment author="jimdickinson" created="2012-09-20T15:20:08Z" id="8732675">+1

This feature would be really useful to me.
</comment><comment author="otisg" created="2012-09-20T22:02:05Z" id="8747260">+1
Just spoke to a company using ES and they, too, would like this.
</comment><comment author="ferdynice" created="2012-10-17T14:32:30Z" id="9529389">This would be very useful!
</comment><comment author="loachli" created="2013-01-24T09:06:33Z" id="12642718">+1, Save top k is enough
</comment><comment author="fmaritato" created="2013-11-19T19:21:12Z" id="28823341">I have done some work on this since I need it for a project. I only implemented it for terms facets at the moment until I get some feedback on implementation. Can someone take a look at the diff of my fork and let me know if this is on the right track? https://github.com/fmaritato/elasticsearch/compare/issue2040

Essentially, I added a "num_results" parameter to the terms facet. The unit test I added uses the elasticsearch-test framework since it is easier and faster than the integration tests the project currently uses. I can take it out or switch it over if desired.

Thanks
</comment><comment author="billoneil" created="2014-01-24T18:55:46Z" id="33250386">+1
</comment><comment author="clintongormley" created="2014-07-18T09:35:55Z" id="49412673">Closing in favour of #6876
</comment><comment author="clintongormley" created="2014-07-18T10:00:17Z" id="49414768">Actually, #6876 doesn't solve this.  Please could I have some info about use cases here.  Why do you need this feature?
</comment><comment author="fmaritato" created="2014-07-21T22:14:26Z" id="49673662">The project I was working on was indexing shopping products from different providers who had inventory. I found when I was doing searches but only returning the first 25 items that the facets returning the brand names and counts reflected the entire search instead of what was being shown on the page. This often lead to a confusing user experience.
</comment><comment author="clintongormley" created="2014-07-22T05:47:49Z" id="49700699">I can see the usefulness of this option. 
</comment><comment author="markharwood" created="2014-07-22T13:03:48Z" id="49735427">Related: I extended the significant_terms agg in https://github.com/elasticsearch/elasticsearch/pull/6796 to add sampling of top results from each shard. For that use case it was also beneficial to eliminate near-duplicates from the analysis because they tend to rank similarly in the results. Using elements of this implementation and the new "deferred" capability in the aggregations base class we could add a sampling capability that would apply to all child aggregations in a tree.  
</comment><comment author="markharwood" created="2015-05-12T14:29:03Z" id="101300993">Closed in favour of https://github.com/elastic/elasticsearch/issues/8108 which is now in master branch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JSON parse error giving nonsensical offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2039</link><project id="" key="" /><description>In indexing with generated JSON using elasticsearch 0.19.4, I get responses of the type:

{
  "error" : "ElasticSearchParseException[failed to parse source for create index]; nested: JsonParseException[Unrecognized character escape '-' (code 45)\n at [Source: [B@78d59861; line: 1, column: 854182]]; ",
  "status" : 400
}

If I assume that the source is the JSON I generated (and lines are counted by \r instead of \n), this still makes no sense, since that spurious character does not appear anywhere near that character position. If the source is not the JSON passed to the request, the error message is useless.
</description><key id="5197806">2039</key><summary>JSON parse error giving nonsensical offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">enkiv2</reporter><labels /><created>2012-06-21T16:37:16Z</created><updated>2013-05-27T16:33:31Z</updated><resolved>2013-05-27T16:33:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-22T18:16:14Z" id="6514675">What do you mean? It would be nice to get a better error message that also outputs the actual json (its coming from the json library we use). Can you gist a sample with the failure so I can check?
</comment><comment author="enkiv2" created="2012-06-24T01:39:12Z" id="6529755">The bug report is about the error message reporting facility. It isn't useful in debugging my JSON generator, because none of the offsets are meaningful.
</comment><comment author="kimchy" created="2012-06-24T10:41:35Z" id="6531945">can you gist an example of where you use a json and the offset is not correct?
</comment><comment author="spinscale" created="2013-05-27T16:33:31Z" id="18506320">Closing this for now, please reopen, if you can provide more information for us. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error starting 0.19.5 / 0.20.0 (snapshot)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2038</link><project id="" key="" /><description>Using java:

```
java version "1.6.0_24"
OpenJDK Runtime Environment (IcedTea6 1.11.1) (suse-3.1-x86_64)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
```

I get:

```
./bin/elasticsearch -f
[2012-06-21 11:44:39,507][INFO ][node                     ] [Tyrant] {0.19.5-SNAPSHOT}[13012]: initializing ...
[2012-06-21 11:44:39,511][INFO ][plugins                  ] [Tyrant] loaded [], sites []
[2012-06-21 11:44:40,043][ERROR][bootstrap                ] {0.19.5-SNAPSHOT}: Initialization Failed ...
- ComputationException[java.lang.VerifyError: (class: org/elasticsearch/search/highlight/HighlightPhase, method: hitExecute signature: (Lorg/elasticsearch/search/internal/SearchContext;Lorg/elasticsearch/search/fetch/FetchSubPhase$HitContext;)V) Incompatible object argument for function call]
    VerifyError[(class: org/elasticsearch/search/highlight/HighlightPhase, method: hitExecute signature: (Lorg/elasticsearch/search/internal/SearchContext;Lorg/elasticsearch/search/fetch/FetchSubPhase$HitContext;)V) Incompatible object argument for function call]
```
</description><key id="5189350">2038</key><summary>Error starting 0.19.5 / 0.20.0 (snapshot)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-06-21T09:46:08Z</created><updated>2012-06-25T18:48:11Z</updated><resolved>2012-06-25T18:48:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rcracel" created="2012-06-25T13:46:12Z" id="6547619">I am having the same issue trying to run on:

java version "1.6.0_22"
OpenJDK Runtime Environment (IcedTea6 1.10.6) (amazon-xxxx.amzn1-x86_64)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)

Is this a compatibility issue with OpenJDK?
</comment><comment author="clintongormley" created="2012-06-25T13:52:52Z" id="6547769">No, it also does it on my system with oracle's java 7.  What OS are you using?
</comment><comment author="rcracel" created="2012-06-25T13:58:23Z" id="6547905">It works just fine on my local computer, running :

--- Mac OSX
java version "1.7.0_04"
Java(TM) SE Runtime Environment (build 1.7.0_04-b21)
Java HotSpot(TM) 64-Bit Server VM (build 23.0-b21, mixed mode)

but I haven't been able to start it on EC2 running OpenJDK (the Amazon Linux dist). I am in the process of installing the Oracle JDK and see if that solves the problem, but I am not very confident after your reply.
</comment><comment author="clintongormley" created="2012-06-25T14:01:53Z" id="6547996">which amazon linux dist? ubuntu? opensuse? fedora? 
</comment><comment author="rcracel" created="2012-06-25T14:25:34Z" id="6548601">Amazon Linux AMI release 2012.03
</comment><comment author="mhorbul" created="2012-06-25T17:38:49Z" id="6554261">I have got the same error on sun java

mhorbul@b22:~$ cat /etc/redhat-release 
Red Hat Enterprise Linux Server release 5.6 (Tikanga)

mhorbul@b22:~$ java -versionjava version "1.6.0_25"
Java(TM) SE Runtime Environment (build 1.6.0_25-b06)
Java HotSpot(TM) 64-Bit Server VM (build 20.0-b11, mixed mode)
</comment><comment author="clintongormley" created="2012-06-25T17:45:03Z" id="6554459">Should be fixed in 0.19.6. @mhorbul are you on 0.19.6 or .5 ?
</comment><comment author="mhorbul" created="2012-06-25T17:53:01Z" id="6554660">.5 downloaded from https://dl.dropbox.com/u/2136051/elasticsearch-0.19.5-SNAPSHOT.zip (https://gist.github.com/2970009/)
</comment><comment author="clintongormley" created="2012-06-25T18:15:28Z" id="6555287">Right, so download http://www.elasticsearch.org/download/2012/06/25/0.19.6.html
</comment><comment author="mhorbul" created="2012-06-25T18:18:20Z" id="6555374">got it already... thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stored Fields Compression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2037</link><project id="" key="" /><description>Compressing the stored fields file (the .fdt file) directly allows to have better compression on the size of the index, specifically when indexing (and storing) small documents. The compression will be considerably more effective compared to compressing each doc on its own (when setting compress on the _source mapper). The downside is that more data needs to be uncompressed when loading documents.

The settings to control it is `index.store.compress.stored` set to `true` (it defaults to `false`), and can be enabled dynamically using the update settings API. This allows to enabled compression at a later stage (i.e. old time based indices), and then optimize the index to make sure it gets compressed.
</description><key id="5162136">2037</key><summary>Stored Fields Compression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-20T03:30:49Z</created><updated>2012-06-22T18:07:34Z</updated><resolved>2012-06-20T03:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Failiure to handel twiter outage in river.twitter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2036</link><project id="" key="" /><description>We are presently having an issue with code introduced in #735, the restart fails if the twitter api is uncontactable for any reason:

---

[2012-06-19 09:51:45,697][WARN ][river.twitter            ] [Orka] [twitter][river_name] stream failure, restarting stream...
Stream closed.Relevant discussions can be on the Internet at:
        http://www.google.co.jp/search?q=70971d2e or
        http://www.google.co.jp/search?q=000687bf
TwitterException{exceptionCode=[70971d2e-000687bf 70971d2e-0006876e], statusCode=-1, retryAfter=-1, rateLimitStatus=null, featureSpecificRateLimitStatus=null, version=2.2.5}
        at twitter4j.AbstractStreamImplementation.handleNextElement(AbstractStreamImplementation.java:167)
        at twitter4j.StatusStreamImpl.next(StatusStreamImpl.java:67)
        at twitter4j.TwitterStreamImpl$TwitterStreamConsumer.run(TwitterStreamImpl.java:443)
Caused by: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
        at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:331)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:830)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:787)
        at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        at sun.net.www.http.ChunkedInputStream.readAheadBlocking(ChunkedInputStream.java:525)
        at sun.net.www.http.ChunkedInputStream.readAhead(ChunkedInputStream.java:582)
        at sun.net.www.http.ChunkedInputStream.read(ChunkedInputStream.java:669)
        at java.io.FilterInputStream.read(FilterInputStream.java:116)
        at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:2672)
        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:264)
        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:306)
        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:158)
        at java.io.InputStreamReader.read(InputStreamReader.java:167)
        at java.io.BufferedReader.fill(BufferedReader.java:136)
        at java.io.BufferedReader.readLine(BufferedReader.java:299)
        at java.io.BufferedReader.readLine(BufferedReader.java:362)
        at twitter4j.AbstractStreamImplementation.handleNextElement(AbstractStreamImplementation.java:86)
##         ... 2 more

This is reliably recreate able by implementing local firewall rules to block access to an already established strem (i.e. using iptables), eventually you will get a timeout and an error as above when stream is attempted to be restarted and a connection timeout occurs.

In this case I believe adding an equivalent try catch encompassing lines 431 -&gt; 436 in the commit, to handel reconnection failure would resolve this issue. 
</description><key id="5143520">2036</key><summary>Failiure to handel twiter outage in river.twitter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Oneiroi</reporter><labels /><created>2012-06-19T10:36:26Z</created><updated>2012-06-19T12:28:27Z</updated><resolved>2012-06-19T11:49:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Oneiroi" created="2012-06-19T11:49:27Z" id="6422634">Applies to river plugin, closing and re-raised here: https://github.com/elasticsearch/elasticsearch-river-twitter/issues/14
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> elasticsearch  score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2035</link><project id="" key="" /><description>score: 108.0   --- &gt;  hit.getScore()
168.0 = sum of:
  6.0 = weight(appPartner:appPartner_0 in 9433), product of:
    6.0 = queryWeight(appPartner:appPartner_0), product of:
      1.0 = idf(docFreq=39044, maxDocs=390473)
      6.0 = queryNorm
    1.0 = fieldWeight(appPartner:appPartner_0 in 9433), product of:
      1.0 = tf(termFreq(appPartner:appPartner_0)=0)
      1.0 = idf(docFreq=39044, maxDocs=390473)
      1.0 = fieldNorm(field=appPartner, doc=9433)
  6.0 = weight(appName:appName_4 in 9433), product of:
    6.0 = queryWeight(appName:appName_4), product of:
      1.0 = idf(docFreq=38590, maxDocs=390473)
      6.0 = queryNorm

score: 108.0   not equal   168.0 = sum of: &#65311;  why &#65311;

please tell me &#65281;

thank you&#12290;

my email   &#65306;iskytek@gmail.com
</description><key id="5141990">2035</key><summary> elasticsearch  score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iskytek</reporter><labels /><created>2012-06-19T08:54:21Z</created><updated>2013-11-08T17:59:00Z</updated><resolved>2012-06-28T03:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.index.mapper.MapperParsingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2034</link><project id="" key="" /><description>try to use jdbc river, but from logs I got a lot exceptions repeated:
org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [geoLong]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:325)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:585)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:449)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:437)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:311)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:156)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [100.516667], tried both date format [dateOptionalTime], and timestamp number        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:412)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseCreateField(DateFieldMapper.java:341)        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:312)
        ... 11 moreCaused by: java.lang.IllegalArgumentException: Invalid format: "100.516667" is malformed at ".516667"
        at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:644)        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:406)
        ... 13 more

while actually geolong is a simple nvarchar(50) type, an example value is geoLong":"100.516667", what's the problem? thanks
</description><key id="5141342">2034</key><summary>org.elasticsearch.index.mapper.MapperParsingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">BlueStalker</reporter><labels /><created>2012-06-19T08:08:26Z</created><updated>2013-07-15T16:21:20Z</updated><resolved>2013-07-15T16:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BlueStalker" created="2012-06-19T08:11:37Z" id="6418876">Also, I found some logs mentioned like: caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [-71.5314], tried both date format [dateOptionalTime], and timestamp number
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:412)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseCreateField(DateFieldMapper.java:341)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:312)
        ... 11 more

It's strange for me that geoLong is just a simple  nvarchar(50) , why it says trying "failed to parse date field [-71.5314], tried both date format [dateOptionalTime], and timestamp number"?
</comment><comment author="spinscale" created="2013-06-07T15:32:14Z" id="19114286">It looks, that you mapped a field as a date, and tried to index something different into it. I guess this is rather a configuration issue (with the river), than an elasticsearch bug.

if you think otherwise, please attach a gist containing all the commands to reproduce your issue.

Thanks!
</comment><comment author="spinscale" created="2013-07-15T16:21:20Z" id="20981223">closing this one due to missing information in order to work further on this. If you provide further information, we are glad to take another look at it. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better fix for mv field highlighting issue #1994</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2033</link><project id="" key="" /><description /><key id="5133396">2033</key><summary>Better fix for mv field highlighting issue #1994</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-06-18T20:48:30Z</created><updated>2014-07-16T21:55:13Z</updated><resolved>2012-06-19T02:14:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-19T02:14:48Z" id="6414431">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Another fix for mv field highlighting issue #1994</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2032</link><project id="" key="" /><description /><key id="5132794">2032</key><summary>Another fix for mv field highlighting issue #1994</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-06-18T20:21:05Z</created><updated>2014-06-14T15:07:14Z</updated><resolved>2012-06-18T20:24:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Better fix for mv field highlighting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2031</link><project id="" key="" /><description /><key id="5130636">2031</key><summary>Better fix for mv field highlighting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-06-18T18:38:13Z</created><updated>2014-07-16T21:55:14Z</updated><resolved>2012-06-18T18:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>update scrolling docs to show difference between scanning/non-scanning scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2030</link><project id="" key="" /><description>Scrolling with scan and without scan have different initial responses, which can be a little confusing. Could the scrolling documentation be updated to explicitly point out how scanning differs from non-scanning scroll?
</description><key id="5129073">2030</key><summary>update scrolling docs to show difference between scanning/non-scanning scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2012-06-18T17:22:25Z</created><updated>2012-06-18T17:22:37Z</updated><resolved>2012-06-18T17:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>update scrolling docs to show difference between scanning/non-scanning scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2029</link><project id="" key="" /><description>Scrolling with scan and without scan have different initial responses, which can be a little confusing. Could the scrolling documentation be updated to explicitly point out how scanning differs from non-scanning scroll?
</description><key id="5129072">2029</key><summary>update scrolling docs to show difference between scanning/non-scanning scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2012-06-18T17:22:24Z</created><updated>2014-07-08T15:15:50Z</updated><resolved>2014-07-08T15:15:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:15:50Z" id="48351765">Done in feb81e228b49c05d5eb8f1ce4bc7fc12edc3eaf7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scan scrolling should return results in the initial response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2028</link><project id="" key="" /><description>Scrolling with the search_type scan have a different initial response compared with scrolling without scan. Scans do not return documents in the initial response, but regular scrolling does. This can be a bit confusing for end users. According to @kimchy, this difference is no longer required by the implementation, but is maintained for backwards compatibility. It would be nice if at some point in the future we could do away with this difference.
</description><key id="5129044">2028</key><summary>Scan scrolling should return results in the initial response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">erickt</reporter><labels><label>adoptme</label><label>breaking</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2012-06-18T17:20:27Z</created><updated>2015-08-24T10:44:40Z</updated><resolved>2015-08-19T17:55:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kul" created="2015-01-22T07:18:36Z" id="70980709">:+1:  #9380   #6294
</comment><comment author="jasontedor" created="2015-08-19T17:54:55Z" id="132720645">The scan search type will be deprecated by #12994 in favor of a scroll search ordered by `_doc` (optimized in #12983). This deprecation obviates the need to address this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Catch LinkageError when executing scripts (most likely recoverable).</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2027</link><project id="" key="" /><description>Sometimes we experience LinkageErrors in 3rd party libraries (mvel) when executing scripts. LinkageErrors (in scripts) are most likely recoverable and could be handled by wrapped into a RuntimeException (and printed as an error in the 'failures' property) instead of just being unhandled and causing the thread to terminate without responding or closing the connection properly.
</description><key id="5126960">2027</key><summary>Catch LinkageError when executing scripts (most likely recoverable).</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lindstromhenrik</reporter><labels><label>discuss</label></labels><created>2012-06-18T15:49:08Z</created><updated>2014-07-18T09:32:52Z</updated><resolved>2014-07-18T09:32:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-16T07:49:48Z" id="21026410">hey, can one create such a linkage error on purpose (i'd like to test this) - except manually throwing this? :)
</comment><comment author="lindstromhenrik" created="2013-07-16T12:15:54Z" id="21037957">We experienced this 'sometimes' when passing scripts in the request using mvel during high loads so I don't have a script example that cause this error all the time. We have stopped passing scripts in the request since  0.19.4 as it caused to many errors with clients hanging and waiting for requests to timeout. However in my opinion I think that maybe all errors thrown in scripts should be caught somewhere and that the server should respond appropriately and terminate the request in a nice way (as for LinkageError it is definitely recoverable from).
</comment><comment author="clintongormley" created="2014-07-18T09:32:52Z" id="49412425">Mvel is in the process of being removed.  No longer an issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test that verifies behavior of GeoDistance.ARC and GeoDistance.PLANE distance calculation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2026</link><project id="" key="" /><description>Included test which verifies the differences in the distance calculations of GeoDistance.ARC and GeoDistance.PLANE, showing why this could cause elliptical results.  Some small tidy up of the test class too.
</description><key id="5092344">2026</key><summary>Test that verifies behavior of GeoDistance.ARC and GeoDistance.PLANE distance calculation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-06-15T13:40:11Z</created><updated>2014-07-16T21:55:15Z</updated><resolved>2012-06-15T21:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-15T21:00:10Z" id="6366488">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add per index path.data setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2025</link><project id="" key="" /><description>It would be great to be able to store data files for different indices in different paths. My use case is that I have servers with both fast SSDs and slow HDDs. I'd like to have one of my indices stored on the SSD while the others would be stored on the HDD.

It would also be cool if this setting could be changed at runtime. Then I could move a "hot" index to the SSD and swap out less hotter ones using cron. We use rolling indices based on date and the two most recent ones are the targets of almost all search/index requests.
</description><key id="5088009">2025</key><summary>Add per index path.data setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ntherning</reporter><labels><label>discuss</label></labels><created>2012-06-15T08:17:35Z</created><updated>2014-07-18T09:32:12Z</updated><resolved>2014-07-18T09:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T10:09:48Z" id="20510485">You could solve this by using multiple nodes on one host and shard allocation, see http://www.elasticsearch.org/guide/reference/modules/cluster/
</comment><comment author="xinuc" created="2014-06-13T12:57:08Z" id="46007669">:+1: this

I think this is a pretty common use case.
Some indices need the speed of ssd, while the others need the capacity.
</comment><comment author="clintongormley" created="2014-07-18T09:32:12Z" id="49412376">This gets really complicated with the disk aware shard allocation decider. Best solution is to use shard allocation filtering to assign busy indices to stronger boxes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplified improvements to normalization of point latitude and longitudes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2024</link><project id="" key="" /><description>As shown in https://github.com/elasticsearch/elasticsearch/pull/1998/ there needs to be some improvements to the normalization of point latitude and longitudes so that both are normalized together.  However I'm not sure about the normalization of the bounding boxes.  It introduces a lot of range queries when really the bounding box should have normalized points coming into query.

Some further testing is included and some improved documentation.
</description><key id="5062407">2024</key><summary>Simplified improvements to normalization of point latitude and longitudes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrismale</reporter><labels /><created>2012-06-14T04:20:50Z</created><updated>2014-07-16T21:55:15Z</updated><resolved>2012-06-14T13:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Multi Search API: Allow to set search_type on REST endpoint URI to apply to all search requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2023</link><project id="" key="" /><description /><key id="5054383">2023</key><summary>Multi Search API: Allow to set search_type on REST endpoint URI to apply to all search requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-13T18:46:56Z</created><updated>2012-06-13T18:47:30Z</updated><resolved>2012-06-13T18:47:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>TR locale problem on Rest </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2022</link><project id="" key="" /><description>Enum -&gt; toString -&gt; toLowerCase used for some Rest Responses. They fail when containing "I" (like OpType.INDEX). TR locale servers convert these enums to "&#305;ndex" instead of "index". (I discover this behavior when a TR locale server joined my cluster)
</description><key id="5045313">2022</key><summary>TR locale problem on Rest </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">frail</reporter><labels /><created>2012-06-13T11:15:10Z</created><updated>2014-07-16T21:55:15Z</updated><resolved>2012-06-13T14:24:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-13T14:24:27Z" id="6302416">it needs also imports to java.util.Locale, I will fix it and push.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Partial update without using a script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2021</link><project id="" key="" /><description>Basic support for performing partial updates from the specified "doc" vs. needing to use a script.  Currently only supports adding new fields or changing existing fields (no deletes).

See comments in #2008 for more information.
</description><key id="5033352">2021</key><summary>Partial update without using a script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2012-06-12T20:59:51Z</created><updated>2014-06-13T05:55:58Z</updated><resolved>2012-06-27T19:51:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-12T21:49:31Z" id="6284121">Heya, generally, looks good!, some notes:
1. We can use the index request ttl values (and others).
2. The update process should be recursive, not only top level properties (which override it). Check XContelHelper#mergeDefaults method, we should have an "update" method there that works in a similar manner.
3. Update the javadoc on the UpdateRequest and UpdateRequestBuilder
4. Add tests for it under (UpdateTests)

Also, it would be great if this pull request would come in a single squashed commit.
</comment><comment author="mattweber" created="2012-06-13T02:48:03Z" id="6291811">Thanks Shay, I have addressed all your items and added some tests.  Let me know if there is anything else that should be done. 
</comment><comment author="mattweber" created="2012-06-13T16:01:26Z" id="6305174">I see that doc was renamed to upsert.  I will update this patch.
</comment><comment author="mattweber" created="2012-06-13T16:29:16Z" id="6305946">Updated to reflect the rename from "doc" to "upsert".
</comment><comment author="kimchy" created="2012-06-15T21:06:12Z" id="6366619">@mattweber I changed it to upsert so we can have an "update" index request as well. You might want to do an upsert and the update doc will be done using the "override" document.

Also, the update part should basically not do any merge, just take the doc source provided, and override fields in a recursive (I don't think we need a flag for that). i.e., if the key exists on both, and its a Map, go in a recursive manner, if it exists in the update doc, override.
</comment><comment author="mattweber" created="2012-06-16T21:24:41Z" id="6376565">OK thanks Shay.  I will refactor things to use "doc" as the update source when a script is not specified vs. using the upsert doc.  All fields will be updated (no merging) and Maps will be updated recursively.  Since I won't be using upset anymore, I won't be using an IndexRequest and can pass the doc directly into the request.  Properties such as ttl, timestamp, etc will come from the original doc or the upsert.
</comment><comment author="mattweber" created="2012-06-17T03:01:00Z" id="6378444">Ok, updated to use "doc" as the update source vs the upsert.  Updates are done recursively and tests pass.
</comment><comment author="kimchy" created="2012-06-27T15:25:03Z" id="6606445">I would say that having the "doc" as another `IndexRequest` would make more sense, since then you can set other aspects to it like ttl and so on.
</comment><comment author="mattweber" created="2012-06-27T17:17:35Z" id="6609506">I changed "doc" to be an IndexRequest.  How does this look?
</comment><comment author="kimchy" created="2012-06-27T19:51:09Z" id="6613704">Pushed, thanks!. @mattweber can you open a separate issue with a REST example and an explanation of how to use it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix bug index.ttl.disable_purge should be in IndexMetaData</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2020</link><project id="" key="" /><description /><key id="5029971">2020</key><summary>fix bug index.ttl.disable_purge should be in IndexMetaData</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-12T18:17:17Z</created><updated>2014-06-13T09:11:34Z</updated><resolved>2012-06-12T21:53:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-12T21:53:31Z" id="6284229">Pushed to 0.19 branch and master, thanks!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: Join process to better validate join request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2019</link><project id="" key="" /><description>The join proces received on the master from a node joining the cluster now tries to validate by connecting to the relevant node. It should also send a message to it to double check that all is in order.
</description><key id="5012919">2019</key><summary>Discovery: Join process to better validate join request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-06-11T21:43:59Z</created><updated>2012-06-11T21:44:34Z</updated><resolved>2012-06-11T21:44:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fixes highlight issue for multivalues fields described in issue #1994</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2018</link><project id="" key="" /><description /><key id="5012039">2018</key><summary>Fixes highlight issue for multivalues fields described in issue #1994</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-06-11T21:00:07Z</created><updated>2014-07-16T21:55:16Z</updated><resolved>2012-06-11T22:27:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-11T22:27:29Z" id="6257920">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>.DS_Store file in .deb package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2017</link><project id="" key="" /><description>There is a .DS_Store file in the .deb package.
It's in /etc/elasticsearch
</description><key id="5001892">2017</key><summary>.DS_Store file in .deb package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierrre</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-11T12:07:52Z</created><updated>2012-06-11T19:06:59Z</updated><resolved>2012-06-11T19:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow to pass es.default. settings to the process, using it as default value unless specified in the config file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2016</link><project id="" key="" /><description /><key id="5001360">2016</key><summary>Allow to pass es.default. settings to the process, using it as default value unless specified in the config file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-11T11:30:41Z</created><updated>2012-06-11T11:31:02Z</updated><resolved>2012-06-11T11:31:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.19</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2015</link><project id="" key="" /><description>Hi,

some minor improvements for the .deb validation with lintian.
these 5 commits remove some lintian errors and warning when the package is checked.
more to come.

Thanks
</description><key id="4996393">2015</key><summary>0.19</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aureq</reporter><labels /><created>2012-06-11T03:51:54Z</created><updated>2014-06-15T00:08:49Z</updated><resolved>2012-06-11T10:29:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-11T10:29:50Z" id="6241242">pushed, thanks!
</comment><comment author="aureq" created="2012-06-11T10:33:44Z" id="6241300">Great, thanks. More to come, soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to filter client and "just" data nodes from controlling elected master (default client to true)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2014</link><project id="" key="" /><description>Allow to set `discovery.zen.master_election.filter_client` (defaults to `true`) which will not use client nodes (or non data and non master nodes) current master as part of the master election. And, allow to set `discovery.zen.master_election.filter_data` (defaults to `false`) to filter out just data nodes (data node that is non master).
</description><key id="4980858">2014</key><summary>Allow to filter client and "just" data nodes from controlling elected master (default client to true)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-08T23:27:38Z</created><updated>2012-06-08T23:28:11Z</updated><resolved>2012-06-08T23:28:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.5.0.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2013</link><project id="" key="" /><description /><key id="4978812">2013</key><summary>Upgrade to Netty 3.5.0.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-08T20:56:50Z</created><updated>2012-06-08T21:20:53Z</updated><resolved>2012-06-08T21:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>geo_bounding_box example issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2012</link><project id="" key="" /><description>The example provided on the guide at http://www.elasticsearch.org/guide/reference/query-dsl/geo-bounding-box-filter.html creates a point but the bounding box searches a range outside of where that point lies and therefore the example does not return the result.

Here is an example of how to reproduce the example:
https://gist.github.com/2896858/0d90b9862b3f058e5beb1006d751ec85dd31c8fa

Here is a working example with simple points:
https://gist.github.com/2896858/5cebe0dc1988a46b55c3cf1547adffbb7145e59e

Thank you!
</description><key id="4974571">2012</key><summary>geo_bounding_box example issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukecampbell</reporter><labels /><created>2012-06-08T17:07:14Z</created><updated>2013-11-26T02:24:56Z</updated><resolved>2013-05-23T10:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-23T10:21:11Z" id="18334820">hey, your gists do not exist anymore, but our documentation is wrong there. I will fix the documentation ASAP and close this issue.

Thanks for reporting!
</comment><comment author="Diolor" created="2013-11-26T02:24:56Z" id="29262585">Hi, it looks like the GeoJson format has some issues (or I'm doing something wrong :) ).
Fyi all data below are in lon/lat format, so no classic geojson typo.

```
curl -XGET 'http://localhost:9200/passionable/venue/_search?' -d '
{
    "query" : {
        "filtered" : {
            "query" : {
                "match_all" : {}
            },
            "filter" : {
                "geo_bounding_box" : {
                    "coordinates" : {
                        "top_left" : [31.331, 49.9],
                        "bottom_right" : [0.12, 34.01]
                    }
                }
            }
        }
    }
}'
```

returns documents like:

```
"coordinates":[-123.35857437065354,48.42710605277848]
```

I noticed that "bottom_right" gets totally ignored and "top_left" is used as [min,max] of the latitude field (in the above example: 48.4271060)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better print error message on native script error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2011</link><project id="" key="" /><description>When there's an error on elasticsearch side, in some native script for instance, there is no explanation at all of the error, there is just an empty "nested: ".
I patched ExceptionsHelper so at least a classname is printed, so "nested: " won't be empty.
I patched CustomScoreQueryParser so it can state that the culprit of the error is the load of the script.
</description><key id="4967513">2011</key><summary>Better print error message on native script error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-06-08T09:59:03Z</created><updated>2014-07-16T21:55:17Z</updated><resolved>2012-06-08T11:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-08T11:32:41Z" id="6199661">pushed to master, thanks!
</comment><comment author="kimchy" created="2012-06-08T11:32:47Z" id="6199664">closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhancement: Add INFO level log for gateway recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2010</link><project id="" key="" /><description>Hi,

I had configured my cluster to start initial recovery after five minutes. Some weeks later I wondered, why all my nodes returned a HTTP/503 after startup (forgot that I had configured that initial recovery time... ).

It would be nice to have some INFO level log messages on startup when a node will return a HTTP/503 for some time.
For example, if "gateway.recover_after_time" if configured, print out something like:
"Suspending operation for XX seconds - resume node operation at YY:YY:YY"

Regards,
Robert
</description><key id="4967084">2010</key><summary>Enhancement: Add INFO level log for gateway recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">snazy</reporter><labels /><created>2012-06-08T09:26:47Z</created><updated>2014-04-25T19:44:49Z</updated><resolved>2014-04-25T19:44:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T09:04:34Z" id="27373752">It should be sufficient in your case to only set the loglevel for for the gateway to DEBUG. Does this expose too much information?
</comment><comment author="spinscale" created="2014-04-25T19:44:49Z" id="41432081">closing this, happy to reopen if you face the same issue with a current version of elasticsearch, and it still does not work as expected. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix an incorrect error message when query parse fail, closes #1996</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2009</link><project id="" key="" /><description /><key id="4966681">2009</key><summary>Fix an incorrect error message when query parse fail, closes #1996</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-06-08T08:57:37Z</created><updated>2014-07-16T21:55:17Z</updated><resolved>2012-06-08T11:46:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-08T11:46:24Z" id="6199839">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API: Allow to upsert, provide a doc and index it if the doc does not exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2008</link><project id="" key="" /><description>Here is a sample:

```
curl -XPUT localhost:9200/test

curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "ctx._source.counter += count",
    "params" : {
        "count" : 4
    },
    "upsert" : {
        "counter" : 0
    }
}'

curl -XGET localhost:9200/test/type1/1
```
</description><key id="4962013">2008</key><summary>Update API: Allow to upsert, provide a doc and index it if the doc does not exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-06-07T23:53:30Z</created><updated>2012-11-07T03:16:04Z</updated><resolved>2012-06-08T00:01:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MrJohnsson77" created="2012-06-09T14:01:32Z" id="6220001">Wonderfull, great work! :-)
</comment><comment author="mattweber" created="2012-06-12T00:58:23Z" id="6260543">Would it be possible to use this new "doc" as the document containing the updated fields without needing to use a script?  

Taking a quick look though the code it seems like it might be possible if we check that no "script" is set then pull the updated source back out of the new IndexRequest. I imagine as we need to get it as a map but it looks like we can only get bytes currently.  Once we have the sources we can merge and index.
</comment><comment author="kimchy" created="2012-06-12T08:34:50Z" id="6265521">The problem here is how to "merge" that new doc values into the updated doc? Simple fields can be overridden or appended, but what happens with array values and the like? we can decide that its a simple override and thats it.
</comment><comment author="Paikan" created="2012-06-12T09:41:32Z" id="6266583">I think a simple override would be nice, more complex operations would be done using scripts.
</comment><comment author="mattweber" created="2012-06-12T14:49:19Z" id="6272438">Yea I think a simple override would be fine (just like the partial update plugin).   If someone needs something more advanced they can use a script like Paikan said.
</comment><comment author="JW200" created="2012-11-06T22:48:58Z" id="10131254">It would be nice to have conditional insert so that insert happens only if the record does not exist.
</comment><comment author="imotov" created="2012-11-06T23:04:23Z" id="10131702">Is [Index](http://www.elasticsearch.org/guide/reference/api/index_.html) with `op_type=create` the functionality you are looking for?
</comment><comment author="JW200" created="2012-11-07T03:14:57Z" id="10136785">Yep it is. Thanks
</comment><comment author="JW200" created="2012-11-07T03:16:04Z" id="10136799">Are there any other values that op_type can take besides create?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding REST endpoint for getting a list of installed plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2007</link><project id="" key="" /><description>Cleaned up pull request #1993
</description><key id="4950980">2007</key><summary>adding REST endpoint for getting a list of installed plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-06-07T14:25:26Z</created><updated>2014-06-13T17:03:36Z</updated><resolved>2013-03-29T16:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T21:29:45Z" id="6188576">Thinking about this a bit more, I think it makes sense to add the list of plugins to the nodes info API, they already have "sub sections" in them, I think we can simply add a plugins info to it (with relevant flag to turn it on), what do you think? This will allow to get the list across the cluster on all the different nodes, for example.
</comment><comment author="mobz" created="2012-06-08T23:48:53Z" id="6215403">I'd want to know which nodes have which plugins installed, so makes sense to me
</comment><comment author="jprante" created="2012-06-12T16:06:40Z" id="6274699">Agreed.

If work on the patch is needed, I'm only occasionally online now since I'm on vacation for a few weeks now. At start of July I'm back.
</comment><comment author="otisg" created="2012-06-21T18:50:42Z" id="6491213">+1
</comment><comment author="tonyxiao" created="2013-03-29T16:19:18Z" id="15648236">did this set of changes make it into the distributable? I was reading the docs at `http://www.elasticsearch.org/guide/reference/modules/plugins/` which says that you can run `curl -XGET 'http://localhost:9200/_plugin/?pretty'` to get a list of latest plugins, however when I try that with elastic search version 0.20.2 nothing shows up when I visit the URL. Is this feature enabled?
</comment><comment author="dadoonet" created="2013-03-29T16:29:10Z" id="15648650">@tonyxiao No. Doc will be updated.
See https://github.com/elasticsearch/elasticsearch.github.com/commit/772fcdbf1bf5237c32abf13c4dae10a4d2d576df

I close this PR as #2668 will hold it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Quoted query_string gives NullPointerException with not_analyzed field (0.19.4)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2006</link><project id="" key="" /><description>After upgrading from Elasticsearch 0.19.2 to 0.19.4 this query failes:

{ 
"from" : 0, 
"size" : 100000, 
"query" : { 
"query_string" : { 
"query" : "\"oai:bibsys.no:biblio:92116324x\" \"oai:bibsys.no:biblio:101924739\" \"oai:bibsys.no:biblio:921163258\" \"oai:bibsys.no:biblio:09372229x\" \"oai:bibsys.no:biblio:921163259\" ", 
"fields" : [ "otherid", "oaiid", "sesamid", "isbn", "urn" ] 
} 
}, 
"explain" : false, 
"sort" : [ { 
"_id" : { 
"order" : "asc" 
} 
} ] 
}

What we want, is to use the query_string to get the exact match.

Mappings:

"otherid":{"type":"string"}, 
"isbn":{"type":"string", "analyzer":"isbn"}, 
"urn":{"type":"string", "index":"not_analyzed", "boost":2.0}, 
"sesamid":{"type":"string", "index":"not_analyzed"}, 
"oaiid":{"type":"string", "index":"not_analyzed", "boost":2.0},

Elasticsearch log:

Parse Failure [Failed to parse source [{"from":0,"size":100000,"query":{"query_string":{"query":"\"oai:bibsys.no:biblio:92116324x\" \"oai:bibsys.no:biblio:101924739\" \"oai:bibsys.no:biblio:921163258\" \"oai:bibsys.no:biblio:09372229x\" \"oai:bibsys.no:biblio:921163259\" ","fields":["otherid","oaiid","sesamid","isbn","urn"]}},"explain":false,"sort":[{"_id":{"order":"asc"}}]}]] 
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:557) 
at org.elasticsearch.search.SearchService.createContext(SearchService.java:469) 
at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:228) 
at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:497)
at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:486)
at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:373) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662) 
Caused by: java.lang.NullPointerException 
at org.elasticsearch.index.mapper.MapperService$SmartIndexNameSearchQuoteAnalyzer.reusableTokenStream(MapperService.java:976)
at org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:574) 
at org.apache.lucene.queryParser.MapperQueryParser.getFieldQuerySingle(MapperQueryParser.java:238) 
at org.apache.lucene.queryParser.MapperQueryParser.getFieldQuery(MapperQueryParser.java:186) 
at org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:752) 
at org.apache.lucene.queryParser.MapperQueryParser.getFieldQuery(MapperQueryParser.java:257) 
at org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1580) 
at org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1317) 
at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:1245) 
at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:1234) 
at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:206) 
at org.elasticsearch.index.query.QueryStringQueryParser.parse(QueryStringQueryParser.java:206) 
at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:187) 
at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:249) 
at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:229) 
at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33) 
at org.elasticsearch.search.SearchService.parseSource(SearchService.java:545)

As you can see we get a NullPointerException in MapperService(976) which is new code in 0.19.4. It looks like it has something to do with the searchQuoteAnalyzer().

If we remove the quotes(\") around each query string, this query runs fine, but we get more hits then we want to. I guess, this is because "otherid" is not set to "not_analyzed".

Another query that also generate the same NPE:
{
"size": 1000,
"query": {
"bool": {
"must": [
{
"bool": {
"must": [
{
"field": {
"title": "\"Flammekaster\""
}
},
{
"field": {
"creator": "\"Dalby, Gene\""
}
},
{
"field": {
"year": "\"1981\""
}
},
{
"field": {
"mediatype": "\"B&#248;ker\""
}
}
]
}
},
{
"field": {
"status": "PROCESSED INDEXED FIELDS_EXTRACTED"
}
}
]
}
},
"fields": [
"title",
"creator",
"isbn",
"year",
"mediatype"
],
"sort": [
{
"dedup_primarySourceScore": {
"order": "desc"
}
},
{
"dedup_modsSize": {
"order": "desc"
}
},
{
"oaiid": {
"order": "asc"
}
}
]
}
</description><key id="4949998">2006</key><summary>Quoted query_string gives NullPointerException with not_analyzed field (0.19.4)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Frezzaldo</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-06-07T13:28:01Z</created><updated>2012-06-10T22:24:27Z</updated><resolved>2012-06-10T22:24:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T13:32:22Z" id="6175804">I think I fixed it in a recent commit to 0.19 branch, si there a chance for a full recreation to verify it?
</comment><comment author="Frezzaldo" created="2012-06-08T07:40:45Z" id="6196216">I ran our software tests with the latest 0.19 (0.19.5-SNAPSHOT). I also tried the master (0.20.0.Beta1-SNAPSHOT).

Exception message:
org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query_fetch], total failure; shardFailures: from[0],size[100000]: Parse Failure [Failed to parse source [:)
&#8422;&#178;om size$5&#128;&#132;query&#8945;&#181;ery_string&#8352;&#162;oai:bibsys.no:biblio:92116324x" "oai:bibsys.no:biblio:101924739" "oai:bibsys.no:biblio:921163258" "oai:bibsys.no:biblio:09372229x" "oai:bibsys.no:biblio:921163259" &#16742;&#169;elds&#27636;heridDoaiidFsesamidCisbnBurn&#7931;&#134;explain"&#131;sort&#3714;&#159;id&#8495;&#178;derBasc&#16121;&#187;]]]; nested: }

The query that failed:

{
  "from" : 0,
  "size" : 100000,
  "query" : {
    "query_string" : {
      "query" : "\"oai\:bibsys.no\:biblio\:92116324x\" \"oai\:bibsys.no\:biblio\:101924739\" \"oai\:bibsys.no\:biblio\:921163258\" \"oai\:bibsys.no\:biblio\:09372229x\" \"oai\:bibsys.no\:biblio\:921163259\" ",
      "fields" : [ "otherid", "oaiid", "sesamid", "isbn", "urn" ]
    }
  },
  "explain" : false,
  "sort" : [ {
    "_id" : {
      "order" : "asc"
    }
  } ]
}
</comment><comment author="kimchy" created="2012-06-08T11:34:11Z" id="6199682">can you provide a curl based recreation? Including setting up the index, indexing some sample data, and search that shows the failure?
</comment><comment author="Frezzaldo" created="2012-06-08T12:40:30Z" id="6201524">I'm going on a Holiday now, but I have informed my colleagues about this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix exception message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2005</link><project id="" key="" /><description /><key id="4938308">2005</key><summary>Fix exception message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2012-06-06T20:49:33Z</created><updated>2014-07-16T21:55:19Z</updated><resolved>2012-06-07T13:19:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T13:19:53Z" id="6175537">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't require lowercase sort order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2004</link><project id="" key="" /><description>Currently this will not return 3... 2... 1...

```
#!/bin/sh

curl -s -XDELETE localhost:9200/test &gt;/dev/null

curl -s -XPUT localhost:9200/test/t/uno -d '{"wat": 1}'; echo

curl -s -XPUT localhost:9200/test/t/dos -d '{"wat": 2}'; echo

curl -s -XPUT localhost:9200/test/t/tres -d '{"wat": 3}'; echo

curl -s -XPOST localhost:9200/test/_refresh\?pretty=1; echo

curl -s localhost:9200/test/_search\?pretty=1 -d '
{
    "query": {
        "match_all": {}
    }, 
    "sort": [
        {
            "wat": {
                "order": "Desc"
            }
        }
    ]
}
'; echo
```
</description><key id="4937771">2004</key><summary>Don't require lowercase sort order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2012-06-06T20:23:38Z</created><updated>2014-07-03T11:11:48Z</updated><resolved>2014-05-06T11:38:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T13:23:28Z" id="6175612">Its better to use `equalsIgnoreCase`, I will merge this and then fix it.
</comment><comment author="kimchy" created="2012-06-07T13:26:24Z" id="6175676">so, thinking about it a bit more, we don't really support different cases in other places, so maybe it makes sense to stay consistent instead of just supporting it here?
</comment><comment author="drewr" created="2012-06-07T14:24:28Z" id="6177068">I tend to agree.  Would you accept a patch that addresses the rest?
</comment><comment author="kimchy" created="2012-06-07T21:31:15Z" id="6188621">Sure, lets map them. I don't think we should support different case field names, just string values, right? Also, its best to use `myString.equalsIgnoreCase` since it does not do a copy of the string.
</comment><comment author="clintongormley" created="2014-05-06T11:38:50Z" id="42292368">Never implemented. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard queries with analysis?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2003</link><project id="" key="" /><description>Is it possible to make Elasticsearch analyzer factories in org.elasticsearch.index.analysis aware of wildcard queries?

For Solr, something similar is described here: http://www.lucidimagination.com/blog/2011/11/29/whats-with-lowercasing-wildcard-multiterm-queries-in-solr/ 
</description><key id="4931498">2003</key><summary>Wildcard queries with analysis?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-06-06T15:22:48Z</created><updated>2014-07-08T15:13:01Z</updated><resolved>2014-07-08T15:13:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:13:01Z" id="48351222">The query string query supports `analyze_wildcards`, which is the only place this is really relevant.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Actually include ES_JAVA_OPTS, since it can be set via defaults.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2002</link><project id="" key="" /><description>Init script is missing the actual inclusion of ES_JAVA_OPTS, set from /etc/defaults/elasticsearch.
</description><key id="4897941">2002</key><summary>Actually include ES_JAVA_OPTS, since it can be set via defaults.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpr5</reporter><labels /><created>2012-06-04T21:56:10Z</created><updated>2014-07-08T15:11:57Z</updated><resolved>2014-07-08T15:11:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T13:45:24Z" id="6176078">I think simply exporting it similar to what we do with `ES_HEAP_SIZE` will do, since the `bin/elasticsearch` script already takes it into account. I will push it to 0.19 and master, can you give it a go?
</comment><comment author="jpr5" created="2012-06-07T15:49:11Z" id="6179538">Sure, that will work too. 

Keep in mind though I think this might violate Debian/Ubuntu/RedHat's intent of what `/etc/default/*` files are supposed to do.  In my own setups, I don't see any other `/etc/default/*` files that contain `export`.
</comment><comment author="kimchy" created="2012-06-07T21:24:36Z" id="6188441">The export is in the `init.d/elasticsearch` file, similar to the original `ES_HEAP_SIZE` usage.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SafeTransportClient for multithreaded environments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2001</link><project id="" key="" /><description>Hi kimchy,

here is my suggestion for an additional feature for the TransportClient. 

The SafeTransportClient is "safe" in a sense that in a multithreaded environment it can limit the active action listeners. If there are too many action listeners active and a new one is going to be executed, the client will stall until other action listeners have responded. And if the SafeTransportClient is closed, for example when the process is suddenly being interrupted, it will first wait for all the responses of the outstanding action listeners before the shutdown continues.

The SafeTransportClient gets three more parameters, maximumActiveListeners, millisBeforeContinue, and maximumTimeouts.

Unfortunately I had to copy the TransportClient source to extend it into a new class SafeTransportClient.

The code is not tested thoroughly but it should work. I hope the sketch is useful. Right now, I guard the TransportClient with a very similar wrapper code.

J&#246;rg
</description><key id="4876287">2001</key><summary>SafeTransportClient for multithreaded environments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-06-03T11:07:57Z</created><updated>2014-06-14T13:25:12Z</updated><resolved>2012-08-17T21:33:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-08-17T21:33:05Z" id="7835804">ActionListener improvements and bulk throttling is now in the new BulkProcessor https://github.com/elasticsearch/elasticsearch/commit/bdea0e2eddb4373b850e00d8e363c5240d78d180
so I don't think this request is needed anymore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>allow sloppy date parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2000</link><project id="" key="" /><description>This patch will allow to parse dates coming from external sources that have invalid date/time syntax.

By now, such dates will fail during mapping. For example,  MySQL may create the infamous 'null' date `0000-00-00 ...`.

```
org.joda.time.IllegalFieldValueException: Cannot parse "0000-00-00": Value 0 for monthOfYear must be in the range [1,12]
```

In some cases, it is convenient to swallow all dates in incoming data, even with invalid dates, into Elasticsearch, for example from a MySQL river.

To enable the sloppy behaviour, the configuration of the index mapping `index.mapping.date.sloppy` must be set to `true`. Default value is `false`.
</description><key id="4872204">2000</key><summary>allow sloppy date parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-06-02T17:34:49Z</created><updated>2014-07-08T15:11:13Z</updated><resolved>2014-07-08T15:11:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:11:13Z" id="48350884">Closing, as we now have the `ignore_malformed` parameter.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add Bulgarian to stemmer token filter language options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1999</link><project id="" key="" /><description>Modified StemmerTokenFilterFactory to add bulgarian stemmer (BulgarianStemFilter)
</description><key id="4858682">1999</key><summary>Analysis: Add Bulgarian to stemmer token filter language options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinhughes</reporter><labels /><created>2012-06-01T15:01:06Z</created><updated>2014-07-16T21:55:20Z</updated><resolved>2012-06-07T13:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T13:37:08Z" id="6175908">Pushed to 0.19 and master branch, thanks!
</comment><comment author="kimchy" created="2012-06-07T13:37:19Z" id="6175914">closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix geo normalization for ranges and points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1998</link><project id="" key="" /><description>See [issue #1997](https://github.com/elasticsearch/elasticsearch/issues/1997).
</description><key id="4855597">1998</key><summary>Fix geo normalization for ranges and points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2012-06-01T12:11:37Z</created><updated>2014-06-13T11:14:39Z</updated><resolved>2013-07-16T10:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-14T13:56:52Z" id="6328633">See #2024.
</comment><comment author="chilling" created="2013-07-16T10:03:45Z" id="21032354">see #3149
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix geo normalization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1997</link><project id="" key="" /><description>We have 3 problems:
- The actual latitude normalization is very wrong: it wraps latitude. Coordinate normalization is broken.
- The range normalization cannot be performed by normalizing the top-left and bottom-right corners.
- The geo polygon normalization cannot be performed by normalizing its vertices.

Let me explain:

When you cross past the 180&#176; meridian, you arrive near the -180&#176; meridian (which is actually the same / does not exist).
In other words longitude wraps.

When you cross past the 90&#176; parallel at the north pole, you are still in the north pole, but you are now going back down along the opposite meridian.
In other words latitude reflects, accompanied with a 180&#176; shift in longitude.

This implies that we cannot normalize latitude without affecting (or even normalizing) longitude.

Moreover when describing an area you have to split it in multiple areas. This affects geo bounding boxes and geo polygons.
The easiest example is considering the range `[(10&#176;;170&#176;);(-10&#176;;190&#176;)]`, it must be splitted into `[(10&#176;;170&#176;);(-10&#176;;180&#176;)]` and `[(10&#176;;-180&#176;);(-10&#176;;-170&#176;)]`.
</description><key id="4854511">1997</key><summary>Fix geo normalization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2012-06-01T10:34:42Z</created><updated>2013-07-16T10:03:45Z</updated><resolved>2013-07-03T09:25:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2012-06-01T10:53:03Z" id="6057942">In multiple classes you can control whether or not to normalize and validate latitude and/or longitude.
Controlling this setting separately for latitude and longitude seems odd, considering the implication of a 180&#176;lon shift in some cases.

I can only think of one use case: One wants to superpose an (indefinite and dynamic) number of layers that must not be merged.
One can then decide to apply a `k*360`&#176;lon shift, so that range queries made in one domain (defined by `x+k*360` with `x in ]-180:180]` for longitude and `y in [-90;90]` for latitude) do not leak to other layers.
But even in such weird use case, a filter on some field containing `k` would make more sense... maybe not performance wise, but still.
</comment><comment author="ofavre" created="2012-06-01T16:26:46Z" id="6064496">### Normalizing geo polygons

**Keep in mind** that a polygon is defined by a list connected of points, defining contiguous edges.
Also, the `GeoPolygonFilter.pointInPolygon()` algorithm uses the crossing number method, hence a self intersection defines a whole.

#### The idea
- A polygon should be split when one of its edges crosses a splitting axis (&#177;90&#176;lat or &#177;180&#176;lon).
  We must consider the intersection point, _normalized_ on both sides.
- The points that are on the opposite side of the splitting axis will be taken care of as part of a new polygon.
- As long as the points considered when iterating are at the opposite side, we ignore them in the context of the current polygon.
- As soon as they come back in and to not cross an other splitting axis (at corners), we resume filling the current polygon, taking care of the intersection points with the axis.
- When creating a new polygon, we must attach a new transformation to it, so that the _transformed_ points it considers are normalized, until they, too, eventually cross a splitting axis.
- In order to close all polygons (especially those started at the middle of the iteration) we must reiterate on the points.
  When the starting point is met again, we close the related polygon, etc. until all polygons are closed.

**Note:** This algorithm keeps parts of the polygon that are in the same normalization space as part of a single polygon, hence self intersections are kept intact. However this comes with eventual edges being along the splitting axis, sometime multiple such edges being superposed, and this may affect the `pointInPolygon()` algorithm, but it shouldn't, considering such edges cover an empty area.

**Complexity:** This algorithm is _O(n)_, _n_ being the number of points defining the polygon to normalize, but it's a bit verbose... (can it really be less?)

#### Pseudo-implementation stub

```
main:
    let inPoints be the list of points defining the polygon to normalize
    let list be the list of polygons
    let polygon be composed of
        tfo // a transform the normalizes the points to consider
        points // a list of points that form the polygon
        startIdx // the index of the first point that created this polygon
        closed = false // whether the polygon has finished its construction
        active = true // if the points considered don't yet cross a splitting axis
        cutPoint = none // the cut point on the splitting axis
        splittingAxis = none // the current splitting axis that paused the construction
    let tfo be a transformation
    set tfo = transformation normalizing the point inPoints[0]
    let n be an integer
    set n = 0
    append new polygon(tfo=tfo, points=[inPoints[0]], startIdx=0) to list
    iterate over the edges : edge
        iterate over list : polygon
            polygon.notify(n, edge)
        increment n
    set n = 0
    let i be the index of the first non closed polygon
    set nc = 0
    iterate over the edges : edge
        for i in nc to list.size-1
            list[i].notify(n, edge)
            if list[i].closed?
                increment nc
        if nc equals list.size
            break
        increment n
    return list

polygon.notify(n, edge):
    transform edge according to tfo
    if not active
        if n == startIdx
            set closed = true
        else if edge crossing back splittingAxis
            set cutPoint accordingly
            set edge.begin = cutPoint
            if edge crossing any splitting axis
                set cutPoint accordingly
                set splittingAxis accordingly
            else
                set active = true
                self.notify(n, edge)
    else
        if edge crossing any splitting axis
            set active = false
            set cutPoint accordingly
            set splittingAxis accordingly
            append cutPoint to points
            set edge.begin = cutPoint
            let newTfo be a transformation
            let newPoly be a polygon
            set newTfo accordingly, to normalize points of the other side of splittingAxis
            set newPoly = new polygon(tfo=newTfo, points=[edge.begin], startIdx=n)
            append newPoly to list
            call newPoly.notify(n, edge)
        else
            append edge.end to points
```

#### Feedback required

I'm not really going to implement it because: I'm not 100% sure it works fine in all cases, and I'm not convinced geo polygon normalization is a real need.

It may even be prevented by other means if such problem arise.
For instance, with a few assumptions one can generate reflected/shifted copies of the polygon, and or together the results.
</comment><comment author="ofavre" created="2012-06-01T16:41:35Z" id="6064812">### Normalizing geo polygons

#### A naive approach

_Why didn't I even think about that one before..._

**Near up to 3 time slower.** No impact if already normalized.

**Assumptions:**
- the geo polygon is no wider than 360&#176;lon and no taller than 180&#176;lat (should usually be the case).
- no undesired point lie outside the normalized range (if the user wants normalization... then it should normally be the case).

**Steps:**
- Compute the geo bounding box of the geo polygon
- Normalize the range, but instead of generating just the missing parts,
  shift and or reflect the whole polygon
- Or-together the generated copies (3 maximum).

#### Feedback required

Do you think such assumptions are fair? the performance impact acceptable?
</comment><comment author="ofavre" created="2012-06-01T17:15:57Z" id="6065556">### Normalizing geo polygons

#### Using triangulation
- Check if the geo bounding box is already normalized and early exit.
- Triangulate the polygon.
- Eventually split each triangle

```
normalize the first point, transforming the two others accordingly.
select the number of adjacent edges of the first point that cross a splitting axis
    if zero
        if the opposite edge crosses a splitting axis
            // the first point is on a splitting axis
            calculate the cut point
            split the triangle in two
            // no need to normalize the two pieces as none of their edge would cross a splitting axis
        // otherwise, the triangle is normalized (as the other two points are within the same normalization space and a triangle is convex!)
    if one
        // the opposite edge is crossing the same splitting axis too
        calculate the cut point
        create a new triangle with the second and third points, and the cut point (all but the first point)
        normalize that new triangle
        modify the current triangle to use the cut point instead of the faulty vertex
        // no need to re-normalize as we've fixed the only crossing of the current triangle
    if two
        calculate the two cut points
        create two triangles with the second and third points, and the two cut points
        normalize them
        modify the current triangle to use the two cut points instead of the second and third points
        // no need to re-normalize as we've fixed the two crossing of the current triangle
```

**Complexity:** Triangulation can be no faster than _O(n*log(n))_ if polygon has holes, _O(n)_ otherwise. Furthermore, re-normalization of split triangles are sometimes needed.

#### Feedback required

This method seems more conventional and simple.

If not handled by the triangulation method, it however it breaks the crossing number method, by ignoring the self-intersections.
The `pointInPolygon` method will also be modified/simplified, but not necessarily to become more performant.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect error message when query parse fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1996</link><project id="" key="" /><description>I have a parsing error is about a reference to missing native script. Here is a kind of message I got:

org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query], total failure; shardFailures {[Yj9xFjsFT8GpiPxvZjWKkQ][scoop][3]: RemoteTransportException[[Strongarm][inet[/10.0.0.209:9300]][search/phase/query]]; nested: SearchParseException[[scoop][3]: from[0],size[24]: Parse Failure [Failed to parse source [:)om size$&#176;&#132;query
...

I had to modify and cut the error message since github doesn't allow posting weird caracters, hope you get the idea: the query is incorrectly String encoded.

As far I can see, the issue is in: org.elasticsearch.search.SearchService.parseSource(SearchContext, byte[], int, int)

The source content type is smile, and converting it in a String for an error message is hence incorrect. I'm not how sure how to fix it, so I don't have a patch to suggest.
</description><key id="4854087">1996</key><summary>Incorrect error message when query parse fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-06-01T10:00:41Z</created><updated>2012-06-08T11:46:15Z</updated><resolved>2012-06-08T11:46:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T13:41:12Z" id="6175993">yea, the smile format is not something simple to represent as a string, we could try and parse it again in smile and convert it to a json string
</comment><comment author="nlalevee" created="2012-06-08T08:59:55Z" id="6197464">It was actually quite trivial to make a String of the source, thanks to XContentHelper.convertToJson().
I've just sent a pull request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cluster api wait_for_status will not wait for states other than green</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1995</link><project id="" key="" /><description>Testing on 0.19.4, issue the following against a cluster in a 'green' state:

curl -XGET "http://host:9200/_cluster/health?wait_for_status=yellow&amp;timeout=50s"

Will return immediately. The only working case I can find of wait_for_status is when the cluster is in some non-green state and wait_for_status=green is passed.
</description><key id="4844254">1995</key><summary>cluster api wait_for_status will not wait for states other than green</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heffergm</reporter><labels /><created>2012-05-31T19:34:37Z</created><updated>2014-06-20T02:51:03Z</updated><resolved>2012-06-07T21:52:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T21:32:52Z" id="6188664">I think you figured this one out, right? wait_for_status does not wait for exact status, if the cluster is green, it is already "better" than yellow, so it returns.
</comment><comment author="heffergm" created="2012-06-07T21:52:11Z" id="6189113">Yup, all set. Was just a little confused on the docs, but makes sense as is. 

Thanks!

## 

Grant Heffernan
~via tiny keyboard

On Jun 7, 2012, at 5:32 PM, Shay Banonreply@reply.github.com wrote:

&gt; I think you figured this one out, right? wait_for_status does not wait for exact status, if the cluster is green, it is already "better" than yellow, so it returns.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1995#issuecomment-6188664
</comment><comment author="kimchy" created="2012-06-07T21:52:48Z" id="6189129">cool!, I will close the issue.
</comment><comment author="alexanderyoung" created="2014-06-20T02:51:03Z" id="46640223">Maybe this is a documentation issue then?  I was confused until I read this bug...

&gt; wait_for_status
&gt; One of green, yellow or red. Will wait (until the timeout provided) until the status of the cluster changes to the one provided. By default, will not wait for any status.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-health.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight: field array concatenated when term_vector set to with_positions_offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1994</link><project id="" key="" /><description>Please take a look at issue #645 again. The erroneous behavior described there is still present in ElasticSearch v0.19.3

I ran @lukas-vlcek script and the same erroneous behavior was observed. When `"term_vector" : "with_positions_offsets"` is used in the mapping, the highlighted fields are concatenated into a single string. This behavior differs from the case where term vectors are not used.

I believe one bug was fixed. In the example from #645 the extra tag string `"two wordish"` was included in the highlight results. This bug is fixed in the case where term vectors are **not** being used for highlighting.

However, when term vectors are enabled and the fast-vector-highlighter is used, all the highlight tags are concatenated together into a single string.

```
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.16273327,
    "hits" : [ {
      "_index" : "test",
      "_type" : "one",
      "_id" : "hx79P59ARRaOylM6yfYH2w",
      "_score" : 0.16273327, "_source" : { "tags" : ["one word","too wordish","three words"]},
      "highlight" : {
        "tags" : [ "one &lt;em&gt;word&lt;/em&gt; too wordish three &lt;em&gt;words&lt;/em&gt;" ]
      }
    } ]
  }
}
```
</description><key id="4843138">1994</key><summary>Highlight: field array concatenated when term_vector set to with_positions_offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TwP</reporter><labels /><created>2012-05-31T18:39:47Z</created><updated>2012-12-07T18:54:55Z</updated><resolved>2012-12-07T18:54:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TwP" created="2012-05-31T22:43:16Z" id="6049525">Possibly related to #1593
</comment><comment author="kimchy" created="2012-06-07T14:42:08Z" id="6177568">This is how the fast vector highlighter seems works..., will check if it can be hacked around or not...
</comment><comment author="kimchy" created="2012-06-11T22:28:14Z" id="6257936">Fixed, will be in upcoming 0.19.5.
</comment><comment author="TwP" created="2012-12-07T18:54:55Z" id="11141717">I can confirm that this is fixed in versions 0.19.10 and 0.19.11
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST endpoint for a list of plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1993</link><project id="" key="" /><description>Hi kimchy,

I'd like to add a REST endpoint for a list of plugins to Elasticsearch.

Only the last commit, please ignore the sitedocs stuff, I'm just wrestling with git branches und github pull requests.

Cheers, 

J&#246;rg
</description><key id="4841519">1993</key><summary>REST endpoint for a list of plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-05-31T17:17:51Z</created><updated>2014-06-13T17:03:16Z</updated><resolved>2012-06-07T14:26:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mobz" created="2012-05-31T23:19:43Z" id="6050070">I'd like to be able to add features in elasticsearch-head that require knowing about plugins
- allow plugins to expose a GUI that is accessable from es-head
- directly integrating plugin feature when available 

+1 this patch!!
</comment><comment author="kimchy" created="2012-06-07T11:13:29Z" id="6173509">can we change this pull request ot only expose the plugins, without all the other changes?
</comment><comment author="jprante" created="2012-06-07T14:26:01Z" id="6177113">ok, see #2007
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mlockall in 0.19.4 does not appear to be working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1992</link><project id="" key="" /><description>elasticsearch.yml : bootstrap.mlockall

elasticsearch.conf: min &amp; max heap both set to 1.9gb

user limits: user memlock - unlimited

core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 31971
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 2047
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

On node restart, I get no memlock messages in the log indicating success or failure to lock the memory. Debug log below.
Bigdesk shows a 1.9gb heap commit and ~100mb used on restart (same as nodes that do no have bootstrap.mlockall : true set).

[2012-05-31 15:15:03,561][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: stopping ...
[2012-05-31 15:15:03,750][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: stopped
[2012-05-31 15:15:03,751][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: closing ...
[2012-05-31 15:15:03,941][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: closed
[2012-05-31 15:15:16,002][INFO ][bootstrap                ] max_open_files [65517]
[2012-05-31 15:15:18,833][INFO ][node                     ] [prod-es-r01] {0.19.4}[1579]: initializing ...
[2012-05-31 15:15:18,893][INFO ][plugins                  ] [prod-es-r01] loaded [transport-thrift, hashing-analyzer], sites [bigdesk]
[2012-05-31 15:15:18,938][DEBUG][env                      ] [prod-es-r01] using node location [[/opt/elastic_search/data/brewster/nodes/0]], local_node_id [0]
[2012-05-31 15:15:20,768][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [generic], type [cached], keep_alive [30s]
[2012-05-31 15:15:20,773][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [index], type [fixed], size [20], queue_size [null], reject_policy [caller]
[2012-05-31 15:15:20,775][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [bulk], type [cached], keep_alive [5m]
[2012-05-31 15:15:20,775][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [get], type [cached], keep_alive [5m]
[2012-05-31 15:15:20,776][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [search], type [fixed], size [20], queue_size [null], reject_policy [abort]
[2012-05-31 15:15:20,776][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [percolate], type [cached], keep_alive [5m]
[2012-05-31 15:15:20,776][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2012-05-31 15:15:20,777][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [flush], type [scaling], min [1], size [10], keep_alive [5m]
[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [merge], type [scaling], min [1], size [20], keep_alive [5m]
[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [refresh], type [cached], keep_alive [1m]
[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [cache], type [scaling], min [1], size [4], keep_alive [5m]
[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [snapshot], type [scaling], min [1], size [5], keep_alive [5m]
[2012-05-31 15:15:20,792][DEBUG][transport.netty          ] [prod-es-r01] using worker_count[8], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1]
[2012-05-31 15:15:20,810][DEBUG][discovery.zen.ping.unicast] [prod-es-r01] using initial hosts [10.180.48.178:9300, 10.183.69.144:9300, 10.182.14.97:9300, 10.180.35.110:9300, 10.180.39.14:9300, 10.180.46.203:9300, 10.180.48.216:9300, 10.180.48.255:9300], with concurrent_connects [10]
[2012-05-31 15:15:20,812][DEBUG][discovery.zen            ] [prod-es-r01] using ping.timeout [15s]
[2012-05-31 15:15:20,818][DEBUG][discovery.zen.elect      ] [prod-es-r01] using minimum_master_nodes [3]
[2012-05-31 15:15:20,819][DEBUG][discovery.zen.fd         ] [prod-es-r01] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2012-05-31 15:15:20,822][DEBUG][discovery.zen.fd         ] [prod-es-r01] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2012-05-31 15:15:20,848][DEBUG][monitor.jvm              ] [prod-es-r01] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name='ParNew', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name='ConcurrentMarkSweep', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]
[2012-05-31 15:15:21,358][DEBUG][monitor.os               ] [prod-es-r01] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@35e5ebbf] with refresh_interval [1s]
[2012-05-31 15:15:21,364][DEBUG][monitor.process          ] [prod-es-r01] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@3fdb8a73] with refresh_interval [1s]
[2012-05-31 15:15:21,367][DEBUG][monitor.jvm              ] [prod-es-r01] Using refresh_interval [1s]
[2012-05-31 15:15:21,368][DEBUG][monitor.network          ] [prod-es-r01] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@3d6a2c7b] with refresh_interval [5s]
[2012-05-31 15:15:21,375][DEBUG][monitor.network          ] [prod-es-r01] net_info
host [prod-es-r01.ihost.brewster.com]
eth1    display_name [eth1]
        address [/fe80:0:0:0:4240:75ff:feca:294d%3] [/10.180.48.178] 
        mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
eth0    display_name [eth0]
        address [/fe80:0:0:0:4240:4fff:feb3:e842%2] [/184.106.133.96] 
        mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
lo  display_name [lo]
        address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1] 
        mtu [16436] multicast [false] ptp [false] loopback [true] up [true] virtual [false]

[2012-05-31 15:15:21,381][DEBUG][monitor.fs               ] [prod-es-r01] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@78214f6b] with refresh_interval [1s]
[2012-05-31 15:15:21,696][DEBUG][cache.memory             ] [prod-es-r01] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]
[2012-05-31 15:15:21,744][DEBUG][cluster.routing.allocation.decider] [prod-es-r01] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [2]
[2012-05-31 15:15:21,745][DEBUG][cluster.routing.allocation.decider] [prod-es-r01] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2012-05-31 15:15:21,745][DEBUG][cluster.routing.allocation.decider] [prod-es-r01] using [cluster_concurrent_rebalance] with [2]
[2012-05-31 15:15:21,749][DEBUG][gateway.local            ] [prod-es-r01] using initial_shards [quorum], list_timeout [30s]
[2012-05-31 15:15:21,834][DEBUG][indices.recovery         ] [prod-es-r01] using max_size_per_sec[10mb], concurrent_streams [2], file_chunk_size [100kb], translog_size [100kb], translog_ops [1000], and compress [true]
[2012-05-31 15:15:21,869][DEBUG][http.netty               ] [prod-es-r01] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb]
[2012-05-31 15:15:21,878][DEBUG][indices.memory           ] [prod-es-r01] using index_buffer_size [203.9mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2012-05-31 15:15:21,883][DEBUG][indices.cache.filter     ] [prod-es-r01] using [node] filter cache with size [20%], actual_size [407.9mb]
[2012-05-31 15:15:22,327][DEBUG][gateway.local.state.shards] [prod-es-r01] took 370ms to load started shards state
[2012-05-31 15:15:24,192][DEBUG][gateway.local.state.meta ] [prod-es-r01] took 1.8s to load state
[2012-05-31 15:15:24,195][INFO ][node                     ] [prod-es-r01] {0.19.4}[1579]: initialized
[2012-05-31 15:15:24,195][INFO ][node                     ] [prod-es-r01] {0.19.4}[1579]: starting ...
[2012-05-31 15:15:24,263][INFO ][thrift                   ] [prod-es-r01] bound on port [9500]
[2012-05-31 15:15:24,297][DEBUG][netty.channel.socket.nio.NioProviderMetadata] Using the autodetected NIO constraint level: 0
[2012-05-31 15:15:24,410][DEBUG][transport.netty          ] [prod-es-r01] Bound to address [/10.180.48.178:9300]
[2012-05-31 15:15:24,411][INFO ][transport                ] [prod-es-r01] bound_address {inet[/10.180.48.178:9300]}, publish_address {inet[/10.180.48.178:9300]}
[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_8#][inet[/10.180.48.255:9300]]]
[2012-05-31 15:15:24,499][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[prod-es-r01][BkIy7zTyQJqXfhQ6vUlaKQ][inet[/10.180.48.178:9300]]{subnet=180.48, datacenter=ORD1}]
[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_2#][inet[/10.183.69.144:9300]]]
[2012-05-31 15:15:24,498][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_6#][inet[/10.180.46.203:9300]]]
[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_5#][inet[/10.180.39.14:9300]]]
[2012-05-31 15:15:24,498][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_7#][inet[/10.180.48.216:9300]]]
[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_4#][inet[/10.180.35.110:9300]]]
[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_3#][inet[/10.182.14.97:9300]]]
[2012-05-31 15:15:39,440][DEBUG][transport.netty          ] [prod-es-r01] disconnected from [[#zen_unicast_7#][inet[/10.180.48.216:9300]]]
[2012-05-31 15:15:39,438][DEBUG][discovery.zen            ] [prod-es-r01] ping responses:
    --&gt; target [[prod-es-r07][MAWcm4KOSpCkfgdf4oMQ_g][inet[/10.180.48.216:9300]]{subnet=180.48, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
    --&gt; target [[prod-es-r04][V0_bFMKDRYWn90hLCKJBVw][inet[/10.180.35.110:9300]]{subnet=180.35, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
    --&gt; target [[prod-es-r05][SoTwCd1aTrCQaiwdnvTl4Q][inet[/10.180.39.14:9300]]{subnet=180.39, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
    --&gt; target [[prod-es-r08][WzZIAFrIRMO2VaR-68UvMw][inet[/10.180.48.255:9300]]{subnet=180.48, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
    --&gt; target [[prod-es-r03][aV17ANXvSPmxbd8wonM4-Q][inet[/10.182.14.97:9300]]{subnet=182.14, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
    --&gt; target [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
    --&gt; target [[prod-es-r06][sAFQAI6XQsu50I3LNdpcvA][inet[/10.180.46.203:9300]]{subnet=180.46, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]
</description><key id="4839043">1992</key><summary>mlockall in 0.19.4 does not appear to be working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">heffergm</reporter><labels /><created>2012-05-31T15:20:22Z</created><updated>2013-06-07T15:45:18Z</updated><resolved>2013-06-07T15:45:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T15:43:32Z" id="19114961">sorry, I dont understand what you are expecting? If you do not get an warn message on startup mlockall should work, but this does not mean that you will use different amount of memories.

can you maybe elaborate?
</comment><comment author="heffergm" created="2013-06-07T15:45:17Z" id="19115073">Ummm.... this is year old, so to be honest, I have no idea what I'm expecting at this point either :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index level allocation filtering: Add _host option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1991</link><project id="" key="" /><description>Add _host option to index level allocation filtering, which will match on either the hostname or ip address.
</description><key id="4833320">1991</key><summary>Index level allocation filtering: Add _host option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-31T09:38:42Z</created><updated>2012-05-31T09:39:15Z</updated><resolved>2012-05-31T09:39:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a file watcher which trigger the reload of the logging configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1990</link><project id="" key="" /><description>When log4j is in use, the default, the configuration is not reloaded on change on the configuration file. The suggested patch makes it check every 10s for change and reload the log config if necessary.
</description><key id="4819481">1990</key><summary>Add a file watcher which trigger the reload of the logging configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nlalevee</reporter><labels><label>feature</label><label>feedback_needed</label></labels><created>2012-05-30T16:38:00Z</created><updated>2014-10-16T19:28:21Z</updated><resolved>2014-10-16T19:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T09:28:36Z" id="49412068">Thanks for the PR. The logging configuration can be updated dynamically now, so I don't think this is required anymore.
</comment><comment author="clintongormley" created="2014-07-18T09:29:56Z" id="49412175">@dakrone likes this idea - so assigning to him
</comment><comment author="dakrone" created="2014-07-23T13:45:48Z" id="49875124">Hi @nlalevee , I think this is a neat feature, can you change this to take advantage of the `ResourceWatcherService`? It should remove the need for a custom file watchdog class.
</comment><comment author="clintongormley" created="2014-10-16T19:28:21Z" id="59416489">No response since August. I'm going to close this for now.  @nlalevee feel free to reopen it if you are still interested in getting this in.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an interface about "boost settable" query builders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1989</link><project id="" key="" /><description>I have some generic code that build my queries. At some point I want to apply some boost. The suggested interface would make my code cleaner.
</description><key id="4819370">1989</key><summary>Add an interface about "boost settable" query builders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-05-30T16:33:05Z</created><updated>2014-07-16T21:55:21Z</updated><resolved>2012-06-07T21:36:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-06-07T21:36:54Z" id="6188768">pushed to master and 0.19, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: not filter with inner filter that uses array element fails (without the `filter` wrapper)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1987</link><project id="" key="" /><description>Repro:

```
curl -XPOST http://localhost:9200/tests/test/1 -d '{v1:"test1", v2:"test2"}'


1) My normal OR works ok

curl -XPOST http://localhost:9200/tests/test/_search -d ' {"query": {
    "filtered": {"query": {"match_all": {}},
     "filter": {or: [{"term": {"v1": "test1"}}]}
    } } }'

2) Wrap OR with NOT and get an error
curl -XPOST http://localhost:9200/tests/test/_search -d ' {"query": {
    "filtered": {"query": {"match_all": {}},
     "filter": {"not": {"or": [{"term": {"v1": "test1"}}]}}
    } } }'

Failed to execute phase [query], total failure; shardFailures
[CUT]
nested: QueryParsingException[[tests] [or] filter does not support [v1]]; 

3) If i wrap the inside of the OR with an AND works fine
curl -XPOST http://locahost:9200/tests/test/_search -d ' {"query": {
    "filtered": {"query": {"match_all": {}},
     "filter": {"not": {"or": [{"and":[{"term": {"v1": "test0"}}]}]}}
    } } }'
```
</description><key id="4805553">1987</key><summary>Query DSL: not filter with inner filter that uses array element fails (without the `filter` wrapper)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-29T22:16:20Z</created><updated>2012-05-29T22:17:28Z</updated><resolved>2012-05-29T22:17:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Proximity and phrases search fast-vector-highlighter vs. highlighter + exposing highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1986</link><project id="" key="" /><description>This issue is being opened per discussion on https://groups.google.com/forum/?fromgroups&amp;hl=en#!starred/elasticsearch/d35-QiElg90

"Hi,
here https://gist.github.com/1222046 is a bug reconstruction for
highlighting issues with fast-vector-highlighter in cases of proximity
search and exact phrases searching (when exact phrases contain stop
words).
Reconstruction shows two cases where default/plain highlighter works
correctly/better then the fast-vector-highlighter (maybe there are
some cases/queries where situation is reversed), so my question is: if
these bugs in fast-vector-highlighter code require some time to be
fixed maybe it would be useful to expose highlighter (as a quick fix),
so in the case where field is stored (with "term_vector" :
"with_positions_offsets") users can chose between using
fast-vector-highlighter and default/plain highlighter. Because, even
if fast-vector-highlighter is much faster and should be used for
highlighting matches in fields with term vectors stored, in some cases
(like proximity and stop-word phrases) it does not work correctly, so
being able to use plain highlighter would help.

Thanks,

Tomislav"
</description><key id="4804123">1986</key><summary>Proximity and phrases search fast-vector-highlighter vs. highlighter + exposing highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robert-abbott</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2012-05-29T21:00:14Z</created><updated>2016-11-24T18:09:41Z</updated><resolved>2016-11-24T18:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="robert-abbott" created="2012-05-29T22:17:48Z" id="5997537">Saw this issue on both 0.19.2 &amp; 0.19.4
</comment><comment author="javanna" created="2013-08-28T09:50:40Z" id="23402867">As a side note, it's now possible to manually choose the highlighter type to be used, using the `type` field. Therefore you can use the plain highlighter even though you stored term_vectors for a specific field.
</comment><comment author="robert-abbott" created="2013-08-30T15:03:36Z" id="23567263">great, when was this introduced?
</comment><comment author="javanna" created="2013-08-30T16:33:39Z" id="23573508">Looks like it was introduced in 0.20.0 after pull request #2350 was pushed to master.
</comment><comment author="ajhalani" created="2014-01-13T19:07:28Z" id="32200060">Was the root cause found for this issue? 

Though it's possible to use the plain highlighter, it kinda defeats the purpose of configuring fvh. Also it adds complexity to application to dynamically switch between different highlighters. 
</comment><comment author="clintongormley" created="2014-07-08T15:10:34Z" id="48350751">The situation has improved - now only the proximity query with FVH doesn't work correctly.
</comment><comment author="nik9000" created="2014-07-08T15:15:49Z" id="48351763">For completeness sake the [highlighter](https://github.com/wikimedia/search-highlighter) we use supports phrases properly with slop and reduces span queries to term queries.  I believe its just a tiny step up from the fvh in that regard.  
</comment><comment author="vineet85" created="2015-12-03T17:22:23Z" id="161722058">@clintongormley - Does the FVH currently support proximity queries? Can you tell me what the latest is on this issue? Thanks!
</comment><comment author="vineet85" created="2015-12-03T19:54:38Z" id="161764511">The FVH still highlights if we use the ES Query_String_Query

```
    POST localhost:9200/testIndex/_search
    {
       "query": {
       "query_string": {
       "default_field": "fulltext.exact",
       "query": "\"significant parsing\"~4"
       }
   }, 
   "highlight": {
      "fields": {
         "fulltext.exact": {
            "type": "fvh",
            "number_of_fragments": 0
         }
      },
      "require_field_match": true
   }
}
```

However it is not supported when using the Span Query

```
POST localhost:9200/testIndex/_search
{
    "query": {
        "filtered": {
            "query": {
                "span_near": {
                    "clauses": [{
                        "span_term": {
                            "fulltext.exact": "significant"
                        }
                    }, {
                        "span_term": {
                            "fulltext.exact": "complexity"
                        }
                    }],
                    "slop": 4,
                    "in_order": false
                }
            },
            "filter": {
                "term": {
                    "documentid": "1de7343c6bdd400b936f76782db93065"
                }
            }
        }
    },
    "highlight": {
        "fields": {
            "fulltext.exact": {
                "matched_fields": ["fulltext.exact"],
                "type": "fvh",
                "number_of_fragments": 0
            }
        },
        "require_field_match": true
    }
}
```
</comment><comment author="clintongormley" created="2016-11-06T09:13:52Z" id="258666555">As of 5.0, the `plain` highlighter works for all cases below, the `fvh` highlighter fails for out of order proximity queries and for all span queries.

Updated recreation:

```
PUT /twitter/

PUT /twitter/tweet/_mapping
{
  "properties": {
    "message": {
      "type": "text",
      "term_vector": "with_positions_offsets"
    }
  }
}

PUT /twitter/tweet/1
{
  "user": "kimchy",
  "post_date": "2009-11-15T14:12:12",
  "message": "trying out Elastic and Search"
}

PUT /twitter/tweet/2
{
  "user": "kimchy",
  "post_date": "2009-11-15T14:12:12",
  "message": "trying out Search and Elastic"
}

GET /twitter/tweet/_search
{
  "query": {
    "query_string": {
      "default_field": "message",
      "query": "\"Elastic Search\"~4"
    }
  },
  "highlight": {
    "fields": {
      "message": {
        "type": "fvh"
      }
    }
  }
}

POST /twitter/tweet/_search
{
  "query": {
    "span_near": {
      "clauses": [
        {
          "span_term": {
            "message": "search"
          }
        },
        {
          "span_term": {
            "message": "elastic"
          }
        }
      ],
      "slop": 4,
      "in_order": false
    }
  },
  "highlight": {
    "fields": {
      "message": {
        "type": "fvh"
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-11-24T18:09:41Z" id="262827250">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>using bulk API with update (using scripts) in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1985</link><project id="" key="" /><description>this issue is based on the thread at location 
https://groups.google.com/forum/?hl=en&amp;fromgroups#!topic/elasticsearch/VCwQoZ1Aj4I

requesting to allow using update API (using scripts) in bulk via the bulk request API.
</description><key id="4789653">1985</key><summary>using bulk API with update (using scripts) in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sujoysett</reporter><labels /><created>2012-05-29T06:54:27Z</created><updated>2013-05-10T14:22:56Z</updated><resolved>2013-05-10T14:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-05-30T09:06:17Z" id="6005454">Bulk updates, when issued concurrently, are subject to conflict with each other, when operating on the same documents at the same time. Maintaining update order is significant to get the correct results. To help resolving conflicts, a client ID and a client timestamp (tick resolution) could be useful, e.g.

{ "action" : { "update" : {  "index" : ... }, "client" : "uniqueID",  "timestamp" : "2012-05-30T10:56:14.123Z" }, ... }

J&#246;rg
</comment><comment author="tarunjangra" created="2012-10-20T06:50:14Z" id="9628678">Any update over that?
</comment><comment author="devilankur18" created="2013-01-05T20:13:02Z" id="11919167">+1
</comment><comment author="newgene" created="2013-01-11T02:48:29Z" id="12129853">+1
</comment><comment author="philipmuir" created="2013-01-11T02:52:02Z" id="12129906">:+1:
</comment><comment author="nhuray" created="2013-01-14T14:17:34Z" id="12219956">:+1: 
</comment><comment author="haizaar" created="2013-01-20T17:50:04Z" id="12474188">+1
</comment><comment author="sujoysett" created="2013-01-29T08:40:18Z" id="12825110">Hi, any updates for this feature in ES 0.20.\* ?
</comment><comment author="kimchy" created="2013-01-29T08:41:22Z" id="12825148">This feature is not available for `0.20.x` release, and will probably won't make it to 0.21.
</comment><comment author="pcarrier" created="2013-02-20T03:19:45Z" id="13813772">:thumbsup: (and sorry for the noise)
</comment><comment author="shumkov" created="2013-04-10T16:00:00Z" id="16183620">:+1: 
</comment><comment author="MrMoins" created="2013-04-12T08:27:03Z" id="16281337">:+1:
</comment><comment author="sebhomengo" created="2013-04-16T08:49:43Z" id="16433278">:+1: 
</comment><comment author="jackatkoeln" created="2013-04-18T08:06:52Z" id="16562903">+1
</comment><comment author="PatrickSauts" created="2013-04-19T00:29:08Z" id="16621326">+1
</comment><comment author="PatrickSauts" created="2013-04-19T16:28:44Z" id="16663275">Also adding child parent relation within bulk queries seems like a good idea
</comment><comment author="AnaMariaF" created="2013-04-19T16:30:46Z" id="16663398">+1
</comment><comment author="remithieblin" created="2013-04-19T16:31:03Z" id="16663413">+1
</comment><comment author="airrr" created="2013-04-19T17:05:37Z" id="16665190">+1
</comment><comment author="khong07" created="2013-04-22T22:20:33Z" id="16827612">do you have already a plan for this feature?
+100 (for all my collegue too ;) )
</comment><comment author="pecke01" created="2013-04-23T11:37:22Z" id="16852975">Since people seems so eager at using it. I have made a bulkupdate that was inspired by Imotov that you can find here: https://github.com/episerver/elasticsearch/tree/BulkUpdate Please feel free to use it. But please stop with the plus ones. 
</comment><comment author="jprante" created="2013-04-23T13:09:13Z" id="16856729">@pecke01 Have you tested concurrent bulk updates against multiple nodes and shards, and permuting positions of documents in the bulk requests?

From what I understand of the source, it will not work unless you use exactly one client with exactly one concurrent request (that is only sequential bulk indexing, no async operations) and in the bulk requests, there must not be duplicate document IDs.
</comment><comment author="pecke01" created="2013-04-23T13:23:14Z" id="16857435">@Jprante I have not tested it against concurrent bulk updates against multiple nodes and shards. It works well for our use case with one client handling the bulks. It can from what I have tested support the same document Id in the same bulk requests.

It is not in anyway perfect. I know from talking to imotov that it is more work needed to support it properly. This is the reason I haven't done any pull request as well.

I thought I put it up here so people can use it if they want to. Should have commented it better.
</comment><comment author="martijnvg" created="2013-05-10T14:22:56Z" id="17722920">Support for update items in bulk api was added with issue #2982 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow for easier implementation of fine-grained security</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1984</link><project id="" key="" /><description>I posted the following on the mailing list: https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/lAmxGvP3pos

I'd like to suggest an implementation which allows override "fields" or "partial_fields" in order to accomplish filtering of certain fields (which are used for fine-grained security).

For example:
1. if a user passes in a query which has no "fields" or "partial_fields" defined, use a predefined "partial_fields" to prevent sensitive fields from appearing in search results.
2. if a user passes a query which has "fields" or "partial_fields", then merge or override fields available in search results.

In other posts I have seen the suggestion to use indexed fields to accomplish fine-grained security. This works well except for the above scenario for trying to filter security-centric fields from search results.

I look forward to hearing about an existing feature or new feature to accomplish the above.
</description><key id="4766187">1984</key><summary>Allow for easier implementation of fine-grained security</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">digitalsanctum</reporter><labels /><created>2012-05-26T14:06:38Z</created><updated>2015-04-08T13:04:30Z</updated><resolved>2015-04-08T13:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-30T08:58:22Z" id="6005320">It can be something that can be allowed to be defined on the index level settings.
</comment><comment author="digitalsanctum" created="2012-05-30T10:45:58Z" id="6006895">Index level settings would be a good first step. Ideally, I'd suggest document-level security using a combination of query filters (designated as security filters) and a setting to always exclude certain fields from results on a per index basis.
</comment><comment author="clintongormley" created="2015-04-08T13:04:30Z" id="90908269">Closing in favour of #10174
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unable to get/delete by ID when saved with routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1983</link><project id="" key="" /><description>Hi,

It seems like if you save a document by specifying the route in the URL you can not address the document by it's ID.
I found the problem in debian squeeze and then made the following test with a fresh installation in OSX 10.7.4, with both 0.19.4 and the latest snapshot.

Without routing everything is fine:

&lt;pre&gt;
% curl -XPOST 'http://localhost:9200/test/test/1' -d '{"foo":"bar"}'
{"ok":true,"_index":"test","_type":"test","_id":"1","_version":1}
% curl -XGET 'http://localhost:9200/test/test/1'
{"_index":"test","_type":"test","_id":"1","_version":1,"exists":true, "_source" : {"foo":"bar"}}
&lt;/pre&gt;

With routing it saves fine:

&lt;pre&gt;
% curl -XPOST 'http://localhost:9200/test/test/2?routing=test' -d '{"foo":"bar"}'
{"ok":true,"_index":"test","_type":"test","_id":"2","_version":1}
&lt;/pre&gt;

can not get by id:

&lt;pre&gt;
% curl -XGET 'http://localhost:9200/test/test/2'
{"_index":"test","_type":"test","_id":"2","exists":false}
&lt;/pre&gt;

can not delete by id:

&lt;pre&gt;
% curl -XDELETE 'http://localhost:9200/test/test/2'
{"ok":true,"found":false,"_index":"test","_type":"test","_id":"2","_version":1}
&lt;/pre&gt;

but the document is there:

&lt;pre&gt;
% curl -XGET 'http://localhost:9200/test/test/_search?pretty=1&amp;q=*'
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 1.0, "_source" : {"foo":"bar"}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "2",
      "_score" : 1.0, "_source" : {"foo":"bar"}
    } ]
  }
}&lt;/pre&gt;
</description><key id="4761978">1983</key><summary>unable to get/delete by ID when saved with routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bictorman</reporter><labels /><created>2012-05-25T22:36:24Z</created><updated>2012-05-25T23:45:46Z</updated><resolved>2012-05-25T23:43:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-25T23:43:09Z" id="5942711">Its not really a bug, you need to specify the routing when you do a GET, otherwise the id is used to route the request to the relevant shard. Btw, questions on the mailing list, if its a problem, we then open specific issue(s) for it.
</comment><comment author="bictorman" created="2012-05-25T23:45:46Z" id="5942744">I see. thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient does not load plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1982</link><project id="" key="" /><description /><key id="4761734">1982</key><summary>TransportClient does not load plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-25T22:13:43Z</created><updated>2012-05-25T22:22:07Z</updated><resolved>2012-05-25T22:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>When using the "and" or "or" filter, "fields" not being recognized and all _source being returned.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1981</link><project id="" key="" /><description>On elasticsearch build e50fffdaeb (May 24, 2012) the following query will return all the _source data instead of just the fields in the "fields" .

https://gist.github.com/2790748

Removing the "and" filter and just using the "term" filter works. Removing the filter entirely works and only the field requested for each item is returned.
</description><key id="4761351">1981</key><summary>When using the "and" or "or" filter, "fields" not being recognized and all _source being returned.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels /><created>2012-05-25T21:41:46Z</created><updated>2014-07-08T15:04:38Z</updated><resolved>2014-07-08T15:04:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-25T23:51:27Z" id="5942796">The structure of the filter you have is incorrect, you place the _cache element outside of the `and` filter which messes up the parsing (we should throw a proper exception in this case... . If you want to enable caching on an `and` filter, you need to have the following format:

```
curl localhost:9200/test/_search -d '{
    "from": 0,
    "size": 999,
    "filter": {
        "and": {
            "filters" : [
                {
                    "term": {
                        "user_role_permission": "administrator"
                    }
                }
            ],
            "_cache" : true
        }
    },
    "fields": "value"
}'
```
</comment><comment author="bryangreen" created="2012-05-26T00:59:22Z" id="5943325">Fortunately, not a bug in elasticsearch. 

Do you want a separate ticket for the proper exception?

The documentation is actually straight-forward for this issue.
http://www.elasticsearch.org/guide/reference/query-dsl/and-filter.html
</comment><comment author="kimchy" created="2012-05-26T14:45:05Z" id="5947377">Lets keep this one open, once its fixed, we can change its title possibly, make sense?
</comment><comment author="bryangreen" created="2012-05-26T14:45:53Z" id="5947379">Sounds like a plan. Thanks for your time. I'll watch this issue.
</comment><comment author="clintongormley" created="2014-07-08T15:04:37Z" id="48349721">This has been fixed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch goes to periodically shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1980</link><project id="" key="" /><description>Hello all. 
We had next problem. Periodically elasticsearch write next in log and 
shutdown: 
&lt;code&gt;
[2012-05-23 23:53:24,099][INFO ][discovery                ] [Mary Walker] 
elasticsearch/7qxiK8tFRaCbw5B81bo_iA 
[2012-05-23 23:53:24,101][INFO ][http                     ] [Mary Walker] 
bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address 
{inet[/XX.XX.XX.XX:9200]} 
[2012-05-23 23:53:24,101][INFO ][node                     ] [Mary Walker] 
{0.19.3}[8971]: started 
[2012-05-23 23:54:29,133][INFO ][node                     ] [Mary Walker] 
{0.19.3}[8971]: stopping ... 
[2012-05-23 23:54:29,281][INFO ][node                     ] [Mary Walker] 
{0.19.3}[8971]: stopped 
[2012-05-23 23:54:29,281][INFO ][node                     ] [Mary Walker] 
{0.19.3}[8971]: closing ... 
[2012-05-23 23:54:29,288][INFO ][node                     ] [Mary Walker] 
{0.19.3}[8971]: closed 
&lt;/code&gt;
Why he doing it? 
</description><key id="4755542">1980</key><summary>elasticsearch goes to periodically shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">swood</reporter><labels /><created>2012-05-25T15:21:51Z</created><updated>2012-05-25T21:00:39Z</updated><resolved>2012-05-25T20:55:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-25T20:55:51Z" id="5940033">I answered it on the mailing list, closing this issue. If there is a problem, we can open a specific one.
</comment><comment author="swood" created="2012-05-25T21:00:39Z" id="5940126">Sorry, I already read your previous comment about same problem. I already to increase limit max_open_files, but it is not have any effect. What I can do to diagnose cause problem?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NumberFormatException when mixing string and integer fields in terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1979</link><project id="" key="" /><description>The following works with 0.18.5, but gives a NumberFormatException with 0.19.4:

`$ curl -XPOST localhost:9200/test -d '{"mappings": {"test": {"propertes": {"text": {"type": "string"}, "number": {"type": "integer"}}}}}'`

`$ curl -XPUT localhost:9200/test/test/1'-d '{"text": "a", "number": 1}`

`$ curl -XGET localhost:9200/test/test/_search?pretty=true -d '{"query":{"query_string":{"query":"a"}}, "filter":{"terms":{"text":["a"], "number":[1]}}}'`

Error:

{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[-3Fhl3bkQECOzrbQ1FuWvQ][test][1]: SearchParseException[[test][1]: query[],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"a\"}}, \"filter\":{\"terms\":{\"text\":[\"a\"], \"number\":[1]}}}]]]; nested: NumberFormatException[For input string: \"a\"]; }{[-3Fhl3bkQECOzrbQ1FuWvQ][test][0]: SearchParseException[[test][0]: query[],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"a\"}}, \"filter\":{\"terms\":{\"text\":[\"a\"], \"number\":[1]}}}]]]; nested: NumberFormatException[For input string: \"a\"]; }{[-3Fhl3bkQECOzrbQ1FuWvQ][test][4]: SearchParseException[[test][4]: query[],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"a\"}}, \"filter\":{\"terms\":{\"text\":[\"a\"], \"number\":[1]}}}]]]; nested: NumberFormatException[For input string: \"a\"]; }{[-3Fhl3bkQECOzrbQ1FuWvQ][test][3]: SearchParseException[[test][3]: query[],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"a\"}}, \"filter\":{\"terms\":{\"text\":[\"a\"], \"number\":[1]}}}]]]; nested: NumberFormatException[For input string: \"a\"]; }{[-3Fhl3bkQECOzrbQ1FuWvQ][test][2]: SearchParseException[[test][2]: query[],from[-1],size[-1]: Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"a\"}}, \"filter\":{\"terms\":{\"text\":[\"a\"], \"number\":[1]}}}]]]; nested: NumberFormatException[For input string: \"a\"]; }]",
  "status" : 500

I don't see why this should cause a NumberFormatException, as the "a" string is explicitly used only with the "text" field. This breaks one of our applications (using the Tire gem), and prevents us from upgrading to 0.19. 
</description><key id="4753207">1979</key><summary>NumberFormatException when mixing string and integer fields in terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikgrinaker</reporter><labels /><created>2012-05-25T13:16:47Z</created><updated>2012-05-30T14:51:38Z</updated><resolved>2012-05-30T14:51:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-05-25T13:24:59Z" id="5930216">I agree that the error doesn't make much sense, but the problem is not what you think: 

The format of your terms clause is incorrect - you cant specify more than one field in a terms clause. Instead you should have two terms clauses, joined by an `and` or `or` clause.
</comment><comment author="erikgrinaker" created="2012-05-30T14:51:38Z" id="6011835">Indeed - thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move the elasticsearch.in.sh before setting JAVA directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1978</link><project id="" key="" /><description>Parse elasticsearch.in.sh before setting JAVA dir as we could define JAVA_HOME in elasticsearch.in.sh
</description><key id="4728035">1978</key><summary>Move the elasticsearch.in.sh before setting JAVA directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-05-24T07:29:50Z</created><updated>2014-07-16T21:55:22Z</updated><resolved>2012-05-24T10:38:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-24T10:38:26Z" id="5897149">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Facet parameters (exclude, size, etc.) support for Terms Stats Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1977</link><project id="" key="" /><description>It would be useful if Terms Stats facet supported Terms facet parameters, like "exclude", "size"
Possibly related to #1776
Thank you.
</description><key id="4727617">1977</key><summary>Terms Facet parameters (exclude, size, etc.) support for Terms Stats Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darklow</reporter><labels /><created>2012-05-24T06:54:35Z</created><updated>2014-07-08T15:03:32Z</updated><resolved>2014-07-08T15:03:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T15:03:32Z" id="48349526">Now possible with aggregations. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get mapping with no index specified on an empty cluster returns 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1976</link><project id="" key="" /><description /><key id="4722802">1976</key><summary>Get mapping with no index specified on an empty cluster returns 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-23T22:56:07Z</created><updated>2012-05-23T22:58:19Z</updated><resolved>2012-05-23T22:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Running ES on OpenVZ instance fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1975</link><project id="" key="" /><description>After clean install on Vmware Client, ES runs successfully.

After clean install (the same Ubuntu distrib, the same ES version) on VPS running OpenVZ, ES fails on run. 

After setting ES_USER to root, it starts correctly. I suppose that issue is related to sigar library and user permissions in OpenVZ.
</description><key id="4722532">1975</key><summary>Running ES on OpenVZ instance fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tomekkup</reporter><labels /><created>2012-05-23T22:36:00Z</created><updated>2013-06-07T15:44:03Z</updated><resolved>2013-06-07T15:44:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-24T10:39:11Z" id="5897161">What is the failure that you get?
</comment><comment author="tomekkup" created="2012-05-24T10:54:48Z" id="5897403">some ubuntu kernels on VPS blocks access to /etc/null device to non root users. that's why init script fails to run
</comment><comment author="dedico" created="2012-07-08T09:47:50Z" id="6830056">Not sure if that helps -&gt; OpenVZ has no swap and from my experience is not recommended when using Java based applications. Move to Xen, much better performance.
</comment><comment author="kimchy" created="2012-07-09T19:50:35Z" id="6857537">@tomekkup you can simply remove the sigar libs, what failure are you getting?
</comment><comment author="tomekkup" created="2012-07-11T08:38:36Z" id="6901041">Guys,
I already checked it out. It was a problem with VPS provider. Big beer for 'dedico' ! Never try to run any Java apps on OpenVZ
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add more fuzzy options in different queries (text, query_string/field)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1974</link><project id="" key="" /><description>- Add `fuzzy_rewrite` parameter to `text` query (follows the same rewrite format).
- Add `rewrite` to `fuzzy` query
- Add `fuzzy_rewrite`, `fuzzy_max_expansions` to `query_string` and `field` queries.
</description><key id="4722399">1974</key><summary>Query DSL: Add more fuzzy options in different queries (text, query_string/field)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-23T22:28:18Z</created><updated>2012-05-23T22:34:01Z</updated><resolved>2012-05-23T22:34:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add max_boost to custom_filters_score to cap boosting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1973</link><project id="" key="" /><description /><key id="4721354">1973</key><summary>Query DSL: Add max_boost to custom_filters_score to cap boosting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-23T21:31:22Z</created><updated>2012-05-23T21:31:49Z</updated><resolved>2012-05-23T21:31:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: term/terms filter performance improvement (bulk reading)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1972</link><project id="" key="" /><description>Improve the performance of term and terms filters by using bulk reading of the docs.
</description><key id="4719366">1972</key><summary>Query DSL: term/terms filter performance improvement (bulk reading)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-23T19:53:05Z</created><updated>2012-05-23T19:54:40Z</updated><resolved>2012-05-23T19:54:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: text query to support minimum_should_match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1971</link><project id="" key="" /><description>Support `minimum_should_match` (similar to `query_string` and `field`).
</description><key id="4691974">1971</key><summary>Query DSL: text query to support minimum_should_match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-22T14:46:40Z</created><updated>2012-05-22T14:47:04Z</updated><resolved>2012-05-22T14:47:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add null_value and existence to missing filter, filtering both null values and missing fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1970</link><project id="" key="" /><description>Add a `null_value` and `existence` flag to `missing` filter. `existence` will filter out docs with no values indexed, `null_value` will also filter out docs that have the configured null values. Defaults are `existence` set to `true`, and `null_value` set to `false.
</description><key id="4691669">1970</key><summary>Query DSL: Add null_value and existence to missing filter, filtering both null values and missing fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-22T14:31:02Z</created><updated>2012-05-22T14:40:36Z</updated><resolved>2012-05-22T14:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add null_value and existence to missing filter, filtering both null values and missing fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1969</link><project id="" key="" /><description>Add a `null_value` and `existence` flag to `missing` filter. `existence` will filter out docs with no values indexed, `null_value` will also filter out docs that have the configured null values. Defaults are `existence` set to `true`, and `null_value` set to `false.
</description><key id="4691667">1969</key><summary>Query DSL: Add null_value and existence to missing filter, filtering both null values and missing fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-22T14:31:02Z</created><updated>2012-07-06T09:47:35Z</updated><resolved>2012-07-06T09:47:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-07-06T09:47:35Z" id="6801154">Grr, this one was already implemented...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for shard balancing based on index size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1968</link><project id="" key="" /><description>It would be very useful to have support for shard balancing based on index size or number of documents.

The current balance algorithm can lead to some strange shard distributions.

I currently have two indices: 
- History. Empty 0 documents (0B);
- Active. 60M documents (79GB).

I have 4 nodes:
- Node 1: 1 shard of history; 19 shards of active;
- Node 2: 19 shards of history: 2 shards of active;
- Node 3: 1 shard 19 shards of active;
- Node 4: 19 shards of history; 0 shards of active;

So basically i have two machines sleeping and two working =).

Also if the distribution was made at the index level it might be easier?
</description><key id="4675753">1968</key><summary>Support for shard balancing based on index size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">MagmaRules</reporter><labels><label>feature</label></labels><created>2012-05-21T17:44:59Z</created><updated>2015-06-06T18:50:08Z</updated><resolved>2014-07-08T15:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2012-06-12T17:37:10Z" id="6277124">+1
Thought I think one then needs to include other factors, like the power of the server, its current/historical hotness/load, etc.
</comment><comment author="bd808" created="2012-07-06T03:35:55Z" id="6796594">+1

Something like a "StorageSizeAllocationDecider" that would let you set a property telling the cluster to try and keep storage on all nodes within N% of each other would be a great start. My 8 node/64 shard index currently has one node with ~50% of the data by volume due to it randomly being assigned 4 of the 5 largest shards in the index.
</comment><comment author="bd808" created="2012-07-06T03:43:51Z" id="6796700">@kimchy I would volunteer to take a shot at implementing this if you could point me in the right direction for figuring out how to get the set of ShardStatus responses I'd need to know how much storage space each node in the cluster is using.
</comment><comment author="TronPaul" created="2013-02-11T19:04:54Z" id="13396585">Would also like to see this done as well.  I'll continue poking around and trying to get the size into a WeightFunction.
</comment><comment author="brusic" created="2013-02-12T18:00:06Z" id="13446091">There are a couple of options available. Note: I have not used either of them.

New allocator committed to main. Has not been mentioned anywhere by the ES teamL
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java

Third-party plugin:
https://github.com/sonian/elasticsearch-equilibrium

Neither will support every use case, but perhaps the code will help someone create their own.
</comment><comment author="TronPaul" created="2013-02-13T15:58:12Z" id="13501044">I had been looking at that new Allocator, didn't know that was not the default allocation scheme.  Adding parameters to `WeightFunction` looks straightforward enough, but how to correctly pull size information in that context (and where to do it) is the difficult part, and what I'm currently trying to puzzle out.
</comment><comment author="s1monw" created="2013-02-13T21:16:29Z" id="13518566">@TronPaul BalancedShardsAllocator is the default in master. I think this is a pretty interesting usecase you are describing. I will see if I can come up with a PoC for this.
</comment><comment author="kimchy" created="2013-02-13T21:18:27Z" id="13518651">To add to @s1monw notes, currently, we decided to keep the weight function "closed" in order to get it ironed out. Once we do, we are going to both start adding more interesting weights (like size of shards, for example), and possibly allow it to be pluggable as well.
</comment><comment author="s1monw" created="2013-02-13T21:53:21Z" id="13520419">thanks @kimchy for adding more insight...
</comment><comment author="MagmaRules" created="2013-02-13T22:13:37Z" id="13521488">Thanks for the update =)
</comment><comment author="clintongormley" created="2014-07-08T15:03:09Z" id="48349452">Enabled in #6201 and #3637. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Using _default_ mapping _routing mapping definition fails to apply when introducing type through indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1967</link><project id="" key="" /><description /><key id="4661640">1967</key><summary>Mapping: Using _default_ mapping _routing mapping definition fails to apply when introducing type through indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-20T18:17:33Z</created><updated>2012-05-20T19:35:15Z</updated><resolved>2012-05-20T19:35:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Error in Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1966</link><project id="" key="" /><description>I'm using elastic search for my application and it was working perfectly but suddenly without me touching the code, i got this error:

Execution exception
IllegalStateException occured : Please re-index, not all indexed items are available in the database

In {module:elasticsearch-0.4}/app/controllers/elasticsearch/ElasticSearchController.java (around line 366)

362:
            Query&lt;M&gt; query = (Query&lt;M&gt;) ElasticSearch.query(qb, entityClass);
363:
            // FIXME Currently we ignore the orderBy and order fields
364:
            query.from((page - 1) \* getPageSize()).size(getPageSize());
365:
            query.hydrate(true);
366:
               return query.fetch();
367:

368:

369:
        }
370:

371:
        /**
372:
         \* Find by id.
This exception has been logged with id 6adhkiajc
</description><key id="4659182">1966</key><summary>Error in Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coder23</reporter><labels /><created>2012-05-20T10:17:49Z</created><updated>2012-05-20T15:12:55Z</updated><resolved>2012-05-20T15:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-20T15:12:55Z" id="5809570">This exception is not logged by elasticsearch, so check where you are getting it from. Also, please don't post questions on the issues list, send it to the mailing list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator: Registering (indexing) a new percolator query will still be stored in memory if actually indexing it fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1965</link><project id="" key="" /><description>We do the registration to the in memory queries at a point where we haven't yet actually indexed it. We should do it after, and under the doc "lock".
</description><key id="4655677">1965</key><summary>Percolator: Registering (indexing) a new percolator query will still be stored in memory if actually indexing it fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-19T17:35:34Z</created><updated>2012-05-19T17:36:14Z</updated><resolved>2012-05-19T17:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping: default mapping with dynamic templates can cause them to double on each restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1964</link><project id="" key="" /><description /><key id="4633158">1964</key><summary>Mapping: default mapping with dynamic templates can cause them to double on each restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-17T22:10:34Z</created><updated>2012-06-13T12:36:40Z</updated><resolved>2012-05-17T22:11:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lindstromhenrik" created="2012-06-13T12:36:40Z" id="6299666">A consequence of this issue is that it also causes indices to be "refreshed" as the master node sends "double mappings" to its slaves forcing them to refresh the index as the master-mapping does not equal the one that is currently registered on the slaves (as the slave mapping is free of doubles). When using dynamic mappings on clusters with many indices this can cause significant delays when creating new indices.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return timestamp value on indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1963</link><project id="" key="" /><description>It would be good to have the timestamp value that is generated when you index a doc returned by the index api, so that we can update the object within the application to have the correct timestamp, in case eg we want to cache it.

The only way to do this at the moment is to set the timestamp manually before indexing the doc, which may or may not be correct, depending on how much delay there is in indexing
</description><key id="4624471">1963</key><summary>Return timestamp value on indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-05-17T14:06:30Z</created><updated>2014-07-03T19:23:04Z</updated><resolved>2014-07-03T19:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2012-05-23T16:14:35Z" id="5877227">Just to be clear, are you suggesting that IndexResponse should have another slot for `_timestamp` that shows up in the json returned from an index request?
</comment><comment author="clintongormley" created="2012-05-23T16:22:09Z" id="5877440">Yes, in the same way that version, index and id show up.  Perhaps only available on request
</comment><comment author="drewr" created="2012-05-23T17:05:25Z" id="5878582">Maybe something like the above patch?  Making it optional would complicate the code needlessly.  I can't imagine kimchy would object to the negligible perf hit of having the timestamp member on IndexResponse, but you never know. :-)

```
 % curl -s -XPUT localhost:9200/test/foo/1\?timestamp=1337791769960\&amp;pretty=1 -d '{"yo":"brah"}'; echo
 {
   "ok" : true,
   "_index" : "test",
   "_type" : "foo",
   "_id" : "1",
   "_version" : 12,
   "_timestamp" : 1337791769960
 }
 % curl -s -XPUT localhost:9200/test/foo/2\?pretty=1 -d '{"yo":"brah"}'; echo
 {
   "ok" : true,
   "_index" : "test",
   "_type" : "foo",
   "_id" : "2",
   "_version" : 1,
   "_timestamp" : 1337792643440
 }
 % 
```
</comment><comment author="clintongormley" created="2012-05-24T08:23:46Z" id="5895090">@drewr looks good to my inexpert eye. What happens if the _timestamp field is not enabled (which it isn't by default)?
Does it return `null` or does it auto-generate a timestamp regardless?
</comment><comment author="Paikan" created="2012-05-24T09:38:34Z" id="5896307">@clintongormley the timestamp value is auto-generated no matter the configuration of the _timestamp field
</comment><comment author="spinscale" created="2013-07-05T10:35:58Z" id="20512586">any reason to include/not to include the patch? Seems a little stale here :)
</comment><comment author="clintongormley" created="2014-07-03T19:23:04Z" id="47973551">You can request the `_timestamp` with the `fields` parameter, as long as it is mapped as `{ "store": true}`.

closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default params from /etc/default/elasticsearch ignored by startup scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1962</link><project id="" key="" /><description>ES installed on Ubuntu 11.04 from deb package. Works but params from 'default file' seems to be ignored.

After setting some params in /etc/default/elasticsearch file, they're visible in /etc/init.d/elasticsearch script, but after calling start-stop-daemon they seems to be lost in script /usr/share/elasticsearch/bin/elasticsearch
</description><key id="4621589">1962</key><summary>Default params from /etc/default/elasticsearch ignored by startup scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tomekkup</reporter><labels /><created>2012-05-17T10:08:03Z</created><updated>2013-05-24T21:57:16Z</updated><resolved>2013-05-24T21:57:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-29T17:36:27Z" id="5990236">It seems like it is getting sourced: https://github.com/elasticsearch/elasticsearch/blob/master/src/deb/init.d/elasticsearch#L86, can you maybe check the latest version, and see if it works?
</comment><comment author="spinscale" created="2013-05-24T12:40:28Z" id="18402374">Are you still having these problems with current versions of the debian package? If so can you provide more information? If not, please close this issue. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get API: Allow to provide a parent value which automatically set the routing value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1961</link><project id="" key="" /><description /><key id="4615327">1961</key><summary>Get API: Allow to provide a parent value which automatically set the routing value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-16T22:27:59Z</created><updated>2012-05-16T22:40:48Z</updated><resolved>2012-05-16T22:40:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Templates settings provided in a config file fails to load properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1960</link><project id="" key="" /><description>See repro here: https://gist.github.com/2705752.
</description><key id="4614397">1960</key><summary>Index Templates settings provided in a config file fails to load properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-16T21:36:10Z</created><updated>2012-05-16T22:01:09Z</updated><resolved>2012-05-16T22:01:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping: Allow to specify enabled set to false on a property without specifying the type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1959</link><project id="" key="" /><description>It will default to `object` type, which, if not enabled, can also handle "core" values.
</description><key id="4611673">1959</key><summary>Mapping: Allow to specify enabled set to false on a property without specifying the type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-16T19:33:23Z</created><updated>2012-05-16T19:33:50Z</updated><resolved>2012-05-16T19:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.4.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1958</link><project id="" key="" /><description /><key id="4611027">1958</key><summary>Upgrade to Netty 3.4.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-16T18:58:19Z</created><updated>2012-05-16T18:58:40Z</updated><resolved>2012-05-16T18:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms facet on string field with "-" problem/bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1957</link><project id="" key="" /><description>Hi, 
I have a problem with terms facet on a field that has "-" in its value. My facet is:

``` json
"facets" : {
   "my_facet" : { "terms" : {"field" : "my_field"} }
}
```

And the result of the facet:

``` json
"terms": [
    {
          "term": "kwidzyn.pl"
          "count": 1
    },
    {
          "term": "forum.e"
          "count": 1
    }
]
```

while the value of my_field in document is "forum.e-kwidzyn.pl" and no other document matches the query. That field is defined in schema like this:

``` json
"my_field": {
    "include_in_all": false,
    "index": "not_analyzed",
    "type": "string"
}
```

Is this a bug or am I missing something?
</description><key id="4607317">1957</key><summary>Terms facet on string field with "-" problem/bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">SakulK</reporter><labels /><created>2012-05-16T15:49:08Z</created><updated>2013-06-11T18:12:45Z</updated><resolved>2013-06-11T18:12:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Rzulf" created="2012-05-24T12:25:24Z" id="5898814">Same issue here, any fix or workaround?
</comment><comment author="Rzulf" created="2012-05-30T16:30:36Z" id="6014525">Anyone?
</comment><comment author="Simek" created="2012-06-01T10:59:46Z" id="6058013">I've got similar problem, so is there any fix for that?
</comment><comment author="damienalexandre" created="2013-06-11T15:22:47Z" id="19268800">Running into a similar issue with ES 0.9.

I've writed a test case: https://gist.github.com/damienalexandre/5757742 but... it work as expected! I have no idea why, in my application, the `Gypsy - Queens` term produce 3 facet results:
- `Gypsy`
- `-` !!!
- `Queens`

I will investigate :cat: 
</comment><comment author="spinscale" created="2013-06-11T16:06:22Z" id="19271839">@SakulK 

this sample works for me[tm]

```
curl -X PUT localhost:9200/facettest/
curl -X PUT localhost:9200/facettest/test/_mapping -d '{ "facettest": { "properties": { "facetField" : { "type" : "string", "index" : "not_analyzed" } }  } }'
curl -X PUT localhost:9200/facettest/test/1 -d '{"facetField":"forum.e-kwidzyn.pl"}'
curl -X POST 'localhost:9200/facettest/test/_search?size=0' -d '{ "facets": { "myFacet" : { "terms" : { "field":"facetField" } } } }'
```

as you roughly did the same, can you ensure by checking the mapping, that the field is really not_analyzed?
</comment><comment author="spinscale" created="2013-06-11T16:11:16Z" id="19272216">@damienalexandre this is expected behaviour when you analyze the field with the standard analyzer. The field then gets splitted into several parts, which are treated as several parts as well when faceting. This does not happen, when the field is not analyzed at all.

I dont see any issue in your gist, but I'll be happy to get pointed at and try to help :-)
</comment><comment author="damienalexandre" created="2013-06-11T16:28:58Z" id="19273346">Ok guys, look like I was doing my tests wrong, and this issue convinced me there were an issue...

As my gist show it, there is no bug in ES about this :] Maybe we can close this as the issue is 1 yo.
</comment><comment author="s1monw" created="2013-06-11T18:12:45Z" id="19281590">moving out.. thanks for clarifying
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide ability to apply query side synonym changes in real-time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1956</link><project id="" key="" /><description>For search analyzers that contain a synonym type filter, it would be great to be able to make changes to the synonym file and have those changes recognized immediately by the associated analyzers without having to close/open the index. Ideally, the synonym cache would be refreshed automatically with changes to the corresponding synonym file. Another option would be to provide API call that could be made to refresh cache on demand. For most ecommerce sites, synonyms provide great value, and therefore are used quite frequently, and it is critical that these changes be implemented without any search outage.
</description><key id="4604148">1956</key><summary>Provide ability to apply query side synonym changes in real-time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">my3sons</reporter><labels><label>:Analysis</label><label>discuss</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2012-05-16T13:15:24Z</created><updated>2015-11-13T10:16:15Z</updated><resolved>2015-11-13T10:16:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2012-07-24T23:26:06Z" id="7236602">Commenting on this issue to provide the back story: http://elasticsearch-users.115913.n3.nabble.com/Query-side-synonyms-and-boosts-td3975715.html

Igor Motov created an abstracted resource watcher service that can be used to update the synonym map: https://github.com/elasticsearch/elasticsearch/pull/2103

If this feature makes it into the master branch, I will attempt to create a fix for this issue.
</comment><comment author="jacobevans" created="2012-08-10T01:02:14Z" id="7631620">+1 on this.  

For non-ecommerce sites, as well, the ability to update the query-side synonym list would be very valuable. 
</comment><comment author="jacobevans" created="2013-02-20T18:48:35Z" id="13849009">@kimchy, I asked you about this at the San Francisco ES training a while back, and you mentioned there might be a solution soon.  Any news?
</comment><comment author="brusic" created="2013-02-20T18:55:38Z" id="13849398">I created my own synonym token filter that refreshes its values from a database at a predetermined interval. Been meaning to open-source it, but it would take some cleanup first to remove code that only applies to my company (database handling). Not too hard to do, just need to modify SynonymTokenFilterFactory and make a plugin around it.
</comment><comment author="kimchy" created="2013-02-20T19:09:18Z" id="13850163">@jacobevans not yet, its still on our plate, hopefully will make it to the next version.
</comment><comment author="jacobevans" created="2013-02-21T17:41:55Z" id="13902252">@kimchy, thanks for the update.
@brusic That solution would fit my use case as well.  Let me know if you'd like a hand with it, I'd be happy to help.
</comment><comment author="lindstromhenrik" created="2013-06-12T08:49:38Z" id="19313314">I wrote a synonym token filter that reloads the synonyms file at given intervals from disk that you can use as a temporary solution until we can use the resource watcher in general: https://github.com/lindstromhenrik/elasticsearch-analysis-file-watcher-synonym 
</comment><comment author="TevinLord" created="2014-03-13T20:21:15Z" id="37581790">@lindstromhenrik does your plugin work with the 1.0 release?
@brusic Did you ever release your plugin?
</comment><comment author="brusic" created="2014-03-13T21:40:32Z" id="37590339">My apologies, but I never did release the plugin. On a positive note, I have improved the plugin greatly so that it can be updated on demand instead of on an interval. I am still on the 0.90 branch, so I will try to update it this coming weekend and finally release it.
</comment><comment author="nizsheanez" created="2014-09-22T04:27:16Z" id="56327753">hello guys, any progress in this thing?
</comment><comment author="clintongormley" created="2015-09-19T17:09:26Z" id="141688552">We currently have no way to distinguish analyzers that would be used at search time only from those that are used at index time.  Changing the synonyms list for index-time analyzers would be a bad idea: you'd end up with incorrect data in your index.

Leaving this open for future discussion
</comment><comment author="brusic" created="2015-09-22T15:56:45Z" id="142332265">My plugin is caught up in the issue Clinton referenced. My production code maintains a static collection of the analyzers, but I found that approach a bit kludgy. I switched to using the IndicesService, but that is where the issue comes into play. 

https://github.com/brusic/refresh-token-filters
</comment><comment author="markharwood" created="2015-11-13T10:16:14Z" id="156387860">We discussed this some more and as Clinton said in his comment above there may be undesirable consequences for indexing if we make it simple for users to switch the definitions on the fly. Of course if you know what you are doing and it is a search-time-only analyzer it is possible to provide a custom plugin as  @lindstromhenrik noted which can safely reload synonym sets, perhaps from a centralised resource.

Given we don't want to make it easy for novice users to do the wrong thing and users who know what they are doing have a work-around the original proposal isn't something we are choosing to add to core.
Sorry, and thanks for raising the discussion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sending a HTTP request to TCP port during recovery results in OutOfMemoryError in Netty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1955</link><project id="" key="" /><description>Sending a HTTP request to the TCP port (eg curl 'http://localhost:9300/') results in an OutOfMemoryError being logged. This appears to be reproducible in 0.19.3 during recovery - the stacktrace logged is:

``` java
52,258][WARN ][transport.netty          ] [nodename] Exception caught on netty layer [[id: 0x00c1df71, /127.0.0.1:50268 =&gt; /127.0.0.1:9300]]
java.lang.OutOfMemoryError: Java heap space
        at org.elasticsearch.common.netty.buffer.HeapChannelBuffer.&lt;init&gt;(HeapChannelBuffer.java:42)
        at org.elasticsearch.common.netty.buffer.BigEndianHeapChannelBuffer.&lt;init&gt;(BigEndianHeapChannelBuffer.java:34)
        at org.elasticsearch.common.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)
        at org.elasticsearch.common.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:69)
        at org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.&lt;init&gt;(DynamicChannelBuffer.java:58)
        at org.elasticsearch.common.netty.buffer.ChannelBuffers.dynamicBuffer(ChannelBuffers.java:221)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:98)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:792)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:94)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:364)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:238)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:38)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
```
</description><key id="4600488">1955</key><summary>Sending a HTTP request to TCP port during recovery results in OutOfMemoryError in Netty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snellm</reporter><labels /><created>2012-05-16T08:58:10Z</created><updated>2013-07-15T16:43:41Z</updated><resolved>2013-07-15T16:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tomekkup" created="2012-05-18T15:26:37Z" id="5788659">have the same issue
</comment><comment author="govale" created="2012-05-18T22:28:20Z" id="5797114">Me too!
</comment><comment author="kimchy" created="2012-05-24T22:07:10Z" id="5918927">Just verifying that you are not thinking that you need to send HTTP requests to port 9300, HTTP requests should go to port 9200. We will fix and protected against it happening, but 9300 is not the port that you should use for HTTP.
</comment><comment author="kimchy" created="2012-05-24T22:40:34Z" id="5919525">One more thing, the fact that you get an OOM does not mean that the server can't handle additional requests, its perfectly fine (in this case), its a recoverable OOM. And, it has nothing to do with recovery..., just sending HTTP over to the transport port.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete child documents when parent is deleted.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1954</link><project id="" key="" /><description>Discussed here:
https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/nfpTXJFePhA
Child documents should be deleted when parent documents is deleted. Especially if it's deleted by ttl.
</description><key id="4600143">1954</key><summary>Delete child documents when parent is deleted.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Avatah</reporter><labels><label>stalled</label></labels><created>2012-05-16T08:35:27Z</created><updated>2015-09-29T18:01:59Z</updated><resolved>2015-09-19T17:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pcdinh" created="2012-06-07T19:02:10Z" id="6184646">Any update on this issue?
</comment><comment author="willtrking" created="2012-10-17T08:08:20Z" id="9519178">+1
</comment><comment author="Mopster" created="2013-04-16T08:59:10Z" id="16433718">Any news on this ?
</comment><comment author="mzafer" created="2013-04-25T19:56:36Z" id="17036548">+1
</comment><comment author="tarang-dawer" created="2013-05-31T09:25:01Z" id="18734708">+1 , any updates ?
</comment><comment author="aseppala" created="2013-08-20T13:12:28Z" id="22943644">+1
</comment><comment author="jugaadi" created="2013-09-26T13:12:23Z" id="25166019">+1
</comment><comment author="synhershko" created="2013-10-24T12:04:29Z" id="26986436">+1

I would also like to see an option where adding a new child resets the TTL of the parent (keep-alive)
</comment><comment author="syedhabib53" created="2014-06-19T10:43:45Z" id="46546185">+1
Any update on this...
</comment><comment author="clintongormley" created="2014-07-18T09:19:59Z" id="49411358">Blocked by #2230 
</comment><comment author="clintongormley" created="2015-09-19T17:05:51Z" id="141688143">Revisiting this issue: I can't seen us implementing this as some kind of built in trigger, especially as we are talking about making the _ttl functionality simpler and less automatic.  

Deleting children of a parent is easy enough to do client side with the delete-by-query plugin, as is deleting expired docs.  I'm going to close this
</comment><comment author="Alireza-DarkMan" created="2015-09-29T07:26:23Z" id="143972081">Hi
I wonder if delete-by-query is deprecated why should we use it? 
</comment><comment author="clintongormley" created="2015-09-29T14:13:39Z" id="144073393">The delete-by-query plugin is not deprecated.
</comment><comment author="Alireza-DarkMan" created="2015-09-29T18:01:59Z" id="144137112">Sorry that was a miss understanding.
Thank you :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update requirements of the deb package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1953</link><project id="" key="" /><description>Virtual packages such as java*-runtime are provided by various java/JRE implementations, which makes it more flexible to use.

Note: I can't say I tested this, but AFAIK it should help. If you can make a test build and link to it on the related ML thread I can try to install it see if it still works with my setup at least.
</description><key id="4593258">1953</key><summary>Update requirements of the deb package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Seldaek</reporter><labels /><created>2012-05-15T21:08:37Z</created><updated>2014-07-16T21:55:22Z</updated><resolved>2012-06-07T13:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-16T19:05:11Z" id="5749065">Here is a test package: http://dl.dropbox.com/u/2136051/elasticsearch-0.20.0.Beta1-SNAPSHOT.deb, can you check it?
</comment><comment author="dhardy92" created="2012-05-31T12:46:29Z" id="6034795">Work for me.
</comment><comment author="chris-rock" created="2012-06-02T16:16:25Z" id="6078671">works well on my side
</comment><comment author="kimchy" created="2012-06-07T13:40:04Z" id="6175962">cool, pushed to 0.19 and master branches
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolate in bulk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1952</link><project id="" key="" /><description>Allow the bulk API to support percolate requests.
</description><key id="4583208">1952</key><summary>percolate in bulk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shadow000fire</reporter><labels /><created>2012-05-15T12:56:11Z</created><updated>2015-06-09T21:26:01Z</updated><resolved>2013-08-12T16:53:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-05-15T13:00:39Z" id="5715148">It does - look for "percolator" on this page: http://www.elasticsearch.org/guide/reference/api/bulk.html
</comment><comment author="shadow000fire" created="2012-05-15T14:17:58Z" id="5716894">According to that page it can only be used as part of an index command.  Am I reading that wrong?  I'm saying I should be able to percolate documents in bulk (NOT as part of an index call).
</comment><comment author="clintongormley" created="2012-05-15T14:54:47Z" id="5717862">Ahhh apologies - that wasn't clear, at least to me :) I'll reopen
</comment><comment author="martijnvg" created="2013-08-12T16:53:10Z" id="22507692">The multi percolate api has just been added via #3488, that allows to bulk percolate requests.
</comment><comment author="webmstr" created="2015-06-09T21:26:01Z" id="110508329">While it might be handy to percolate multiple documents at once, being able to percolate while using the bulk API would be even better.  Right now I have to send the documents twice??
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added statisticalFacet to the testFacets test to show example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1951</link><project id="" key="" /><description>It wasn't really obvious to get correct results of a Statistical Facet with nested documents and a facet filter applied.
updated testFacets test to demonstrate.
</description><key id="4575806">1951</key><summary>added statisticalFacet to the testFacets test to show example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">bdargan</reporter><labels /><created>2012-05-15T01:06:57Z</created><updated>2014-07-16T21:55:23Z</updated><resolved>2013-07-16T10:04:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bdargan" created="2013-07-16T03:57:45Z" id="21019846">CLA has been signed
</comment><comment author="spinscale" created="2013-07-16T10:04:51Z" id="21032408">thx, just included it in https://github.com/elasticsearch/elasticsearch/commit/94fd152eb19ffd632045981e711641f4a042c466

thanks for your contribution!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolate features to match search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1950</link><project id="" key="" /><description>Pretty much all search functionality should be available via percolate.  Things like highlighting, explain, etc. 

Percolate currently round robins among the shards making each percolate call not a distributed process.  It might be nice to gave other options such as a true distributed percolate request.  Think of the scenario where there's ~250,000 registered queries ~1,000 documents percolated per minute.  Allowing users to configure shards, routing, caching, etc could seriously improve percolate performance.
</description><key id="4571751">1950</key><summary>percolate features to match search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shadow000fire</reporter><labels /><created>2012-05-14T20:22:11Z</created><updated>2013-08-26T14:46:28Z</updated><resolved>2013-08-26T14:46:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-26T14:46:28Z" id="23267311">The recent work that has been done in master does pretty much what you describe here from the distributed point of view (#3173). Also highlighting has recently been added (#3574), explain might be added later on.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator Index: Don't index the `query` element in a percolated query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1949</link><project id="" key="" /><description>Currently, we index the `query` element in a percolated query, we shouldn't. Automatically add a `_default_` mapping when the `_percolator` index is created that disables indexing the `query` element.
</description><key id="4552062">1949</key><summary>Percolator Index: Don't index the `query` element in a percolated query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-13T10:50:21Z</created><updated>2012-05-13T11:11:30Z</updated><resolved>2012-05-13T11:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolator: Wrongly using analyzer configured for the actual index on percolator filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1948</link><project id="" key="" /><description>When "indexing"  a percolator query with additional metdata, it gets indexed with the analysis set for the `_percolator` index (which is good). But, when adding a filtering query to the percolator queries to run, they use the analysis of the actual index percolated against, not the `_percolator` index.
</description><key id="4551994">1948</key><summary>Percolator: Wrongly using analyzer configured for the actual index on percolator filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-13T10:35:22Z</created><updated>2012-05-13T10:36:07Z</updated><resolved>2012-05-13T10:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>deb: ES_HEAP_SIZE not exported</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1947</link><project id="" key="" /><description /><key id="4551264">1947</key><summary>deb: ES_HEAP_SIZE not exported</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-13T07:11:17Z</created><updated>2012-05-13T07:11:43Z</updated><resolved>2012-05-13T07:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add option to shutdown node clients during cluster shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1946</link><project id="" key="" /><description>It'd be great if there were an option to shutdown node clients during cluster shutdown. This was previous behavior, but changed with this issue:
https://github.com/elasticsearch/elasticsearch/issues/1939

Thanks!
</description><key id="4550961">1946</key><summary>Add option to shutdown node clients during cluster shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels><label>discuss</label></labels><created>2012-05-13T05:13:28Z</created><updated>2014-07-18T09:18:52Z</updated><resolved>2014-07-18T09:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:59:48Z" id="48348823">@ppearcy what's the use case here?  surely clients would be shut down by the application?
</comment><comment author="ppearcy" created="2014-07-08T15:18:43Z" id="48352292">What I was looking for was more granularity on the nodes shutdown API to optionally allow shutting down of client nodes, as well as, data nodes. This can be convenient during maintenance when I want my front end services to shutdown with the rest of the cluster. 

I don't personally need and probably wouldn't use this behavior anymore since I am using better tooling for deployments (eg, ansible). 

Feel free to close out, unless you think there is value for others. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>debian package, fix init.d script and update default configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1945</link><project id="" key="" /><description>Commit a135c9bd8b4c29c7f147d125762a9a6456cbc35d introduce ES_HEAP_SIZE but do not export it.

Default configuration file have not been updated with it.
</description><key id="4549039">1945</key><summary>debian package, fix init.d script and update default configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benoit-intrw</reporter><labels /><created>2012-05-12T19:55:20Z</created><updated>2014-07-16T21:55:23Z</updated><resolved>2012-05-13T07:12:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-13T07:10:18Z" id="5674493">Yea, I will push a fix with just the heap size export,
</comment><comment author="kimchy" created="2012-05-13T07:12:44Z" id="5674504">See #1947.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo query keeps failing on a specific type/table</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1944</link><project id="" key="" /><description>Using v0.18.7. Problem is that for a given type/table in an index geo search keeps failing. I can load the same data in another type/table and all is fine but even if I delete the type/table and then reload the query fails. Here's the stack trace

&lt;pre&gt;
... node[FA1mzqRITkCwHB_hw6Faxw], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3c02e8fe]
org.elasticsearch.transport.RemoteTransportException: [Death][inet[/193.34.146.144:9300]][search/phase/query]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [...][0]: query[filtered(ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@d40d7f57))-&gt;FilterCacheFilterWrapper(_type:30d6d9aa-0f07-4db3-9ddd-ba615d00ec21)],from[0],size[10]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:238)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:447)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:438)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:358)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1949)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldData$StringTypeLoader.collectTerm(GeoPointFieldData.java:177)
        at org.elasticsearch.index.field.data.support.FieldDataLoader.load(FieldDataLoader.java:55)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldData.load(GeoPointFieldData.java:160)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:51)
        at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:34)
        at org.elasticsearch.index.field.data.FieldData.load(FieldData.java:110)
        at org.elasticsearch.index.cache.field.data.support.AbstractConcurrentMapFieldDataCache.cache(AbstractConcurrentMapFieldDataCache.java:119)
        at org.elasticsearch.index.search.geo.GeoDistanceFilter.getDocIdSet(GeoDistanceFilter.java:114)
        at org.apache.lucene.search.DeletionAwareConstantScoreQuery$DeletionConstantWeight.scorer(DeletionAwareConstantScoreQuery.java:53)
        at org.apache.lucene.search.FilteredQuery.getFilteredScorer(FilteredQuery.java:149)
        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:117)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:577)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:199)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:445)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:426)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:342)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:330)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)
        ... 7 more
&lt;/pre&gt;


Any ideas what causes this problem and how it could be fixed (it's a real pain for us to renames types/tables ...)
</description><key id="4548931">1944</key><summary>Geo query keeps failing on a specific type/table</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rufuspollock</reporter><labels /><created>2012-05-12T19:33:04Z</created><updated>2013-07-15T16:20:10Z</updated><resolved>2013-07-15T16:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-13T07:14:46Z" id="5674509">This failure probably means that you have another type with the mentioned field that is not mapped as geo_point. Note, the fact that you delete a type does not "clean" it completely, since its documents are just marked as deleted in the index. Running optimize with expunge deletes can help (though can be expensive).
</comment><comment author="zachlatta" created="2013-03-06T00:39:37Z" id="14475394">Was this issue ever resolved?
</comment><comment author="spinscale" created="2013-04-15T13:01:27Z" id="16383202">@rgrp can you provide us some gists to reproduce your issue or is the problem solved for you after you ran the optimize with expunge deletes?
</comment><comment author="spinscale" created="2013-07-15T16:20:10Z" id="20981140">closing this due to missing feedback in order to investigate this issue further. Happy to reopen with more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for proper GeoJSON points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1943</link><project id="" key="" /><description>Should be able to have a document like:

&lt;pre&gt;
{
  ...
  location:  {
       "type": "Feature",
      "geometry": {"type": "Point", "coordinates": [102.0, 0.5]},
      "properties": {"prop0": "value0"}
   }
}
&lt;/pre&gt;


And then set in mapping:

&lt;pre&gt;
{
  properties: {
    location: geo_point
  }
}
&lt;/pre&gt;


And then use location for geo queries in the usual way.

Aside: any plans/thoughts on full polygon support in addition to supporting geo points?
</description><key id="4548042">1943</key><summary>Support for proper GeoJSON points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rufuspollock</reporter><labels /><created>2012-05-12T16:48:38Z</created><updated>2013-05-23T07:15:57Z</updated><resolved>2013-05-23T07:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-05-12T19:05:43Z" id="5670955">Regarding the side note, http://www.elasticsearch.org/guide/reference/query-dsl/geo-polygon-filter.html should do polygon geo search afaik. It's not using much GeoJSON syntax though.
</comment><comment author="rufuspollock" created="2012-05-12T19:23:57Z" id="5671099">@jprante but I'm talking about storing polygons not searching for intersection of points with polygons. i.e. i'd like to store polygons and then queries on those (e.g. around intersections - show me all features that intersect or are contained in this bounding box).
</comment><comment author="jprante" created="2012-05-12T19:37:43Z" id="5671189">A nice idea! I don't know if https://github.com/spatial4j/spatial4j is ready to get used by ES, perhaps as a plugin? See also #1486
</comment><comment author="spinscale" created="2013-05-23T07:15:57Z" id="18327168">Supported via `geo_shape` since some time. So closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES Service init script intermittently fails to start on Ubuntu</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1942</link><project id="" key="" /><description>The Elastic Search service init script intermittently fails to start on Ubuntu. The status shows "failed" however, no entries are updated in the log. 

Possibility relevant input
- Heap size set to 3GB on a EC2 instance with 7.5GB RAM.
- The server also runs tomcat, and 2GB allocated for that.
- OS is Ubuntu 11.10
- If /var/log/elasticsearch folder is emptied, the log file isn't even created.
- Found this on a few nodes in a cluster on EC2.
- ElasticSearch version is 0.19.2

Believe it could be related to memory allocation since same config starts at other times.
</description><key id="4541675">1942</key><summary>ES Service init script intermittently fails to start on Ubuntu</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vijayakumark</reporter><labels /><created>2012-05-11T21:32:09Z</created><updated>2013-08-02T13:38:50Z</updated><resolved>2013-04-12T07:01:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-13T07:15:26Z" id="5674512">Nothing in the OS log files regarding why it failed?
</comment><comment author="vijayakumark" created="2012-05-13T10:21:28Z" id="5675308">Yes. Except for the auth log having an entry about switching user to elasticsearch there is no other entry in the OS logs at all. At times, it starts just fine. 

When it doesn't start, If I manually ran the script at /usr/share/elasticsearch/bin/elasticsearch with same parameters, ran just fine.
</comment><comment author="oleiade" created="2013-04-05T06:13:41Z" id="15940508">+1 but in my case, elasticsearch actually nevers starts using the init.d.

After a few digging, I discovered lauching via `sudo  -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch` that their might be a owner/rights problem over `/usr/share/elasticsearch` as it thrown:

``` shell
{0.20.6}: Initialization Failed ...
- ElasticSearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/usr/share/elasticsearch/data/elasticsearch]]
    IOException[failed to obtain lock on /usr/share/elasticsearch/data/elasticsearch/nodes/49]
        IOException[Cannot create directory: /usr/share/elasticsearch/data/elasticsearch/nodes/49]
```

Once I `chwon -R elasticsearch:elasticsearch /usr/share/elasticsearch` the error goes away, but still, the service does not start using the init.d script, and nothing appears in logs.

Any further ideas?
Thanks
</comment><comment author="oleiade" created="2013-04-05T06:41:22Z" id="15941146">Okay, I found out, and write it down here as it might be useful to someone else.
Trying to launch elasticsearch using the same command as the init.d script through the elasticsearch user, I had this error:

``` shell
$ sudo -u elasticsearch /usr/share/elasticsearch/bin/elasticsearch -p /var/run/elasticsearch.pid -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch

leiade@gesicht:~$ {0.20.6}: Initialization Failed ...
- ElasticSearchIllegalStateException[Failed to obtain node lock, is the following location writable?: [/var/elasticsearch/data/pluto]]
    IOException[failed to obtain lock on /var/elasticsearch/data/pluto/nodes/49]
        IOException[Cannot create directory: /var/elasticsearch/data/pluto/nodes/49]
```

Yet another rights problem. After `mkdir /var/elasticsearch &amp;&amp; chown -R elasticsearch:elasticsearch /var/elasticsearch` the init.d script works fine.
</comment><comment author="spinscale" created="2013-04-05T07:15:54Z" id="15942036">hey,

why do you have /var/elasticsearch as your data dir. It should be /var/lib/elasticsearch - did you reconfigure it?

Anyway it should work (if the directory was specified in your configuration). In the current init script, we are setting the permissions and the ownership for those directories in `/etc/init.d/elasticsearch` - maybe you can add `set -x` to the init script and see if the calls changing the permissions happen.
</comment><comment author="oleiade" created="2013-04-05T08:16:51Z" id="15943725">Hey!

thanks for this quick answer!

Yes, I've reconfigured the data dir to point to `/var/elasticsearch/data`, that's why it points there. No problem for me as I'm deploying elasticsearch via puppet, and can easily set the right permissions myself. 

I've added `set -x` to the init script, and here's the result:

```
+ PATH=/bin:/usr/bin:/sbin:/usr/sbin
+ NAME=elasticsearch
+ DESC='ElasticSearch Server'
+ DEFAULT=/etc/default/elasticsearch
++ id -u
+ '[' 0 -ne 0 ']'
+ . /lib/lsb/init-functions
++ FANCYTTY=
++ '[' -e /etc/lsb-base-logging.sh ']'
++ . /etc/lsb-base-logging.sh
+++ LOG_DAEMON_MSG=
+ '[' -r /etc/default/rcS ']'
+ . /etc/default/rcS
++ TMPTIME=0
++ SULOGIN=no
++ DELAYLOGIN=no
++ UTC=yes
++ VERBOSE=no
++ FSCKFIX=yes
+ ES_USER=elasticsearch
+ ES_GROUP=elasticsearch
+ JDK_DIRS='/usr/lib/jvm/java-7-oracle /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/java-6-sun /usr/lib/jvm/java-6-openjdk'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-oracle/bin/java -a -z '' ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk/bin/java -a -z '' ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk-amd64//bin/java -a -z '' ']'
+ JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-7-openjdk-i386//bin/java -a -z /usr/lib/jvm/java-7-openjdk-amd64/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-6-sun/bin/java -a -z /usr/lib/jvm/java-7-openjdk-amd64/ ']'
+ for jdir in '$JDK_DIRS'
+ '[' -r /usr/lib/jvm/java-6-openjdk/bin/java -a -z /usr/lib/jvm/java-7-openjdk-amd64/ ']'
+ export JAVA_HOME
+ ES_HOME=/usr/share/elasticsearch
+ MAX_OPEN_FILES=65535
+ LOG_DIR=/var/log/elasticsearch
+ DATA_DIR=/var/lib/elasticsearch
+ WORK_DIR=/tmp/elasticsearch
+ CONF_DIR=/etc/elasticsearch
+ CONF_FILE=/etc/elasticsearch/elasticsearch.yml
+ '[' -f /etc/default/elasticsearch ']'
+ . /etc/default/elasticsearch
+ PID_FILE=/var/run/elasticsearch.pid
+ DAEMON=/usr/share/elasticsearch/bin/elasticsearch
+ DAEMON_OPTS='-p /var/run/elasticsearch.pid -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch'
+ export ES_HEAP_SIZE
+ export ES_HEAP_NEWSIZE
+ export ES_DIRECT_SIZE
+ export ES_JAVA_OPTS
+ test -x /usr/share/elasticsearch/bin/elasticsearch
+ case "$1" in
+ '[' -z /usr/lib/jvm/java-7-openjdk-amd64/ ']'
+ '[' -n '' -a -z '' ']'
+ log_daemon_msg 'Starting ElasticSearch Server'
+ '[' -z 'Starting ElasticSearch Server' ']'
+ log_use_fancy_output
+ TPUT=/usr/bin/tput
+ EXPR=/usr/bin/expr
+ '[' -t 1 ']'
+ FANCYTTY=0
+ case "$FANCYTTY" in
+ false
+ echo ' * Starting ElasticSearch Server'
 * Starting ElasticSearch Server
+ COL=
+ start-stop-daemon --test --start --pidfile /var/run/elasticsearch.pid --user elasticsearch --exec /usr/lib/jvm/java-7-openjdk-amd64//bin/java
+ log_progress_msg '(already running)'
+ :
+ log_end_msg 0
+ '[' -z 0 ']'
+ '[' '' ']'
+ '[' 0 -eq 0 ']'
+ echo '   ...done.'
   ...done.
+ return 0
+ exit 0
```

Grep can't find any mentions of `chmod` or `chown` in it, so it doesn't seem like the script is modifying permissions or owner in any way; but I'm not a shell script rock star, maybe I've missed a thing :)
Just to be sure, I've reset the initial permissions over /var/elasticsearch to root:root in order to see if the permission change happens, and it's not.
</comment><comment author="spinscale" created="2013-04-05T09:01:52Z" id="15945310">the output is not from a real startup, because elasticsearch is already running (see the last ten lines). Or at least the pid file is still existing.
</comment><comment author="oleiade" created="2013-04-05T09:19:09Z" id="15945957">Woops, my bad, sorry :D
Actually, once starting correctly with `set -x` it seems that, yes, it chowns the directories, but not those I've set up in the config.

``` shell
$ sudo /etc/init.d/elasticsearch &amp;&gt; out
$ cat res | grep "chown"
+ chown elasticsearch:elasticsearch /var/log/elasticsearch /var/lib/elasticsearch /tmp/elasticsearch
+ chown elasticsearch:elasticsearch /var/run/elasticsearch.pid
```
</comment><comment author="spinscale" created="2013-04-05T09:25:25Z" id="15946190">just to make sure:
Did you set `DATA_DIR=/var/elasticsearch` in `/etc/default/elasticsearch` or did you set it in `elasticsearch.yml` - when reading `/etc/default/elasticsearch` the default DATA_DIR setup should be overwritten in the init script, what seems not to be the case judging from your last output.
</comment><comment author="oleiade" created="2013-04-05T09:53:44Z" id="15947384">I've just set it up in `elasticsearch.yml`, here it is:

``` yaml
### MANAGED BY PUPPET ###
---
cluster: 
  name: pluto
node: 
  name: gesicht
path: 
  data: /var/elasticsearch/data
  logs: /var/log/elasticsearch
```

and here's my `/etc/default/elasticsearch`:

``` ini
# Run ElasticSearch as this user ID and group ID
#ES_USER=elasticsearch
#ES_GROUP=elasticsearch

# Heap Size (defaults to 256m min, 1g max)
#ES_HEAP_SIZE=2g

# Heap new generation
#ES_HEAP_NEWSIZE=

# max direct memory
#ES_DIRECT_SIZE=

# Maximum number of open files, defaults to 65535.
#MAX_OPEN_FILES=65535

# Maximum locked memory size. Set to "unlimited" if you use the
# bootstrap.mlockall option in elasticsearch.yml. You must also set
# ES_HEAP_SIZE.
#MAX_LOCKED_MEMORY=unlimited

# ElasticSearch log directory
#LOG_DIR=/var/log/elasticsearch

# ElasticSearch data directory
#DATA_DIR=/var/lib/elasticsearch

# ElasticSearch work directory
#WORK_DIR=/tmp/elasticsearch

# ElasticSearch configuration directory
#CONF_DIR=/etc/elasticsearch

# ElasticSearch configuration file (elasticsearch.yml)
#CONF_FILE=/etc/elasticsearch/elasticsearch.yml

# Additional Java OPTS
#ES_JAVA_OPTS=
```

Should I make sure puppet sets DATA_DIR in `/etc/default/elasticsearch` too then?
</comment><comment author="spinscale" created="2013-04-12T07:01:21Z" id="16278601">This depends on your setup. If you handle chmod/chown yourself, there is no need for updating `/etc/default/elasticsearch` - if you want to have it handled by the init script, you should add it there as well and distribute the file correctly.

As the original report is 11 months old and no useful information has been added and your problem looks fixed to me, I will close this one.
</comment><comment author="haf" created="2013-08-02T13:38:50Z" id="22006011">The problem was probably that the puppet module didn't set the CONF_DIR variable in `/etc/{default,sysconf}/elasticsearch` to `/etc/elasticsearch`.

```

  $service_settings = merge_hashes({
    'CONF_DIR' =&gt; '/etc/elasticsearch'
  },$elasticsearch::service_settings)
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More like this API doesn't return named filters matching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1941</link><project id="" key="" /><description>When performing a query containing named filters using the /_mlt API endpoint, the returned hits do not contain information about which filters they matched, whereas the same query using the /_search endpoint correctly returns the "matched_filters" key in the hit.
</description><key id="4533548">1941</key><summary>More like this API doesn't return named filters matching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rslinckx</reporter><labels /><created>2012-05-11T13:30:58Z</created><updated>2014-07-08T14:58:42Z</updated><resolved>2014-07-08T14:58:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-14T18:20:03Z" id="35109959">Not sure I follow, maybe the issue is a bit outdated :)  Could you post a recreation please?
</comment><comment author="clintongormley" created="2014-07-08T14:58:42Z" id="48348625">You can't pass in a query to the MLT APi, so named queries/filters are not supported.  Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>High CPU usage when idle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1940</link><project id="" key="" /><description>Version: 0.19.3
JDK: 7.0.4
OS: Xubuntu 12.04

Seems like the worker threads for the transport are spinning and using around 1.5 CPUs when idle on my box.

elasticsearch[Mastermind of the UK]transport_client_worker-pool-18-thread-9 192111252328    192,111.252 (0.0%)  21.101
elasticsearch[Mastermind of the UK]transport_server_worker-pool-20-thread-5 193049489597    193,049.49 (0.0%)   16.043

There are 9 of each of the above.
</description><key id="4532961">1940</key><summary>High CPU usage when idle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjgp2</reporter><labels /><created>2012-05-11T12:48:55Z</created><updated>2013-11-28T04:02:47Z</updated><resolved>2013-07-15T17:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mjgp2" created="2012-05-11T13:18:02Z" id="5650261">FYI This seems to be Netty related: setting network.tcp.blocking: true causes the CPU usage to drop back down to zero.
</comment><comment author="kimchy" created="2012-05-12T08:42:25Z" id="5666445">Can you do a thread dump when it happening? Is it something you can easily recreate?
</comment><comment author="pierrre" created="2012-06-02T17:46:47Z" id="6079395">Version: 0.19.4
JDK: 7
OS: ubuntu 12.04
(it's a virtual machine, VirtualBox)

Same problem here:
- CPU usage around 20% when ES is idle
- CPU usage under 3% with network.tcp.blocking: true

I use the default configuration provided with the .deb package
</comment><comment author="kimchy" created="2012-06-03T13:27:18Z" id="6085484">Is there a chance for a thread dump (use jstack) and gist it? Several of those (3-4) done and then gisted would help trying to track it down where in the netty layer there is the problem.
</comment><comment author="pierrre" created="2012-06-06T09:12:02Z" id="6146123">I used jstack -F -l [pid]
Is it ok?

https://gist.github.com/2880846
</comment><comment author="kimchy" created="2012-06-07T13:29:06Z" id="6175729">@pierrre which jdk version is it exactly (version number)? can you see if it happens with jdk 1.6?
</comment><comment author="pierrre" created="2012-06-07T13:49:16Z" id="6176163">java version "1.7.0_04"
Java(TM) SE Runtime Environment (build 1.7.0_04-b20)
Java HotSpot(TM) 64-Bit Server VM (build 23.0-b21, mixed mode)

I'm currently using oracle-java7-installer package from ppa:webupd8team/java
Do you have another (good) ppa repository for Java 6?
</comment><comment author="ebuildy" created="2012-06-08T10:29:01Z" id="6198819">I have a similar problem, after trying to index 1 000 000 documents (in a dummy loop, only for test purpose), my ES is dead, stop/start doesnt work, the CPU is always to 100% ;-(

java version "1.6.0_18"
OpenJDK Runtime Environment (IcedTea6 1.8.13) (6b18-1.8.13-0+squeeze1)
OpenJDK 64-Bit Server VM (build 14.0-b16, mixed mode)

Is there some requirement about Java env. to run ES ?
</comment><comment author="pierrre" created="2012-06-09T13:52:53Z" id="6219918">Sorry, I can't install Java6 from ppa:flexiondotorg/java
Missing dependencies :/
</comment><comment author="michaelklishin" created="2012-06-09T14:33:26Z" id="6220308">@pierrre OpenJDK 6 is available from the [main ubuntu repository](http://packages.ubuntu.com/search?suite=default&amp;section=all&amp;arch=any&amp;searchon=names&amp;keywords=openjdk-6-jdk).

I have been using OpenJDK 7 and Oracle JDK 7 with ES for a few months now, never had this issue.
</comment><comment author="pierrre" created="2012-06-09T15:56:14Z" id="6220982">Ok, no problem with OpenJDK 6 &amp; 7.

The High CPU usage only occurs with Oracle JDK 7 (but I can't test with Oracle JDK 6)
</comment><comment author="pierrre" created="2012-06-09T16:44:27Z" id="6221504">I indexed 8000000 documents, and now the cpu usage is around 200% (over 800%)
I use Open JDK 7 with the default config

Jstack: https://gist.github.com/2901703
</comment><comment author="ebuildy" created="2012-06-10T07:22:59Z" id="6226508">I think the problem occurs when we try to index to many documents in a loop (w/o sleep break)

Restart ES, still huge CPU, is it possible to "reset" it ?
</comment><comment author="kimchy" created="2012-06-10T20:53:11Z" id="6231830">What do you mean by reset it? it seems like its busy reading data, how do you index the data? Can you share the code?
</comment><comment author="pierrre" created="2012-06-11T11:18:34Z" id="6241925">There are 2 distincts problems:
- CPU usage when idle, no indexed data, Oracle Java 7
- Very High CPU usage when idle after a search, many indexed documents (8 million), all JVM
</comment><comment author="pierrre" created="2012-06-11T11:21:28Z" id="6241972">My code: https://gist.github.com/2909640
I run populate(), the search()

After search(), the CPU usage is really high (around 200%)
</comment><comment author="ebuildy" created="2012-06-11T14:24:35Z" id="6245659">By reset, I mean stop very long indexing process ...... here my code: http://pastebin.com/pnbG5Rdk , I am using a Php library (from https://github.com/nervetattoo/elasticsearch), the mongodb has 2 000 000 items (no long text, max 200 chars.)

Thanks
</comment><comment author="dosht" created="2012-07-01T13:43:34Z" id="6693554">I have a similar problem. I have es on 2 ec2 nodes. I removed the data from the 2 nodes and the s3 and restart es but the problem remains. I also turned off the replication and the same happened.
</comment><comment author="ebuildy" created="2012-07-02T14:18:12Z" id="6709308">Just asked to some friends, every body had the same issue with ES and don't use it anymore, hight CPU when idle, delete index, restart ES didn't change anything. Tried to reinstall it, same thing ;-( 

I follow the tweet tutorial, index 10 000 messages, fine, but after 10 hours, CPU goes to 200% for Java grrrrrrrrr 

How can I give more details ? Thanks
</comment><comment author="tgautier-silicon" created="2012-07-03T08:36:32Z" id="6728872">We had the same symptoms but they weren't related at all with this issue.

Since the 1st of July 2012, we had our CPU going through the roof and we didn't know why.
This is related to the leap second (check #leapocalypse).

Just restarting ES didn't do the trick but either rebooting the whole server, or more preferably stopping ntpd to adjust the date and time manually the restart ES, fixed everything.

So don't throw away everything, ES is just fine :)
</comment><comment author="ebuildy" created="2012-07-04T11:05:07Z" id="6757886">Yes true, 

Another solution I have found is to set the refresh_interval to -1 to perform bulk indexing 

But still high CPU and ES timeout ;-( Is there a way to remove ES data from filesystem ?
</comment><comment author="mjgp2" created="2012-07-05T09:08:38Z" id="6774285">Are you using Java7?

It seems to be a Netty+Java7 issue. Using Java 6, or Java7 without NIO, I
do not get these issues.

On Wed, Jul 4, 2012 at 12:05 PM, eBuildy &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; Yes true,
&gt; 
&gt; Another solution I have found is to set the refresh_interval to -1 to
&gt; perform bulk indexing
&gt; 
&gt; But still high CPU and ES timeout ;-( Is there a way to remove ES data
&gt; from filesystem ?
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1940#issuecomment-6757886
</comment><comment author="ebuildy" created="2012-07-06T08:31:04Z" id="6799897">Yes I am on Java7 now, how disable NIO ? thanks
</comment><comment author="mjgp2" created="2012-07-06T08:37:40Z" id="6800009">As per the comment above:

```
FYI This seems to be Netty related: setting network.tcp.blocking: true
```

causes the CPU usage to drop back down to zero.

On Fri, Jul 6, 2012 at 9:31 AM, eBuildy &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; Yes I am on Java7 now, how disable NIO ? thanks
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1940#issuecomment-6799897
</comment><comment author="alexsisu" created="2012-08-01T14:01:27Z" id="7428049">High,

I'm also experiencing this issue. Starting from monday morning(arround 00:00), we noticed a constant cpu consumption (~200%). 
I've created a gist here. https://gist.github.com/3227023 
Java version is: 1.6.0_27-b07
ElasticSearch version: 0.19.4

I've also googled a bit arround, and I've noticed other people having this issue (here: http://dev.eclipse.org/mhonarc/lists/jetty-users/msg02163.html).

(The server is in production, and I cant for the moment change the java version, or upgrade the elasticsearch.)

So, any suggestions/explanations will be appreciated.

Tnx in advance.
</comment><comment author="kimchy" created="2012-08-02T07:52:30Z" id="7449239">@alexsisu can you try and use a newer JVM version, and a newer elasticsearch version (which includes a newer netty version) and see if it happens? You mention that its in production, so possibly for now just set the blocking flag to true.
</comment><comment author="ebuildy" created="2012-08-02T09:14:32Z" id="7450515">What version do you suggest ? (OpenJDK, Oracle , 1.7 ... with Java I am always lost ....) ?

And do you have any documentation about setting a good production configuration (such as this blocking flag) ?

Thanks (and again, good job ! thanks for this product)
</comment><comment author="michaelklishin" created="2012-08-02T09:44:08Z" id="7451061">OpenJDK 7 is fine but in most distributions you will get a fairly old patch level. Oracle's one is not tied to what distributions provide, so you can get a very recent patch level like 7u5).
</comment><comment author="alexsisu" created="2012-08-02T09:55:18Z" id="7451259">Additional information. 
By using the command: 
top -H -d 1 -n 20 -p 5174 
in linux I've managed to retrieve the cpu consumption by thread. 
Here you have a snapshot in moments of inactivity.
https://gist.github.com/3235953

From what I understood,  the pids displayed are in fact the native ids. 
So, i've took the PID 5128 and translated to hexa obtaining the native id for a thread. Which is: 0x1462.

Grepping the jstack log, I found this line:

"Concurrent Mark-Sweep GC Thread" prio=10 tid=0x00007fefdc3ae000 nid=0x1462 runnable.
That implying that the CMS GC Thread might be one  of the consuming CPU threads.

Is this correct? Is there the possibility of having high cpu gc, in the idle moments of elasticsearch? 
(we hit it pretty hard during the day, especially faceting, that increasing the heap size up to 60~80 GB.)
</comment><comment author="alexsisu" created="2012-08-02T10:02:47Z" id="7451410">@kimchy regarding the upgrade:
What is the cost of performance when disabling the nio (by setting the blocking to True).
Is there a known problem regarding the netty &amp; java in es? 
How can I verify that is a netty problem?

I'm asking that because before if I'll do the upgrade of jvm and es, I'll might loose this behavior, and it might occur even after the update.
What other type of information can I provide?

Probably we'll do the update in the near future anyhow, but I'd like to know more about this issue.

Tnx :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shutdown API: When sending an "all" shutdown, it also shutsdown node clients</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1939</link><project id="" key="" /><description /><key id="4521484">1939</key><summary>Shutdown API: When sending an "all" shutdown, it also shutsdown node clients</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-10T20:33:01Z</created><updated>2014-07-08T14:59:48Z</updated><resolved>2012-05-10T20:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow access to Lucene field infos via API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1938</link><project id="" key="" /><description>At times it's interesting to get an overview over the created Lucene fields while working with compolex mappings. This info could be added to the index status API.

When opening an index with [Luke](http://code.google.com/p/luke/) it immediately shows a list of fields with corresponding term counts. That would certainly be a good starting point.
</description><key id="4511821">1938</key><summary>Allow access to Lucene field infos via API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels><label>:Stats</label><label>adoptme</label><label>feature</label></labels><created>2012-05-10T12:18:59Z</created><updated>2016-05-24T13:42:44Z</updated><resolved>2016-05-24T13:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-24T10:25:10Z" id="221228302">@sfussenegger are there still stats that you're after than aren't already exposed via other APIs?
</comment><comment author="sfussenegger" created="2016-05-24T13:38:42Z" id="221271674">@clintongormley I haven't followed elasticsearch development as closely for the last two years, so I really don't know what is available by now. So as far as I'm concerned you can close this issue.
</comment><comment author="clintongormley" created="2016-05-24T13:42:43Z" id="221273041">thanks @sfussenegger 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query String URI: Add `lenient` to the URI `q` parameters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1937</link><project id="" key="" /><description /><key id="4510671">1937</key><summary>Query String URI: Add `lenient` to the URI `q` parameters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-10T10:53:17Z</created><updated>2012-05-10T10:53:52Z</updated><resolved>2012-05-10T10:53:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: query_string syntax to support wildcard fieldnames in the query text</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1936</link><project id="" key="" /><description>Some samples:

```
curl -XPUT localhost:9200/test/type/1 -d '{
    "city" : {
        "name" : "xxx",
        "address" : "yyy"
    }
}'

curl 'localhost:9200/test/_search?q=city.\*:xxx'

curl localhost:9200/test/_search -d '{
    "query" : {
        "query_string" : {
            "query" : "city.\\*:xxx"
        } 
    }
}'
```

Note, `\` is used to escape `*` for the query string syntax, and the second one is for the json (you probably don't need it if you "to_json" it).
</description><key id="4510607">1936</key><summary>Query DSL: query_string syntax to support wildcard fieldnames in the query text</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-10T10:48:35Z</created><updated>2012-07-21T20:56:02Z</updated><resolved>2012-05-10T10:49:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-07-20T17:16:59Z" id="7135772">As a result of this change, the query `*:*` is now translated into `field1:* OR field2:* OR .... OR filedN:*`, for all fields in the mapping. Is this intentional?
</comment><comment author="kimchy" created="2012-07-21T20:56:02Z" id="7156641">It should be resolved, I will push a fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to filter percolators when stemming is used on the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1935</link><project id="" key="" /><description>I've only tested this in 0.19.4 so far. First I create an index using stemming, then I register a percolator with a term value. I then percolate a document with a query that matches the term value and text that matches the percolator query. The percolator does not match. If I remove the term query from the percolation request the match is successful. Also strangley if I put a '*' on the end of the term query it works. Recreation:

```
curl -XPUT http://localhost:9200/test/ -d "{ \"index\" : { \"analysis\" : { \"analyzer\" : { \"default\" : { \"tokenizer\": \"standard\", \"filter\" :  [\"standard\", \"my_snow\"] }}, \"filter\" : {\"my_snow\" : { \"type\" : \"snowball\", \"language\" : \"English\" }}}}}"
curl -XPUT localhost:9200/_percolator/test/perc_1 -d "{\"keywordtype\" : \"inclusion\",\"query\" : {\"query_string\" : {\"query\" : \"tennis\"}}}"
curl -XGET localhost:9200/test/type1/_percolate -d "{ \"query\" :  { \"query_string\" :  { \"query\" : \"keywordtype:inclusion\" }}, \"doc\" : { \"txt\" : \"tennis\" }}"
curl -XGET localhost:9200/test/type1/_percolate -d "{ \"doc\" : { \"txt\" : \"tennis\" }}"
curl -XGET localhost:9200/test/type1/_percolate -d "{ \"query\" :  { \"query_string\" :  { \"query\" : \"keywordtype:inclusion*\" }}, \"doc\" : { \"txt\" : \"tennis\" }}"

curl -XDELETE http://localhost:9200/test/
```

If I run this code without the settings that add stemming to the index, all three percolations return a match.
</description><key id="4510479">1935</key><summary>Unable to filter percolators when stemming is used on the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brett--anderson</reporter><labels /><created>2012-05-10T10:38:45Z</created><updated>2012-05-14T00:12:42Z</updated><resolved>2012-05-14T00:12:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-13T10:37:40Z" id="5675383">I tracked down the problem and opened an issue for it (more explicit in the description): #1948.
</comment><comment author="brett--anderson" created="2012-05-14T00:12:42Z" id="5680770">Great, thanks for sorting that one :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an optional _all field like index to mapped objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1934</link><project id="" key="" /><description>Objects should become searchable by an additional index that contains all properties of this object, similar to what the _all field is to the root object. 

The content of the index should be built equal to the default _all field: honoring `include_in_all` and containing values of all fields, including contained objects. This index should be named after the object, maybe with an optional `index_name` option on the object mapping.

To give an example, let's assume this JSON source:

```
{
  name: "CN Tower",
  city: {
    name: "Toronto"
    state: {
      name: "Ontario"
      country {
        name: "Canada",
        code: "CA"
      }
    }
  }
}
```

The following query strings should find this object:

`Canada`
`city:Canada`
`city.state:Canada`
`city.state.country:Canada`
</description><key id="4509917">1934</key><summary>Add an optional _all field like index to mapped objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels /><created>2012-05-10T09:59:17Z</created><updated>2014-07-08T14:56:49Z</updated><resolved>2014-07-08T14:56:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-05-10T10:33:08Z" id="5622106">A challenge is to avoid false hits, e.g. for queries such as

city:Ontario -&gt; should give 0 hits
city.state.name:Canada &gt; should give 0 hits
</comment><comment author="sfussenegger" created="2012-05-10T10:43:48Z" id="5622248">In the above example I'd argue that `city:Ontario` should hit as state is nested inside city. If this isn't the desired behavior, state could be moved out of city. I was just thinking about queries like `city:"Toronto Ontario"` or `city:"Boston MA"`. That's why I've chosen this example. Thinking about exclusions from specific _all fields only (say include in _all, exclude from city._all) could make mapping quite complicated. I think it should be as simple as this:

```
state: {
  _all: {enabled: true}
  path: "just_name",
  index_name: "s",
  properties: { ...}
}
```

that would enable `s:"ontario canada"`
</comment><comment author="sfussenegger" created="2012-05-10T11:08:50Z" id="5622577">As a workaround, one can use wildcard fieldnames, e.g. `city.*`, in query text, see #1936

Ad additional workaround is using multi_field with non-unique index_name to create additional indexes, see [sample mapping](https://gist.github.com/2651930). The sample mapping allows querying using `country`, `country.name` and `country.code`
</comment><comment author="clintongormley" created="2014-07-08T14:56:49Z" id="48348256">The `copy_to` functionality in #4796 allows custom `_all` fields, which will do what you need.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make 'path' configurable for all core type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1933</link><project id="" key="" /><description>&gt; [11:24] kimchy: it would be nice if it was possible to specify the path type on the "core" mapping as well
</description><key id="4509587">1933</key><summary>Make 'path' configurable for all core type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels /><created>2012-05-10T09:34:52Z</created><updated>2014-07-08T14:55:06Z</updated><resolved>2014-07-08T14:55:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:55:06Z" id="48347932">The `path` setting has been removed.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query String: Add `lenient` flag to support *value* parse failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1932</link><project id="" key="" /><description>For example, when searching against several fields, one of them might be numeric, in this case, with `lenient` set to `true`, it will not fail.

Can be set globally by setting `index.query_string.lenient` to `true`.
</description><key id="4509119">1932</key><summary>Query String: Add `lenient` flag to support *value* parse failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-10T09:02:54Z</created><updated>2014-02-04T18:27:59Z</updated><resolved>2012-05-10T09:03:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow to customize quote analyzer to be used when quoting text in a query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1931</link><project id="" key="" /><description>This includes the following:
- `index.analysis.default_search_quote` that will be used by default for all quoted search.
- type mapping `search_quote_analyzer` to define on a specific type
- `string` field mapping `search_quote_analyzer` specific setting
- `query_string` to support custom `quote_analyzer` which forces to use it regardless of mapping
- `query_string` to support `quote_field_suffix` to automatically add a suffix to the field name search on when using quoted search.
</description><key id="4508688">1931</key><summary>Allow to customize quote analyzer to be used when quoting text in a query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-10T08:30:53Z</created><updated>2012-05-10T08:52:58Z</updated><resolved>2012-05-10T08:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>StackOverflowError in TransportClientNodesService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1930</link><project id="" key="" /><description>I observed a StackOverflowError lately

```
Exception in thread "elasticsearch[generic]-pool-1-thread-24" java.lang.StackOverflowError
        at java.net.Inet4Address.getHostAddress(Inet4Address.java:322)
        at java.net.InetAddress.toString(InetAddress.java:663)
        at java.net.InetSocketAddress.toString(InetSocketAddress.java:276)
        at java.lang.String.valueOf(String.java:2902)
        at java.lang.StringBuilder.append(StringBuilder.java:128)
        at org.elasticsearch.common.transport.InetSocketTransportAddress.toString(InetSocketTransportAddress.java:150)
        at java.lang.String.valueOf(String.java:2902)
        at java.lang.StringBuilder.append(StringBuilder.java:128)
        at org.elasticsearch.transport.ActionTransportException.buildMessage(ActionTransportException.java:71)
        at org.elasticsearch.transport.ActionTransportException.&lt;init&gt;(ActionTransportException.java:46)
        at org.elasticsearch.transport.ConnectTransportException.&lt;init&gt;(ConnectTransportException.java:44)
        at org.elasticsearch.transport.ConnectTransportException.&lt;init&gt;(ConnectTransportException.java:32)
        at org.elasticsearch.transport.NodeNotConnectedException.&lt;init&gt;(NodeNotConnectedException.java:32)
        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:637)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:445)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:185)
        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:63)
        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:100)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:217)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
        [...endlessly repeated... ]
```
</description><key id="4502222">1930</key><summary>StackOverflowError in TransportClientNodesService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jprante</reporter><labels /><created>2012-05-09T21:42:01Z</created><updated>2014-02-18T07:19:58Z</updated><resolved>2013-10-30T09:08:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-05-10T09:25:48Z" id="5621236">Situation where this can happen:
- many threads in parallel
- TransportClient shared between threads
- TransportClient in sniff mode
- ES cluster on more than one node
- heavy bulk transfer started, with lots of outstanding bulk responses
- StackOverflowError might show up after bulk transfer is stopped with ctrl-c

A mockup client is here: https://gist.github.com/2652024

Bulk traffic needs to get simulated.
</comment><comment author="jprante" created="2012-05-12T13:22:47Z" id="5668426">A few minutes ago I needed to interrupt my high-volume indexer with kill (so it causes InterruptedExceptions in all threads), and the StackOverflow happened again, this time mixed with additional messages.

See in this gist for the messages:

https://gist.github.com/2666459

I'm clueless, I think it's my fault, because I treat TransportClient badly - I just let it go.

How can I interrupt a TransportClient with many pending bulk requests so that it can shut down cleanly?

Is it possible to wait for outstanding bulks got processed (waiting for BulkResponses)? 
</comment><comment author="jprante" created="2012-05-12T13:29:56Z" id="5668469">Just noticed the StackOverflow happens while handling something with IPv6 , because I added IPv6 addresses to the TransportClient. But IPv6 cluster wide addressing is not possible because IPv6 is not supported by the network administration team. Hmmm. I should quit using IPv6 in TransportClient.
</comment><comment author="kimchy" created="2012-05-25T21:09:52Z" id="5940280">Heya @jprante, did you manage to recreate it at the end? (btw, there is no way to wait for pending bulk requests, sorry for the late answer).
</comment><comment author="jprante" created="2012-05-29T20:49:20Z" id="5995402">Sorry for the lag. I still need some time writing an error-provoking bulk indexing text case and see how far it goes. As a workaound, I'm thinking about something as a "SafeTransportClient" that can wait for outstanding ActionListeners if InterruptedExceptions come in.
</comment><comment author="spinscale" created="2013-10-30T08:48:31Z" id="27373031">hey,

anything we can do here to help? Can you still reproduce this? Should we close this?
</comment><comment author="jprante" created="2013-10-30T09:08:22Z" id="27373945">I'm quite sure the Mac OS X JVM version at that time was playing tricks on me with Java exceptions around open/close sockets/file descriptors and tight system resource limits. I have not seen it after the workaround. Also on Linux or Solaris, I never encountered this situation.

Actually I do not intend to dig deeper, beside it is hard to reproduce, it would involve tracing at OS level, maybe OS or JVM devs have to be bothered in that case. Will also soon move to OS X Maverick in the hope for new exciting bugs :)

So it's ok the issue was closed, I close it again. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian: Update deps to be on openjdk-7-jre-headless first, then 6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1929</link><project id="" key="" /><description /><key id="4501180">1929</key><summary>Debian: Update deps to be on openjdk-7-jre-headless first, then 6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-09T20:46:12Z</created><updated>2012-05-09T20:46:34Z</updated><resolved>2012-05-09T20:46:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Updated JAVA_HOME directories for Ubuntu 12.04 x86_64</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1928</link><project id="" key="" /><description>Hi,

Following this thread:
http://groups.google.com/group/elasticsearch/browse_thread/thread/669c7487354500a1

I've added the JAVA_HOME paths to match the packages openjdk-7-jre-headless and openjdk-6-jre-headless from Ubuntu 12.04. Tested on my test machine and it works as expected.

Best regards,
Radu
</description><key id="4491731">1928</key><summary>Updated JAVA_HOME directories for Ubuntu 12.04 x86_64</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels /><created>2012-05-09T12:42:50Z</created><updated>2014-07-09T19:59:24Z</updated><resolved>2012-05-09T21:03:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-09T21:03:24Z" id="5611680">Ok, thanks!, pulled the change, munged it a bit (added i386 as well) and pushed to master and 0.19 branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get API: When _source is disabled, the source is still used if fetched from the transaction log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1927</link><project id="" key="" /><description>We need to be more consistent and not return fields if source is disabled and the field is not stored and fetching it from the transaction log
</description><key id="4474074">1927</key><summary>Get API: When _source is disabled, the source is still used if fetched from the transaction log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-08T14:47:10Z</created><updated>2012-05-08T14:58:37Z</updated><resolved>2012-05-08T14:58:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>GET document response returns exists field twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1926</link><project id="" key="" /><description /><key id="4473202">1926</key><summary>GET document response returns exists field twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.0.RC1</label></labels><created>2012-05-08T14:01:27Z</created><updated>2012-05-08T14:01:55Z</updated><resolved>2012-05-08T14:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolator: Filtering percolators based on a query can cause wrong matches to be returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1925</link><project id="" key="" /><description>See here: https://gist.github.com/2632887
</description><key id="4472897">1925</key><summary>Percolator: Filtering percolators based on a query can cause wrong matches to be returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-08T13:47:33Z</created><updated>2012-05-08T13:55:54Z</updated><resolved>2012-05-08T13:55:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Nested facet doesn't dedup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1924</link><project id="" key="" /><description>Provided i have a document like this:

```
{ 
    "makers": [ { "name": "a" }, { "name": "a" } { "name": "b" }], 
    "title":"xyz" 
}
```

and I search on `title:xyz` while doing a nested facet on `makers.name` I get one result as expected but the facet for `maker.name` mentions 2 documents for `a`. The backing index for the nested type might be at play here?

I'm using elasticsearch 19.2
</description><key id="4472769">1924</key><summary>Nested facet doesn't dedup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2012-05-08T13:40:43Z</created><updated>2014-07-08T14:54:47Z</updated><resolved>2014-07-08T14:54:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:54:47Z" id="48347868">Expected behaviour as nested objects are separate documents. You could use the `include_in_parent` setting to also index them as type `object`, then do a normal (non-nested) facet/agg.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Instance cutting off http connections/randomly quitting?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1923</link><project id="" key="" /><description>Using `0.19.3`. Sometimes while I am updating an index my connection will get confused and then I can no longer connect to my ES instance. I then have to restart the instance to get it working again. I've been looking in the logs and also set the level to `DEBUG` but I don't see any evidence as to what's causing it. Any suggestions?

I am using the `tire` ruby gem to interact with ES. So this is the output I'm experiencing during my import

```
[IMPORT] Starting import for the 'Post' class
--------------------------------------------------------------------------------
1219/1219 | 100% [ERROR] Connection refused - connect(2), retrying (1)...#######
[ERROR] Connection refused - connect(2), retrying (2)...
[ERROR] Connection refused - connect(2), retrying (3)...
[ERROR] Connection refused - connect(2), retrying (4)...
[ERROR] Connection refused - connect(2), retrying (5)...
[ERROR] Too many exceptions occured, giving up. The HTTP response was: Connection refused - connect(2)

================================================================================
Import finished in 8.93761 second
```
</description><key id="4470653">1923</key><summary>Instance cutting off http connections/randomly quitting?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">axsuul</reporter><labels /><created>2012-05-08T11:00:46Z</created><updated>2012-05-09T10:46:23Z</updated><resolved>2012-05-09T10:46:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="axsuul" created="2012-05-09T10:46:23Z" id="5597185">It turns out ES default maximum memory is 1024 MB while my system only had 512 MB. I resolved this by setting the `ES_MAX_MEM` environment variable to something more appropriate like `256m`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.common.collect.Tuple have 2 identical methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1922</link><project id="" key="" /><description>```
...
public static &lt;V1, V2&gt; Tuple&lt;V1, V2&gt; tuple(V1 v1, V2 v2) {
    return new Tuple&lt;V1, V2&gt;(v1, v2);
}
...
public static &lt;V1, V2&gt; Tuple&lt;V1, V2&gt; create(V1 v1, V2 v2) {
    return new Tuple&lt;V1, V2&gt;(v1, v2);
}
```

The first one not used in ES core, but looks like it's intended to use with static import like:

```
import static org.elasticsearch.common.collect.Tuple.*
...
return tuple("Hello", "Moto")
```

The second one is used 2 times in ES core.
So my proposal is to remove create method and use "static" way to create tuple

Here is the patch for that https://gist.github.com/2633914
I haven't created pull request because it's too small and not so important.
</description><key id="4469755">1922</key><summary>org.elasticsearch.common.collect.Tuple have 2 identical methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">voituk</reporter><labels /><created>2012-05-08T09:45:27Z</created><updated>2012-05-08T10:38:22Z</updated><resolved>2012-05-08T10:38:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-08T10:37:59Z" id="5571780">Will remove the create and keep tuple, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>River name re-use not possible between node shutdowns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1921</link><project id="" key="" /><description>While playing with https://github.com/jprante/elasticsearch-river-oai I was looking for a method how to restart rivers gracefully, with slightly modified parameters. I found some interesting behavior on a single node cluster.
1. After a river is created, a river can be deleted, and a new river with the same river name can be created without problems.
   
   curl -XPUT 'localhost:9200/_river/my_river/_meta' -d ' { ... }'
   _river runs_
   curl -XDELETE 'localhost:9200/_river/my_river/'
   _river stops_
   curl -XPUT 'localhost:9200/_river/my_river/_meta' -d ' { ... }'
   _river runs_
2. But when the node is being shutdown after river deletion, and the node is being started up again, there is a response, but no reaction and no activity when re-using the river name.
   
   curl -XPUT 'localhost:9200/_river/my_river/_meta' -d ' { ... }'
   _river runs_
   curl -XDELETE 'localhost:9200/_river/my_river/'
   _river stops_
   _node stop_
   _node start_
   curl -XPUT 'localhost:9200/_river/my_river/_meta' -d ' { ... }'
   _no river activity_

The only workaround I am aware of is to use a different river name, e.g. by enumerating rivers (my_river_1, my_river_2 ...) 

Is this behavior intentional? I think it should be possible to re-use a river name in any case.
</description><key id="4463952">1921</key><summary>River name re-use not possible between node shutdowns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>bug</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-05-07T22:39:34Z</created><updated>2012-06-23T18:26:34Z</updated><resolved>2012-06-23T18:26:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to guava 12.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1920</link><project id="" key="" /><description /><key id="4457925">1920</key><summary>Upgrade to guava 12.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-07T17:08:21Z</created><updated>2012-05-07T17:08:50Z</updated><resolved>2012-05-07T17:08:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Binary field is stored by default, allow to disable it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1919</link><project id="" key="" /><description>By default, binary field is stored (which makes sense, since if mapped, nothing can be done with it except storing it). But, allow to set `store` to `no` on it.

Original Request:

It seems that the following mapping results in a stored binary field even if the store attribute is not set and should default to false:

```
"file" : {
    "_source" : { "enabled" : "false" },
  "properties" : {
        "file" : {
      "type" : "binary",
      "compress" : "true"
    }
  }
}
```

Tested on ES 0.19.3.
</description><key id="4457811">1919</key><summary>Binary field is stored by default, allow to disable it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-07T17:01:59Z</created><updated>2012-05-07T17:29:10Z</updated><resolved>2012-05-07T17:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Null pointer exception when closing TransportClient</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1918</link><project id="" key="" /><description>05-07 11:11:55,107 WARN pool-1-thread-44 common.thread:105 - Failed to clean thread locals
java.lang.NullPointerException
at sun.reflect.UnsafeFieldAccessorImpl.ensureObj(UnsafeFieldAccessorImpl.java:54)
at sun.reflect.UnsafeObjectFieldAccessorImpl.get(UnsafeObjectFieldAccessorImpl.java:36)
at java.lang.reflect.Field.get(Field.java:372)
at org.elasticsearch.common.thread.ThreadLocals.clearThreadLocalMap(ThreadLocals.java:108)
at org.elasticsearch.common.thread.ThreadLocals.clearReferencesThreadLocals(ThreadLocals.java:74)
at org.elasticsearch.client.transport.TransportClient.close(TransportClient.java:238)

This happens sometimes when called on pre-destroy of stateless RESTful bean.

@PreDestroy
    public void destroy() {
        client.close();
    }
</description><key id="4452435">1918</key><summary>Null pointer exception when closing TransportClient</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rzulf</reporter><labels /><created>2012-05-07T11:32:47Z</created><updated>2012-08-16T17:36:46Z</updated><resolved>2012-08-16T17:36:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-07T13:50:53Z" id="5550382">You can ignore this failure, I will fix it so it won't fail in this case.
</comment><comment author="Rzulf" created="2012-05-07T15:14:06Z" id="5552264">Thanks for response :)
</comment><comment author="Manokrrish" created="2012-08-16T07:42:37Z" id="7778768">Hi Kimchy,
   Has this been fixed?
</comment><comment author="kimchy" created="2012-08-16T17:36:46Z" id="7792894">@Manokrrish yea, I will close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Index: Allow to provide index warmers when creating an index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1917</link><project id="" key="" /><description>```
{
    "settings" : {
        "index.number_of_shards" : 1
    },
    "warmers" : {
        "warmer_1" : {
            "types" : [],
            "source" : {
                "query" : {
                    "match_all" : {}
                }
            }
        }
    }
}
```
</description><key id="4452304">1917</key><summary>Create Index: Allow to provide index warmers when creating an index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-05-07T11:19:55Z</created><updated>2012-05-07T11:27:37Z</updated><resolved>2012-05-07T11:27:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Template: Allow to register index warmers in an index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1916</link><project id="" key="" /><description>Allow to register index warmers as part of an index template, here is an example:

```
{
    "template" : "*",
    "warmers" : {
        "warmer_1" : {
            "types" : [],
            "source" : {
                "query" : {
                    "match_all" : {}
                }
            }
        }
    }
}
```
</description><key id="4452080">1916</key><summary>Index Template: Allow to register index warmers in an index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-05-07T10:58:00Z</created><updated>2012-05-07T11:00:47Z</updated><resolved>2012-05-07T11:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.common.collect.computationexception </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1915</link><project id="" key="" /><description>Hey guys,

Iam trying to develop a native elasticsearch service for my project. Im using Java API and followed all guide but i got these exception below:

org.elasticsearch.common.collect.ComputationException: java.lang.NoClassDefFoundError: org/apache/lucene/analysis/ga/IrishAnalyzer

I searched any-all Google results and much of them say that this issue is related with Maven dependency besides it is not meaningful to me. Here is my usage:

Node node = NodeBuilder.nodeBuilder().node();
Client _nodeClient = node.client();
SearchResponse resSearch = null;
try{
resSearch = _nodeClient.prepareSearch("videos")
.setSearchType(SearchType.DEFAULT)
.setQuery(QueryBuilders.queryString("q:anytext"))
.setFrom(0).setSize(60).setExplain(true)
.execute()
.actionGet();

Could you give a hand pls? I can not keep moving on :(
</description><key id="4451911">1915</key><summary>org.elasticsearch.common.collect.computationexception </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fatihzkaratana</reporter><labels /><created>2012-05-07T10:40:50Z</created><updated>2013-05-23T07:18:48Z</updated><resolved>2013-05-23T07:18:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fatihzkaratana" created="2012-05-07T13:02:10Z" id="5549467">I removed all inner codes and left only

Node node = NodeBuilder.nodeBuilder().client(true).node();

But still getting this error org.elasticsearch.common.collect.ComputationException: java.lang.NoClassDefFoundError: org/apache/lucene/analysis/ga/IrishAnalyzer

I searched and can not find any response. I could not figured it out why this error coming up :(

Need help.
</comment><comment author="kimchy" created="2012-05-07T14:03:36Z" id="5550632">Are you using the correct Lucene version? 0.19.3 is Lucene 3.6. Also, questions on the mailing list (much more eyeballs), before opening an issue.
</comment><comment author="fatihzkaratana" created="2012-05-07T19:02:40Z" id="5557880">Yeap! Using 0.19.3 and Lucene 3.6. I tried to subscribe mailing list but i got an error about subscribing to the mailing list via email has been suspended so i could not deal with.
</comment><comment author="kimchy" created="2012-05-08T10:40:39Z" id="5571817">I don't think you use Lucene 3.6 with 0.19.3, because the IrishAnalyzer is new in Lucene 3.6. Just join the google group, don't try and join through nabble (if thats what you tried). Anybody can join the google group.
</comment><comment author="spinscale" created="2013-05-23T07:18:48Z" id="18327251">Closing this as it does not seem an issue after a year without posts. If you still have problems (using a current version of elasticsearch), please open a new ticket with a full stack trace. And your dependencies listed (i.e. via `mvn dependency:tree`). We are glad to help.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to netty 3.4.3.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1914</link><project id="" key="" /><description /><key id="4444350">1914</key><summary>Upgrade to netty 3.4.3.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-06T16:51:41Z</created><updated>2012-05-06T16:52:06Z</updated><resolved>2012-05-06T16:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Warmup API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1913</link><project id="" key="" /><description>Index warming allows to run registered search requests to warm up the index before it is available for search. With the near real time aspect of search, cold data (segments) will be warmed up before they become available for search.

Warmup searches typically include requests that require heavy loading of data, such as faceting or sorting on specific fields.

The warmup APIs allows to register warmup (search) under specific names, remove them, and get them.

Index warmup can be disabled by setting `index.warmer.enabled` to `false`. It is supported as a realtime setting using update settings API. This can be handy when doing initial bulk indexing, disabling pre registered warmers to make indexing faster and less expensive and then enable it.
## Put Warmer

Allows to put a warmup search request on a specific index (or indices), with the body composing of a regular search request. Types can be provided as part of the URI if the search request is designed to be run only against the specific types.

Here is an example that registers a warmup called `warmer_1` against index `test` (can be alias or several indices), for a search request that runs against all types:

```
curl -XPUT localhost:9200/test/_warmer/warmer_1 -d '{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "facet_1" : {
            "terms" : {
                "field" : "field"
            }
        } 
    }
}'
```

And an example that registers a warmup against specific types:

```
curl -XPUT localhost:9200/test/type1/_warmer/warmer_1 -d '{
    "query" : {
        "match_all" : {}
    },
    "facets" : {
        "facet_1" : {
            "terms" : {
                "field" : "field"
            }
        } 
    }
}'
```
## Delete Warmer

Removing a warmer can be done against an index (or alias / indices) based on its name. The provided name can be a simple wildcard expression or omitted to remove all warmers. Some samples:

```
# delete warmer named warmer_1 on test index
curl -XDELETE localhost:9200/test/_warmer/warmer_1 

# delete all warmers that start with warm on test index
curl -XDELETE localhost:9200/test/_warmer/warm* 

# delete all warmers for test index
curl -XDELETE localhost:9200/test/_warmer/
```
## GETting Warmer

Getting a warmer for specific index (or alias, or several indices) based on its name. The provided name can be a simple wildcard expression or omitted to get all warmers. Some examples: 

```
# get warmer named warmer_1 on test index
curl -XGET localhost:9200/test/_warmer/warmer_1 

# get all warmers that start with warm on test index
curl -XGET localhost:9200/test/_warmer/warm* 

# get all warmers for test index
curl -XGET localhost:9200/test/_warmer/
```
</description><key id="4443992">1913</key><summary>Index Warmup API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.20.0.RC1</label></labels><created>2012-05-06T15:49:58Z</created><updated>2014-08-06T16:03:40Z</updated><resolved>2012-05-06T15:50:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppearcy" created="2012-05-07T14:45:59Z" id="5551593">NICE ONE!!! Thanks!
</comment><comment author="kimchy" created="2012-05-07T16:52:33Z" id="5554647">Note also #1916 and #1917 for the ability to register them on index creation, and in an index template.
</comment><comment author="medcl" created="2012-08-05T14:09:47Z" id="7510199">awesome!
</comment><comment author="tlrx" created="2012-08-06T09:48:19Z" id="7520360">Looks great, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When setting `index.recovery.initial_shards` in the config file, it is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1912</link><project id="" key="" /><description /><key id="4424315">1912</key><summary>When setting `index.recovery.initial_shards` in the config file, it is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-04T14:06:13Z</created><updated>2012-05-04T14:06:42Z</updated><resolved>2012-05-04T14:06:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>API to pin a shard to a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1911</link><project id="" key="" /><description>As a stopgap to index-based allocation, could offer an API to move a shard to a specific node.

This can be used to move large shards to specific nodes, then let smaller shards reallocate around them.

A plugin like elasticsearch-head could let you drag &amp; drop a shard to a node to manually rebalance.
</description><key id="4415076">1911</key><summary>API to pin a shard to a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jeremy</reporter><labels /><created>2012-05-03T22:20:50Z</created><updated>2015-01-19T21:01:21Z</updated><resolved>2013-10-30T08:47:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-22T14:21:27Z" id="21347096">Do you really need this on a shard level? This makes things quite complex :-)

Always keep in mind that it is the same amount of resource two query, if you create on index with 10 shards or two indices with five shards. So maybe having more indices can help you.

If you create two indices, create an aliases which uses these two indices, is there anything you cannot built using the existing allocation functionality (include, exclude, require)? http://www.elasticsearch.org/guide/reference/index-modules/allocation/
</comment><comment author="spinscale" created="2013-10-30T08:47:05Z" id="27372977">Closing this due to lack of feedback.

I think you can archive most of this using shard allocation, or if you want to go wild, use your own custom AllocationDecider that fits your need.

if you miss anything, please reopen and we'll try to help. Thanks!
</comment><comment author="KrzysztofMadejski" created="2015-01-12T05:01:58Z" id="69530305">I have kind of similar need and I wonder if I can do it with existing functionality. I have two farms of ES servers and I would like to keep my data redundant, but keeping each farm as copy of the other. So if one farm is down the other is providing data, but they load-balance when working together.

I could create two indices and configure them so they land on separate farms + create an alias and index documents in both indices. But I wonder if I won't get duplicates in search results while doing that? I'm afraid that ES won't be aware of this duplication.

Ideally I would like to configure ES so that replicas would go to one farm, and primary shards on the other.
</comment><comment author="clintongormley" created="2015-01-13T20:41:58Z" id="69815932">@KrzysztofMadejski see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-cluster.html#allocation-awareness
</comment><comment author="KrzysztofMadejski" created="2015-01-19T21:01:21Z" id="70559826">I was looking exactly for that. It seems I need to do more reading. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport Client: Add `client.transport.ignore_cluster_name` to ignore the cluster name validation, defaults to `false`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1910</link><project id="" key="" /><description /><key id="4414941">1910</key><summary>Transport Client: Add `client.transport.ignore_cluster_name` to ignore the cluster name validation, defaults to `false`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-03T22:11:17Z</created><updated>2012-05-03T22:12:04Z</updated><resolved>2012-05-03T22:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.4.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1909</link><project id="" key="" /><description /><key id="4410990">1909</key><summary>Upgrade to Netty 3.4.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-03T18:40:13Z</created><updated>2012-05-03T19:03:40Z</updated><resolved>2012-05-03T19:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to jackson 1.9.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1908</link><project id="" key="" /><description /><key id="4410977">1908</key><summary>Upgrade to jackson 1.9.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-03T18:39:47Z</created><updated>2012-05-03T18:39:54Z</updated><resolved>2012-05-03T18:39:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-03T18:39:54Z" id="5494900">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Char Codes in Mapping Char Filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1907</link><project id="" key="" /><description>Hello.
My question is:
Is it possible to replace in the document  characters with some code to another? (For example, map non-breaking space (\xA0) to regular (\20))

If yes, how can I do it?
If no, can this functionality be added some day?
</description><key id="4406165">1907</key><summary>Char Codes in Mapping Char Filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">unikoid</reporter><labels /><created>2012-05-03T14:35:53Z</created><updated>2013-07-09T21:51:30Z</updated><resolved>2013-07-09T21:51:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imdhmd" created="2013-07-08T08:16:11Z" id="20591225">Non-breaking space can be replaced using a char filter. you need define the following character filter and add it to your analyzer:

``` yml
char_filter:
      whitespace_mapping:
        type: mapping
        mappings: ["\\u00A0=&gt;\\u0020"]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport Client: When adding an address was already added, ignore it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1906</link><project id="" key="" /><description>Additional improvement is to allow for a list of addresses to be added at once.
</description><key id="4404623">1906</key><summary>Transport Client: When adding an address was already added, ignore it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-03T13:10:27Z</created><updated>2012-05-03T13:11:22Z</updated><resolved>2012-05-03T13:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ClassCastException during percolation query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1905</link><project id="" key="" /><description>elasticsearch v0.19.3 / Lucene 3.6

org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache.FilterCacheFilterWrapper#getDocIdSet()
gets an instance of org.apache.lucene.index.memory.CustomMemoryIndex.MemoryIndexReader
but tries to cast it to a org.apache.lucene.index.SegmentReader in WeightedFilterCache line #169

The CustomMemoryIndex is created in org.elasticsearch.index.percolator.PercolatorExecutor#percolate(DocAndQueryRequest) at line 316

This is worked with elasticsearch 0.19.2 / Lucene 3.5

full stack trace:

org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution
    at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:90)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:49)
    at search.PercolationTest.percolateRawTest(PercolationTest.java:139)
...
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: java.util.concurrent.ExecutionException: java.lang.ClassCastException: org.apache.lucene.index.memory.CustomMemoryIndex$MemoryIndexReader cannot be cast to org.apache.lucene.index.SegmentReader
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:272)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
    ... 29 more
Caused by: java.lang.ClassCastException: org.apache.lucene.index.memory.CustomMemoryIndex$MemoryIndexReader cannot be cast to org.apache.lucene.index.SegmentReader
    at org.elasticsearch.index.cache.filter.weighted.WeightedFilterCache$FilterCacheFilterWrapper.getDocIdSet(WeightedFilterCache.java:169)
    at org.apache.lucene.search.FilteredQuery.getFilteredScorer(FilteredQuery.java:136)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:117)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:577)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:383)
    at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:359)
    at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:303)
    at org.elasticsearch.index.percolator.PercolatorService.percolate(PercolatorService.java:110)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:93)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:41)
    at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:176)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
</description><key id="4402354">1905</key><summary>ClassCastException during percolation query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snazy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-03T10:10:34Z</created><updated>2012-05-03T14:57:31Z</updated><resolved>2012-05-03T14:57:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="snazy" created="2012-05-03T10:22:59Z" id="5484743">My JUnit that works with 0.19.2 but fails with 0.19.3

```
    @Test
    public void percolateRawTest() throws Exception {

        Client client = createTransportClient();

        QueryBuilder qb = QueryBuilders.boolQuery().must(QueryBuilders.queryString("hallo")).must(
                QueryBuilders.queryString("welt"));
        System.out.println("qb: " + qb.toString());

        XContentBuilder xContentBuilder = XContentFactory.jsonBuilder();
        xContentBuilder.startObject();
        xContentBuilder.field("query");
        xContentBuilder = qb.toXContent(xContentBuilder, ToXContent.EMPTY_PARAMS);
        xContentBuilder.endObject();
        System.out.println("xContentBuilder: " + xContentBuilder.string());

        IndexRequestBuilder irb = new IndexRequestBuilder(client, null);
        String indexType = "default";
        String indexIndex = "_percolator";
        String indexId = "mySuperQuery";
        irb = irb.
                setIndex(indexIndex).
                setId(indexId).
                setType(indexType).
                setSource(xContentBuilder);
        System.out.println("indexRequestBuilder: " + irb.toString());
        IndexResponse indexResponse = irb.execute().actionGet();
        System.out.println("indexResponse: " + indexResponse);
        System.out.println("indexResponse: " + indexResponse.getId());

        XContentBuilder sourceBuilder = XContentFactory.jsonBuilder();
        sourceBuilder.startObject();
        sourceBuilder.rawField("doc", "{\"text\": \"hallo welt hier bin ich\"}".getBytes());
        sourceBuilder.endObject();
        System.out.println("sourceBuilder: " + sourceBuilder.string());

        sourceBuilder = XContentFactory.jsonBuilder();
        sourceBuilder.startObject();
        sourceBuilder.field("doc");
        sourceBuilder.startObject();
        sourceBuilder.field("text", "hallo welt hier bin ich");
        sourceBuilder.endObject();
        sourceBuilder.endObject();
        System.out.println("sourceBuilder: " + sourceBuilder.string());

        String percolateIndex = "default";
        String percolateType = "PERCOLATE_TYPE";
        PercolateRequestBuilder prb = new PercolateRequestBuilder(client, percolateIndex, percolateType).setSource(
                sourceBuilder);
        System.out.println(prb);
        PercolateResponse percolateResponse = prb.execute().actionGet();
        List&lt;String&gt; matches = percolateResponse.matches();
        Assert.assertTrue(matches.contains("mySuperQuery"));
        System.out.println("PercolateResponse.matches: " + matches);
    }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Preference: Add _shards prefix to explicitly list shards, and add _prefer_node option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1904</link><project id="" key="" /><description>Add `_shards:` prefix to explicitly list shards, for example: `_shards:1,2` to search on the first shard. Additional preferences can be placed after it with `;` delimiter. For example: `_shards:0;_prefere_node:xhsVFER`.

Add `_prefer_node:` to prefere shards that exists on a specific node id. For example: `_prefere_node:xhsVFER`.
</description><key id="4394503">1904</key><summary>Search Preference: Add _shards prefix to explicitly list shards, and add _prefer_node option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-02T21:59:40Z</created><updated>2012-05-02T22:12:29Z</updated><resolved>2012-05-02T22:12:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Ubuntu WrapperSimpleApp Error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1903</link><project id="" key="" /><description>I've followed the instructions on the official website about how to install Elastic Search on Ubuntu but It's not working. I've got the following error http://pastie.org/3850745 when I try to run `sudo service elasticsearch start`.

Ubuntu version

&gt; Linux version 2.6.32-24-generic-pae (buildd@rothera) (gcc version 4.4.3 (Ubuntu 4.4.3-4ubuntu5) ) #39-Ubuntu SMP Wed Jul 28 07:39:26 UTC 2010
</description><key id="4394008">1903</key><summary>Ubuntu WrapperSimpleApp Error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rainerborene</reporter><labels /><created>2012-05-02T21:32:59Z</created><updated>2012-05-03T15:36:59Z</updated><resolved>2012-05-03T15:36:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-05-02T22:01:06Z" id="5473640">Are you using ES version 0.18? The new servicewrapper works with 0.19.
</comment><comment author="rainerborene" created="2012-05-03T15:36:59Z" id="5490509">I've just updated to the latest version and It worked. Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Java Date when serializing update parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1902</link><project id="" key="" /><description /><key id="4393236">1902</key><summary>Support Java Date when serializing update parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-02T20:50:02Z</created><updated>2012-05-02T20:59:51Z</updated><resolved>2012-05-02T20:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Merge Scheduler: Configuring using `serial` fails to load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1901</link><project id="" key="" /><description>Though full class name obviously works, we should fix it.
</description><key id="4389860">1901</key><summary>Index Merge Scheduler: Configuring using `serial` fails to load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-02T17:52:05Z</created><updated>2012-05-02T17:53:02Z</updated><resolved>2012-05-02T17:53:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: filtered query to support null filter or {} filter (in which case, just the query is executed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1900</link><project id="" key="" /><description /><key id="4383364">1900</key><summary>Query DSL: filtered query to support null filter or {} filter (in which case, just the query is executed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-02T11:58:02Z</created><updated>2012-05-02T11:58:45Z</updated><resolved>2012-05-02T11:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Network: Add network.address.serialization.resolve setting (defaults to false) to always resolve publish address based on host name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1899</link><project id="" key="" /><description>Note, this setting needs to be set on the actual nodes that publish their address, since thats where the address gets serialized. We could, potentially, in the future, always also serialize the host name, and then the flag can be set on the client side, but that will break backward comp.
</description><key id="4383015">1899</key><summary>Network: Add network.address.serialization.resolve setting (defaults to false) to always resolve publish address based on host name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-02T11:24:38Z</created><updated>2012-05-02T11:27:49Z</updated><resolved>2012-05-02T11:27:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Java API: Improve TransportClient in sniff mode to be more lightweight on connections and API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1898</link><project id="" key="" /><description>Two improvements that we can do:
1. For listed nodes added to the transport client, we can create a lightweight connection to them, since we end up connecting to the actual nodes listed by the cluster and using them.
2. We can use the cluster state API (local) to get the list of nodes, instead of using the more expensive node info API.
</description><key id="4380640">1898</key><summary>Java API: Improve TransportClient in sniff mode to be more lightweight on connections and API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.4</label><label>v0.20.0.RC1</label></labels><created>2012-05-02T08:43:54Z</created><updated>2012-05-02T08:44:27Z</updated><resolved>2012-05-02T08:44:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support facets with key from document and value from nested document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1897</link><project id="" key="" /><description>Would love to be able to have facets like term_stats and range get their value_field from matching nested documents and the key_field from the document containing the nested documents. For example [range facet](https://gist.github.com/1126866) so we can group by rating, then get the average price.

See also this discussion https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/C8ujVO3ljJk
</description><key id="4363059">1897</key><summary>Support facets with key from document and value from nested document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">deverton</reporter><labels /><created>2012-05-01T04:37:04Z</created><updated>2012-05-01T05:33:20Z</updated><resolved>2012-05-01T05:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deverton" created="2012-05-01T05:33:20Z" id="5433972">Sorry, search fail. Thought I hadn't already raised this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When a node disconnects from the cluster (not enough master nodes, or a client node) and rejoins it might not update its internal routing table</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1896</link><project id="" key="" /><description>See more here: https://groups.google.com/d/topic/elasticsearch/-L6jFujIcvI/discussion.
</description><key id="4343562">1896</key><summary>When a node disconnects from the cluster (not enough master nodes, or a client node) and rejoins it might not update its internal routing table</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-29T21:46:44Z</created><updated>2012-04-30T10:58:41Z</updated><resolved>2012-04-30T10:58:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-30T10:58:41Z" id="5415260">Fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange highlighting issue with custom_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1895</link><project id="" key="" /><description>When i DON&#8217;T give the type name in the URL , highlighting is coming in result.
And when i give the type name  in URL , highlighting is NOT working.

Version i am using - 0.19.2

Recreation - https://gist.github.com/2516361

Igor's observation - 
The script that Vineeth provided works fine in 0.18.5 and lower but breaks in 0.18.6 and higher.   
</description><key id="4341381">1895</key><summary>Strange highlighting issue with custom_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2012-04-29T15:21:46Z</created><updated>2012-04-29T15:24:05Z</updated><resolved>2012-04-29T15:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Vineeth-Mohan" created="2012-04-29T15:22:55Z" id="5405705">Duplicate to #1894
</comment><comment author="kimchy" created="2012-04-29T15:24:05Z" id="5405711">See #1894, it fixes it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Using "plain" (non term vector) highlighting with custom score within a filtered query fails to highlight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1894</link><project id="" key="" /><description>The usual problem with the non term vector based highlighting, since we can't really plug our logic into it, we need to try and extract the query to highlight ourself. So, we need to handle filtered query as well, and extract the query from it.
</description><key id="4341368">1894</key><summary>Highlighting: Using "plain" (non term vector) highlighting with custom score within a filtered query fails to highlight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-29T15:19:57Z</created><updated>2012-04-29T15:20:23Z</updated><resolved>2012-04-29T15:20:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>require_field_match works incorrect in a query which has nested query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1893</link><project id="" key="" /><description>I have the following mapping and data:

--Mapping:

```
curl -XPUT "http://localhost:9200/test?pretty=1" -d '
{
  "mappings": {
    "programlanguage": {
      "properties": {
        "name": {
          "type": "string"
        },
        "components": {
          "properties": {
            "name": {
              "type": "string"
            }
          },
          "type": "nested"
        }
      }
    }
  }
}
'
```

--Index data:

```
curl -XPUT 'http://localhost:9200/test/programlanguage/2' -d '{
    "name" : "java",
    "components" : [{"name":"java core"},{"name":"abc"},{"name": "bbb"}]
}'
```

-----query

```
{
  "query": {
    "bool": {
      "should": [
        {
          "text": {
            "name": {
              "query": "java core",
              "operator": "and"
            }
          }
        },
        {
          "nested": {
            "path": "components",
            "query": {
              "text": {
                "components.name": {
                  "query": "java core",
                  "operator": "and"
                }
              }
            }
          }
        }
      ],
      "minimum_number_should_match": 1,
      "boost": 1
    }
  },
  "highlight": {
    "tags_schema": "styled",
    "fields": {
      "name": {      }
    },
    "require_field_match": true
  }
}
```

With the query, I expected the highligh list is only "java" from "programlanguage" field which is not my expectation.
Please help me verify it
</description><key id="4341184">1893</key><summary>require_field_match works incorrect in a query which has nested query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">duythai</reporter><labels /><created>2012-04-29T14:48:37Z</created><updated>2014-02-14T18:05:27Z</updated><resolved>2014-02-14T18:05:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-05-13T23:20:45Z" id="17847624">Perhaps you can rephrase your question using github's formatting mechanism. It's hard to mentally parse your issue. Any additional context would help as well. Thanks.
</comment><comment author="javanna" created="2013-10-18T17:46:05Z" id="26615339">I tried to reproduce the issue but actually I'm not sure I even understood what the problem was.

You're querying both `name` and `components.name`, and you highlight the `name` field, where there is a match, and that's what you get back highlighted. @duythai can you please help me out with this?
</comment><comment author="javanna" created="2014-02-14T18:05:22Z" id="35108624">Closing for lack of activity, feel free to reopen if you still see the same issue and you have info on how to reproduce it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JDK 7 requires minimum of 256k stack size, update scripts to set it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1892</link><project id="" key="" /><description /><key id="4340990">1892</key><summary>JDK 7 requires minimum of 256k stack size, update scripts to set it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-29T14:13:10Z</created><updated>2012-04-29T14:16:06Z</updated><resolved>2012-04-29T14:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Parent object in fields result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1891</link><project id="" key="" /><description>Hello,

I implemented parent support for elasticsearch-head. Now if parent exists for a type, the parent _source fields are displayed in the browser table. (https://github.com/eugeis/elasticsearch-head/commit/9cf167a105abf420976745ac178ae07c3be1740a)

But in the current implementation, I have to collect from first result all parent ids and parent types and then send a second "mget" request in order to get parent objects and then integrate it in the first result.

It would be nice have similar to "fields { "_parent" } definition in search request also like "fields { "_parent_object" } to get the parent object itself.

What do you think?

Thanks
</description><key id="4337813">1891</key><summary>Parent object in fields result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eugeis</reporter><labels /><created>2012-04-28T22:11:22Z</created><updated>2014-07-08T14:53:18Z</updated><resolved>2014-07-08T14:53:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:53:18Z" id="48347611">Closing in favour of #761.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>adding fixIndex() method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1890</link><project id="" key="" /><description>Not for the faint hearted, but maybe Lucene's CheckIndex.fixIndex() method should be available in the case a new setting parameter index.shard.check_on_startup_and_fix is set to 'true' (default is 'false').
</description><key id="4328449">1890</key><summary>adding fixIndex() method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2012-04-27T19:52:09Z</created><updated>2014-06-12T08:48:00Z</updated><resolved>2012-04-27T21:39:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-27T20:21:09Z" id="5389497">Quick notes:
1. Lets change the setting `index.shard.check_on_startup` to allow to have `false` for not checking, `true` or `check` for just checking, and `check_and_fix`/`checkAndFix` for checking and fixing?
2. Why fix when its a successful check? We should only fix on a failed check.
</comment><comment author="jprante" created="2012-04-27T20:36:14Z" id="5389744">Ok, no new parameter, just changing 'check_on_startup' from boolean to string type, with possible values 'false' (default) for ignoring checking, 'fix' for fixing index (dangerous), and all others for just checking index.

Sigh. Logic changed, fixing index only if index check failed.
</comment><comment author="kimchy" created="2012-04-27T21:39:19Z" id="5390922">PUshed the change, also fixed not throwing an exception if we fix the index.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>implement explain for top_children query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1889</link><project id="" key="" /><description>It would be nice to have explain implemented for top_children that includes the explains for all of the top children considered to produce the parent hit, e.g. have a step like

...
- nn max score of children
  - [child 1 explain]
  - [child 2 explain]

...
</description><key id="4327066">1889</key><summary>implement explain for top_children query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kireet</reporter><labels><label>adoptme</label></labels><created>2012-04-27T18:19:40Z</created><updated>2015-08-26T14:03:18Z</updated><resolved>2015-08-26T14:03:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:51:08Z" id="48347240">and for has_child/has_parent
</comment><comment author="jpountz" created="2015-08-26T14:03:18Z" id="135032109">top_children has been removed, and hash_child/has_parent now have a basic explain output that gives the join value
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent shard-relocation of existing index upon creation of a new index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1888</link><project id="" key="" /><description>Hi,

I have a cluster of 4 nodes with an existing index (consisting of 32 shards, 1 replica).

When I add another new index to the cluster, the cluster begins to relocate shards of the "old" index.
I do not think that this is necessary.
Hint: During relocation of the "old" shards, there might be unallocated shards of the new index.

It seems that shard relocation of old index(es) triggers while the new index has not been fully created yet.

Robert.
</description><key id="4324414">1888</key><summary>Prevent shard-relocation of existing index upon creation of a new index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">snazy</reporter><labels /><created>2012-04-27T15:41:43Z</created><updated>2013-10-07T08:40:15Z</updated><resolved>2013-10-07T08:40:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-22T12:13:47Z" id="21339894">Hey,

is this still valid for you? Can you provide more information? JVM version, elasticsearch version, etc.
Can you reproduce this problem simply by creating one index with 32 shards and then create another one and then the first index is relocated?

More information would be really helpful! Thanks!
</comment><comment author="spinscale" created="2013-10-07T08:40:15Z" id="25792518">closing it for now. Please provide more information if this is still valid for you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog: Buffering translog does not write directly to the file channel but to RAF, which causes problems reading from the channel on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1887</link><project id="" key="" /><description>We need to write through the channel and not through RAF.
</description><key id="4324357">1887</key><summary>Translog: Buffering translog does not write directly to the file channel but to RAF, which causes problems reading from the channel on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-27T15:38:55Z</created><updated>2012-04-27T15:47:32Z</updated><resolved>2012-04-27T15:47:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>transport client disconnected after ping timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1886</link><project id="" key="" /><description>For some requests which return large amounts of data we are seeing the following exception.

```
JsQS0K1nRNCb8GNgA][inet[/192.168.1.100:9300]], disconnecting... 
org.elasticsearch.transport.ReceiveTimeoutTransportException: [prod-node-prod][inet[/192.168.1.100:9300]][cluster/nodes/info] request_id [143146] timed out after [5002ms] 
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:347) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
```

We have 2 nodes in the cluster, and are connecting using the transport client. 
We can reproduce this error by running a query that takes tens of seconds to complete. 
We're currently working around this by increase the value of client.transport.ping_timeout on the client to longer than the query takes to return.

It looks like this is related to the node disconnect on timeout introduced in https://github.com/elasticsearch/elasticsearch/commit/eb4f6709d97287b0c9de6af9bf5f4a42fcf98991
and doesn't seem to be fixed by https://github.com/elasticsearch/elasticsearch/commit/ac4aa17e167f74b47fd4862828f75fbcce745ede

We're running 0.19.0, but I've verified that the problem still exists in 0.19.2, and it was working fine on 0.18.7
I'll add a standalone re-creation shortly.
</description><key id="4306455">1886</key><summary>transport client disconnected after ping timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davie</reporter><labels /><created>2012-04-26T16:22:23Z</created><updated>2015-03-23T16:27:28Z</updated><resolved>2012-05-04T14:46:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="davie" created="2012-04-26T18:37:51Z" id="5364622">I've created a standalone testcase for this https://gist.github.com/2501687
I start with an empty elasticsearch instance, run the test, and get the exception:

```
 40051 [elasticsearch[generic]-pool-1-thread-1] INFO org.elasticsearch.client.transport  - [Aardwolf] failed to get node info for [#transport#-1][inet[localhost/127.0.0.1:9300]], disconnecting...
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[localhost/127.0.0.1:9300]][cluster/nodes/info] request_id [19] timed out after [5001ms]
        at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:347)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
```

uncommenting the line client.transport.ping_timeout and running again shows that it returns in ~60s for me.
It's not a particularly realistic case - I've created a small number of documents with really big text field. 
On our production system we were seeing it will ~10k docs, total size ~145 MB. 
</comment><comment author="kimchy" created="2012-04-27T21:20:21Z" id="5390560">Heya, I have started an embedded node and connected to it with the transport client using 0.19.2 and it seems to wrok fine. The ping now uses the "high" transport level and it should solve this problem (it doesn't fully solve it when sniff is enabled). Can you try this code and tell me if you get a failure?: https://gist.github.com/2513321
</comment><comment author="davie" created="2012-04-27T22:22:12Z" id="5391592">Hi I've just tried this.
It fails on 0.19.1, but you're right, it does work fine on 0.19.2.
I also tried an out of process ES instance and it gives the same results.
I'll upgrade our production-like instances and confirm that it's fixed there too.
Thanks
</comment><comment author="davie" created="2012-05-04T14:46:58Z" id="5511997">I've now retried 0.19.2 and checked 0.19.3 and they both work fine - (i think i previously ran a 0.19.2 transportclient against a 0.19.0 node) closing.
Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort by _geo_distance showing wrong order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1885</link><project id="" key="" /><description>Here is the complete data (index, mapping and data) for the tests:
https://gist.github.com/2483009

How is it possible that sorting by **_geo_distance (arc) ASC** results shows in following order:
Maybe i am doing something wrong or is it a bug? Thanks.

```

Pos Name        Location                        Distance
---|----------|-------------------------------|-------------------
1   far         56.9440017845,24.117343883      1.7865229796151898
2   average     56.9624382022,24.1378033877     1.4899987260417218
3   closest     56.958344835,24.110908139       0.243505338088285
4   farthest    56.9428606076,24.077425299      2.9042299224497112
```
</description><key id="4268168">1885</key><summary>Sort by _geo_distance showing wrong order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darklow</reporter><labels /><created>2012-04-24T19:52:49Z</created><updated>2012-04-27T08:56:14Z</updated><resolved>2012-04-24T21:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="darklow" created="2012-04-24T21:23:15Z" id="5317858">Spent half a day running tests, googling and finally decided it could be a bug and even reported. Thanks to @imotov noticed this is because of different GeoJson format [lon, lat] - opposite of how we used to set lat lon arguments.

There should be some warning or exclamation marks like **Important notice** on docs where it mentions format changes :) 
It too easy to mis-read lat, lon vs lon, lat.
</comment><comment author="kimchy" created="2012-04-27T08:56:14Z" id="5377386">Heya, its hard to find it, but its here: http://www.elasticsearch.org/guide/reference/api/search/sort.html in the formatting for sorting. The funny thing is that the array structure used to be the other way around, and we changed it to match the geojson spec...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Histogram Facet: Add `quarter` as an interval </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1884</link><project id="" key="" /><description /><key id="4262700">1884</key><summary>Date Histogram Facet: Add `quarter` as an interval </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-24T16:03:45Z</created><updated>2012-04-24T16:04:17Z</updated><resolved>2012-04-24T16:04:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>fixed always returning 1 if launching in background without pidpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1883</link><project id="" key="" /><description>Currently, the returned value depends on this line:

[ ! -z "$pidpath" ] &amp;&amp; printf '%d' $! &gt; "$pidpath"

rather than 

exec "$JAVA" ...

which is just before. This certainly isn't intended as a successful start (script returning 0) shouldn't depend on the creation of a PID file.
</description><key id="4255441">1883</key><summary>fixed always returning 1 if launching in background without pidpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels /><created>2012-04-24T08:14:02Z</created><updated>2014-07-16T21:55:24Z</updated><resolved>2012-04-24T12:11:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-24T12:11:45Z" id="5302955">Pushed to master and 0.19 branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing TextQueryBuilder and FuzzyQueryBuilder methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1882</link><project id="" key="" /><description>A couple of methods in query builders seem to be missing.
</description><key id="4241388">1882</key><summary>Add missing TextQueryBuilder and FuzzyQueryBuilder methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-04-23T14:34:05Z</created><updated>2014-06-28T08:31:56Z</updated><resolved>2012-04-24T12:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-24T12:10:32Z" id="5302937">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to add pure load balancer that doesn't do aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1881</link><project id="" key="" /><description>Currently if you set `node.data=false` and `node.master=false` you can set up a load-balancer-ish node that just routes queries to other boxes, but the problem for some setups is that those nodes do the aggregation phase as well.

I'd like to have the ability to set up a local ES node that _only_ does proxying to other nodes in the cluster. I.e. it would pick some node, run a query on it, and then that node would ask other machines for data if needed, do the aggregation, and only return results.

This would enable me to use ElasticSearch as a daemon on webserving boxes that keeps track of what nodes are currently active / OK,  I could make queries to it on localhost:9200, but because it wouldn't do any aggregation the resources it would eat would be very predictable.
</description><key id="4238071">1881</key><summary>Add ability to add pure load balancer that doesn't do aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avar</reporter><labels /><created>2012-04-23T10:50:47Z</created><updated>2013-07-15T17:14:18Z</updated><resolved>2013-07-15T17:14:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-15T17:14:18Z" id="20984760">duplicate of #2448
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_boost do not affect geo_point score when we add document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1880</link><project id="" key="" /><description>we add 2 geo documents
1:  { ... "geo":{"lat":50,"lon":100}, "_boost":5.0}  for id1 with type restaurant
2:  { ... "geo":{"lat":50,"lon":100}, "_boost":1.0}  for id2 with type restaurant

then we try to search:
{
  "query" : {
    "match_all" : {
    }
  },
  "facets" : {
    "geo" : {
      "geo_distance" : {
        "restaurant.geo" : [ 100.0, 50.0 ],
        "ranges" : [ {
          "to" : 100.0
        } ],
        "unit" : "km"
      }
    }
  }
}
and get 2 hits with the same score 1.0

but indeed we need one score x5.0
</description><key id="4234987">1880</key><summary>_boost do not affect geo_point score when we add document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">levyfan</reporter><labels /><created>2012-04-23T05:57:53Z</created><updated>2013-07-22T12:47:43Z</updated><resolved>2013-07-22T12:47:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-22T12:47:43Z" id="21341418">a match_all query does not do a real scoring computation, but rather returns always a constant for the boost (and a constant for the score).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix issue 1870, clear the top docs phase before re-executing the search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1879</link><project id="" key="" /><description>in 1870 i reported that when the top children query must do an incremental search, no results were being returned. I found this happened because in TopChildrenQuery.createWeight(), there is an if/else there based on if parentDocs is not null. As a fix, in QueryPhase I added a call to clear(). I also edited a test case to create a reproduction scenario.
</description><key id="4230232">1879</key><summary>fix issue 1870, clear the top docs phase before re-executing the search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kireet</reporter><labels /><created>2012-04-22T16:11:14Z</created><updated>2014-07-16T21:55:25Z</updated><resolved>2013-08-12T11:21:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-12T11:21:28Z" id="22486733">Issue doesn't occur any more, see #1870 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.4.1.Final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1878</link><project id="" key="" /><description /><key id="4223440">1878</key><summary>Upgrade to Netty 3.4.1.Final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-21T13:01:50Z</created><updated>2012-04-21T13:03:24Z</updated><resolved>2012-04-21T13:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Concurrent update failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1877</link><project id="" key="" /><description>(the title is not amazing, but until we figure out whats going on, its good enough). more info in this thread: https://groups.google.com/d/topic/elasticsearch/oONRce0IbjA/discussion.

```
curl -XPOST "http://localhost:9200/sample-index" -d '
{
  "settings" : { "number_of_shards" : 1, "number_of_replicas" : 0 },

  "mappings" : {
    "sample" : {
       "properties" : {
          "names": {
            "properties" : {
              "value": {  "type" : "string", "index" : "no" },
              "count": { "type" : "integer", "index" : "no" }
            }
          }
       }
    }
  }
}
'
```

And here is the self-contained java code:

```
import org.elasticsearch.action.get.GetResponse;
import org.elasticsearch.client.Client;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.transport.InetSocketTransportAddress;


public class ElasticUpdateTest {

  static final String updateScript = 
      "int i=0; "+
      "found=false; "+
      "while (!found &amp;&amp; i &lt; ctx._source.names.size()) { "+
        "if (ctx._source.names.get(i).value == name) { " +
           "found = true; " +
        "} " +
        "i++; " +
      "} "+
      "if (found) { " +
        "ctx._source.names.get(i-1).count += 1; " +
      "} " +
      "else { " +
        "ctx._source.names.add({'value' : name, 'count' : 1 }); " +
      "} ";

   static final String indexName = "sample-index";

   static final String indexType = "sample";

    private static class ElasticUpdate
        implements Runnable {
        public void run() {
          for(int i = 0; i&lt;100000;i++) {
            String name = "name"+(i % 100);
            String key = "key"+ (i % 11);
            //System.out.println("Thread "+Thread.currentThread().getName()+" updating "+key+" with name "+name);
            GetResponse getResponse = client.prepareGet(indexName, indexType, key).execute().actionGet();
            if (!getResponse.exists()) {
                client.prepareIndex(indexName, indexType, key).setSource("{ \"names\" : [] }")
                  .execute()
                  .actionGet();
            }
            client.prepareUpdate(indexName, indexType, key)
              .setRetryOnConflict(20)
              .setScript(updateScript)
              .addScriptParam("name", name)
              .execute()
              .actionGet();
          }  
        }

        Client client;

        ElasticUpdate (Client client) { 
          this.client = client;
        }
    }

    public static void main(String args[]) throws InterruptedException {
        Client client = new TransportClient()
          .addTransportAddress(new InetSocketTransportAddress("localhost", 9300));

        int nthreads = 4;
        if (args.length &gt; 0) {
            try {
                nthreads = Integer.parseInt(args[0]);
            } catch (NumberFormatException e) {
                System.err.println("Argument must be an integer.");
                System.exit(1);
            }
        }
        System.out.println("Starting "+nthreads+" threads");
        for(int i=0;i&lt;nthreads;i++) {
          Thread t = new Thread(new ElasticUpdate(client));
          t.start();
        }
    }
}
```
</description><key id="4223150">1877</key><summary>Concurrent update failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label></labels><created>2012-04-21T11:54:57Z</created><updated>2013-02-22T22:50:39Z</updated><resolved>2013-02-22T22:50:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghost" created="2012-04-23T14:46:34Z" id="5282748">I am running 64-bit Windows 7 Service Pack 1 on an Intel Core i7 CPU (8 cores). I will try to reproduce the problem on linux. 
</comment><comment author="kimchy" created="2012-04-27T15:40:07Z" id="5383981">After a session with @zmcr (thanks!), we figured out the issue. Opened a specific issue for it: #1887. We can close this once 0.19.3 is out and we double verify.
</comment><comment author="s1monw" created="2013-02-18T22:40:19Z" id="13747150">@kimchy is this still valid? It seems we can close this, no?
</comment><comment author="kimchy" created="2013-02-22T22:50:39Z" id="13977417">yea, I think its safe to close this. Closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JavaAPI:  UncategorizedExecutionException when trying execute a Multiget Request with an empty results set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1876</link><project id="" key="" /><description>With issuing a bulk the java API, when I do a multiget into elasticsearch,  If there are no results for the multiget I get a UncategorizedExecutionException returned from the client.  The detail in the error says that the server had a NullPointerException. (likely when trying to peek into the empty results).

When I run in EC2, I do not see this happening.  This is error is present in both ElasticSearch-0.18.5, as well as ElasticSearch-0.19.0.

I have found that this can be circumvented by catching and ignoring the error.
</description><key id="4215145">1876</key><summary>JavaAPI:  UncategorizedExecutionException when trying execute a Multiget Request with an empty results set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">echaz</reporter><labels /><created>2012-04-20T17:50:35Z</created><updated>2013-07-05T10:00:38Z</updated><resolved>2013-07-05T10:00:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="echaz" created="2012-04-20T17:53:48Z" id="5249590">some sample code that I am using:

```
MultiGetRequestBuilder request = esClient.prepareMultiGet();

    // stick all the resources into a single request:
    for (Resource thisResource : resources) {
        request.add("myindex",null,resource.id);
    }

    // Get a response from ElasticSearch
    MultiGetResponse response; 

    try {
        response = request.execute().actionGet();
    }

    // I haven't yet seen this in EC2.  Ignore the error, and return null.
    catch (UncategorizedExecutionException e) {
        logger.warn("This shouldn't happen: " + e);
        return null;
    }
```
</comment><comment author="kimchy" created="2012-04-21T09:42:16Z" id="5259336">Can you gist the full stack trace of the failure that you get?
</comment><comment author="echaz" created="2012-04-23T12:57:36Z" id="5280383">Sorry...

https://gist.github.com/2470724
</comment><comment author="kimchy" created="2012-04-23T14:56:47Z" id="5283033">Upgrade to 0.19.2 it was fixed there.
</comment><comment author="echaz" created="2012-05-03T13:32:01Z" id="5487527">Thank you, I will upgrade ASAP...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better handling of fields that have `.` in their name when doing property based navigation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1875</link><project id="" key="" /><description>A field named `doc.text` and using it to highlight based on source will fail because it expects to find a json object named `doc` with a field named `text`.
</description><key id="4192190">1875</key><summary>Better handling of fields that have `.` in their name when doing property based navigation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-19T14:04:13Z</created><updated>2012-04-19T14:28:21Z</updated><resolved>2012-04-19T14:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RFE: Whole cluster startup / shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1874</link><project id="" key="" /><description>Hi,

I'd like to have a feature to gracefully shutdown all cluster nodes.

During a "normal" shutdown procedure, where one node is shut down after another, remaining nodes start to rebalance shards. This is not what I want, when I shutdown the whole cluster.

After starting the whole cluster, many shards get re-initialized and re-balanced, which is not necessary (because data should be consistent).
But since it is not possible to start up all nodes exactly at once (within the same "nanosecond" ;-) ), it would be nice to have a "cluster startup feature". All nodes should wait for all other nodes to start up and setup the cluster again without too much re-initialization and re-balancing.

Robert
</description><key id="4190063">1874</key><summary>RFE: Whole cluster startup / shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snazy</reporter><labels /><created>2012-04-19T11:49:51Z</created><updated>2012-07-04T09:31:39Z</updated><resolved>2012-07-04T09:31:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeremy" created="2012-04-20T17:51:28Z" id="5249554">You can disable shard and replica allocation, then shutdown.

http://www.elasticsearch.org/guide/reference/modules/cluster.html
</comment><comment author="kimchy" created="2012-04-22T09:15:11Z" id="5266577">Also, you have the shutdown API where you can shutdown a whole cluster, and the `gateway.recover_after_nodes` to make sure to start recovery (and allocation) only after the number of nodes are part of the cluster.
</comment><comment author="snazy" created="2012-07-04T09:31:39Z" id="6756265">Functionality to shutdown all cluster nodes exists, as jeremy posted ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes drop their copy of auto-expanded data when coming up, only to sync it again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1873</link><project id="" key="" /><description>When you have an index with `index.auto_expand_replicas=0-all` running on 3 nodes and you bring down one node the number of replicas will be reduced by the master from 2 to 1. Then when the node that just went down comes up again ElasticSearch on that node will will:
1. Go up, notice that the number of replicas for that index is 1, and promptly drop its own data as redundant
2. The master will notice that it has a new node in the cluster, set the number of replicas to 2.
3. The node that just dropped its data will now have the data it just dropped re-synced to it.

Instead ElasticSearch should:
1. Go up, wait for the master to adjust the number of replicas if needed
2. Only after that's done drop anything, if needed.
3. Not re-sync any data since it didn't drop the data in the brief interim when the master was adjusting the number of replicas from 1 to 2.

This'll aid recovery time where you have a setup where a relatively small index is available on all the nodes for capacity reasons, and you bring up a new node that should serve search requests right away.
</description><key id="4175410">1873</key><summary>Nodes drop their copy of auto-expanded data when coming up, only to sync it again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">avar</reporter><labels><label>:Allocation</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2012-04-18T16:16:56Z</created><updated>2017-03-07T15:22:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-07-02T08:23:16Z" id="6702835">Yeah, I was just hit by this one too.  Wonder what happens if you disable reallocation before shutting down?
</comment><comment author="martijnvg" created="2014-07-18T09:23:07Z" id="49411619">Perhaps scheduling the deletion of the physical shard files when a shard is no longer allocated on a node can help here. Then there is time window the master node can react the the node rejoining and the deletion of physical shard files can be cancelled.
</comment><comment author="portante" created="2017-03-07T13:50:49Z" id="284726212">@ywelsch, @clintongormley, is this issue going to be addressed for 2.4.x any time soon?  Is this issue a problem with 5.x?</comment><comment author="lukas-vlcek" created="2017-03-07T15:22:00Z" id="284752062">@portante I just tested with ES `v5.2.2` and I am able to replicate this issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to do one-way replication to hosts that won't be promoted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1872</link><project id="" key="" /><description>If you had two datacenters for say log data and configured the shard allocation such that the logging data from datacenter A would never migrate to datacenter B you'd be completely safe when it comes to split-brains, since there'll never be a question of which group owns the data.

This'll allow you to do searches across datacenters, but all the searches will have to span two datacenters to get results, which depending on your query load might be a problem.

You could alleviate that by replicating shards that you only ever insert to in datacenter A from datacenter A to B,  but you now have the problem that if you have a split brain you'll have those replicas in datacenter B elect themselves as primary shards.

This could be fixed by allowing you to not only replicate data, but also declare that that data will _never_ become primary on nodes with some given attributes.

This allows you to emulate traditional RDBMS replication where the data has one canonical source, and a cutoff with the primary source won't result in the local receiver of the replicated data electing itself as the master. Then when your connection re-establishes you'll happily sync updates over.
</description><key id="4175324">1872</key><summary>Add ability to do one-way replication to hosts that won't be promoted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avar</reporter><labels /><created>2012-04-18T16:12:14Z</created><updated>2014-08-01T16:41:23Z</updated><resolved>2014-08-01T16:41:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T09:10:14Z" id="49410568">The tribe node now allows you to do cross data-centre cluster.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index issues on restarts with defaults</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1871</link><project id="" key="" /><description>Elasticsearch 0.19.2
Init script:  https://github.com/arimus/log-processing/blob/master/rpmbuild/SOURCES/init.d-elasticsearch
Java version: java-1.6.0-openjdk-1.6.0.0-1.43.1.10.6.el6_2.x86_64

This issue is predictable and happens every restart.  I've seen other posts that seem related to indexes in memory, however I'm using the defaults which should be indexes on disk.  FYI, I have anywhere from 1k-2k messages/sec coming in through the rabbitmq-river and have about 100M+ entries in elasticsearch.  I'm restarting as messages are flooding in.

Additionally...I have had to kill elasticsearch manually and restart it.  I did have a couple logstash instances with embedded elasticsearch 0.19.2 fire up and multicast try to connect to this elasticsearch instance, which gave me the following exceptions in the logs.  Not sure if either of those issues could have corrupted my indexes somehow...?

Any ideas would be appreciated.
# MULTICAST EXCEPTIONS

[2012-04-14 15:28:25,270][WARN ][discovery.zen.ping.multicast] [Fusion] failed to connect to requesting node [DuQuesne, Jacques][NycDaOkvTX66fpepYRPEXA][inet[/xxxx:9301]]{client
org.elasticsearch.transport.ConnectTransportException: [DuQuesne, Jacques][inet[/xxxx:9301]] connect_timeout[30s]
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:560)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:503)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:482)
        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:128)
        at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver$1.run(MulticastZenPing.java:530)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
Caused by: java.net.ConnectException: Connection timed out
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:592)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.connect(NioClientSocketPipelineSink.java:400)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:362)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:284)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        ... 3 more
# INDEX EXCEPTIONS

Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:38,765][WARN ][indices.cluster          ] [Charlie-27] [logstash-2012.04.14][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:107)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
[2012-04-18 03:19:38,789][WARN ][cluster.action.shard     ] [Charlie-27] sending failed shard for [logstash-2012.04.14][2], node[I__43ZnNSxSUyZzjuwagPg], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:38,789][WARN ][cluster.action.shard     ] [Charlie-27] received shard failed for [logstash-2012.04.14][2], node[I__43ZnNSxSUyZzjuwagPg], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:38,947][WARN ][indices.cluster          ] [Charlie-27] [logstash-2012.04.14][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:107)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
[2012-04-18 03:19:38,953][WARN ][cluster.action.shard     ] [Charlie-27] sending failed shard for [logstash-2012.04.14][2], node[I__43ZnNSxSUyZzjuwagPg], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:38,953][WARN ][cluster.action.shard     ] [Charlie-27] received shard failed for [logstash-2012.04.14][2], node[I__43ZnNSxSUyZzjuwagPg], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:39,310][WARN ][indices.cluster          ] [Charlie-27] [logstash-2012.04.14][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:107)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
[2012-04-18 03:19:39,315][WARN ][cluster.action.shard     ] [Charlie-27] sending failed shard for [logstash-2012.04.14][2], node[I__43ZnNSxSUyZzjuwagPg], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:39,315][WARN ][cluster.action.shard     ] [Charlie-27] received shard failed for [logstash-2012.04.14][2], node[I__43ZnNSxSUyZzjuwagPg], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2012.04.14][2] shard allocated for local recovery (post api), should exists, but doesn't]]]
[2012-04-18 03:19:39,697][INFO ][river.rabbitmq           ] [Charlie-27] [rabbitmq][my_river] creating rabbitmq river, host [localhost], port [5672], user [guest], vhost [/]
[2012-04-18 03:19:39,814][INFO ][river.rabbitmq           ] [Charlie-27] [rabbitmq][logstash-sl01] creating rabbitmq river, host [216.70.64.111], port [5672], user [guest], vhost [/]
</description><key id="4168446">1871</key><summary>Index issues on restarts with defaults</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arimus</reporter><labels /><created>2012-04-18T08:38:19Z</created><updated>2012-04-30T17:26:47Z</updated><resolved>2012-04-21T03:18:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-04-21T08:00:59Z" id="5258832">You have a crashed index. The reason can be OOM, and the Java VM can't continue when resources are tight, even writing the index will fail (but cluster state updates might not). There should be more severe errors in the logs, stating "OutOfMemoryException". The situation can become worse when you kill elasticsearch with "kill -9", instead try stop indexing and shut it down with the shutdown API call. To avoid OOM, adjust JVM heap memory to a size as high as possible for your system. Don't rely on the default settings, they are very small, for instance for logstash.
</comment><comment author="arimus" created="2012-04-22T06:07:39Z" id="5265820">So once this happens, is there any way to recover/repair the index?  I assume there is some way to get indexes happy again, rather than having to throw everything away?

Btw, on restart there are no memory issues, although there may have been initially.  When elasticsearch tries to spin back up again (like during a regular restart), these exceptions are still present.  

p.s. I have since pushed the indexes up to 16GB, which seems to have helped some of my issues although these restart issues persist.

Thanks
</comment><comment author="kimchy" created="2012-04-27T08:51:48Z" id="5377338">OOM will not cause the index or shards to be removed, and it seems liek they did. Did logstash that you started had an elasticsearch version 0.18.x? Was it on the same box as elasticsearch?
</comment><comment author="arimus" created="2012-04-27T14:10:54Z" id="5382138">Nothing appeared to be removed actually.  It's just that the indexes seemed to have issues upon restart.  I did initially have logstash start up with elasticsearch embedded and try to multicast join this instance (I thought unsuccessfully), however I built logstash with ES 0.19.2 and not 0.18.x.  You can see the patch that I applied here if you want...

https://github.com/arimus/log-processing/blob/master/rpmbuild/SOURCES/logstash_amqp09_queuefix.patch

All that said, if an index does get into a corrupt (even partially) state, is there any way to repair it or do you just need to delete it?  It appeared that despite the log messages with errors, the index was still happily being updated and was searchable.  It's didn't seem completely foobar'd, which is why I assumed there might be something to go in and do some form of consistency check on it.
</comment><comment author="jprante" created="2012-04-27T16:14:47Z" id="5384694">There is an index checker at org.apache.lucene.index.CheckIndex but it is not exposed via an Elasticsearch API yet. Nice opportunity for writing a plugin, though.
</comment><comment author="arimus" created="2012-04-27T16:27:30Z" id="5384941">Good to know, thanks.
</comment><comment author="jprante" created="2012-04-27T18:12:38Z" id="5387168">Hm. I was half in error. CheckIndex is already integrated in Elasticsearch, I just discovered a setting index.shard.check_on_startup. This parameter can be set to 'true' (default is 'false') and a node will run the checkIndex() method of org.apache.lucene.index.CheckIndex at startup time on each shard. This may take a long time to complete. But, CheckIndex also offers a fixIndex() method, and that is not used when setting index.shard.check_on_startup to 'true'. Well, in the end, I think an extra step of index repair after a successful check could be triggered by a new parameter index.shard.check_on_startup_and_fix or something - of course, only for the not so faint hearted. I'll try to set up a pull request.
</comment><comment author="arimus" created="2012-04-29T01:55:32Z" id="5401934">Yeah, that would be pretty handy.  It might also be nice to expose a flag via the API that simply returns if any runtime index errors have occurred and/or the startup CheckIndex found any issues.  This way, a fix index could be triggered, preferably also through the management API, at the most convenient time: maintenance window, low traffic hours, etc.

What would really be nice is if you could keep that specific index offline and repair it while everything else was up and running, then bring it back online once ready.  Of course that's much more complicated, since you probably still need to be able to receive updates for that index as soon as ES comes back online (e.g. for todays index which got corrupted, but still has more updates coming in).  Then you'd have to rename the corrupt index and either merge those entries back in once fixed (likely very heavy) or let apps include the new index in their searches (by convention, all index searches, etc.?)

/foodForThought
</comment><comment author="jprante" created="2012-04-30T10:02:11Z" id="5414606">arimus, kimchy accepted a pull request, fixIndex() is now included: https://github.com/elasticsearch/elasticsearch/pull/1890 

Exposing a fix method via API has some drawbacks. First, fixing the Lucene segments while the node is up is damn risky. It would impact the whole cluster performance and stability, and would not stop the cluster from being more unstable, you described the problem when new data is stil coming in. Second, node shutdown and startup API usage for checking and fixing indexes is potentially a candidate for race conditions. So, a node must leave the cluster and be stopped to perform the lengthy check and the subsequent optional fix. 

The method which is implemented by kimchy is to take the defect node down (shutdown api or kill), let the ES cluster readjust to the new situation, enable a "fix" check setting value explicitly in the defect node index setting and start the node again. Then, the node will enter a quite lengthy check and repair mode, before it re-enables all the index structures and tries to rejoin the cluster.

A warning: because fixing a broken Lucene segment can lead to data loss, this is not for the faint hearted. It should never be assumed fixIndex() should be called on a regular basis at every startup. There is no guarantee at all that fixIndex() will cure all defects. It is a last resort for the advanced adaministrator if nothing else helps and data loss is accepted.
</comment><comment author="arimus" created="2012-04-30T16:56:25Z" id="5421871">Great, thanks for adding that!  And yes, I understand the difficulties in trying to do any type of repairs while any of the cluster is online.  That was more of a theoretical nice to have :)  With a multi-node ES cluster, it would probably fairly pointless anyways.

Question though.  If you have a couple nodes, fix that node's corrupt indexes (results in some data loss), then re-add back into the cluster...will the ES cluster detect and replicate data from the good node back over to the repaired node or is replication only handled at the time that data was initially added?  Basically, what practical affects does this have on the overal cluster state and health?  If there is already documentation on this somewhere, please advise...I don't want to eat up more of your time.

Thanks again.
</comment><comment author="jprante" created="2012-04-30T17:18:48Z" id="5422325">If you had a good index before, well, the bad index should already have been replaced with good data by the ES recovery even without the fixIndex() method. Fixing the index should only be a method for ultima ratio, such as the recovery fails badly for a non-replicated index and ES finds no way to continue with a working index, with weird exceptions showing up in the log file, and there is no source data backup available so re-indexing is not an option.
</comment><comment author="arimus" created="2012-04-30T17:26:47Z" id="5422520">Sounds completely reasonable.  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top_children not returning results when incremental child query required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1870</link><project id="" key="" /><description>I am facing an issue where a top_children query is not returning results when the code must issue an incremental child query to try to get more results. I think this is clearly a bug b/c when I set the size such that the first query suffices, results are returned for the same query. I have created a gist to reproduce the problem, basically it just creates an index with a single parent doc and multiple child docs. Then it does 2 top_children queries with identical queries but differing size, factor and incremental factor values. One returns results and the other doesn't. This is on 0.19.2

https://gist.github.com/2382459
</description><key id="4159357">1870</key><summary>top_children not returning results when incremental child query required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kireet</reporter><labels /><created>2012-04-17T19:11:31Z</created><updated>2013-08-12T11:21:28Z</updated><resolved>2013-08-12T11:20:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-12T11:20:36Z" id="22486695">@kireet Not sure when this was fixed, but this issue doesn't occur in version 0.90.0 and later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow wildcards for aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1869</link><project id="" key="" /><description>Allowing a wildcard in an alias definition would be really great. For example, { "add" : { "index" : "test-*", "alias" : "test" } } would add an alias that mapped /test to /test-1, /test-2, /test-3, and any other current and future test-* aliases created. This would simplify dated indicies such as a rolling 30 days of data where the oldest day is dropped each night and a new index created. There would be no need to add or remove aliases after the initial one was set up.
</description><key id="4159228">1869</key><summary>Allow wildcards for aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jacwright</reporter><labels /><created>2012-04-17T19:03:43Z</created><updated>2012-07-13T15:15:24Z</updated><resolved>2012-07-13T15:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrewclegg" created="2012-06-30T14:50:38Z" id="6685771">Another option would be to allow wildcards in the search URL, e.g.

http://localhost:9200/twitter_2012*/tweet/_search

or

http://localhost:9200/twitter_*/tweet/_search

In some ways this would be simpler as you could just define what indexes to hit at query time, without bothering with keeping aliases up to date.
</comment><comment author="jacwright" created="2012-07-12T22:17:25Z" id="6949482">That works.
</comment><comment author="andrewclegg" created="2012-07-13T14:05:20Z" id="6963944">jacwright -- that feature was actually added (quietly!) in the last release:

http://www.elasticsearch.org/blog/2012/07/02/0.19.8-released.html
</comment><comment author="jacwright" created="2012-07-13T15:15:24Z" id="6965761">sweet!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoNodeAvailableException after 2 hours of bulk indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1868</link><project id="" key="" /><description>Hi,

I continuously receive the following exceptions after my bulk indexer runs for appox 2 hours.

I'm using a cluster with 4 elasticsearch nodes and all nodes were always running. One process issues bulk request with 100 index requests each with a throughput of about 1000~2000 docs per second.

The elasticsearch server log files say nothing.

org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:214)
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:182)
    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:97)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:141)
    at org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:295)
    at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:128)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
</description><key id="4156338">1868</key><summary>NoNodeAvailableException after 2 hours of bulk indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">snazy</reporter><labels /><created>2012-04-17T16:23:45Z</created><updated>2016-11-10T17:38:14Z</updated><resolved>2013-09-16T07:38:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GrantGochnauer" created="2012-04-21T13:47:47Z" id="5260604">We also receive this exact same problem - seemingly randomly with ES version 0.19.2 on both server and client. 

We have been unable to pin point down the problem but we have tried running multiple clusters, a single cluster, etc on the server side and it doesn't seem to effect anything. 

It happens almost daily for us and we're looking for ways to narrow down why this is. Using ES head, our cluster health is green and there is nothing that appears out of the ordinary.

The exception I received just a few moments ago:

org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:214)
    org.elasticsearch.action.TransportActionNodeProxy$1.handleException(TransportActionNodeProxy.java:77)
    org.elasticsearch.transport.TransportService$Adapter$2$1.run(TransportService.java:310)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
</comment><comment author="GrantGochnauer" created="2012-04-21T13:50:07Z" id="5260612">The other exception we get that results in a NoNodeAvailable Exception is this:

org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:214)
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:182)
    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:97)
    at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:171)
    at org.elasticsearch.client.transport.TransportClient.get(TransportClient.java:315)
    at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:126)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
</comment><comment author="holdenk" created="2012-04-23T20:23:26Z" id="5290808">I'm curious, are you using the transport client in sniff mode or in regular mode?
</comment><comment author="jereanon" created="2012-04-23T20:27:54Z" id="5290928">@holdenk -- @GrantGochnauer is using ES in regular mode.
</comment><comment author="holdenk" created="2012-04-23T20:29:43Z" id="5290970">You might want to try it with sniff mode turned on and see if it still happens. I can take a look at the regular mode code, but I know we used to see it in sniff mode and then fixed a bug for in the sniff mode transport and it worked.
</comment><comment author="jereanon" created="2012-04-23T20:54:20Z" id="5291562">@holdenk thanks for the advice, we'll give it a try.
</comment><comment author="jereanon" created="2012-04-25T21:09:01Z" id="5343000">@holdenk - Unfortunately this problem still happens with what seems to be the same frequency as it did before turning sniff mode on.
</comment><comment author="snazy" created="2012-05-02T09:48:22Z" id="5457522">As a workaround I just coded, that my Indexer shall retry, if such an Exception occured:

while (true) {
  try {
    bulk.execute().actionGet(getRetryTimeout());
    break;
  }
  catch (NoNodeAvailableException cont) {
    Thread.sleep(5000);
    continue;
  }
}
</comment><comment author="kimchy" created="2012-05-02T15:19:56Z" id="5463961">Which version are you using? Also, can you turn on logging to TRACE on org.elasticsearch.client.transport to see why it gets disconnected?
</comment><comment author="snazy" created="2012-05-03T10:18:32Z" id="5484680">I used elasticsearch 0.19.2

I'm not sure - but this might be caused due to our networking components (sometimes some SSH sessions get disconnected without any obvious reason - ssh is configured with keep-alive).
Is it possible to add some "magic" that a lost connection is re-established and that the connection will definitly fail after a configurable timeout?

For example:
When TransportClientNodesService$RetryListener.onFailure detects that all nodes have been asked, it waits a configureable amount of time (e.g. 100ms) and tries to re-establish the connection - if that fails after a configureable number of loops, it issues "listener.onFailure(new NoNodeAvailableException());"
</comment><comment author="GrantGochnauer" created="2012-05-03T10:22:19Z" id="5484729">We are also using 0.19.2 and get the NoNodeException when both the ES server process and ES client process are on the same physical machine without any network requirements.

We're trying to use snazzy's Thread.sleep code above as a work-around. We've also enabled Trace logging.
</comment><comment author="kimchy" created="2012-05-03T18:43:48Z" id="5495015">It would be interesting to see the logs that I asked for...
</comment><comment author="jereanon" created="2012-05-03T18:49:01Z" id="5495147">@kimchy - This seems related:

13:29:23.306 [New I/O client worker #1-3] WARN  org.elasticsearch.transport.internalWarn[104] - [Aardwolf] Received response for a request that has timed out, sent [9546ms] ago, timed out [1526ms] ago, action [cluster/nodes/info], node [[Damian, Margo][UIg42PPiS0SE2X_HO3wLFA][inet[/192.168.150.62:9301]]], id [1831]
13:29:26.855 [elasticsearch[generic]-pool-1-thread-3] INFO  org.elasticsearch.client.transport.internalInfo[99] - [Aardwolf] failed to get node info for [Damian, Margo][UIg42PPiS0SE2X_HO3wLFA][inet[/192.168.150.62:9301]], disconnecting...
org.elasticsearch.transport.ReceiveTimeoutTransportException: [Damian, Margo][inet[/192.168.150.62:9301]][cluster/nodes/info] request_id [1831] timed out after [8020ms]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:347) [elasticsearch-0.19.2.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) [na:1.7.0]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) [na:1.7.0]
    at java.lang.Thread.run(Thread.java:722) [na:1.7.0]
</comment><comment author="kimchy" created="2012-05-03T18:51:02Z" id="5495195">Yea, so timeout is not a disconnection... . Are you using sniff or not now? Turn off sniffing, add several nodes, and use 0.19.3, lets see if you still get it? (there are more improvements when sniff is set in 0.19.4 coming up).
</comment><comment author="jereanon" created="2012-05-25T17:16:21Z" id="5935655">Hello @kimchy,

We're using 0.19.3 and sniff is turned on. 

I put together a simple reconnect piece of code that attempts to reconnect a few times before giving up. Today we saw an interesting stacktrace that we haven't seen before:

08:54:39.694 [http-bio-8080-exec-65] WARN c.v.p.s.r.ElasticSearchRepository - NoNodeAvailableException caught, retrying request.
Exception in thread "elasticsearch[generic]-pool-1-thread-3" java.lang.StackOverflowError
at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:598)
at java.lang.StringBuilder.append(StringBuilder.java:212)
at java.net.Inet4Address.numericToTextFormat(Inet4Address.java:349)
at java.net.Inet4Address.getHostAddress(Inet4Address.java:304)
at java.net.InetAddress.toString(InetAddress.java:635)
at java.net.InetSocketAddress.toString(InetSocketAddress.java:256)
at java.lang.String.valueOf(String.java:2826)
at java.lang.StringBuilder.append(StringBuilder.java:115)
at org.elasticsearch.common.transport.InetSocketTransportAddress.toString(InetSocketTransportAddress.java:150)
at java.lang.String.valueOf(String.java:2826)
at java.lang.StringBuilder.append(StringBuilder.java:115)
at org.elasticsearch.transport.ActionTransportException.buildMessage(ActionTransportException.java:71)
at org.elasticsearch.transport.ActionTransportException.&lt;init&gt;(ActionTransportException.java:46)
at org.elasticsearch.transport.ConnectTransportException.&lt;init&gt;(ConnectTransportException.java:44)
at org.elasticsearch.transport.ConnectTransportException.&lt;init&gt;(ConnectTransportException.java:32)
at org.elasticsearch.transport.NodeNotConnectedException.&lt;init&gt;(NodeNotConnectedException.java:32)
at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:637)
at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:445)
at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:185)
at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:63)
at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:100)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:217)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)
at org.elasticsearch.client.transport.Tra
</comment><comment author="kimchy" created="2012-05-25T21:08:41Z" id="5940261">@jereanon another user reported this failure (the stack overflow), but we never managed to recreate it. Its very strange, see #1930.
</comment><comment author="kimchy" created="2012-05-25T21:32:13Z" id="5940648">@jereanon is there a chance that you can recreate it?
</comment><comment author="jaytaylor" created="2012-11-02T05:46:50Z" id="10005530">FWIW, I was experiencing the NoNodeAvailableException during bulk indexing with 0.19.8.  Even after reducing the batch size from 5000 down to 1000, the exception was still occurring with ES and the indexer running locally.

After updating to 0.19.1 bulk indexing is working flawlessly in batches of 5000.
</comment><comment author="spinscale" created="2013-08-09T12:05:26Z" id="22390185">is anyone of the participants of this issue still having these kind of problems with a current version and is able to provide more verbose information/logfiles, we could use to track this down?
</comment><comment author="jaytaylor" created="2013-08-09T19:39:39Z" id="22418681">After upgrading ES from 19.x to 20.x, I no longer have the issue.

On Fri, Aug 9, 2013 at 5:05 AM, Alexander Reelsen
notifications@github.comwrote:

&gt; is anyone of the participants of this issue still having these kind of
&gt; problems with a current version and is able to provide more verbose
&gt; information/logfiles, we could use to track this down?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/1868#issuecomment-22390185
&gt; .
</comment><comment author="spinscale" created="2013-09-16T07:38:29Z" id="24493093">Closing due to feedback. Happy to reopen if this still happens with the current release! Thanks for feedback.
</comment><comment author="aterreno" created="2014-01-29T09:46:49Z" id="33569621">Currently happening with ES server 0.90 and client api 0.90.7 . 

I'll try increasing the log levels and I might try to turn on the sniff mode. 

From what I've googled few people think it can be version mismatch, is that possible? 
</comment><comment author="davidbkemp" created="2014-05-21T23:17:12Z" id="43826616">I am now getting this in version 1.1.1.  After about an hour of bulk indexing of 7 million docs in batches of 1000 into a 3 node cluster with one shard replicated to the other two nodes.
</comment><comment author="madhugujjalapudi" created="2014-05-29T09:01:38Z" id="44511550">We are also getting similar errors(version 1.0.1,We are using single node Elasticsearch), we are trying to index events from Flume using ElasticsearchSink. Events are being indexed successfully for some time and then it starts giving the exceptions, please find below the exception stack trace. 

Unable to deliver event. Exception follows.

org.elasticsearch.client.transport.NoNodeAvailableException: No node available

```
            at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:263)

            at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:231)

            at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)

            at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:147)

            at org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:360)

            at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:165)

            at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)

            at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)

            at org.apache.flume.sink.elasticsearch.ElasticSearchSink.process(ElasticSearchSink.java:198)

            at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)

            at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)

            at java.lang.Thread.run(Thread.java:662)
```
</comment><comment author="javanna" created="2014-07-11T13:57:34Z" id="48733017">Some of these issues might be caused by a bug in the transport client retry mechanism, see #6829 . What happens there is that when some nodes drop the transport client doesn't necessarily retry the request with all nodes among the connected ones.
</comment><comment author="abhisheksachan" created="2015-06-10T05:47:11Z" id="110598433">hello sir
i am getting an error in elasticsearch that while using TransportClient I am storing data one by one. and after storing 140 the elasticsearch throws error. i am not getting idea why it is limit my storage capacity.
here are few screenshots of error.
please help me.
![elastic](https://cloud.githubusercontent.com/assets/7925483/8075769/153f01de-0f62-11e5-82f2-2e8aabd302e1.png)
![local](https://cloud.githubusercontent.com/assets/7925483/8075773/1974af9c-0f62-11e5-9d03-34ed5a1111c6.png)
</comment><comment author="markwalkom" created="2015-06-10T05:51:27Z" id="110598807">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help, we reserve Github for confirmed bugs and feature requests :)
</comment><comment author="mohammadfarooqui" created="2015-06-19T18:01:38Z" id="113592001">Hi,
I am using elasticsearch-1.6.0. When I am using TransportClient then continuously i am getting error:
Below is Java code and error ([inet[/127.0.0.1:9200]][cluster/nodes/info] request_id [0] timed out after [100003ms] and No node available) -

Does anybody had this problem? Or could someone give some ideas how to fix this?

Code:-

CsvToJson jsonDocument = new CsvToJson();

```
    Settings settings = ImmutableSettings.settingsBuilder()
            .put("cluster.name", "tempcluster")
            .put("client.transport.ping_timeout", "100s").build();

    TransportClient transportClient = new TransportClient(settings)
            .addTransportAddress(new InetSocketTransportAddress(
                    "127.0.0.1", 9200));

    Client client = transportClient;

    client.prepareIndex("globallocation", "article", "1")
            .setSource(jsonDocument.readfile()).execute().actionGet();
```

Error:-

2015-06-19 12:55:20 INFO  plugins:104 - [Jones, Gabe] loaded [], sites []
2015-06-19 12:57:01 INFO  transport:109 - [Jones, Gabe] failed to get node info for [#transport#-1][inet[/127.0.0.1:9200]], disconnecting...
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[/127.0.0.1:9200]][cluster/nodes/info] request_id [0] timed out after [100003ms]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:342)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Exception in thread "main" org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:202)
    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)
    at org.elasticsearch.client.support.AbstractClient.index(AbstractClient.java:80)
    at org.elasticsearch.client.transport.TransportClient.index(TransportClient.java:308)
    at org.elasticsearch.action.index.IndexRequestBuilder.doExecute(IndexRequestBuilder.java:315)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:62)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:57)
</comment><comment author="hussainshahzad250" created="2016-03-30T09:42:46Z" id="203349854">while indexing through java programme, i am getting 

org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:202)
    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:149)
    at org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:340)
    at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:129)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:62)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:57)
    at com.process.Main.postBulk(Main.java:434)
    at com.process.Main.main(Main.java:222)

can anybody tell me solution, mail me if any suggetion related to this problem

shahzad.hussain@raftaar.in
</comment><comment author="dadoonet" created="2016-03-30T10:10:09Z" id="203364484">Just read this comment and don't hijack old threads: https://github.com/elastic/elasticsearch/issues/1868#issuecomment-110598807
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix WildcardQueryBuilder when only rewrite is changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1867</link><project id="" key="" /><description>If only the rewrite field is changed, and not the boost one,
the serilization did not write the rewrite field.
</description><key id="4151863">1867</key><summary>Fix WildcardQueryBuilder when only rewrite is changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2012-04-17T12:29:35Z</created><updated>2014-07-16T21:55:25Z</updated><resolved>2012-04-22T09:22:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-22T09:22:42Z" id="5266608">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On recovery (startup), the recovery translog file handle is not properly closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1866</link><project id="" key="" /><description /><key id="4148943">1866</key><summary>On recovery (startup), the recovery translog file handle is not properly closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-17T08:27:24Z</created><updated>2012-04-17T09:14:30Z</updated><resolved>2012-04-17T09:14:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in geo_distance_range without to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1865</link><project id="" key="" /><description>i was trying to use the `gt` options for the geo_distance_range filter like:

``` json
{
  "geo_distance_range": {
    "gt": "50km",
    "location": {
      "lat": "51.16104233",
      "lon": "7.42149815"
    }
}
```

but it seems that the `GeoDistanceRangeFilterParser` depends on a `to` being available:

```
Caused by: java.lang.NullPointerException
  at org.elasticsearch.common.unit.DistanceUnit.parse(DistanceUnit.java:93)
  at org.elasticsearch.index.query.GeoDistanceRangeFilterParser.parse(GeoDistanceRangeFilterParser.java:224)
  at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:224)
  at org.elasticsearch.index.query.AndFilterParser.parse(AndFilterParser.java:63)
  at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:224)
  at org.elasticsearch.index.query.ConstantScoreQueryParser.parse(ConstantScoreQueryParser.java:66)
  at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:192)
  at org.elasticsearch.index.query.BoolQueryParser.parse(BoolQueryParser.java:89)
  at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:192)
  at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:243)
  at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:223)
  at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
  at org.elasticsearch.search.SearchService.parseSource(SearchService.java:545)
```
</description><key id="4148647">1865</key><summary>NullPointerException in geo_distance_range without to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phoet</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-17T08:01:48Z</created><updated>2012-04-17T12:51:56Z</updated><resolved>2012-04-17T12:51:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-04-17T09:15:42Z" id="5171100">In 0.19.2, `gt` works fine, but it appears the the `geo_distance_range` requires an upper and lower limit.  You can't specify just an upper or lower limit, eg: `gt: '5km'` or ''from: '5km'`

This works:

```
curl -XGET 'http://127.0.0.1:9200/geonames/_search?pretty=1'  -d '
{
   "filter" : {
      "geo_distance_range" : {
         "from" : "1km",
         "to" : "500km",
         "location" : {
            "lat" : "-6.6",
            "lon" : 54
         }
      }
   }
}
'
```

This doesn't:

```
curl -XGET 'http://127.0.0.1:9200/geonames/_search?pretty=1'  -d '
{
   "filter" : {
      "geo_distance_range" : {
         "to" : "500km",
         "location" : {
            "lat" : "-6.6",
            "lon" : 54
         }
      }
   }
}
'
```
</comment><comment author="phoet" created="2012-04-17T10:07:11Z" id="5171854">could you then give a short overview of how to use the `lt, gt etc` parameters? there is no documentation or example queries in the docs :(

i would like to query all documents that are not within a a certain distance.
</comment><comment author="clintongormley" created="2012-04-17T10:16:31Z" id="5171974">Just to note: I still think this is a bug, just a different bug from the one that you described :)

`from` is equivalent to `gte` (greater than or equal).  `to` is equivalent `lt` (less than).  You specify 'not within a range' which means that you should wrap your geo distance filter in a `not` filter
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Lucene 3.6 query time joining</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1864</link><project id="" key="" /><description>https://issues.apache.org/jira/browse/LUCENE-3602
</description><key id="4136305">1864</key><summary>Support Lucene 3.6 query time joining</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels /><created>2012-04-16T15:19:39Z</created><updated>2012-04-23T17:58:36Z</updated><resolved>2012-04-23T17:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-16T16:22:02Z" id="5156144">Parent child feature is effectively a query time join between parents and children, and its there in elasticsearch. Note, effectively, any joining that is going to be implemented must have the joined docs in the same shards (which parent child automatically supports).
</comment><comment author="bryangreen" created="2012-04-23T17:58:36Z" id="5287273">My hope was that Lucene 3.6 would allow for native query time joining of documents for elasticsearch (sharding still is a big concern.) I like the parent-child relationship it's just that I'd like more flexibility. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 3.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1863</link><project id="" key="" /><description /><key id="4124089">1863</key><summary>Upgrade to Netty 3.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-15T15:15:06Z</created><updated>2012-04-30T10:59:05Z</updated><resolved>2012-04-15T15:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 3.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1862</link><project id="" key="" /><description /><key id="4123919">1862</key><summary>Upgrade to Lucene 3.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-15T14:39:16Z</created><updated>2012-04-15T14:39:47Z</updated><resolved>2012-04-15T14:39:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix for issue #1860 : Index Templates API - Set Source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1861</link><project id="" key="" /><description>Hi Shay,

This is a pull request to fix issue #1860 regarding the template API.

Cheers,

Nicolas
</description><key id="4122914">1861</key><summary>Fix for issue #1860 : Index Templates API - Set Source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nhuray</reporter><labels /><created>2012-04-15T10:57:39Z</created><updated>2014-06-26T18:15:14Z</updated><resolved>2012-04-15T15:23:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-15T15:23:33Z" id="5139907">Pushed to 0.19 and master branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Templates API - Set Source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1860</link><project id="" key="" /><description>This issue refers to this [discussion](https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/jfciwKNk28w)
## Context :

There's no way with the Java client API to create a template and set the source like that : 

``` java
PutIndexTemplateResponse response = client.admin().indices()
   .preparePutTemplate("twitter_template").
   .setSource(sourceTemplate)
   .execute()
   .actionGet();
```

instead to set settings, mappings ... like that :

``` java
PutIndexTemplateResponse response = client.admin().indices()
   .preparePutTemplate("twitter_template")
   .setTemplate("twitter*")
   .addMapping("tweet", sourceMapping)
   .setSettings(sourceSettings)
   .execute()
   .actionGet();
```
## Solution :

We have to move the parsing logic from `RestPutIndexTemplateAction.handleRequest()` method to the `PutIndexTemplateRequest.setSource()` method
</description><key id="4122890">1860</key><summary>Index Templates API - Set Source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nhuray</reporter><labels /><created>2012-04-15T10:51:10Z</created><updated>2012-04-15T19:10:17Z</updated><resolved>2012-04-15T19:10:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NoShardAvailableActionException in Tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1859</link><project id="" key="" /><description>We are running a Test suite against our elasticsearch using Ruby with Tire and there is one Test failing once a while.

It's recreating an Index and tries to retrieve a non-existing Document.

It fails 1 out of 10 with `{"error"=&gt;"NoShardAvailableActionException[[test_index][1] No shard available for [[test_index][document][_meta]: routing [null]]]", "status"=&gt;500}` 

This is what we (tire) do to recreate the index and get the doc:

``` bash
#2012-04-13 08:23:08:803 [DELETE] ("test_index")
#
curl -X DELETE "http://localhost:9200/test_index"
#2012-04-13 08:23:08:804 [200]

#2012-04-13 08:23:08:830 [CREATE] ("test_index")
#
curl -X POST "http://localhost:9200/test_index" -d '{}'
#2012-04-13 08:23:08:830 [200]

#2012-04-13 08:23:08:833 [_refresh] ("test_index")
#
curl -X POST "http://localhost:9200/test_index/_refresh"
#2012-04-13 08:23:08:833 [200]

#2012-04-13 08:23:08:836 [_meta] ("test_index")
#
curl -X GET "http://localhost:9200/test_index/document/_meta"
#2012-04-13 08:23:08:837 [500]
```

Is there any call i can do, that ensures, that all shards are available?

``` json
{
ok: true,
status: 200,
name: "Lady Lark",
version: {
number: "0.19.2",
snapshot_build: false
},
tagline: "You Know, for Search"
}
```
</description><key id="4098395">1859</key><summary>NoShardAvailableActionException in Tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phoet</reporter><labels /><created>2012-04-13T06:30:26Z</created><updated>2013-05-27T16:36:07Z</updated><resolved>2013-05-27T16:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-13T09:15:39Z" id="5111194">Yes, you can use the cluster health API, with `wait_for_status` set to `yellow`. See more here: http://www.elasticsearch.org/guide/reference/api/admin-cluster-health.html. Btw, its best to ask questions on the mailing list, as more people can see them, where not many people monitor the issues.
</comment><comment author="phoet" created="2012-04-13T09:42:56Z" id="5111580">thx, will try that! ok, i will register to the mailing list asap.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes plugin install failure on Windows 7 / JDK 7u3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1858</link><project id="" key="" /><description>Without the fix installation of plugins fails with this error message (sorry for my swedish locale :-)):

C:\java\elasticsearch-0.19.2\bin&gt;plugin.bat -install mobz/elasticsearch-head
Fel: Hittar inte eller kan inte ladda huvudklassen C:\java\elasticsearch-0.19.2.
lib.jna-3.3.0.jar
</description><key id="4092242">1858</key><summary>Fixes plugin install failure on Windows 7 / JDK 7u3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KingBuzzer</reporter><labels /><created>2012-04-12T21:20:53Z</created><updated>2014-06-26T18:36:47Z</updated><resolved>2012-04-15T11:32:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-15T11:32:35Z" id="5138597">Pushed in master and 0.19 branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When creating an index, fail properly on classpath error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1857</link><project id="" key="" /><description>I was struggling with an elasticsearch which was not responding at all to my "create index" request. I was able to find out the error until I had the idea of starting in foreground, so I could get the stdout. There was some thread dying because of a NoClassDefFoundError. One of my plugin was not properly setup.

The suggested patch catch the NoClassDefFoundError and throws a proper IndexCreationException, which in the end produce an error on the client side rather than having it hang.

Probably RiversService#createRiver suffer from the same issue.
</description><key id="4085212">1857</key><summary>When creating an index, fail properly on classpath error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-04-12T14:53:54Z</created><updated>2014-07-16T21:55:26Z</updated><resolved>2012-04-22T09:24:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-22T09:24:54Z" id="5266616">Cheers!, pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>total_terms propery for facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1856</link><project id="" key="" /><description>At the moment the `missing`, `total`, `other` properties pertain document counts. e.g:

```
techniques: {
    _type: terms
   missing: 1017
   total: 787
   other: 109
```

In addition to these i'd like a `total_terms` property that returns how many term facets there are. 

I need to make a "load more facets" functionality, and for this an `offset` property to go along with `size` would help alot too.
</description><key id="4082489">1856</key><summary>total_terms propery for facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2012-04-12T12:06:50Z</created><updated>2014-07-08T14:49:23Z</updated><resolved>2014-07-08T14:49:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="d2kagw" created="2013-05-02T02:09:55Z" id="17317287">_bump_
</comment><comment author="jredburn" created="2013-06-04T15:15:04Z" id="18915910">This would be great. The docs (and code comments) suggest this is already the case for `total` and `other` but that does not appear to be the case.
</comment><comment author="clintongormley" created="2014-07-08T14:49:23Z" id="48346952">Closing as aggregations have replace facets and provide this functionality.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range facet on an integer field will return double values but range filter will not take double values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1855</link><project id="" key="" /><description>A range facet will always return double values (i.e. 1.0, 2.0) even when applied to an integer field. A range filter, however, will not take double values as input. It would be convenient to either be able to specify integer output for a facet or to be able to use doubles on an integer field, so the range facet results can be directly used to construct a corresponding filter. 

Currently, feeding a double value to a range filter on an integer field will result in a NumberFormatException:

https://gist.github.com/eb1b4d355df33489008f
</description><key id="4081374">1855</key><summary>Range facet on an integer field will return double values but range filter will not take double values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hbrunsting</reporter><labels /><created>2012-04-12T10:26:34Z</created><updated>2014-07-08T14:48:56Z</updated><resolved>2014-07-08T14:48:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:48:56Z" id="48346890">Both of these issues appear to have been fixed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated windows start path, replacing / with \</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1854</link><project id="" key="" /><description /><key id="4075993">1854</key><summary>Updated windows start path, replacing / with \</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrisgittings</reporter><labels /><created>2012-04-11T23:52:56Z</created><updated>2014-07-08T14:45:15Z</updated><resolved>2014-07-08T14:45:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>XContentBuilder throws NPE on null Boolean</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1853</link><project id="" key="" /><description>XContentBuilder.value(Boolean) is the only .value(..) method to not check for null. This is inconsistent and probably not wanted.

Casting explicitly to Object - i.e. calling value(Object) - or invoking nullValue() manually solves the problem.
</description><key id="4066495">1853</key><summary>XContentBuilder throws NPE on null Boolean</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-11T15:01:51Z</created><updated>2012-04-11T17:54:28Z</updated><resolved>2012-04-11T17:54:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sfussenegger" created="2012-04-11T15:07:05Z" id="5070924">fix:

```
public XContentBuilder value(Boolean value) throws IOException {
    if (value == null) {
        return nullValue();
    }
    return value(value.booleanValue());
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting index.auto_expand_replicas in the elasticsearch config file does not apply</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1852</link><project id="" key="" /><description>Index level settings can be applied on the configuration file itself (like number of shards), which changes the default value for all indices created without explicitly setting it . The `index.auto_expand_replicas` setting does not apply when setting it in the config file.
</description><key id="4062962">1852</key><summary>Setting index.auto_expand_replicas in the elasticsearch config file does not apply</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-11T11:26:10Z</created><updated>2012-04-11T17:53:15Z</updated><resolved>2012-04-11T17:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Add timestamp per node stats element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1851</link><project id="" key="" /><description /><key id="4061470">1851</key><summary>Node Stats: Add timestamp per node stats element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-11T09:19:50Z</created><updated>2012-04-11T09:20:33Z</updated><resolved>2012-04-11T09:20:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Bulk API: Allow to control if its compressed or not using `action.bulk.compress` (defaults to true which is current behavior)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1850</link><project id="" key="" /><description>Allow to control if the bulk request (or the shard level bulk requests that spawn from a single bulk request) are compressed or not. Currently, it is compressed, but with the changes done on better slicing and dicing the network buffer, it actually might behave better when its not compressed (at the expense of more data transfered over the wire). We will keep the default `true` as the existing behavior, and open a different issue if it makes sense to change the default to `false`.
</description><key id="3992447">1850</key><summary>Bulk API: Allow to control if its compressed or not using `action.bulk.compress` (defaults to true which is current behavior)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-05T17:47:19Z</created><updated>2012-04-05T17:47:47Z</updated><resolved>2012-04-05T17:47:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RFE: dynamic analyzer resolution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1849</link><project id="" key="" /><description>Hi,

I have a document that contains information about a city.
Cities have a "native" name plus a huge list of translations of the city name.

For example:

{
  native_name: "K&#246;ln",
  native_lang: "de",
  translations: [
    {
      lang: "en",
      name: "Cologne"
    },
    {
      lang: "de",
      name: "K&#246;ln"
    },
    {
      lang: "gr",
      name: "Kolonia"
    },
... (many more)
  ]
}

I want to use the correct analyzer for translations.name depending on the content of translations.lang.

It would be nice to have a per-field resolution "similar" to "index._analyzer.path".

Robert
</description><key id="3986278">1849</key><summary>RFE: dynamic analyzer resolution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snazy</reporter><labels /><created>2012-04-05T10:53:27Z</created><updated>2014-07-08T14:42:15Z</updated><resolved>2014-07-08T14:42:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:42:14Z" id="48345642">This wouldn't work well as the tokens for all languages would be mixed up in the same field.  Better to have a field per language.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_analyze request issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1848</link><project id="" key="" /><description>elasticsearch v 0.19.2
elasticsearch.yml:

``` yaml
index:
  analysis:
    analyzer:
      text_lt:
        type: custom
        tokenizer: standard
        filter: [standard, lowercase, stop]
```

POST `http://localhost:9200/_analyze?analyzer=text_lt`
with data: `message about error is here`

``` json
{"error":"ElasticSearchIllegalArgumentException[failed to find analyzer [text_lt]]","status":400}
```

```
[2012-04-05 12:37:14,622][DEBUG][action.admin.indices.analyze] [Cassiopea] failed to execute [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@673c9f]
org.elasticsearch.ElasticSearchIllegalArgumentException: failed to find analyzer [text_lt]
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:147)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:57)
        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$1.run(TransportSingleCustomOperationAction.java:143)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
```

POST `http://localhost:9200/_analyze?analyzer=standard` works as expected:

``` json
{"tokens":[{"token":"message","start_offset":0,"end_offset":7,"type":"&lt;ALPHANUM&gt;","position":1},{"token":"about","start_offset":8,"end_offset":13,"type":"&lt;ALPHANUM&gt;","position":2},{"token":"error","start_offset":14,"end_offset":19,"type":"&lt;ALPHANUM&gt;","position":3},{"token":"here","start_offset":23,"end_offset":27,"type":"&lt;ALPHANUM&gt;","position":5}]}
```
</description><key id="3985538">1848</key><summary>_analyze request issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">xawiers</reporter><labels /><created>2012-04-05T09:43:56Z</created><updated>2014-12-24T15:13:15Z</updated><resolved>2014-12-24T15:13:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xawiers" created="2012-04-05T12:18:22Z" id="4975393">it works when using http://localhost:9200/index/_analyze?analyzer=text_lt url structure
maybe this is not issue?
</comment><comment author="kimchy" created="2012-04-05T17:43:42Z" id="4981725">The way its structured now, custom analyzers are only created when an index gets created, while the built in analyzers are created on the "node" level, so, this is why you get the failure. It would be nice to detect that the config comes from an node level config, and create those analyzers on the node level, but its not there yet...
</comment><comment author="xawiers" created="2012-04-05T17:46:21Z" id="4981767">Ok understood. Sould I leave issue open?
</comment><comment author="kimchy" created="2012-04-05T17:49:05Z" id="4981831">Yea, lets keep it open for now, it makes perfect sense in terms of usability.
</comment><comment author="brusic" created="2012-05-17T00:50:14Z" id="5755930">Commenting in order to subscribe to future comments. Just got hit by this issue as well, thankfully the workaround is easy.
</comment><comment author="clintongormley" created="2014-07-08T14:38:19Z" id="48344877">@kimchy is this something you're interested in including in an analyzers cleanup?
</comment><comment author="kimchy" created="2014-09-07T11:38:14Z" id="54744312">yea, I think that analyzers that are configured on the config should be added to the "node" level. I would have loved to simply have one place to hold analyzers, or something like that, that is node level, and then work only with that.
</comment><comment author="clintongormley" created="2014-12-24T15:13:15Z" id="68057686">Closing in favour of #8961
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add plugin mechanism for external source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1847</link><project id="" key="" /><description>As per our discussion in https://github.com/elasticsearch/elasticsearch/pull/1815
</description><key id="3981035">1847</key><summary>add plugin mechanism for external source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-04-05T00:03:13Z</created><updated>2014-06-13T09:12:17Z</updated><resolved>2012-07-03T16:04:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2012-07-03T16:04:31Z" id="6738524">See https://github.com/elasticsearch/elasticsearch/pull/2080
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>support geo sorting on multiple geo point values per doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1846</link><project id="" key="" /><description>I have multiple geo points per document and would like to be able to sort based on the closest matching location in the document to my center point. Apparently the way things currently work is the sort will be based on a random location on the document, as in the following scenario:

I have the following mapping:

```
$ curl -XGET 'http://localhost:9200/work/offices/_mapping?pretty=true'
{
  "offices" : {
    "properties" : {
      "location" : {
        "properties" : {
          "address" : {
            "type" : "string"
          },
          "point" : {
            "type" : "geo_point"
          }
        }
      },
      "name" : {
        "type" : "string"
      }
    }
  }
}
```

Then I have the following query and response (I've only included hits to cut down on noise):

```
$ curl -XGET 'http://localhost:9200/work/offices/_search?pretty=true' -d '{ "fields": [ "name", "location" ], "query": { "match_all": {} }, "sort": { "_geo_distance": {"location.point": [-0.0976398, 51.4962307], "order": "asc", "unit": "km"} }}'
...
    "hits" : [ {
      "_index" : "work",
      "_type" : "offices",
      "_id" : "IDtt2WnSQnWEhuAxhlbPgw",
      "_score" : null,
      "fields" : {
        "location" : [ {
          "point" : [ -0.01655, 51.5007324 ],
          "address" : "E14 9SH"
        }, {
          "point" : [ -0.0976398, 51.4962307 ],
          "address" : "SE1 6PL"
        } ],
        "name" : "office3"
      },
      "sort" : [ 0.0 ]
    }, {
      "_index" : "work",
      "_type" : "offices",
      "_id" : "xzaVAHUoSGON8gY1-ggILQ",
      "_score" : null,
      "fields" : {
        "location" : [ {
          "point" : [ -0.01655, 51.5007324 ],
          "address" : "E14 9SH"
        }, {
          "point" : [ -0.0684337, 51.4843866 ],
          "address" : "SE1 5BA"
        } ],
        "name" : "office10"
      },
      "sort" : [ 2.413161036894697 ]
    }, {
      "_index" : "work",
      "_type" : "offices",
      "_id" : "ZWjuiVkWSD6H99ooKcKNuA",
      "_score" : null,
      "fields" : {
        "location" : [ {
          "point" : [ -0.0976398, 51.4962307 ],
          "address" : "SE1 6PL"
        }, {
          "point" : [ -0.0684337, 51.4843866 ],
          "address" : "SE1 5BA"
        } ],
        "name" : "office8"
      },
      "sort" : [ 2.413161036894697 ]
    }
...
```

office8 and office3 both have locations 0km away from my search point, yet office10 is sneaking in between the two at 2.4km away and office8 is also placing itself 2.4km away (which it is, but but only on the further location point).

This was originally discussed here: https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/DxIUevwZfOs
</description><key id="3978974">1846</key><summary>support geo sorting on multiple geo point values per doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tommyvn</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2012-04-04T21:36:08Z</created><updated>2015-07-06T12:30:13Z</updated><resolved>2013-03-28T16:16:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="DominicWatson" created="2012-09-07T21:48:44Z" id="8380080">While waiting for this enhancement, I've managed to get this working using the scripting functionality of ElasticSearch (which I have to say is utterly awesome, this is such a refreshing API to work with). 

The following solution is very rough. It currently only supports kilometers and the `lang-javascript` plugin is required.

## A temporary solution

I have the following javascript in `./config/scripts/geo/closestdistance.js`:

```
(function(){
    var i, locations, calculateDistance, closest, distance;

    if ( !doc['latlon'].multiValued ){
        return doc['latlon'].distanceInKm( lat, lon )
    }

    calculateDistance = function( lat1, lat2, lon1, lon2 ){
        var R = 6371 // km
          , dLat = (lat2-lat1) * Math.PI / 180
          , dLon = (lon2-lon1) * Math.PI / 180
          , lat1 = lat1 * Math.PI / 180
          , lat2 = lat2 * Math.PI / 180
          , a = Math.sin(dLat/2) * Math.sin(dLat/2) + Math.sin(dLon/2) * Math.sin(dLon/2) * Math.cos(lat1) * Math.cos(lat2)
          , c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));

        return R * c;
    }

    locations = doc['latlon'].getValues();
    closest = calculateDistance( lat, locations[0].lat(), lon, locations[0].lon() );

    for( i=1; i &lt; locations.length; i++ ){
        distance = calculateDistance( lat, locations[i].lat(), lon, locations[i].lon() );
        closest = distance &lt; closest ? distance : closest;
    }

    return closest;
})();
```

Then, in my search body, I have something like:

```
{
    query : {
        custom_score : {
              query : {
                  query_string : {
                      query : "myquerystring..."
                  }
              }
            , params : {
                  lat : 54.5881
                , lon : -5.85829
              }
            , script : "geo_closestdistance"
        }
    },
    sort : [{_score : { order : "asc"}}]
}
```

Hopefully someone will find this useful.
</comment><comment author="ghost" created="2012-09-08T04:12:04Z" id="8384391">jest sewz you know dominic:  ESP dev centre left geo-location searching 
as a rectangulation problem, despite how the measurements are laid out 
on the globe.  That was  the search engine bought by Microsoft. I've no 
idea if they got geo into SharePoint search and whether they got 
something smarter than right angles... i look forward to leveraging this 
solution.

b.

On 09/07/2012 05:48 PM, Dominic Watson wrote:

&gt; While waiting for this enhancement, I've managed to get this working 
&gt; using the scripting functionality of ElasticSearch (which I have to 
&gt; say is utterly awesome, this is such a refreshing API to work with).
&gt; 
&gt; The following solution is very rough. It currently only supports 
&gt; kilometers and the |lang-javascript| plugin is required.
&gt; 
&gt; ```
&gt; A temporary solution
&gt; ```
&gt; 
&gt; I have the following javascript in 
&gt; |./config/scripts/geo/closestdistance.js|:
&gt; 
&gt; |(function(){
&gt;      var i, locations, calculateDistance, closest, distance;
&gt; 
&gt; ```
&gt;  if ( !doc['latlon'].multiValued ){
&gt;      return doc['latlon'].distanceInKm( lat, lon )
&gt;  }
&gt; 
&gt;  calculateDistance = function( lat1, lat2, lon1, lon2 ){
&gt;      var R = 6371 // km
&gt;        , dLat = (lat2-lat1) * Math.PI / 180
&gt;        , dLon = (lon2-lon1) * Math.PI / 180
&gt;        , lat1 = lat1 * Math.PI / 180
&gt;        , lat2 = lat2 * Math.PI / 180
&gt;        , a = Math.sin(dLat/2) * Math.sin(dLat/2) + Math.sin(dLon/2) * Math.sin(dLon/2) * Math.cos(lat1) * Math.cos(lat2)
&gt;        , c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));
&gt; 
&gt;      return R * c;
&gt;  }
&gt; 
&gt;  locations = doc['latlon'].getValues();
&gt;  closest = calculateDistance( lat, locations[0].lat(), lon, locations[0].lon() );
&gt; 
&gt;  for( i=1; i &lt; locations.length; i++ ){
&gt;      distance = calculateDistance( lat, locations[i].lat(), lon, locations[i].lon() );
&gt;      closest = distance &lt; closest ? distance : closest;
&gt;  }
&gt; 
&gt;  return closest;
&gt; ```
&gt; 
&gt; })();
&gt; |
&gt; 
&gt; Then, in my search body, I have something like:
&gt; 
&gt; |{
&gt;      query : {
&gt;          custom_score : {
&gt;                query : {
&gt;                    query_string : {
&gt;                        query : "myquerystring..."
&gt;                    }
&gt;                }
&gt;              , params : {
&gt;                    lat : 54.5881
&gt;                  , lon : -5.85829
&gt;                }
&gt;              , script : "geo_closestdistance"
&gt;          }
&gt;      },
&gt;      sort : [{_score : { order : "asc"}}]
&gt; }
&gt; |
&gt; 
&gt; Hopefully someone will find this useful.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1846#issuecomment-8380080. 
</comment><comment author="tbug" created="2013-03-27T18:52:05Z" id="15545482">I would also very much like to see this implemented. 
Preferably controlled by the `mode` option as described in
http://www.elasticsearch.org/guide/reference/api/search/sort/
</comment><comment author="tbug" created="2013-03-28T14:04:44Z" id="15589986">As a response to @DominicWatson:
This will run ~30% faster:

``` javascript
(function(){
    var i, locations, arcDist, closest, distance;
    arcDist = function( lat1, lat2, lon1, lon2 ){
        var R = 6371 // earth radius
          , CI = 0.017453292519943295 // pi/180
          , dLat = (lat2-lat1) * CI / 2
          , dLon = (lon2-lon1) * CI / 2
          , sinDLat = Math.sin(dLat)
          , sinDLon = Math.sin(dLon)
          , a = sinDLat*sinDLat + sinDLon*sinDLon * Math.cos(lat1*CI)*Math.cos(lat2*CI)
          , c = 2*Math.atan2(Math.sqrt(a), Math.sqrt(1-a));
        return R * c;
    }
    locations = doc["locations"].getValues();
    closest = arcDist( lat, locations[0].lat(), lon, locations[0].lon() );
    for( i=1; i &lt; locations.length; i++ ) {
        distance = arcDist( lat, locations[i].lat(), lon, locations[i].lon() );
        if(distance &lt; closest) {
          closest = distance;
        }
    }
    return closest;
})();
```
</comment><comment author="martijnvg" created="2013-03-28T16:17:52Z" id="15597780">The next 0.90 release will have a `mode` option (min &amp; max) support for sorting by geo distance.
</comment><comment author="mouzt" created="2015-07-06T12:30:13Z" id="118840094"> Does this question have new solution which is not rough, now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Number of replicas setting can not be set on a closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1845</link><project id="" key="" /><description>As discussed in forum : https://groups.google.com/forum/?hl=fr&amp;fromgroups#!topic/elasticsearch/dVPq9GAh8-g

We can not set number of replicas on a closed index but only on an opened one.

Error log is here : https://gist.github.com/2278029 
</description><key id="3976317">1845</key><summary>Number of replicas setting can not be set on a closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-04T18:58:52Z</created><updated>2012-04-04T21:15:35Z</updated><resolved>2012-04-04T21:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-04-04T21:15:35Z" id="4963255">It seems that @kimchy have already fix it here : https://github.com/elasticsearch/elasticsearch/commit/f512f43ac54ae8e167594773ba0474ec84adead0

You should only mark it as part of 0.19.2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API Query DSL:  Add wrapper filter similar to wrapper query accepting a json filter in raw format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1844</link><project id="" key="" /><description /><key id="3974153">1844</key><summary>Java API Query DSL:  Add wrapper filter similar to wrapper query accepting a json filter in raw format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-04T16:52:55Z</created><updated>2012-04-04T16:53:25Z</updated><resolved>2012-04-04T16:53:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Stop iterating on sub readers if the parent is found in TopChildrenQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1843</link><project id="" key="" /><description>There are some comment saying "we found a match, add it and break", but there's no actual break.

In the suggested patch, it rewrote the loop to make the loop ending more explicit. I could have just added a "break", but I don't like much "break". Well, actually the main reason is this patch avoid too much diff with fetch-children branch :).
</description><key id="3971385">1843</key><summary>Stop iterating on sub readers if the parent is found in TopChildrenQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-04-04T14:23:08Z</created><updated>2014-07-16T22:03:00Z</updated><resolved>2014-07-16T22:03:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:37:01Z" id="48344689">@martijnvg is this PR still relevant?
</comment><comment author="martijnvg" created="2014-07-16T10:30:45Z" id="49147571">@clintongormley This PR is still relevant, but it is very outdated. I lost track of this one, but I think with a simple change this optimisation can be achieved.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for Latvian stemming: `latvian` analyzer and `latvian` language for stemmer filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1842</link><project id="" key="" /><description>Would it be possible to add Latvian stemming and analyzer?

I see that Lucene has it:
http://lucene.apache.org/core/old_versioned_docs/versions/3_5_0/api/contrib-analyzers/org/apache/lucene/analysis/lv/package-summary.html

Add `latvian` analyzer, and `latvian` value for `language` in the stemmer filter.

Thank you
</description><key id="3970376">1842</key><summary>Support for Latvian stemming: `latvian` analyzer and `latvian` language for stemmer filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darklow</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-04T13:22:02Z</created><updated>2012-04-04T13:39:59Z</updated><resolved>2012-04-04T13:39:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PhoneticFilter.incrementToken wasn't defined with final signal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1841</link><project id="" key="" /><description>Hi,

I met the same issue with https://github.com/elasticsearch/elasticsearch/issues/1397 when working with PhoneticFilter in version 0.19.1, the issue is  PhoneticFilter.incrementToken wasn't defined with final signal
</description><key id="3966262">1841</key><summary>PhoneticFilter.incrementToken wasn't defined with final signal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhthu</reporter><labels /><created>2012-04-04T07:21:12Z</created><updated>2012-04-04T08:06:53Z</updated><resolved>2012-04-04T08:06:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-04T08:06:53Z" id="4949066">Open the issue here: https://github.com/elasticsearch/elasticsearch-analysis-phonetic.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PhoneticFilter.incrementToken wasn't defined with final signal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1840</link><project id="" key="" /><description>Hi,

I met the same issue with https://github.com/elasticsearch/elasticsearch/issues/1397 when working with PhoneticFilter in version 0.19.1, the issue is  PhoneticFilter.incrementToken wasn't defined with final signal

Please check it this is a bug.
</description><key id="3966056">1840</key><summary>PhoneticFilter.incrementToken wasn't defined with final signal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">anhthu</reporter><labels /><created>2012-04-04T06:55:46Z</created><updated>2013-08-09T11:18:48Z</updated><resolved>2013-08-09T11:18:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T09:54:58Z" id="20509948">Hey,

can you provide a fully recreation of what you did, so we can try to jump in and help? Does this still happen with the current 0.90.2 release?

Thanks!
</comment><comment author="spinscale" created="2013-08-09T11:18:48Z" id="22388388">Closing this one. If you still think, there is a bug in the current release, please provide us with more information and we will have a look!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Relocation of shards causes bulk indexing client to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1839</link><project id="" key="" /><description>I have set up 4 big servers (lots of cores, lots of disks, lots of ram) - each running an elasticsearch node.
One client reads rows from a database and continuously submits indexing requests to the cluster. Indexing requests are bundled into bulk requests with 2500 indexing requests.
The index has 32 shards.
My client is using the Java client API.

So far so good.

I just wanted to know what happens, if I shutdown a node and restart it again.
Shutdown works fine (except: see below).
Restart works fine...
Until the cluster starts to relocate shards.

When a bulk request "hits" a shard being relocated, the cliend hangs forever.

I have tried several networking settings, transport client vs. node client - nothing helped.

One thing fixed the issue for me:
Previously, the code was:

```
            Client client = (TransportClient)...
            BulkRequestBuilder bulk = client.prepareBulk();
            for ( 1 to 2500 ) {
                IndexRequestBuilder request = buildIndexingRequest();
                request = request.setReplicationType(ReplicationType.ASYNC); // no effect
                bulk.add(request);
            }
            BulkResponse response;
            response = bulk.execute().actionGet(); // &lt;--- RETURNS NEVER, IF SHARD IS RELOCATED
            if (response != null &amp;&amp; response.hasFailures()) {
                // some error handling...
            }
```

When I use actionGet(timeout) with a timeout, the method throws a ElasticSearchTimeoutException in such a situation and I can submit the bulk request again.

```
                    while (true) {
                        try {
                            response = bulk.execute().actionGet(getRetryTimeout()); // &lt;--- TIMES OUT, IF SHARD IS RELOCATED
                            break;
                        }
                        catch (ElasticSearchTimeoutException timeout) {
                            warning("TIMEOUT", timeout, null, null);
                        }
                    }
```

In such a situation I see no activity in the elasticsearch threads and no activity in "my" calling thread - it just waits in org.elasticsearch.common.util.concurrent.BaseFuture.Sync#acquireSharedInterruptibly forever.

None of the cluster log files indicate an error.

I do not know if this behaviour affects searches.
</description><key id="3944516">1839</key><summary>Relocation of shards causes bulk indexing client to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">snazy</reporter><labels><label>bug</label><label>v0.19.3</label><label>v0.20.0.RC1</label></labels><created>2012-04-03T13:18:29Z</created><updated>2013-11-20T02:30:49Z</updated><resolved>2012-04-05T17:26:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="snazy" created="2012-04-03T13:21:39Z" id="4900972">Verified with elasticsearch 0.18.7 and 0.19.1
</comment><comment author="kimchy" created="2012-04-04T14:22:07Z" id="4954340">Hey, can you help write a standalone test case, and the scenario (i.e. start 4 nodes, restart one node while test case is bulk indexing data), that recreates it? It will help speed things up to see where the problem is.
</comment><comment author="snazy" created="2012-04-05T10:45:05Z" id="4974197">OK
</comment><comment author="snazy" created="2012-04-05T11:47:48Z" id="4975010">OK - here it is.

Just edit and execute the class (with a dependency to elasticsearch 0.19.1 jar).
1. Setup a cluster with 4 nodes
2. Start the class (it will automatically re-create index and mapping)
3. Wait until the index has about 1000000 docs (means: relocation of the shards will take some time)
4. Gracefully stop one node (I did it using elasticsearch head's SHUTDOWN functionality)
5. Main class still runs
6. Restart the stopped node
7. Node appears (without any shards)
8. Some shards will be relocated (purple color in elasticsearch head)
9. Bang - indexer hangs ... forever

Here's the code:

package org.elasticsearch.issue1839;

import org.elasticsearch.action.bulk.BulkRequestBuilder;
import org.elasticsearch.action.index.IndexRequestBuilder;
import org.elasticsearch.client.IndicesAdminClient;
import org.elasticsearch.client.transport.TransportClient;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.transport.InetSocketTransportAddress;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.node.NodeBuilder;

import java.util.Date;
import java.util.Random;

public final class Main {
    public static void main(String[] args) {
        new Main().doit();
    }

```
static final int THREADS = 4;
static final String CLUSTER = "elasticsearch";
static final String NAME = "issue1839";

private void doit() {

    NodeBuilder nodeBuilder = NodeBuilder.nodeBuilder().client(true);
    ImmutableSettings.Builder settings = nodeBuilder.settings();
    settings = settings.
            put("cluster.name", CLUSTER).//
            //
            put("client.transport.sniff", "false").//
            put("transport.tcp.compress", true).//
            put("transport.tcp.connect_timeout", "10s").//
            put("network.tcp.keep_alive", "true").//
            put("network.tcp.send_buffer_size", "64k").//
            put("network.tcp.receive_buffer_size", "64k");

    TransportClient client = new TransportClient(settings);

    client.addTransportAddress(new InetSocketTransportAddress("10.40.101.211", 9300));
    client.addTransportAddress(new InetSocketTransportAddress("10.40.101.212", 9300));
    client.addTransportAddress(new InetSocketTransportAddress("10.40.101.213", 9300));
    client.addTransportAddress(new InetSocketTransportAddress("10.40.101.214", 9300));

    IndicesAdminClient indicesAdmin = client.admin().indices();

    try {
        indicesAdmin.prepareDelete(NAME).execute().actionGet();
    }
    catch (IndexMissingException ignore) {
        //
    }

    String indexSettings = "{\"index\": {" +//
            "    \"number_of_shards\" : \"32\"," +//
            "    \"number_of_replicas\" : \"1\"" +//
            "  }" +//
            '}';
    indicesAdmin.prepareCreate(NAME).setSettings(indexSettings).execute().actionGet();

    String indexMapping = "{\"" + NAME + "\": {" +//
            "  \"properties\": {" +//
            "    \"my_text_1\" : {" +//
            "      \"type\" : \"string\"," +//
            "      \"store\" : \"yes\"," +//
            "      \"index\" : \"analyzed\"," +//
            "      \"include_in_all\" : \"true\"" +//
            "    }," +//
            "    \"my_text_2\" : {" +//
            "      \"type\" : \"string\"," +//
            "      \"store\" : \"yes\"," +//
            "      \"index\" : \"analyzed\"," +//
            "      \"include_in_all\" : \"true\"" +//
            "    }," +//
            "    \"my_text_3\" : {" +//
            "      \"type\" : \"string\"," +//
            "      \"store\" : \"yes\"," +//
            "      \"index\" : \"analyzed\"," +//
            "      \"include_in_all\" : \"true\"" +//
            "    }," +//
            "    \"when\" : {" +//
            "      \"type\" : \"date\"," +//
            "      \"store\" : \"yes\"," +//
            "      \"include_in_all\" : \"true\"" +//
            "    }" +//
            "  }" +// properties
            '}' +// issue1839
            '}';
    indicesAdmin.preparePutMapping(NAME).setType(NAME).setSource(indexMapping).execute().actionGet();

    for (int n = 0; n &lt; THREADS; n++)
        new Thread(new Indexer(client), "indexer#" + n).start();

    while (true)
        try {
            Thread.sleep(500);
        }
        catch (InterruptedException e) {
            break;
        }
}

@SuppressWarnings("UseOfSystemOutOrSystemErr")
static final class Indexer implements Runnable {
    private final TransportClient client;
    private final Random rand = new Random(System.currentTimeMillis() + System.nanoTime());

    Indexer(TransportClient client) {
        this.client = client;
    }

    @Override
    public void run() {
        while (true) {
            try {
                Thread.sleep(10);
            }
            catch (InterruptedException e) {
                break;
            }

            BulkRequestBuilder bulk = client.prepareBulk();
            for (int n = 0; n &lt; 2500; n++)
                bulk.add(createIndexRequest());
            System.out.println(new Date().toString() + " : " + Thread.currentThread().getName() + " indexing 2500 docs");
            bulk.execute().actionGet();
            System.out.println(new Date().toString() + " : " + Thread.currentThread().getName() + " indexed 2500 docs");
        }
    }

    private IndexRequestBuilder createIndexRequest() {
        StringBuilder document = new StringBuilder().//
                append('{').//
                append("  \"").append(NAME).append("\" : {").//
                append("    \"my_text_1\" : \"").append(createSomeText()).append("\", ").//
                append("    \"my_text_2\" : \"").append(createSomeText()).append("\", ").//
                append("    \"my_text_3\" : \"").append(createSomeText()).append("\", ").//
                append("    \"when\" : \"").append(System.currentTimeMillis()).append("\" ").//
                append("  }").//
                append('}');
        return client.prepareIndex(NAME, NAME).setSource(document.toString());
    }

    private String createSomeText() {
        StringBuilder text = new StringBuilder();
        for (int n = 0; n &lt; rand.nextInt(40) + 3; n++) {
            for (int m = 0; m &lt; rand.nextInt(15) + 3; m++)
                text.append((char) (rand.nextInt(26) + 65));
            text.append(' ');
        }
        return text.append(rand.nextInt(10000000)).toString();
    }
}
```

}
</comment><comment author="kimchy" created="2012-04-05T17:24:49Z" id="4981334">Hi, thanks for the recreation, I managed to recreate it locally as well. I found the problem, it revolves around not properly handling a relocation of a primary shard when just when the one we relocated from gets closed. I will post a fix in both 0.19 and master branches (closing this issue in the commit, so we can keep track of the change). If you can check it yourself as well it would be great.
</comment><comment author="snazy" created="2012-04-05T19:48:18Z" id="4984215">Cool - that was fast :-)

I'll try it when 0.19.3 is released - so I can rollback my "timeout loop".
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API: Allow to specify fields in the request to return updated fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1838</link><project id="" key="" /><description>Allow to specify fields in the update request, returning the fields out of hte updated doc. `_source` can be used as a field, resulting in returning the full doc. the results in under `get` element, and has similar structure to the `get` response (with `_source`, if requested, and `fields` elements).
</description><key id="3942820">1838</key><summary>Update API: Allow to specify fields in the request to return updated fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.0.RC1</label></labels><created>2012-04-03T11:09:05Z</created><updated>2012-06-27T19:26:42Z</updated><resolved>2012-04-03T11:11:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JW200" created="2012-06-27T19:17:31Z" id="6612833">I'm running on 0.19.17. How is fields parameter used? I tried the bellow but userid is not returned.

{
"script" : "ctx._source.counter += counter",
"fields":"userid",
"params" : {
"counter":11
}
}
</comment><comment author="kimchy" created="2012-06-27T19:21:11Z" id="6612963">@JW200 this is only available in the upcoming 0.20 release, not in 0.19.x
</comment><comment author="JW200" created="2012-06-27T19:26:42Z" id="6613103">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add fields parameter for update API (#1822)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1837</link><project id="" key="" /><description>The plan has changed a little bit since the issue #1822 was opened.

The idea is to have something simpler exposing a "fields" parameter similar to what is offered by the get API.
</description><key id="3941458">1837</key><summary>add fields parameter for update API (#1822)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-04-03T09:13:20Z</created><updated>2014-07-06T17:44:16Z</updated><resolved>2012-04-03T11:09:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-03T11:09:22Z" id="4898820">I will push this with some minor changes under #1838.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Race condition / unusable index with illegal index setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1836</link><project id="" key="" /><description>Hi,

I've accidently corrupted my index by simply setting the value 
index.merge.policy.merge_factor
in ES 0.19.1 to an empty string.

It is not possible to reopen the index after the setting has been applied. It is even not possible to close it because the open index action loops infinitely.

All I wanted to do is to remove the setting (and apply the default value) - is this possible?

Robert
</description><key id="3940453">1836</key><summary>Race condition / unusable index with illegal index setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">snazy</reporter><labels /><created>2012-04-03T07:40:05Z</created><updated>2013-08-09T11:16:29Z</updated><resolved>2013-08-09T11:16:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-03T08:59:09Z" id="4897110">There is no way to "remove" a setting. Can't you simply close the index, update the settings, and then open it again? I missed the part where open loops, cause its already opened, no?
</comment><comment author="snazy" created="2012-04-03T13:22:52Z" id="4900987">Yes, now I know that there's no way to remove a setting (although it would be nice ;-)

I was not able to remove the empty string from the setting.

The setting was applied using:
1. close index
2. submit "update setting" with "merge_factor" : ""
3. open index
--&gt; leads to an endless loop of exceptions in server log

There was no way to close the index again and correct the setting.
</comment><comment author="spinscale" created="2013-07-05T09:59:33Z" id="20510114">I was not able to set the merge_factor to a bogus setting leading to this problem with 0.90.2 - can you still reproduce your issue?
</comment><comment author="spinscale" created="2013-08-09T11:16:29Z" id="22388317">Closing this. If you provide more infos which helps us to reproduce this bug on a current release, please do so! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>synonym removed ngram entry?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1835</link><project id="" key="" /><description>Hi,

I'm trying to use edge ngram analyzer with synonym analyzer during index time, and just the default analyzer during query time. However, it appears that for synonyms defined as "word =&gt; synonym", "word" is not indexed at all.

https://gist.github.com/2287663

Is this a known bug, or is there some other way for me to achieve the same behavior, namely, 
1. no massive query expansions in wildcard prefix queries, and
2. synonyms are only applied in one direction (instead of marking "word" and "synonym" as equivalent, or applying synonyms in query time)

Thanks!
</description><key id="3935986">1835</key><summary>synonym removed ngram entry?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dantam</reporter><labels /><created>2012-04-02T22:38:28Z</created><updated>2012-04-08T22:15:34Z</updated><resolved>2012-04-08T22:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dantam" created="2012-04-08T22:15:34Z" id="5018859">fix is actually quite simple, define synonym as "word=&gt;word, synonym"
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disabling deletion of all indices does not work when using _all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1834</link><project id="" key="" /><description>Setting action.disable_delete_all_indices: true only works for '/', not '_all':

root@staging-es-r01:/opt/elastic_search# curl -XDELETE 'http://staging-es-r01:9200/'
{"error":"ElasticSearchIllegalArgumentException[deleting all indices is disabled]","status":400}

root@staging-es-r01:/opt/elastic_search# curl -XDELETE 'http://staging-es-r01:9200/_all/'
{"ok":true,"acknowledged":true}
</description><key id="3929902">1834</key><summary>Disabling deletion of all indices does not work when using _all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heffergm</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-02T16:42:51Z</created><updated>2012-08-21T20:38:01Z</updated><resolved>2012-04-03T09:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2012-08-21T15:59:51Z" id="7905986">In fact, it also does not work with something like `-foo`:

```
curl -X DELETE localhost:9200/-this-should-not-happen
```

But it happens.
</comment><comment author="kimchy" created="2012-08-21T20:38:01Z" id="7915255">@karmi this happens because of the new regex support, fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>requesting fields=_timestamp,_source won't return _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1833</link><project id="" key="" /><description>```

curl -XDELETE 'http://127.0.0.1:9200/twitter'
curl -XPUT 'http://127.0.0.1:9200/twitter'
curl -XPUT 'http://127.0.0.1:9200/twitter/user/_mapping?pretty=1'  -d '
{
   "user" : {
      "_timestamp" : {
         "store" : 1,
         "enabled" : 1
      },
      "_source" : {
         "compress" : true
      },
      "dynamic" : false,
      "properties" : {
         "nickname" : {
            "index" : "not_analyzed",
            "store" : "yes",
            "type" : "string"
         },
         "name" : {
            "index" : "not_analyzed",
            "store" : "yes",
            "type" : "string"
         },
         "updated" : {
            "store" : "yes",
            "type" : "date"
         }
      }
   }
}
'

# [Mon Apr  2 18:30:21 2012] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Mon Apr  2 18:30:21 2012] Protocol: http, Server: 192.168.1.65:9900
curl -XPUT 'http://127.0.0.1:9200/twitter/user/mo?pretty=1&amp;refresh=true'  -d '
{
   "nickname" : "mo",
   "name" : "Moritz Onken",
   "updated" : "2012-04-02T16:30:21"
}
'

# [Mon Apr  2 18:30:22 2012] Response:
# {
#    "ok" : true,
#    "_index" : "twitter",
#    "_id" : "mo",
#    "_type" : "user",
#    "_version" : 1
# }

ok 2 - Put mo ok
# [Mon Apr  2 18:30:22 2012] Protocol: http, Server: 192.168.1.65:9900
curl -XPOST 'http://127.0.0.1:9200/twitter/user/_search?pretty=1&amp;version=1'  -d '
{
   "fields" : [
      "_source",
      "_timestamp"
   ],
   "query" : {
      "match_all" : {}
   },
   "size" : 1
}
'

# [Mon Apr  2 18:30:22 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "fields" : {
#                "_timestamp" : 1333384221848
#             },
#             "_index" : "twitter",
#             "_id" : "mo",
#             "_type" : "user",
#             "_version" : 1
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "took" : 83
# }
```

The `_source` is not being returned. This used to work in 0.18.6. This is 0.19.1.
</description><key id="3929748">1833</key><summary>requesting fields=_timestamp,_source won't return _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">monken</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-02T16:34:25Z</created><updated>2012-04-03T16:26:06Z</updated><resolved>2012-04-03T09:49:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="monken" created="2012-04-03T16:26:05Z" id="4908219">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent facet counts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1832</link><project id="" key="" /><description>I've got an issue with facet counts that I've managed to simplify into a re-creatable example (attached).

When searching an index I get a facet count for 'fabric' as 12 however when I then filter on that attribute it increases to 13, not sure how this is possible as by adding a must query I can surely only decrease the facet counts.

Attached is the script which should recreate the issue in a test_bug index, it will insert 118 documents and run a query where the "fabric" facet comes out at 12.

Query looks like:

``` json
{
  "query": {
    "bool": {
      "must": [
        {
          "query_string": {
            "default_operator": "AND",
            "fields": [
              "_all"
            ],
            "query": "double"
          }
        }
      ]
    }
  },
  "facets": {
    "styles": {
      "terms": {
        "field": "product_styles"
      }
    }
  },
  "size": 1
}
```

Facet comes out as:

``` json
"facets" : {
    "styles" : {
      "_type" : "terms",
      "missing" : 11,
      "total" : 243,
      "other" : 141,
      "terms" : [ {
        "term" : "beds",
        "count" : 19
      }, {
        "term" : "fabric",
        "count" : 12
      }, {
        "term" : "products",
        "count" : 11
      }, {
        "term" : "new",
        "count" : 11
      }, {
        "term" : "luxury",
        "count" : 10
      }, {
        "term" : "furniture",
        "count" : 10
      }, {
        "term" : "upholstered",
        "count" : 8
      }, {
        "term" : "kingsize",
        "count" : 7
      }, {
        "term" : "double",
        "count" : 7
      }, {
        "term" : "bedroom",
        "count" : 7
      } ]
    }
```

When adding a filter on the fabric like this:

``` json
{
  "query": {
    "bool": {
      "must": [
        {
          "query_string": {
            "default_operator": "AND",
            "fields": [
              "_all"
            ],
            "query": "double"
          }
        },
        {
          "term": {
            "product_styles": "fabric"
          }
        }
      ]
    }
  },
  "facets": {
    "styles": {
      "terms": {
        "field": "product_styles"
      }
    }
  },
  "size": 1
}
```

the fabric facet then increases to 13:

``` json
"facets" : {
    "styles" : {
      "_type" : "terms",
      "missing" : 0,
      "total" : 186,
      "other" : 97,
      "terms" : [ {
        "term" : "fabric",
        "count" : 13
      }, {
        "term" : "beds",
        "count" : 12
      }, {
        "term" : "upholstered",
        "count" : 9
      }, {
        "term" : "luxury",
        "count" : 9
      }, {
        "term" : "products",
        "count" : 8
      }, {
        "term" : "new",
        "count" : 8
      }, {
        "term" : "kingsize",
        "count" : 8
      }, {
        "term" : "double",
        "count" : 8
      }, {
        "term" : "furniture",
        "count" : 7
      }, {
        "term" : "bedroom",
        "count" : 7
      } ]
    }
```

Hopefully I've explained everything and you can also recreate this. I'm using elasticsearch-0.19.1 on Mac OS X Lion.

Script for recreating data is at https://gist.github.com/2283964
</description><key id="3927539">1832</key><summary>Inconsistent facet counts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stuartloxton</reporter><labels /><created>2012-04-02T14:42:14Z</created><updated>2014-07-08T16:09:38Z</updated><resolved>2013-07-05T10:55:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2012-04-02T15:39:26Z" id="4881079">Hi, did you try to run your example on a single shard? Assuming your example run on a 3 shards which are there by default. May be you are hitting similar issue to this: https://github.com/elasticsearch/elasticsearch/issues/667
</comment><comment author="stuartloxton" created="2012-04-02T16:26:40Z" id="4882226">I've just run it against an index with 1 shard and the numbers are correct however is there a way to make this work with multiple shards? Is this the expected behaviour or is this still a bug?
</comment><comment author="lukas-vlcek" created="2012-04-02T19:57:45Z" id="4886972">It is not a bug per se. It is a performance trade-off for the distributed calculation. You can try to increate the size to minimize the effect of it. AFAIK Shay have a plan to implement some improvements to allow more accurate results.
</comment><comment author="stuartloxton" created="2012-04-03T08:10:06Z" id="4896405">I've just tried changing the size on the query to various numbers between 1 and the size of the dataset and it always returns 12, for an index with roughly 150 documents and 10 properties on each one is there a reason NOT to use just 1 shard?
</comment><comment author="lukas-vlcek" created="2012-04-03T08:35:17Z" id="4896744">I was referring to size parameter of terms facet, not the size of the query.
http://www.elasticsearch.org/guide/reference/api/search/facets/terms-facet.html

In fact what is happening is the following: when ES wants to calculate top term facets then it calculates top 'size' terms per shard [or may be (top 'size' x number_of_shards), do not remember from the top of my head, but it is not important much] and then all these 'top' sets are collected by a single node (the one that started the query) and are aggregated into final top 'size' result. This strategy does not always lead to correct global results, it also depends on the nature of your data and their distribution among shards.

If you have only 150 documents then using one shard will help. Other option would be using terms facet 'size' = 150.
</comment><comment author="apatrida" created="2012-09-20T15:26:27Z" id="8733070">Solr provides accuracy here with a 2nd call to shards to calculate counts found on share A but so far missing from B,C,...Z.  And vis versa.  
</comment><comment author="spinscale" created="2013-07-05T10:55:40Z" id="20513319">looks like a duplicate of #1305
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>problem loading river plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1831</link><project id="" key="" /><description>Not sure what I am doing wrong, but _seems_ like something is broken with the river plugin I am trying to use.  I am attempting to load just the rabbitmq river plugin.  It shows up in the logs like so, with no further logging in 0.19.1.

```
[2012-04-02 08:09:41,463][INFO ][plugins                  ] [Canasta] loaded [river-rabbitmq], sites []
```

Perhaps there is something silly that I am missing, but I swear that I got this river module running before.  I can't recall with certainty which rev of elasticsearch I successfully had it running under, but could have sworn it was 0.19.1.  In any case, if it is useful I've traced the code and found PluginsService.java [line 139], which appears to be what would prevent the module from being executed:

```
if (reference.moduleClass.isAssignableFrom(module.getClass())) {
```

But it appears that this will never succeed, because...

```
reference.moduleClass  =&gt;  class org.elasticsearch.river.RiversModule
and...
module.getClass()  =&gt;  class org.elasticsearch.plugins.PluginsModule
```

Thoughts?  Perhaps the above is appropriate and I'm just missing something in the config?
</description><key id="3925433">1831</key><summary>problem loading river plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arimus</reporter><labels /><created>2012-04-02T12:22:38Z</created><updated>2012-04-03T17:49:09Z</updated><resolved>2012-04-03T17:49:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="arimus" created="2012-04-03T08:28:00Z" id="4896650">Well, with the latest source I am able to verify the RiversModule actually does get loaded on start-up, but it appears that the RabbitmqRiver class never gets instantiated and RabbitmqRiverModule.configure() doesn't seem to get called.

I can verify that onModule() does get called.  Not sure what else is necessary to get the module running?
</comment><comment author="kimchy" created="2012-04-03T11:47:39Z" id="4899276">I have got a fresh 0.19.1, and installed the `1.1.0` version (which _only_ works with 0.19.1), and ran the sample river in the README, and it works well...
</comment><comment author="arimus" created="2012-04-03T17:48:48Z" id="4916051">Well, it was indeed something simple.  I had re-installed and failed to
create the river again using curl.  I got caught up in some other details
so much, that this escaped me.  Hopefully my failure here helps the next
sleep-deprived person trying to solve this issue :)

Thanks for the help!

On Tue, Apr 3, 2012 at 4:47 AM, Shay Banon &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; I have got a fresh 0.19.1, and installed the `1.1.0` version (which _only_
&gt; works with 0.19.1), and ran the sample river in the README, and it works
&gt; well...
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1831#issuecomment-4899276
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: `term` filter to support the more "complex" form similar to `term` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1830</link><project id="" key="" /><description>Currently `term` query can come in two forms, and `term` filter only supports a single form, with `"term" : { "field_name" : "field_value" }`. Support the second `term` query format in the `term` filter as well: `"term" { "field_name" : { "value" : "field_value" } }`.
</description><key id="3922538">1830</key><summary>Query DSL: `term` filter to support the more "complex" form similar to `term` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-02T08:08:26Z</created><updated>2012-04-02T08:17:22Z</updated><resolved>2012-04-02T08:17:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Bool Filter problem in 0.19: [filtered] query does not support [should] </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1829</link><project id="" key="" /><description>Hi,

i have the following (sample) query:

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {
        }
      },
      "filter": {
        "bool": {
          "must": {
            "term": {
              "user": {
                "value": "kimchy"
              }
            }
          },
          "must_not": {
            "range": {
              "age": {
                "from": 10,
                "to": 20
              }
            }
          },
          "should": [
            {
              "term": {
                "tag": {
                  "value": "wow"
                }
              }
            },
            {
              "term": {
                "tag": {
                  "value": "elasticsearch"
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

When I run it against Elasticsearch 0.19.1, I get the following message:

```
$ curl -XPOST localhost:9200/test
$ curl -XPOST localhost:9200/_all/_search -d @query.json
... long output
nested: QueryParsingException[[test] [filtered] query does not support [should]]; }]","status":500}
```

When I change the term queries to their simpler form, it works:

```
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {
        }
      },
      "filter": {
        "bool": {
          "must": {
            "term": {
              "user": "kimchy"
            }
          },
          "must_not": {
            "range": {
              "age": {
                "from": 10,
                "to": 20
              }
            }
          },
          "should": [
            {
              "term": {
                "tag": "wow"
              }
            },
            {
              "term": {
                "tag": "elasticsearch"
              }
            }
          ]
        }
      }
    }
  }
}
```

Both forms work against 0.18.7.
</description><key id="3918162">1829</key><summary>Bool Filter problem in 0.19: [filtered] query does not support [should] </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skade</reporter><labels /><created>2012-04-01T18:49:22Z</created><updated>2012-04-02T08:29:21Z</updated><resolved>2012-04-02T08:09:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-04-02T08:09:01Z" id="4874023">The `term` filter never supported the more "complex" form of `term` query (because it did not need to, since there is no boosting), thats why you get hte failure (a bit misleading, but we can fix that). But, I think that it would be nice to also support the more "complex" form of `term` query in `term` filter as well, for consistency sake. Opened an issue: #1830.
</comment><comment author="skade" created="2012-04-02T08:28:54Z" id="4874247">Ah, so 0.18 basically just hid the error? I don't really bother, but for generation purposes, #1830 makes sense.
</comment><comment author="kimchy" created="2012-04-02T08:29:21Z" id="4874254">Yea, sadly 0.18 hid the error...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting _settings or _mapping for non-existing index returns 200 OK</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1828</link><project id="" key="" /><description>When doing a request for index settings or mapping, when the index does not exist, ES returns a `200 OK` response:

```
$ curl -i localhost:9200/NOTEXISTING/_mapping
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 2

{}

$ curl -i localhost:9200/NOTEXISTING/_settings
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 2

{}
```

It should return an error response, such as `404 Not Found`.
</description><key id="3917641">1828</key><summary>Getting _settings or _mapping for non-existing index returns 200 OK</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-04-01T17:26:23Z</created><updated>2012-04-03T11:35:18Z</updated><resolved>2012-04-03T11:35:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Feature Request: search and highlight on all fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1827</link><project id="" key="" /><description>Currently if we don't specify which fields to search and/or highlight, _all field is used.
There are some limitations using _all field however. For example, it doesn't let you specify different analyzer for different field. Also, highlighting on _all field is difficult and expensive.

We'd like to have a mechanism to search and/or highlight on all fields without specifying all of them.
</description><key id="3906287">1827</key><summary>Feature Request: search and highlight on all fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jimmychen</reporter><labels /><created>2012-03-30T22:43:27Z</created><updated>2013-08-26T23:41:59Z</updated><resolved>2013-08-26T23:41:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Vineeth-Mohan" created="2013-01-07T14:55:01Z" id="11954264">@jimmychen  - I guess this issue address your same need - https://github.com/elasticsearch/elasticsearch/issues/2396
</comment><comment author="javanna" created="2013-08-26T20:10:37Z" id="23290868">#2396 was solved, therefore it is now possible to use wildcards when specifying the fields to highlight.
@jimmychen do you think that might be a reasonable solution for this issue as well?
</comment><comment author="jimmychen" created="2013-08-26T23:41:58Z" id="23303684">Sounds good to me. Thanks for the information. I am closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GET to _search/ should return result of match all query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1826</link><project id="" key="" /><description>Should be able to do GET on {index/type}/_search (or even on /{index/type}/) and get results of matchall query. At the moment you have to do something like:

{index/type}/_search?size=5
</description><key id="3899692">1826</key><summary>GET to _search/ should return result of match all query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rufuspollock</reporter><labels /><created>2012-03-30T15:28:04Z</created><updated>2012-04-01T09:43:15Z</updated><resolved>2012-04-01T09:43:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2012-03-30T15:35:02Z" id="4844900">Makes sense to me, if the `size` parameter is all it takes to return the first results of the `match_all` query, then maybe `&lt;MYINDEX&gt;/_search` is enough and makes a nice default endpoint?
</comment><comment author="dadoonet" created="2012-03-31T08:25:29Z" id="4856895">It could be a count instead of a search (for /{index/type}/), just like what couchDb does. http://wiki.apache.org/couchdb/HTTP_database_API#Database_Information
</comment><comment author="kimchy" created="2012-03-31T18:43:32Z" id="4860651">@rgrp which version are you using? In 0.19, you can execute `curl host:9200/index/type/_search` / `curl localhost:9200/index/_search` and you get back a result as if you executed a match all.
</comment><comment author="rufuspollock" created="2012-04-01T09:43:15Z" id="4865326">I was using 0.18 -- Great to hear this is already in!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow setting aliases on index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1825</link><project id="" key="" /><description>Allow setting aliases on index template so the newly created index is automatically assigned to the alias.
</description><key id="3890582">1825</key><summary>Allow setting aliases on index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">tsouza</reporter><labels><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2012-03-30T00:10:24Z</created><updated>2014-03-06T18:23:06Z</updated><resolved>2014-03-06T10:25:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TwP" created="2012-10-23T23:56:56Z" id="9722994">This would be a useful feature. :+1:
</comment><comment author="olagache" created="2013-02-01T17:17:08Z" id="13003846">:+1: 
</comment><comment author="YannBrrd" created="2013-12-23T15:08:41Z" id="31123566">:+1: 
</comment><comment author="rbraley" created="2014-02-09T05:44:45Z" id="34566277">this would be great
</comment><comment author="rbraley" created="2014-03-06T11:36:48Z" id="36847188">This is great, but would it be possible to specify a different pattern than just "{index}-alias" in the alias name? I have a use case where I have an index per day with the date after it:  "index_name_20140306" and I want to add it to an alias "index_name" to group them all together for searching.

Can we make the alias name a script field so we can use mvel to calculate the alias name? Or perhaps add more things we can use like the value of a field defined on a template's mapping? "{field.path}" perhaps similar to how we can define routing by the value of a field? 
</comment><comment author="javanna" created="2014-03-06T12:00:44Z" id="36849786">Hi @rbraley we had the same discussion in #5180 and decided to merge the PR in as is since we didn't reach a consensus on how to support what you (and other users too) are asking for. On the other hand, we can always improve this, would you mind opening a new specific issue for this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search/Get: Add preference option of `_primary_first` trying to primary first and then replicas</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1824</link><project id="" key="" /><description /><key id="3890083">1824</key><summary>Search/Get: Add preference option of `_primary_first` trying to primary first and then replicas</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-29T23:24:24Z</created><updated>2012-03-29T23:31:47Z</updated><resolved>2012-03-29T23:31:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>IndexMissingException caused by shutting down single node in cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1823</link><project id="" key="" /><description>When performing get requests against an 8 node cluster (many small indices, 2 replicas per index), we shut down a single node. Doing so raises a short burst of the following exceptions that cause the requests to fail:

Caused by: org.elasticsearch.indices.IndexMissingException: [contact_documents-22-0] missing at org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:243) at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:95) at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:42) at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$ShardTransportHandler.messageReceived(TransportShardSingleOperationAction.java:253) at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$ShardTransportHandler.messageReceived(TransportShardSingleOperationAction.java:239) at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:374) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source),
</description><key id="3885918">1823</key><summary>IndexMissingException caused by shutting down single node in cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heffergm</reporter><labels /><created>2012-03-29T19:04:43Z</created><updated>2012-03-29T23:30:17Z</updated><resolved>2012-03-29T23:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-29T23:26:46Z" id="4833557">After talking to @outoftime, we figured out it happens because of using `preference` set to `primary` when doing the get, and while the node is being shutdown, and a primary shard is allocated to it, it takes some time to elect the new primary shard. We could potentially block till it happens, but thats tricky, for now, I opened #1824 which will provide a nice solution as well.
</comment><comment author="heffergm" created="2012-03-29T23:30:17Z" id="4833609">Ah, I'd hypothesized that's what was going on. 1824 sounds like a good fix. 

Thanks!

## 

Grant Heffernan
~via tiny keyboard

On Mar 29, 2012, at 7:26 PM, Shay Banonreply@reply.github.com wrote:

&gt; After talking to @outoftime, we figured out it happens because of using `preference` set to `primary` when doing the get, and while the node is being shutdown, and a primary shard is allocated to it, it takes some time to elect the new primary shard. We could potentially block till it happens, but thats tricky, for now, I opened #1824 which will provide a nice solution as well.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/1823#issuecomment-4833557
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add return script option to the update API </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1822</link><project id="" key="" /><description>This return script would be executed on the updated doc and return what's left from it.
By default it would be null and return nothing.
A special value like _all_ will return the complete updated source.
</description><key id="3877119">1822</key><summary>add return script option to the update API </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-03-29T12:25:00Z</created><updated>2013-03-02T16:20:20Z</updated><resolved>2013-03-02T16:20:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JW200" created="2012-06-27T19:14:36Z" id="6612765">I'm running on 0.19.17. How is fields parameter used? I tried the bellow but userid is not returned. 

{
         "script" : "ctx._source.counter += counter",
         "fields":"userid",
         "params" : {
                     "counter":11
         }
}
</comment><comment author="Paikan" created="2012-06-27T22:25:11Z" id="6617248">@JW200 This feature has only been pushed on master branch. So you can try it using this branch or you will have to wait ES 0.20 I guess
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix case in javadoc of the query fuzzy_like_this_field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1821</link><project id="" key="" /><description /><key id="3873208">1821</key><summary>fix case in javadoc of the query fuzzy_like_this_field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-03-29T09:40:39Z</created><updated>2014-07-16T21:55:28Z</updated><resolved>2012-03-29T10:01:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-29T10:01:56Z" id="4804040">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Fix for issue #1819 where TransportClient (sniff) fails to reconnect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1820</link><project id="" key="" /><description /><key id="3869945">1820</key><summary> Fix for issue #1819 where TransportClient (sniff) fails to reconnect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">holdenk</reporter><labels /><created>2012-03-29T04:35:46Z</created><updated>2014-06-19T08:43:38Z</updated><resolved>2012-03-29T09:20:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="holdenk" created="2012-03-29T04:43:49Z" id="4796781">Related issue is https://github.com/elasticsearch/elasticsearch/issues/1819 
</comment><comment author="jprante" created="2012-03-29T08:14:56Z" id="4798782">Hey, this looks like this also could fix #1818 :)
</comment><comment author="holdenk" created="2012-03-29T08:42:10Z" id="4799141">I suppose it could if #1818 was caused by the countdown latch not reaching zero correctly. I didn't try to repro #1818 though so I'm not sure, I'll take a look tomorrow and see if it fixes that too.
</comment><comment author="kimchy" created="2012-03-29T09:20:59Z" id="4801203">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient (sniff) fails to reconnect to nodes once removed if all nodes are removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1819</link><project id="" key="" /><description>Reproduction steps:

Create transportclient pointed at a single node cluster. Query cluster. Shutdown cluster. Query. Bring back up node. Query.

The exception is:

org.elasticsearch.client.transport.NoNodeAvailableException: No node available
    at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:214)
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:182)
    at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:97)
    at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:206)
    at org.elasticsearch.client.transport.TransportClient.search(TransportClient.java:345)
    at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:743)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
    at com.foursquare.slashem.ElasticSchema$$anonfun$18.apply(Schema.scala:506)
    at com.foursquare.slashem.ElasticSchema$$anonfun$18.apply(Schema.scala:470)
    at com.twitter.util.Try$.apply(Try.scala:13)
    at com.twitter.util.ExecutorServiceFuturePool$$anon$2.run(FuturePool.scala:65)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
    at java.util.concurrent.FutureTask.run(FutureTask.java:166)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
</description><key id="3869774">1819</key><summary>TransportClient (sniff) fails to reconnect to nodes once removed if all nodes are removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">holdenk</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-29T04:04:51Z</created><updated>2012-03-29T09:22:11Z</updated><resolved>2012-03-29T09:22:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-29T09:22:11Z" id="4801289">Fixed in #1818.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportClient hangs when in sniff mode and no node running </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1818</link><project id="" key="" /><description>When TransportClient is started in sniff mode, but no ES node is running at a single given transport address, only a "Connection refused" exception is silently logged at debug level.  After that, TransportClient hangs in "addTransportAddress".

Code:

```
    Settings settings = ImmutableSettings.settingsBuilder()
            .put("cluster.name", "elasticsearch")
            .put("client.transport.sniff", true)
            .build();
    try {
        Client client = new TransportClient(settings);        
        client.addTransportAddress(address);
        client.close();
    } catch (Exception e) {
        logger.log(Level.WARNING, e.getMessage());            
    }
```

I expect a NoNodeAvailableException  (or a MasterNotDiscoveredException?) instead.

Here is the debug level message.

```
[22:12:51,680][DEBUG][org.elasticsearch.client.transport] [Riot] failed to connect to node [[#transport#-1][inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]]], removed from nodes list
org.elasticsearch.transport.ConnectTransportException: [][inet[Jorg-Prantes-MacBook-Pro.local/192.168.1.113:9300]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:560)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:503)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:482)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:128)
    at org.elasticsearch.client.transport.TransportClientNodesService$SniffNodesSampler$1.run(TransportClientNodesService.java:327)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.connect(NioClientSocketPipelineSink.java:400)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:362)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:284)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    ... 3 more
```
</description><key id="3863602">1818</key><summary>TransportClient hangs when in sniff mode and no node running </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-28T20:39:28Z</created><updated>2012-03-29T09:21:14Z</updated><resolved>2012-03-29T09:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2012-03-29T08:15:49Z" id="4798802">See also #1819
</comment><comment author="kimchy" created="2012-03-29T09:19:36Z" id="4801109">Yes, seems like #1819 will fix it.
</comment><comment author="kimchy" created="2012-03-29T09:21:14Z" id="4801221">Fixed in #1819.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_explain service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1817</link><project id="" key="" /><description>The rest and client and transport implementations of a service to explain a particular document against a query
</description><key id="3859211">1817</key><summary>/_explain service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-03-28T18:12:14Z</created><updated>2014-06-25T14:37:51Z</updated><resolved>2012-09-30T11:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mbarrien" created="2012-06-08T18:16:05Z" id="6209516">We could really use this right now. Any word on status or word on how we could help clean this up if needed to get it pulled in?
</comment><comment author="otisg" created="2012-06-21T18:51:52Z" id="6491243">I have the use for this, too. +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing a document in smile format and getting it through REST in json format fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1816</link><project id="" key="" /><description>The automatic conversion from smile to json fails to properly parse the input
</description><key id="3847855">1816</key><summary>Indexing a document in smile format and getting it through REST in json format fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-28T10:02:55Z</created><updated>2012-03-28T10:03:58Z</updated><resolved>2012-03-28T10:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add plugin mechanism for handling source storage and retrieval - index level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1815</link><project id="" key="" /><description>Here is another version to compare. I moved configuration completely to the index level and kept compression and include/exclude functionality in the SourceFieldMapper. I think it looks much simpler and doesn't loose any functionality.
</description><key id="3829108">1815</key><summary>add plugin mechanism for handling source storage and retrieval - index level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-03-27T14:57:11Z</created><updated>2014-07-16T21:55:30Z</updated><resolved>2012-04-05T00:07:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-29T11:06:40Z" id="4808060">Heya, looks much better. Some notes:
- Why not have the SourceFilter work with BytesHolder when serializing and return BytesHolder? In many cases, the extra parsing now required is not needed, for example, when storing the full source in an external system. Also, in this case, if the filter returns null, we don't really need to store the _source at all. This also means that the part that deserializes the source should handle BytesHolder, and it might be null.
- With the above note, SourceFilter name might not make sense, maybe ExternalSourceProvider?
- Still on the fence on the per type source thingy, and not index wide. Teh main reason currently is that we can optimize things if its on the index leve, for example, a system that stores the full source externally does not even need to load the _suorce from the document, so it can denote it somehow using the ExternalSourceProvider, so we can optimize the load process of fields from the doc. On the other hand, in Lucene 4.0, _source will probably move to be stored as a doc value, so it won't be relevant...
- FetchPhase call copyBytes to pass the byte[] to the InternalHIt, why not just pass the bytes as is, and not do the extra copy?
</comment><comment author="imotov" created="2012-03-29T19:03:51Z" id="4828516">I just pushed the following changes
- Serialization now works with BytesHolder and returns BytesHolder. I have also added the abstract class ParsingExternalSourceProvider that does parsing to simplify writing providers that need it.
- SourceFilter is renamed into ExternalSourceProvider. 
- Regarding type-level implementation, it would be difficult to optimize things on the index level because source extraction logic is encapsulated inside SourceFieldMapper, which operates on the type level. One idea that I had is to group results by type on the FetchPhase level and then retrieve all sources for a given type in a single request. That might improve performance for external source providers with high request latency (SQL-based, for example). So, it might make more sense to optimize it on this level in the future. In the current implementation, however, providers should return null to indicate that a record source is not available, and no source processing should be done on retrieval; an empty array to indicate that a record source is available, but they don't need any additional information to retrieve source; or any additional field that are required to obtain source during retrieval. 
- Unnecessary FetchPhase call to copyBytes is removed.
</comment><comment author="kimchy" created="2012-04-03T10:24:30Z" id="4898260">More comments :): Also, can you squash your comments into a single one?
- In SourceFieldMapper, why the fact that an external source provider exists requires "filtering". It would be much simpler to just call the source provider right after we initialize the data variables here: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/internal/SourceFieldMapper.java#L244, and update them if the source provider returns a value. Then, lets the filtering, and compression take their course if configured.
- Can you add a test for a case where the external source provider returns null, but can still return the source in this case? I think it will fail because of the check for null on the source field.
</comment><comment author="imotov" created="2012-04-03T14:15:30Z" id="4902128">I have updated SourceFieldMapper as you requested, squashed everything into a single commit and rebased it against the master.

Regarding the test, I intentionally implemented it in such a way that if the dehydrate method returns null, the rehydrate method is not called on retrieval. I thought it would be a nice way for the provider to indicate if source is "enabled" or "disabled" for the given record. Should I change it so rehydrate is always called?

By the way, here is an alternative version with a provider implemented completely on the index level: https://github.com/imotov/elasticsearch/commit/58931d37810ead45728fcab60083390d9f3d9fd8 If you still think that implementation with one provider per index is better, I can close this pull request and open another one based on that version.
</comment><comment author="kimchy" created="2012-04-04T13:35:28Z" id="4953408">I like the index level source provider much better :), lets go with that one for now, can you create another pull request?

Regarding returning null, I think that it makes sense for a case where the source provider does not persist anything the index index, but still can provide the source based on the type/id.
</comment><comment author="imotov" created="2012-04-05T00:07:50Z" id="4966002">Moved to https://github.com/elasticsearch/elasticsearch/pull/1847
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Calling SearchHit#sourceAsString will not decompress the source to convert it to string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1814</link><project id="" key="" /><description /><key id="3826586">1814</key><summary>Java API: Calling SearchHit#sourceAsString will not decompress the source to convert it to string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-27T12:50:59Z</created><updated>2012-03-27T12:57:39Z</updated><resolved>2012-03-27T12:57:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping: Allow to configure position_offset_gap for string mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1813</link><project id="" key="" /><description>High `position_offset_gap` allows to prevent phrase queries to cross multi valued field boundaries. For example:

```
curl -XPUT localhost:9200/test -d '{
    "mappings" : {
        "type" : {
            "properties" : {
                "text" : {
                    "type" : "string", 
                    "position_offset_gap" : 100
                }
            }
        }
    }
}'

curl -XPUT localhost:9200/test/type/1 -d '{
    "text" : ["xxx yyy", "zzz"]
}'

curl localhost:9200/test/type/_search -d '{
    "query" : {
        "text_phrase" : {
            "text" : "yyy zzz"
        }
    }
}'
```
</description><key id="3792017">1813</key><summary>Mapping: Allow to configure position_offset_gap for string mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-24T12:58:41Z</created><updated>2013-05-27T09:06:41Z</updated><resolved>2012-03-24T12:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Analysis: Custom analyzer to allow to configure position_offset_gap and offset_gap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1812</link><project id="" key="" /><description /><key id="3791689">1812</key><summary>Analysis: Custom analyzer to allow to configure position_offset_gap and offset_gap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-24T11:45:53Z</created><updated>2013-05-27T09:06:41Z</updated><resolved>2012-03-24T11:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add extended validation information</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1811</link><project id="" key="" /><description>This pull request is adding an extended validation information option to the _validate/query request. By adding **explain=true** to the URL it's now possible to get text of the error message if validation fails or translated query if validation succeeds. Such information is very useful when debugging queries that return unexpected results. 

Example of a valid query:

```
$ curl 'localhost:9200/twitter/_validate/query?pretty=true&amp;explain=true' -d '{
  "query_string" : {
    "query" : "some query string here"
  }
}'
```

```
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "twitter",
    "valid" : true,
    "explanation" : "_all:some _all:query _all:string _all:here"
  } ]
} 
```

Example if an invalid query

```
$ curl 'localhost:9200/twitter/_validate/query?pretty=true&amp;explain=true' -d '{
  "query_string" : {
    "query" : "missing ("
  }
}'
```

```
{
  "valid" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "twitter",
    "valid" : false,
    "error" : "org.elasticsearch.index.query.QueryParsingException: [twitter] Failed to parse query [missing (]; org.apache.lucene.queryParser.ParseException: Cannot parse 'missing (': Encountered \"&lt;EOF&gt;\" at line 1, column 9.\nWas expecting one of:\n    &lt;NOT&gt; ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    &lt;QUOTED&gt; ...\n    &lt;TERM&gt; ...\n    &lt;PREFIXTERM&gt; ...\n    &lt;WILDTERM&gt; ...\n    \"[\" ...\n    \"{\" ...\n    &lt;NUMBER&gt; ...\n    &lt;TERM&gt; ...\n    \"*\" ...\n    "
  } ]
}
```
</description><key id="3790177">1811</key><summary>add extended validation information</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-03-24T03:18:26Z</created><updated>2014-06-27T06:17:56Z</updated><resolved>2012-03-24T11:40:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-24T11:40:46Z" id="4673104">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryBuilders should be Serializable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1810</link><project id="" key="" /><description>So that queries can be passed around especially in map reduce applications, it would be convenient if the queryBuilders were serializable. 
</description><key id="3787532">1810</key><summary>QueryBuilders should be Serializable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">kraythe</reporter><labels><label>feedback_needed</label></labels><created>2012-03-23T21:13:22Z</created><updated>2014-08-08T09:51:01Z</updated><resolved>2014-08-08T09:51:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-18T09:11:30Z" id="49410668">@kraythe do you mean serializable in the sense that they provide methods for serializing, or do you mean that they should implement `Serializable` in the Java sense?
</comment><comment author="clintongormley" created="2014-08-08T09:51:01Z" id="51582785">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch goes down periodically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1809</link><project id="" key="" /><description>Hi there, 
We use ElasticSearch in one of our projects. The system goes down periodically, without any reason. And than we need to restart everything from command line. 

How can we understand the reason? 

Thanks
Tolga
</description><key id="3777206">1809</key><summary>ElasticSearch goes down periodically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tftr</reporter><labels /><created>2012-03-23T09:28:43Z</created><updated>2013-08-09T11:15:33Z</updated><resolved>2013-08-09T11:15:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-23T09:42:09Z" id="4656146">Have you seen any JVM crash files created? something like `hs_err_pid` file in the working directory of the process.
</comment><comment author="tftr" created="2012-03-23T09:49:30Z" id="4656217">No I don't see that file in the system. But we have logs in the log dir, can those help us? 
</comment><comment author="kimchy" created="2012-03-23T09:56:36Z" id="4656279">You haven't looked at the logs? Yea, check hte logs and see what its printing. Btw, the best place for this is the mailing list, which is more aimed to help resolve these problems, not the issues, which are more aimed at specific issues found or features.
</comment><comment author="tarunjangra" created="2012-04-01T00:39:53Z" id="4863041">It might be "too_many_opened_files" problem. If so than you can google around and find solution on elasticsearch.org itself.
</comment><comment author="spinscale" created="2013-07-05T09:50:52Z" id="20509781">Do you still have this problem? Have you taken a look at the log files? Anything we can help to debug? Checked the OOM killer from the kernel?
</comment><comment author="spinscale" created="2013-08-09T11:15:33Z" id="22388293">Closing due to lack of information. Happy to reopen if more information is provided to help us debugging this on a current release! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add an index level setting to disable/enable purging of expired docs (is...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1808</link><project id="" key="" /><description>...sue #1791)
</description><key id="3772410">1808</key><summary>add an index level setting to disable/enable purging of expired docs (is...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-22T22:41:40Z</created><updated>2014-06-21T19:18:20Z</updated><resolved>2012-03-23T09:47:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-23T09:47:08Z" id="4656194">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Update Settings API does not update settings in real time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1807</link><project id="" key="" /><description>Settings are still updated, but the process that applies them in real time stopped working...
</description><key id="3768293">1807</key><summary>Index Update Settings API does not update settings in real time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-22T18:15:34Z</created><updated>2012-03-22T18:16:18Z</updated><resolved>2012-03-22T18:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Provide a way to set the getPositionIncrementGap() in the mapping for multivalued fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1806</link><project id="" key="" /><description>Currently if you index a document like:

```
 { "title" : [ "it's time to pay income tax", "return library books on time" ] }
```

An AND query for for "tax AND return" will match the field. The reason is that ES's analyzers leave getPositionIncrementGap() at 0. As covered in section 4.7.1 of Lucene in Action the way to get around this is to subclass an analyzer and set getPositionIncrementGap to a sufficiently high value, e.g. 100 if you don't expect any string to be above 100 characters.

By using this feature you can have accurate AND matching for multi-valued fields, and it'll be possible to do exact matches against these fields. Currently if you have two documents like:

```
{ "city": [ "London", "Lund&#250;nir" ] }
{ "city": [ "London" ] }
```

The former will be considered less relevant since it isn't an exact match.
</description><key id="3765307">1806</key><summary>Provide a way to set the getPositionIncrementGap() in the mapping for multivalued fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avar</reporter><labels /><created>2012-03-22T15:52:24Z</created><updated>2012-04-18T19:49:57Z</updated><resolved>2012-04-18T19:49:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-24T13:01:29Z" id="4673454">I added support for this in #1813 and #1812, but, ti won't solve what you are after..., it only applies for phrase queries not passing cross multi valued fields values.
</comment><comment author="avar" created="2012-04-18T19:49:57Z" id="5207057">I think I misunderstood Lucene's capabilities on this issue. Unless it actually does this let's just close this bug as invalid.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include Sigar libs for AIX</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1805</link><project id="" key="" /><description>(As discussed in http://groups.google.com/group/elasticsearch/browse_thread/thread/268be85e7bdc77fe)

Include the Sigar libs for AIX in the build.

The libs can be found here:
=&gt; http://support.hyperic.com/display/SIGAR/Home 
</description><key id="3764609">1805</key><summary>Include Sigar libs for AIX</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anweibel</reporter><labels /><created>2012-03-22T15:19:24Z</created><updated>2012-04-04T13:34:13Z</updated><resolved>2012-04-03T12:35:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-22T19:14:39Z" id="4646115">The aix lib that I see is aix-5 in sigar, which AIX version do you use? Adding it is going to add a 1mb to the installation..., not too many users using aix out there :)
</comment><comment author="anweibel" created="2012-03-23T09:33:39Z" id="4656033">We are using the 64bit version of AIX 5.3. Thus, we'd be interested in libsigar-ppc64-aix-5.so .

&gt; not too many users using aix out there :)

Yes, I know that this is a bit extravagant... :)
</comment><comment author="anweibel" created="2012-04-03T12:35:12Z" id="4900025">We have tried to migrate our Elasticsearch 0.13.1 to 0.19.1 on AIX 5.3. After running into constant Java crashes due to segmentation faults, we decided to do the only right thing: Design our search architecture in such a way that we can avoid installing Elasticsearch on AIX boxes altogether, and using a less extravagant OS to run Elasticsearch. :-)

Thus, we don't need the Sigar AIX libs anymore.

Thanks anyway!
</comment><comment author="kimchy" created="2012-04-04T13:34:13Z" id="4953393">@anweibel great! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: OS stats reporting free as the key instead of used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1804</link><project id="" key="" /><description /><key id="3761908">1804</key><summary>Node Stats: OS stats reporting free as the key instead of used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-22T12:27:13Z</created><updated>2012-03-22T12:27:36Z</updated><resolved>2012-03-22T12:27:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Have streams provided to gateway (shared one) allow marking</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1803</link><project id="" key="" /><description>Seems like the aws code wraps a stream in repeatable one in order to support marking and resting for md5 calculation. And, it fails sometimes (#1800). We can provide the streams with mark supported and implement marking / reseting, so there won't be a need for the aws lib to try and support it using repeatable stream wrapper.
</description><key id="3760449">1803</key><summary>Have streams provided to gateway (shared one) allow marking</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-22T10:19:11Z</created><updated>2012-03-22T10:20:07Z</updated><resolved>2012-03-22T10:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Chapter II. "Concepts" is empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1802</link><project id="" key="" /><description>In http://www.elasticsearch.org/guide/ the second chapter "Concepts" is empty. This is annoying in order to understand how we can use ES in a efficient manner.
</description><key id="3759337">1802</key><summary>Chapter II. "Concepts" is empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">manuco</reporter><labels /><created>2012-03-22T08:25:58Z</created><updated>2013-07-05T09:52:54Z</updated><resolved>2013-07-05T09:52:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-24T11:30:15Z" id="4673055">Yea, a lot of work is left on improving the docs...
</comment><comment author="spinscale" created="2013-07-05T09:52:54Z" id="20509868">Please report documentation issues in the documentation repository at https://github.com/elasticsearch/elasticsearch.github.com

Apart from that we are trying to improve the documentation on a daily base, so, if there are concrete issues to document, feel free to create an issue! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>integers are coerced into strings when setting templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1801</link><project id="" key="" /><description>When I set a template which contains for instance:

`'settings': {'index.number_of_replicas':1,
                   'index.number_of_shards': 6},`

A query against the cluster_state api returns that same snippet but with the integers coerced into strings:

`'settings': {'index.number_of_replicas':"1",
                   'index.number_of_shards': "6"},`

Only integers are valid for both of the key/value pairs in my example, so it wouldn't seem to make any sense to return them as string literals when introspecting the settings. Furthermore this makes it difficult to programatically audit templates/mappings/etc since there is no guarantee that the configuration sent to the server will match what is returned by the server. 

I posit that it is a bug that any type coercion is happening, and a further bug that a string literal of an integer is acceptable for any value which should always be an integer.
</description><key id="3754045">1801</key><summary>integers are coerced into strings when setting templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drawks</reporter><labels /><created>2012-03-21T21:42:49Z</created><updated>2014-07-08T14:31:33Z</updated><resolved>2014-07-08T14:31:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-22T09:10:06Z" id="4634770">Its by design, the configuration settings in ES are simple string key -&gt; value paris, and they are parsed into numeric value if they are expected to be ones when queried.
</comment><comment author="drawks" created="2012-03-22T16:15:38Z" id="4641874">I understand that they are key value pairs, but why on earth would it return a string literal for an integer? Especially when you set the value as an integer literal? If this is just a dumb key store for configuration it doesn't make any sense to coerce the types.
</comment><comment author="kimchy" created="2012-03-22T16:29:26Z" id="4642207">Thats because the settings used in ES are key value pairs and both are strings (a map of string key and value), and actual type is extracted when referenced. So, since its stored that way internally, thats how its returned. We could have had a smarter configuration representation that help relevant types, but we don't.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.19.0: "Failed to snapshot translog" ("Failed to get [__2mn1]"), then all 1285 threads are blocked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1800</link><project id="" key="" /><description>Hi,

I have a three-machine cluster running v0.19.0. After a period of heavy feeding, one machine wrote this to the log:

&lt;pre&gt;
[2012-03-21 20:46:07,722][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.net.SocketException) caught when processing request: Broken pipe
[2012-03-21 20:46:07,734][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,722][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.net.SocketException) caught when processing request: Broken pipe
[2012-03-21 20:46:07,736][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,722][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.net.SocketException) caught when processing request: Broken pipe
[2012-03-21 20:46:07,736][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,722][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.net.SocketException) caught when processing request: Broken pipe
[2012-03-21 20:46:07,736][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,775][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.io.IOException) caught when processing request: Input stream cannot be reset as 1568768 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,775][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,775][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.io.IOException) caught when processing request: Input stream cannot be reset as 2764800 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,776][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,784][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.io.IOException) caught when processing request: Input stream cannot be reset as 1421312 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,785][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,803][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.io.IOException) caught when processing request: Input stream cannot be reset as 1568768 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,803][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,810][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.io.IOException) caught when processing request: Input stream cannot be reset as 1421312 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,810][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,836][WARN ][com.amazonaws.http.AmazonHttpClient] Unable to execute HTTP request: Input stream cannot be reset as 1421312 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,836][WARN ][com.amazonaws.http.AmazonHttpClient] Unable to execute HTTP request: Input stream cannot be reset as 1568768 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,881][INFO ][org.apache.http.impl.client.DefaultHttpClient] I/O exception (java.io.IOException) caught when processing request: Input stream cannot be reset as 2764800 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,882][INFO ][org.apache.http.impl.client.DefaultHttpClient] Retrying request
[2012-03-21 20:46:07,901][WARN ][index.gateway            ] [aws-e1b-15.xxxxtest.net] [twitter][7] failed to snapshot (scheduled)
org.elasticsearch.index.gateway.IndexShardGatewaySnapshotFailedException: [twitter][7] Failed to snapshot translog
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:307)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshot(BlobStoreIndexShardGateway.java:160)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:271)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:265)
    at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:1043)
    at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:528)
    at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:265)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$SnapshotRunnable.run(IndexShardGatewayService.java:366)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOException: Failed to get [__2mn1]
    at org.elasticsearch.common.blobstore.support.BlobStores.syncWriteBlob(BlobStores.java:60)
    at org.elasticsearch.cloud.aws.blobstore.S3ImmutableBlobContainer.writeBlob(S3ImmutableBlobContainer.java:59)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshotTranslog(BlobStoreIndexShardGateway.java:701)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:305)
    ... 10 more
Caused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Input stream cannot be reset as 1568768 bytes have been written, exceeding the available buffer size of 131072
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:298)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:170)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:2632)
    at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1060)
    at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:944)
    at org.elasticsearch.cloud.aws.blobstore.S3ImmutableBlobContainer$1.run(S3ImmutableBlobContainer.java:48)
    ... 3 more
Caused by: java.io.IOException: Input stream cannot be reset as 1568768 bytes have been written, exceeding the available buffer size of 131072
    at com.amazonaws.services.s3.internal.RepeatableInputStream.reset(RepeatableInputStream.java:84)
    at com.amazonaws.services.s3.internal.MD5DigestCalculatingInputStream.reset(MD5DigestCalculatingInputStream.java:60)
    at com.amazonaws.http.RepeatableInputStreamRequestEntity.writeTo(RepeatableInputStreamRequestEntity.java:123)
    at org.apache.http.entity.HttpEntityWrapper.writeTo(HttpEntityWrapper.java:96)
    at org.apache.http.impl.client.EntityEnclosingRequestWrapper$EntityWrapper.writeTo(EntityEnclosingRequestWrapper.java:108)
    at org.apache.http.impl.entity.EntitySerializer.serialize(EntitySerializer.java:120)
    at org.apache.http.impl.AbstractHttpClientConnection.sendRequestEntity(AbstractHttpClientConnection.java:263)
    at org.apache.http.impl.conn.AbstractClientConnAdapter.sendRequestEntity(AbstractClientConnAdapter.java:227)
    at org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:255)
    at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:123)
    at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:633)
    at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:266)
    ... 8 more
[2012-03-21 20:46:07,927][WARN ][com.amazonaws.http.AmazonHttpClient] Unable to execute HTTP request: Input stream cannot be reset as 2764800 bytes have been written, exceeding the available buffer size of 131072
[2012-03-21 20:46:07,928][WARN ][index.gateway            ] [aws-e1b-15.xxxxtest.net] [facebook][9] failed to snapshot (scheduled)
org.elasticsearch.index.gateway.IndexShardGatewaySnapshotFailedException: [facebook][9] Failed to snapshot translog
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:307)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshot(BlobStoreIndexShardGateway.java:160)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:271)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:265)
    at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:1043)
    at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:528)
    at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:265)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$SnapshotRunnable.run(IndexShardGatewayService.java:366)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.io.IOException: Failed to get [__2ga7]
    at org.elasticsearch.common.blobstore.support.BlobStores.syncWriteBlob(BlobStores.java:60)
    at org.elasticsearch.cloud.aws.blobstore.S3ImmutableBlobContainer.writeBlob(S3ImmutableBlobContainer.java:59)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshotTranslog(BlobStoreIndexShardGateway.java:701)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:305)
    ... 10 more
Caused by: com.amazonaws.AmazonClientException: Unable to execute HTTP request: Input stream cannot be reset as 2764800 bytes have been written, exceeding the available buffer size of 131072
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:298)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:170)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:2632)
    at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1060)
    at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:944)
    at org.elasticsearch.cloud.aws.blobstore.S3ImmutableBlobContainer$1.run(S3ImmutableBlobContainer.java:48)
    ... 3 more
Caused by: java.io.IOException: Input stream cannot be reset as 2764800 bytes have been written, exceeding the available buffer size of 131072
    at com.amazonaws.services.s3.internal.RepeatableInputStream.reset(RepeatableInputStream.java:84)
    at com.amazonaws.services.s3.internal.MD5DigestCalculatingInputStream.reset(MD5DigestCalculatingInputStream.java:60)
    at com.amazonaws.http.RepeatableInputStreamRequestEntity.writeTo(RepeatableInputStreamRequestEntity.java:123)
    at org.apache.http.entity.HttpEntityWrapper.writeTo(HttpEntityWrapper.java:96)
    at org.apache.http.impl.client.EntityEnclosingRequestWrapper$EntityWrapper.writeTo(EntityEnclosingRequestWrapper.java:108)
    at org.apache.http.impl.entity.EntitySerializer.serialize(EntitySerializer.java:120)
    at org.apache.http.impl.AbstractHttpClientConnection.sendRequestEntity(AbstractHttpClientConnection.java:263)
    at org.apache.http.impl.conn.AbstractClientConnAdapter.sendRequestEntity(AbstractClientConnAdapter.java:227)
    at org.apache.http.protocol.HttpRequestExecutor.doSendRequest(HttpRequestExecutor.java:255)
    at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:123)
    at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:633)
    at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:266)
    ... 8 more
[2012-03-21 20:46:11,802][WARN ][index.gateway            ] [aws-e1b-15.xxxxtest.net] [facebook][5] failed to snapshot (scheduled)
org.elasticsearch.index.gateway.IndexShardGatewaySnapshotFailedException: [facebook][5] Failed to perform snapshot (index files)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.doSnapshot(BlobStoreIndexShardGateway.java:246)
    at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.snapshot(BlobStoreIndexShardGateway.java:160)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:271)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$2.snapshot(IndexShardGatewayService.java:265)
    at org.elasticsearch.index.engine.robin.RobinEngine.snapshot(RobinEngine.java:1043)
    at org.elasticsearch.index.shard.service.InternalIndexShard.snapshot(InternalIndexShard.java:528)
    at org.elasticsearch.index.gateway.IndexShardGatewayService.snapshot(IndexShardGatewayService.java:265)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$SnapshotRunnable.run(IndexShardGatewayService.java:366)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
Caused by: Status Code: 400, AWS Service: Amazon S3, AWS Request ID: 51D95E9EC62C322C, AWS Error Code: RequestTimeout, AWS Error Message: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed., S3 Extended Request ID: ayC++VYMCFNkYrX1xhULdplw2F1ZTcDiXw20ou+25Yl98oNBd5EvF82vEb4cdGTh
    at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:548)
    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:288)
    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:170)
    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:2632)
    at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1060)
    at com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:944)
    at org.elasticsearch.cloud.aws.blobstore.S3ImmutableBlobContainer$1.run(S3ImmutableBlobContainer.java:48)
    ... 3 more
&lt;/pre&gt;


Now the node is unavailable; all 1285 threads are blocked. The full jstack output, additional logs, and verification that the number of open FDs is less than the limit is available https://gist.github.com/f8dcbb7ae2862327c4d4

I'm on IRC as usual. Thank you!

Alex
</description><key id="3753758">1800</key><summary>0.19.0: "Failed to snapshot translog" ("Failed to get [__2mn1]"), then all 1285 threads are blocked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alambert</reporter><labels /><created>2012-03-21T21:24:28Z</created><updated>2012-03-22T10:19:41Z</updated><resolved>2012-03-21T21:47:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alambert" created="2012-03-21T21:47:09Z" id="4627978">I had GC logging enabled: I lots of concurrent-mode &amp; parallel promotion failures around that time. So likely on my end &#8211; I don't see any place in the code that those threads could all be blocked on, and top shows the CPU usage is 100%, so it looks like the threads are blocking on stop-the-world GC. Will close while I investigate because I'm not sure that it's an ES issue.

Thanks!
</comment><comment author="kimchy" created="2012-03-22T10:19:41Z" id="4635635">The exception with the reset and mark is strange though..., we can improve on that, see #1803.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes give up trying to discover another node after 3x30s and never try again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1799</link><project id="" key="" /><description>Yesterday we had some network upgrades which made 1/3 of our nodes unavailable for some time. We're using unicast discovery with a manual host list on 0.18.7.

I think if a node can't discover its peer nodes it should try them again periodically to re-establish contact with the rest of the cluster, as-is it seems that if the network is having problems for 3x30s the cluster might split completely without any attempt to re-establish itself.

Here's the relevant logs from the node that got disconnected from the rest:

```
$ zgrep removed /var/log/elasticsearch/elasticsearch.log-20120320.gz
[2012-03-19 15:31:46,039][INFO ][cluster.service          ] [dc01search-02] removed {[dc01search-03][i_Dqb2XLRi-CqqZgOIpSdA][inet[/10.149.206.44:9300]],}, reason: zen-disco-node_failed([dc01search-03][i_Dqb2XLRi-CqqZgOIpSdA][inet[/10.149.206.44:9300]]), reason failed to ping, tried [3] times, each with maximum [30s] timeout
[2012-03-19 15:31:46,451][INFO ][cluster.service          ] [dc01search-02] removed {[dc01search-01][SGsefxoKSX2EdFYS2cku_w][inet[/10.149.206.41:9300]],}, reason: zen-disco-node_failed([dc01search-01][SGsefxoKSX2EdFYS2cku_w][inet[/10.149.206.41:9300]]), reason failed to ping, tried [3] times, each with maximum [30s] timeout
```

And here's logs from one of the other nodes:

```
$ zcat /var/log/elasticsearch/elasticsearch.log-20120320.gz
[2012-03-19 15:31:46,038][INFO ][discovery.zen            ] [dc01search-03] master_left [[dc01search-02][drJcyXbfSr687MW4PSN7Fg][inet[/10.149.208.42:9300]]], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]
[2012-03-19 15:31:46,039][INFO ][cluster.service          ] [dc01search-03] master {new [dc01search-01][SGsefxoKSX2EdFYS2cku_w][inet[/10.149.206.41:9300]], previous [dc01search-02][drJcyXbfSr687MW4PSN7Fg][inet[/10.149.208.42:9300]]}, removed {[dc01search-02][drJcyXbfSr687MW4PSN7Fg][inet[/10.149.208.42:9300]],}, reason: zen-disco-master_failed ([dc01search-02][drJcyXbfSr687MW4PSN7Fg][inet[/10.149.208.42:9300]])
```

So at the time dc01search-02 was the master, it became unavailable due
to network issues, dc01search-01 was elected the master, and until I
manually restarted ES on dc01search-02 today the cluster was in a
split brain state.
</description><key id="3745282">1799</key><summary>Nodes give up trying to discover another node after 3x30s and never try again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avar</reporter><labels /><created>2012-03-21T13:20:16Z</created><updated>2014-07-08T14:31:14Z</updated><resolved>2014-07-08T14:31:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Jagdeep1" created="2012-06-18T08:00:25Z" id="6390156">I am also facing the same issue. ES version 0.19.2
</comment><comment author="moliware" created="2012-06-25T11:18:50Z" id="6544839">I'm suffering from this issue also in version 0.19.2
</comment><comment author="artemredkin" created="2012-07-03T10:07:57Z" id="6730602">I've seen this with 0.19.5
</comment><comment author="jimdickinson" created="2013-01-03T15:54:01Z" id="11848468">This issue happens to us often in EC2, with 0.19.9.
</comment><comment author="ferhatsb" created="2013-01-03T17:01:19Z" id="11851311">After upgrading to 0.20.1 + JDK 7 in EC2 somehow we have faced this 2 times in a week.Though need more information to find out the root cause.
</comment><comment author="fygrave" created="2013-02-25T01:55:12Z" id="14022402">seeing the same issue with 20.5. any hints on "best strategy" of dealing with it? (the way I deal with it is a python script that monitors cluster wreck and forces disconnecting nodes to restart and re-discover the master).
</comment><comment author="nordbergm" created="2013-03-19T10:34:49Z" id="15107200">Also seeing the same issue in 0.20.5, JRE 6 and 7, unicast discovery. The cluster pretty much goes split brain every night and we sometimes have to do a full cluster restart to get the nodes to join up again. I can't remember having this issue with 0.19. Back in those days we used EC2 API discovery, but we're not on AWS anymore and now use unicast instead. Could this be a unicast related bug?
</comment><comment author="kimchy" created="2013-03-19T10:37:43Z" id="15107307">The idea is to set the minimum_master_nodes setting, in which case the node disconnected will not elect itself as master, but get back to a state where it keeps on retrying to form the cluster again.
</comment><comment author="HenleyChiu" created="2014-02-10T02:21:57Z" id="34596371">@kimchy That's a workaround, but is there a way to increase the ping timeout ?
</comment><comment author="clintongormley" created="2014-07-08T14:31:14Z" id="48343859">As @kimchy commented, `minimum_master_nodes` is the right answer here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add plugin mechanism for handling source storage and retrieval</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1798</link><project id="" key="" /><description>A possible implementation for elasticsearch/elasticsearch#67. Example of a file-based source provider can be found in https://github.com/imotov/elasticsearch-source-file
</description><key id="3740061">1798</key><summary>add plugin mechanism for handling source storage and retrieval</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2012-03-21T04:03:49Z</created><updated>2014-06-13T09:11:55Z</updated><resolved>2012-04-05T00:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-22T09:38:52Z" id="4635112">Heya, first, looks good, a few points:
- I am not sure that source provider configuration is best done on a "per type". It feels like the best place to configure it is on a "per index" level. Simply setting the relevant source provider as an index level setting, and all types will use it.
- The current source provider does the include/exclude logic, as well as compression. This means that other providers will not benefit from it and need to implement it. I think that the logic of compression and include/exclude should remain in the source mapper. Obviously, the provider can do its own logic as well, without someone configuring the those mapping options if relevant.
</comment><comment author="imotov" created="2012-03-23T11:11:20Z" id="4657205">I see your points. I was really struggling with figuring out which level source provider should function on. On one side making it index level would significantly simplify design. On the other side, it would be nice to keep ability to control type-level settings. It might come really handy if different types are coming from different sources (for example, main record can be retrieved from an external database, while child records have to be stored in the _source field). The fact that currently compression and include/exclude logic are configured on the type level is strong indicator that ability to handle types differently is desired. I agree that in most cases users will want to have most of configuration specified on the index or even global level. If external source is MySql database, for example, it might be very inconvenient to store connection string in each mapping. That's why I implemented source provider as two-layered system: SourceProviderParser configured on the index level which creates SourceProvider by parsing information on the type level. This approach will give source provider implementors ability to decide if they want to use type level, index level or only global level configuration based on their use cases. 

Considering a hypothetical example of MySql source provider that retrieves source from MySql database, a possible configuration for it might looks like this:  

```
curl -XPUT http://localhost:9200/test-idx -d '{
  "settings" : {
    "index.source.provider": {
      "my_connection": {
        "type": "mysql",
        "database": "my_records"
      }
    }
  },
  "mappings" : {
    "users" : {
      "_source" : { 
        "provider": "my_connection",
        "table": "users"
      },
    },
    "tweets" : {
      "_source" : { 
        "provider": "my_connection",
        "table": "messages"
      },
    }
  }
}'
```

Most of the logic of such provider might go into a singleton MySqlSourceProviderService that would be also responsible for handling connection credential, configuration, and pooling. 

Either way, it certainly make sense to allow users to specify source provider on the index level. So, I have just pushed changes that allow replacing default source provider for all types on the index level like this:

```
curl -XPUT http://localhost:9200/test-idx -d '{
  "settings": {
    "index.source.provider": {
      "default" : {
        "type": "my_provider",
        "param1": "value1",
        "param2": "value2"
      }
    }
  }
}'
```

---

Regarding include/exclude logic and compression, I was also thinking that it might be beneficial to enable it for other providers. I was going to implement it as a stack of filters, where one filter could handle include/exclude logic and another one compression, but then I realized that other providers would unlikely benefit from this functionality. 

The current workflow for handling source is this:
- **index:** source -&gt; filter fields (user-controlled) -&gt; compress -&gt; _source field
- **fetch/get:** _source field -&gt; uncompress -&gt; source

A user decides which fields should be stored during indexing and decision about compression is made based on the original size of the source. 

A typical workflow of an external source provider will be 
- **index:** source -&gt; filter fields (provider-controlled) -&gt; _source field (could be empty)
- **fetch/get:** _source field -&gt; generate source from external database -&gt; source

So, in case of external source, only source provider can decide which fields (if any) should be stored based on information that it will need to restore source later. As a result, user-controlled include/exclude logic is not very useful here. The stored source would be very different from the original source, so the threshold parameter for compression wouldn't make much sense either. Moreover, the stored source will tend to be really small (typically just database id) and therefore wouldn't benefit from compression.
</comment><comment author="imotov" created="2012-04-05T00:07:03Z" id="4965992">Moved to https://github.com/elasticsearch/elasticsearch/pull/1847
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add mean ordering for terms stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1797</link><project id="" key="" /><description>Added mean and reverse_mean order options to terms stats.  Also made
ordering based on double values more robust by using Double.compare().
</description><key id="3735682">1797</key><summary>Add mean ordering for terms stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">carwashi</reporter><labels /><created>2012-03-20T21:02:03Z</created><updated>2014-07-16T21:55:32Z</updated><resolved>2012-03-22T09:21:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-22T09:21:39Z" id="4634893">Pushed to master and 0.19 branch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting using the default highlighter (not using term vectors) only highlights on the first 50*1024 chars</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1796</link><project id="" key="" /><description>By default, enable highlighting all the data, we might add an option to control it in the future, but the default should be highlighting across all data.
</description><key id="3726263">1796</key><summary>Highlighting using the default highlighter (not using term vectors) only highlights on the first 50*1024 chars</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-20T11:32:56Z</created><updated>2012-03-20T11:39:56Z</updated><resolved>2012-03-20T11:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Java API for require_field_match </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1795</link><project id="" key="" /><description>require_field_match can be set to true which will cause a field to be highlighted only if a query matched that field. false means that terms are highlighted on all requested fields regardless if the query matches specifically on them.

Currently, elasticsearch does not support java api to set require_field_match field in highlight.
</description><key id="3704769">1795</key><summary>Java API for require_field_match </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhthu</reporter><labels><label>enhancement</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-19T03:21:00Z</created><updated>2012-03-20T09:42:24Z</updated><resolved>2012-03-20T09:42:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Get/MultiGet API with no type provided and doc does not exists causes a failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1794</link><project id="" key="" /><description>This happens because the type is not properly serialized back, where if the doc does not exists, the type to return is `null`, since we did not get it in the request...
</description><key id="3699154">1794</key><summary>Get/MultiGet API with no type provided and doc does not exists causes a failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.18.8</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-18T10:05:39Z</created><updated>2012-03-18T10:06:20Z</updated><resolved>2012-03-18T10:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>When `node.data` is set to `false`, the upgrade shards process from 0.18 fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1793</link><project id="" key="" /><description>It shouldn't really go through it..., but it still does, and fails because no data is configured.
</description><key id="3699122">1793</key><summary>When `node.data` is set to `false`, the upgrade shards process from 0.18 fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-18T09:55:14Z</created><updated>2012-03-18T09:55:40Z</updated><resolved>2012-03-18T09:55:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Failed to install cloud-aws, reason: failed to download v0.19.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1792</link><project id="" key="" /><description>Following the [install instructions](http://www.elasticsearch.org/tutorials/2011/08/22/elasticsearch-on-ec2.html) and it looks like version 19 is unable to install cloud-aws because of an incorrect url.  

``` bash
wget https://github.com/downloads/elasticsearch/elasticsearch/elasticsearch-0.19.0.zip
...snip...
[ec2-user@x elasticsearch]$ cd elasticsearch-0.19.0/
[ec2-user@x elasticsearch-0.19.0]$ sudo bin/plugin -install cloud-aws
-&gt; Installing cloud-aws...
Trying http://elasticsearch.googlecode.com/svn/plugins/cloud-aws/elasticsearch-cloud-aws-0.19.0.zip...
Failed to install cloud-aws, reason: failed to download
```

I was able to install cloud-aws by using v0.18.7.

Thanks
</description><key id="3698001">1792</key><summary>Failed to install cloud-aws, reason: failed to download v0.19.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jspooner</reporter><labels /><created>2012-03-18T03:09:54Z</created><updated>2013-08-09T14:29:43Z</updated><resolved>2013-08-09T14:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-18T10:09:03Z" id="4560258">In 0.19, all the plugins have moved to their own github repo, and the installation is different, see more on the readme here: https://github.com/elasticsearch/elasticsearch-cloud-aws.
</comment><comment author="jspooner" created="2012-03-18T14:40:58Z" id="4561593">Thanks I was able to install v19 with cloud-aws with that.
https://github.com/elasticsearch/elasticsearch.github.com/pull/162
</comment><comment author="jasonpolites" created="2013-06-07T21:49:10Z" id="19134810">Might want to update/change the public docs for this:

http://www.elasticsearch.org/guide/reference/modules/discovery/ec2/

bin/plugin -install cloud-aws does not work for me, but bin/plugin -install elasticsearch/elasticsearch-cloud-aws/1.12.0 did
</comment><comment author="spinscale" created="2013-08-09T14:29:42Z" id="22397931">The official documentation now references to the plugin website, see https://github.com/elasticsearch/elasticsearch.github.com/commit/f49db322dd7c23dbc8183479281b4fbb05e64ba6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an index level setting to disable/enable purging of expired docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1791</link><project id="" key="" /><description>As discussed here https://groups.google.com/d/topic/elasticsearch/cQWpLg9rhzs/discussion.

I will open a pull request for it soon.
</description><key id="3693823">1791</key><summary>Add an index level setting to disable/enable purging of expired docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels><label>enhancement</label><label>v0.19.2</label><label>v0.20.0.RC1</label></labels><created>2012-03-17T10:46:25Z</created><updated>2012-03-23T09:57:05Z</updated><resolved>2012-03-23T09:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RemoteTransportException with _mget and 0.18.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1790</link><project id="" key="" /><description>I'm running into a RemoteTransportException between a 0.18.7 node.client and a server. Here's a reduction of the problem:

% curl -XPOST "http://localhost:9200/test/test/1" -d '{}'
{"ok":true,"_index":"test","_type":"test","_id":"1","_version":1}
% curl -XPOST "http://localhost:9200/test/_mget?pretty=true" -d '{"docs":[{"_id":"1"},{"_id":"2"}]}'
{
  "docs" : [ {
    "_index" : "test",
    "_type" : "test",
    "_id" : "1",
    "_version" : 1,
    "exists" : true, "_source" : {}
  }, {
    "_index" : "test",
    "_type": null,
    "_id" : "b-1",
    "error" : "RemoteTransportException[[&lt;master&gt;][inet[/&lt;ipaddress&gt;:9300]][indices/mget/shard/s]]; nested: "
  } ]
}

The client ES node throws this exception:

[2012-03-16 23:49:10,416][DEBUG][action.get               ] [Atum] Index Shard [test][3]: Failed to execute [org.elasticsearch.action.get.MultiGetShardRequest@61735602]
org.elasticsearch.transport.RemoteTransportException: [Mojo][inet[/&lt;ipaddress&gt;:9300]][indices/mget/shard/s]
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.io.stream.HandlesStreamOutput.writeUTF(HandlesStreamOutput.java:54)
    at org.elasticsearch.index.get.GetResult.writeTo(GetResult.java:326)
    at org.elasticsearch.action.get.GetResponse.writeTo(GetResponse.java:190)
    at org.elasticsearch.action.get.MultiGetShardResponse.writeTo(MultiGetShardResponse.java:85)
    at org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:136)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:74)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:66)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$ShardTransportHandler.messageReceived(TransportShardSingleOperationAction.java:237)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$ShardTransportHandler.messageReceived(TransportShardSingleOperationAction.java:225)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:358)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)

I think this is pretty much the same issue as #1770, and hopefully will be solved just as simply. Also, is there any chance a fix could land in a 0.18.8 release? I can't transition to the 0.19.x series yet. Thanks!
</description><key id="3691309">1790</key><summary>RemoteTransportException with _mget and 0.18.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2012-03-17T00:14:06Z</created><updated>2013-07-05T09:49:12Z</updated><resolved>2013-07-05T09:49:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-17T10:48:37Z" id="4553207">Is there a chance for a full recreation? Including indexing some sample data? No problem with fixing it on the 0.18 branch if it only happens there, of course, but a recreation will go a long way.
</comment><comment author="erickt" created="2012-03-17T15:04:27Z" id="4554545">Hi Kimchy! It's perfectly repeatable for me, the little bit of code I pasted triggered the bug for me. Node A was 

```
node.master: true
node.data: true
```

Node B was

```
node.master: false
node.data: false
```

Nothing was indexed. On Node B, I ran

```
% curl -XPOST "http://localhost:9200/test/test/1" -d '{}'
% curl -XPOST "http://localhost:9200/test/_mget?pretty=true" -d '{"docs":[{"_id":"1"},{"_id":"2"}]}'
```

Which triggered the bug above.
</comment><comment author="kimchy" created="2012-03-18T10:07:57Z" id="4560249">@erickt I found the problem and opened an issue here: #1794, I fixed it in 0.19 branch and master. The problem is that type is not provided, and it fails to serialize properly with documents that do not exists. If you provide the type of the doc itself, it won't happen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default return for _cluster/node/stats inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1789</link><project id="" key="" /><description>When you have a single node environment and make a request to:

http://localhost:9200/_cluster/node/stats?callback=jsonp... 

you will get the jvm object in the response.

However, when you have a multi node cluster environment, the jvm object does not come back in the response by default.

You have to specify the jvm=true parameter to get it back.

This currently breaks bigdisk in multi node cluster environment.
</description><key id="3686899">1789</key><summary>default return for _cluster/node/stats inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jhnlsn</reporter><labels /><created>2012-03-16T18:18:02Z</created><updated>2012-03-18T15:18:01Z</updated><resolved>2012-03-18T15:18:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jhnlsn" created="2012-03-16T18:20:17Z" id="4545290">Looks like this is a fix that is being done by bigdisk, not sure if this is really a bug or just expected behavior for version 0.19.0
</comment><comment author="lukas-vlcek" created="2012-03-16T19:09:15Z" id="4546127">Note, bigdesk 1.x does not support ES 0.19.x
I am working hard on new bigdesk 2.x that will support ES 0.19.x
</comment><comment author="kimchy" created="2012-03-17T10:43:31Z" id="4553179">The node stats API changed a bit in 0.19, @lukas-vlcek maybe it make sense to try and make current bigdesk work with 0.19? It just means passing all the flags to get the results by default.
</comment><comment author="kimchy" created="2012-03-17T10:44:16Z" id="4553185">@johnymonster There is no differnece between single node and multi node in terms of hte API, jvm stats are not enabled by default, and you need to explicitly ask for them.
</comment><comment author="jhnlsn" created="2012-03-17T15:42:49Z" id="4554793">@kimchy what I was reporting is that there is a difference in the response object from a single node cluster and a multi node cluster.  To test this make a request to a single node cluster, you will see the jvm object return for the node.

Then make a request to the multi node cluster, there will not be a jvm object in any of the node objects that return.
</comment><comment author="kimchy" created="2012-03-18T09:51:00Z" id="4560186">@johnymonster I've fired up a single node, executed: `curl localhost:9200/_cluster/nodes/stats?pretty=1`, and I don't see the jvm stats element in it...
</comment><comment author="jhnlsn" created="2012-03-18T15:17:40Z" id="4561814">Ah my bad, I do realize now that my production single node is 0.18.7.  I was testing clustering, but using 0.19.  Sorry for the confusion.
</comment><comment author="jhnlsn" created="2012-03-18T15:18:01Z" id="4561816">not an issue anymore obviously.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[internal method naming] Strings#format1Decimals -&gt; String#formatDecimals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1788</link><project id="" key="" /><description>org.elasticsearch.common.Strings#format1Decimals
Shouldn't it be called formatDecimals? (without "1" character)
</description><key id="3681836">1788</key><summary>[internal method naming] Strings#format1Decimals -&gt; String#formatDecimals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2012-03-16T13:03:49Z</created><updated>2013-07-04T12:58:48Z</updated><resolved>2013-07-04T12:58:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-17T10:42:06Z" id="4553163">Its just means that formats a double value with a single decimal point.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: indices filter type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1787</link><project id="" key="" /><description>Since 0.19 there is now the new indices query type. (see issue https://github.com/elasticsearch/elasticsearch/issues/1416).

It would make sense to have the same available as a filter type.
</description><key id="3679205">1787</key><summary>Query DSL: indices filter type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels><label>enhancement</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-16T08:47:07Z</created><updated>2012-03-17T11:24:42Z</updated><resolved>2012-03-17T11:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RabbitMQ River Validation Failed: 1: no requests added;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1786</link><project id="" key="" /><description>I'm trying to use rabbitMQ river and I'm facing a problem.

my sample data :

{"index":{"_index":"error","_type":"browsingLog","_id":"125558794","_ttl":{"enabled":true,"default":"30d"}}}
{"browsingLog":{"browseRequestMethod":"POST","browseRequestParams":"param","language":"fr","country":"fr","memberId":4568,"source":null,"flow":null,"browseHistory":"No log","browseOrigin":"fixture-test","browseIp":"127.0.0.1","browseUserAgent":"fixture-test","browseUserAgentType":"fixture-test-type","browseRequestUri":"/fit/nesse2","creationDate":1331682229271,"exceptionStackTrace":"java.lang.RuntimeException: fixture message\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.send(BrowsingLogGenerate.java:77)\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.main(BrowsingLogGenerate.java:119)\r\n","exceptionType":"java.lang.RuntimeException","exceptionMessage":"fixture message"}}

Using the command with the above sample 
curl -XPUT 'http://localhost:9200/_bulk' --data-binary @bulk.data
the data are well inserted into ES

Here is what I inserted into the queue
First try :

curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{\"index\":{\"_index\":\"error\",\"_type\":\"browsingLog\",\"_id\":\"125558794\",\"_ttl\":{\"enabled\":true,\"default\":\"30d\"}}}"}'

curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{\"browsingLog\":{\"browseRequestMethod\":\"POST\",\"browseRequestParams\":\"param\",\"language\":\"fr\",\"country\":\"fr\",\"memberId\":4568,\"source\":null,\"flow\":null,\"browseHistory\":\"No log\",\"browseOrigin\":\"fixture-test\",\"browseIp\":\"127.0.0.1\",\"browseUserAgent\":\"fixture-test\",\"browseUserAgentType\":\"fixture-test-type\",\"browseRequestUri\":\"/fit/nesse2\",\"creationDate\":1331682229271,\"exceptionStackTrace\":\"java.lang.RuntimeException: fixture message\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.send(BrowsingLogGenerate.java:77)\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.main(BrowsingLogGenerate.java:119)\r\n\",\"exceptionType\":\"java.lang.RuntimeException\",\"exceptionMessage\":\"fixture message\"}}"}'

I had 2 messages in the queue 

Message 1

The server reported 1 messages remaining.
Exchange    LOG_BROWSING
Routing Key     #.#.#
Redelivered     &#9675;
Properties  
content_encoding:   UTF-8
content_type:   application/json
Payload 109 bytes Encoding: string  

{"index":{"_index":"error","_type":"browsingLog","_id":"125558794","_ttl":{"enabled":true,"default":"30d"}}}

Message 2

The server reported 0 messages remaining.
Exchange    LOG_BROWSING
Routing Key     #.#.#
Redelivered     &#9675;
Properties  
content_encoding:   UTF-8
content_type:   application/json
Payload 715 bytes Encoding: string  

{"browsingLog":{"browseRequestMethod":"POST","browseRequestParams":"param","language":"fr","country":"fr","memberId":4568,"source":null,"flow":null,"browseHistory":"No log","browseOrigin":"fixture-test","browseIp":"127.0.0.1","browseUserAgent":"fixture-test","browseUserAgentType":"fixture-test-type","browseRequestUri":"/fit/nesse2","creationDate":1331682229271,"exceptionStackTrace":"java.lang.RuntimeException: fixture message\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.send(BrowsingLogGenerate.java:77)\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.main(BrowsingLogGenerate.java:119)\r\n","exceptionType":"java.lang.RuntimeException","exceptionMessage":"fixture message"}}

I launched RabbitMQ River with the command :
curl -XPUT 'localhost:9200/_river/error/_meta' -d '{
      "type" : "rabbitmq",
       "rabbitmq" : {
           "host" : "192.168.200.246", 
           "port" : 5672,
           "user" : "guest",
           "pass" : "guest",
           "vhost" : "integ2",
           "queue" : "LOG_BROWSING",
           "exchange" : "LOG_BROWSING",
           "routing_key" : "#.#.#",
           "exchange_type" : "topic",
           "exchange_durable" : true,
           "queue_durable" : true,
           "queue_auto_delete" : false
       },
       "index" : {
           "bulk_size" : 100,
           "bulk_timeout" : "10ms",
           "ordered" : false,
           "indices.ttl.interval" : "30d"
       }
   }'

I had this message in ES logs

[2012-03-15 15:27:58,481][INFO ][river.rabbitmq           ] [Zzzax] [rabbitmq][my_river] creating rabbitmq river, host [192.168.200.246], port [5672], user [guest], vhost [integ2]
[2012-03-15 15:27:58,643][WARN ][river.rabbitmq           ] [Zzzax] [rabbitmq][my_river] bulkRequestBuilder adding task{"index":{"_index":"error","_type":"browsingLog","_id":"125558794","_ttl":{"enabled":true,"default":"30d"}}}
[2012-03-15 15:27:58,657][WARN ][river.rabbitmq           ] [Zzzax] [rabbitmq][my_river] failed to execute bulk
org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: no requests added;
    at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)
    at org.elasticsearch.action.bulk.BulkRequest.validate(BulkRequest.java:259)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:55)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:141)
    at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:128)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
    at org.elasticsearch.river.rabbitmq.RabbitmqRiver$Consumer.run(RabbitmqRiver.java:278)
    at java.lang.Thread.run(Thread.java:619)

Then I tried to insert into the queue in a single message
curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{\"index\":{\"_index\":\"error\",\"_type\":\"browsingLog\",\"_id\":\"125558794\",\"_ttl\":{\"enabled\":true,\"default\":\"30d\"}}}\r\n{\"browsingLog\":{\"browseRequestMethod\":\"POST\",\"browseRequestParams\":\"param\",\"language\":\"fr\",\"country\":\"fr\",\"memberId\":4568,\"source\":null,\"flow\":null,\"browseHistory\":\"No log\",\"browseOrigin\":\"fixture-test\",\"browseIp\":\"127.0.0.1\",\"browseUserAgent\":\"fixture-test\",\"browseUserAgentType\":\"fixture-test-type\",\"browseRequestUri\":\"/fit/nesse2\",\"creationDate\":1331682229271,\"exceptionStackTrace\":\"java.lang.RuntimeException: fixture message\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.send(BrowsingLogGenerate.java:77)\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.main(BrowsingLogGenerate.java:119)\r\n\",\"exceptionType\":\"java.lang.RuntimeException\",\"exceptionMessage\":\"fixture message\"}}"}'

So I had one message in the queue

Message 1

The server reported 0 messages remaining.
Exchange    LOG_BROWSING
Routing Key     #.#.#
Redelivered     &#9675;
Properties  
content_encoding:   UTF-8
content_type:   application/json
Payload 825 bytes Encoding: string  

{"index":{"_index":"error","_type":"browsingLog","_id":"125558794","_ttl":{"enabled":true,"default":"30d"}}}
{"browsingLog":{"browseRequestMethod":"POST","browseRequestParams":"param","language":"fr","country":"fr","memberId":4568,"source":null,"flow":null,"browseHistory":"No log","browseOrigin":"fixture-test","browseIp":"127.0.0.1","browseUserAgent":"fixture-test","browseUserAgentType":"fixture-test-type","browseRequestUri":"/fit/nesse2","creationDate":1331682229271,"exceptionStackTrace":"java.lang.RuntimeException: fixture message\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.send(BrowsingLogGenerate.java:77)\r\n\tat com.viadeo.fixtures.mq.browsinglogs.BrowsingLogGenerate.main(BrowsingLogGenerate.java:119)\r\n","exceptionType":"java.lang.RuntimeException","exceptionMessage":"fixture message"}}

I launched the same command to create RabbitMQ River and I had this message in ES logs
[2012-03-15 15:43:40,430][INFO ][gateway                  ] [Davis, Leila] recovered [2] indices into cluster_state
[2012-03-15 15:43:40,663][INFO ][river.rabbitmq           ] [Davis, Leila] [rabbitmq][my_river] creating rabbitmq river, host [192.168.200.246], port [5672], user [guest], vhost [integ2]
[2012-03-15 15:43:40,838][WARN ][river.rabbitmq           ] [Davis, Leila] [rabbitmq][my_river] failed to execute bulk
org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: no requests added;
    at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)
    at org.elasticsearch.action.bulk.BulkRequest.validate(BulkRequest.java:259)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:55)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:141)
    at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:128)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
    at org.elasticsearch.river.rabbitmq.RabbitmqRiver$Consumer.run(RabbitmqRiver.java:278)
    at java.lang.Thread.run(Thread.java:619)

I don't understand what is the difference between the _bulk and the _river, I mean why does it work in _bulk mode and not in rabbitMQ river mode.

Thank you for your help !!
</description><key id="3674875">1786</key><summary>RabbitMQ River Validation Failed: 1: no requests added;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PatrickSauts</reporter><labels /><created>2012-03-15T22:49:13Z</created><updated>2012-03-20T18:45:38Z</updated><resolved>2012-03-20T18:45:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="PatrickSauts" created="2012-03-15T23:07:37Z" id="4531079">I tried with the example messages

{ "index" : { "_index" : "twitter", "_type" : "tweet", "_id" : "1" }
{ "tweet" : { "text" : "this is a tweet" } }
{ "delete" : { "_index" : "twitter", "_type" : "tweet", "_id" : "2" } }
{ "create" : { "_index" : "twitter", "_type" : "tweet", "_id" : "1" }
{ "tweet" : { "text" : "another tweet" } }    

curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{ \"index\" : { \"_index\" : \"twitter\", \"_type\" : \"tweet\", \"_id\" : \"1\" }"}'
curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{ \"tweet\" : { \"text\" : \"this is a tweet\" } }"}'
curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{ \"delete\" : { \"_index\" : \"twitter\", \"_type\" : \"tweet\", \"_id\" : \"2\" } }"}'
curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{ \"create\" : { \"_index\" : \"twitter\", \"_type\" : \"tweet\", \"_id\" : \"1\" }"}'
curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST --data-binary '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{ \"tweet\" : { \"text\" : \"another tweet\" } } "}'

And I had the same error

[2012-03-15 16:07:00,811][WARN ][river.rabbitmq           ] [Destroyer of Demons] [rabbitmq][my_river] bulkRequestBuilder adding task{ "tweet" : { "text" : "another tweet" } } 
[2012-03-15 16:07:00,824][WARN ][river.rabbitmq           ] [Destroyer of Demons] [rabbitmq][my_river] failed to execute bulk
org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: no requests added;
    at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)
    at org.elasticsearch.action.bulk.BulkRequest.validate(BulkRequest.java:259)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:55)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)
    at org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:141)
    at org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:128)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
    at org.elasticsearch.river.rabbitmq.RabbitmqRiver$Consumer.run(RabbitmqRiver.java:278)
    at java.lang.Thread.run(Thread.java:619)
</comment><comment author="PatrickSauts" created="2012-03-20T18:45:38Z" id="4602619">I found the problems looking into the code
elasticsearch-river-rabbitmq uses '\n' as a separator.
meaning inside you Json you must remove every '\n' (I replaced those with '&lt;br&gt;') you mustn't use formated Json
after each Json command including the last one you have to have a '\n'

The command 'index' or 'create' must be within a single message
exemple:
{ "index" : { "_index" : "twitter", "_type" : "tweet", "_id" : "1" }
{ "tweet" : { "text" : "this is a tweet" } }

curl -L --show-error http://guest:guest@integ-mq:55672/api/exchanges/integ2/LOG_BROWSING/publish -XPOST -d '{"properties":{"content_type":"application/json","content_encoding":"UTF-8"},"routing_key":"#.#.#","payload_encoding":"string","payload":"{ \"index\" : { \"_index\" : \"twitter\", \"_type\" : \"tweet\", \"_id\" : \"1\" }
{ \"tweet\" : { \"text\" : \"this is a tweet\" } }
"}'
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Faceting field caching issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1785</link><project id="" key="" /><description>I am opening a bug report for the issue described here:

http://groups.google.com/group/elasticsearch/browse_thread/thread/f666209e0095f6e1/cba28b4a143baf15
</description><key id="3664068">1785</key><summary>Faceting field caching issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ctrochalakis</reporter><labels /><created>2012-03-15T10:59:13Z</created><updated>2014-04-25T19:45:46Z</updated><resolved>2014-04-25T19:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ctrochalakis" created="2012-04-02T08:29:49Z" id="4874258">Would this be solved if we add a dynamic template for these fields?
</comment><comment author="Atinux" created="2012-05-30T09:33:14Z" id="6005836">+1
</comment><comment author="spinscale" created="2013-06-26T15:10:53Z" id="20054257">@ctrochalakis yes, you could ensure that every field is mapped as a string (or the same data type) via dynamic mapping (if that is possible due to your data)
</comment><comment author="spinscale" created="2014-04-25T19:45:46Z" id="41432179">closing this due lying around! Please reopen, if you cannot solve this issue by index templates or found other problems. Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearc-lang-* projects have not be pushed to maven repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1784</link><project id="" key="" /><description>https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch-lang-groovy/ as an example. They only go up to 0.18.7. Although, looking in github, it appears the projects have been updated to coincide with 0.19.0.
</description><key id="3651171">1784</key><summary>elasticsearc-lang-* projects have not be pushed to maven repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lucaslward</reporter><labels /><created>2012-03-14T16:17:25Z</created><updated>2012-03-15T16:28:10Z</updated><resolved>2012-03-15T09:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-15T09:04:39Z" id="4516017">All the plugins moved to their own repo, with their own versioning in 0.19. The repo for groovy is https://github.com/elasticsearch/elasticsearch-lang-groovy, and the latest version is `1.1.0`.
</comment><comment author="lucaslward" created="2012-03-15T16:28:10Z" id="4523262">I see it now at:

https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch-lang-groovy/1.1.0/

I saw the note with 19.0 but didn't connect the dots. Sorry for the confusion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix comparison test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1783</link><project id="" key="" /><description>Fortunately, this issues was invisible due to default setting of level value :-)
</description><key id="3647851">1783</key><summary>Fix comparison test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2012-03-14T13:19:26Z</created><updated>2014-07-16T21:55:32Z</updated><resolved>2012-03-15T09:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-15T09:54:20Z" id="4516598">Applied on both 0.19 and master, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Code refactoring] IndicesStats -&gt; IndicesStatsResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1782</link><project id="" key="" /><description>Rename (org.elasticsearch.action.admin.indices.stats) IndicesStats to IndicesStatsResponse.
All classes extending BroadcastOperationResponse follow *Response pattern. May be IndicesStats could too?
</description><key id="3646844">1782</key><summary>[Code refactoring] IndicesStats -&gt; IndicesStatsResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>breaking</label></labels><created>2012-03-14T11:49:31Z</created><updated>2013-02-23T13:23:42Z</updated><resolved>2013-02-23T13:23:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-18T22:49:27Z" id="13747534">maybe we should do this now. We already have a bunch or breaking things going on in master. Thoughts?
</comment><comment author="kimchy" created="2013-02-22T22:52:06Z" id="13977556">@s1monw I think it makes sense, lets wait for @dadoonet to push his last changes the request classes, and then we can make this change as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strange search behaviour when using snowball analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1781</link><project id="" key="" /><description>So let's say I have an index defined like this

```
curl -XPUT 'http://localhost:9200/test' -d '{
  "mappings" : {
      "example" : {
          "properties" : {
              "text" : { "type" : "string", "analyzer" : "snowball" }
          }
      }
  }
}'

curl -XPUT 'http://localhost:9200/test/example/1' -d '{
  "text": "foo bar organization"
}'
```

When I search for "foo organizations" with snowball analyzer, both keywords match as expected:

```
curl -XGET http://localhost:9200/test/example/_search -d '{
  "query": { "text": { "_all": { "query": "foo organizations", "analyzer" : "snowball" } } },
  "highlight": { "fields": { "text": {} } }
}'

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.015912745,"hits":[{"_index":"test","_type":"example","_id":"1","_score":0.015912745, "_source" : {
  "text": "foo bar organization"
},"highlight":{"text":["&lt;em&gt;foo&lt;/em&gt; bar &lt;em&gt;organization&lt;/em&gt;"]}}]}}
```

But when I search for only "organizations" I don't get any result at all which is very weird:

```
curl -XGET http://localhost:9200/test/example/_search -d '{
  "query": { "text": { "_all": { "query": "organizations", "analyzer" : "snowball" } } },
  "highlight": { "fields": { "text": {} } }
}'

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

However, if I search for "bars" it still hits:

```
curl -XGET http://localhost:9200/test/example/_search -d '{
  "query": { "text": { "_all": { "query": "bars", "analyzer" : "snowball" } } },
  "highlight": { "fields": { "text": {} } }
}'

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.10848885,"hits":[{"_index":"test","_type":"example","_id":"1","_score":0.10848885, "_source" : {
  "text": "foo bar organization"
},"highlight":{"text":["foo &lt;em&gt;bar&lt;/em&gt; organization"]}}]}}
```

I guess the difference between "bar" and "organization" is that "organization" is stemmed to "organ" while "bar" is stemmed to itself. But how do I get the proper behaviour so that 2nd search hits?
</description><key id="3646463">1781</key><summary>Strange search behaviour when using snowball analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tycooon</reporter><labels /><created>2012-03-14T11:16:53Z</created><updated>2012-03-17T07:11:08Z</updated><resolved>2012-03-17T07:11:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tycooon" created="2012-03-17T07:11:07Z" id="4552364">Nevermind, found the answer [here](http://stackoverflow.com/questions/9700962/elasticsearch-strange-search-behaviour-when-using-snowball-analyzer).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disable ability to make a doc named /test/test/_query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1780</link><project id="" key="" /><description>I was playing around with delete-by-query, and I found it's possible to make a document with the id `_query`:

```
% curl -XPOST "http://localhost:9200/test/test/_query" -d "{}"
{"ok":true,"_index":"test","_type":"test","_id":"_query","_version":1}
% curl -XGET "http://localhost:9200/test/test/_query"
{"_index":"test","_type":"test","_id":"_query","_version":1,"exists":true, "_source" : {}}
```

Could this be made an error? The only way to get rid of this document is to use delete-by-query:

```
% curl -XDELETE "http://localhost:9200/test/test/_query"
{"error":null}
% curl -XDELETE "http://localhost:9200/test/test/_query" -d '{"ids":{"values":["_query"]}}'
{"ok":true,"_indices":{"test":{"_shards":{"total":5,"successful":5,"failed":0}}}}
% curl -XGET "http://localhost:9200/test/test/_query"
{"_index":"test","_type":"test","_id":"_query","exists":false}
```
</description><key id="3641238">1780</key><summary>disable ability to make a doc named /test/test/_query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickt</reporter><labels /><created>2012-03-14T00:45:27Z</created><updated>2014-07-08T14:29:22Z</updated><resolved>2014-07-08T14:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T14:29:22Z" id="48343579">Closing in favour of #6736 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.common.collect.ImmutableList missing!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1779</link><project id="" key="" /><description>import org.elasticsearch.common.collect.ImmutableList;
is missing from the master source. It is referenced from AnalysisICUPlugin.java (master)

Cannot build the ICU plugin until this is resolved. Could it be that the import should actually be:
import com.google.common.collect.ImmutableList; ?
</description><key id="3611872">1779</key><summary>org.elasticsearch.common.collect.ImmutableList missing!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barsk</reporter><labels /><created>2012-03-12T14:25:01Z</created><updated>2012-03-16T12:49:55Z</updated><resolved>2012-03-16T12:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-14T11:54:53Z" id="4496662">You should open the issue in the relevant plugin repo. But I don't see its missing, I can both build it in the IDE, and run `mvn clean package` successfully.
</comment><comment author="barsk" created="2012-03-16T12:49:55Z" id="4539041">I will repoen the issue in the ICU plugin. If I browse what is in the repos there is no org.elasticsearch.common.collect.ImmutableList.java. At least not in the core repo...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get: Add a specific `get` thread pool that handles get request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1778</link><project id="" key="" /><description>Currently, the `search` thread pool handles it, and it makes sense to divide the two into different executions so one won't block the other in case we have bounded thread pools for search.
</description><key id="3588021">1778</key><summary>Get: Add a specific `get` thread pool that handles get request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-09T19:07:35Z</created><updated>2012-03-09T19:07:58Z</updated><resolved>2012-03-09T19:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add `cache` thread pool to handle cache loading of async caches (bloom filter) </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1777</link><project id="" key="" /><description>Currently, it executes on the generic (cached) thread pool, and we don't want to overload the system and loading many of those concurrently
</description><key id="3587508">1777</key><summary>Add `cache` thread pool to handle cache loading of async caches (bloom filter) </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-09T18:36:35Z</created><updated>2012-03-09T18:48:07Z</updated><resolved>2012-03-09T18:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support "size": 0 for Term Facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1776</link><project id="" key="" /><description>Is it possible to add support for setting "size": 0 to simple Term Facets in order to return all terms matching the hits?

This should be similar to the TermsStats Facet api.
</description><key id="3587078">1776</key><summary>Support "size": 0 for Term Facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ctrochalakis</reporter><labels /><created>2012-03-09T18:09:27Z</created><updated>2014-07-29T14:27:38Z</updated><resolved>2014-01-22T10:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-09T19:21:59Z" id="4422701">Yes, its definitely possible. I plan to do some refactoring in this area, once its done, we can then add this.
</comment><comment author="ctrochalakis" created="2012-03-10T10:35:07Z" id="4430186">Great, keep up the good work :)
</comment><comment author="daetal-us" created="2012-03-10T21:41:04Z" id="4434582">Excellent.

Just wanted to give a big thumb up (two, even!) to this idea.

also, when implemented this might also close #1530
</comment><comment author="TLoD-Snake" created="2012-03-17T00:19:31Z" id="4550723">+1 vote for this feature!
</comment><comment author="jzelez" created="2012-04-12T14:33:57Z" id="5092284">+1
</comment><comment author="edwardsmit" created="2012-05-01T20:13:11Z" id="5446954">+1 vote for this feature
</comment><comment author="michaelirey" created="2012-09-05T18:06:36Z" id="8307996">Loving elasticsearch... I would really benefit from this feature. Any update on the status of this issue?
</comment><comment author="ghost" created="2012-09-05T18:10:03Z" id="8308101">Haven't had a chance to look at the issue, but past experience with earlier lucene and ESP, would suggest you are asking for a 0 to represent a null entry? Is that really what is meant?

On Sep 5, 2012, at 2:06 PM, Michael Irey wrote:

&gt; Loving elasticsearch... I would really benefit from this feature. Any update on the status of this issue?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="michaelirey" created="2012-09-05T18:24:16Z" id="8308553">Really just looking for a way to ask for all facets to be returned... Perhaps omitting the the `size` param or having it set to `all`.  Right now I have it set to a really high number. See: http://stackoverflow.com/questions/12285371/elasticsearch-number-of-facets-returned
</comment><comment author="MrJohnsson77" created="2012-09-23T17:18:28Z" id="8800476">+1
</comment><comment author="jperelli" created="2012-10-02T15:22:01Z" id="9074775">Why is this issue open and documentation says: "The size parameter controls how many facet entries will be returned. It defaults to 10. Setting it to 0 will return all terms matching the hits"?
http://www.elasticsearch.org/guide/reference/api/search/facets/terms-stats-facet.html
What is the truly behavior?
</comment><comment author="tikitu" created="2012-10-09T11:45:49Z" id="9257670">@jperelli the docs you're looking at are for the _terms stats_ facet, not the _terms_ facet: in the terms facet setting `size` to 0 just gives you no terms back.
</comment><comment author="jperelli" created="2012-10-09T13:30:36Z" id="9260418">You are right, in a closer look, I confused this issue with that part of the docs. Sorry!
</comment><comment author="gjb83" created="2012-12-20T23:02:43Z" id="11596203">+1
</comment><comment author="pvulgaris" created="2013-01-04T23:44:22Z" id="11905199">+1
</comment><comment author="kimchy" created="2013-01-04T23:49:43Z" id="11905352">Fellows, just an update, its high on our prio list!, we are going to go through the mentioned refactoring and implement it. Its simpler now with Lucene 4 upgrade effectively done.
</comment><comment author="ZogStriP" created="2013-01-16T17:51:57Z" id="12331013">:+1: 
</comment><comment author="tfreitas" created="2013-01-29T14:31:58Z" id="12836927">+1
</comment><comment author="pjrt" created="2013-03-03T21:16:24Z" id="14355555">+!
</comment><comment author="fmardini" created="2013-03-14T08:09:36Z" id="14890735">+1
</comment><comment author="pjrt" created="2013-03-15T16:16:07Z" id="14969881">Isn't this already implemented with the "all_terms" option? 
</comment><comment author="tikitu" created="2013-03-20T16:31:33Z" id="15186814">@pjrt: Nope. The "all_terms" option says "also include in the list terms that appear somewhere in the index for this field but that have no hits in this query" (i.e., entries with a count of zero). But the "size" parameter says "how many items from the top of the list should I actually return?"

On 15 mrt. 2013, at 18:16, Pedro Rodriguez notifications@github.com wrote:

&gt; Isn't this already implemented with the "all_terms" option?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="nickstreet" created="2013-08-08T15:27:34Z" id="22331562">+1 :thumbsup: 
</comment><comment author="trenpixster" created="2013-09-09T15:31:57Z" id="24086548">+1
</comment><comment author="quentin389" created="2013-09-11T10:47:53Z" id="24230356">+1
</comment><comment author="joafeldmann" created="2013-09-30T15:52:27Z" id="25375365">Since there is a "size" field, is it possible to also have a "from" field to paginate through facets?
</comment><comment author="taijinlee" created="2013-10-14T18:20:14Z" id="26276826">+1
</comment><comment author="JohanTan" created="2013-11-08T00:18:01Z" id="28020435">+1
</comment><comment author="mbedna" created="2013-11-19T15:39:41Z" id="28800010">+1
</comment><comment author="bobrik" created="2013-12-02T08:29:16Z" id="29601191">Any updates? @kimchy said about priority list 11 months ago :)

I noticed weird thing today: `all_terms` with `terms_stats` returns all terms, but with `terms` facet you should specify size explicitly.

I guess this is not going to happen, aggregations are coming.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for issue 1774</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1775</link><project id="" key="" /><description>See:
https://gist.github.com/2006593
</description><key id="3583265">1775</key><summary>Fix for issue 1774</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barnybug</reporter><labels /><created>2012-03-09T13:54:35Z</created><updated>2014-07-16T21:55:33Z</updated><resolved>2012-03-09T19:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-09T19:21:11Z" id="4422689">Applied, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IOException when restricting fields and returning nested arrays.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1774</link><project id="" key="" /><description>See:
https://gist.github.com/2006593

pull request coming
</description><key id="3583262">1774</key><summary>IOException when restricting fields and returning nested arrays.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barnybug</reporter><labels><label>bug</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-09T13:54:20Z</created><updated>2012-03-09T19:20:57Z</updated><resolved>2012-03-09T19:20:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-09T19:20:57Z" id="4422686">Pulled, from #1775, thanks!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mlt_field doesn't support boost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1773</link><project id="" key="" /><description>In 0.19 I'm getting an error saying that the mlt_field query doesn't support the `boost` param, eg:

```
curl -XGET 'http://127.0.0.1:9200/iannounce_object/_search?pretty=1'  -d '
{
   "query" : {
      "mlt_field" : {
         "name_keywords" : {
            "max_query_terms" : "100",
            "min_doc_freq" : "1",
            "boost" : 3,
            "like_text" : "Joe Bloggs",
            "min_term_freq" : "1",
            "percent_terms_to_match" : 0.3
         }
      }
   }
}
'
```

But according to the docs, it does: http://www.elasticsearch.org/guide/reference/query-dsl/mlt-query.html

Which is correct?
</description><key id="3581047">1773</key><summary>mlt_field doesn't support boost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-09T10:32:36Z</created><updated>2012-03-10T18:50:40Z</updated><resolved>2012-03-10T18:50:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-03-09T10:34:24Z" id="4414119">Same goes for the `mlt` query
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Closing a shard can cause a search/stats request that tries to acquire a handle to search while its closing to spin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1772</link><project id="" key="" /><description /><key id="3574802">1772</key><summary>Closing a shard can cause a search/stats request that tries to acquire a handle to search while its closing to spin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-08T23:56:21Z</created><updated>2012-03-09T00:00:53Z</updated><resolved>2012-03-09T00:00:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Blocks: Add index.blocks.write, index.blocks.read, and index.blocks.metadata settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1771</link><project id="" key="" /><description>Allow to specifically block (on an index level) write operations, read operations and metadata operations. Setting `index.blocks.write` to `true` will block write operations, setting it back to `false` will allow them. The settings can be dynamically set using the index update settings API.
</description><key id="3570887">1771</key><summary>Index Blocks: Add index.blocks.write, index.blocks.read, and index.blocks.metadata settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-08T19:54:28Z</created><updated>2012-03-08T19:56:21Z</updated><resolved>2012-03-08T19:56:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RemoteTransportException on _analyze request to specific index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1770</link><project id="" key="" /><description>When routing an `_analyze` request to a specific index, we'll see a `RemoteTransportException` intermittently:

```
{
  "error" : "RemoteTransportException[[Wiccan][inet[/10.60.121.149:9301]][indices/analyze/s]]; nested: IOException[Expected handle header, got [101]]; ",
  "status" : 500
}
```

We were able to reproduce with the following cluster characteristics:
- `index.number_of_shards` set to 1
- At least three nodes in the cluster

Sending the analyze request to all three nodes, at least one of them will fail.

Here's a full reduction ([download](https://raw.github.com/gist/2001546/9fe1435ec53ea298eb83d3e9d40d290427e1c537/analyzer-error.sh)):

``` bash
pkill -f $(pwd)
for i in {0..2}
do
    bin/elasticsearch
done
echo -n "waiting for cluster to come up"
until curl -s localhost:9200/_cluster/health?wait_for_status=green &gt; /dev/null
do
    echo -n '.'
    sleep 1
done
echo ''

curl -XDELETE localhost:9200?pretty=true
echo ''
curl -XPUT localhost:9200/test?pretty=true -d '{"index":{"number_of_shards":1}}'
echo ''
sleep 5
for i in {0..2}
do
    curl "localhost:920$i/test/_analyze?pretty=true&amp;text=test"
    echo ''
done
```
</description><key id="3564683">1770</key><summary>RemoteTransportException on _analyze request to specific index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels><label>bug</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-08T15:45:50Z</created><updated>2012-03-08T19:11:11Z</updated><resolved>2012-03-08T19:11:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>expose the classic analyzer and the classic tokenizer (issue #1768)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1769</link><project id="" key="" /><description /><key id="3563914">1769</key><summary>expose the classic analyzer and the classic tokenizer (issue #1768)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-03-08T14:58:50Z</created><updated>2014-06-18T07:46:41Z</updated><resolved>2012-03-08T20:03:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose the classic analyzer and tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1768</link><project id="" key="" /><description>The standard implementation have changed with Lucene 3.1. The old implementation is still available but under the name ClassicAnalyzer
</description><key id="3563878">1768</key><summary>Expose the classic analyzer and tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nlalevee</reporter><labels /><created>2012-03-08T14:56:42Z</created><updated>2013-05-27T16:39:12Z</updated><resolved>2013-05-27T16:39:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Analyze API not handling text in post body correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1767</link><project id="" key="" /><description>Compare query string request:

```
curl -XGET 'http://127.0.0.1:9200/_analyze?pretty=1&amp;text=hello_world12&amp;analyzer=simple' 

# [Thu Mar  8 09:02:58 2012] Response:
# {
#    "tokens" : [
#       {
#          "end_offset" : 5,
#          "position" : 1,
#          "start_offset" : 0,
#          "type" : "word",
#          "token" : "hello"
#       },
#       {
#          "end_offset" : 11,
#          "position" : 2,
#          "start_offset" : 6,
#          "type" : "word",
#          "token" : "world"
#       }
#    ]
# }
```

with body request (incorrect):

```
curl -XGET 'http://localhost:9200/_analyze?analzyer=simple&amp;pretty=1' -d 'hello_world12'
{
  "tokens" : [ {
    "token" : "hello_world12",
    "start_offset" : 0,
    "end_offset" : 13,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  } ]
}
```
</description><key id="3557441">1767</key><summary>Analyze API not handling text in post body correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-03-08T08:05:53Z</created><updated>2012-03-08T22:21:52Z</updated><resolved>2012-03-08T22:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-08T19:27:05Z" id="4400319">:), the second curl, with the body sample, does not spell `analyzer` correctly...
</comment><comment author="clintongormley" created="2012-03-08T21:21:37Z" id="4402901">doh!  i copied that straight from the initial email :)
</comment><comment author="kimchy" created="2012-03-08T22:21:52Z" id="4404142">I'll close it then...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Queries break words on underscore regardless of analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1766</link><project id="" key="" /><description>See https://gist.github.com/1995783

I'd expect with using the simple or keyword analyzer, that underscores would not be a term delimiter. The _analyze endpoint agrees, but when "hello_world" is used as a search query, the query explanation shows separate scoring on terms hello and world.
</description><key id="3549975">1766</key><summary>Queries break words on underscore regardless of analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels /><created>2012-03-07T20:28:31Z</created><updated>2012-03-08T20:15:23Z</updated><resolved>2012-03-08T08:06:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-03-08T08:06:37Z" id="4386450">Reopened as issue #1767
</comment><comment author="clintongormley" created="2012-03-08T08:09:23Z" id="4386483">Hi James

On Wed 07 Mar 2012 21:28:31 CET, James Wilson wrote:

&gt; See https://gist.github.com/1995783
&gt; 
&gt; I'd expect with using the simple or keyword analyzer, that underscores would be not a term delimiter. The _analyze endpoint agrees, but when "hello_world" is used as a search query, the query explanation shows separate scoring on terms hello and world.

The thing that is confusing you is the result of the analyze call.  
Unfortunately, it turns out that there is a bug in the analyze API when 
passing the text in the request body instead of the query string.  See 
https://github.com/elasticsearch/elasticsearch/issues/1767

The 'simple' analyzer strips out underscores, which is why you're not 
seeing the expected results.

Try using the keyword analyzer instead, or create custom analyzer by 
combining a tokenizer and multiple token filters which do exactly what 
you are after: 
http://www.elasticsearch.org/guide/reference/index-modules/analysis/

clint
</comment><comment author="jmwilson" created="2012-03-08T20:15:23Z" id="4401267">Awesome, looks like keyword is what I want.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Init script in Debian package ignores some options in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1765</link><project id="" key="" /><description>In the line&#8230;

```
DAEMON_OPTS="-p $PID_FILE -Des.config=$CONF_FILE -Des.path.home=$ES_HOME -Des.path.logs=$LOG_DIR -Des.path.data=$DATA_DIR -Des.path.work=$WORK_DIR -Des.path.conf=$CONF_DIR"
```

&#8230;in `/etc/init.d/elasticsearch`, several options, like es.path.data, get paved over by the defaults from `/etc/default/elasticsearch` or by env vars. Any `path.data` option in `elasticsearch.yml` is ignored.
</description><key id="3537685">1765</key><summary>Init script in Debian package ignores some options in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikrose</reporter><labels><label>enhancement</label><label>v0.19.5</label><label>v0.20.0.RC1</label></labels><created>2012-03-07T04:24:55Z</created><updated>2012-06-11T11:52:21Z</updated><resolved>2012-06-11T11:50:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-07T21:36:41Z" id="4379241">Yes, that happens because the parameters provided in the command line override the ones in the config file (they have precedence). Why not change it using the relevant env vars?
</comment><comment author="erikrose" created="2012-03-07T22:43:41Z" id="4380592">&gt; Yes, that happens because the parameters provided in the command line override the ones in the config file (they have precedence). Why not change it using the relevant env vars?

It's weird that things like the data path are _never_ obeyed in the config file.

Also, there's not really an opportunity to set env vars before the init.d script runs.
</comment><comment author="pierrre" created="2012-06-11T09:57:17Z" id="6240770">+1 this behavior is really weird
I tried to change the data directory in elasticsearch.yml, and it didn't work...
You should remove parameters in /etc/init.d/elasticsearch
</comment><comment author="kimchy" created="2012-06-11T11:48:13Z" id="6242384">I added #2016, which will allow to solve this problem, will push a fix for this soon.
</comment><comment author="pierrre" created="2012-06-11T11:52:21Z" id="6242448">Thank you! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature request] Highlighting with parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1764</link><project id="" key="" /><description>docID and the highlighted part of the matched document needs to be given back.

&lt;h2&gt;Requirement&lt;/h2&gt;

On the query side, we would specify we want the children to be returned along with their parent data, and optionally some highlighting on the children fields.

```
{
    "query": {
        "top_children" : {
            "type": "subcontent",
            "query" : {
                "term" : {
                    "name" : "bike"
                }
            }
            "score" : "max",
            "factor" : 5,
            "incremental_factor" : 2
        }
    },
    "children": {  // just having this tags means we at least want the ids of the children
        "size" : 2, // maximum number of children by parent to return
        "full_data" : false, // if true return not just the id of the child but also its data
        "highlight" : { // same syntax as for normal queries
            "fields" : {
                "name" : {}
            }
        }
    }
}
```

Then as results we would have something like:

```
"hits" : {
    "total" : 7,
    "max_score" : 3.366573
    "hits" : [
        {
            "_index" : "es",
            "_type" : "twitter",
            "_id" : 8001,
            "_score" : 3.366573,
            "children": {
                "total" : 4,
                "hits" : [ // hits on children of this current parent, ordered by score
                    {
                        "_id" : 654,
                        "highlight" : {
                           "name" : [ "my lovely &lt;em&gt;bike&lt;/em&gt;" ]
                         }
                    },
                    {
                        "_id" : 987,
                        "highlight" : {
                           "name" : [ "my nice &lt;em&gt;bike&lt;/em&gt;" ]
                         }
                    }
               ]
            }
        },
        {
            "_index" : "es",
            "_type" : "twitter",
            "_id" : 8004,
.... etc....
}
```

Efforts in this direction is documented here - http://elasticsearch-users.115913.n3.nabble.com/Highlighting-with-parent-child-queries-td3275351.html
</description><key id="3532863">1764</key><summary>[Feature request] Highlighting with parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>:Highlighting</label><label>discuss</label></labels><created>2012-03-06T21:12:57Z</created><updated>2014-07-18T09:05:51Z</updated><resolved>2014-07-18T09:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="srijiths" created="2012-03-07T04:11:37Z" id="4362027">Very usefull feature
</comment><comment author="siminoushad" created="2012-03-07T04:14:24Z" id="4362055">i also feel this feature quite usefull . +1
</comment><comment author="shyam0586" created="2012-03-07T04:19:08Z" id="4362101">I also love to see this one in action . +1
</comment><comment author="febintt" created="2012-03-07T04:26:06Z" id="4362169">This is one killer feature . +1
</comment><comment author="Vineeth-Mohan" created="2012-06-25T19:50:47Z" id="6557756">@kimchy  - Is there update to this issue. Do you think this is feasible in any close future ?
</comment><comment author="clintongormley" created="2014-07-18T09:05:51Z" id="49410198">This will be possible with the `top_hits` aggregation once parent-child aggs are supported.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Allow to disable dynamic script execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1763</link><project id="" key="" /><description>Allow to set `script.disable_dynamic` to `true`, which will disable dynamic scripting execution. It will still allow execution of scripts provided through the config location, or "native" Java scripts registered through plugins.
</description><key id="3530520">1763</key><summary>Scripting: Allow to disable dynamic script execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-06T20:02:33Z</created><updated>2012-03-06T20:03:11Z</updated><resolved>2012-03-06T20:03:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Can't remap index alias pointing to read-only index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1762</link><project id="" key="" /><description>See reduction ([download](https://raw.github.com/gist/1987485/f658b25f70d51bc1736c2741697719dd5e1e8809/remap-readonly.sh)):

```
curl -XDELETE http://localhost:9200/test1?pretty=true
echo ''
curl -XDELETE http://localhost:9200/test2?pretty=true
echo ''
curl -XPUT http://localhost:9200/test1?pretty=true -d '{}'
echo ''
curl -XPOST http://localhost:9200/_aliases?pretty=true -d '{"actions":[{"add":{"index":"test1","alias":"test"}}]}'
echo ''
curl -XPUT http://localhost:9200/test2?pretty=true -d '{}'
echo ''
curl -XPUT http://localhost:9200/test1/_settings?pretty=true -d '{"index":{"blocks":{"read_only":true}}}'
echo ''
curl -XPOST http://localhost:9200/_aliases?pretty=true -d '{"actions":[{"remove":{"index":"test1","alias":"test"}},{"add":{"index":"test2","alias":"test"}}]}'
echo ''
```
</description><key id="3527474">1762</key><summary>Can't remap index alias pointing to read-only index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels /><created>2012-03-06T17:00:21Z</created><updated>2012-03-08T19:57:40Z</updated><resolved>2012-03-08T19:57:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-07T21:52:49Z" id="4379522">Mmm, the problem here is that read_only was defined as not being allowed to "write" or do "metadata" changes to the index. Adding an alias constitutes metadata change... . It makes sense (though it could have easily been defined the to only disallow writes), and I am reluctant to change it now.

We can potentially add another option, like disable writes, but it feels like its going to be confusion. Not sure what the best answer for this is currently...
</comment><comment author="outoftime" created="2012-03-07T22:02:28Z" id="4379727">Gotcha. I also noticed that doing e.g. a `DELETE /` fails if any of your indexes are read-only. This makes sense given your definition of "readonly", but for what it's worth, I think a "disable writes" would be much more useful than the read-only functionality as currently defined...
</comment><comment author="kimchy" created="2012-03-08T19:57:40Z" id="4400902">Opened #1771, which will resolve this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Installing the couchdb-river plugin does not work in 0.19.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1761</link><project id="" key="" /><description>Running using the .deb package gives me:

```
sudo /usr/share/elasticsearch/bin/plugin  -install river-couchdb
-&gt; Installing river-couchdb...
Trying http://elasticsearch.googlecode.com/svn/plugins/river-couchdb/elasticsearch-river-couchdb-0.19.0.zip...
Failed to install river-couchdb, reason: failed to download
```

There is no zip for 0.19.0 on http://elasticsearch.googlecode.com/svn/plugins/river-couchdb/
</description><key id="3515826">1761</key><summary>Installing the couchdb-river plugin does not work in 0.19.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cortex</reporter><labels /><created>2012-03-05T23:02:55Z</created><updated>2012-12-14T17:21:38Z</updated><resolved>2012-03-20T00:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="holdenk" created="2012-03-06T06:19:05Z" id="4338793">Try running bin/plugin -install elasticsearch/elasticsearch-river-couchdb/1.1.0 in 0.19.0 you have to specify a version.
</comment><comment author="cortex" created="2012-03-20T00:20:31Z" id="4586789">Thanks, works just fine!
</comment><comment author="e-d" created="2012-10-19T13:36:06Z" id="9601181">Perfect, thanks!
</comment><comment author="narendrachoudhary" created="2012-10-31T01:33:06Z" id="9929535">when i have tried the command  it is  failing to install ... i have installed the verison 0.19.1

 ./bin/plugin -install  elasticsearch/elasticsearch-river-couchdb/0.19.1
-&gt; Installing elasticsearch/elasticsearch-river-couchdb/0.19.1...
Trying https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/elasticsearch-river-couchdb-0.19.1.zip...
Trying https://github.com/elasticsearch/elasticsearch-river-couchdb/zipball/v0.19.1...
Failed to install elasticsearch/elasticsearch-river-couchdb/0.19.1, reason: failed to download
</comment><comment author="dadoonet" created="2012-10-31T03:55:42Z" id="9932111">See README: https://github.com/elasticsearch/elasticsearch-river-couchdb

&gt; In order to install the plugin, simply run: bin/plugin -install elasticsearch/elasticsearch-river-couchdb/1.1.0
</comment><comment author="narendrachoudhary" created="2012-10-31T04:31:35Z" id="9932610">I have tried your suggestion earlier but it gives me the same problem ... 
bin/plugin -install elasticsearch/elasticsearch-river-couchdb/1.1.0
-&gt; Installing elasticsearch/elasticsearch-river-couchdb/1.1.0...
Trying https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/elasticsearch-river-couchdb-1.1.0.zip...
Trying https://github.com/elasticsearch/elasticsearch-river-couchdb/zipball/v1.1.0...
Failed to install elasticsearch/elasticsearch-river-couchdb/1.1.0, reason: failed to download

bin/plugin is the directory of my elasticsearch which is installed under /usr/share/elasticsearch.
</comment><comment author="narendrachoudhary" created="2012-10-31T04:32:24Z" id="9932625">and more over when i tried to open the https://github.com/elasticsearch/elasticsearch-river-couchdb/zipball/v1.1.0.. this is giving me the 404 error on the browser 
</comment><comment author="dadoonet" created="2012-11-04T21:39:09Z" id="10055792">I suspect something wrong with your internet access a temporary issue with github website (outage due to Sandy storm ?).
I tested it just now and it worked fine:

Tested on 0.19.4 under windows.

```
C:\Users\David\Desktop\ZPDemo\elasticsearch-0.19.4\bin&gt;plugin -install elasticsearch/elasticsearch-river-couchdb/1.1.0
-&gt; Installing elasticsearch/elasticsearch-river-couchdb/1.1.0...
Trying https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/elasticsearch-river-couchdb-1.1.0.zip...
Downloading .....DONE
Installed river-couchdb
```
</comment><comment author="Ganican" created="2012-12-13T18:43:09Z" id="11347098">Hi dadoonet, I get the same error as narendrachoudhary. Check the following

****_with 1.1.0**_****************
C:\Program Files\elasticsearch\bin&gt;plugin -install elasticsearch/elasticsearch-r
iver-couchdb/1.1.0
-&gt; Installing elasticsearch/elasticsearch-river-couchdb/1.1.0...
Trying https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/el
asticsearch-river-couchdb-1.1.0.zip...
Trying https://github.com/elasticsearch/elasticsearch-river-couchdb/zipball/v1.1
.0...
Failed to install elasticsearch/elasticsearch-river-couchdb/1.1.0, reason: faile
d to download

****_with 0.19.0**_****************
The same error..

Right now I installed elasticsearch 0.19.0; But I see the same error for elasticsearch 0.20.1
</comment><comment author="dadoonet" created="2012-12-13T21:36:06Z" id="11354505">I just checked it now and it works fine for under windows using ES 0.19.4

```
C:\Dev\elasticsearch\elasticsearch-0.19.4\bin&gt;plugin -install elasticsearch/elasticsearch-river-couchdb/1.1.0
-&gt; Installing elasticsearch/elasticsearch-river-couchdb/1.1.0...
Trying https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/elasticsearch-river-couchdb-1.1.0.zip...
Downloading ....DONE
Installed river-couchdb
```

Double check that you are able to download https://github.com/downloads/elasticsearch/elasticsearch-river-couchdb/elasticsearch-river-couchdb-1.1.0.zip from your browser.
If not, check your internet connection (are you behind a proxy that does not support https connections?)
</comment><comment author="Ganican" created="2012-12-13T22:10:39Z" id="11355893">Thanks for the response.

I am able to download zip from the url but getting the same error with install..

I unzipped the file and kept the jar file under "plugins" directory

Then tried create a index ...but it seem to be not working

Here is the code with which  I created index
curl -XPUT "localhost:9200/_river/tasks/_meta" -d "{
\"type\" : \"couchdb\",
\"couchdb\" : {
    \"host\" : \"localhost\",
    \"port\" : 5984,
    \"db\" : \"task_list\",
    \"filter\" : null
}
}"

But its giving an "IndexMissingException" when I try to access it.

How to I check if "river-couchdb" is working?
</comment><comment author="narendrachoudhary" created="2012-12-14T03:40:52Z" id="11364173">Hi ,

Try to download the head plugin for the Elastic search which give you 
the GUI admin for the river and others.

Thanks
Narendra

On 12/14/2012 03:40 AM, Ganiraju Manyam wrote:

&gt; Thanks for the response.
&gt; 
&gt; I am able to download zip from the url but getting the same error with 
&gt; install..
&gt; 
&gt; I unzipped the file and kept the jar file under "plugins" directory
&gt; 
&gt; Then tried create a index ...but it seem to be not working
&gt; 
&gt; Here is the code with which I created index
&gt; curl -XPUT "localhost:9200/_river/tasks/_meta" -d "{
&gt; \"type\" : \"couchdb\",
&gt; \"couchdb\" : {
&gt; \"host\" : \"localhost\",
&gt; \"port\" : 5984,
&gt; \"db\" : \"task_list\",
&gt; \"filter\" : null
&gt; }
&gt; }"
&gt; 
&gt; But its giving an "IndexMissingException" when I try to access it.
&gt; 
&gt; How to I check if "river-couchdb" is working?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1761#issuecomment-11355893. 
</comment><comment author="Ganican" created="2012-12-14T16:15:29Z" id="11381697">Narendra, 

I ham getting the same issue with head plugin, 

C:\Program Files\elasticsearch\bin&gt;plugin -install Aconex/elasticsearch-head
-&gt; Installing Aconex/elasticsearch-head...
Trying https://github.com/downloads/Aconex/elasticsearch-head/elasticsearch-head
-0.20.1.zip...
Trying https://github.com/Aconex/elasticsearch-head/zipball/v0.20.1...
Trying https://github.com/Aconex/elasticsearch-head/zipball/master...
Failed to install Aconex/elasticsearch-head, reason: failed to download

Even this one didn't work. 

plugin -install mobz/elasticsearch-head

I am trying to install it on windows7. I also see the url it is trying to download from is also not working.
</comment><comment author="dadoonet" created="2012-12-14T16:48:07Z" id="11382967">That means that you have a network issue. Are you behind a proxy ?
</comment><comment author="Ganican" created="2012-12-14T17:03:21Z" id="11383517">I am not sure whats happening with the head. But for river-couchdb I am able download the zip.

How I can manually setup the river-couchdb plugin?
</comment><comment author="dadoonet" created="2012-12-14T17:10:46Z" id="11383809">&gt; I am not sure whats happening with the head. But for river-couchdb I am able download the zip.

It happens the same problem. You probably are behind a firewall/proxy (but you didn't answer to this). Your java is not setup to use the proxy so it doesn't work in command line.

&gt; How I can manually setup the river-couchdb plugin?

Unzip the zip file in `elasticsearch/plugins/river-couchdb`.
Restart your node and look at logs. You should find the couchdb river loaded as a plugin.
</comment><comment author="Ganican" created="2012-12-14T17:21:38Z" id="11384224">I think it is initialized now...
[2012-12-14 11:14:43,427][INFO ][org.elasticsearch.node   ] [Corbo, Jared] {0.20.1}[7776]: initializing ...
[2012-12-14 11:14:43,454][INFO ][org.elasticsearch.plugins] [Corbo, Jared] loaded [river-couchdb], sites []
[2012-12-14 11:14:46,577][INFO ][org.elasticsearch.node   ] [Corbo, Jared] {0.20.1}[7776]: initialized

Even for the head plugin can I do the same ..that is

Unzip the zip file in elasticsearch/plugins/head

Thanks for your help.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Maven javadoc jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1760</link><project id="" key="" /><description>As seen here : https://groups.google.com/forum/?hl=fr&amp;fromgroups#!topic/elasticsearch/m0RjOrnwUB0

We can add javadoc to be deployed as a jar in maven repositories.

HTH
David.
</description><key id="3513157">1760</key><summary>Add Maven javadoc jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-03-05T20:26:02Z</created><updated>2015-03-12T21:25:57Z</updated><resolved>2013-05-28T07:28:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-09-25T11:47:48Z" id="8851309">I'm wondering if we should close this one?
@kimchy What do you think?
</comment><comment author="yeroc" created="2014-03-17T19:16:39Z" id="37858106">I'm curious why this ticket has been closed.  As far as I can tell Javadocs are still missing from the public maven repository.  Though, ironically, for versions 0.18.7 and prior they are available.  Can we get this re-opened?  Or should I open a separate ticket?
</comment><comment author="barmic" created="2015-03-11T16:44:19Z" id="78302928">You can find JavaDoc here : http://javadoc.kyubu.de/elasticsearch/HEAD/index.html
</comment><comment author="yeroc" created="2015-03-12T21:25:34Z" id="78625400">@barmic, @dadoonet  This ticket is specifically about adding a Maven javadoc jar.  This allows anyone using Maven or similar to add ElasticSearch as a dependency on their project and then (with appropriate IDE integration) immediately see Javadoc for the Java API right within their IDE.  Having an external link to a website which is hosting Javadoc is not the same thing.  Note that up until the v0.18.7 release a Javadoc jar was published along with the other build artifacts in the public Maven repository.  Since that release Javadoc hasn't been published.  The trivial patch attached to this ticket resolves this oversight.  It's not clear to me why this fix hasn't been applied. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix wrong test validating index templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1759</link><project id="" key="" /><description /><key id="3509665">1759</key><summary>fix wrong test validating index templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-03-05T16:58:45Z</created><updated>2014-06-13T06:29:37Z</updated><resolved>2012-03-07T22:00:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-07T21:55:30Z" id="4379596">I think there is no reason that template has to be lowercase, I will jsut fix the message to say it relates to name.
</comment><comment author="Paikan" created="2012-03-07T22:00:27Z" id="4379688">Sure! I am closing this now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1758</link><project id="" key="" /><description>Would be nice if the documentation would be available per released version.
Right now I'm working with 0.18.7 (had 0.18.4 last week) and it's not very clear from the documentation if something works on the version I'm working with.
</description><key id="3508447">1758</key><summary>Documentation versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hvdklauw</reporter><labels /><created>2012-03-05T15:51:57Z</created><updated>2013-07-05T09:46:07Z</updated><resolved>2013-07-05T09:46:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2012-03-05T16:13:45Z" id="4325702">For now you can try checkout documentation based on git commit id and run locally. It is not hard, it is using jekyll. Let me know if you are interested in details.
</comment><comment author="spinscale" created="2013-07-05T09:46:07Z" id="20509594">In the long term we try to have versioned documentation (you simply need the same repo for source and documentation, right?). For now, we try to document if a certain feature is only available with a certain version.

Feel free to drop us a note in the elasticsearch documentation repo at https://github.com/elasticsearch/elasticsearch.github.com if the documentation does not mention a needed version explicetely.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add script_fields to document GET requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1757</link><project id="" key="" /><description>Per [this list thread](https://groups.google.com/d/topic/elasticsearch/mWAAlaokOx4/discussion), it would be useful to add the same script_fields functionality to individual document GET requests as is now available for search.
</description><key id="3498651">1757</key><summary>Add script_fields to document GET requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apendleton</reporter><labels><label>adoptme</label></labels><created>2012-03-04T20:42:59Z</created><updated>2014-11-09T11:11:50Z</updated><resolved>2014-11-09T11:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-09T11:11:50Z" id="62299638">It seems that the original use case for this request was to retrieve just part of the `_source` field, which can now be done with the `_source` parameter.

The `script_fields` parameter would have to be passed in the body, but a document GET request doesn't accept a body, making this change rather drastic.

Given that the original requirement is now served by the `_source` param, I'm going to close this issue. Please feel free to reopen if you come up with another use case.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort on _id not working?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1756</link><project id="" key="" /><description>Just switched to 0.19.0 and having some odd behavior.

Using java API:

&gt; SortBuilders.fieldSort("_id").order(SortOrder.ASC)

produces the following json:
"sort" : [ {
    "_id" : {
      "order" : "asc"
    }
  } ]

But result has field

&gt; "sort" : [ null ],

anything wrong with new ES or is it me?
</description><key id="3493741">1756</key><summary>Sort on _id not working?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">be-my-e</reporter><labels /><created>2012-03-04T00:56:25Z</created><updated>2013-02-04T06:36:18Z</updated><resolved>2012-03-04T01:15:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="be-my-e" created="2012-03-04T01:15:38Z" id="4306684">ok, it's _uid and not _id.
Somehow this is missleading if you look at the data stored inside ES.

Thanks anyway.
</comment><comment author="cao7113" created="2013-02-04T06:36:18Z" id="13064004">Doesn't work in ES 0.20.2, Why?

What's the difference between _id and _uid?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings defined at index creation time in YAML are ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1755</link><project id="" key="" /><description>According to documentation I would have assumed that creation of mappings work in YAML too (since the first example on that page uses it):
http://www.elasticsearch.org/guide/reference/api/admin-indices-create-index.html

In JSON, it works as expected:
```curl -XPUT $SERVER/$INDEX/ -d '
{
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
    },
    "content": {  
        "properties": {  
             "exact": {
                "index": "not_analyzed",
                "type": "string"
            }
        }
    }
}

```

But, the following doesn't work:
```

curl -XPUT $SERVER/$INDEX/ -d '
settings:
        number_of_shards: 1
        number_of_replicas: 0
mappings:
    content:
        properties:
            exact:
                index: not_analyzed
                type: string
'

```

Below is my indexing and searching setup:
```

curl -XPUT $SERVER/$INDEX/content/1 -d '  
{"exact": "The Field", "title": "one" }  
'  
curl -XPUT $SERVER/$INDEX/content/2 -d '  
{"exact": "Astro Nomical", "title": "The second title" }  
'                                                                                         

curl -XPOST $SERVER/$INDEX/_refresh                                                       

curl -XPOST $SERVER/$INDEX/content/_search -d '  
{  
  "query": {  
    "filtered": {  
      "filter": {  
        "term": {  
          "exact": "The Field"  
        }  
      },  
      "query": {  
        "match_all": {}  
      } 
    }  
  }  
} 
'

```

tested with 0.18 and 0.19.
Sorry for formatting. Github Markdown seems broken.
```
</description><key id="3488041">1755</key><summary>Mappings defined at index creation time in YAML are ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yannk</reporter><labels /><created>2012-03-03T06:12:16Z</created><updated>2017-04-14T06:43:24Z</updated><resolved>2014-07-08T14:25:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-03-03T22:12:16Z" id="4305157">Not for the create index API, in this case, ti is expected to be in json...
</comment><comment author="kimchy" created="2012-03-03T22:13:01Z" id="4305165">Sorry, I meant when you provide both settings and mappings, it is expected to be provided in json, not `yml`. Only when you create just with settings, it supports `yml`. I will fix the docs.
</comment><comment author="yannk" created="2012-03-03T22:22:55Z" id="4305221">Thanks! I'm sure documenting this will be useful to others.
</comment><comment author="swackhamer" created="2017-04-14T06:43:24Z" id="294101343">Is this still the case?  The documentation is still unclear: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Special host config notation `_[interfaceName]_` might fail to resolve the host address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1754</link><project id="" key="" /><description /><key id="3477837">1754</key><summary>Special host config notation `_[interfaceName]_` might fail to resolve the host address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.1</label><label>v0.20.0.RC1</label></labels><created>2012-03-02T14:58:36Z</created><updated>2012-03-02T14:59:01Z</updated><resolved>2012-03-02T14:59:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Highlight percolator results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1753</link><project id="" key="" /><description>Currently the percolator does not return highlight information on the words which were matched in the document. This would be a useful feature addition.
</description><key id="3471844">1753</key><summary>Highlight percolator results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brett--anderson</reporter><labels /><created>2012-03-02T04:20:30Z</created><updated>2013-08-26T14:39:27Z</updated><resolved>2013-08-26T14:39:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-26T14:39:27Z" id="23266790">This has just been added via #3574
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Grouping in Elastic search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1752</link><project id="" key="" /><description>A few months ago a integrated the Lucene grouping module into ES. It is not a perfect solution, but at least allows users to group their search results in a non sharded environment. I don't expect this code to be added to ES, but I just want to let you guys know about this. Let me know what you think. I updated the code to what was yesterday the latest commit in the master branch. I would be great if some day ES would have grouping too!
</description><key id="3454732">1752</key><summary>Grouping in Elastic search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2012-03-01T14:17:27Z</created><updated>2014-06-26T10:32:44Z</updated><resolved>2012-09-06T13:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gustavobmaia" created="2012-04-13T13:08:31Z" id="5114144">Hi,
Some idea when it will go to ES master ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi level parent/child mapping and search fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1751</link><project id="" key="" /><description>https://gist.github.com/1941432
</description><key id="3453349">1751</key><summary>Multi level parent/child mapping and search fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0</label></labels><created>2012-03-01T12:23:31Z</created><updated>2012-06-29T03:16:51Z</updated><resolved>2012-03-01T12:24:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rauanmayemir" created="2012-06-26T15:35:38Z" id="6579067">I'm having issues with multi level parent/child searching.
Tried to run the example from the specified gist, the exceptions are the same as in my mapping scheme.

... nested: QueryParsingException[[parent_child] Type [product] does not have parent mapping]; }]...

ES version is 0.19.4.
</comment><comment author="rauanmayemir" created="2012-06-26T15:51:30Z" id="6579517">The same thing for v0.19.6.
</comment><comment author="kimchy" created="2012-06-28T23:07:19Z" id="6644349">@rauanmaemirov can you gist your recreation?
</comment><comment author="rauanmayemir" created="2012-06-29T03:16:50Z" id="6647333">I tried the gist from the issue. https://gist.github.com/1941432
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>set missing create param in PutRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1750</link><project id="" key="" /><description>the create parameter is not working when adding index templates so POST doesn't raise IndexTemplateAlreadyExistsException when it should.
</description><key id="3440101">1750</key><summary>set missing create param in PutRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-02-29T16:58:19Z</created><updated>2014-07-16T21:55:36Z</updated><resolved>2012-03-01T12:54:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when asking for null valued json field when fetching search request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1749</link><project id="" key="" /><description>https://gist.github.com/1933431
</description><key id="3437424">1749</key><summary>NullPointerException when asking for null valued json field when fetching search request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0</label></labels><created>2012-02-29T14:43:00Z</created><updated>2012-02-29T14:43:37Z</updated><resolved>2012-02-29T14:43:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Backport for 0.18: Fix for XContentMapConverter with nulls in arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1748</link><project id="" key="" /><description>(sorry fluffed that last one!)
</description><key id="3434935">1748</key><summary>Backport for 0.18: Fix for XContentMapConverter with nulls in arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barnybug</reporter><labels /><created>2012-02-29T11:33:54Z</created><updated>2014-07-16T21:55:36Z</updated><resolved>2013-05-29T16:02:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-29T16:02:20Z" id="18627063">Hey,

I will close this PR as it is rather outdated and it looks like we wont update es 0.18 anymore. We fixed it in a more generic way in the current version. Anyway, sorry for not giving any feedback here. We try to do better next time!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport for 0.18: Fix for XContentMapConverter with nulls in arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1747</link><project id="" key="" /><description /><key id="3434904">1747</key><summary>Backport for 0.18: Fix for XContentMapConverter with nulls in arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barnybug</reporter><labels /><created>2012-02-29T11:32:49Z</created><updated>2014-07-06T16:12:20Z</updated><resolved>2012-02-29T18:34:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Issue mentioned on list "Possible bug when using fields=... on arrays"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1746</link><project id="" key="" /><description>see:
https://groups.google.com/group/elasticsearch/browse_thread/thread/9a38f64ee43ffdca/0301e877640a46f1
</description><key id="3434598">1746</key><summary>Issue mentioned on list "Possible bug when using fields=... on arrays"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barnybug</reporter><labels /><created>2012-02-29T11:11:36Z</created><updated>2014-07-02T15:19:02Z</updated><resolved>2012-03-09T13:06:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>wrong exit value in startup script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1745</link><project id="" key="" /><description>The startup script returns 1 when started with no "-p" option.

``` bash
  else
        exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS $es_parms -cp $ES_CLASSPATH $props \
                    org.elasticsearch.bootstrap.ElasticSearch &lt;&amp;- &amp;

        [ ! -z "$pidpath" ] &amp;&amp; printf '%d' $! &gt; "$pidpath"
    fi

    return $?
```

The return value of exec should be stored. Then checked.
Only then optional writing of the pid should be done.
- Tom
</description><key id="3433505">1745</key><summary>wrong exit value in startup script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tvburger</reporter><labels /><created>2012-02-29T09:37:58Z</created><updated>2013-08-23T11:21:50Z</updated><resolved>2013-08-23T11:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tvburger" created="2012-03-20T14:27:40Z" id="4596339">This would fix the issue...
- Tom

```
diff --git a/bin/elasticsearch b/bin/elasticsearch
index 4adb76c..696d900 100755
--- a/bin/elasticsearch
+++ b/bin/elasticsearch
@@ -121,14 +121,21 @@ launch_service()
         es_parms="$es_parms -Des.foreground=yes"
         exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS $es_parms -Des.path.home="$ES_HOME
                 org.elasticsearch.bootstrap.ElasticSearch
+        status_code=$?
     else
         # Startup ElasticSearch, background it, and write the pid.
         exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS $es_parms -Des.path.home="$ES_HOME
                     org.elasticsearch.bootstrap.ElasticSearch &lt;&amp;- &amp;
-        [ ! -z "$pidpath" ] &amp;&amp; printf '%d' $! &gt; "$pidpath"
+        status_code=$?
+        pid=$!
+        if [ "$status_code" -eq 0 -a ! -z "$pidpath" ]; then
+            printf '%d' "$pid" &gt; "$pidpath"
+            status_code=$?
+        fi
     fi

-    return $?
+    return "$status_code"
 }

 # Parse any command line options.
```
</comment><comment author="spinscale" created="2013-08-09T11:32:25Z" id="22388952">This looks fixed to me in 0.90.3, though in a different way. Can you confirm it works for you as expected?
</comment><comment author="tvburger" created="2013-08-18T05:56:33Z" id="22825292">Hi,

It looks like that in the master branch it is solved. Seen from the core issue I'm happy. I have some second thoughts on quality though, please see below.
- Write PID On Failure?
  There is off course still this question if you want to write the pid unconditionally. I would opt that only the pid is written when successfully the daemon is started. This is what I expected from a pid file, that a process with that pid exists.
  We already know that no such process exists (anymore) since exec didn't return 0...
  (explains the different of "$status_code -eq 0 -a" in the if statement.)
- Failure When Writing PID Fails?
  The last difference is, what to do if writing the pid file files... I take this into account on wether we are succesful, but then again, the daemon is running. Not a trivial decision. (Although some scripts most likely would not function properly without the pid file present, and thus already giving an early error seems a good option to me.)
  This also explains why the name "status_code" and not "execval" was used in the above code snippet, because it is not only storing the return value of the exec call.

Anyways, do what you like. ;)

Kind regards, Tom
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Histogram Facet: Add `pre_zone_adjust_large_interval`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1744</link><project id="" key="" /><description>Will update the docs with its meaning, `pre_zone_adjust_large_interval`. Here is an excerpt:

p. Lets take an example. For @2012-04-01T04:15:30Z@, with a @pre_zone@ of @-08:00@. For @day@ interval, the actual time by applying the time zone and rounding falls under @2012-03-31@, so the returned value will be (in millis) of @2012-03-31T00:00:00Z@ (UTC). For @hour@ interval, applying the time zone results in @2012-03-31T20:15:30@, rounding it results in @2012-03-31T20:00:00@, but, we want to return it in UTC (@post_zone@ is not set), so we convert it back to UTC: @2012-04-01T04:00:00Z@. Note, we are consistent in the results, returning the rounded value in UTC.

p. @post_zone@ simply takes the result, and adds the relevant offset.

p. Sometimes, we want to apply the same conversion to UTC we did above for @hour@ also for @day@ (and up) intervals. We can set @pre_zone_adjust_large_interval@ to @true@, which will apply the same conversion done for @hour@ interval in the example, to @day@ and above intervals (it can be set regardless of the interval, but only kick in when using @day@ and higher intervals).
</description><key id="3429438">1744</key><summary>Date Histogram Facet: Add `pre_zone_adjust_large_interval`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0</label></labels><created>2012-02-29T00:52:24Z</created><updated>2012-02-29T00:53:50Z</updated><resolved>2012-02-29T00:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>make the BaseQueryBuilder interface implement Clonable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1743</link><project id="" key="" /><description>Because of this interface not implementing Clonable it's impossible to prepare and reuse parts of the query like this:

``` java
BoolQueryBuilder queryPart1 = boolQuery()
for(int i = 0; i &lt; arr.size(); i++) {
    queryPart1.must(....);
}

BoolQueryBuilder mainQuery= boolQuery()
    .should(
        queryPart1
        .must(...)
    )
    .should(
        queryPart1
        .sould(...)
   );


```

I'm forced to create a new bool query at each point of reuse or externalize the creation of queryPart1 to a separate method and call it whenever i'm in need of that query part
</description><key id="3423807">1743</key><summary>make the BaseQueryBuilder interface implement Clonable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dnavre</reporter><labels /><created>2012-02-28T18:49:44Z</created><updated>2016-10-10T03:53:10Z</updated><resolved>2014-03-13T12:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-28T22:14:03Z" id="4226931">You can safely reuse queryPart1 (in your code) in several places. Did you run into problems with it?
</comment><comment author="dnavre" created="2012-02-29T16:20:51Z" id="4240704">not exactly :) sorry, I was not clear enough.
What i mean is this: let's suppose i have imaginary conditions condition1, condition2 and condition3. and i need to find all rows satisfying query: (condition1 &amp;&amp; condition 2) || (condition 1 and condition3). This is just an example, in real life the query i have is pretty damned complex and i need to reuse some of it's conditions in many places. So what happens when i use this code:

``` java
BoolQueryBuilder queryPart1 = boolQuery()
for(int i = 0; i &lt; arr.size(); i++) {
    queryPart1.must(....); // building up condition1
}

BoolQueryBuilder mainQuery= boolQuery()
    .should(
        queryPart1 // reusing condition1
        .must(...) // adding condition2
    )
    .should(
        queryPart1 // reusing condition1
        .sould(...) // adding condition3
   );
```

in the end I'm ending up with this query: (condition1 &amp;&amp; condition2) || (condition1 &amp;&amp; condition2 &amp;&amp; condition3)

what i'd like to be able to do is:

``` java
// building condition1
BoolQueryBuilder queryPart1 = boolQuery()
for(int i = 0; i &lt; arr.size(); i++) {
    queryPart1.must(....);
}

BoolQueryBuilder mainQuery= boolQuery()
    .should(
        queryPart1.clone() // reusing condition1
        .must(...) // adding condition2
    )
    .should(
        queryPart1.clone() // reusing condition1
        .sould(...) // adding condition3
   );
```

this way i'm actually able to reuse the condition throughout the query.
</comment><comment author="dnavre" created="2012-02-29T16:24:23Z" id="4240787">If you're okay with it I can even try to implement this functionality myself and then do a pull request to you :)
</comment><comment author="kimchy" created="2012-03-01T12:52:30Z" id="4257478">I still don't understand why you would need it. You mean be able to clone a query, and then modify it? The problem with supporting clone is the semantics for more complex query builders, one that wrap a filter or another query.
</comment><comment author="dnavre" created="2012-03-15T10:59:32Z" id="4517433">sorry for the late reply.
I'd like to be able to clone the queries so i would be able to reuse a single query several times with different modifications.

lets suppose i have a boolQuery which i wrote and assigned to myBoolQuery var. I'm going to use it in several places in my final query. The first time i use it i just need to boost it, the second time i have an additional condition and the third time i have another condition. so the final result should look something like this:

``` java
boolQuery()
.should(myBoolQuery.boost(10F))
.should(myBoolQuery.must(/*blah */))
.should(myBoolQuery.must(/*another blah */);
```

This looks like what i'd really like to do but in reality it is not cause each time i call some method on myBoolQuery object it's changing the state of the object. What i'm ending with when using this query is the following conditional statement:

```
myPreparedCondition.boost()
|| (myPreparedCondition.boost() &amp;&amp; must(/*blah*/))
|| (myPreparedCondition.boost() &amp;&amp; must(/*blah*/) &amp;&amp; must(/*another blah*/))
```

to get the query I originally wanted i need to wrap my prepared query in an additional bool query like this:

``` java
boolQuery()
.should(boolQuery().must(myBoolQuery).boost(10F))
.should(boolQuery().must(myBoolQuery).must(/*blah */))
.should(boolQuery().must(myBoolQuery).must(/*another blah */);
```

which in my opinion makes the code less readable and more cumbersome.

If the queries were clonable i would be able to write the query i originally wanted to have this way:

``` java
boolQuery()
.should(myBoolQuery.clone().boost(10F))
.should(myBoolQuery.clone().must(/*blah */))
.should(myBoolQuery.clone().must(/*another blah */);
```

another option to do this same thing is by making so that each query modifying method would not change the state of the existing query object but rather return a new query object with the modification inside of it. But I think this will be a damn lot harder to implement and I'm not even sure if this is something worth doing at all.
</comment><comment author="alex-lx" created="2016-10-10T03:53:10Z" id="252534535">hi all, does this feature getting supported now? it seems muBollQuery is not clonable on version 2.4.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Corrected erroneous stem filter (loads the Spanish, not the Swedish ligh...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1742</link><project id="" key="" /><description>...t stemmer):

} else if ("light_swedish".equalsIgnoreCase(language) || "lightSwedish".equalsIgnoreCase(language)) {
            return new SpanishLightStemFilter(tokenStream);
}
to "return new SwedishLightStemFilter(tokenStream);"
</description><key id="3401790">1742</key><summary>Corrected erroneous stem filter (loads the Spanish, not the Swedish ligh...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">barsk</reporter><labels /><created>2012-02-27T15:01:54Z</created><updated>2014-07-16T21:55:37Z</updated><resolved>2012-02-27T17:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-27T17:36:06Z" id="4198655">Good catch!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1741</link><project id="" key="" /><description /><key id="3401778">1741</key><summary>fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-02-27T15:01:15Z</created><updated>2014-07-16T21:55:37Z</updated><resolved>2012-02-27T17:37:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Orderly shutdown with unicast discovery might cause the shutdown node to still be part of the election process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1740</link><project id="" key="" /><description>Relates to #1730 and #1731.
</description><key id="3398149">1740</key><summary>Orderly shutdown with unicast discovery might cause the shutdown node to still be part of the election process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0</label></labels><created>2012-02-27T09:54:55Z</created><updated>2012-02-27T09:56:12Z</updated><resolved>2012-02-27T09:56:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Search against indexed bounding box</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1739</link><project id="" key="" /><description>ES indexes points, and allows a range of searches using another point, a bounding box, or a polygon.

I believe it should also do the reverse.  ES should be able to store a bounding box or polygon, then allow a search using point (is it in the indexed area), a bounding-box (does it overlap with the indexed area) or polygon.

This support would allow ES to store (for example) polygons representing states.  A client could then pass in a lat/lon representing a town, and ES would return the state that contains that town.
</description><key id="3397883">1739</key><summary>Search against indexed bounding box</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IanMayo</reporter><labels /><created>2012-02-27T09:28:45Z</created><updated>2013-04-18T06:28:57Z</updated><resolved>2013-04-17T13:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="IanMayo" created="2012-02-27T09:30:32Z" id="4190290">Further, if ES stored indexed polylines then these could be used to store roads.  A client could try to find the polyline nearest to a lat/lon - to find the road nearest to a particular mobile phone user.
</comment><comment author="dstuebe" created="2012-06-11T15:22:28Z" id="6247256">Hi Ian
In thinking about your more general geospatial model you might take a look at the operations and semantics of GML:
Here is a snap shot of the cases they define:
https://img.skitch.com/20120423-tsepdxgscn12n4iymix3xg2ae3.jpg
See GML OGC Reference Model V3.2.1, Section 2.6 Geometry and Topology
David
</comment><comment author="IanMayo" created="2012-06-11T15:26:21Z" id="6247359">Hi there David, 
thanks for your contribution.

But, I'm not fully sure what you're recommending. I recognise that GML may provide the language used to correctly express the requirement, but it's not going to provide the search capabilities I'm looking for - or is it?
cheers,
Ian
</comment><comment author="dstuebe" created="2012-06-11T15:39:17Z" id="6247739">Hi Ian
Unfortunately GML will not provide the capability, but it will let you be precise about the capability that you are providing. 
Please let me know if you want to discuss use cases or implementation.
David
</comment><comment author="ghost" created="2013-04-15T19:25:23Z" id="16406074">So if I understand correctly if I index documents that have a bounding box property and map it as a geo_shape, with Elasticsearch right now it is impossible to send a query with a longitude and latitude and get back all the documents for which the bounding box includes the given coordinates?
</comment><comment author="spinscale" created="2013-04-17T13:03:19Z" id="16504124">@hydrozen maybe I got your requirement wrong, but it is possible to index shapes and use points to check which shape contains the point

```
curl -X DELETE localhost:9200/geotest
curl -X PUT localhost:9200/geotest
curl -X PUT localhost:9200/geotest/geotest/_mapping -d '{ "geotest": { "properties" : { "shape" : { "type" : "geo_shape" } } } }'

# index two envelope shapes, one is munich, one is a part of south bavaria (including munich)
curl -X PUT localhost:9200/geotest/geotest/munich -d '{"shape":  { "type" : "envelope", "coordinates" : [[11.399689,48.226045], [11.727905, 48.063855]] } }'
curl -X PUT localhost:9200/geotest/geotest/bavaria -d '{"shape":  { "type" : "envelope", "coordinates" : [[10.442505, 48.770672], [12.579346, 47.794707]] } }'

# search with a point inside of munich (two results)
curl -X POST localhost:9200/geotest/geotest/_search -d '{
    "query": {
        "geo_shape": {
            "shape": {
                "shape": {
                    "type": "point",
                    "coordinates": [11.557617, 48.180739]
                }
            }
        }
    }
}'

# search with a point inside south bavaria (one result)
curl -X POST localhost:9200/geotest/geotest/_search -d '{
    "query": {
        "geo_shape": {
            "shape": {
                "shape": {
                    "type": "point",
                    "coordinates": [11.167603, 47.916342]
                }
            }
        }
    }
}'

# search with a point at nuremberg (no results)
curl -X POST localhost:9200/geotest/geotest/_search -d '{
    "query": {
        "geo_shape": {
            "shape": {
                "shape": {
                    "type": "point",
                    "coordinates": [11.074219, 49.453843]
                }
            }
        }
    }
}'
```
</comment><comment author="IanMayo" created="2013-04-17T13:17:56Z" id="16504795">Great, thanks @spinscale, that's exactly what I was looking for. 
</comment><comment author="lukecampbell" created="2013-04-17T14:13:24Z" id="16507945">Just for some clarification this appears to be for 0.90+ 0.20.6 requires a relation to be defined and there are some issues listed on the [API page for `geo_shape`](http://www.elasticsearch.org/guide/reference/query-dsl/geo-shape-query/)
</comment><comment author="lukecampbell" created="2013-04-17T14:22:25Z" id="16508508">I don't know much about spatial4j but does anyone know how it handles projection?  Specifically how it deals with envelopes that cross the date line or the north pole?
</comment><comment author="spinscale" created="2013-04-18T06:28:57Z" id="16559940">@lukecampbell you are right, I did those things with 0.90. Sorry for not mentioning.

On your other question: Yes, this is a problem right now. We have started working on that as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_routing` in child type mapping prevents indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1738</link><project id="" key="" /><description>It appears that the child document's routing value is, by default, its parent's ID. Thus, if the parent has a `_routing` path set in its mapping, the child and parent will be mapped to different shards (which is bad).

Adding `_routing` to the child mapping does not help, as the value from the document does not override the default routing value (i.e. the parent's ID). Instead, it causes an exception while indexing the child document.

Reproduced in 0.19.0.RC3: https://gist.github.com/1902998
</description><key id="3377876">1738</key><summary>`_routing` in child type mapping prevents indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevingessner</reporter><labels><label>adoptme</label><label>bug</label></labels><created>2012-02-24T19:30:56Z</created><updated>2015-06-08T00:09:02Z</updated><resolved>2014-11-27T11:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-26T22:40:06Z" id="4185270">Yea, at least for now you need to explicitly pass the routing value (which is a good practice if you can). It happens because setting the parent automatically set the routing value when indexing, so it does not go and tries to extract the routing value from the actual document.
</comment><comment author="spinscale" created="2014-07-18T10:19:29Z" id="49416249">still the same behaviour with 1.2, quick reproduction with sense

```
DELETE test
PUT test

PUT /test/parent/_mapping
{
  "parent": {
    "_routing": {
      "required": true,
      "path": "account"
    }
  }
}

PUT /test/child/_mapping
{
  "child": {
    "_parent": {
      "type": "parent"
    },
    "_routing": {
      "required": true,
      "path": "account"
    }
  }
}

# error returned
PUT /test/child/c1?parent=p1
{
    "account": 1
}

# no error
PUT /test/child/c2?parent=p1&amp;routing=1
{
    "account": 1
}
```
</comment><comment author="clintongormley" created="2014-07-18T11:13:52Z" id="49419997">@spinscale perhaps close this in favour of #6730?
</comment><comment author="spinscale" created="2014-07-18T12:05:21Z" id="49423377">if we decide to leave things as as they are now, this still needs to be fixed - and/or we could do it before 2.0 :-)
</comment><comment author="clintongormley" created="2014-11-27T11:50:59Z" id="64780996">Closed in favour of #6730
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sorting failed when same name under different types with different mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1737</link><project id="" key="" /><description>if there are same fieldname with different data type in different type,
for example,datetime and string,
type_a have field:time,data type is date
type_b have field:time,data type is string,(this type may be created dynamic)

if i want to query against type_a use this query:
http://localhost:9200/index/type_a/_search?q=*&amp;sort=time

perhaps,you may get  this error(not always,occasional after a restart)

{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[Pqdw_LAFSbOfyo9yVU9aaw][index][0]: QueryPhaseExecutionException[[index][0]: query[ConstantScore(_:_)],from[0],size[10],sort[&lt;custom:\"time\": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@1693f17f&gt;]: Query Failed [Failed to execute main query]]; nested: IOException[Can't sort on string types with more than one value per doc, or more than one token per field]; }{[Pqdw_LAFSbOfyo9yVU9aaw][index][1]: QueryPhaseExecutionException[[index][1]: query[ConstantScore(_:_)],from[0],size[10],sort[&lt;custom:\"time\": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@3a18c8ca&gt;]: Query Failed [Failed to execute main query]]; nested: IOException[Can't sort on string types with more than one value per doc, or more than one token per field]; }{[Pqdw_LAFSbOfyo9yVU9aaw][xxx][2]: QueryPhaseExecutionException[[index][2]: query[ConstantScore(_:_)],from[0],size[10],sort[&lt;custom:\"time\": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@31266392&gt;]: Query Failed [Failed to execute main query]]; nested: IOException[Can't sort on string types with more than one value per doc, or more than one token per field]; }]","status":500}

this line:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/sort/SortParseElement.java#L159
it returned the wrong FieldMapper,it should be "LongFieldMapper" but actually returned is :"StringFieldMapper",did'nt load the right data,so sorting failed ,
with the given parameter "fieldName",it didn't know which mapping will be used,and elasticsearch should be more smart,to choose the right mapping,in this case,if we have explicit specify the type(in url:http://localhost:9200/index/type_a/_search),es can select the mapping within type:type_a.
</description><key id="3375242">1737</key><summary>sorting failed when same name under different types with different mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2012-02-24T16:39:50Z</created><updated>2014-07-08T14:25:14Z</updated><resolved>2014-07-08T14:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-03-14T06:34:48Z" id="4493028">also query will be failed,if other type have the same name,this should be avoid.@kimchy
</comment><comment author="clintongormley" created="2014-07-08T14:25:14Z" id="48342979">This is a known limitation.  Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>msearch should accept a leading \n</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1736</link><project id="" key="" /><description>This query, with  a `\n` at the start of the request body, throws an error:

```
curl -XGET 'http://127.0.0.1:9200/_all/_msearch?pretty=1'  -d '
{"index":"bar"}
{}
'
{
  "responses" : [ {
    "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[M13f_44ASzCtykP8D-y-Og][geonames_temp][0]: SearchParseException[[geonames_temp][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[geonames_temp][0]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][iannounce_object_1329422272][3]: SearchParseException[[iannounce_object_1329422272][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[iannounce_object_1329422272][3]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][foo][4]: SearchParseException[[foo][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[foo][4]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][geonames_1329424673][0]: SearchParseException[[geonames_1329424673][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[geonames_1329424673][0]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][foo][1]: SearchParseException[[foo][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[foo][1]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][iannounce_object_1329422272][2]: SearchParseException[[iannounce_object_1329422272][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[iannounce_object_1329422272][2]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][iannounce_object_1329422272][1]: SearchParseException[[iannounce_object_1329422272][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[iannounce_object_1329422272][1]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }{[M13f_44ASzCtykP8D-y-Og][foo][0]: SearchParseException[[foo][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\"index\":\"bar\"}]]]; nested: SearchParseException[[foo][0]: from[-1],size[-1]: Parse Failure [No parser for element [index]]]; }]"
  } ]
}
```

But the same format for `bulk` is accepted.
</description><key id="3371438">1736</key><summary>msearch should accept a leading \n</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.19.0</label></labels><created>2012-02-24T11:59:38Z</created><updated>2012-02-26T22:27:55Z</updated><resolved>2012-02-26T22:27:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>boolean fields returned as string in a query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1735</link><project id="" key="" /><description>I defined a boolean property in a mapping, and when I do get operation:

http://localhost:9200/index/user/&lt;id&gt;?fields=enabled

the JSON type of field is boolean as I would expect ("enabled": true).

When I do a search operation, requesting that field return in the result, the value is wrapped as a string ("enabled": "true").
</description><key id="3366163">1735</key><summary>boolean fields returned as string in a query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels /><created>2012-02-24T00:39:19Z</created><updated>2014-07-08T14:24:45Z</updated><resolved>2014-07-08T14:24:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-27T11:57:18Z" id="4192140">This seems to work fine with 0.19, and 0.18. Can you gist a full recreation?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flush: Add a specific thread pool for flush operations (scheduled and API)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1734</link><project id="" key="" /><description /><key id="3364488">1734</key><summary>Flush: Add a specific thread pool for flush operations (scheduled and API)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0</label></labels><created>2012-02-23T22:38:21Z</created><updated>2012-02-23T23:36:30Z</updated><resolved>2012-02-23T23:36:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Searchresponse contain total 49 but no hits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1733</link><project id="" key="" /><description>Can somebody help me?

{total=44, searchResults=[]}
</description><key id="3357804">1733</key><summary>Searchresponse contain total 49 but no hits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Ferparishuertas</reporter><labels /><created>2012-02-23T15:43:03Z</created><updated>2013-07-15T16:41:08Z</updated><resolved>2013-07-15T16:41:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MagmaRules" created="2012-02-23T15:59:40Z" id="4138922">Think you should use the google groups to get help:
https://groups.google.com/group/elasticsearch

Still you need to provide at least the query and some more info so someone can help you out.
</comment><comment author="lukas-vlcek" created="2012-02-23T16:16:18Z" id="4139263">Please see http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error when trying to get a facet from a Date field accross indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1732</link><project id="" key="" /><description>I have a template for my two indices that defines that all fields starting with "Dt" are dates.

When i try to get a facet for one of these fields and in one index i don't have values in this field i get the error:

&lt;pre&gt;
Caused by: java.lang.ClassCastException: org.elasticsearch.search.facet.terms.longs.InternalLongTermsFacet cannot be cast to org.elasticsearch.search.facet.terms.strings.InternalStringTermsFacet
    at org.elasticsearch.search.facet.terms.strings.InternalStringTermsFacet.reduce(InternalStringTermsFacet.java:200)
    at org.elasticsearch.search.facet.terms.TermsFacetProcessor.reduce(TermsFacetProcessor.java:208)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:298)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:184)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:169)
&lt;/pre&gt;


It seems that when elasticsearch doesn't find values in index1 it assumes the default of string and I end up with string for index1 and long for index2.
</description><key id="3357036">1732</key><summary>Error when trying to get a facet from a Date field accross indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MagmaRules</reporter><labels /><created>2012-02-23T14:56:44Z</created><updated>2014-07-08T14:24:24Z</updated><resolved>2014-07-08T14:24:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-26T22:32:12Z" id="4185216">Yes, the fields need to be of the same type, across the indices.
</comment><comment author="MagmaRules" created="2012-02-27T10:48:02Z" id="4191301">Yes i understand that =). They have the same type. 

The problem is that in one of the indices the field has not been initialized. It appears that if the field has no values then ES assumes string. This is a problem when we first start our application because one of the indices may take longer to get entries indexed.

Isn't there a way ES can know if a field has been initialized? If the field has not been initialized ES should ignore it.
</comment><comment author="kimchy" created="2012-02-27T11:55:08Z" id="4192113">Yes, that might be possible.
</comment><comment author="vermaport" created="2012-03-19T16:43:03Z" id="4577717">I am also getting this error when I have configured a river, which seems to create an internal _river index that is included when _all indices are searched. I am trying to facet on a long field named "severity" that exists (and is a long) in all of my regular indices (except for _river, of course). Oddly, if I facet on just the _river index, or on all indices except the _river index, it works as expected (I get 0 hits on the _river index, which is fine). If I specify _all or no index/type, I get the ClassCastException. 
</comment><comment author="vermaport" created="2012-03-28T17:39:50Z" id="4774310">Here's a gist of the issue. Very easy to reproduce. The only workaround appears to be installing all mappings in all indices so that fields are never missing, or only faceting on indices with mappings that are known to have the field. Or only using a single index.

https://gist.github.com/2228486
</comment><comment author="nahap" created="2013-05-24T06:52:30Z" id="18389257">will this also happen if i have a shard in which the field has no values yet?
</comment><comment author="clintongormley" created="2014-07-08T14:24:23Z" id="48342847">This works correctly in aggregations. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Zen discovery stops working after a ConnectException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1731</link><project id="" key="" /><description>In 0.19.0-RC3, I started a two-node cluster with `minimum_master_nodes: 2`, waited for the cluster state to become green then shut down one of the nodes. This showed up in the logs for the remaining node:

```
[2012-02-23 11:09:21,023][INFO ][discovery.zen            ] [Malus, Karl] master_left [[Nitro][VUh02v-qSp2F-0m0wbDIDA][inet[/192.168.1.147:9303]]], reason [shut_down]
[2012-02-23 11:09:21,024][WARN ][discovery.zen            ] [Malus, Karl] not enough master nodes after master left (reason = shut_down), current nodes: {[Malus, Karl][wN5y7pQ_TYCLi2ywrcTSjw][inet[192.168.1.147/192.168.1.147:9304]],}
[2012-02-23 11:09:21,025][INFO ][cluster.service          ] [Malus, Karl] removed {[Nitro][VUh02v-qSp2F-0m0wbDIDA][inet[/192.168.1.147:9303]],}, reason: zen-disco-master_failed ([Nitro][VUh02v-qSp2F-0m0wbDIDA][inet[/192.168.1.147:9303]])
[2012-02-23 11:09:24,035][WARN ][discovery.zen            ] [Malus, Karl] failed to connect to master [[Nitro][VUh02v-qSp2F-0m0wbDIDA][inet[/192.168.1.147:9303]]], retrying...
org.elasticsearch.transport.ConnectTransportException: [Nitro][inet[/192.168.1.147:9303]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:562)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:505)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:484)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:128)
    at org.elasticsearch.discovery.zen.ZenDiscovery.innterJoinCluster(ZenDiscovery.java:312)
    at org.elasticsearch.discovery.zen.ZenDiscovery.access$500(ZenDiscovery.java:69)
    at org.elasticsearch.discovery.zen.ZenDiscovery$1.run(ZenDiscovery.java:266)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.connect(NioClientSocketPipelineSink.java:400)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:362)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:284)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    ... 3 more
```

At this point, zen discovery stopped working. I restarted the other node, but they cannot find/connect to eachother properly. The node that was not shut down raises MasterNotDiscoveredExceptions, while the restarted selects itself as the master and permanently has cluster state: red.

I cannot _consistently_ reproduce this issue, but I'm thinking maybe this exception should be handled?
</description><key id="3353908">1731</key><summary>Zen discovery stops working after a ConnectException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2012-02-23T10:24:22Z</created><updated>2012-03-08T13:49:37Z</updated><resolved>2012-03-08T13:49:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2012-02-23T10:27:41Z" id="4133943">Most of the times, I get the following exception (different traceback, but same exception class), which seems to be perfectly recoverable, but with the above exception, the cluster cannot recover until I've restarted the affected node:

```
org.elasticsearch.transport.ConnectTransportException: [][inet[/192.168.1.147:9304]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannelsLight(NettyTransport.java:535)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:501)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNodeLight(NettyTransport.java:479)
    at org.elasticsearch.transport.TransportService.connectToNodeLight(TransportService.java:132)
    at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$3.run(UnicastZenPing.java:264)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.net.ConnectException: Connection refused
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.connect(NioClientSocketPipelineSink.java:400)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.processSelectedKeys(NioClientSocketPipelineSink.java:362)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientSocketPipelineSink$Boss.run(NioClientSocketPipelineSink.java:284)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    ... 3 more
```
</comment><comment author="nkvoll" created="2012-03-08T13:49:37Z" id="4391446">#1740 seems to have fixed this. Closing the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minimum master nodes not respected.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1730</link><project id="" key="" /><description>I configured two ElasticSearch instances with unicast ping discovery to eacher, and set

```
discovery.zen.minimum_master_nodes: 2
```

.. in order to avoid a split-brain syndrome, however, when I shut down one of the nodes via the nodes shutdown API.

I then expected the cluster to become unavailable since the minimum_master_nodes was no longer met, but instead, the last remaining node decides to re-elect itself as a master.

The logs for both servers can be found at https://gist.github.com/3b80693463782b0bc7e8
</description><key id="3350033">1730</key><summary>Minimum master nodes not respected.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nkvoll</reporter><labels /><created>2012-02-23T01:17:15Z</created><updated>2012-03-08T13:49:28Z</updated><resolved>2012-03-08T13:49:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2012-02-23T01:38:52Z" id="4129065">Similarly, if I start with a three-node cluster with `discovery.zen.minimum_master_nodes: 3`, a master will not be elected with only 2 nodes up, but once a master has been elected with 3 nodes up, it is possible to shut down one node, and the two remaining nodes will decide on a master:

See for a snipped example: https://gist.github.com/f99752d25ca38f0693e8
</comment><comment author="nkvoll" created="2012-02-23T09:45:55Z" id="4133395">This seems to work fine with 0.19.0.RC3, based on a little bit of manual testing. Has anything changed in this area since 0.18.7? RC3 does not even log any tracebacks, which I guess is a good sign :)
</comment><comment author="kimchy" created="2012-02-27T09:55:31Z" id="4190648">Hey, #1731 relates to this one, it seems. What happens is that shutdown causes the node to go and explicitly leave the cluster, but, then, the master it leaves (if its not the master) is too fast and manages to send a ping request where the node that shutsdown still manages to answer, which causes this problem and #1731. I will push a fix with a dedicated issue for this: #1740. If you can give it a twirl as well (I managed to recreate it), it would be great and we can close these two.
</comment><comment author="nkvoll" created="2012-03-08T13:49:28Z" id="4391443">#1740 seems to have fixed this. Closing the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Initial RPM spec files for ElasticSearch 0.19</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1729</link><project id="" key="" /><description>Initial copy of RPM spec files for ElasticSearch 0.19.1 based directly on tavisto's work in https://github.com/tavisto/elasticsearch-rpms. As with tavisto's work, these spec files produce RPMs intended for use on RedHat Enterprise Linux systems such as RHEL, Fedora or CentOS.

I have only tested the spec files and generated RPMs on CentOS 6.2.
</description><key id="3347559">1729</key><summary>Initial RPM spec files for ElasticSearch 0.19</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">organicveggie</reporter><labels /><created>2012-02-22T22:11:22Z</created><updated>2014-07-10T09:46:43Z</updated><resolved>2012-04-19T17:04:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dhardy92" created="2012-02-23T09:09:46Z" id="4132933">Hello,

Init.d script, sysconfig, log config and logrotate config (maybe others) should be merged with the files used for deb packaging to avoid duplications.
And what do you think about #1697 (split elasticsearch and libelasticsearch) as well for rpm packages ?
</comment><comment author="clintongormley" created="2012-02-23T10:06:28Z" id="4133657">@organicveggie if you send me a few instructions, I'll try installing the RPMs on openSUSE
</comment><comment author="tavisto" created="2012-03-11T22:26:18Z" id="4442909">@organicveggie is there any reason you didn't submit this back to my repo? I was just about to split all those plugins up myself. If we all want to work on having an official rpm repo for elasticsearch I would be more than happy to help. 
</comment><comment author="organicveggie" created="2012-03-12T01:10:35Z" id="4444070">@tavisto Ugh. Sorry, no slight intended... just a rather stupid oversight on my part. Blame it on the sheer volume of work I've been putting it at my job. I feel we could go a couple different directions at this point, both of which would work fine.

A) I scrap this pull request and submit a new pull request to you. We work through the suggestions from dhardy92 and submit a new pull request to elasticsearch from your repo. Admittedly, we'll still have to fork elasticsearch to get a pull request submitted.

B) We can move my fork over to a Team/Organization on GitHub and both have full access. Fix it up and submit/resubmit this pull request.

Any thoughts or preferences?
</comment><comment author="tavisto" created="2012-03-12T03:18:42Z" id="4444960">@organicveggie No big deal. I was just about to do this exact work in my repo and saw that you already had done it. I have a branch with your spec files from this patch, but it still needs a bit of work before I would call it production ready.

I noticed that you moved the base folder from '%{_javadir}' to '/opt'. I like the idea, but I wonder about the upgrade from older rpm's that use the old way. 

As for moving the spec files I am open to any of the options. I think that if the spec for elasticsearch goes into the main project, then the plugin packages should probably go in their respective repos as well. 

Another option would be to move a fork of my repo into the elasticsearch organization, but I think it makes more sense if they were part of the projects.

As for the init scripts, I will have to compare the debian one with the rhel version and see what the difference is. I have never tried to use one init script on both types of systems. 

For now I will update my repo with these new plugin spec files and we can go from there. If they accept the patch then I will do what I can to point anyone interested in my repo to the official specs in the project.
</comment><comment author="organicveggie" created="2012-04-10T20:23:31Z" id="5055326">Any thoughts on whether the spec files for plugins should be part of the main project? Or part of each individual plugin?
</comment><comment author="organicveggie" created="2012-04-10T20:24:44Z" id="5055350">@tavisto Have you had a chance to compare the RedHat and Debian versions of the init scripts? I've never tried to use one init script for both platforms...
</comment><comment author="deverton" created="2012-04-10T23:35:52Z" id="5059066">Given that Elasticsearch has moved to Maven and each plugin is now its own project, would it be worth migrating the RPM generation to Maven? We've had great success with the [Maven RPM Plugin](http://mojo.codehaus.org/rpm-maven-plugin/) for generating quite complicated packaging. It makes version management much easier it's all in one place, the POM, rather than split between the POM and the SPEC file.
</comment><comment author="organicveggie" created="2012-04-19T17:04:45Z" id="5226696">While I'm sure the Maven RPM Plugin is a great tool, I'm not really interested in taking this on and rewriting everything again. My company has spec files that work just fine for us and that's all we really needed. Unfortunately, I can't really afford to put any more time into this. Sorry.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>debian package lib dir not found in target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1728</link><project id="" key="" /><description>[ERROR] Failed to create debian package /tmp/es/elasticsearch/target/releases/elasticsearch-0.19.0.RC4-SNAPSHOT.deb
org.vafer.jdeb.PackagingException: Failed to create debian package /tmp/es/elasticsearch/target/releases/elasticsearch-0.19.0.RC4-SNAPSHOT.deb

Caused by: java.io.FileNotFoundException: Data source not found : /tmp/es/elasticsearch/target/lib

created a symbolic link to ../lib to solve my issue.

thank you,
Pierre.
</description><key id="3335880">1728</key><summary>debian package lib dir not found in target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">pierre-p</reporter><labels /><created>2012-02-22T16:56:49Z</created><updated>2013-05-24T12:14:06Z</updated><resolved>2013-05-24T12:14:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-22T19:01:24Z" id="4115338">When do you get it, when you are running what? Creating a symlink does not sounds like it will really "solve" the problem.
</comment><comment author="spinscale" created="2013-05-24T12:14:06Z" id="18401311">No more info supplied. Closing for now. In case this still happens, please reopen this ticket and provide additional information. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog Flush: When disabling flush and enabling it again, scheduled flush stops executing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1727</link><project id="" key="" /><description>Also, fix wrong warn logging on scheduled flush if we can't do it on the shard, and allow for "-1" settings on flush paramaters to disable them.
</description><key id="3332156">1727</key><summary>Translog Flush: When disabling flush and enabling it again, scheduled flush stops executing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.19.0</label></labels><created>2012-02-22T12:56:08Z</created><updated>2012-02-22T13:01:06Z</updated><resolved>2012-02-22T13:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>debian package violates naming convention</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1726</link><project id="" key="" /><description>Check out 7.3 here: http://www.debian.org/doc/manuals/debian-faq/ch-pkg_basics.html

tl;dr the packages should be named elasticsearch_version-revision_arch.deb. Those underscores are dashes at the moment.
</description><key id="3323663">1726</key><summary>debian package violates naming convention</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">pitluga</reporter><labels><label>discuss</label></labels><created>2012-02-21T23:21:50Z</created><updated>2014-07-18T09:02:05Z</updated><resolved>2014-07-18T09:01:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-06T10:06:50Z" id="19035786">There is also no revision or arch in the package, as their are not needed. Packages are only done on new releases, so there is no need for a revision. There is also no need for an arch, as we only ship jars and shell scripts, and try to be independent...

What do you propose in that case?

Thanks for your input.
</comment><comment author="pitluga" created="2013-06-06T12:04:42Z" id="19040620">At the time, I was having issues importing the deb into our internal apt repo because of package name. The tool we were using was expecting the standard format. For revision, if it is not needed, it would be better to just default it to 1 or something. Also, when a package applies to all architectures you should use "all". Given that, the current version of elasticsearch would be named:

```
elasticsearch_0.9.1-1_all.deb
```

Sorry to be pedantic. It was just causing us to do extra work and seemed like a simple fix.
</comment><comment author="nik9000" created="2013-10-03T13:36:12Z" id="25620657">&gt; Sorry to be pedantic.

As much as I love Debian, as an ecosystem it is pretty pedantic and opinionated.  Hell, sometimes I love it _because_ it is both of those things.

Do you know if changing the package name would cause trouble for people upgrading?
</comment><comment author="clintongormley" created="2014-07-18T09:01:00Z" id="49409803">No feedback received.  Clsoing
</comment><comment author="clintongormley" created="2014-07-18T09:02:05Z" id="49409891">/cc @electrical FYI
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query never returns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1725</link><project id="" key="" /><description>When executing a boolean query that contains a constant score query filter, the query appears to lock up elasticsearch or lucene and never return a response.  Details how to reproduce can be found in the following gist:

https://gist.github.com/1860568
</description><key id="3319446">1725</key><summary>query never returns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>bug</label><label>v0.19.0</label></labels><created>2012-02-21T20:22:53Z</created><updated>2012-02-21T23:50:31Z</updated><resolved>2012-02-21T23:50:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-21T23:49:40Z" id="4092283">This one was a tricky one..., but I have a fix (and a small optimization), will push shortly...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Real time get on a stored _size field does not return its value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1724</link><project id="" key="" /><description /><key id="3313564">1724</key><summary>Real time get on a stored _size field does not return its value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0</label></labels><created>2012-02-21T16:08:26Z</created><updated>2012-02-21T16:10:02Z</updated><resolved>2012-02-21T16:10:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 3.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1723</link><project id="" key="" /><description>Could ES support Query time joining in Lucene http://www.searchworkings.org/blog/-/blogs/query-time-joining-in-lucene?
</description><key id="3307536">1723</key><summary>Upgrade to Lucene 3.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rualatngua</reporter><labels /><created>2012-02-21T07:26:38Z</created><updated>2012-06-26T00:43:43Z</updated><resolved>2012-06-26T00:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-21T11:20:09Z" id="4073031">First, Lucene 3.6 is not out yet. And, you have a solution now with the parent child feature (which also makes sure the parent and the children will exist on the same shard).
</comment><comment author="ppearcy" created="2012-04-11T23:29:09Z" id="5081517">FYI, looks like lucene 3.6 is almost out and a RC is available:
http://mail-archives.apache.org/mod_mbox/lucene-dev/201204.mbox/%3CCAOdYfZXCMnU0x+JZUVh_-HHjrnCb+2h2+UHY-EH_BwP0PEqz=A@mail.gmail.com%3E

Looking forward to this release for the fix around highlighting and synonyms. 

Thanks!
</comment><comment author="ppearcy" created="2012-04-11T23:30:33Z" id="5081539">Errr, sorry, that link was mangled and the links that it referenced weren't active anyways... Should be out very soon, though. 
</comment><comment author="tfreitas" created="2012-04-14T23:07:44Z" id="5135457">Lucene 3.6 is ready
</comment><comment author="kimchy" created="2012-04-15T10:16:15Z" id="5138288">Yea, going to finish some changeset I am working on and then do the upgrade.
</comment><comment author="grigorescu" created="2012-06-25T22:59:37Z" id="6562527">This seems to be closed: https://github.com/elasticsearch/elasticsearch/issues/1862
</comment><comment author="kimchy" created="2012-06-26T00:43:43Z" id="6564189">Closing then, missed this one and opened another issue by mistake...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API: Multi Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1722</link><project id="" key="" /><description>The multi search API allows to execute several search requests within the same API. The endpoint for it is `_msearch`. 

The format of the request is similar to the bulk API format, and the structure is:

```
header\n
body\n
header\n
body\n
```

The header part includes which index / indices to search on, optional (mapping) types to search on, the `search_type`, `preference`, and `routing`. The body includes the typical search body request (including the `query`, `facets`, `from`, `size`, and so on). Here is an example:

```
$ cat requests
{"index" : "test"}
{"query" : {"match_all" : {}}, "from" : 0, "size" : 10}
{"index" : "test", "search_type" : "count"}
{"query" : {"match_all" : {}}}
{}
{"query" : {"match_all" : {}}}

{"query" : {"match_all" : {}}}
{"search_type" : "count"}
{"query" : {"match_all" : {}}}

$ curl -XGET localhost:9200/_msearch --data-binary @requests; echo
```

Note, the above includes an example of an empty header (can also be just without any content) which is supported as well.

The response returns a `responses` array, which includes the search response for each search request matching its order in the original multi search request. If there was a complete failure for that specific search request, an object with `error` message will be returned in place of the actual search response.

The endpoint allows to also search against an index/indices and type/types in the URI itself, in which case it will be used as the default unless explicitly defined otherwise in the header. For example:

```
$ cat requests
{}
{"query" : {"match_all" : {}}, "from" : 0, "size" : 10}
{}
{"query" : {"match_all" : {}}}
{"index" : "test2"}
{"query" : {"match_all" : {}}}

$ curl -XGET localhost:9200/test/_msearch --data-binary @requests; echo
```

The above will execute the search against the `test` index for all the requests that don't define an index, and the last one will be executed against the `test2` index.
</description><key id="3297709">1722</key><summary>API: Multi Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC3</label></labels><created>2012-02-20T16:56:14Z</created><updated>2012-02-20T20:02:47Z</updated><resolved>2012-02-20T16:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>improve fetch optimizations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1721</link><project id="" key="" /><description>From mailing list:

"there is an optimization to not check each mapping specifically per hit, but try to guess what to fetch on the "search" level (and not per hit) and then use it for all the hits. We can improve it though."

This is in response to an issue where documents from 2 different types containing the same fields names and different "stored" settings, do not return the requested fields from the source because ES thinks they are already stored (as they are in the first type).
</description><key id="3296376">1721</key><summary>improve fetch optimizations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2012-02-20T15:30:19Z</created><updated>2014-07-18T09:00:00Z</updated><resolved>2014-07-18T09:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T09:00:00Z" id="49409714">Closed in favor of #4081 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchRequestBuilder.toString() prints only the body of the request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1720</link><project id="" key="" /><description>As the title says the toString() is not giving the complete picture of the request like the document type and index that is set in the requestbuilder. Right now its only printing the body of the request. 
</description><key id="3276432">1720</key><summary>SearchRequestBuilder.toString() prints only the body of the request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">georgel2004</reporter><labels /><created>2012-02-18T00:52:14Z</created><updated>2013-09-20T20:02:53Z</updated><resolved>2013-09-20T20:01:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-09-11T12:37:06Z" id="24236247">True, we print the internal search source, which more or less represents the request body when using the REST api. The reason for that is that we have it stored in a way that makes it easy to print it out as a json object for debug purposes, which is not the case when it comes to other properties like type, index, and many more.

I personally wouldn't rely too much on the `toString` method if I were you and I would rather avoid to add every bit of information to it. You can always get back the `SearchRequest` object through the `request()` method and read all the properties that you need using the available methods (e.g. indices, types). Would that be reasonable for you?
</comment><comment author="javanna" created="2013-09-20T20:01:55Z" id="24837538">Closing this one. The `toString` method returns the search request body, the missing bits (e.g. indices, types) can be taken from the `SearchRequest` which can be retrieved using the `request()` method.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add refresh support to update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1719</link><project id="" key="" /><description /><key id="3275873">1719</key><summary>add refresh support to update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-02-17T23:46:12Z</created><updated>2014-07-16T21:55:39Z</updated><resolved>2012-02-18T19:11:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Local Gateway: Delete dangling indices after a (configurable) timeout, and not immediately </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1718</link><project id="" key="" /><description>Dangling indices can happen for example when one starts a single fresh node (with no "recover after" settings), which goes on and forms a single node cluster with no indices. Then, another node is started, which does hold certain data. The node joins the cluster, and currently, its data will be deleted since there are no indices in the cluster. Those indices that the other node has are dangling indices.

We should try and help and protect against a mistake a user makes and not immediately delete dangling indices. We can do that now (0.19) thanks to the fact that local gateway state is stored per index / shard.

Add a setting, `indices.store.dangling_timeout`, which defaults to `2h` (2 hours). When a dangling index is detected, an INFO level logging will be logged explaining that the relevant dandling index will be deleted in the set timeout.

The timeout can be set to `0` to delete it immediately, or to `-1` to never delete dangling indices.
</description><key id="3275856">1718</key><summary>Local Gateway: Delete dangling indices after a (configurable) timeout, and not immediately </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-17T23:43:51Z</created><updated>2015-12-07T13:22:24Z</updated><resolved>2012-02-17T23:44:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jhnlsn" created="2012-03-16T15:46:08Z" id="4542048">Looks like the default value of this setting is 2m.  Just lost 16g of data due to this very problem and saw the info message in the logs, tried to bring the node down before it got deleted but it was just too late.  anyways to set this to default to -1 to never delete?  I think index auto deletion is a very very bad thing especially when multicast is default.
</comment><comment author="kimchy" created="2012-03-16T15:52:48Z" id="4542190">I will change the default value to 2 hours, I think it will be better. It was a mistake to have it set by default to `2m`....
</comment><comment author="shekharonlin" created="2014-05-20T15:59:35Z" id="43645631">My cluster has just one node. How can I set the value of 'indices.store.dangling_timeout' to 0 as I dont want any dangling indices to be imported. Please guide.
</comment><comment author="vineet01" created="2015-12-07T12:08:26Z" id="162506815">As is obvious setting dangling_timeout to 0 doesn't seem like a good idea. In my setup, i have a 3 node cluster, so if i delete one index while one of node is down, index gets re-created when that node comes up, and cluster health turns to red (it shoes unassigned_shards 9), i have set number of replicas to be 2. If i close, delete that dangling index, cluster health turns green. I will like to know what is the suggested best practice to handle dangling indices? 
</comment><comment author="bleskes" created="2015-12-07T13:22:24Z" id="162522517">@vineet01 for now, this is a choice between having erroneously deleted if a master is elected without access to the old cluster state (people forget to configure minimum master nodes, bring down their cluster for maintenance and bring it up with new nodes, but the new nodes start first and become master) and having the annoyance of a deleted index re-appear if a node was down while it was deleted. Longer term we have some plans to be smarted, but for now we choose for the later... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed shards allocation can "poison" allocation and might cause not allocating other shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1717</link><project id="" key="" /><description>Its pretty rare, but might happen, specifically when having many indices (and many shards) and if some shards fail, it might end in a poison state where failed shards are retried and because of shards concurrent initialization limits per node, the same might be attempted to start each time.

Obviously, it would be great to have something that identified that a specific shard has failed on all its different replicas, and then throttle its attempted allocation to a later time, but thats a different issue.
</description><key id="3274848">1717</key><summary>Failed shards allocation can "poison" allocation and might cause not allocating other shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-17T22:11:34Z</created><updated>2012-02-17T22:20:08Z</updated><resolved>2012-02-17T22:20:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add update integration tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1716</link><project id="" key="" /><description /><key id="3274840">1716</key><summary>add update integration tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-02-17T22:10:55Z</created><updated>2014-07-16T21:55:39Z</updated><resolved>2012-02-18T19:10:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add missing setPercolate method to UpdateRequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1715</link><project id="" key="" /><description /><key id="3269369">1715</key><summary>add missing setPercolate method to UpdateRequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-02-17T15:45:49Z</created><updated>2014-07-16T21:55:40Z</updated><resolved>2012-02-17T17:53:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add mandatory plugins support in conf</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1714</link><project id="" key="" /><description /><key id="3267583">1714</key><summary>add mandatory plugins support in conf</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2012-02-17T13:46:02Z</created><updated>2014-07-07T22:21:59Z</updated><resolved>2012-02-18T19:06:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-17T17:54:56Z" id="4024622">Can you change the setting to be `plugin.mandatory` (or something like that), but not prefixed by `node`.
</comment><comment author="Paikan" created="2012-02-17T21:55:13Z" id="4028568">Done!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return 503 for search against a node which is not yet ready</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1713</link><project id="" key="" /><description>Issue #1633 changed a request to / to return 503 if the node is not yet able to serve requests.

It'd be helpful to have that same error code returned for any request if the node is not yet ready.  That'd make it easier to mark that node as temporarily unavailable, and to automatically retry the next node in the list.

Currently, we'd have to run a test GET / for each node before using it, and if the node restarts, we could have incorrect outdated node status info in the API (meaning that future requests would throw an error, rather than retrying).

eg, set config:

```
gateway:
    recover_after_nodes:                2
    expected_nodes:                     2
```

then start only 1 node.

/ returns 503 correctly:

```
curl -XGET 'http://127.0.0.1:9200/?pretty=1' 
{
  "ok" : true,
  "status" : 503,
  "name" : "Tomorrow Man",
  "version" : {
    "number" : "0.19.0.RC3",
    "snapshot_build" : true
  },
  "tagline" : "You Know, for Search"
}
```

/_search returns 500 incorrectly:

```
curl -XGET 'http://127.0.0.1:9200/_search?pretty=1' 
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [initial], No indices / shards to search on, requested indices are []]",
  "status" : 500
}    
```
</description><key id="3266940">1713</key><summary>Return 503 for search against a node which is not yet ready</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-17T12:43:13Z</created><updated>2012-02-17T13:20:28Z</updated><resolved>2012-02-17T13:20:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolators: Support delete-by-query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1712</link><project id="" key="" /><description>The error doesn't happen when I delete the queries one by one.

It only happens when you delete the percolator index itself for a index.

Gist for reproducing the error:

https://gist.github.com/1678150
</description><key id="3259147">1712</key><summary>Percolators: Support delete-by-query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">onilton</reporter><labels><label>enhancement</label><label>stalled</label></labels><created>2012-02-16T21:50:43Z</created><updated>2014-08-28T23:23:05Z</updated><resolved>2014-07-28T09:29:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timjnh" created="2012-02-17T15:37:42Z" id="4022147">Can't say I tested deleting the percolator index itself but I have run across this issue when deleting _percolator queries by query rather than by id.
</comment><comment author="ajhalani" created="2012-03-13T19:12:49Z" id="4483177">I have encountered this bug too. 
</comment><comment author="ajhalani" created="2012-03-20T13:52:33Z" id="4595573">+1, encountered it too.
</comment><comment author="anweibel" created="2012-07-13T15:19:15Z" id="6965862">+1, encountered it too.

We create the percolators each night anew during re-indexing. It would be great if we could just delete the _percolator index in order to delete all percolators. This doesn't work, as described above.

Instead, our workaround is:
1) lookup all percolators
2) delete each percolator individually

That's neither elegant nor efficient, but it works so far.
</comment><comment author="georgi0u" created="2012-12-21T20:56:32Z" id="11626745">+1, also experiencing this issue. 
</comment><comment author="julianhille" created="2013-02-20T09:32:12Z" id="13823045">Same problem here. The index as the type of the index deleting does nothing.
Its really weird, because trying to get the percolator by id says exist false. Deleting by id says exist false but also tells that deletion was successful.
</comment><comment author="cjbottaro" created="2013-05-02T22:31:39Z" id="17369026">The workaround doesn't work for me.  Searching for percolator documents yields nothing:

```
$ curl -X GET 'http://localhost:9200/_percolator/_search?pretty' -d '{}'
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

But then percolating a document yields results...

```
$ curl -X GET "http://localhost:9200/scholarship_manager/dev%7EApplication%3A%3AAutomatch/_percolate?pretty" -d '{"doc":{"form_field_34":4.0,"type":"dev~Application::Automatch"}}'
{
  "ok" : true,
  "matches" : [ "opportunity_46:qualification_group_1:qualification_1", "dev~opportunity_46:qualification_group_1:qualification_1" ]
}
```

Not sure how to get out of this state as deleting the _percolator index does nothing.  I'm using Elasticsearch 0.20.6.
</comment><comment author="spinscale" created="2013-05-22T08:10:04Z" id="18263259">Deleting a type from the percolator index via

```
curl -X DELETE localhost:9200/_percolator/myindex
```

internally issues a delete-by query, which succeeds, but does not update the registered percolator queries. This is the reason for both failing at the moment.
</comment><comment author="JimminiKin" created="2013-08-01T13:25:47Z" id="21935815">So, once a percolator has been "deleted" the wrong way, it's stuck forever in the system?

Is there any solution to work around this issue ? Or do I have to start over on a clean setup ?
</comment><comment author="georgi0u" created="2013-08-01T13:31:42Z" id="21936179">Restarting the cluster is a temporary solution.

On Thu, Aug 1, 2013 at 9:26 AM, Jimmy Thomas notifications@github.comwrote:

&gt; So, once a percolator has been deleted the wrong way, it's stuck forever
&gt; in the system?
&gt; 
&gt; Is there any solution to work around this issue ? Or do I have to start
&gt; over on a clean setup ?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/1712#issuecomment-21935815
&gt; .

## 

adamgeorgiou.com
</comment><comment author="JimminiKin" created="2013-08-01T13:36:33Z" id="21936467">Oh yes, thanks !

Recreating the percolators and deleting them one by one also worked.
</comment><comment author="martijnvg" created="2013-08-05T14:55:41Z" id="22111477">When a type is being deleted, under the hood a delete by query is executed to delete all document that have the type you are deleting. Unfortunately the percolator doesn't support delete by query (only normal deletes) and therefore the loaded queries aren't updated.
</comment><comment author="cjbottaro" created="2013-08-05T14:59:27Z" id="22111729">When will the percolator support delete by query?
</comment><comment author="martijnvg" created="2013-08-05T19:50:39Z" id="22136073">In order to update the percolator queries in realtime when a delete by query gets executed, the delete by query needs to be implemented differently. Not sure when this will be done.

The new percolator in master (soon to be 1.0 beta1), does update the in memory percolate queries in realtime when a percolate index or the `_percolator` type of a percolate index gets removed. (it doesn't rely on delete by query to do this)
</comment><comment author="clintongormley" created="2014-07-08T14:19:48Z" id="48342198">The old percolator has been removed. Closing
</comment><comment author="matthuhiggins" created="2014-07-10T06:33:49Z" id="48569907">I can still reproduce this problem with the new percolator on v1.2.1. The registered percolators are stuck in the registry, even after issuing a delete query.

``` Shell
curl -XPUT 'localhost:9200/my-index/.percolator/1' -d '{
    "query" : {
        "match" : {
            "message" : "bonsai tree"
        }
    }
}'
```

At this point, the following returns '1' in the hits:

``` Shell
curl -XGET 'localhost:9200/my-index/.percolator/_search' -d '{
    "query" : {
        "match_all" : {}
    }
}'
```

After issuing a delete all query, the percolator no longer shows up in the search:

``` Shell
curl -XDELETE 'localhost:9200/my-index/.percolator/_query' -d '{
    "query" : {
        "match_all" : {}
    }
}'
```

```
curl -XGET 'localhost:9200/my-index/.percolator/_search' -d '{
    "query" : {
        "match_all" : {}
    }
}'
```

However, the percolator continues to show up as a result in percolate runs. The following returns the percolator with id '1':

```
curl -XGET 'localhost:9200/my-index/message/_percolate' -d '{
    "doc" : {
        "message" : "A new bonsai tree in the office"
    }
}'
```
</comment><comment author="clintongormley" created="2014-07-10T12:01:50Z" id="48595549">Yeah sorry - closed this without testing.  Delete by query doesn't work with percolators: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html#_important_notes
Or at least it works, but only deregisters percolators once the index has been closed.

I'll reopen
</comment><comment author="clintongormley" created="2014-07-18T08:56:22Z" id="49409399">Blocked by #2230 
</comment><comment author="clintongormley" created="2014-07-28T09:29:20Z" id="50316773">Closing in favour of #7052 
</comment><comment author="donwalrus" created="2014-08-28T23:23:05Z" id="53820023">has there been a fix proposed for this, or just to avoid deleting percolators with a filter query? 

version: {
number: 1.1.1
build_hash: f1585f096d3f3985e73456debdc1a0745f512bbc
build_timestamp: 2014-04-16T14:27:12Z
build_snapshot: false
lucene_version: 4.7
}
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_validate/query no longer working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1711</link><project id="" key="" /><description>```
# [Thu Feb 16 22:28:34 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPUT 'http://127.0.0.1:9200/foo/?pretty=1' 

# [Thu Feb 16 22:28:34 2012] Response:
# {
#    "ok" : true,
#    "acknowledged" : true
# }

# [Thu Feb 16 22:28:36 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/foo/bar?pretty=1'  -d '
{
   "text" : "foo"
}
'

# [Thu Feb 16 22:28:36 2012] Response:
# {
#    "ok" : true,
#    "_index" : "foo",
#    "_id" : "mqluWX4wQLmMkP5i2x5WZQ",
#    "_type" : "bar",
#    "_version" : 1
# }

# [Thu Feb 16 22:28:38 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/_all/_validate/query?pretty=1&amp;q=*' 

# [Thu Feb 16 22:28:38 2012] Response:
# {
#    "status" : 404,
#    "error" : "IndexMissingException[[_all] missing]"
# }

# [Thu Feb 16 22:28:44 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/foo/_validate/query?pretty=1&amp;q=*' 

# [Thu Feb 16 22:28:44 2012] Response:
# {
#    "status" : 400,
#    "error" : "InvalidTypeNameException[mapping type name [_validate] can't start with '_']"
# }

# [Thu Feb 16 22:28:47 2012] Protocol: http, Server: 192.168.5.10:9200
curl -XPOST 'http://127.0.0.1:9200/foo/bar/_validate/query?pretty=1&amp;q=*' 

# [Thu Feb 16 22:28:47 2012] Response:
# No handler found for uri [/foo/bar/_validate/query?q=*] and metho&gt; d [POST]
```
</description><key id="3258679">1711</key><summary>_validate/query no longer working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2012-02-16T21:15:41Z</created><updated>2012-08-03T19:17:24Z</updated><resolved>2012-02-17T12:28:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-02-17T12:28:43Z" id="4019378">My fault - I had a bad version compiled locally.  It is working.
</comment><comment author="mdoroudi" created="2012-06-22T01:07:10Z" id="6498539">Hi,
I wonder what 'bad version' was compiled locally? I'm having similar problem with _bulk
InvalidTypeNameException[mapping type name [_bulk] can't start with '_'

Thanks!
</comment><comment author="mahemoff" created="2012-08-03T19:17:24Z" id="7491542">@mdoroudi I had the same thing and it turns out to be a version issue. The problem was that homebrew didn't update itself, so homebrew didn't know the latest elasticsearch.

So I basically did:

brew update
brew uninstall elasticsearch
brew install elasticsearch   # remove and install because just trying to update it didn't do anything

Make sure to restart elasticsearch too. For some reason, when I re-started elasticsearch according to instructions, the old one was still running. So I just kill -9'd the old instance and ran again. Finally bulk works again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: collect and expose index-level cache stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1710</link><project id="" key="" /><description>Expose index-level cache stats via HTTP/JSON + JMX.

See thread:
https://groups.google.com/group/elasticsearch/browse_thread/thread/93ee6d01e18f74e6
</description><key id="3255677">1710</key><summary>Stats: collect and expose index-level cache stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels /><created>2012-02-16T18:12:03Z</created><updated>2014-07-08T14:19:21Z</updated><resolved>2014-07-08T14:19:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-17T11:27:41Z" id="4018759">A quick note, this is a bit complicated to do currently, since the cache stats are stored on "index" level abstraction in a node, and not shard level (simpler to use in this case, still segment based though). So, getting totals is difficult to do. We can potentially get the index cache size, and divide it by the number of shards active for that index in the node, to get an average per shard on that node. 
</comment><comment author="clintongormley" created="2014-07-08T14:19:21Z" id="48342128">Index-level cache stats are now available in the indices and nodes stats APIs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>top_children query returns no results when child field has the same name as a nested field as a nested field in the parent mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1709</link><project id="" key="" /><description>What reproduces the problem is when you have a field, say, "body", in the child type, and you're searching on that field in `top_children`. If the parent type also has a nested field with the name "body", then the `top_children` query will return no results; the same `has_child` query/filter does return results.

Here's a reduction (or [download](https://gist.github.com/1846196)):

```
curl -XDELETE localhost:9200/test?pretty=true
echo ""
curl -XPUT localhost:9200/test?pretty=true
echo ""
curl -XPUT localhost:9200/test/parent/_mapping?pretty=true -d '{"parent":{"properties":{"name":{"type":"string","index":"analyzed"},"notes":{"properties":{"body":{"type":"string"}}}}}}'
echo ""
curl -XPUT localhost:9200/test/child/_mapping?pretty=true -d '{"child":{"_parent":{"type":"parent"}, "properties":{"body":{"type":"string","index":"analyzed"}}}}'
echo ""
curl -XPUT localhost:9200/test/parent/1?pretty=true -d "{\"name\":\"parent document\"}"
echo ""
curl -XPOST 'localhost:9200/test/child/_create?parent=1&amp;pretty=true&amp;refresh=true' -d '{"body":"pizza monster"}'
echo ""
curl -XGET localhost:9200/test/parent/_search?pretty=true -d '{"query":{"top_children":{"type":"child","query":{"query_string":{"query":"pizza monster","fields":["body"]}}}}}'
echo ""
curl -XGET localhost:9200/test/parent/_search?pretty=true -d '{"query":{"has_child":{"type":"child","query":{"query_string":{"query":"pizza monster","fields":["body"]}}}}}'
echo ""
```
</description><key id="3253800">1709</key><summary>top_children query returns no results when child field has the same name as a nested field as a nested field in the parent mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">outoftime</reporter><labels><label>bug</label><label>v0.19.0.RC3</label></labels><created>2012-02-16T16:22:11Z</created><updated>2012-05-29T15:12:24Z</updated><resolved>2012-02-19T13:13:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="powmedia" created="2012-05-29T15:12:24Z" id="5986299">Has this fix been released? I'm getting the same problem when using a filter on a top level attribute where there is a child with the same attribute name.

I'm on v19.4 OSX 64bit
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Mapping: Support "date math" when searching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1708</link><project id="" key="" /><description>Date math on `date` types allows to compose search values that include simple data math expressions.

The expression starts with an "anchor" date, which can be either `now` or a date string (in the applicable format) ending with `||`. It can then follow by a math expression, supporting `+`, `-` and `/` (rounding). The units supported are `M` (month), `w` (week), `h` (hour), `m` (minute), and `s` (second).

Here are some samples: `now+1h`, `now+1h+1m`, `now+1h/d`, `2012-01-01||+1M/d`.

Now, when doing `range` type searches, and the upper value is inclusive, the rounding will properly be rounded to the ceiling instead of flooring it.
</description><key id="3253610">1708</key><summary>Date Mapping: Support "date math" when searching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.19.0.RC3</label></labels><created>2012-02-16T16:09:06Z</created><updated>2015-06-17T08:21:18Z</updated><resolved>2012-02-16T16:10:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeremy" created="2012-02-16T18:08:34Z" id="4005976">Killer feature. This is very convenient. :metal:
</comment><comment author="trekr5" created="2015-06-17T08:18:31Z" id="112713301">How would you use date math to find values from a previous day's index? would you have to put in the actual date or could you use things like "midnight", "yesterday"?

Say I wanted to find values for an es query for yesterday with a moving window of 2 hours? would I put in my date range "gt"=&gt; "2015-06-16||-2h/d"
</comment><comment author="markwalkom" created="2015-06-17T08:21:18Z" id="112714210">@trekr5 Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed #1706</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1707</link><project id="" key="" /><description>Fixed StackOverflowError when accessing NodeIndicesStats.refresh()
</description><key id="3252074">1707</key><summary>Fixed #1706</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels /><created>2012-02-16T14:30:35Z</created><updated>2014-07-16T21:55:40Z</updated><resolved>2012-02-16T19:15:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>StackOverflowError when accessing NodeIndicesStats.refresh()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1706</link><project id="" key="" /><description>Accessing NodeIndicesStats.refresh() leads to a StackOverflowError. Looking at the source, it seems a trivial copy / paste problem that keeps calling the method in a loop.

``` java
    public RefreshStats refresh() {
        return this.refresh();
    }

    public RefreshStats getRefresh() {
        return this.refresh();
    }
```
</description><key id="3251260">1706</key><summary>StackOverflowError when accessing NodeIndicesStats.refresh()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jfiedler</reporter><labels><label>bug</label><label>v0.19.0.RC3</label></labels><created>2012-02-16T13:40:30Z</created><updated>2012-02-16T19:15:36Z</updated><resolved>2012-02-16T19:15:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-16T19:15:36Z" id="4007239">Pushed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_source field not being returned when a script field is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1705</link><project id="" key="" /><description>Hiya, in 0.19.0RC2, the `fields` param is being ignored.

Compare v 0.19RC2:

```
curl -XGET 'http://127.0.0.1:9200/geonames/_search?pretty=1'  -d '
{
   "fields" : [
      "_source"
   ],
   "script_fields" : {
      "distance" : {
         "params" : {
            "lat" : 2.27,
            "lon" : 50.3
         },
         "script" : "doc[\u0027location\u0027].distanceInKm(lat,lon)"
      }
   }
}
'

# [Thu Feb 16 11:26:29 2012] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "fields" : {
#                "distance" : 466.844095463887
#             },
#             "_index" : "geonames_1318324623",
#             "_id" : "6436641_en",
#             "_type" : "place"
#          },
```

With 0.17:

```
{
  "hits" : {
    "total" : 1869924,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "geonames2_1318699167",
      "_type" : "place",
      "_id" : "3096514_en",
      "_score" : 1.0, 
      "_source" : {"location":{"lat":"15.53226","lon":"53.69808"},"parent_ids":["3337499","798544"],"tokens":[{"tokens":["karwowo"]},{"tokens":["karwowo","west","pomeranian","voivodeship","poland"]}],"_boost":0.5,"context":"/en","place_id":"3096514","label_short":"Karwowo","label":"Karwowo, West Pomeranian Voivodeship, Poland","rank":1},
      "fields" : {
        "distance" : 1524.010379445455
      }
    }, 
```
</description><key id="3248814">1705</key><summary>_source field not being returned when a script field is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2012-02-16T10:30:06Z</created><updated>2012-02-16T20:42:33Z</updated><resolved>2012-02-16T20:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-16T19:21:26Z" id="4007381">I just pushed a fix, can you double check if it works?
</comment><comment author="clintongormley" created="2012-02-16T20:42:33Z" id="4008969">Works for me - ta!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: When using `_all` for types, field name/type resolution might fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1704</link><project id="" key="" /><description /><key id="3234457">1704</key><summary>Search: When using `_all` for types, field name/type resolution might fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC3</label></labels><created>2012-02-15T13:41:25Z</created><updated>2012-02-15T13:50:30Z</updated><resolved>2012-02-15T13:50:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Templates: Allow to place them under config/templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1703</link><project id="" key="" /><description>Allow to place index templates under the `config` location within the `templates` directory. For example, a file called `config/templates/tempalte_1.json` can be placed and it will be applied when it matches an index creation. A sample content for the mentioned file can be: 

```
{
    "template_1" : {
        "template" : "*",
        "settings" : {
            "index.number_of_shards" : 2
        },
        "mappings" : {
            "_default_" : {
                "_source" : {
                    "enabled" : false
                }
            },
            "type1" : {
                "_all" : {
                    "enabled" : false
                }
            }
        }
    }
}
```
</description><key id="3232064">1703</key><summary>Index Templates: Allow to place them under config/templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-15T09:45:33Z</created><updated>2012-03-19T07:57:41Z</updated><resolved>2012-02-15T09:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-03-19T07:57:41Z" id="4569054">this feature is very useful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Retrieving byte type stored field fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1702</link><project id="" key="" /><description>If you write a single java.lang.Byte type via json, it can't be read back.

The following error occurs:
Failed to execute phase [query_fetch], total failure; shardFailures {RemoteTransportException[[Spot][inet[/192.168.0.124:9300]][search/phase/query+fetch]]; nested: IOException[Can't write type [class java.lang.Byte]]; }
## 
## The following code produces the error:

Index-Mapping (template)
...
 "ct" : {
                    "type" : "byte",
                    "store" : "yes",
                    "index" : "no"
}
## ...
## Writing the document:

XContentBuilder jsonB = jsonBuilder();
jsonB.startObject();
...
jsonB.field("ct", (byte)5);
jsonB.endObject();
...
es.getClient().prepareIndex(INDEX_DOCUMENTS, "document",uid).setSource(jsonB).execute().actionGet();

---
## Reading back:

QueryBuilder b = QueryBuilders.queryString(str).defaultOperator(Operator.OR).allowLeadingWildcard(false).useDisMax(true); 
SearchRequestBuilder request = es.getClient().prepareSearch(INDEX_DOCUMENTS)
                    .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                    .addFields("ct")
                    .setFrom(n).setSize(limit)
                    .setQuery(b);
</description><key id="3231953">1702</key><summary>Retrieving byte type stored field fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marchh</reporter><labels><label>bug</label><label>v0.19.0.RC3</label></labels><created>2012-02-15T09:33:05Z</created><updated>2012-02-15T13:28:09Z</updated><resolved>2012-02-15T13:28:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>CustomFiltersScoreQueryBuilder not recognizing _score?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1701</link><project id="" key="" /><description>After updating to RC2, I'm getting the following error when using:
CustomFiltersScoreQueryBuilder sqb = ...

...
sqb.add(colorRangeFilter, "_score \* 2");

&gt; Error: unresolvable property or identifier: _score

Anything changed in custom filters score?
</description><key id="3228793">1701</key><summary>CustomFiltersScoreQueryBuilder not recognizing _score?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heyarny</reporter><labels /><created>2012-02-15T00:51:38Z</created><updated>2012-05-03T14:27:07Z</updated><resolved>2012-02-15T16:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-15T10:58:12Z" id="3978388">Yes, because of the addition of `multiply` score mode, then the script provided is the _factor_ that the score will eventually be multiplied with, so referring to the _score no longer makes sense. Also, if all you do is "boost", then use the boost option, which is faster: add(filter, float boost).
</comment><comment author="heyarny" created="2012-02-15T16:07:19Z" id="3983005">ok, works again.
Yes, but I just simplified my script here.

It would be probably good to have some pre-pared constants.
Something like

&gt; sqb.scoreMode(CustomFiltersScoreQueryBuilder.SCOREMODE_MUTIPLY);

Thanks
</comment><comment author="unikoid" created="2012-04-28T11:51:50Z" id="5396346">If I want to add some number based on document type to the _score of document, how can I do it?
I tryed next query:

``` javascript
{

  "query": {
    "custom_filters_score": {
      "query": {
        "text": {
          "name": "my_word"
        }
      },
      "filters": [
        {
          "filter": {
            "type": {
              "value": "type1"
            }
          },
          "script": "_score+100"
        },
        {
          "filter": {
            "type": {
              "value": "type2"
            }
          },
          "script": "_score + 50"
        }
      ],
      "score_mode": "first"
    }
  }
}
```

Obviously it doesn't work.
Yes, I can store an additional value for each type of document and use custom_score query, but is it really good idea?
</comment><comment author="kimchy" created="2012-04-29T14:52:32Z" id="5405477">@unikoid There is no option to add to the score, only multiply the score by the result of the boost / script. Additions don't really make sense when it comes to scoring..., weighting (multiplications) do...
</comment><comment author="unikoid" created="2012-05-03T14:27:07Z" id="5488744">So, what is the right way to place matches of one type higher than others, for example?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pypes for ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1700</link><project id="" key="" /><description>Alll,

I played around with Pypes http://www.pypes.org/ about a year ago and really found it to be a powerful open source front end for something like a search index. Its basically the open source alternative to Yahoo! Pipes. What would it take to get the same thing working with ElasticSearch rather than Solr?

Adam
</description><key id="3228491">1700</key><summary>Pypes for ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geodawg</reporter><labels /><created>2012-02-15T00:20:20Z</created><updated>2013-05-27T16:43:26Z</updated><resolved>2013-05-27T16:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-16T19:26:42Z" id="4007479">Just by looking at the diagram on the site, probably writing a publisher there, not really much to do in ES itself.
</comment><comment author="mattweber" created="2012-02-22T20:38:37Z" id="4122050">Adam,  I am one of the maintainers of pypes and use it with ElasticSearch frequently.  I will work on doing a release with our ElasticSearch publisher included.  In the meantime, you can use the existing solr publisher along with my ElasticSearch plugin that mocks the Solr interface... https://github.com/mattweber/elasticsearch-mocksolrplugin.
</comment><comment author="geodawg" created="2012-02-22T20:40:35Z" id="4122190">w00t! I have been very much interested in Pypes since I first saw it (and
met you ) at the Lucene Revolution in Boston 2 years ago. Thanks for the
heads up and look forward to working with your code.

On Wed, Feb 22, 2012 at 3:38 PM, Matt Weber &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; Adam,  I am one of the maintainers of pypes and use it with ElasticSearch
&gt; frequently.  I will work on doing a release with our ElasticSearch
&gt; publisher included.  In the meantime, you can use the existing solr
&gt; publisher along with my ElasticSearch plugin that mocks the Solr
&gt; interface... https://github.com/mattweber/elasticsearch-mocksolrplugin.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1700#issuecomment-4122050
</comment><comment author="egaumer" created="2012-02-22T20:41:29Z" id="4122252">I'm one of the developers on pypes. All you need to do is write a publisher. In fact, we've been using elasticsearch since it was released and we have a publisher for it. We just never released it as part of the code base. I'll see that it makes it in there but it's really trivial to write (even more so than Solr and its clunky XML).
</comment><comment author="spinscale" created="2013-05-27T16:43:26Z" id="18506672">Closing this one, as it is not elasticsearch specific (except we need to add a feature in elasticsearch in order to get it up and running).

@mattweber @egaumer If you really want ES support, one of you will put it in I guess :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClassCastException - ShardScoreDoc cannot be cast to FieldDoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1699</link><project id="" key="" /><description>The item (and mapping) do not exist in one of the indexes but exists in the other indexes. A sort is performed.

Works fine when the sort is removed from the query.

Here is a gist.
https://gist.github.com/1820642
</description><key id="3209632">1699</key><summary>ClassCastException - ShardScoreDoc cannot be cast to FieldDoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels /><created>2012-02-13T21:34:57Z</created><updated>2013-02-15T20:10:21Z</updated><resolved>2012-02-15T22:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-14T10:56:02Z" id="3958633">Yes, this will not work when sorting on a field that exists on one index and not on another. There is already an issue opened to (somehow) support this.
</comment><comment author="bryangreen" created="2012-02-15T22:29:08Z" id="3990609">Sorry for the duplicate ticket. I'm pulling back the mappings now and just testing to see if the item exists in each index. Cheers!
</comment><comment author="vpernin" created="2013-02-15T20:10:21Z" id="13625612">I have to problem with a 0.20.2.
In the SearchPhaseController.java line 341, there is a cast :
if (sorted) {
                        FieldDoc fieldDoc = (FieldDoc) shardDoc;
shardDoc is an instance of ShardScoreDoc which does not implement FieldDoc like ShardFieldDoc does.

I searched the other issues unsuccessfuly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices query should accept alias names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1698</link><project id="" key="" /><description>If you pass an alias name to an indices query, instead of the real index name, then the `no_match_query` is applied instead of the `query`

I think that the `indices` key should be able to accept aliases and expand these to the real index names before executing.
</description><key id="3207824">1698</key><summary>Indices query should accept alias names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-13T19:44:44Z</created><updated>2012-02-17T13:04:02Z</updated><resolved>2012-02-17T13:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Split debian package in libelasticsearch and elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1697</link><project id="" key="" /><description>Hello,

When you have dependency on elasticsearch in java-app to instantiate client node for example, you rarely need to have an elasticssearch server running aside.

My idea is to package all the jars and lib dependencies in a "libelasticsearch" package to be deployed with java-app, and all the executables and init.d script in an "elasticsearch" package (depending on libelasticsearch) to be deployed on active cluster nodes.

Cheers.
</description><key id="3197535">1697</key><summary>Split debian package in libelasticsearch and elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dhardy92</reporter><labels /><created>2012-02-13T09:23:12Z</created><updated>2013-06-05T12:55:53Z</updated><resolved>2013-06-05T12:55:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-13T11:24:41Z" id="3938013">Most java programs would use mvn though to manage dependencies, no? Do people really use "libXXX" in Java/JVM based projects?
</comment><comment author="dhardy92" created="2012-02-13T11:39:49Z" id="3938217">certainly but as an adminsys if I want to upgrade an whole elasticsearch cluster, now I have to ask every webapp make a new release to do so and upgrade everything at the same time.
With an external package webapp can go with an "provided" scope package not released with webapp and system can upgrade libelasticsearch (without risk of starting a external node) and restart webapp to take effects (after some integrations tests of course).
</comment><comment author="nhuray" created="2012-02-13T14:36:13Z" id="3940665">IMO it makes sense to modularize the debian package with 2 modules : 
- libelasticsearch-java which contains java librairies for elasticsearch
- elasticsearch which contains services and configuration to run elasticsearch server

Many official debian package like Tomcat or Jetty are modularized like that as you can see here :  http://packages.debian.org/sid/jetty
</comment><comment author="jprante" created="2012-03-08T18:33:26Z" id="4398705">-1

libelasticsearch is not really a wise idea. First, there is no real library libelasticsearch at all, so it's misleading. Instead I'd love to see separate packages for the dependecies (guice, netty, gnu trove, joda etc.) and let the package manager resolve the dependencies. This is a lot of effort and double the Maven deps, but it could be worth it when the aim should be the integration of ES into Linux distributions -  they have strict packaging guidelines. Also, as far as I understand, Shay uses some (slightly) modified versions and moves them into the org.elasticsearch package hierarchy. This is different from Tomcat or Jetty. The conclusion is for me that preparing an ES package is preferable with Shay's modifications included. 
</comment><comment author="dhardy92" created="2012-03-08T19:19:55Z" id="4400177">The libelasticsearch package would be just the same that the one existing today but without config and init script.
That allow tiers java applications to depend on it easily without seeing a unwanted node starting beside the app and joining a ghost cluster. 
It also permit sysops to maintain versions of ES (for a whole cluster with tons of projets using it) without deploy a new version of all the tiers app using it (meaning supplementary job for devs to release) : just upgrade ES an rolling restart the apps it works for everybody and the whole cluster is up to date.
</comment><comment author="dhardy92" created="2012-03-28T10:16:59Z" id="4752495">Maybe one thing could be done: just prevent elastic search for starting up after startup, upgrade or installation.

in /etc/default/elasticsearch add :
AVAILABLE=False

in init script start with :

if [ ${AVAILABLE} == "False" ]; then
  exit;
fi

Libs are available for webapps and no service is started by default. you have to explicitly remove the variable in the /etc/default/elasticsearch to allow services start and joinb the cluster.

This could also be helpfull to set the right configuration (other than default) before starting the service.
</comment><comment author="spinscale" created="2013-06-05T10:39:16Z" id="18967877">hey, we came to the conclusion that we are going to leave it as one package for now and do not start splitting it up (which would contain only a few scripts).

Regarding the other issue about starting elasticsearch on package installation &amp; upgrades, I am not really satisfied with the current situation as well. I want something which applies to the RPM and the debian package as well, so they work in a similar manner (might be a sysconfig/default config option) - I will create a separate issue for that. Same applies for starting the package automatically after installation.

Will tackling the problem this way instead of creating two packages help you as well?
</comment><comment author="dhardy92" created="2013-06-05T12:37:02Z" id="18972711">Yes it does, install package as cold code usable as lib and as a service if needed.
Let the possibility to set it up with coordinator tools like puppet before starting it without creating unused /var/lib or /var/log files.
in fact that is the point in #2538
(I let you close this one)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Allow to execute search with no parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1696</link><project id="" key="" /><description>Basically resolves to match all query
</description><key id="3191070">1696</key><summary>Search: Allow to execute search with no parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-12T14:58:14Z</created><updated>2012-02-12T14:58:53Z</updated><resolved>2012-02-12T14:58:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Creating an index and alias in the same request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1695</link><project id="" key="" /><description>Hi,

As discussed in the mailing list, here is a feature request.

I would like to be able to define an alias for an index while I'm creating the index.

```
curl -XPUT http://localhost:9200/twitter/ -d ' 
{ 
  "settings": { 
    "number_of_shards": 3, 
    "number_of_replicas": 2 
  }, 
  "alias": [ 
    "myalias" 
    ] 
} 
'
```

It would be nice to have in the Java API also :

``` java
CreateIndexRequestBuilder cirb = client.admin().indices().prepareCreate(index);
cirb.setSettings(source);
// New method here
cirb.setAlias("myalias");
cirb.execute().actionGet();
```

Here is the signature expected for setAlias method : `setAlias(String... alias)`

Discussion here : https://groups.google.com/group/elasticsearch/browse_thread/thread/f60bff3fda0f93dc?hl=fr
</description><key id="3190830">1695</key><summary>Creating an index and alias in the same request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-02-12T13:58:18Z</created><updated>2014-02-19T11:18:49Z</updated><resolved>2014-02-19T11:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-02-12T20:57:50Z" id="3930948">Hi Shay,

I started to dig in code and modified CreateIndexRequestBuilder and CreateIndexRequest to add option.
Where to go next ? NodeIndicesAdminClient or InternalTransportIndicesAdminClient ?

Thanks for you help
David.
</comment><comment author="javanna" created="2014-02-19T11:18:49Z" id="35488407">This can be closed, implemented in #4920 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `T` (on top of `:`, `-`, and `/`) as a way to pre detect of a string is a date for dynamic date formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1694</link><project id="" key="" /><description /><key id="3190776">1694</key><summary>Add `T` (on top of `:`, `-`, and `/`) as a way to pre detect of a string is a date for dynamic date formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Dates</label><label>adoptme</label><label>enhancement</label></labels><created>2012-02-12T13:43:31Z</created><updated>2016-12-22T13:36:00Z</updated><resolved>2016-12-22T13:36:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-12T14:04:44Z" id="3927915">This is problematic, unless we find a good solution for it, since `1T` parses fine with the `dateOptionalTime` parser.
</comment><comment author="clintongormley" created="2014-08-08T09:49:32Z" id="51582662">Depends on #6227 
</comment><comment author="jpountz" created="2015-09-11T09:51:24Z" id="139503379">Removing the `stalled` label as #6227 is fixed now.
</comment><comment author="jpountz" created="2016-12-09T10:19:14Z" id="265980270">Discussed in FixitFriday: we might want to remove this list of characters entirely and test the list of dynamic date formats on every unmapped field. This sould fix #10961 as well. Performance is not a concern as mappings are expected to converge quickly as more documents are added.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add trim token filter that trims whitespaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1693</link><project id="" key="" /><description>The `trim` token filter allows to trim whitespaces. `update_offsets` (defaults to `false`) will cause offsets to also be updated.
</description><key id="3190349">1693</key><summary>Analysis: Add trim token filter that trims whitespaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC3</label></labels><created>2012-02-12T11:42:14Z</created><updated>2016-06-21T02:45:25Z</updated><resolved>2012-02-12T11:54:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="shavo007" created="2016-06-20T04:56:32Z" id="227050360">How do you apply this filter? I cant find any samples online! 
</comment><comment author="nik9000" created="2016-06-20T13:37:56Z" id="227144093">First: this kind of question is better to ask on https://discuss.elastic.co/.

Second, you apply it by making a [custom analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-custom-analyzer.html) and specifying the `trim` filter in the list of filters.
</comment><comment author="shavo007" created="2016-06-21T01:42:54Z" id="227318557">Possibly..

But I always find it strange on any github issue (no matter what project), there is an enhancement or fix closed without conveying to the end user how it works! 
</comment><comment author="jasontedor" created="2016-06-21T01:56:55Z" id="227320341">@shavo007 When you create a new issue, the issue template says:

&gt; GitHub is reserved for bug reports and feature requests. The best place to ask a general question is at the Elastic Discourse forums at https://discuss.elastic.co.

Every repository manages these things individually, this is how we manage ours (for example, Apache projects usually use an Apache-hosted Jira and user mailing lists, Apple Swift uses Jira and mailing lists too, and has issues locked on their repository). The community at our Discourse forum is very helpful, and you'll even see a lot of the same people there that you see here. 
</comment><comment author="shavo007" created="2016-06-21T02:45:25Z" id="227326355">I understand that. 

But based on my search term i came here! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.18 - use class-context class loader </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1692</link><project id="" key="" /><description>I updated the default class loader logic found in Classes to use the class-context class loader, rather than the thread context.  The reason for this change is that when using elasticsearch api within an osgi container, I get exceptions similar to those found in the gist https://gist.github.com/1805733.
</description><key id="3188696">1692</key><summary>0.18 - use class-context class loader </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trajar</reporter><labels /><created>2012-02-12T01:51:09Z</created><updated>2014-06-18T21:31:22Z</updated><resolved>2012-02-13T13:30:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-12T16:59:28Z" id="3929088">Where did you get problems with it? The thread context class loaded is the right one to use, specifically in web container situations (thats what Hibernate or Spring uses). You can override it by setting the class loader when you create the settings for a node.
</comment><comment author="trajar" created="2012-02-12T20:33:42Z" id="3930773">I'm running a bunch of index put/get operations via scheduled jobs and get ClassNotFoundExceptions thrown within the ThrowableObjectInputStream.  I'm using Quartz http://quartz-scheduler.org, so it seems the thread context for during these scheduled jobs is the class-context class loader associated with the Quartz module.  I could force/correct the class loader during this jobs - I'll see if that is a better solution (a workaround on my end).

I'm also getting exceptions in the Environment#resolveConfig method, as the config/names.txt cannot be found.  This is because when I setup the elasticsearch API, that is done within an activator in my code, which uses it's own thread-context (an its own class loader).  In order for the code Environment to work within an osgi container, lines 186 and 192 should be using the class-context class loader.

So if I injected the class loader during the settings builder process - that trickles down all the way to ThrowableObjectInputStream api?
</comment><comment author="kimchy" created="2012-02-12T21:16:47Z" id="3931115">No, it won't trickle to the ThrowableObjectInputStream, we might be able to fix that, but I wonder what exception is caused that is thrown from quartz and ends up in an elasticsearch stream..., what exactly are you doing? In any case, I am not going to apply your fix, since it only applies to your usecase and breaks other usecases. I see two possible fixes: 1. A quick one, is to add a system property that will control the behavior of the default class loader, and 2. Propagate the classloader provided in the settings to all the relevant places.
</comment><comment author="trajar" created="2012-02-12T21:21:41Z" id="3931167">@kimchy see the gist in the original post for the full stacktrace.  I'm attempting to retrieve an indexed object, and as it tries to unpack an object via the ThrowableObjectInputStream it throws class-not-found exceptions when trying to instantiate objects.  I understand that this fix might is a little too brute force - I'll see if I can find a way to address this in a better set of changes.
</comment><comment author="kimchy" created="2012-02-12T21:56:54Z" id="3931429">I've improved the class loading to use the class loader provided in the settings in all the applicable settings in master (0ff84d222f64f7f18f70d56d6f5cb5625ef47997). If we still need it, we can add the system property as well to the Classes, but we shouldn't need it now.
</comment><comment author="trajar" created="2012-02-12T23:36:58Z" id="3932325">Nope - agreed, your commit is great, much cleaner than a property setting solution.  How far is the master from the 18 release?  Do you think I could get by with most basic put/get/search operations if my elasticsearch cluster backend ran on 18 and my java-api client used master?
</comment><comment author="kimchy" created="2012-02-13T11:16:28Z" id="3937889">Master is the upcoming 0.19 release, there has been a 0.19.0.RC2 release already. You won't be able to run 0.18 code against it.
</comment><comment author="trajar" created="2012-02-13T13:30:18Z" id="3939636">Ok I'll work off the 0.19 release branch for the moment.  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>pom.xml contains dependency to filesystem jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1691</link><project id="" key="" /><description>When I download the pom.xml file, I can see in dependencies :

```
&lt;dependency&gt;
  &lt;groupId&gt;sigar&lt;/groupId&gt;
  &lt;artifactId&gt;sigar&lt;/artifactId&gt;
  &lt;version&gt;1.6.4&lt;/version&gt;
  &lt;scope&gt;system&lt;/scope&gt;
  &lt;systemPath&gt;/Users/kimchy/work/elasticsearch/elasticsearch/lib/sigar/sigar-1.6.4.jar&lt;/systemPath&gt;
  &lt;optional&gt;true&lt;/optional&gt;
&lt;/dependency&gt;
```

This makes Eclipse Maven plugin not loading the transitive dependencies.

```
12/02/12 00:32:56 CET: [WARN] The POM for org.elasticsearch:elasticsearch:jar:0.19.0.RC2 is invalid, transitive dependencies (if any) will not be available: 1 problem was encountered while building the effective model for org.elasticsearch:elasticsearch:0.19.0.RC2
[ERROR] 'dependencies.dependency.systemPath' for sigar:sigar:jar must specify an absolute path but is /Users/kimchy/work/elasticsearch/elasticsearch/lib/sigar/sigar-1.6.4.jar @ 
```

This dependency is generated from https://github.com/elasticsearch/elasticsearch/blob/master/pom.xml#L190

Is there any way to correct this ?

Thanks,
David
</description><key id="3188156">1691</key><summary>pom.xml contains dependency to filesystem jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-02-11T23:44:58Z</created><updated>2013-08-19T09:01:47Z</updated><resolved>2012-02-25T12:45:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2012-02-11T23:52:01Z" id="3924553">Perhaps I should only raise an issue for the Maven Eclipse plugin project to simply ignore errors on optional dependencies ?
</comment><comment author="kimchy" created="2012-02-12T17:03:18Z" id="3929120">Thats strange... . First, not sure why the pom gets rewritten to the full path of the sigar dependency, and to be honest, I did not find how to properly add an external optional dependency like sigar, that also includes native libs... .
</comment><comment author="dadoonet" created="2012-02-12T20:37:48Z" id="3930800">I found that we can deal with this issue with maven profiles.

I think that sigar dependency is only used when releasing a new version, right ?

So, if you put this in pom.xml, sigar dependency will be used when you release the project with mvn -Pes-release install
You have also to remove sigar dependency in the dependencies section.

```
    &lt;profiles&gt;
        &lt;profile&gt;
            &lt;id&gt;es-release&lt;/id&gt;

            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;sigar&lt;/groupId&gt;
                    &lt;artifactId&gt;sigar&lt;/artifactId&gt;
                    &lt;version&gt;1.6.4&lt;/version&gt;
                    &lt;scope&gt;system&lt;/scope&gt;
                    &lt;systemPath&gt;${basedir}/lib/sigar/sigar-1.6.4.jar&lt;/systemPath&gt;
                    &lt;optional&gt;true&lt;/optional&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
        &lt;/profile&gt;
    &lt;/profiles&gt;
```

What do you think ?
Do I send a pull request with that changes ?

BTW, I could add the following lines in the profile section :

```
            &lt;activation&gt;
                &lt;file&gt;
                    &lt;exists&gt;/Users/kimchy/work&lt;/exists&gt;
                &lt;/file&gt;
            &lt;/activation&gt;
```

It will activate the profile for you (and other users that have a file dir named /Users/kimchy/work ;-) ), so you will be able to make mvn install as usual... But, pom.xml could look strange for other users...

HTH
David.
</comment><comment author="cwensel" created="2012-02-12T20:57:43Z" id="3930944">sigar is a optional runtime dependency.

would be great if it could be pulled down as an artifact along with the native libs.

at a minimum, i'm happy to fetch it from a repo manually when I make a release build as I embed ES and don't want to lose the sigar features.
</comment><comment author="kimchy" created="2012-02-12T22:40:03Z" id="3931810">@dadoonet We need sigar to compile ES distribution as well (since it has code that depends on it, but it will work without sigar in the classpath at runtime).

@cwensel Yea, I agree. But, I don't know how to do it with maven (I don't think its possible)...
</comment><comment author="cwensel" created="2012-02-12T22:43:07Z" id="3931838">mvn stumps me, 

but maybe some additional classifier named "native" (like "sources") that can be listed as a dependency?
</comment><comment author="dadoonet" created="2012-02-13T07:08:45Z" id="3935361">@kimchy So when you build ES distribution, is it fine for you to add -Pes-release in your mvn command line ?
What is the mvn command you use to build distribution ?
</comment><comment author="karussell" created="2012-02-13T10:57:17Z" id="3937627">&gt; Perhaps I should only raise an issue for the Maven Eclipse plugin project to simply ignore errors on optional dependencies ?

After changing this to ${basedir}/lib/sigar/sigar-1.6.4.jar all should be fine. Or are there still problems?
Then maybe its only an eclipse issue? mvn and netbeans have no problems with that IMO
</comment><comment author="dadoonet" created="2012-02-13T11:28:04Z" id="3938083">In fact, I don't think that there is problem when you work directly with elasticsearch project opened in your eclipse IDE.
But, if you only define elasticsearch 0.19.0.RC2 in your own pom.xml project file, the pom.xml in maven repository doesn't link anymore to ${basedir} but to /Users/kimchy/work/elasticsearch/elasticsearch/.

See here : https://oss.sonatype.org/service/local/repositories/releases/content/org/elasticsearch/elasticsearch/0.19.0.RC2/elasticsearch-0.19.0.RC2.pom

David
</comment><comment author="karussell" created="2012-02-13T12:14:14Z" id="3938710">Uha, now I understand. Probably sigar.so needs to be deployed as maven dep as well via 

```
&lt;type&gt;so&lt;/type&gt;
```

? Then there is no need for system dep (I hope)

http://code.google.com/p/maven-android-plugin/wiki/NativeLibsAsDependencies

http://stackoverflow.com/q/7073039/194609
</comment><comment author="karussell" created="2012-02-13T12:30:47Z" id="3938904">sigar guys suggest downloading the zip file + configure via antrun :/

http://communities.vmware.com/thread/350252?tstart=180

But I found a config which looks good but you still need to add the nativ lib to the java lib path:

http://code.google.com/p/jodconverter/wiki/Configuration
</comment><comment author="dadoonet" created="2012-02-13T12:48:14Z" id="3939132">Looks like a nice config. Did you have time to try it ?
</comment><comment author="kimchy" created="2012-02-13T19:55:48Z" id="3947894">The solution should include placing the sigar jar and the native libs in the same location, so they will be loaded automatically without needing to mess with things like java.library.path.

But, as a quick possible fix, does someone know how to remove a dependency completely from a published pom? I would prefer just removing sigar for now from the generated pom file that gets published.
</comment><comment author="dadoonet" created="2012-02-13T20:01:45Z" id="3948032">Perhaps you could log in https://oss.sonatype.org with your jira account provided by sonatype and use the staging upload function. I think you can upload the pom.xml file only...
</comment><comment author="kimchy" created="2012-02-13T22:33:04Z" id="3951005">I could munge it manually, but want to have a proper process to do it.
</comment><comment author="kimchy" created="2012-02-14T10:54:38Z" id="3958617">Pushed using sigar from maven central, but just the jar, no natives. They are still packaged properly. Not a nice solution, but at least it will make the errors go away.
</comment><comment author="ppearcy" created="2012-02-14T17:38:39Z" id="3965799">Hmmm.. . I don't see any differences in the POM file here:
https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/elasticsearch/0.19.0.RC2/elasticsearch-0.19.0.RC2.pom

Should it be reflected there? 

Thanks,
Paul
</comment><comment author="kimchy" created="2012-02-14T18:18:24Z" id="3966574">@ppearcy no, I just pushed the change today, it will be part of the next release.
</comment><comment author="ppearcy" created="2012-02-15T21:10:30Z" id="3989177">I'd like to give this RC a spin. Still need to dig around more, but is there a way to tell mvn to still pull in the elasticsearch dependencies. 

Alternatively, I could add all the es dependencies into my pom.xml. 

Thanks!
</comment><comment author="dadoonet" created="2012-02-15T21:42:04Z" id="3989785">You can do something very bad :
Modify `elasticsearch-0.19.0.RC2.pom` in your `~/.m2/repository/org/elasticsearch/elasticsearch/0.19.0.RC2` dir and modify the line 

```
      &lt;systemPath&gt;/Users/kimchy/work/elasticsearch/elasticsearch/lib/sigar/sigar-1.6.4.jar&lt;/systemPath&gt;
```

It could work...
</comment><comment author="ppearcy" created="2012-02-15T23:22:58Z" id="3991789">Hehe, thanks David... Nice and hacky, but works! Appreciate the hint. 
</comment><comment author="dadoonet" created="2012-02-25T12:45:02Z" id="4172596">So it's now fixed in 0.19.0.RC3. Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_formats doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1690</link><project id="" key="" /><description>Changing default date format by using date_formats in type mapping doesn't work.

curl -XPUT http://localhost:9200/i
curl -XPUT http://localhost:9200/i/t/_mapping -d '{ "t" :  { "date_formats" : [ "basic_date_time" ]}}'
curl -XPUT http://localhost:9200/i/t/1 -d ' { "basic_date" : "20120209T180027.513+0100", "date_opt_time" : "2012-02-09T18:00:27.513+0100"} '
curl -XGET http://localhost:9200/i/t/_mapping

{"t":{"dynamic_date_formats":["basic_date_time"],"properties":{"basic_date":{"type":"string"},"date_opt_time":{"type":"string"}}}}

Field basic_date wasn't properly recognized as a date.

Related discussion:
https://groups.google.com/d/topic/elasticsearch/NeEtjBlOiPA/discussion
</description><key id="3171786">1690</key><summary>date_formats doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Avatah</reporter><labels /><created>2012-02-10T13:13:09Z</created><updated>2014-08-13T07:58:06Z</updated><resolved>2014-07-18T09:42:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-12T13:41:27Z" id="3927801">The problem here is that `:`, `-`, and `/` are used to first see if a string should be parsed as a date (otherwise, is `20120101` a date or a number?). We can improve things a bit and have `T` as part of the check as well.
</comment><comment author="kimchy" created="2012-02-12T13:49:32Z" id="3927838">I added #1694 for using `T` as well for date detection.
</comment><comment author="Avatah" created="2012-02-13T09:16:38Z" id="3936413">What if somebody defines an unusual date format as a default like 'dd.MM.yyyy' or one of the formats: year/year_month/year_month_day?
I think that if somebody wants to give a number, than he uses JSON int type, not a string.
Maybe all new string fields should be tried parsed as a date using currently defined date_formats? If date is misdetected, we can change default date_formats and configure it to be detected properly.
</comment><comment author="kimchy" created="2012-02-13T11:20:15Z" id="3937944">The string provided might not be a number at all, just a string that might have numeric values. The idea of the auto detection is to do the best job it can, for most use cases, but more importantly, have as little false positives as possible. You always have the option to explicitly mark fields as dates with the relevant very custom format.

That said, the date time package I use (Joda) feels too lenient, though I have not found a good way to have it fail more explicitly. If there is one, we can do strict parsing as well.
</comment><comment author="Avatah" created="2012-02-13T11:44:46Z" id="3938279">So maybe add new boolean property in mapping configuration with meaning "try to detect all string fields as dates", default false? Then it will be configurable if false positive detections or true negative detections are more harmful.
</comment><comment author="colings86" created="2014-07-18T09:42:07Z" id="49413245">Closing as the automatic mapping is a best effort attempt and incorrect mappings can be prevented by explicitly setting the format for each field in the mapping
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet for clustering geo_points</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1689</link><project id="" key="" /><description>elasticsearch supports bounding box queries and even has a distance
facet. But in order to show a large number of points on a map, it would be useful to also have a facet that clusters nearby points together.

The code at https://github.com/ejain/elasticsearch-geocluster provides a simple GeoClusterBuilder that generates GeoClusters from nearby GeoPoints, and a GeoClusterReducer that merges nearby clusters.

Here is some sample output for 1,000 points:
- https://www.google.com/fusiontables/DataSource?snapid=S390301uCMp (4000/2000km)
- https://www.google.com/fusiontables/DataSource?snapid=S3903022wAS (1000/500km)
- https://www.google.com/fusiontables/DataSource?snapid=S390401WTnD (500/250km)
</description><key id="3167580">1689</key><summary>Facet for clustering geo_points</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels /><created>2012-02-10T04:00:33Z</created><updated>2013-05-10T23:19:58Z</updated><resolved>2012-11-17T02:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dustMason" created="2012-09-25T03:19:32Z" id="8842377">this looks intriguing. can you share some info on how to implement your geocluster code? 
</comment><comment author="ejain" created="2012-11-17T02:13:18Z" id="10469422">I've published the geo_point clustering code as an elasticsearch plugin at https://github.com/zenobase/geocluster-facet. I'd be happy to assign the code to elasticsearch, if there is interest.
</comment><comment author="kimchy" created="2012-11-23T19:42:04Z" id="10668210">There is!. We are hard at work on the lucene 4 upgrade, once its done, we are going to do some refactoring on teh field data abstraction, and then tackle facets back. 
</comment><comment author="d2kagw" created="2013-05-10T22:54:29Z" id="17748700">would be great to see this in ES core!
</comment><comment author="kimchy" created="2013-05-10T22:55:21Z" id="17748722">@d2kagw agreed, we are working on some additional refactoring in our facet code base for the 1.0 release, and once its in, this will be one of the additional facets we will have.
</comment><comment author="d2kagw" created="2013-05-10T23:19:58Z" id="17749394">As an aside, do you have a list of the additional facets that will be introduced?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The example for Filtered Queries does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1688</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/reference/query-dsl/filtered-query.html

returns this:

``` javascript

SearchParseException[[questions][3]: from[-1],size[-1]: Parse Failure [No parser for element [filtered]]]; }]","status":500

```

I'm having a really hard time creating queries that I want from the docs.
</description><key id="3162065">1688</key><summary>The example for Filtered Queries does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pennyfx</reporter><labels /><created>2012-02-09T19:24:40Z</created><updated>2016-03-11T22:24:30Z</updated><resolved>2012-02-12T17:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-12T17:11:16Z" id="3929177">This is the correct format for the `filtered` query. You can place that `filtered` query where you want, as a top level search query (in which case, it needs to exists within the search `query` element), or within a `bool` query, or the several other "compound" queries out there. So, the example is correct, its just that when using it in a search request, you need to place it within the correct structure.
</comment><comment author="pennyfx" created="2012-02-12T19:11:21Z" id="3930096">Ok, why not show the example in a correct structure?
</comment><comment author="kimchy" created="2012-02-12T21:11:43Z" id="3931070">It is the correct structure, of the filtered query. Where it is "placed", in a search query, in another bool query, or other places, is not relevant to the filtered query structure.
</comment><comment author="fesnault" created="2012-04-27T22:13:03Z" id="5391476">I have the same problem. The examples are really not user friendly in my opinion.
I can make a simple query work, but as soon as i add some "less basic" structure, it fails. I had exactly the same problem with filtered. Can't make it work. Followed the doc, and looked at your prez in devoxx (on parleys).

I agree with pennyfx, you should provide full examples. Or a better documentation.

For example, this one works : 
curl -X POST http://localhost:9200/vo/vehicle/_search?pretty=true  -d '{ "query" : { "field" : { "NAME" : "+GARAGE" }  } } } }'

This one does not. Built it looking at your slide on parleys :

curl -X POST http://localhost:9200/vo/vehicle/_search?pretty=true  -d ' { "filtered" : { { "query" : { "field" : { "NAME" : "+GARAGE" }  } } } }   "filter" : { "range" : { "MILEAGE" :  { "from": 10000, "to": 15000 } }  }   } '
Fails with :

nested: SearchParseException[[vo][3]: from[-1],size[-1]: Parse Failure [No parser for element [filtered]]]; }]",

Any clue? Any doc? Any FULL example ?
</comment><comment author="kimchy" created="2012-04-29T14:54:33Z" id="5405492">The second sample that you have is not valid, the "filtered" part needs to be within a "query" element.
</comment><comment author="fesnault" created="2012-04-29T15:32:55Z" id="5405788">So how do you explain the example on your website ?
The filtered part is not in any query.
(Note in my sample, i have a braces problem, but even fixed, not working).

I was able to send this query using the Java API, but not using a http REST
query.

{
    "filtered" : {
        "query" : {
            "term" : { "tag" : "wow" }
        },
        "filter" : {
            "range" : {
                "age" : { "from" : 10, "to" : 20 }
            }
        }
    }
}

Frederic
On Sun, Apr 29, 2012 at 4:54 PM, Shay Banon &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; The second sample that you have is not valid, the "filtered" part needs to
&gt; be within a "query" element.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1688#issuecomment-5405492
</comment><comment author="kimchy" created="2012-04-29T22:46:34Z" id="5409520">The query DSL docs for each query show samples of _just_ the query itself, not the full search request. You can take that filtered query sample, place it under the `query` element in the search, and use it (thats how all the samples of each query and filter works in the Query DSL section). Not sure how you use the Java API.
</comment><comment author="fesnault" created="2012-04-30T11:21:12Z" id="5415536">Ok so what you say is that the request body is always (Request Body from
the doc) :
{
    query : [the query]
}

And that in the query value placholder we can put any thing from the query
dsl, like :

{
    query : {
        filtered : {
            query : {...}
            filter : {...}
       }
    }
}

Right ?
But seems to me i tried already to wrap the filtered element with a query,
but maybe i had my curly braces problem at this time.
Anyway i'll try this tonight.

Frederic

On Mon, Apr 30, 2012 at 12:46 AM, Shay Banon &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; The query DSL docs for each query show samples of _just_ the query itself,
&gt; not the full search request. You can take that filtered query sample, place
&gt; it under the `query` element in the search, and use it (thats how all the
&gt; samples of each query and filter works in the Query DSL section). Not sure
&gt; how you use the Java API.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1688#issuecomment-5409520
</comment><comment author="fesnault" created="2012-05-01T10:24:17Z" id="5436476">Ok you're right, it works  :
curl -X POST http://localhost:9200/vo/vehicle/_search?pretty=true  -d '{ "fields": [  "NAME", "LOCATION", "MILEAGE" ], "query": { "filtered" : {"query" : { "field" : { "NAME" : "+PARIS" }  }, "filter" : { "range" : { "MILEAGE" : { "from" : 10000 "to" : 15000} } } } } }'

is working.

For information, i used the java api this way :
QueryBuilder qb = filteredQuery(
                matchAllQuery(),
                geoDistanceFilter("LOCATION").lon(2.3675999959309895).lat(48.86186930338542).distance(5, DistanceUnit.KILOMETERS)
        );
        SearchResponse response = client.prepareSearch("vo")
                .setQuery(qb)
                .setFrom(0).setSize(10)
                .addField("CARROSSERIE")
                .addField("MILEAGE")
                .addField("NAME")
                .execute()
                .actionGet();

Well the geolocation search does not work, but maybe because i name my fields longitude and latitude, and not lon and lat.

So i should have read the request body part first then query dsl part. That's the link between them i missed.
Thanks for the information :)
</comment><comment author="kimchy" created="2012-05-01T11:56:31Z" id="5437340">Great. Please, other questions send to teh mailing list.
</comment><comment author="fesnault" created="2012-05-01T14:54:26Z" id="5440117">Yes i Will. Thanx for being reactive:)
Fr&#233;d&#233;ric
Le 1 mai 2012 13:56, "Shay Banon" &lt;
reply@reply.github.com&gt;
a &#233;crit :

&gt; Great. Please, other questions send to teh mailing list.
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1688#issuecomment-5437340
</comment><comment author="conrosebraugh" created="2013-12-09T22:09:03Z" id="30178882">Hi, I just found this thread after running into the same problem. I agree that the docs need to explicitly say that these query examples need to be wrapped in a query. It is still not clear, 2 years later.
</comment><comment author="diffoperator" created="2014-03-07T23:17:58Z" id="37078658">Seconded. The documentation is not user friendly at all.
</comment><comment author="peterklipfel" created="2014-05-19T16:30:55Z" id="43526323">+1 The documentation might make sense if it is read wholly and carefully from front to back, but often I really just want to look at an api method and see if it's what I'm looking for. It might make sense to have an "example" section at the bottom of each page.
</comment><comment author="lfender6445" created="2014-05-27T12:28:58Z" id="44268706">:+1:  - I fell for the same thing, helped me a lot when doing a fuzzy filter.
</comment><comment author="Blacktiger" created="2014-06-05T14:21:18Z" id="45224885">+1 The documentation is very confusing about where various parts of the query dsl are valid. It would be _extremely_ helpful to have some more complete examples.
</comment><comment author="exussum" created="2014-06-18T21:14:17Z" id="46495102">Definitely confusing.  This ticket has proven more helpful.
</comment><comment author="myjpa" created="2014-06-20T22:00:49Z" id="46731389">+1 I don't know how to use filter after reading the document until I saw this ticket.
</comment><comment author="clintongormley" created="2014-06-20T22:04:05Z" id="46731624">@myjpa I rewrote the docs completely. They still don't make sense to you? after the rewrite?

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filtered-query.html?1
</comment><comment author="myjpa" created="2014-06-20T23:04:43Z" id="46735402">@clintongormley Oh, yes, this is much more helpful. Thanks for the new doc!

I was reading the filter documentation from here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filters.html

Maybe it's just me, the first sentence is somehow misleading: "As a general rule, filters should be used instead of queries:". This make me think to replace "query" with "filtered", not embed "filtered" within "query". And the following examples are using "filtered" as the top level field.

I think it would be helpful to have an example(or a link to the curl example) in this chapter, too. 
</comment><comment author="leegee" created="2014-07-29T09:00:43Z" id="50451598">Very useful thread, very hard to find &#8212;&#160;the docs still need a lot of work.
</comment><comment author="clintongormley" created="2014-07-29T11:03:56Z" id="50462464">@leegee specific suggestions and PRs welcome :)
</comment><comment author="brunogoossens" created="2014-07-29T13:55:59Z" id="50478653">Lost a lot of time on this. It would be useful to have working examples with full structures so we can better understand.
</comment><comment author="leegee" created="2014-08-01T08:20:41Z" id="50860519">@clintongormley I know we should all sit and read the docs from end-to-end, but you know, no-one really does that &#8212;&#160;we tend to get something half-working, and then dip in to the docs for clarification. I guess it is unreasonable to wrap all the elements in the doc examples, but it would be a helpful compromise to repeatedly explicitly point out that the samples are fragments, and perhaps to have a few full queries. ES (and Lucene) are such great products, it is great to see them doing so well: my thanks to everyone who works on them (and helps me scrape a living with what they produce).
</comment><comment author="clintongormley" created="2014-08-01T08:58:26Z" id="50863381">@leegee sure i agree.  my intention is to add runnable snippets for each code block in the reference docs the same way as we are doing in the book (eg see View in Sense link under the code block here: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/match-multi-word.html ).  It's just a matter of getting there, as there is a lot of other stuff to do.

My point with my previous comment (https://github.com/elasticsearch/elasticsearch/issues/1688#issuecomment-50462464) is this: the original issue was about the docs for the `filtered` query, which I rewrote.  Subsequent comments have just been general complaints that the docs are bad, without any useful suggestions for making them better: eg "this page in particular is confusing because it doesn't explain XYZ."

I'm happy to improve things but I have to prioritise, so pointers to specific problems really help.
</comment><comment author="leegee" created="2014-08-16T08:22:05Z" id="52387338">The docs aren't _very_ bad &#8212;&#160;they're actually very helpful if one sits and works at it, as indeed on should for such a complicated and important task. It's just that ES has become very popular, and so is attracting a lot of search newbies &#8212;&#160;the curse of popularity...!

One thing that springs to mind is that the docs would benefit from more cross-references, such as http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-ngram-tokenfilter.html

The guide, though, is so useful and well-written that I think it would be beneficial to all of us if it was somehow more prominently linked.
</comment><comment author="sdhull" created="2014-12-15T19:55:57Z" id="67054606">For what it's worth, I'm running es 1.3.x and also found the docs confusing. The newer 1.4 docs have important improvements, but I didn't feel like following the newer docs was a good idea, since if something breaks I won't know if it's because of API changes or because I formed my queries incorrectly.

So it'd be nice to update the 1.3 docs as well with full working examples, instead of just the part that goes in the `query` portion (which is how I'm reading it now, after skimming this thread).

If you can point me to a repo where I could update these docs, I'd be happy to do the work and submit a PR.
</comment><comment author="nik9000" created="2014-12-15T20:16:07Z" id="67057709">&gt; The newer 1.4 docs have important improvements, but I didn't feel like following the newer docs was a good idea, since if something breaks I won't know if it's because of API changes or because I formed my queries incorrectly.
&gt; 
&gt; If you can point me to a repo where I could update these docs, I'd be happy to do the work and submit a PR.

My understanding is that docs are written in the master branch with notes in them that say things like "coming in 1.4" or "deprecated in 1.0".  The book is https://github.com/elasticsearch/elasticsearch-definitive-guide .
</comment><comment author="clintongormley" created="2014-12-17T10:45:54Z" id="67305526">@sdhull The docs for v1.4 are maintained in the 1.4 branch, for 1.3 and earlier, in the 1.3 branch.  If you are looking at the 1.3 docs (eg http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.3/query-dsl-filtered-query.html#query-dsl-filtered-query ) then you will see an "Edit" button next to every title.  This takes you to the relevant asciidoc page, and allows you to submit PRs against that branch. 

Usually users submit PRs against the current branch (1.4) and then we also apply them to 1.x, master, 1.3, as appropriate.
</comment><comment author="lwisne" created="2015-01-05T05:09:45Z" id="68669043">+1 for the documentation for ElasticSearch is terrible.  Trying to switch from SOLR but about to give up out of pure annoyance with the lack of clear examples and documentation.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Spurious action error message when attemping to do nested queries on empty indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1687</link><project id="" key="" /><description>This is mainly just an irritation (I think - the es instance crashed but I had been messing about with JVM settings so it was likely unrelated)

https://gist.github.com/1775185

"doc_dummy" in the above gist is an empty index that has lots of aliases (users always have their own personal index, but until it gets loaded with data it redirects to this empty index, to avoid having 100s of them).

So it looks like because "doc_dummy" has no documents (and hence presumably no documents containing nested objects), that my searches that contain nested elements (queries and facets) cause errors, filling up my log and making it harder to debug any real errors.

Is there any workaround other than to create a dummy document containing a nested object of the various types?
</description><key id="3149396">1687</key><summary>Spurious action error message when attemping to do nested queries on empty indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels /><created>2012-02-08T22:57:50Z</created><updated>2012-02-13T18:18:38Z</updated><resolved>2012-02-13T13:47:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-12T17:13:58Z" id="3929199">Its an illegal search request executed on that relevant shard in the index. An index can be empty, but still have mappings that indicate the fact that it has a nested structure. Moreover, you _have_ to set that explicit mapping before you index data to make use of nested objects.
</comment><comment author="Alex-Ikanow" created="2012-02-13T13:47:15Z" id="3939871">Ugh - sorry! My automated import updates the mapping but my manual recovery import doesn't, so any queries in between the two will generate those errors. Poor debugging skills on my part, apologies...
</comment><comment author="kimchy" created="2012-02-13T18:18:38Z" id="3945948">No problem :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport buffer overrun can happen because of byte buffer reading optimization introduced in 0.19.0.RC1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1686</link><project id="" key="" /><description /><key id="3148783">1686</key><summary>Transport buffer overrun can happen because of byte buffer reading optimization introduced in 0.19.0.RC1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC2</label></labels><created>2012-02-08T22:13:21Z</created><updated>2012-02-08T22:15:18Z</updated><resolved>2012-02-08T22:15:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.19.0.RC1: NullPointer exception in XContentFactory.xContent (failed to execute search (building response))</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1685</link><project id="" key="" /><description>Hi,

I just upgraded my cluster to v0.19.0.RC1 with fresh machines and new indexes, and some searches are now returning this error:

&lt;pre&gt;
[2012-02-08 13:32:38,014][DEBUG][rest.action.search       ] [x] failed to execute search (building response)
java.lang.NullPointerException
        at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:106)
        at org.elasticsearch.rest.action.support.RestXContentBuilder.restDocumentSource(RestXContentBuilder.java:92)
        at org.elasticsearch.search.internal.InternalSearchHit.toXContent(InternalSearchHit.java:366)
        at org.elasticsearch.search.internal.InternalSearchHits.toXContent(InternalSearchHits.java:179)
        at org.elasticsearch.search.internal.InternalSearchResponse.toXContent(InternalSearchResponse.java:71)
        at org.elasticsearch.action.search.SearchResponse.toXContent(SearchResponse.java:272)
        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:96)
        at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:90)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:195)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:175)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:155)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:149)
        at org.elasticsearch.search.action.SearchServiceTransportAction$8.handleResponse(SearchServiceTransportAction.java:359)
        at org.elasticsearch.search.action.SearchServiceTransportAction$8.handleResponse(SearchServiceTransportAction.java:350)
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:244)
        at org.elasticsearch.transport.netty.MessageChannelHandler.process(MessageChannelHandler.java:215)
        at org.elasticsearch.transport.netty.MessageChannelHandler.callDecode(MessageChannelHandler.java:138)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:92)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:553)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:343)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:274)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:194)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
&lt;/pre&gt;


I am still investigating this, but the debugger says the XContentType guesser is returning null. I have uploaded a screenshot of the debugger at http://i.imgur.com/agchg.png . The interesting thing is that Unicode.fromBytes(source, offset, length) returns a truncated document that is missing the initial {.

Still investigating over here...trying to get a smaller repro case.
</description><key id="3143702">1685</key><summary>0.19.0.RC1: NullPointer exception in XContentFactory.xContent (failed to execute search (building response))</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alambert</reporter><labels /><created>2012-02-08T17:00:01Z</created><updated>2012-02-08T22:22:09Z</updated><resolved>2012-02-08T22:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alambert" created="2012-02-08T17:33:00Z" id="3871856">I was able to access the document directly with http://&lt;host&gt;:9200/&lt;index name&gt;/post/23058676201_10150143092481202 without an error. I was also able to surface the document with these queries without an error:

&lt;pre&gt;
{
  "query": {
    "ids": {
      "values": [
        "23058676201_10150143092481202"
      ]
    }
  }
}
&lt;/pre&gt;


&lt;pre&gt;
{
  "query": {
    "bool": {
      "should": [
        {
          "ids": {
            "values": [
              "23058676201_10150143092481202"
            ]
          }
        },
        {
          "match_all": {}
        }
      ]
    }
  }
}
&lt;/pre&gt;


In all three cases, the correct, full document came back with no errors.
</comment><comment author="alambert" created="2012-02-08T18:01:50Z" id="3872384">OK, I reran the query that was causing the NPE with the additional filter clause

&lt;pre&gt;

      {
        "not": {
          "query": {
            "ids": {
              "values": [
                "23058676201_10150143092481202"
              ]
            }
          }
        }
      },
&lt;/pre&gt;


to exclude the document that was causing the NPE. Instead, we crashed on another document; Unicode.fromBytes(source.bytes(), source.offset(), source.length()) looks similarly corrupted in the new screenshot at http://i.imgur.com/gBc6O.png

So it looks like this is independent of the particular document we're surfacing.

&lt;pre&gt;
[2012-02-08 17:55:32,525][DEBUG][rest.action.search       ] [x] failed to execute search (building response)
java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:106)
    at org.elasticsearch.rest.action.support.RestXContentBuilder.restDocumentSource(RestXContentBuilder.java:92)
    at org.elasticsearch.search.internal.InternalSearchHit.toXContent(InternalSearchHit.java:366)
    at org.elasticsearch.search.internal.InternalSearchHits.toXContent(InternalSearchHits.java:179)
    at org.elasticsearch.search.internal.InternalSearchResponse.toXContent(InternalSearchResponse.java:71)
    at org.elasticsearch.action.search.SearchResponse.toXContent(SearchResponse.java:272)
    at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:96)
    at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:90)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:195)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:175)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:155)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:149)
    at org.elasticsearch.search.action.SearchServiceTransportAction$8.handleResponse(SearchServiceTransportAction.java:359)
    at org.elasticsearch.search.action.SearchServiceTransportAction$8.handleResponse(SearchServiceTransportAction.java:350)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:244)
    at org.elasticsearch.transport.netty.MessageChannelHandler.process(MessageChannelHandler.java:215)
    at org.elasticsearch.transport.netty.MessageChannelHandler.callDecode(MessageChannelHandler.java:138)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:92)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:553)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:343)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:274)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:194)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
&lt;/pre&gt;
</comment><comment author="kimchy" created="2012-02-08T18:31:07Z" id="3872968">Hey, are you running multiple nodes in the cluster? It might be related to an optimization I made in the deserialization of responses (for example, from another node).
</comment><comment author="alambert" created="2012-02-08T18:34:26Z" id="3873048">Yep, this is a three-node cluster. I'm on IRC if you want a remote debugger connection.
</comment><comment author="kimchy" created="2012-02-08T22:22:09Z" id="3877615">Fixed in #1686.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>highlights are using different encodings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1684</link><project id="" key="" /><description>This is what I get making a query with highlights. Note the different encoding in the word "F&#225;cil" in the description field (F\u00e1cil) and in the highlighted description field (F&#195;&#161;cil which is F\xc3\xa1cil utf8 I guess):

```
{
    "_index":"test",
    "_type":"product",
    "_id":"yuLU-imnSkW7VQqhBxpzBg",
    "_score":56.39113, 
    "_source" : {
        "name": "Sombrilla maclaren universal powder pink", 
        "price": 35.0, 
        "description": "Sombrilla de color powder pink. \n- Disponible en colores a juego\n- F\u00e1cil de sujetar al cochecito\n- Regulable para \u00f3ptima protecci\u00f3n solar\n\nImprescindible para este verano !"
    },
    "highlight":{
        "description":["&lt;em&gt;Sombrilla&lt;/em&gt; de color powder pink. \n- Disponible en colores a juego\n- F&#195;&#161;cil de sujetar al cochecito\n- Regulable"]
    }
}
```
</description><key id="3143582">1684</key><summary>highlights are using different encodings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peillis</reporter><labels /><created>2012-02-08T16:51:47Z</created><updated>2014-07-08T14:15:32Z</updated><resolved>2014-07-08T14:15:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="peillis" created="2012-02-08T18:01:49Z" id="3872383">I forgot to mention, I'm using the thrift protocol.
</comment><comment author="clintongormley" created="2014-07-08T14:15:32Z" id="48341566">This seems to be fixed in 1.x.  Please reopen if that's not the case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Question/comment about multi-value field data construction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1683</link><project id="" key="" /><description>Feel free just to delete this if I'm missing something obvious, but I'm curious about the design decisions in the construction of the "ordinals" array for multi-valued field types.

You create an ArrayList of size &lt;max array size of field across all documents&gt;, each entry of the ArrayList being an int array of size maxDocs (containing the term ids)

Presumably the way you use the ordinals array is to loop over the terms for a given document id, ie

ArrayList.get(i).get(docId) for an iterating index 'i'

This seems sub-optimal for 2 reasons:
- As discussed in https://groups.google.com/group/elasticsearch/browse_thread/thread/31d87c84dd387367#, the memory usage for document sets that can have a large number of values for a field can get massive
- (Less importantly) You're having to reload your L2 cache with each "inner loop" iteration of 'i'

If you switched round the order, you'd have more pain initializing the field data, but after that it would be much more compact in memory and faster (maybe only marginally?) ... which seems like the right trade-off, since you're constructing the field data once (particularly with the default resident cache) but using it many times.

Or alternatively (eg if creating maxDocs \* small objects is too painful for whatever reason), take the top X% of documents (in terms of array length) after you're done, treat them differently (ie indexed the other way round, in a separate structure), and then delete as many ordinals entries as you can. (etc etc)

I'm sure many other es users are going to have decent-sized arrays of fields in their documents, what with the trendiness of social networks (friend lists), geo etc etc.
</description><key id="3142687">1683</key><summary>Question/comment about multi-value field data construction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels /><created>2012-02-08T16:04:47Z</created><updated>2012-10-24T18:34:54Z</updated><resolved>2012-10-24T18:34:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-08T18:42:03Z" id="3873212">It actually used to be in the reverse order, but then, multi valued fields with small amount of valued would cause a lot of memory to be allocated (many more arrays) and make the job of the GC harder.
</comment><comment author="Alex-Ikanow" created="2012-02-08T18:58:04Z" id="3873571">I figured it was GC-related! 

That was partly the reason for my alternative suggestion of picking a smaller number of "offending" documents, giving them an ordinals.get(0).geo(docId) value of -1 and then indexing them the other way round.

I can't believe my sort of scenario will be that rare, seems like it would be something you'd want to the tool to handle without application-level "grovelling".

(On the subject of which, I'm just testing out putting docs with &gt;20 geos in their own index now, which I hope will have a similar effect.)

Incidentally, since I just discovered Java ints are always 32b (ahem!), I'm now out by x4 on my geo usage estimates, ie based on a single instance of the ordinals array I'd expect "4B \* 2.3M \* 155"= 1.4GB per replica/original, ie ~3GB in total for original + 1 replica ... whereas my actual field cache usage is 6GB the first time I run the facet, and 12GB the second time. 

Any idea where my estimation error might be? How many copies of ordinals do you think are being made? The only other significant memory usage would be some array of unique terms (maybe 10K), which should be relatively tiny.
</comment><comment author="kimchy" created="2012-02-22T19:13:42Z" id="4116181">The reason for the current structure is simple, imagine having maximum of 5 "tags" pre doc (multi valued part) and having a million docs (in a single Lucene segment). Instead of having 1 million arrays with 5 ordinals, with the current scheme, you have 5 arrays with 1 million slots (much better memory usage wise).
</comment><comment author="imotov" created="2012-02-22T20:23:32Z" id="4120902">We also have a couple of use cases where the number of tokens per field varies greatly. There are few records with hundreds of tokens in a field, and many records that have only a couple of tokens in the same field. The distribution doesn't match Zipf's law but gets quite close. The current data structure seems to be inefficient in such use cases. 

Another issue with the current data structure is that a single outlier record with a lot of tokens, when merged into a large segment, can significantly and suddenly increase memory requirements on an index that was running fine before that. 

Shay, are you planning to change it in 0.20? 
</comment><comment author="clintongormley" created="2012-02-23T09:59:00Z" id="4133573">++ we have a similar issue
</comment><comment author="Alex-Ikanow" created="2012-02-23T16:19:02Z" id="4139331">Since I bizarrely don't seem to have explicitly mentioned it in this thread: note that there's a simple workaround for the issue: create a separate index and put your "top" X% of documents (in terms of number of fields) in that. 

This results in a small index with many fields and a large index with few fields. In my case I reduced memory usage from ~14GB to ~2GB by putting ~0.1% docs in the new index (I picked the thresholds pretty lazily so I'm sure you could do even better).

(Apologies for repeating myself in every thread!)
</comment><comment author="Alex-Ikanow" created="2012-10-24T18:34:54Z" id="9750921">Eh just noticed I never closed this - apologies!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GC reporting / usage mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1682</link><project id="" key="" /><description>In https://groups.google.com/group/elasticsearch/browse_thread/thread/31d87c84dd387367#, I reported unusual CPU activity (continuous 40-60% for at least several days even under idle conditions) after creating 12GB of field cache (across 3 nodes) via a facet on a multi-valued field.

Last night I ran jstack while 2 of the nodes were in this state. All of the user threads were WAITING, THREAD_WAITING, or RUNNABLE (in some poll).

Cross-referencing the process ids from "top -H" with the nids reported in jstack, the following threads were actively using the CPU:

~19 minutes in ""Concurrent Mark-Sweep GC Thread"
~2 minutes in each of  4x "Gang worker#0 (Parallel GC Threads)" (worker#1,worker#2,worker#3)
~3 minutes in "VM Thread" prio=10 tid=0x00002aad1c6d0000 nid=0x7041 runnable

Empirically, ongoing CPU time seemed to be split between either Mark-Sweep and 1 of the GC threads, or Mark-Sweep and the VM thread.

However checking both bigdesk and head for the node in question, the following GC times were reported:
ParNew  12  5 seconds and 61 milliseconds
ConcurrentMarkSweep 4   5 seconds and 884 milliseconds

Presumably there's nothing that can be done about the CPU activity itself, that's just what the VM needs to do when I allocate a large amount of (multi valued) field cache. That's obviously fine if so.

Presuambly you also just read some standard stats to get the GC usage out - but I thought the fact the "actual values" weren't getting reported was issue worthy.
</description><key id="3141935">1682</key><summary>GC reporting / usage mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels /><created>2012-02-08T15:07:39Z</created><updated>2012-02-08T19:22:09Z</updated><resolved>2012-02-08T19:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-08T18:40:51Z" id="3873185">I get the GC values on reported time they ran from the formal GC stats of the JVM. Maybe you can enable GC logging and see if it helps?
</comment><comment author="Alex-Ikanow" created="2012-02-08T19:22:09Z" id="3874064">I think my natural curiosity on this issue has reached its end :)

If you're happy you're making the right API call to the JVM, then I don't think there's anything left to say. For future reference though, get people to use "top -H" (to show threads) and cross reference vs jstack if they're complaining about CPU usage, those 2 GC times (or at least the concurrent mark sweep) don't seem reliable
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian pkg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1681</link><project id="" key="" /><description>related https://github.com/elasticsearch/elasticsearch/issues/1525#issuecomment-3859516
</description><key id="3137321">1681</key><summary>Debian pkg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karussell</reporter><labels /><created>2012-02-08T07:46:35Z</created><updated>2014-07-09T07:22:26Z</updated><resolved>2012-02-08T13:49:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2012-02-08T07:51:00Z" id="3863886">ups, too many commits (?) but the diff is ok ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Avoid placing a shard replica on the same machine as shard itself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1680</link><project id="" key="" /><description>The setting is `cluster.routing.allocation.same_shard.host` and should be set to `true`.

Shay wrote:
"Note, if you want to run several instances on the same box, a 
feature needs to be added to make sure that a shard and a replica 
won't be allocated on the same _machine_ (not node), which is quite 
simple to add, can you open a feature? " 

See thread:
http://groups.google.com/group/elasticsearch/browse_thread/thread/4f94cb14f4e2cd5a
</description><key id="3133112">1680</key><summary>Avoid placing a shard replica on the same machine as shard itself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels><label>enhancement</label><label>v0.19.0.RC2</label></labels><created>2012-02-07T23:09:53Z</created><updated>2012-02-08T13:39:11Z</updated><resolved>2012-02-08T13:39:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>GC logging: Enable back automatic gc logging based on thresholds (even if last gc is not available)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1679</link><project id="" key="" /><description>One nice aspect is the addition of specific gc logging based on the garbage collection. See the bottom of the pushed elasticsearch config file for samples.
</description><key id="3132622">1679</key><summary>GC logging: Enable back automatic gc logging based on thresholds (even if last gc is not available)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC2</label></labels><created>2012-02-07T22:34:51Z</created><updated>2012-02-07T22:35:36Z</updated><resolved>2012-02-07T22:35:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>A content decompressor that throws a human readable message when compression is disabled and the user sends compressed content</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1678</link><project id="" key="" /><description>Currently the user gets an obscure error message about content that cannot be decoded because ES handles the compressed content as uncompressed content.
I personally think that we should not care about broken clients and respond to requests according to the HTTP specs.
Compression is very useful when you use the bulk-API btw....
</description><key id="3130983">1678</key><summary>A content decompressor that throws a human readable message when compression is disabled and the user sends compressed content</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">derryx</reporter><labels><label>feedback_needed</label></labels><created>2012-02-07T20:49:35Z</created><updated>2014-08-12T16:57:46Z</updated><resolved>2014-08-12T16:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-18T08:56:25Z" id="49409402">Hi @derryx, sorry for the long wait, I'm going to take a look at this.

In the meantime, can you sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so I can merge this in once it looks good?
</comment><comment author="s1monw" created="2014-07-23T07:53:13Z" id="49843206">I looked at it briefly and I think this looks good though. Yet, the indent seems to be off  and it seems to use tabs...
</comment><comment author="derryx" created="2014-08-06T11:32:18Z" id="51322918">Should I create a new pull request?
</comment><comment author="clintongormley" created="2014-08-06T11:34:30Z" id="51323095">@derryx it looks like you have deleted your original fork of the elasticsearch repo? in which case yes: please create a new PR.

Also, we'll need you to sign the CLA before we can merge it in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index API: Don't wait for new mappings to be applied on the cluster by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1677</link><project id="" key="" /><description>Change `action.wait_on_mapping_change` to `false` by default (from `true`).
</description><key id="3129742">1677</key><summary>Index API: Don't wait for new mappings to be applied on the cluster by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC2</label></labels><created>2012-02-07T19:48:47Z</created><updated>2012-02-07T19:51:29Z</updated><resolved>2012-02-07T19:51:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Nodes Info API: Add `all` flag to return all data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1676</link><project id="" key="" /><description /><key id="3127513">1676</key><summary>Nodes Info API: Add `all` flag to return all data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC2</label></labels><created>2012-02-07T17:19:17Z</created><updated>2012-02-07T17:19:42Z</updated><resolved>2012-02-07T17:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>JMX and REST+JSON stats should be in sync</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1675</link><project id="" key="" /><description>JMX and REST+JSON APIs should expose the same stats and remain in sync in future ES releases.

Thread:
http://groups.google.com/group/elasticsearch/browse_thread/thread/606c3fc976065c96
</description><key id="3118577">1675</key><summary>JMX and REST+JSON stats should be in sync</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">otisg</reporter><labels /><created>2012-02-07T03:10:13Z</created><updated>2013-06-26T16:05:43Z</updated><resolved>2013-06-26T16:05:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-26T16:05:43Z" id="20059556">Closing this one, as JMX support has been dropped with 0.90.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Add more options to get specific stats in REST: /_nodes/fs/stats (in addition to /_nodes/stats/fs)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1674</link><project id="" key="" /><description>Similar with relevant node parameters
</description><key id="3115764">1674</key><summary>Node Stats: Add more options to get specific stats in REST: /_nodes/fs/stats (in addition to /_nodes/stats/fs)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-06T22:37:21Z</created><updated>2012-02-06T23:23:01Z</updated><resolved>2012-02-06T23:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Some external libs move from org.elasticsearch.common to their real packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1673</link><project id="" key="" /><description>Hi there,

Just adding this issue as it's a breaking change for users (like me) who used for example org.elasticsearch.common.joda.time.format.ISODateTimeFormat instead of org.joda.time.format.ISODateTimeFormat

With the pom.xml (0.19), joda is now a standard dependency, just like jackson, mvel and so on...

Feel free to close this issue if it's useless or mark it as a breaking change...

HTH
David
</description><key id="3115356">1673</key><summary>Some external libs move from org.elasticsearch.common to their real packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2012-02-06T22:05:40Z</created><updated>2012-02-07T20:17:16Z</updated><resolved>2012-02-07T20:17:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-07T09:57:10Z" id="3845234">Its not a standard dependency, its gets shaded through the build process into `org.elasticsearch.common.joda` so the elasticsearch jar file contains it.
</comment><comment author="dadoonet" created="2012-02-07T10:07:05Z" id="3845348">Hum... Look strange.
I will double check soon. It seems that for my RSSRiver, org.elasticsearch.common.joda package is not seen anymore.
Perhaps it's an Eclipse classpath issue.

Let me check.

Cheers
</comment><comment author="kimchy" created="2012-02-07T17:29:28Z" id="3852249">Make sure you use the elasticsearch jar from maven, it already includes the embedded libraries in it like Joda.
</comment><comment author="dadoonet" created="2012-02-07T20:17:16Z" id="3855251">You are right as usual !
In fact, ES project was opened in my eclipse wokspace and Rss River plugin also.
So Eclipse Maven Plugin use ES workspace project as dependency instead of jar file.

Note for eclipse users who hit this _issue_ :
Closing ES project makes everything fine.
Other option is to disable workspace resolution for RSS River project in eclipse.

Sorry to have raised a fake issue... :-(
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JVM Flags: Remove SurvivorRatio and MaxTenuringThreshold since the defaults are good with new JVMs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1672</link><project id="" key="" /><description /><key id="3113656">1672</key><summary>JVM Flags: Remove SurvivorRatio and MaxTenuringThreshold since the defaults are good with new JVMs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-06T20:10:13Z</created><updated>2012-02-06T20:11:30Z</updated><resolved>2012-02-06T20:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>bin: elasticsearch script to support ES_HEAP_SIZE to easily set the heap size to a single value (min and max) and ES_HEAP_NEWSIZE to optionally set the new gen</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1671</link><project id="" key="" /><description>Usually, we want to set min and max heap to the same value, have a simple `ES_HEAP_SIZE` env var to set it (`ES_MIN_MEM` and `ES_MAX_MEM` still work). Also, add `ES_HEAP_NEWSIZE` to easily set the new generation size.
</description><key id="3113637">1671</key><summary>bin: elasticsearch script to support ES_HEAP_SIZE to easily set the heap size to a single value (min and max) and ES_HEAP_NEWSIZE to optionally set the new gen</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-06T20:09:17Z</created><updated>2012-02-06T20:09:39Z</updated><resolved>2012-02-06T20:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Backporting greek lowercase filter to 0.18</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1670</link><project id="" key="" /><description>Hello.

Is it possible to backport SHA: 32f1edf (Add language setting to lowercase filter, supporting greek and turkish, closes #1503) to 0.18 branch? `GreekLowerCaseFilter` is needed by the greek stemmer.

Thanks!
</description><key id="3109479">1670</key><summary>Backporting greek lowercase filter to 0.18</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ctrochalakis</reporter><labels /><created>2012-02-06T15:29:01Z</created><updated>2012-02-07T10:10:37Z</updated><resolved>2012-02-07T10:10:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-07T09:57:49Z" id="3845242">I am going to release 0.19 today or the latest tomorrow.
</comment><comment author="ctrochalakis" created="2012-02-07T10:10:37Z" id="3845391">Even better! 

thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change plugin.bat classpath to use ext.dirs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1669</link><project id="" key="" /><description>The wildcarded classpath does not seem to work on Windows.  Changing
to setting the ext dirs to the lib folder does.

Fix for issue #1660
</description><key id="3100907">1669</key><summary>Change plugin.bat classpath to use ext.dirs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2012-02-05T19:03:07Z</created><updated>2014-06-26T18:06:20Z</updated><resolved>2012-02-06T05:38:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-05T20:59:44Z" id="3820718">Changing java ext dir is not really the way to solve it, one should not change it. Wildcard does work on windows, that at least I know. Does elasticsearch.bat work for you? Its the same as the plugin.
</comment><comment author="mattweber" created="2012-02-05T21:33:05Z" id="3820951">OK, just copied how some other bat files I found were doing it.  Yes, elasticsearch.bat does work fine.  I wonder if order if the jars being placed on the classpath make a difference.
</comment><comment author="mattweber" created="2012-02-06T05:38:23Z" id="3823867">Maybe this is something with my environment, I will attempt to track it down and reopen if necessary.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread Pool: Add a specific thread pool for bulk indexing operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1668</link><project id="" key="" /><description>Add a `bulk` thread pool (defaults to `cached` type), allowing to specifically control bulk thread pool and the index thread pool (for discrete index / delete operations).

This is a breaking change since if one configured the `index` thread pool, the `bulk` thread pool will need to be explicitly configured as well.
</description><key id="3100600">1668</key><summary>Thread Pool: Add a specific thread pool for bulk indexing operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T18:12:20Z</created><updated>2012-03-19T08:10:11Z</updated><resolved>2012-02-05T18:12:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="medcl" created="2012-03-19T08:10:11Z" id="4569135">mark
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Network: Allow to specify sub interfaces (virtual) in network configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1667</link><project id="" key="" /><description>for example, something like `_eth0:1_`. 
</description><key id="3100238">1667</key><summary>Network: Allow to specify sub interfaces (virtual) in network configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T17:06:28Z</created><updated>2012-02-05T17:06:58Z</updated><resolved>2012-02-05T17:06:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Jmx: Only register JMX beans when jmx.create_connector is set to `true`, or explicitly set by setting `jmx.export` to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1666</link><project id="" key="" /><description>A minor improvement to not even register JMX beans with the management platform if not really needed, or even registering those in Guice.
</description><key id="3100036">1666</key><summary>Jmx: Only register JMX beans when jmx.create_connector is set to `true`, or explicitly set by setting `jmx.export` to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T16:36:43Z</created><updated>2012-02-05T16:53:08Z</updated><resolved>2012-02-05T16:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Node Stats: JVM stats to provide memory pools allocations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1665</link><project id="" key="" /><description /><key id="3099748">1665</key><summary>Node Stats: JVM stats to provide memory pools allocations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T15:46:36Z</created><updated>2012-02-05T15:47:06Z</updated><resolved>2012-02-05T15:47:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index / Delete API: timeout should apply automatically to the auto create index API master timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1664</link><project id="" key="" /><description>When specifying timeout on the index/delete API, it should be used as the master timeout in the create index API that is automatically called when the index does not exists.
</description><key id="3098662">1664</key><summary>Index / Delete API: timeout should apply automatically to the auto create index API master timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T11:44:04Z</created><updated>2012-02-05T11:45:39Z</updated><resolved>2012-02-05T11:45:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: query_string/field/text do not fail when an invalid analyzer is provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1663</link><project id="" key="" /><description /><key id="3098525">1663</key><summary>Query DSL: query_string/field/text do not fail when an invalid analyzer is provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T11:06:30Z</created><updated>2012-02-05T11:07:12Z</updated><resolved>2012-02-05T11:07:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Indices / Nodes stats: All `all` flag to easily return all stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1662</link><project id="" key="" /><description /><key id="3098357">1662</key><summary>Indices / Nodes stats: All `all` flag to easily return all stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-05T10:03:29Z</created><updated>2012-02-05T10:04:32Z</updated><resolved>2012-02-05T10:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Bounding Box Query is slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1661</link><project id="" key="" /><description>Hi there ,
 i have been writing about this issue in the elastic search google groups , but unfortunately no one address the issue.
So i am creating the Ticket here .
Problem : 
Setup  4 core cpu with 24gb of ram .
Index size is 10gb , Using ES latest version 
Now when i have no where clause in the query its gives results in no time say 20 ms. but when i added BoundingBox filter its started giving result in &gt; 200 ms .
Please correct me if i am wrong , i have used the same example you guys have mentioned in the Elastic Search (GeoBoundingBoxTests.java).
Do i have to do something else or is this the limitation of ES.
Regards
Prashant 
</description><key id="3093020">1661</key><summary>Bounding Box Query is slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">prashantvicky</reporter><labels /><created>2012-02-04T10:40:44Z</created><updated>2013-08-09T11:13:32Z</updated><resolved>2013-08-09T11:13:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2012-02-08T09:38:06Z" id="3864970">@prashantvicky i suggest you gist a test case demonstrating the issue.  I have no idea if it can be solved or not, but without a test case, nothing is likely to happen
</comment><comment author="prashantvicky" created="2012-02-10T04:38:45Z" id="3901108">Hi , clinton ,
Sorry for the late reply , i have gist the the code (2 files one is uploader &amp; one is searcher)
https://gist.github.com/1786636
Waiting for your reply
Regards
Prashant
</comment><comment author="prashantvicky" created="2012-02-10T04:39:36Z" id="3901112">Hi , clinton ,
Sorry for the late reply , i have gist the the code (2 files one is
uploader &amp; one is searcher)
https://gist.github.com/1786636
Waiting for your reply
Regards
Prashant

On Wed, Feb 8, 2012 at 3:08 PM, Clinton Gormley &lt;
reply@reply.github.com

&gt; wrote:
&gt; 
&gt; @prashantvicky i suggest you gist a test case demonstrating the issue.  I
&gt; have no idea if it can be solved or not, but without a test case, nothing
&gt; is likely to happen
&gt; 
&gt; ---
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; 
&gt; https://github.com/elasticsearch/elasticsearch/issues/1661#issuecomment-3864970
</comment><comment author="spinscale" created="2013-06-10T15:30:53Z" id="19206285">Hey, do you still have performance problems (with a current version of elasticsearch) or did you find the issue?

if you still have issues, can you drop show us your elasticsearch configuration (if you changed it), your index configuration, your mappings and sample queries (maybe not as java source, but rather as a curl sample - so everyone of us developers can take a look at without needing to dig into java)

Thanks a lot!
</comment><comment author="spinscale" created="2013-08-09T11:13:32Z" id="22388220">closing due to lack of information. happy to reopen if more information is provided to help us debugging this on a current release! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin install fails on Windows (missing jline jar)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1660</link><project id="" key="" /><description>When trying to install a plugin on the Windows platform, I receive a missing jar message:

C:\Users\Matt\elasticsearch-0.18.7&gt;bin\plugin.bat -install elasticsearch/elasticsearch-lang-groovy/1.0.0
Error: Could not find or load main class C:\Users\Matt\elasticsearch-0.18.7.lib.jline-0.9.94.jar

I am running Windows 7, 32-bit, JDK 7u2
</description><key id="3091878">1660</key><summary>Plugin install fails on Windows (missing jline jar)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2012-02-04T04:00:11Z</created><updated>2012-03-26T14:33:24Z</updated><resolved>2012-02-06T05:38:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-05T10:17:39Z" id="3816755">Can you try and check it down? Maybe paste the result of adding an `echo` to the invocation of java in the `plugin.bat`?
</comment><comment author="dadoonet" created="2012-02-05T22:34:56Z" id="3821427">Hi there,

bin\plugin -install elasticsearch/elasticsearch-lang-groovy/1.0.0 works fine for me under windows using es 0.18.7 and  java 6.
I did not test with java 7.

HTH
</comment><comment author="mattweber" created="2012-02-06T05:38:07Z" id="3823866">Maybe this is something with my environment, I will attempt to track it down and reopen if necessary.
</comment><comment author="maxhauser" created="2012-03-26T12:42:30Z" id="4693162">I had the same issue.
Seams to be indeed an issue with java 7 (I am running 7u3). With 6r31 everything works fine for me (bin\plugin).
</comment><comment author="kimchy" created="2012-03-26T14:33:24Z" id="4695264">What failure are you getting exactly? Which version are you using?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting broken with synonym substitution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1659</link><project id="" key="" /><description>If a synonym matches a term that is shorter than the returned value, highlighting returns an error.

E.g. the synonym file has this line:

al, alan, albert, alex, alexander, alexandr

and I have a document such as:

{
  "name": "Al Pacino"
  ...
}

if I do a search on "Albert Pacino" with highlighting enabled for the name field, I get an error:

Fetch Failed [Failed to highlight field [name]]]; nested: InvalidTokenOffsetsException[Token alexandr exceeds length of provided text sized 7];
</description><key id="3088738">1659</key><summary>Highlighting broken with synonym substitution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmwilson</reporter><labels /><created>2012-02-03T20:59:00Z</created><updated>2013-05-23T07:39:01Z</updated><resolved>2013-05-23T07:39:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jmwilson" created="2012-02-03T21:03:50Z" id="3804444">Highlighting has some other weird behaviors with synonyms: e.g. searching for "jim" in a field that matches "james" with highlighting will return &lt;b&gt;jam&lt;/b&gt;es. This looks odd.
</comment><comment author="kimchy" created="2012-02-07T23:48:31Z" id="3859540">I believe this is a bug in Lucene, I need to find the bug and once I do will post it here. There is another issue in ES issues that talks about it and references the Lucene bug.
</comment><comment author="ppearcy" created="2012-03-13T19:56:39Z" id="4484553">Dup of:
https://github.com/elasticsearch/elasticsearch/issues/1401

Which references this ticket:
https://issues.apache.org/jira/browse/LUCENE-3668
</comment><comment author="ppearcy" created="2012-05-04T19:18:41Z" id="5518168">Should be fixed...
</comment><comment author="spinscale" created="2013-05-23T07:39:01Z" id="18327940">Closing this one as it has been fixed in Lucene. If you encounter problems, please reopen. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow for plugins to register REST filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1658</link><project id="" key="" /><description>Allow to register REST filters that can create a processing chain for rest requests.
</description><key id="3066402">1658</key><summary>Allow for plugins to register REST filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-02T11:23:02Z</created><updated>2012-02-02T18:19:25Z</updated><resolved>2012-02-02T18:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nkvoll" created="2012-02-02T12:17:09Z" id="3776936">Commenting to be notified of changes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query: Add `index.query.default_field` allowing to control the default field used to search on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1657</link><project id="" key="" /><description>Applies to queries like `query_string`, where by default it searches on the `_all` field, but using the `index.query.default_field`, one can change that (assuming for example, `_all` field is disabled).
</description><key id="3060244">1657</key><summary>Query: Add `index.query.default_field` allowing to control the default field used to search on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-02-01T22:49:08Z</created><updated>2012-02-01T22:49:42Z</updated><resolved>2012-02-01T22:49:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Robustness issues with mapping type conflicts (0.18.7)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1656</link><project id="" key="" /><description>This occurs 100% on my operational cluster, but my attempt to recreate via a simple GIST (https://gist.github.com/1717268) fails.

I was indexing a large number of documents into several different indexes, with an identical mapping. The documents can also contain "unmapped" fields containing arrays of "user-specified" objects.

In this case, in the first document with such an unmapped field, the object contained a field with a null that my calling program had incorrectly turned into {}

(Somewhat less simplified example of documents: https://gist.github.com/1717326)

When the next document (with a correct string field) was indexed, the calling thread went into a tight loop forever (ie the index hung, 1 CPU went to 100%). Trying multiple times, more CPUs went to 100%.

When I closed down the offending node, one of the other nodes would thrash, until I had closed all the nodes down and restarted them 1-by-1.

Assuming the above is no use (!), when I get some more time I'll try to make my reproduction more and more like my real world case until it fails. (Be a couple of weeks, will leave this as a placeholder until then?)
</description><key id="3052358">1656</key><summary>Robustness issues with mapping type conflicts (0.18.7)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alex-Ikanow</reporter><labels /><created>2012-02-01T14:33:40Z</created><updated>2012-04-09T12:37:27Z</updated><resolved>2012-04-09T12:37:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-02-05T10:21:41Z" id="3816769">Can you issue a stack dump on a node the exhibit this? (see jstack: http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jstack.html), it will help see where the thrashing is coming from.
</comment><comment author="Alex-Ikanow" created="2012-02-06T13:44:06Z" id="3828544">I tightened up the application logic to prevent it from happening, but I'll have a play with trying to reproduce it in the lab. Sorry, I should have jstacked it when it happened, but it was late, and I just assumed it would reproduce easily because it occurred so consistently across both my clusters. 
</comment><comment author="Alex-Ikanow" created="2012-04-09T12:37:27Z" id="5024461">Sorry, couldn't reproduce this with simple GISTs. I'll close it for now, will do a better job of qualifying if it ever happens again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Swap indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1655</link><project id="" key="" /><description>I'm a bit at a loss how to correctly swap two indexes.

My full index process indexes in to a fresh index B/X how do i swap A/X over so it now holds the contents of B/X so that I can delete B/X again ?

It would be nice if this became a properly exposed method on the API too since its (i assume) a common usecase.
</description><key id="3050964">1655</key><summary>Swap indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>feedback_needed</label></labels><created>2012-02-01T12:20:39Z</created><updated>2014-07-18T09:15:16Z</updated><resolved>2014-07-18T09:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexbrasetvik" created="2012-02-02T10:16:35Z" id="3775516">Have you looked into index-aliases? http://www.elasticsearch.org/guide/reference/api/admin-indices-aliases.html
</comment><comment author="Mpdreamz" created="2012-02-02T16:26:30Z" id="3780899">Yes i have and my question still stands: 

The section which mentions `Renaming an index is a simple remove then add operation within the same API:` is misleading it will rename the alias but not the actual index.

Which makes actual swapping impossible. I want to reindex into a fresh (empty) index and then swap that with the real index that all my searchers point to (they point to index/type).

Perhaps its possible with the aliasing feauture but i'm not sure which sequence of add/remove to issue so that it performs a hotswap.
</comment><comment author="alexbrasetvik" created="2012-02-02T16:29:29Z" id="3780952">Have your indexers/searches reference the aliases, and not the indexes directly. :)
</comment><comment author="Mpdreamz" created="2012-02-02T16:46:57Z" id="3781309">Thanks Alex but I'm aware of existing workarounds. I'm looking for an answer if this is possible currently or perhaps support for it can be added in the future.
</comment><comment author="lotyrin" created="2012-02-02T17:49:02Z" id="3782587">I'm not sure I'd call that a "workaround" so much as "the existing way to solve your problem"...
</comment><comment author="Mpdreamz" created="2012-02-02T23:00:34Z" id="3788501">Either way it's something i'd like to be able to do and right now i can't. 
Honestly apreciating the feedback guys but being dismissive without reasoning why its a bad idea is not helping me at all. 
</comment><comment author="clintongormley" created="2012-02-08T09:28:02Z" id="3864849">@Mpdreamz What @lotyrin  said is correct - it is The Way to do it.  You can't rename indices.  That's what aliases are for.  So just use aliases.  I have my actual indices names `foobar_$TIMESTAMP` and an alias called `foobar` which points to the current index.

You can achieve exactly what you are asking for using aliases.  I'm not sure why you think this is sub-optimal.
</comment><comment author="Mpdreamz" created="2012-02-08T17:08:34Z" id="3871364">I know its The Way to do it and i don't even think its suboptimal, i love aliases! The whole point i am trying to make is why should it be The Only Way to do it? Literally swapping two indexes is no less suboptimal provided the system provides for such a feature. It's a tad dissapointing this suggestion is faced with opposition only because there is already another way to go about it using different semantics. 

A swap operation could also be benficial if you use aliases more to create 'views' a swap operation could make sure these stay valid without updating all of them manually having to point them to foo_TIMESTAMP
</comment><comment author="clintongormley" created="2014-07-18T08:50:58Z" id="49408950">Hi @Mpdreamz 

Given all the features that Elasticsearch has added since this ticket was opened, do you still have a use case for this that isn't handled.  Note: we could only do a `rename` on a closed index anyway.
</comment><comment author="Mpdreamz" created="2014-07-18T09:15:16Z" id="49410992">Yikes I must have had something up my whahooo when posting this, so angry... thanks for heads up @clintongormley I'll see if I have other issues to prune. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto opening and closing of indices after a period of inactivity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1654</link><project id="" key="" /><description>In a multi tenant environment where each customer has one or several indices (accessed through an alias), it would be very interesting to have those indices opened only when necessary. As the cost of opening an index may be huge, closing them after a configurable period of inactivity could be a very good way to deal with a large number of customer without using resources when not relevant.
</description><key id="3034047">1654</key><summary>Auto opening and closing of indices after a period of inactivity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ahfeel</reporter><labels><label>discuss</label></labels><created>2012-01-31T10:44:26Z</created><updated>2014-07-18T08:48:52Z</updated><resolved>2014-07-18T08:48:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="otisg" created="2012-02-06T20:04:58Z" id="3835703">+1 (didn't check the docs to make sure this is not already implemented.... maybe it is?)
</comment><comment author="ahfeel" created="2012-02-06T20:10:59Z" id="3835826">Not yet... but Shay has approved that this is a useful feature, so hopefully we'll get it soon ! ;)
</comment><comment author="ahfeel" created="2012-03-28T08:48:43Z" id="4751031">Hi Shay, Any idea on when you could get an eye on this one ? Thanks !
</comment><comment author="peillis" created="2012-05-27T20:29:41Z" id="5956165">+1
</comment><comment author="dakrone" created="2014-07-18T08:48:52Z" id="49408765">The issue with this is that if indices are opened and closed automatically, the amount of resources used cannot be determined without the cluster entering into a (potentially) bad state. This is something that, if implemented, should be implemented on the client side since it is much easier to track index activity from that level versus inside of Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport Client: Improve remote node freeze handling by adding another timeout layer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1653</link><project id="" key="" /><description>Add another setting: `client.transport.ping_timeout` (defaults to `5s`) which will timeout on cases where a request for the remote node info is not returned and the socket does not close. In such a case, disconnect from the node as well.
</description><key id="3025909">1653</key><summary>Transport Client: Improve remote node freeze handling by adding another timeout layer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-30T20:08:25Z</created><updated>2012-01-30T20:40:04Z</updated><resolved>2012-01-30T20:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>text_phrase_prefix query does not support highlighting unless field has term_vectors set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1652</link><project id="" key="" /><description>When performing a text_phrase_prefix query no highlighting is returned on fields that do not have text_phrase_prefix set to 'with_positions_offsets'.  I see highlighting for all fields when performing a normal text and a text_phrase query even if they do not have term vectors set.

I am using elasticsearch 0.18.6.

It looks like this user experienced the same issue (which led me to try setting term vectors):
http://elasticsearch-users.115913.n3.nabble.com/Text-Phrase-Query-td3409587.html
</description><key id="3023309">1652</key><summary>text_phrase_prefix query does not support highlighting unless field has term_vectors set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">f1sherman</reporter><labels /><created>2012-01-30T17:05:34Z</created><updated>2014-07-08T14:10:07Z</updated><resolved>2014-07-08T14:10:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2012-01-30T18:28:46Z" id="3724638">Thats right, the plain highlighter that works when term vectors are not enabled does not support this query.
</comment><comment author="clintongormley" created="2014-07-08T14:10:07Z" id="48340755">This works with the `match_phrase_prefix` query and the plain highlighter in 1.x.

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Allocation: cluster.routing.allocation.allow_rebalance does not allow for rebalancing on relocating shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1651</link><project id="" key="" /><description>The setting `cluster.routing.allocation.allow_rebalance` (defaults to `indices_all_active` means wait till the shards are active till we start rebalancing. This helps with allocating all indices before starting rebalance.

Currently though, it does not take rebalancing shard into account, meaning that once a single shard is moving (rebalanced), it will not allow for any other shard to be moved around. This effectively disables the `cluster.routing.allocation.cluster_concurrent_rebalance` setting (defaults to 2) by allowing for a single shard to rebalance and until its done, none will be allowed.
</description><key id="3014475">1651</key><summary>Cluster Allocation: cluster.routing.allocation.allow_rebalance does not allow for rebalancing on relocating shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-29T23:47:49Z</created><updated>2012-01-29T23:58:58Z</updated><resolved>2012-01-29T23:58:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Allocation: allow to specify maximum total number of shards per node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1650</link><project id="" key="" /><description>Allow to set the maximum number of shards allowed per node for a specific index by setting: `index.routing.allocation.total_shards_per_node`. This setting can be changed dynamically using the index update settings API, both to enable it (setting it to a value higher than 0), or disabel it (setting it to -1).
</description><key id="3014443">1650</key><summary>Index Allocation: allow to specify maximum total number of shards per node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-29T23:42:34Z</created><updated>2012-01-29T23:43:29Z</updated><resolved>2012-01-29T23:43:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Custom index-level metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1649</link><project id="" key="" /><description>Provide a way to attach metadata to an index, similar to what can be
done with type mappings.
</description><key id="3013132">1649</key><summary>Custom index-level metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels><label>discuss</label></labels><created>2012-01-29T20:20:34Z</created><updated>2014-07-18T08:45:01Z</updated><resolved>2014-07-18T08:45:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alambert" created="2012-02-08T22:01:43Z" id="3877204">Is this what you're looking for? http://www.elasticsearch.org/guide/reference/mapping/meta.html
</comment><comment author="ejain" created="2012-02-08T22:25:36Z" id="3877688">No, that's the type-mapping-level meta data I referred to.
</comment><comment author="alexliu68" created="2012-11-02T09:11:24Z" id="10008519">I attach a pull @ https://github.com/elasticsearch/elasticsearch/pull/2378
</comment><comment author="alexliu68" created="2012-11-02T09:16:55Z" id="10008634">create index with custom meta

   {
     "settings" : {
          "number_of_shards" : 3,
          "number_of_replicas" : 2
       },
       "custom_meta" : {
           "name1" : "value1",
           "name2" : { "attr1": "value1", "attr2":"value2"},
           "name3" : number
        }
   }

delete
  /{index}/_custom_meta
  /{index}/_custom_meta/{name}

put
  /{index}/_custom_meta/{name}

get
  /{index}/_custom_meta
  /{index}/_custom_meta/{name}
</comment><comment author="clintongormley" created="2014-07-08T14:06:21Z" id="48340236">Perhaps something that could be supported via index settings?
</comment><comment author="clintongormley" created="2014-07-18T08:45:01Z" id="49408474">After discussion, this could simply be implemented by using a type `meta` and storing one document in it.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: prefix query to support _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1648</link><project id="" key="" /><description>Even though we don't index the `_id` field explicitly, we can support prefix style query/filter on it (in a similar manner that we support term(s) query/filter on it - though there is also an explicit `ids` query). 

There isn't really a need to add an `ids_prefix` style query/filter, since we can provide the same functionality automatically with `prefix` (again, in a similar manner that `ids` query/filter is no longer really needed).

The `prefix` query/filter will automatically work only against the specific type(s) if searching against the type (defined in the search request), or the field is prefixed by it (`my_type._id` as the prefix field name).
</description><key id="3012706">1648</key><summary>Query DSL: prefix query to support _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.19.0.RC1</label></labels><created>2012-01-29T19:08:47Z</created><updated>2012-01-29T19:09:19Z</updated><resolved>2012-01-29T19:09:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms Facet sorting does not handle null values properly with new JDK 7 sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/1647</link><project id="" key="" /><description /><key id="3011798">1647</key><summary>Terms Facet sorting does not handle null values properly with new JDK 7 sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.19.0.RC1</label></labels><created>2012-01-29T16:04:17Z</created><updated>2012-01-29T16:17:46Z</updated><resolved>2012-01-29T16:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item></channel></rss>