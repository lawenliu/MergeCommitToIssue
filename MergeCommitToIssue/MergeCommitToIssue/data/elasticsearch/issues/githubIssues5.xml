<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Remove PROTOTYPEs from Ranges</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17858</link><project id="" key="" /><description>Relates to #17085
</description><key id="149479767">17858</key><summary>Remove PROTOTYPEs from Ranges</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T14:36:10Z</created><updated>2016-04-19T15:48:01Z</updated><resolved>2016-04-19T15:47:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T14:39:47Z" id="211953286">LGTM
</comment><comment author="nik9000" created="2016-04-19T15:48:01Z" id="211988755">Thanks for the review @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `action.get.realtime` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17857</link><project id="" key="" /><description>PR for #12543
</description><key id="149477936">17857</key><summary>Remove `action.get.realtime` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Settings</label><label>breaking</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T14:30:26Z</created><updated>2016-04-19T15:21:49Z</updated><resolved>2016-04-19T15:21:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T14:59:21Z" id="211965753">Left minor suggestions, mostly around wording in the docs. Other than those small things LGTM.
</comment><comment author="martijnvg" created="2016-04-19T15:12:16Z" id="211972485">thanks @nik9000! I've addressed your comments and pushed a new commit.
</comment><comment author="nik9000" created="2016-04-19T15:13:23Z" id="211973035">LGTM
</comment><comment author="martijnvg" created="2016-04-19T15:21:48Z" id="211976083">Pushed to master: https://github.com/elastic/elasticsearch/commit/ba08313417b96086eea8de9d5a8d8501bf2b9806
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename percolator query to percolate query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17856</link><project id="" key="" /><description /><key id="149474294">17856</key><summary>Rename percolator query to percolate query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T14:17:32Z</created><updated>2016-04-20T13:25:42Z</updated><resolved>2016-04-20T13:25:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-20T07:27:02Z" id="212297721">This looks good to me but let's give @clintongormley a chance to have a look at it before merging.
</comment><comment author="clintongormley" created="2016-04-20T12:34:15Z" id="212405847">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing privileges in shield documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17855</link><project id="" key="" /><description>On Kibana shield integration page the following privilege is mentioned 'view_index_metadata'
https://www.elastic.co/guide/en/shield/2.3/kibana.html

However within the shield documentation this privilege is not mentioned
https://www.elastic.co/guide/en/shield/current/shield-privileges.html

In the past there were also a much more fine grained privileges like these
indices:admin/mappings/fields/get
indices:admin/validate/query
indices:admin/get

But I could not find any documentation about this in the current version.
Has the privilege management changed in 2.3?
</description><key id="149461050">17855</key><summary>Missing privileges in shield documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">janbernhardt</reporter><labels /><created>2016-04-19T13:26:23Z</created><updated>2016-04-20T00:44:37Z</updated><resolved>2016-04-20T00:44:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2016-04-20T00:44:37Z" id="212185672">Hey @janbernhardt .  For community support of Shield, we use our [discuss forums](https://discuss.elastic.co/c/shield) rather than the ES/Kibana github repos.  (Commercial support comes directly through our support team for our paying customers).  I'm going to close this issue here, but feel free to open this or other topics on discuss if my below answer isn't satisfactory.

For this particular issue, there does appear to be a documentation gap in the Shield privileges documentation.  The fine-grained privileges you listed often caused confusion and warranted a re-evaluation.   I've let the team know about the documentation gap, so thanks for reporting this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings in master yield different results compared to alpha1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17854</link><project id="" key="" /><description>I am not yet a hundred percent sure if this is a bug or a feature... After yesterdays mapping refactoring there is a discrepancy in the mapping being returned between master and alpha1

To reproduce simply run the below snippet against both versions

``` bash
port=9200
curl -X DELETE localhost:$port/_template/my-template
curl -X PUT localhost:$port/_template/my-template -d '{
  "template": "my-index",
  "mappings": {
    "my-type": {
      "dynamic_templates": [
        {
          "disabled_payload_fields": {
            "path_match": "transform(\\..+)*\\.payload",
            "match_pattern": "regex",
            "mapping": {
              "type": "object",
              "enabled": false
            }
          }
        }
      ],
      "dynamic": false,
      "properties": {
        "transform" : {
          "type" : "object",
          "dynamic": true
        }
      }
    }
  }
}'

curl -X DELETE localhost:$port/my-index
curl -X PUT localhost:$port/my-index/my-type/1 -d '{
  "name" : "foo",
  "transform" : {
    "foo" : {
      "status" : "the-state" ,
      "payload" : { "_value" : "the payload" }
    }
  }
}'

curl localhost:$port/my-index/my-type/_mapping?pretty
```

On master the last mapping call returns

``` json
{
  "my-index" : {
    "mappings" : {
      "my-type" : {
        "dynamic" : "false",
        "dynamic_templates" : [ {
          "disabled_payload_fields" : {
            "path_match" : "transform(\\..+)*\\.payload",
            "match_pattern" : "regex",
            "mapping" : {
              "enabled" : false,
              "type" : "object"
            }
          }
        } ],
        "properties" : {
          "transform" : {
            "dynamic" : "true",
            "properties" : {
              "foo" : {
                "type" : "object"
              }
            }
          }
        }
      }
    }
  }
}
```

where as on alpha1 this is returned (which looks correct to me, as the mapping is supposed to be dynamic)

``` json
{
  "my-index" : {
    "mappings" : {
      "my-type" : {
        "dynamic" : "false",
        "dynamic_templates" : [ {
          "disabled_payload_fields" : {
            "path_match" : "transform(\\..+)*\\.payload",
            "match_pattern" : "regex",
            "mapping" : {
              "enabled" : false,
              "type" : "object"
            }
          }
        } ],
        "properties" : {
          "transform" : {
            "dynamic" : "true",
            "properties" : {
              "foo" : {
                "dynamic" : "true",
                "properties" : {
                  "payload" : {
                    "type" : "object",
                    "enabled" : false
                  },
                  "status" : {
                    "type" : "text",
                    "fields" : {
                      "keyword" : {
                        "type" : "keyword",
                        "ignore_above" : 256
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</description><key id="149457883">17854</key><summary>Mappings in master yield different results compared to alpha1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T13:11:47Z</created><updated>2016-04-19T21:25:49Z</updated><resolved>2016-04-19T21:25:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-19T13:29:59Z" id="211923658">I think this is related to #17759 cc @rjernst.
</comment><comment author="clintongormley" created="2016-04-19T17:02:35Z" id="212017705">Is it that, or https://github.com/elastic/elasticsearch/issues/17644 ?
</comment><comment author="rjernst" created="2016-04-19T17:13:55Z" id="212022901">I think it is both. In #17759 I removed a single line of code which would have been making this "work" for this specific use case, but would have been broken in others. So it is correct that the root problem is #17644.

I'm working on a change to rework how dynamic is looked up while parsing. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>POST index/_settings with body produces wrong exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17853</link><project id="" key="" /><description>**Elasticsearch version**: 2.1 - master

**Description of the problem including expected versus actual behavior**: Sending even an empty body (`url localhost:9200/test/_settings -d ''` or curl localhost:9200/test/_settings -d '{}') results in a (wrong) exception - `mapper_parsing_exception`

**Steps to reproduce**:
1. `curl -X PUT localhost:9200/test`
2. `curl localhost:9200/test/_settings -d ''`
</description><key id="149447545">17853</key><summary>POST index/_settings with body produces wrong exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-04-19T12:31:38Z</created><updated>2017-03-12T23:34:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nikoncode" created="2016-04-19T12:52:49Z" id="211911027">What behaviors are expected?
</comment><comment author="HonzaKral" created="2016-04-19T14:15:39Z" id="211939943">Either for it to ignore the body and respond with the settings or provide a relevant exception saying that sending body is not allowed with this API
</comment><comment author="clintongormley" created="2016-04-20T11:41:55Z" id="212392155">It's a POST request, not a GET request, and it is somehow matching a `POST /{index}/_mapping` handler
</comment><comment author="alexshadow007" created="2016-04-24T09:52:01Z" id="213920915">@clintongormley 
It is matching `POST /{index}/{type}` handler.
It is trying to create dynamic mapping for type `_settings`, but can't and throws MapperParsingException.
</comment><comment author="jeff303" created="2016-05-17T04:03:52Z" id="219614104">I'm new to the project, so take everything here with a huge grain of salt.  It's not clear to me how responding to the POST the same way as to the GET, or changing the exception could be accomplished cleanly.

`GET /{index}/_settings` is mapped in `RestGetIndicesAction`, with the _settings portion being covered by a feature.  In theory, a specific POST handler could be added for `/{index}/_settings` but it probably shouldn't go in this class, since it's explicitly for GET requests.

Absent a special handler for this, even changing the exception would require some kind of special handling in `RestIndexAction`.  But it would seem that prevents someone from dynamically indexing a document with type `_settings`, and why shouldn't that be allowed?
</comment><comment author="SushantVarshney" created="2016-06-30T05:15:15Z" id="229561320">Also facing the same issue.Is there any way to overcome this?
</comment><comment author="qwerty4030" created="2016-08-28T18:47:35Z" id="242991893">@clintongormley @jbertouch I think this may be user error. curl and sense automatically convert a `GET` request with a body to a `POST`. I was able to reproduce the error on latest master, but changing the request to the following gives the correct index settings (request body ignored):
`curl -X GET localhost:9200/test/_settings -d ''`
</comment><comment author="jasontedor" created="2016-08-28T20:24:57Z" id="242997116">&gt; I think this may be user error.

@qwerty4030 Whether or not it's user error, there is the issue that a `POST` request to `/{index}/_settings` is being treated as an index request when ideally it would match the update settings handler for `/{index}/_settings` and return a 405 since this endpoint only accepts `PUT` requests. Instead, it's matching the `POST` handler for `/{index}/{type}` and that's why the we see a mapper parsing exception here.
</comment><comment author="AdityaJNair" created="2017-03-12T23:29:22Z" id="285986770">Hey do you mind if we can have a go at this issue?</comment><comment author="jasontedor" created="2017-03-12T23:34:48Z" id="285987208">&gt; Hey do you mind if we can have a go at this issue?

Thanks for your interest. There's already an open PR to address this issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES running in docker container can not discover the host's name and the host's ip</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17852</link><project id="" key="" /><description>Elasticsearch version:  v2.3.1
JVM version:   1.8.0_77
OS version:  centos:6.6

ES running in docker container can not discover the host's name and the host's ip, I used the HOST network model&#65292; but the es in container can not connect to the master, the logs like this( i run a es cluster on docker,  but they can not recognize each other) : 

```
[2016-04-19 19:53:25,536][INFO ][discovery.zen            ] [Cloak] failed to send join request to master [{Krakkan}{hzJNm3weSkqc-mXxf7qV4g}{127.0.0.1}{127.0.0.1:9300}], reason [RemoteTransportException[[Cloak][127.0.0.1:9300][internal:discovery/zen/join]]; nested: IllegalStateException[Node [{Cloak}{hFZIlf9cQaCgltOMgq88CA}{127.0.0.1}{127.0.0.1:9300}] not master for join request]; ]
```

And i use the cat api, shows like:

```
host      ip        heap.percent ram.percent load node.role master name    
127.0.0.1 127.0.0.1            3          94 0.03 d         *      Krakkan
```

hostname and ip seem not correct for discovery, is something wrong?  And how can i  handle this?
</description><key id="149438748">17852</key><summary>ES running in docker container can not discover the host's name and the host's ip</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wenma</reporter><labels /><created>2016-04-19T12:02:00Z</created><updated>2016-04-19T12:18:31Z</updated><resolved>2016-04-19T12:18:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T12:18:31Z" id="211893751">Hi @wenma 

We don't provide or support a docker image of Elasticsearch.  I don't know what the HOST network model is, or how you're configuring Elasticsearch, but the network config looks incorrect or missing.

I suggest asking questions like these in the forum instead: https://discuss.elastic.co/

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FiltersAggregatorBuilder: Don't create new context for inner parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17851</link><project id="" key="" /><description>We don't have to create new parse contexts for inner query parsing if we use the same parser and parseFieldMatcher.
</description><key id="149436083">17851</key><summary>FiltersAggregatorBuilder: Don't create new context for inner parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T11:51:06Z</created><updated>2016-04-19T12:36:39Z</updated><resolved>2016-04-19T12:28:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T12:16:10Z" id="211892027">LGTM
</comment><comment author="cbuescher" created="2016-04-19T12:36:39Z" id="211903322">This was merged to master with bbe03c92c2c75971bd32e49c9342a856398d0382, somehow GH got the hickups...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Appending object to an existing array of objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17850</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:2.2.1

**JVM version**: 7

**OS version**: CentOS 6.5

**Description of the problem including expected versus actual behavior**: I am trying to append a new object to an existing array of objects, but instead of appending, the whole array is replaced with the new value.

```
POST temp1
{
  "mappings": {
    "conv": {
      "properties": {
        "name": {
          "type": "string"
        },
        "participants": {
          "type": "nested",
          "properties": {
            "name": {
              "type": "string"
            },
            "nick": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}

POST /temp/conv/1/_update
{
  "script": "ctx._source.participants += new_creator",
  "params": {
    "type": "object",
    "new_creator": [
      {
        "name": "a",
        "nick": "nick-c"
      }
    ]
  }
}
```

Could you pls let me know whether this is a bug or am I doing it wrongly?
</description><key id="149423334">17850</key><summary>Appending object to an existing array of objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sachinbrraj</reporter><labels /><created>2016-04-19T10:53:40Z</created><updated>2016-04-19T11:10:41Z</updated><resolved>2016-04-19T11:10:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T11:10:41Z" id="211863687">This works just fine, as long as the original document (which you didn't provide) already has an array for `participants`:

```
PUT temp/conv/1
{
  "participants": [
    {
      "name": "a",
      "nick": "nick-a"
    }
  ]
}

POST /temp/conv/1/_update
{
  "script": "ctx._source.participants += new_creator",
  "params": {
    "type": "object",
    "new_creator": {
      "name": "b",
      "nick": "nick-b"
    }
  }
}

GET /temp/conv/1
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue on shield role documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17849</link><project id="" key="" /><description>On https://www.elastic.co/guide/en/shield/current/defining-roles.html

you refer to the user URL when talking about roles
`GET /_shield/user/my_admin_role,log_admin_role`

from my understanding it should be 
`GET /_shield/role/my_admin_role,log_admin_role`
</description><key id="149419651">17849</key><summary>Issue on shield role documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">janbernhardt</reporter><labels><label>docs</label></labels><created>2016-04-19T10:40:25Z</created><updated>2016-04-19T10:57:18Z</updated><resolved>2016-04-19T10:57:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T10:57:14Z" id="211860332">thanks @janbernhardt - i've fixed that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms that not match are being highlighted </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17848</link><project id="" key="" /><description>**Elasticsearch version**: 2.0.1 **JVM version**:1.8.0_31 **OS version**: Win7

**Description**:
Terms are being highlighted even if they don't participate in the query match.

**Setup**:

``` shell
# an index
curl -XPUT 'localhost:9200/my-index' -d '{
    "mappings": {
        "my-type": {
            "properties": {
                "message": {
                    "type": "string",
                    "store": "yes",
                    "index": "analyzed",
                    "analyzer": "my_word_delimiter"
                }
            }
        }
    },
    "settings": {
        "analysis": {
            "analyzer": {
                "my_word_delimiter": {
                    "type": "custom",
                    "tokenizer": "whitespace",
                    "filter": ["word_delimiter"]
                }
            }
        }
    }
}'

# a document
curl -XPOST 'localhost:9200/my-index/my-type' -d '{
    "message": "Bruno de Carvalho &#233; o presidente do Sporting"
}'
```

**Testing highlight**:

``` shell
curl -XPOST 'localhost:9200/my-index/my-type/_search' -d '{
  "query": {
    "query_string": {
      "query": "(message:\"Clube de Portugal\") OR (message:Sporting)",
      "auto_generate_phrase_queries": true
    }
  },
  "highlight": {
    "require_field_match": true,
    "fields": {
      "message": {}
    }
  }
}'

# results
{
    "hits": [{
        "_source": {
            "message": "Bruno de Carvalho &#233; o presidente do Sporting"
        },
        "highlight": {
            "message": [
                "Bruno &lt;em&gt;de&lt;/em&gt; Carvalho &#233; o presidente do &lt;em&gt;Sporting&lt;/em&gt;"
            ]
        }
    }]
}
```

**Expected result**: Only "Sporting" should be highlighted. The query part "Clube de Portugal" does not match and thus the word "de" shouldn't be highlighted.

**Testing the search**:

``` shell
curl -XPOST 'localhost:9200/my-index/my-type/_search' -d '{
  "query": {
    "query_string": {
      "query": "(message:\"Clube de Portugal\")",
      "auto_generate_phrase_queries": true
    }
  },
  "highlight": {
    "require_field_match": true,
    "fields": {
      "message": {}
    }
  }
}'

# results
"hits": [ ]
```

Result OK: "Clube de Portugal" does not match.
</description><key id="149416949">17848</key><summary>terms that not match are being highlighted </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fmont</reporter><labels><label>:Highlighting</label></labels><created>2016-04-19T10:29:35Z</created><updated>2016-11-25T15:52:02Z</updated><resolved>2016-11-25T15:52:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T10:51:16Z" id="211857099">All I can tell you is that the fast vector highlighter does a better job with phrase queries.
</comment><comment author="fmont" created="2016-04-19T15:56:54Z" id="211992970">definitely works better, thank you!
</comment><comment author="clintongormley" created="2016-11-25T15:52:02Z" id="262984503">This has been fixed in a recent release</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script_sort using "_score" alone is not valid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17847</link><project id="" key="" /><description>**Elasticsearch version 2.3.1**:

**JVM version 1.8.0_65**:

**OS version CentOS 6.5**:

**Steps to reproduce**:
1. Using only _score variables

``` json
"sort": [
    {
      "_script": {
        "script": {
          "inline": "_score"
        },
        "type": "number",
        "reverse": true
      }
    }
  ]
```

result : 

``` json
"_score": null,
"_source": { },
"sort": [
     0
],
```

2.Using _score variables and score-sort

``` json
"sort": [
    {
      "_script": {
        "script": {
          "inline": "_score"
        },
        "type": "number",
        "reverse": true
      }
    },
    "_score"
  ]
```

result:

``` json
"_score": 1.6844505,
"_source": { },
"sort": [
          1.6844505071640015,
          1.6844505
]
```
</description><key id="149408588">17847</key><summary>Script_sort using "_score" alone is not valid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">4onni</reporter><labels><label>:Scripting</label><label>:Search</label><label>discuss</label></labels><created>2016-04-19T09:55:11Z</created><updated>2016-04-21T04:32:56Z</updated><resolved>2016-04-21T04:32:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T10:39:40Z" id="211851333">Scores are not calculated unless they're actually going to be used.  Your first example doesn't explicitly use scores, so score calculation is disabled.  You can force the score to be calculated anyway by adding `?track_scores` to your search request.

The question is: can we detect that `_score` is being used inside the script automatically?
</comment><comment author="4onni" created="2016-04-19T12:22:13Z" id="211897881">Thanks for your reply, adding  `track_scores` has been solved. What I don't understand is how to use it explicitly. Is "_score + 1" this way? Through my experiments, I still can't do it. For example:

``` json
"sort": [
    {
      "_script": {
        "script": {
          "inline": "_score + 1"
        },
        "type": "number",
        "reverse": true
      }
    }
  ]
```

result:

``` json
"_score": null,
        "_source": { },
        "sort": [
          1
        ],
```
</comment><comment author="clintongormley" created="2016-04-20T11:22:20Z" id="212386401">&gt; What I don't understand is how to use it explicitly. Is "_score + 1" this way?

No.  Scripts don't count.  Your original example showed how to use it explicitly: by sorting by `_score`.
</comment><comment author="4onni" created="2016-04-21T04:32:56Z" id="212738011">Thank you, I understand.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moves reduce to the AggregatorBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17846</link><project id="" key="" /><description>This adds the reduce method to AggregatorBuilder so that reduce only fields such as Pipeline Aggregators and Aggregation Metadata do not need to be serialised to the shards and back again, simplifying the aggregation framework.
</description><key id="149403300">17846</key><summary>Moves reduce to the AggregatorBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>review</label><label>stalled</label></labels><created>2016-04-19T09:32:25Z</created><updated>2016-05-11T13:28:27Z</updated><resolved>2016-05-11T13:23:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-19T15:11:28Z" id="211972210">I left a comment but I like the idea. This way we could stop having to propagate metadata and pipeline aggs all the way through factories, aggregators and aggregations.
</comment><comment author="jpountz" created="2016-04-20T07:39:19Z" id="212301311">LGTM
</comment><comment author="colings86" created="2016-05-11T13:28:27Z" id="218458811">Although this would be nice to add, this implementation needs more thinking about / exploring as the reductions of sub aggregations is not straight forward when moving them to the AggregationBuilder implementations
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java.lang.NullPointerException: Cannot get property &#8216;x&#8217; on null object - when running a groovy script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17845</link><project id="" key="" /><description>**Elasticsearch version**: 2.0.0

**JVM version**: Not sure - running on elastic cloud

**OS version**: Not sure - running on elastic cloud

**Description of the problem including expected versus actual behavior**:
My use case is trying to update a document simultaneously with 2  of the options of the update api, sometimes update fails and sometimes it is successful.

I use a Transport client using elastic4s as my client.

so my document looks like this:

```
{ 
   "field": {
      "names": ["A","B","C"],
      "A": { "name": "A is awesome" },
      "B": { "name": "B is awesome" },
      "C": { "name": "C is awesome" }
   },
   "email": "x@gmail.com"
}
```

I am trying to update the names field with one request using inline script, and add another field under "field" with another request using the doc api. Those 2 requests are running simultaneously.

So here is my code:

```
  override def addTerms(String, terms: Set[String]) = {
    val groovyScript =
      s"""if (ctx._source.field.names== null)
          |   ctx._source.field.names=newItems
          |else
          |   ctx._source.field.names&lt;&lt; newItems;
          |ctx._source.field.names= ctx._source.field.names.flatten().unique();""".stripMargin
    client.execute(ElasticDsl.update id id in indexName / documentType retryOnConflict 5 script {
      script(groovyScript).params(Map("newItems" -&gt; terms.toArray))
    })
  }   

  override def update(id: String, field: NestedFieldValue) = {
    client.execute {
      ElasticDsl.update(id) in indexName / documentType retryOnConflict 5 doc field
    }
  }
```

Executing the above method in parallel fails, but when I run it sequentially it works.
This works: 

`update(id, field).flatMap(_ =&gt; addTerms(id, path, terms))`

**Steps to reproduce**:
1. Running the above methods in parallel fails with a NullPointerException coming from groovy engine.
   So here is a piece of code:

```
for{
 _ &lt;- update(id, field)
 _ &lt;- addTerms(id, path, terms)
}yield{}

```

**Provide logs (if relevant)**:

```

Caused by: java.lang.IllegalArgumentException: failed to execute script 
   at org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:256) ~[org.elasticsearch.elasticsearch-2.0.1.jar:2.0.1] 
   at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:196) ~[org.elasticsearch.elasticsearch-2.0.1.jar:2.0.1] 
   at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:79) ~[org.elasticsearch.elasticsearch-2.0.1.jar:2.0.1] 
   at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170) ~[org.elasticsearch.elasticsearch-2.0
   at org.elasticsearch.action.update.TransportUpdateAction$3$1.doRun(TransportUpdateAction.java:227) ~[org.elasticsearch.elasticsearch-2.0.1.
   at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[org.elasticsearch.elasticsearch-2.0.1.jar:2.0.1]
   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_74-cedar14] 
   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_74-cedar14] 
   at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_74-cedar14] 
 Caused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: failed to run inline script [if (ctx._source.field.names == null) 
     ctx._source.field.names=newItems 
  else 
     ctx._source.field.names&lt;&lt; newItems; 
     ctx._source.field.names = ctx._source.field.names.flatten().unique()] using lang [groovy] 
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253) ~[org.elasticsearch.
    at org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:251) ~[org.elasticsearch.elasticsearch-2.0.1.jar:2.0.1] 
    ... 8 common frames omitted 
  Caused by: java.lang.NullPointerException: Cannot get property 'names' on null object 
    at org.codehaus.groovy.runtime.NullObject.getProperty(NullObject.java:60) ~[org.codehaus.groovy.groovy-all-2.4.0.jar:2.4.0] 
    at org.codehaus.groovy.runtime.InvokerHelper.getProperty(InvokerHelper.java:172) ~[org.codehaus.groovy.groovy-all-2.4.0.jar:2.4.0] 
    at org.codehaus.groovy.runtime.callsite.NullCallSite.getProperty(NullCallSite.java:47) ~[org.codehaus.groovy.groovy-all-2.4.0.jar:2.4.0] 
    at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGetProperty(AbstractCallSite.java:296) ~[org.codehaus.groovy.groovy-all-2.4.0.
    at 889babff0826c95e16279a9fb1fd2eb97a63e642.run(889babff0826c95e16279a9fb1fd2eb97a63e642:1) ~[na:na] 
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:248) ~[org.elasticsearch.
```
</description><key id="149401161">17845</key><summary>Java.lang.NullPointerException: Cannot get property &#8216;x&#8217; on null object - when running a groovy script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tpraizler</reporter><labels><label>:CRUD</label><label>:Scripting</label><label>feedback_needed</label></labels><created>2016-04-19T09:22:54Z</created><updated>2016-05-24T10:36:17Z</updated><resolved>2016-05-24T10:36:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T10:32:16Z" id="211848238">From the error message, it looks like your `field` is missing some times, hence the NPE.  I'm unable to replicate this.  I tried running these two requests in parallel:

```
POST /t/t/1/_update?pretty=1&amp;retry_on_conflict=5
{
   "script" : "if (ctx._source.field.names== null) {ctx._source.field.names=newItems} else {ctx._source.field.names&lt;&lt; newItems}; ctx._source.field.names= ctx._source.field.names.flatten().unique();",
   "params" : {
      "newItems" : [
         "foo",
         "bar"
      ]
   }
}
```

and

```
POST /t/t/1/_update?pretty=1&amp;retry_on_conflict=5
{
  "retry_on_conflict": 5,
  "doc": {
    "field": {
      "foo": {
        "name": "foo is awesome"
      }
    }
  }
}
```

and I reset the doc every now and again with 

```
POST t/t/1
{ 
   "field": {
      "names": ["A","B","C"],
      "A": { "name": "A is awesome" },
      "B": { "name": "B is awesome" },
      "C": { "name": "C is awesome" }
   },
   "email": "x@gmail.com"
}
```

No joy.  I think you have some code somewhere which is updating this doc with a source that doesn't include `field`.  A runnable recreation would be appreciated (and in something other than Scala :) )
</comment><comment author="tpraizler" created="2016-04-20T14:37:53Z" id="212450902">Thanks @clintongormley for looking into this! 
Did you run it on one documents? I run around 80 simultaneously requests to ES, 2 for every one of the 40 documents. This behavior is pretty random, it fails almost every execution of those 80 futures, but on different document every time.
Does this information help in some way? Are you able to create such scenario?
</comment><comment author="clintongormley" created="2016-04-20T14:55:13Z" id="212460774">@tpraizler your original description lacks some steps.  could you provide an exact recreation (something easy to run without installing lots of libraries) to try it out?
</comment><comment author="clintongormley" created="2016-05-24T10:36:17Z" id="221230838">No further feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API : Fix/deprecated filters in analyze in 2x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17844</link><project id="" key="" /><description>Accept "filter"/"char_filter" and deprecated  "filters"/"char_filters" in analyze API

This PR is for 2.x branch.

Closes #15189
</description><key id="149400766">17844</key><summary>Analyze API : Fix/deprecated filters in analyze in 2x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v2.3.2</label></labels><created>2016-04-19T09:20:56Z</created><updated>2016-04-21T07:21:40Z</updated><resolved>2016-04-20T17:31:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T13:34:51Z" id="211924983">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API : Rename filters/token_filters/char_filter in Analyze API in master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17843</link><project id="" key="" /><description>Analyze API : Rename filters/token_filters/char_filters to filter/token_filter/char_filter

This PR is for master branch.

Closes #15189
</description><key id="149400446">17843</key><summary>Analyze API : Rename filters/token_filters/char_filter in Analyze API in master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>breaking</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T09:19:27Z</created><updated>2016-04-28T15:00:36Z</updated><resolved>2016-04-21T09:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2016-04-19T09:23:15Z" id="211821908">@clintongormley I renamed only request params and didn't change response and Java API. 
Does it make sense? Or should we change response and Java methods?
What do you think?
</comment><comment author="clintongormley" created="2016-04-20T16:43:24Z" id="212508017">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistency with index time between _stats api and the _bulk respone api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17842</link><project id="" key="" /><description>When I do a bulk indexing and look at the return response, the "took" attribute in the response I believe is the time taken to index the given set of documents. The "took" time doesn't match the index time in _stats api

Below is a sample response and _stats api data for it
_bulk response
{
### took: 67,

  errors: false,
  items: [..]
}

_stats api

indexing: {
index_total: 10,
**index_time_in_millis: 220**,
index_current: 0,
delete_total: 0,
delete_time_in_millis: 0,
delete_current: 0,
noop_update_total: 0,
is_throttled: false,
throttle_time_in_millis: 0
}

**Elasticsearch version**: 2.0.2

**JVM version**:1.8

**OS version**: Windows 7
</description><key id="149380849">17842</key><summary>Inconsistency with index time between _stats api and the _bulk respone api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SKumarMN</reporter><labels /><created>2016-04-19T08:07:46Z</created><updated>2016-05-18T08:59:04Z</updated><resolved>2016-04-19T10:10:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T10:10:07Z" id="211841034">The `took` time measures how long the request took.  The index time measures how much time was spent in indexing threads. Threads can execute in parallel, so two threads in parallel which take 10ms each would result in 20ms of indexing time.
</comment><comment author="SKumarMN" created="2016-05-13T07:51:14Z" id="218975122">@clintongormley  Thanks. How to find out the exact time of indexing activity. We are interested in finding the exact time taken for indexing millions of documents we have. Based on this we want to see how we can fine tune any indexing params
</comment><comment author="danielmitterdorfer" created="2016-05-13T08:13:57Z" id="218979490">@SKumarMN I am not sure what you mean by "exact time". There are two possibilities:
- You want to know how many documents / second you can index, meaning you are interested in throughput. Then you should look at "took" (although this is only the time spent in request processing in ES. It does not and cannot include the time for the whole roundtrip. For that you have to measure on the client).
- You want to know how much time Elasticsearch spends during indexing in total (this is _not_ Wall clock time but a sum of the time spend in all indexing threads as explained by @clintongormley earlier). Then you need to look at `index_time_in_millis`
</comment><comment author="SKumarMN" created="2016-05-18T07:20:08Z" id="219946069">@danielmitterdorfer thanks for your response. If I want to find out the wall clock time then I need to consider "took" + "round trip" at client side right ? Using stats API is there a way to get it after certain time period? . Also how do we measure if the indexing is optimal in our cluster . Do I need to concentrate on index rate number of docs/index_time_in_millis ?
</comment><comment author="danielmitterdorfer" created="2016-05-18T08:58:45Z" id="219966647">@SKumarMN The "+" sign is implying that you want to sum up these values. Don't. "took" is the time it took Elasticsearch to process your request. The round trip time is something you have to measure yourself (Elasticsearch cannot know this time). You basically need to start a timer, when you issue the request and stop it when you get the response. The difference is the round trip time.

The stats API just tells you the total indexing time but not Wall clock time AFAIK.

The [Indexing Performance Tips](https://www.elastic.co/guide/en/elasticsearch/guide/current/indexing-performance.html) section of the Elasticsearch Definitive Guide should get you started on measuring. You should concentrate on the documents / second that you can add to the index. I think that [Rally](https://github.com/elastic/rally) could also help you if you don't have the necessary test infrastructure yet. But keep the following caveats in mind:
- You need a Unix machine for Rally (I see you mention Windows 7 in the ticket). It's ok to benchmark a Windows machine though
- You need to add your own benchmark data to Rally. It's doable but I work on simplifying this over the coming weeks.
- You want to vary the bulk request size. That is also doable but also not well supported currently.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update has-parent-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17841</link><project id="" key="" /><description>Change reference to `score_mode`, to `score`
</description><key id="149343275">17841</key><summary>Update has-parent-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">russcam</reporter><labels><label>docs</label></labels><created>2016-04-19T04:06:23Z</created><updated>2016-04-20T12:14:52Z</updated><resolved>2016-04-19T09:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="russcam" created="2016-04-19T04:06:59Z" id="211721135">I think this change should also be captured in https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_search_changes.html#_changes_to_queries
</comment><comment author="clintongormley" created="2016-04-19T09:56:01Z" id="211832839">&gt; I think this change should also be captured in https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_search_changes.html#_changes_to_queries

Fixed in https://github.com/elastic/elasticsearch/commit/c02450484203a0363461d72ff36512a0134ff8f1
</comment><comment author="russcam" created="2016-04-20T00:36:34Z" id="212184590">To confirm, is `score_mode` _deprecated_ or _removed_?
</comment><comment author="clintongormley" created="2016-04-20T12:14:52Z" id="212401628">deprecated
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove readFrom from some classes in index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17840</link><project id="" key="" /><description>These methods aren't used. They were just there to implement an interface
and that interface is losing that method.

Relates to #17085
</description><key id="149321423">17840</key><summary>Remove readFrom from some classes in index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T01:33:56Z</created><updated>2016-04-20T12:24:50Z</updated><resolved>2016-04-20T12:16:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T01:34:07Z" id="211675763">More of the prototype busting, @cbuescher 
</comment><comment author="jasontedor" created="2016-04-19T01:49:45Z" id="211679740">The serialization in `StoreFileMetaData` is weird; can we find out what is going on there?
</comment><comment author="nik9000" created="2016-04-20T11:24:58Z" id="212386896">@jasontedor I cleaned this up a tiny bit. Want to look again?
</comment><comment author="jasontedor" created="2016-04-20T11:37:26Z" id="212390793">Oh, now it's clear. LGTM.
</comment><comment author="nik9000" created="2016-04-20T12:08:38Z" id="212399819">&gt; Oh, now it's clear. LGTM.

Thanks for reviewing! It really is 100% better when reading and writing are right next to each other and one line per field.
</comment><comment author="jasontedor" created="2016-04-20T12:24:50Z" id="212403589">&gt; It really is 100% better when reading and writing are right next to each other and one line per field.

Yes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove yet more of the specific methods for reading and writing NamedWriteables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17839</link><project id="" key="" /><description>This is just more cleanup related to #17682.
</description><key id="149319668">17839</key><summary>Remove yet more of the specific methods for reading and writing NamedWriteables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T01:23:30Z</created><updated>2016-05-02T12:11:45Z</updated><resolved>2016-04-19T15:37:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T01:23:52Z" id="211672299">@ywelsch this is the second half of #17838, it removes the last of the specialized readers.
</comment><comment author="ywelsch" created="2016-04-19T08:42:25Z" id="211801575">Thanks for the cleanup @nik9000. LGTM.
</comment><comment author="nik9000" created="2016-04-19T15:38:00Z" id="211983509">Thanks for reviewing @ywelsch !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove more of the specific methods for reading and writing NamedWriteables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17838</link><project id="" key="" /><description>This is just more cleanup related to #17682.
</description><key id="149315504">17838</key><summary>Remove more of the specific methods for reading and writing NamedWriteables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-19T01:03:00Z</created><updated>2016-05-02T12:11:52Z</updated><resolved>2016-04-19T14:33:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T01:03:54Z" id="211662959">@ywelsch more in the vein of the last one.
</comment><comment author="ywelsch" created="2016-04-19T08:38:58Z" id="211798965">LGTM
</comment><comment author="nik9000" created="2016-04-19T14:34:14Z" id="211948495">Thanks for reviewing @ywelsch !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor UUID-generating methods out of Strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17837</link><project id="" key="" /><description>This commit refactors the UUID-generating methods out of `Strings` into
their own class. The primary motive for this refactoring is to avoid a
chain of class initializers from loading this class earlier than
necessary. This was discovered when it was noticed that starting
Elasticsearch without any active network interfaces leads to some
logging statements being executed before logging had been
initailized. Thus:
- these UUID methods have no place being on `Strings`
- removing them reduces spooky action-at-distance loading of this class
- removed the troublesome, logging statements from `MacAddressProvider`,
  logging using statically-initialized instances of `ESLogger` are prone
  to this problem

Closes #17819
</description><key id="149305629">17837</key><summary>Refactor UUID-generating methods out of Strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T23:58:54Z</created><updated>2016-04-19T09:43:58Z</updated><resolved>2016-04-19T01:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-19T01:21:03Z" id="211670570">LGTM. Left a comment about not getting a log message we weren't getting anyway. It'd be nice if we knew if we were falling back to a bogus mac address, I guess, but maybe we can fix that problem in another PR outside of the "move stuff" PR.
</comment><comment author="jasontedor" created="2016-04-19T01:51:33Z" id="211680679">Thanks @nik9000. I'm not heartbroken over the loss of the logging statement that was never there to begin with, especially since I do not immediately see a clean way to achieve logging there without using an offensive statically-initialized static logger.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTO from InnerHits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17836</link><project id="" key="" /><description>Relates to #17085
</description><key id="149302500">17836</key><summary>Remove PROTO from InnerHits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T23:38:52Z</created><updated>2016-05-02T12:11:57Z</updated><resolved>2016-04-19T14:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T23:39:07Z" id="211633600">Yet more for @cbuescher if you want it!
</comment><comment author="cbuescher" created="2016-04-19T09:13:42Z" id="211819065">LGTM
</comment><comment author="nik9000" created="2016-04-19T14:25:11Z" id="211943940">Thanks for reviewing @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify processors option for field name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17835</link><project id="" key="" /><description>Date processor parses `match_field`, Geoip processor parses `source_field`, while others parse simply `field`. It may get tricky with processors that support both `source_field` and `target_field`, but shall we try and streamline the processors configuration so that we don't have to go and check the documentation for each single processor when writing a pipeline?
</description><key id="149287304">17835</key><summary>Unify processors option for field name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-18T22:19:59Z</created><updated>2016-04-21T11:42:57Z</updated><resolved>2016-04-21T11:42:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-19T09:16:41Z" id="211820228">I'd standardise on using `field`. Also, I was surprised to see that eg `split` didn't support `target_field` - I think that should be standard too, no?  i.e. `target_field` is optional and defaults to `field` 
</comment><comment author="martijnvg" created="2016-04-19T10:05:52Z" id="211838962">+1 

Just one question, so in cases where there is a source field and a target field we will have `field` and `target_field` as options?
</comment><comment author="clintongormley" created="2016-04-19T10:43:58Z" id="211853454">&gt; Just one question, so in cases where there is a source field and a target field we will have field and target_field as options?

I think that works.  Currently we only support `target_field` on `convert` and `date`, with it defaulting to `@timestamp` for the `date` processor.  I think it is OK to always use `field` to mean "the field we're getting the value from" and `target_field` to mean "the field we're applying the change to, which defaults to `field`"

Seems pretty clear to me.  And the benefit of consistency makes it less likely that users will get confused.
</comment><comment author="martijnvg" created="2016-04-20T14:48:02Z" id="212455486">The `geoip` and `attachment` processors also have a `fields` option, which controls what properties from maxmind db and tika are used to enrich the document being processed. Naming wise this can become confusing if we stream line to use `field`. I think we should rename `fields` in both of these processors. The option name `properties` feels like a good choice to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOs from TransportAddresses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17834</link><project id="" key="" /><description>We have this TransportAddressSerializers that works similarly to
NamedWriteables except it uses shorts instead of streams. I don't know
enough to propose removing it in favor of NamedWriteables to I just ported
it to using Writeable.Reader and left it alone.

Relates to #17085
</description><key id="149283277">17834</key><summary>Remove PROTOs from TransportAddresses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T21:57:39Z</created><updated>2016-05-02T12:12:02Z</updated><resolved>2016-04-18T23:00:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T21:58:03Z" id="211598718">@cbuescher is this a thing you can take a look at?
</comment><comment author="jasontedor" created="2016-04-18T22:04:32Z" id="211600308">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Remove XContentBuilderString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17833</link><project id="" key="" /><description>This was previously used by xcontentbuilder to support camelCase.
However, it is no longer used, and can be replaced with just String.

This is a follow up to #17774.
</description><key id="149278607">17833</key><summary>Internal: Remove XContentBuilderString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T21:33:13Z</created><updated>2016-04-19T09:14:46Z</updated><resolved>2016-04-18T21:55:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T21:38:21Z" id="211593756">LGTM
</comment><comment author="jpountz" created="2016-04-18T21:40:33Z" id="211594318">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up serialization on some stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17832</link><project id="" key="" /><description>Removes readFrom which is no longer required/actively discouraged and
replaces some instances of `Streamable` with `Writeable`.

Relates to #17085
</description><key id="149274801">17832</key><summary>Clean up serialization on some stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T21:18:32Z</created><updated>2016-04-19T14:17:24Z</updated><resolved>2016-04-19T14:17:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T21:19:47Z" id="211584569">I'm not sure who is best to review this. @jasontedor's been in this code recently iirc. @cbuescher is my default for this serialization quest.
</comment><comment author="jasontedor" created="2016-04-19T01:59:14Z" id="211685123">LGTM.
</comment><comment author="nik9000" created="2016-04-19T14:17:24Z" id="211940488">Thanks for reviewing @jasontedor ! Sorry for the bad diff!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split processor documentation doesn't include separator option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17831</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/master/split-processor.html

Docs say:

&gt; Splits a field into an array using a separator character.

But the table below doesn't describe the API for specifying the separator character.
</description><key id="149262448">17831</key><summary>Split processor documentation doesn't include separator option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bargs</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-04-18T20:26:50Z</created><updated>2016-04-19T09:12:12Z</updated><resolved>2016-04-19T09:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>	Normalize registration for SignificanceHeuristics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17830</link><project id="" key="" /><description>When I pulled on the thread that is "Remove PROTOTYPEs from
SignificanceHeuristics" I ended up removing SignificanceHeuristicStreams
and replacing it with readNamedWriteable. That seems like a lot at once
but it made sense at the time. And it is what we want in the end, I think.

Anyway, this also converts registration of SignificanceHeuristics to
use ParseFieldRegistry to make them consistent with Queries, Aggregations
and lots of other stuff.

Related to #17085
</description><key id="149257751">17830</key><summary>	Normalize registration for SignificanceHeuristics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T20:09:50Z</created><updated>2016-04-19T13:58:33Z</updated><resolved>2016-04-19T13:58:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T20:19:13Z" id="211558363">Or not. Sadly, this hits "failed to check serialization" because it touches streams. I'm not sure what to do with this now.
</comment><comment author="nik9000" created="2016-04-18T20:59:08Z" id="211572568">&gt; Or not. Sadly, this hits "failed to check serialization" because it touches streams. I'm not sure what to do with this now.

Fixed by adding a shiny new hack.
</comment><comment author="colings86" created="2016-04-19T08:16:18Z" id="211787643">Could you quickly explain why `ElasticsearchAssertions` failed before the fix you added? I am just wondering what the difference between creating a `NamedWriteableRegistry` then populating it using the `SearchModule` and getting the `NamedWriteableRegistry` from the test cluster since I thought both should amount to the same thing?
</comment><comment author="nik9000" created="2016-04-19T11:28:22Z" id="211867458">Plugins! One test failed: it registered a significance heuristic in a
plugin. Since the heuristic is returned in the response this checks it. It
used to work because the registration was static.
On Apr 19, 2016 4:16 AM, "Colin Goodheart-Smithe" notifications@github.com
wrote:

&gt; Could you quickly explain why ElasticsearchAssertions failed before the
&gt; fix you added? I am just wondering what the difference between creating a
&gt; NamedWriteableRegistry then populating it using the SearchModule and
&gt; getting the NamedWriteableRegistry from the test cluster since I thought
&gt; both should amount to the same thing?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17830#issuecomment-211787643
</comment><comment author="colings86" created="2016-04-19T12:20:52Z" id="211896489">Ok, that makes sense. This PR LGTM
</comment><comment author="nik9000" created="2016-04-19T13:58:12Z" id="211932792">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make (read|write)NamedWriteable public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17829</link><project id="" key="" /><description>- Make (read|write)NamedWriteable public
- Remove some of the lesser used readXYZ/writeXYZ methods
- Deprecate all remaining readXYZ/writeXYZ methods

I believe it is safe to remove all of the remaining readXYZ/writeXYZ method because the only release that contained them was 5.0.0-alpha1 but that'd make the PR too large. readQuery/writeQuery is very common. The others are rare, but there are too many of them.

Closes #17682
</description><key id="149240681">17829</key><summary>Make (read|write)NamedWriteable public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T18:58:43Z</created><updated>2016-04-18T20:31:09Z</updated><resolved>2016-04-18T20:30:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T19:03:25Z" id="211531717">Any of @jpountz @ywelsch @cbuescher @javanna around still and want to have a look?
</comment><comment author="ywelsch" created="2016-04-18T19:20:12Z" id="211537999">Left two minor comments. I would prefer we remove all of the remaining readXYZ/writeXYZ from StreamInput/Output before 5.0.0-beta. Can be done in a follow-up PR though. Also, if people prefer the readQuery(), writeQuery()-style methods, they can still be added as static methods to the respective class (QueryBuilder). For example:

```
public static readQuery(StreamInput in) {
    return in.readNamedWriteable(QueryBuilder.class);
}

...
```
</comment><comment author="ywelsch" created="2016-04-18T19:20:51Z" id="211538171">Forgot to say. Once comments addressed, feel free to push. Thanks @nik9000.
</comment><comment author="nik9000" created="2016-04-18T19:56:33Z" id="211550432">Thanks for reviewing @ywelsch ! I'll clean up and push in a bit!
</comment><comment author="nik9000" created="2016-04-18T20:31:09Z" id="211564323">Thanks again for the review @ywelsch! I've pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from ExtendedBounds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17828</link><project id="" key="" /><description>and make it implement Writeable because it is Writeable.

Relates to #17085
</description><key id="149227849">17828</key><summary>Remove PROTOTYPE from ExtendedBounds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T18:08:22Z</created><updated>2016-05-02T12:12:08Z</updated><resolved>2016-04-19T14:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T07:49:28Z" id="211780247">LGTM
</comment><comment author="nik9000" created="2016-04-19T14:06:59Z" id="211936301">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from KeyedFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17827</link><project id="" key="" /><description>Relates to #17085
</description><key id="149226389">17827</key><summary>Remove PROTOTYPE from KeyedFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T18:01:39Z</created><updated>2016-05-02T12:12:13Z</updated><resolved>2016-04-19T13:45:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T07:47:50Z" id="211779878">LGTM
</comment><comment author="nik9000" created="2016-04-19T13:45:42Z" id="211928928">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from AggregatorFactories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17826</link><project id="" key="" /><description>Relates to #17085
</description><key id="149226306">17826</key><summary>Remove PROTOTYPE from AggregatorFactories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T18:01:14Z</created><updated>2016-05-02T12:12:17Z</updated><resolved>2016-04-19T13:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T07:47:21Z" id="211779781">I left one comment but LGTM
</comment><comment author="nik9000" created="2016-04-19T12:41:37Z" id="211904460">Thanks for reviewing @colings86 ! The method you mentioned dropped out in the rebase required after I merged #17825.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated registration methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17825</link><project id="" key="" /><description>Removes deprecated registration methods from SearchModule and
NamedWriteableRegistry and removes the "shims" used to migrate
aggregations to the new registration methods.

Relates to #17085
</description><key id="149196874">17825</key><summary>Remove deprecated registration methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T16:09:38Z</created><updated>2016-04-19T12:41:50Z</updated><resolved>2016-04-19T12:36:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-19T07:43:53Z" id="211779078">LGTM
</comment><comment author="nik9000" created="2016-04-19T12:41:50Z" id="211904560">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut remaining pipeline aggregations registerPipelineAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17824</link><project id="" key="" /><description>Relates to #17085
</description><key id="149162775">17824</key><summary>Cut remaining pipeline aggregations registerPipelineAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T14:12:31Z</created><updated>2016-05-02T12:12:23Z</updated><resolved>2016-04-18T15:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-18T14:14:23Z" id="211399285">@colings86 these are the last of the aggregations. The next step is to remove the bits I added for the migration marked by NORELEASEs. Then remove the remaining PROTOTYPEs from aggregations.
</comment><comment author="colings86" created="2016-04-18T14:15:31Z" id="211399738">LGTM. Awesome, looks like we are almost done then &#128516; 
</comment><comment author="nik9000" created="2016-04-18T14:16:21Z" id="211400011">&gt; LGTM. Awesome, looks like we are almost done then

Thanks! I'll merge in a bit!
</comment><comment author="nik9000" created="2016-04-18T15:04:42Z" id="211418753">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest failure metadata fields refer to compound processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17823</link><project id="" key="" /><description>The compound processor is used internally in ingest when handling failures. As I understand though it should be an internal concept that users don't know about. If you define a processor that fails, which has an `on_failure` block defined, which fails again, and the pipeline has its own `on_failure`, the metadata about the failure itself will be referring to the compound processor rather than the one that really failed.

```
GET _ingest/pipeline/_simulate
{
  "pipeline" : {
    "processors" : [
      {
        "convert" : {
          "field": "doesnotexist",
          "type" : "integer",
          "on_failure" : [
            {
              "fail" : {
                "message" : "custom error message"
              }    
            }
          ]
        }
      }
    ],
    "on_failure" : [
      {
        "set" : {
          "field" : "failure",
          "value" : {
            "message" : "{{_ingest.on_failure_message}}",
            "processor" : "{{_ingest.on_failure_processor_type}}",
            "tag" : "{{_ingest.on_failure_processor_tag}}"
          }
        }
      }
    ]
  },
  "docs" : [
    {
      "_source" : {
        "message" : "test"
      }
    }
  ]
}
```

```
{
  "docs": [
    {
      "doc": {
        "_type": "_type",
        "_index": "_index",
        "_id": "_id",
        "_source": {
          "message": "test",
          "failure": {
            "tag": "CompoundProcessor-null-null",
            "message": "custom error message",
            "processor": "compound"
          }
        },
        "_ingest": {
          "timestamp": "2016-04-18T13:20:28.981+0000"
        }
      }
    }
  ]
}
```

The `tag` and the `processor` that goes in the document as part of the failure object are a bit off, they should be coming from the fail processor, which caused the failure. That information gets lost with the double wrapping.

Here is also a failing unit test to add to `CompoundProcessorTests` that reproduces the problem:

```
public void testFail() throws Exception {
        TestProcessor firstProcessor = new TestProcessor("id1", "first", ingestDocument -&gt; {throw new RuntimeException("error");});
        FailProcessor failProcessor = new FailProcessor("tag_fail", new TestTemplateService.MockTemplate("custom error message"));
        TestProcessor secondProcessor = new TestProcessor("id2", "second", ingestDocument -&gt; {
            Map&lt;String, String&gt; ingestMetadata = ingestDocument.getIngestMetadata();
            assertThat(ingestMetadata.size(), equalTo(3));
            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_MESSAGE_FIELD), equalTo("custom error message"));
            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_TYPE_FIELD), equalTo("fail"));
            assertThat(ingestMetadata.get(CompoundProcessor.ON_FAILURE_PROCESSOR_TAG_FIELD), equalTo("tag_fail"));
        });

        CompoundProcessor failCompoundProcessor = new CompoundProcessor(Collections.singletonList(firstProcessor),
                Collections.singletonList(failProcessor));

        CompoundProcessor compoundProcessor = new CompoundProcessor(Collections.singletonList(failCompoundProcessor),
                Collections.singletonList(secondProcessor));
        compoundProcessor.execute(ingestDocument);

        assertThat(firstProcessor.getInvokedCounter(), equalTo(1));
        assertThat(secondProcessor.getInvokedCounter(), equalTo(1));
}
```
</description><key id="149161333">17823</key><summary>Ingest failure metadata fields refer to compound processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-04-18T14:07:57Z</created><updated>2016-05-26T09:18:27Z</updated><resolved>2016-05-24T21:25:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-18T14:32:01Z" id="211405150">@martijnvg @talevy I bumped into this while preparing a demo, I looked into it but I didn't manage to find a quick solution, tricky.
</comment><comment author="martijnvg" created="2016-04-18T15:40:11Z" id="211437095">Right, any exception that gets bubbled up to a higher level (in this case the root processor level) we fetch the ingest `tag` and `processor` attributes.

I think we need to solve this differently. Just like in the processor factories we always throw an error with the the processor tag, processor type and property name present. I think we should do the same for exceptions being thrown at ingest preprocess time and then instead of fetching tag and type from current processor in `CompoundProcessor` we extract that information from the exception itself.
</comment><comment author="talevy" created="2016-04-19T14:57:41Z" id="211964733">I am aware of this issue. I had a fork of this feature that hid the `CompoundProcessor` away, but it made for really ugly non-reusable code. I will try at it again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use try-with-resource when creating new parser instances where possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17822</link><project id="" key="" /><description>We usually wrap a newly created XContent parser in a try-with-resources block so it gets properly closed after it has been used, but I found this was missing in several places in core ES. This PR fix all the occurances I found by quickly grepping core (excluding tests) where it was possible to use try-with-resources syntax.
</description><key id="149154455">17822</key><summary>Use try-with-resource when creating new parser instances where possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T13:43:44Z</created><updated>2016-04-18T14:38:56Z</updated><resolved>2016-04-18T14:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-18T14:06:57Z" id="211395978">I left some minor comments but agree we should do this!
</comment><comment author="javanna" created="2016-04-18T14:10:10Z" id="211397226">thanks for doing this @cbuescher ! 
</comment><comment author="cbuescher" created="2016-04-18T14:15:45Z" id="211399821">@jpountz @javanna I reverted the one file you pointed out where the diff only re-organized imports. 
</comment><comment author="jpountz" created="2016-04-18T14:17:39Z" id="211400400">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Immutable ShardRouting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17821</link><project id="" key="" /><description>This PR makes ShardRouting immutable, which lets us get rid of defensive copying of all ShardRouting objects whenever we do a reroute. With immutable objects we only make copies for shard routings that change.
</description><key id="149146016">17821</key><summary>Immutable ShardRouting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-18T13:08:22Z</created><updated>2016-05-10T17:11:20Z</updated><resolved>2016-05-10T17:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-04-20T13:13:41Z" id="212418496">I've added a small commit that makes UnassignedInfo immutable as well (06e774f).
</comment><comment author="ywelsch" created="2016-05-03T13:54:26Z" id="216534826">@s1monw Can you have a look at this one?
</comment><comment author="s1monw" created="2016-05-10T11:21:48Z" id="218129585">@ywelsch looks great - left some minors
</comment><comment author="ywelsch" created="2016-05-10T13:17:53Z" id="218154009">@s1monw I've pushed a change addressing your review comments. Please have another look.
</comment><comment author="s1monw" created="2016-05-10T13:23:36Z" id="218155522">LGTM thanks for all the javadocs
</comment><comment author="ywelsch" created="2016-05-10T17:11:20Z" id="218225799">Thanks for the review @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dis Min Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17820</link><project id="" key="" /><description>Elasticsearch query DSL has Dis Max query https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-dis-max-query.html

We could also use a "Dis Min query", as for some specialized scoring scenarios we are interested in giving the query the score of the lowest scoring sub-query. Yup.

As always, will be happy to send a PR with this addition.
</description><key id="149131583">17820</key><summary>Dis Min Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Query DSL</label><label>discuss</label><label>feedback_needed</label></labels><created>2016-04-18T12:03:50Z</created><updated>2016-04-23T23:54:09Z</updated><resolved>2016-04-22T09:27:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-18T12:07:22Z" id="211351518">Hi @synhershko 

Could you expand on why a dis-min query would be useful?
</comment><comment author="david206" created="2016-04-18T12:31:37Z" id="211358913">Hi @clintongormley 

I am using boosting-query as one of the sub-queries, and in case there is a match on the boosting-query I want to return its (lower) score as the query score (the other sub-queries have different logic with the score I want to get if the boosting query doesn't match).
</comment><comment author="clintongormley" created="2016-04-19T08:35:36Z" id="211797038">Hi @david206 

From your description, it doesn't sound like `dis_min` will do what you want.  It'll return the lowest score from whichever clause, it won't just choose the score from the boosting query if it has matches.

I'm still not seeing the use case for `dis_min`.
</comment><comment author="synhershko" created="2016-04-19T08:39:27Z" id="211799313">@clintongormley correct, the lowest score from whichever clause is exactly what we are after. We build our queries in a way that will make the boosting query the lowest scoring bit.
</comment><comment author="clintongormley" created="2016-04-19T08:43:25Z" id="211802171">@synhershko this seems awfully use-case specific.  I'm struggling to understand how this would be generically useful.
</comment><comment author="jpountz" created="2016-04-22T09:27:13Z" id="213351740">Discussed in Fixit Friday. This feels too specific indeed. Closing in favour of the more generic #17116.
</comment><comment author="synhershko" created="2016-04-23T23:54:09Z" id="213854042">Sounds good (albeit more complicated than just a dis_min, which essentially shared the same code as dis_max but with a polarity change).

One question though - IIRC dis_max has some performance gains associated with it in terms of parallel execution and query termination. If that indeed is the case - would function score query benefit from the same?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No appenders could be found for logger (common) printed out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17819</link><project id="" key="" /><description>If I turn off my wi-fi connection and run elasticsearch, tried with 1.7, 2.3 and 5.0 and it is always the same, the process prints out the following to start with:

```
log4j:WARN No appenders could be found for logger (common).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
```

The `logging.yml` is the default one. After that everything looks ok, usual logging etc.. I guess this is due to the order of initialization of our logging, we may be ok with this initial warning but maybe there is something that we want to do about it.
</description><key id="149116167">17819</key><summary>No appenders could be found for logger (common) printed out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Logging</label><label>bug</label></labels><created>2016-04-18T10:55:32Z</created><updated>2016-04-19T17:20:24Z</updated><resolved>2016-04-19T01:43:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-18T23:59:03Z" id="211638735">This one is fun. The trouble starts in the innocuous looking [line 199 `Bootstrap.java`](https://github.com/elastic/elasticsearch/blob/6941966b1654106ed9249cd01c32bfa2e06b452f/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java#L199):

``` java
199:        if (Strings.hasLength(pidFile)) {
```

This triggers class initialization for `Strings` which causes the field `TIME_UUID_GENERATOR` to be initialized on [line 64](https://github.com/elastic/elasticsearch/blob/6941966b1654106ed9249cd01c32bfa2e06b452f/core/src/main/java/org/elasticsearch/common/Strings.java#L64):

``` java
64:    private static final UUIDGenerator TIME_UUID_GENERATOR = new TimeBasedUUIDGenerator();
```

This in turn triggers class initialization for `TimeBasedUUIDGenerator` which causes the field `secureMungedAddressed` to be initialized on [line 38](https://github.com/elastic/elasticsearch/blob/6941966b1654106ed9249cd01c32bfa2e06b452f/core/src/main/java/org/elasticsearch/common/TimeBasedUUIDGenerator.java#L38):

``` java
38:    private static final byte[] secureMungedAddress = MacAddressProvider.getSecureMungedAddress();
```

Now the coup de gr&#226;ce. The method [`MacAddressProvider#getSecureMungedAddress`](https://github.com/elastic/elasticsearch/blob/6941966b1654106ed9249cd01c32bfa2e06b452f/core/src/main/java/org/elasticsearch/common/MacAddressProvider.java#L64) tries to get a secure MAC address for use in generating the time-based UUIDs. If all the network interfaces are disabled, then there is no MAC address to use so the check on [line 73](https://github.com/elastic/elasticsearch/blob/6941966b1654106ed9249cd01c32bfa2e06b452f/core/src/main/java/org/elasticsearch/common/MacAddressProvider.java#L73) passes:

``` java
73:        if (!isValidAddress(address)) {
```

which leads to the fatal logging statement on [line 74](https://github.com/elastic/elasticsearch/blob/6941966b1654106ed9249cd01c32bfa2e06b452f/core/src/main/java/org/elasticsearch/common/MacAddressProvider.java#L74):

``` java
74:             logger.warn("Unable to get a valid mac address, will use a dummy address");
```

This line being fatal because it's invoked _before_ logging has been initialized.

Two lessons:
1. `static` is evil, don't do `static`
2. logging in static methods using a statically-initialized static logger (instead of an instance of `logger` from an `AbstractComponent`) is prone to this problem

I opened #17837.
</comment><comment author="javanna" created="2016-04-19T17:20:24Z" id="212024821">&gt; This one is fun.

Sounds like it, thanks a lot for digging and fixing @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing index name to search slow log.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17818</link><project id="" key="" /><description>This commits adds the index name as part of the logging message.
Closes #17025
</description><key id="149107490">17818</key><summary>Add missing index name to search slow log.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Logging</label><label>bug</label><label>review</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T10:14:07Z</created><updated>2016-04-18T12:32:56Z</updated><resolved>2016-04-18T10:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-04-18T10:39:42Z" id="211322445">LGTM
</comment><comment author="jimczi" created="2016-04-18T10:40:21Z" id="211322917">Thanks @ywelsch !
</comment><comment author="jimczi" created="2016-04-18T12:32:56Z" id="211359200">@markharwood thanks, I've missed it. Though I had to use the SearchContext for 2.x because the IndexSettings is not available in those branches. I'll update the master branch, thanks.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix calculation of took time of term vectors request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17817</link><project id="" key="" /><description>Fix for #12565
</description><key id="149104518">17817</key><summary>Fix calculation of took time of term vectors request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">alexshadow007</reporter><labels><label>:Term Vectors</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-18T10:01:56Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-21T19:46:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-18T10:28:55Z" id="211318948">@alexsahdow007 Thanks for the pull request. I left a comment. 
</comment><comment author="alexshadow007" created="2016-04-18T12:28:38Z" id="211358225">@jasontedor I replaced `System#currentTimeMillis` with `System#nanoTime`
</comment><comment author="alexshadow007" created="2016-04-19T09:30:10Z" id="211823805">@jasontedor done
</comment><comment author="jasontedor" created="2016-04-19T15:25:43Z" id="211977469">@alexshadow007 I left another comment regarding the test.
</comment><comment author="alexshadow007" created="2016-04-19T20:51:26Z" id="212122047">@jasontedor Done
</comment><comment author="jasontedor" created="2016-04-20T18:37:38Z" id="212551223">Great work in e468c2e00ceb9fa17c4f18b082c99ec0ebd6d05e! I left a comment about randomizing the `start` and `end` times. Let's address that and then we'll go in for a careful final review. :smile:
</comment><comment author="alexshadow007" created="2016-04-20T19:39:42Z" id="212573957">@jasontedor Done
</comment><comment author="jasontedor" created="2016-04-21T19:27:14Z" id="213079578">Thanks so much for your contribution here, it looks great @alexshadow007. I will merge this soon.
</comment><comment author="alexshadow007" created="2016-04-21T19:29:55Z" id="213080746">@jasontedor do i need squash commits?
</comment><comment author="jasontedor" created="2016-04-21T19:30:36Z" id="213081109">&gt; do i need squash commits?

@alexshadow007 Nope, I'll squash on merge. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop top level inner hits in favour of inner hits defined in the query dsl</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17816</link><project id="" key="" /><description>Also Fixed a limitation that prevent from hierarchical inner hits be defined in query dsl.

Closes #11118
</description><key id="149090332">17816</key><summary>Drop top level inner hits in favour of inner hits defined in the query dsl</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>breaking</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-18T09:05:57Z</created><updated>2016-04-29T16:08:51Z</updated><resolved>2016-04-29T09:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-26T09:36:39Z" id="214684437">After discussing this PR with @jpountz we decided to make the following changes:
- When inner hits are inlined we shouldn't inline the provided inner hit, because a make a copy and inline that.
- The method that does the inlining relies on a lot of `instanceof` checks and that is fragile. In order to make the integration better we should add a `buildInnerhits()` method that makes an inlined copy of the user specified inner hits in the query dsl. It is a noop on most queries and on nested, has_child and has_parent it actual returned an inlined copy of the inner hit builder. Compound queries like bool query just delegate.
</comment><comment author="jpountz" created="2016-04-28T09:26:23Z" id="215364939">I left some comments. The registration looks better to me. Now I would like to see if we can stop modifying in-place objects that are passed in the QueryBuilder setters?
</comment><comment author="martijnvg" created="2016-04-28T14:24:05Z" id="215442075">@jpountz I've updated the PR. InnerHitBuilders instances are no longer modified in-place.
</comment><comment author="jpountz" created="2016-04-29T08:41:57Z" id="215660011">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch: build failed with compilation failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17815</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: elasticsearch 2.x

I tried with 2.0.2 and 2.3.1

**JVM version**: 1.7.0_03-icedtea

**OS version**: CentOS release 6.7 (Final)

$ mvn -v
Apache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T09:29:23-08:00)
Maven home: /opt/maven/apache-maven-3.2.5
Java version: 1.7.0_03-icedtea, vendor: Oracle Corporation
Java home: /usr/lib/jvm/icedtea7-native/jre
Default locale: en_US, platform encoding: ANSI_X3.4-1968
OS name: "linux", version: "2.6.32-573.22.1.el6.x86_64", arch: "amd64", family: "unix"

**Description of the problem including expected versus actual behavior**:

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 27.904 s
[INFO] Finished at: 2016-04-18T02:00:58-07:00
[INFO] Final Memory: 122M/5437M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project elasticsearch: Compilation failure -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project elasticsearch: Compilation failure
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.compiler.CompilationFailureException: Compilation failure
        at org.apache.maven.plugin.compiler.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:915)
        at org.apache.maven.plugin.compiler.CompilerMojo.execute(CompilerMojo.java:129)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 19 more
[ERROR]
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &lt;goals&gt; -rf :elasticsearch

**Steps to reproduce**:
1. mvn -X clean compile

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="149089856">17815</key><summary>elasticsearch: build failed with compilation failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jackiehjm</reporter><labels /><created>2016-04-18T09:04:31Z</created><updated>2016-04-18T10:46:24Z</updated><resolved>2016-04-18T10:46:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-18T09:34:12Z" id="211295376">Can you provide the logs of the maven compile plugin execution part?
</comment><comment author="jasontedor" created="2016-04-18T10:46:24Z" id="211324421">As the 2.3 and 2.x branches of Elasticsearch currently build successfully on the Elastic CI, this is going to boil down to an environment issue on your end. Elastic reserves GitHub for verified bug reports and feature requests. If you provide details on the [Elastic Discourse forums](https://discuss.elastic.co), the community there would be happy to help. :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add 'date_math_index_name' processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17814</link><project id="" key="" /><description>We should add a processor that overrides / sets the index based on a timestamp / date field. This is useful for event ingestion where the actual date of an event is inside the source.

This processor should utilize the date math index name support: 
https://www.elastic.co/guide/en/elasticsearch/reference/master/date-math-index-names.html

The processor should look something like this:

```
PUT /_ingest/pipeline/test
{
  "processors" : [
    {
      "date_math_index_name" : {
        "timestamp_field" : "timestamp",
        "optional_date_format: ....,
        "index_name_template" : "events-{{date}}" // 'date' is the formatted date string
      }
    }
  ]
}
```
</description><key id="149071951">17814</key><summary>Add 'date_math_index_name' processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>feature</label></labels><created>2016-04-18T07:56:07Z</created><updated>2016-04-29T15:24:04Z</updated><resolved>2016-04-29T15:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>style(logging.xml): change log to print package name, class name, lin&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17813</link><project id="" key="" /><description>&#8230;e number
</description><key id="149030037">17813</key><summary>style(logging.xml): change log to print package name, class name, lin&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dengshilong</reporter><labels /><created>2016-04-18T03:30:20Z</created><updated>2016-04-18T05:30:44Z</updated><resolved>2016-04-18T05:30:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dengshilong" created="2016-04-18T03:43:49Z" id="211178153">I have signed the CLA, why i still can't pull a request?
</comment><comment author="jasontedor" created="2016-04-18T05:30:44Z" id="211207662">Thank you for the pull request, but including the full package name, class name, method name, and line number is too verbose for the default out-of-the-box logging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>style(logging.xml): change log to print package name, class name, lin&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17812</link><project id="" key="" /><description>&#8230;e number
</description><key id="149028142">17812</key><summary>style(logging.xml): change log to print package name, class name, lin&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dengshilong</reporter><labels /><created>2016-04-18T03:11:40Z</created><updated>2016-04-18T03:29:58Z</updated><resolved>2016-04-18T03:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Persistent Node Ids</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17811</link><project id="" key="" /><description>Node IDs are currently randomly generated during node startup. That means they change every time the node is restarted. While this doesn't matter for ES proper, it makes it hard for external services to track nodes. Another, more minor, side effect is that indexing the output of, say, the node stats API results in creating new fields due to node ID being used as keys.

The first approach I considered was to use the node's published address as the base for the id. We already [treat nodes with the same address as the same](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/discovery/zen/NodeJoinController.java#L387) so this is a simple change (see [here](https://github.com/elastic/elasticsearch/compare/master...bleskes:node_persistent_id_based_on_address)). While this is simple and it works for probably most cases, it is not perfect. For example, if after a node restart, the node is not able to bind to the same port (because it's not yet freed by the OS), it will cause the node to still change identity. Also in environments where the host IP can change due to a host restart, identity will not be the same. 

Due to those limitation, I opted to go with a different approach where the node id will be persisted in the node's data folder. This has the upside of connecting the id to the nodes data. It also means that the host can be adapted in any way (replace network cards, attach storage to a new VM). I

t does however also have downsides - we now run the risk of two nodes having the same id, if someone copies clones a data folder from one node to another. To mitigate this I changed the semantics of the protection against multiple nodes with the same address to be stricter - it will now reject the incoming join if a node exists with the same id but a different address. Note that if the existing node doesn't respond to pings (i.e., it's not alive) it will be removed and the new node will be accepted when it tries another join.

Last, and most importantly, this change requires that _all_ nodes persist data to disk. This is a change from current behavior where only data &amp; master nodes store local files. This is the main reason for marking this PR as breaking.

Other less important notes:
- DummyTransportAddress is removed as we need a unique network address per node. Use `LocalTransportAddress.buildUnique()` instead.
- I renamed `node.add_lid_to_custom_path` to `node.add_lock_id_to_custom_path` to avoid confusion with the node ID which is now part of the `NodeEnvironment` logic.
- I removed the `version` paramater from `MetaDataStateFormat#write` , it wasn't really used and was just in the way :)
- TribeNodes are special in the sense that they do start multiple sub-nodes (previously known as client nodes). Those sub-nodes do not store local files and will this have a new id every restart. I have some ideas on how to change this but it will require more sublte changes that I rather be tackled in a follow up issue if needed (/cc @pickypg  @uboness @clintongormley )
</description><key id="148943209">17811</key><summary>Persistent Node Ids</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>breaking</label><label>enhancement</label><label>review</label></labels><created>2016-04-17T11:24:59Z</created><updated>2016-04-26T18:29:49Z</updated><resolved>2016-04-26T12:50:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-04-19T15:29:20Z" id="211978802">Left a few comments but in general looks good. I hope we won't have to re-add ephemeral node ids at a later point... ;-)
</comment><comment author="bleskes" created="2016-04-19T17:37:50Z" id="212034536">@ywelsch thanks. I addressed your comments and pushed some new commits. Sadly, my Jenkins server signaled an issue with our testing cluster. In the name of reproducibility, it captures the initial seeds of each nodes and when it resets the cluster and start up nodes that were shut down but tests, it will re-use that seed so a node will have the same id. However, because how things work elsewhere, this id may be taken by another node that re-used the data folder of the old node. I added an assertion to LocalDiscovery that surfaces this clearly (had to chase it down, which was sneaky). I'm still thinking about a proper solution. Will ping once it's there.
</comment><comment author="bleskes" created="2016-04-20T06:04:46Z" id="212274791">@ywelsch I found a simple way to work around the test cluster issues. This is ready for review again...
</comment><comment author="ywelsch" created="2016-04-26T12:50:20Z" id="214729293">superseded by #17987 (which keeps process semantics for DiscoveryNode)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster names should be restricted to ASCII</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17810</link><project id="" key="" /><description>We no longer use index names for dir/filenames for various reasons, one of which is that unicode and filesystems often don't play well together.

Similarly, we should restrict the cluster name to ASCII only, as the cluster name is used as a directory name.  This is a breaking change, but possibly all that is required is to change the cluster name and rename the directory.  Not sure about snapshots?
</description><key id="148940829">17810</key><summary>Cluster names should be restricted to ASCII</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Core</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha4</label></labels><created>2016-04-17T10:53:00Z</created><updated>2016-06-07T16:35:07Z</updated><resolved>2016-06-07T16:35:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-04-17T11:09:04Z" id="211001382">+1
</comment><comment author="s1monw" created="2016-05-05T09:18:36Z" id="217110224">+100 make this a blocker
</comment><comment author="clintongormley" created="2016-05-05T09:18:52Z" id="217110258">We could use the cluster UUID instead of the cluster name same as we do for indices now.  We should also provide a command line tool to allow renaming clusters and indices.
</comment><comment author="s1monw" created="2016-05-05T09:20:15Z" id="217110453">yeah I mean that is the ultimate solution just don't use it anywhere outside of the system. 
</comment><comment author="bleskes" created="2016-05-18T08:52:09Z" id="219965131">we discussed it some more and I think I summarize things with:

1) One option is to limit cluster names to be pure ascii. Down side here is the BWC breakage (people who's cluster names are non-ascii, will need to re-configure and change folder names). The up side is the simplicity and the transparency - a common mistake in training is to start ES twice, one normally and one with a non-default cluster name). Looking at the data folder immediately shows what's wrong and what people should do.
2) Another solution is to use the cluster_uuid as a top level folder name. This would mean people are free to choose whatever they want as a cluster name. The down side is that it produces a chicken-and-egg issues:
- in order to use the right data folder, a new node needs to first join the cluster to get it's uuid. This means we can't persist anything /create folders before we join. 
- Existing nodes can scan their data folders to find one that has the right cluster name internally in it's _state file. This adds complexity and ambiguity 
  - what happens if we find two matching folders? (we can probably not start, but then the user needs to choose, meaning they need tooling to see which one is goog).
  - what happens if a node joins the cluster after having written stuff/read stuff to it's data folder (for example, we plan to write some persistent node id in there, which is needed on join), only to discover that the cluster has another uuid than the folder used locally.

Personally, I see the reason to have an "opaque" identifier for a cluster, but I'm not sure it's worth the complexity.
</comment><comment author="clintongormley" created="2016-05-20T10:11:02Z" id="220568052">Discussed in FixItFriday - there's no need to have the cluster name as a directory at all.  If you want multiple clusters in the same mount point, then just change the `path.data`.

On startup, we should check the data path for any folder other than `nodes`. If any exists, then we should fail hard and tell the user to either: (1) move the `cluster/nodes` up to the top level or (2) change the `path.data` setting.
</comment><comment author="clintongormley" created="2016-05-20T10:13:27Z" id="220568608">The data path can also be set as `path.data = /path/to/data/${cluster.name}`
</comment><comment author="dakrone" created="2016-05-20T15:40:22Z" id="220641055">We'll also need to check for the (extremely rare) case where someone has named their cluster "nodes" and act accordingly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17809</link><project id="" key="" /><description>I'm using elasticsearch to find similar documents to a given document using the "more like this" query.

Is there an easy way to get the elasticsearch scoring between 0 and 1 (using cosine similarity) ?

Thanks!
</description><key id="148864504">17809</key><summary>Elasticsearch scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rfyiamcool</reporter><labels /><created>2016-04-16T15:49:53Z</created><updated>2017-04-12T17:20:11Z</updated><resolved>2016-04-18T10:39:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-18T10:39:52Z" id="211322542">Hi @rfyiamcool 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="olalonde" created="2017-04-12T17:20:11Z" id="293648438">@rfyiamcool wondering the same here... did you figure this out?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated copyright years to include 2016</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17808</link><project id="" key="" /><description>:+1: 
</description><key id="148859253">17808</key><summary>Updated copyright years to include 2016</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danilovaz</reporter><labels><label>docs</label></labels><created>2016-04-16T14:47:25Z</created><updated>2016-04-18T10:39:23Z</updated><resolved>2016-04-18T10:39:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-18T10:39:19Z" id="211322243">thanks @danilovaz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sporadically failing tests indicate Stats API may have a race condition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17807</link><project id="" key="" /><description>`org.elasticsearch.index.engine.InternalEngineMergeIT#testMergesHappening` has been failing sporadically for the past month (since March 17), for example: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-os-compatibility/os=sles/117/console

The test fails on this line:

```
logger.info("index round [{}] - segments {}, total merges {}, current merge {}", i, stats.getPrimaries().getSegments().getCount(), stats.getPrimaries().getMerge().getTotal(), stats.getPrimaries().getMerge().getCurrent());
```

Which indicates that `getSegments()` or `getMerge()` occasionally return `null` despite both segments and merge stats being set on the `IndicesStatsRequestBuilder`.
</description><key id="148814216">17807</key><summary>Sporadically failing tests indicate Stats API may have a race condition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Stats</label><label>jenkins</label><label>test</label></labels><created>2016-04-16T04:06:43Z</created><updated>2017-06-16T16:59:17Z</updated><resolved>2017-06-16T16:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-05T07:48:46Z" id="217095635">Most recent failure Apr 26: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.3+multijob-os-compatibility/os=sles/94/console

@mikemccand could this be related to https://github.com/elastic/elasticsearch/pull/18094 ?
</comment><comment author="mikemccand" created="2016-05-05T14:24:53Z" id="217168130">@clintongormley hmm, possibly?  If we hit the `AlreadyClosedException`, because of #18094, would this test then get a null back for segments stats?  Seems possible.
</comment><comment author="javanna" created="2017-06-16T16:59:16Z" id="309079367">given that this issue had no activity in more than a year, I would close it. We can always reopen if we encounter the same failure again.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy Module's IndicesRequestTests.testSuggest fails on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17806</link><project id="" key="" /><description>There are several test failures for `org.elasticsearch.messy.tests.IndicesRequestTests.testSuggest` that seem specific to Windows on 2.x, for example: http://build-us-00.elastic.co/job/es_core_2x_window-2008/853/

Reproduces With:
`mvn test -Pdev -pl org.elasticsearch.module:lang-groovy -Dtests.seed=307485C36A7BB178 -Dtests.class=org.elasticsearch.messy.tests.IndicesRequestTests -Dtests.method="testSuggest" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Des.node.mode=network -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:-UseCompressedOops -Djava.net.preferIPv4Stack=true" -Dtests.locale=lv-LV -Dtests.timezone=Etc/GMT-9`
</description><key id="148811799">17806</key><summary>Groovy Module's IndicesRequestTests.testSuggest fails on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2016-04-16T03:11:34Z</created><updated>2017-03-12T17:41:39Z</updated><resolved>2017-03-12T17:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2017-03-12T17:41:36Z" id="285960677">Groovy is removed in master, and this test has been refactored in 5.x.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backports Notify GatewayRecoveryListener on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17805</link><project id="" key="" /><description>If the recovery throws an exception we fail to notify the recovery
listener and bubble up the uncaught exception.  Commit https://github.com/elastic/elasticsearch/commit/8ca284862dce41968f455b1aa40c0f30ab4e1181 fixed this for master, but the problem remains on 2.x, causing intermittent test failures of `org.elasticsearch.client.transport.TransportClientIT#testNodeVersionIsUpdated`.

This PR backports the aforementioned commit.
</description><key id="148811124">17805</key><summary>Backports Notify GatewayRecoveryListener on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Recovery</label><label>bug</label><label>review</label><label>v2.3.2</label><label>v2.4.0</label></labels><created>2016-04-16T02:59:57Z</created><updated>2016-04-18T14:57:19Z</updated><resolved>2016-04-18T14:57:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-16T03:00:57Z" id="210723609">Example of the test failure on `org.elasticsearch.client.transport.TransportClientIT#testNodeVersionIsUpdated`: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+upload-snapshot/73/console
</comment><comment author="jasontedor" created="2016-04-18T13:36:52Z" id="211381452">LGTM.
</comment><comment author="abeyad" created="2016-04-18T14:57:19Z" id="211415804">Closed by https://github.com/elastic/elasticsearch/commit/3ea2bedb34697f2d5afa0c609ed0ea817b16bb49 and https://github.com/elastic/elasticsearch/commit/09ac891cc6983f8abc978ea4eccf7c5cbfeb60ff
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Send HTTP Warning Header(s) for any Deprecation Usage from a REST request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17804</link><project id="" key="" /><description>This adds support for `ThreadContext`s to maintain _unique_ Response headers (repeated header values are ignored) in addition to Request headers. In particular, this enables REST requests (e.g., `PUT /test`) that hop across the nodes to properly return headers as part of the overall request. With the response headers `Thread` aware thanks to the `ThreadContext`, we can associate individual requests with deprecation logging and therefore warn developers when they unwittingly use deprecated features:

``` http
PUT /test
{
  "mappings": {
    "type": {
      "properties": {
        "field": {
          "type": "string"
        },
        "field2": {
          "type": "string"
        }
      }
    }
  }
}
```

The headers that would get returned by this would be:

```
HTTP/1.1 200 OK
Warning: The [string] field is deprecated, please use [text] or [keyword] instead on [field]
Warning: The [string] field is deprecated, please use [text] or [keyword] instead on [field2]
Content-Type: application/json; charset=UTF-8
Content-Length: 21
```

Alongside that feature, this adds a `DeprecationRestHandler` that logs a deprecation warning whenever it is used, but it otherwise behaves identically to normal `RestHandler`s. This allows REST APIs to be deprecated in a consistent manner instead of dropped without any non-documentation-level deprecation warnings in advance.

Closes #17687
</description><key id="148801727">17804</key><summary>Send HTTP Warning Header(s) for any Deprecation Usage from a REST request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>:REST</label><label>enhancement</label><label>release highlight</label><label>v5.0.0-alpha5</label></labels><created>2016-04-16T01:03:05Z</created><updated>2016-07-07T15:53:01Z</updated><resolved>2016-07-06T23:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-16T01:51:02Z" id="210710153">@pickypg Which endpoints is this going to target in 2.x?
</comment><comment author="jasontedor" created="2016-04-16T02:11:06Z" id="210714795">I left a lot of comments, some of them apply to other places but I refrained from repeating the comment. I think the approach is good, but it's tricky to review without seeing the uses.
</comment><comment author="pickypg" created="2016-04-18T17:02:27Z" id="211475648">&gt; Which endpoints is this going to target in 2.x?

To start:
- `/_optimize` becoming `/_forcemerge`
- `/{index}/_optimize` becoming `/{index}/_forcemerge`.

The second of which is likely used by a lot of cron jobs out in the wild.
</comment><comment author="pickypg" created="2016-04-18T22:12:01Z" id="211601837">&gt; I think the approach is good, but it's tricky to review without seeing the uses.

Yeah, it's the particular downside to making backporting easy. :( In general though, the way that I expect to see it (armed with the knowledge that endpoints already have a `DeprecationLogger`):

``` java
controller.registerHandler(POST, "/_forcemerge", this);
controller.registerHandlerAsDeprecationHandler(POST, "/_optimize", this,
                                               "Deprecated endpoint [/_optimize] used, replaced by [/_forcemerge]",
                                               deprecationLogger);
controller.registerHandler(POST, "/{index}/_forcemerge", this);
controller.registerHandlerAsDeprecationHandler(POST, "/{index}/_optimize", this,
                                               "Deprecated endpoint [/{index}/_optimize] used, replaced by [/{index}/_forcemerge]",
                                               deprecationLogger);
```

Not every endpoint will be renamed, so it's not as simple as just giving it the new name and letting it generate the description for us. For example, we used to support `GET` with `_optimize`, which is different from a rename.
</comment><comment author="pickypg" created="2016-04-18T22:52:46Z" id="211618065">@jasontedor / @nik9000 I've modified from review.
</comment><comment author="clintongormley" created="2016-04-19T09:21:03Z" id="211821357">&gt; &gt; Which endpoints is this going to target in 2.x?
&gt; &gt; To start:
&gt; 
&gt; /_optimize becoming /_forcemerge
&gt; /{index}/_optimize becoming /{index}/_forcemerge.

Hmmm I was hoping that this PR would capture anything that being sent to the deprecation log, and include that info in the headers.  We shouldn't have a separate set of deprecation rules for headers.
</comment><comment author="pickypg" created="2016-04-19T15:25:34Z" id="211977378">@clintongormley

&gt; Hmmm I was hoping that this PR would capture anything that being sent to the deprecation log, and include that info in the headers. We shouldn't have a separate set of deprecation rules for headers.

Interesting, that didn't occur to me with the issue (the issue was purpose-driven :)). The only way for us to do that would be to intercept/listen to calls to the deprecation logger, then somehow associate them with the current request/response and add them as a separate warning header (which is the appropriate way to do it according to the RFC as well, which is a good thing).

Worse, we may not even be the node that does the deprecation logging (e.g., a cluster state change getting handled by the elected master, with the REST call being handled by a data node). That makes it a bigger change, but I think it's doable with a little research on my part by adding it to the transport header instead of even worrying about REST for the most part (because REST auto-fills from the transport headers).

That said, deprecating of REST endpoints still needs to be handled and acknowledged with each request because once we start handling it, we effectively ignore its origin and at the start of handling we don't even have the transport headers created yet, thus making it the exception to the rule. So I think that would go in tandem with this PR.
</comment><comment author="clintongormley" created="2016-04-20T11:45:50Z" id="212392908">&gt; That said, deprecating of REST endpoints still needs to be handled and acknowledged with each request 

I'd use the deprecation logger for deprecated REST endpoints, same as we do for everything else. Then, when (if?) we manage to return deprecation warnings in headers, then we'll get REST endpoints for free.
</comment><comment author="uboness" created="2016-05-04T10:26:20Z" id="216822874">what's the status of this PR?
</comment><comment author="pickypg" created="2016-05-20T05:10:56Z" id="220518670">@uboness It was stalled, but it's back on track.
</comment><comment author="pickypg" created="2016-05-20T17:25:27Z" id="220667511">I have it working to get _all_ deprecation log statements now, but I have a few issues to work out.

``` bash
curl -XPUT -i localhost:9200/test -d '{
&gt;   "settings": {
&gt;     "number_of_shards": 1,
&gt;     "number_of_replicas": 0
&gt;   },
&gt;   "mappings": {
&gt;     "type": {
&gt;       "properties": {
&gt;         "field" : {
&gt;           "type" : "string"
&gt;         }
&gt;       }
&gt;     }
&gt;   }
&gt; }'
HTTP/1.1 200 OK
Warning: The [string] field is deprecated, please use [text] or [keyword] instead on [field]
Content-Type: application/json; charset=UTF-8
Content-Length: 21

{"acknowledged":true}
```

Here's an example with multiple deprecations (I locally deprecated `/{index}/_search` using the method added by this PR to make an easy example):

``` bash
curl -XPOST -i localhost:9200/test/_search -d '{
&gt;   "query": {
&gt;     "bool": {
&gt;       "must": [
&gt;         {
&gt;           "indices": {
&gt;             "indices": [
&gt;               "test"
&gt;             ],
&gt;             "query": {
&gt;               "match_all": {}
&gt;             }
&gt;           }
&gt;         }
&gt;       ]
&gt;     }
&gt;   }
&gt; }'
HTTP/1.1 200 OK
Warning: /_search is not really deprecated
Warning: indices query is deprecated. Instead search on the '_index' field
Content-Type: application/json; charset=UTF-8
Content-Length: 236

{"took":17,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"test","_type":"type","_id":"1","_score":1.0,"_source":{
  "field": "value",
  "message": "raw text"
}
}]}}
```

It's pretty cool and it works pretty well. It's just a matter of squashing some bugs. /cc @clintongormley 
</comment><comment author="pickypg" created="2016-05-21T17:44:23Z" id="220791184">This is ready for review.
</comment><comment author="pickypg" created="2016-05-21T17:53:06Z" id="220791593">@jaymode Would you mind taking a look?
</comment><comment author="jaymode" created="2016-05-24T18:13:41Z" id="221357223">@pickypg how does this work with calls that hit multiple different nodes and the response is combined? Some examples of what I am talking about are extenders of `TransportBroadcastAction`, `TransportBroadcastByNodeAction`, and `TransportNodesAction` (there may be others). 

My understanding is the response headers from the "sub requests" will get dropped except for the last one, which finishes the action.
</comment><comment author="pickypg" created="2016-05-24T18:55:39Z" id="221368766">@jaymode That's exactly what happens currently. There's _currently_ no scenario that I'm aware of where it makes any difference, but I can imagine that changing as we expand its usage.

I have been thinking about adding a `readResponseHeaders` method that combines the existing `ThreadContext`'s response headers with the responding one. What do you think?
</comment><comment author="jaymode" created="2016-05-24T21:10:17Z" id="221402263">&gt; I have been thinking about adding a readResponseHeaders method that combines the existing ThreadContext's response headers with the responding one. What do you think?

Where would you do that? When a response comes back and you are reading the headers in, the execution is happening in a transport thread, which is most likely going to be different than the one that made the request so I am not sure where/how you will get the existing context.

I think this would need to be kept track of in the `AsyncAction` classes that send requests to all nodes and handle responses. Something like accumulate/merge headers on each response and then set in the context when finished.
</comment><comment author="pickypg" created="2016-05-25T19:46:29Z" id="221685756">You're right. I am taking a look at the `TransportService` itself to avoid forcing every `AsyncAction` to do this (manually).
</comment><comment author="pickypg" created="2016-06-17T05:29:49Z" id="226683793">@jaymode Alright, so I've gotten back to this and I've looked over what it will take to get it to do the scatter / merge in the async action (effectively). It's doable, but it adds complexity to do something that we currently won't take advantage of at all.

So what I want to do is this:
1. Add a flag so that response headers are only accumulated in the `ThreadContext` if they're expected (when the request originates from a REST call).
2. Add more tests. I already have an integration test in place that tests / proves that it will not accumulate sub-request's responses.

Merge that, then backport it to the 2.x branch. Then implement the sub-request handling on master only. What do you think?
</comment><comment author="jaymode" created="2016-06-17T11:14:56Z" id="226743813">&gt; Add a flag so that response headers are only accumulated in the ThreadContext if they're expected (when the request originates from a REST call).

Do we really need this?

&gt; Merge that, then backport it to the 2.x branch. Then implement the sub-request handling on master only

That works for me
</comment><comment author="pickypg" created="2016-07-05T22:08:03Z" id="230617493">@jaymode Please take a second pass now that I have rebased it back onto master. I think it's ready to merge with all tests passing.
</comment><comment author="jaymode" created="2016-07-06T11:58:44Z" id="230751715">left one comment, other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lucene dependency can't be automatically downloaded in maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17803</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha1

**JVM version**: 1.8.0_65-b17

**OS version**: OS X Yosemite 10.10.5 (14F1605)

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. in my pom.xml, I included elasticsearch 5.0.0. alpha-1: 
              &lt;dependency&gt;
           &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
           &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
           &lt;version&gt;5.0.0-alpha1&lt;/version&gt;
       &lt;/dependency&gt;
2. when I do a maven build, I got the following error:

[ERROR] Failed to execute goal on project pdfprototype: Could not resolve dependencies for project com.tina.index:pdfprototype:jar:0.0.1-SNAPSHOT: The following artifacts could not be resolved: org.apache.lucene:lucene-core:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-join:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-queryparser:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-misc:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-highlighter:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-sandbox:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-spatial3d:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-backward-codecs:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-spatial-extras:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-grouping:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-analyzers-common:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-memory:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-queries:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-spatial:jar:6.0.0-snapshot-f0aa4fc, org.apache.lucene:lucene-suggest:jar:6.0.0-snapshot-f0aa4fc: Failure to find org.apache.lucene:lucene-core:jar:6.0.0-snapshot-f0aa4fc in https://repo.maven.apache.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced -&gt; [Help 1]

I have to manually add all the lucene library into my pom file to get rid of this compiling error. 
 3.

**Provide logs (if relevant)**:
</description><key id="148778565">17803</key><summary>Lucene dependency can't be automatically downloaded in maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lijuan8</reporter><labels /><created>2016-04-15T21:54:05Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-15T21:58:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-15T21:58:42Z" id="210662893">You need to add a repository, something like this:

``` xml
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;Lucene snapshots&lt;/id&gt;
        &lt;url&gt;https://download.elasticsearch.org/lucenesnapshots/f0aa4fc&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
```
</comment><comment author="lijuan8" created="2016-04-15T22:02:57Z" id="210663751">thanks!
</comment><comment author="jasontedor" created="2016-04-15T22:06:20Z" id="210664457">&gt; thanks!

You are most welcome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move parsing of allocation commands into REST and remove support for plugins to register allocation commands</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17802</link><project id="" key="" /><description>Port them to the ObjectParser.

Closes #17894
</description><key id="148774975">17802</key><summary>Move parsing of allocation commands into REST and remove support for plugins to register allocation commands</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-15T21:33:36Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-05-24T16:00:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-15T21:34:03Z" id="210656046">@javanna this is what you asked me to do in https://github.com/elastic/elasticsearch/commit/fe4d1297dfb5d07d4214757358b7927034a18606#commitcomment-17117314 I think.
</comment><comment author="javanna" created="2016-04-16T06:23:47Z" id="210745775">thanks @nik9000 ! do we have tests for this parse method somewhere? Maybe we should unit test it, especially if while moving we switch it to object parser.
</comment><comment author="nik9000" created="2016-04-16T11:52:47Z" id="210796793">&gt; Maybe we should unit test it, especially if while moving we switch it to object parser.

yeah, that certainly occurred to me as I was going about my evening last night. I'll look into that when I get a chance.

We have some fairly basic REST tests for it - never for parsing more than one at a time.
</comment><comment author="nik9000" created="2016-04-18T17:37:06Z" id="211491430">@javanna I added round trip testing for parsing and streaming to this. Most of the work is implementing equals and hashCode.
</comment><comment author="javanna" created="2016-04-20T08:17:00Z" id="212318604">thanks for adding the tests, I left a few more comments
</comment><comment author="nik9000" created="2016-04-20T12:06:34Z" id="212399007">@javanna I believe I answered your comments either inline or with the patch I just pushed.
</comment><comment author="nik9000" created="2016-04-20T12:52:57Z" id="212411888">@javanna looks like the discussion about the commands method is getting eaten when I push new patches. My point is that making commands a thing you can register means that plugins can do it so we may as well give them a good way to test. It isn't that much work in our test to do that. I've pushed some javadocs explaining it on the test. I get your point about losing it the next time someone changes the test though.
</comment><comment author="javanna" created="2016-04-20T12:57:39Z" id="212413200">@nik9000 can we test this extension point for commands by plugging a custom one in so that also this test utilities it's used the way plugins would use it?
</comment><comment author="nik9000" created="2016-04-20T14:02:56Z" id="212439002">Sure
</comment><comment author="nik9000" created="2016-04-20T15:07:14Z" id="212468018">&gt; Sure

Done.
</comment><comment author="nik9000" created="2016-05-16T15:06:30Z" id="219449483">@javanna I rebased this and removed the ability for plugins to register allocation commands. It was super out of date. Sorry for the rebase!
</comment><comment author="javanna" created="2016-05-24T08:46:54Z" id="221204953">@nik9000 LGTM please add breaking label and highlight the removal of custom allocation commands.
</comment><comment author="nik9000" created="2016-05-24T15:46:11Z" id="221314392">@javanna I've rebased to resolve some minor conflicts and added a note to the breaking changes docs. Can you have a look a the breaking changes note?
</comment><comment author="javanna" created="2016-05-24T15:52:23Z" id="221316437">left a comment on breaking changes docs, feel free to push once updated
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow JSON with unquoted field names by enabling system property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17801</link><project id="" key="" /><description>In Elasticsearch 5.0.0, by default unquoted field names in JSON will be
rejected. This can cause issues, however, for documents that were
already indexed with unquoted field names. To alleviate this, a system
property has been added that can be enabled so migration can occur.

This system property will be removed in Elasticsearch 6.0.0

Resolves #17674
</description><key id="148773388">17801</key><summary>Allow JSON with unquoted field names by enabling system property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:REST</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T21:27:02Z</created><updated>2016-04-19T16:03:09Z</updated><resolved>2016-04-19T16:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-15T21:39:00Z" id="210657637">Left some comments but otherwise fine by me.

I suspect we can also have a unit test for this? Maybe it can fail if the current major version is 6?
</comment><comment author="jasontedor" created="2016-04-15T21:53:54Z" id="210661495">I think this needs a note in the migration docs. Did the original issue that added the strict parsing on the JSON parser add a note? If so, can we add one along side that note? Otherwise, can we add a full note describing the issue and the solution here? Otherwise, looks great.
</comment><comment author="jasontedor" created="2016-04-16T18:47:26Z" id="210872794">I wonder if this should receive deprecation logging immediately out of the box in 5.0.0?
</comment><comment author="dakrone" created="2016-04-18T15:02:39Z" id="211417646">@jasontedor I pushed a couple of commits about making the option parsing extremely strict and extended the blurb in the migration guide about this
</comment><comment author="jasontedor" created="2016-04-19T00:43:19Z" id="211656598">Can we sprinkle some `TODO` in `JsonXContent` and `Node`? Otherwise, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate camelCase usages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17800</link><project id="" key="" /><description>There are a few places where parameters and output are duplicated in camelCase. This change adds deprecation logging for those cases, so that support for providing magical camelCase can be removed in 5.0.

see #8988
</description><key id="148769136">17800</key><summary>Deprecate camelCase usages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:REST</label><label>deprecation</label><label>v2.3.2</label><label>v2.4.0</label></labels><created>2016-04-15T21:03:58Z</created><updated>2016-04-18T18:27:18Z</updated><resolved>2016-04-18T18:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-16T18:39:32Z" id="210872068">Luca made a comment which I think we should fix to avoid weird corner cases. Otherwise this looks great!
</comment><comment author="rjernst" created="2016-04-16T23:44:44Z" id="210924970">Good catch @javanna, I pushed another commit.
</comment><comment author="jpountz" created="2016-04-17T08:46:34Z" id="210979405">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex should never report negative throttled_until</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17799</link><project id="" key="" /><description>Just clamp the value at 0. It isn't useful to tell the user "this
thread should have woken 5ms ago".

Closes #17783
</description><key id="148767232">17799</key><summary>Reindex should never report negative throttled_until</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T20:54:44Z</created><updated>2016-04-15T21:03:12Z</updated><resolved>2016-04-15T21:03:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-15T21:00:37Z" id="210644858">LGTM
</comment><comment author="nik9000" created="2016-04-15T21:02:37Z" id="210645364">Thanks for reviewing @dakrone !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut moving_avg aggregation to registerPipelineAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17798</link><project id="" key="" /><description>and remove its PROTOTYPE.

Relates to #17085
</description><key id="148756262">17798</key><summary>Cut moving_avg aggregation to registerPipelineAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T20:08:53Z</created><updated>2016-05-02T12:12:30Z</updated><resolved>2016-04-18T13:53:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-18T07:40:40Z" id="211245625">LGTM
</comment><comment author="nik9000" created="2016-04-18T13:53:25Z" id="211390427">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut bucket_script and bucket_selector to registerPipelineAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17797</link><project id="" key="" /><description>and remove their PROTOTYPEs.

Relates to #17085
</description><key id="148752626">17797</key><summary>Cut bucket_script and bucket_selector to registerPipelineAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T19:50:53Z</created><updated>2016-05-02T12:12:36Z</updated><resolved>2016-04-18T13:26:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-18T07:33:36Z" id="211243701">LGTM
</comment><comment author="nik9000" created="2016-04-18T13:26:37Z" id="211377940">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut serial_diff and cumulative_sum to registerPipelineAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17796</link><project id="" key="" /><description>and remove their PROTOTYPEs.

Relates to #17085
</description><key id="148741147">17796</key><summary>Cut serial_diff and cumulative_sum to registerPipelineAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T18:58:46Z</created><updated>2016-05-02T12:12:41Z</updated><resolved>2016-04-18T12:57:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-18T07:34:26Z" id="211243825">LGTM
</comment><comment author="nik9000" created="2016-04-18T12:58:18Z" id="211370132">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range Aggregation Returns No Results on 5.0.0-alpha1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17795</link><project id="" key="" /><description>**Elasticsearch version**:
5.0.0-alpha1

**JVM version**:
java 8

**OS version**:

Elasticsearch Docker Image (based on java:8-jre)

**Description of the problem including expected versus actual behavior**:

I've got a bunch of documents like this:

```
{
  "_index": "test-rtes",
  "_type": "rtesevent",
  "_id": "AVQbPk8vbhx2Mf7tEPJS",
  "_score": 1,
  "_source": {
    "call": {
      "risk_score": 44.8285,
      "site_name": "atlanta",
      "toll_free_number": ""
    },
    "current_time": "2016-04-15T18:45:49Z",
    "event_id": "c9a89d35-f166-47f1-b9f3-5d9a9d7f0582",
    "event_type": "risk_score",
    "version": 1
  },
  "fields": {
    "current_time": [
      1460745949000
    ]
  }
}
```

And I want to aggregate on call.risk_score:

```
{
  "size": 0,
  "aggs": {
    "2": {
      "range": {
        "field": "call.risk_score",
        "ranges": [
          {
            "from": 0,
            "to": 50
          },
          {
            "from": 51,
            "to": 100
          }
        ],
        "keyed": true
      }
    }
  }
}
```

And I always get 0 results back:

```
{
  "_shards": {
    "failed": 0,
    "successful": 5,
    "total": 5
  },
  "aggregations": {
    "2": {
      "buckets": {
        "0.0-50.0": {
          "doc_count": 0,
          "from": 0.0,
          "from_as_string": "0.0",
          "to": 50.0,
          "to_as_string": "50.0"
        },
        "51.0-100.0": {
          "doc_count": 0,
          "from": 51.0,
          "from_as_string": "51.0",
          "to": 100.0,
          "to_as_string": "100.0"
        }
      }
    }
  },
  "hits": {
    "hits": [],
    "max_score": 0.0,
    "total": 1805264
  },
  "timed_out": false,
  "took": 119
}
```

the relevant chunk of my mapping is:

```
    "mappings": {
      "rtesevent": {
        "properties": {
          "call": {
            "properties": {
              "risk_score": {
                "type": "float"
              }
            },
```

Am I holding it wrong? I tried generating the query in Kibana 5.0.0-alpha1 and it returns 0 results as well. A count query returns many results.
</description><key id="148740318">17795</key><summary>Range Aggregation Returns No Results on 5.0.0-alpha1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tebriel</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2016-04-15T18:54:31Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-05-24T10:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-15T20:39:59Z" id="210637139">@tebriel I'm not able to reproduce this, do you have a small reproduction you can share?

## Create Index&lt;a id="orgheadline1"&gt;&lt;/a&gt;

``` es
POST /i
{
  "mappings": {
    "d": {
      "properties": {
        "call": {
          "properties": {
            "risk_score": {
              "type": "float"
            }
          }
        }
      }
    }
  }
}
```

``` es
{"acknowledged":true}
```

``` es
GET /i/d/_mapping?pretty
{}
```

``` es
{
  "i" : {
    "mappings" : {
      "d" : {
        "properties" : {
          "call" : {
            "properties" : {
              "risk_score" : {
                "type" : "float"
              }
            }
          }
        }
      }
    }
  }
}
```

## Index Doc&lt;a id="orgheadline2"&gt;&lt;/a&gt;

``` es
POST /i/d/1
{
  "call": {
    "risk_score": 44.8285,
    "site_name": "atlanta",
    "toll_free_number": ""
  },
  "current_time": "2016-04-15T18:45:49Z",
  "event_id": "c9a89d35-f166-47f1-b9f3-5d9a9d7f0582",
  "event_type": "risk_score",
  "version": 1
}
```

``` es
{"_index":"i","_type":"d","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}
```

## Range Agg&lt;a id="orgheadline3"&gt;&lt;/a&gt;

``` es
POST /i/_search?pretty
{
  "size": 0,
  "aggs": {
    "2": {
      "range": {
        "field": "call.risk_score",
        "ranges": [
          {
            "from": 0,
            "to": 50
          },
          {
            "from": 51,
            "to": 100
          }
        ],
        "keyed": true
      }
    }
  }
}
```

``` es
{
  "took" : 37,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "2" : {
      "buckets" : {
        "0.0-50.0" : {
          "from" : 0.0,
          "from_as_string" : "0.0",
          "to" : 50.0,
          "to_as_string" : "50.0",
          "doc_count" : 1
        },
        "51.0-100.0" : {
          "from" : 51.0,
          "from_as_string" : "51.0",
          "to" : 100.0,
          "to_as_string" : "100.0",
          "doc_count" : 0
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-05-24T10:35:53Z" id="221230751">No further feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't pass XContentParser to ParseFieldRegistry#lookup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17794</link><project id="" key="" /><description>Passing the parser in is not good because we are not parsing anything in the lookup methods, we only need it to retrieve the xcontent location from it so that in case there is an error we emit where we were with the parsing.
</description><key id="148727905">17794</key><summary>Don't pass XContentParser to ParseFieldRegistry#lookup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T18:07:03Z</created><updated>2016-04-20T14:38:52Z</updated><resolved>2016-04-20T14:38:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-15T18:07:13Z" id="210568637">@nik9000  I believe this is for you ;)
</comment><comment author="javanna" created="2016-04-18T16:53:52Z" id="211471524">I updated this PR @nik9000 
</comment><comment author="nik9000" created="2016-04-18T17:31:04Z" id="211488287">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut stats_bucket and extended_stats_bucket to registerPipelineAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17793</link><project id="" key="" /><description>and remove their PROTOTYPE.

Relates to #17085
</description><key id="148726671">17793</key><summary>Cut stats_bucket and extended_stats_bucket to registerPipelineAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T18:01:17Z</created><updated>2016-05-02T12:12:46Z</updated><resolved>2016-04-18T12:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-18T07:36:42Z" id="211244537">LGTM
</comment><comment author="nik9000" created="2016-04-18T12:31:44Z" id="211358944">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tophits-aggregation.asciidoc: fix a typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17792</link><project id="" key="" /><description /><key id="148723258">17792</key><summary>tophits-aggregation.asciidoc: fix a typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels><label>docs</label></labels><created>2016-04-15T17:44:08Z</created><updated>2016-04-18T09:41:06Z</updated><resolved>2016-04-18T07:24:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-18T07:25:10Z" id="211241809">thanks @golubev 
</comment><comment author="golubev" created="2016-04-18T09:41:06Z" id="211298725">Thanks, @martijnvg!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut global, missing, cardinality, and value_count to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17791</link><project id="" key="" /><description>and remove their PROTOTYPEs.

Related to #17085
</description><key id="148718705">17791</key><summary>Cut global, missing, cardinality, and value_count to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T17:23:57Z</created><updated>2016-05-02T12:12:50Z</updated><resolved>2016-04-18T12:44:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-15T17:25:49Z" id="210554649">@colings86 that is the last of the non-pipeline aggregations! I'll try and finish the pipeline aggregations in smaller chunks this afternoon so we can finish off the PROTOTYPE removal early next week. Once we finish these we've still got a few PROTOTYPEs sitting around for things like the ranges.
</comment><comment author="colings86" created="2016-04-18T07:32:10Z" id="211243251">LGTM
</comment><comment author="nik9000" created="2016-04-18T12:44:37Z" id="211364791">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut percentiles and percentile_ranks to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17790</link><project id="" key="" /><description>and remove their PROTOTYPEs.

Relates to #17085
</description><key id="148710862">17790</key><summary>Cut percentiles and percentile_ranks to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T16:43:56Z</created><updated>2016-05-02T12:12:54Z</updated><resolved>2016-04-18T15:10:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-18T07:30:22Z" id="211242929">LGTM
</comment><comment author="nik9000" created="2016-04-18T15:10:30Z" id="211422267">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut max, min, stats, and extended stats aggregations over to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17789</link><project id="" key="" /><description>and remove their PROTOTYPEs.

Relates to #17085
</description><key id="148706035">17789</key><summary>Cut max, min, stats, and extended stats aggregations over to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T16:26:46Z</created><updated>2016-05-02T12:13:00Z</updated><resolved>2016-04-18T15:45:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-18T07:27:51Z" id="211242348">LGTM
</comment><comment author="nik9000" created="2016-04-18T15:46:00Z" id="211439407">Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't resolve the revision hash during the build unless it needs it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17788</link><project id="" key="" /><description>This fixes the build for folks that build without git installed locally
and should speed up the general case because we aren't trying to resolve
git information when it isn't really needed.
</description><key id="148693644">17788</key><summary>Don't resolve the revision hash during the build unless it needs it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T15:36:49Z</created><updated>2016-04-15T16:28:46Z</updated><resolved>2016-04-15T16:25:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-15T15:38:26Z" id="210510391">@rjernst and @spinscale 95579ca95ad050d1573255d5b461c4a9cf570726 broke my workhorse machine because I don't sync the git files to it. Fix seems useful in general though.
</comment><comment author="rjernst" created="2016-04-15T15:42:01Z" id="210511764">LGTM
</comment><comment author="nik9000" created="2016-04-15T15:44:17Z" id="210512460">Thanks for the quick review @rjernst !
</comment><comment author="spinscale" created="2016-04-15T16:28:46Z" id="210530716">thx @nik9000 - hadnt thought of that one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut significant_terms to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17787</link><project id="" key="" /><description>and remove its PROTOTYPE.

Relates to #17085
</description><key id="148690616">17787</key><summary>Cut significant_terms to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T15:27:08Z</created><updated>2016-05-02T12:13:10Z</updated><resolved>2016-04-15T18:32:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-15T15:27:45Z" id="210505680">I wasn't able to test this because the build doesn't work on my machine any more after something merged a few minutes ago. So I'll hunt that down before I do any more of these....
</comment><comment author="colings86" created="2016-04-15T15:29:13Z" id="210506360">LGTM
</comment><comment author="colings86" created="2016-04-15T15:30:23Z" id="210507189">oh, thats possibly the new points stuff? I know Adrien was mentioning that sig terms on long fields would break following that change so maybe its related to that?
</comment><comment author="nik9000" created="2016-04-15T15:38:55Z" id="210510547">&gt; oh, thats possibly the new points stuff? I know Adrien was mentioning that sig terms on long fields would break following that change so maybe its related to that?

It was #17788. I think the points stuff is probably going to be transparent. Hopefully!
</comment><comment author="nik9000" created="2016-04-15T18:34:06Z" id="210581772">Merged! Thanks for the review @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut top_hits aggregation to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17786</link><project id="" key="" /><description>and remove its PROTOTYPE.

Relates to #17085
</description><key id="148688125">17786</key><summary>Cut top_hits aggregation to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T15:17:43Z</created><updated>2016-05-02T12:13:16Z</updated><resolved>2016-04-15T18:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T15:23:34Z" id="210503536">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut geo aggregations to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17785</link><project id="" key="" /><description>and remove their prototypes.

Relates to #17085
</description><key id="148687912">17785</key><summary>Cut geo aggregations to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T15:16:40Z</created><updated>2016-05-02T12:13:20Z</updated><resolved>2016-04-15T18:11:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T15:22:26Z" id="210503080">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update aws sdk to 1.10.69 and add use_throttle_retries repository setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17784</link><project id="" key="" /><description>This PR brings 2 changes:
- Upgrade to AWS SDK 1.10.69
- Add `repositories.s3.use_throttle_retries` setting
# Upgrade to AWS SDK 1.10.69
## Release notes highlights from 1.10.33 for the SDK
- Moving from JSON.org to Jackson for request marshallers.
- The Java SDK now supports retry throttling to limit the rate of retries during periods of reduced availability. This throttling behavior can be enabled via ClientConfiguration or via the system property "-Dcom.amazonaws.sdk.enableThrottledRetry".
- Fixed String case conversion issues when running with non English locales.
- AWS SDK for Java introduces a new dynamic endpoint system that can compute endpoints for services in new regions.
- Introducing a new AWS region, ap-northeast-2.
- Added a new metric, HttpSocketReadTime, that records socket read latency. You can enable this metric by adding enableHttpSocketReadMetric to the system property com.amazonaws.sdk.enableDefaultMetrics. For more information, see [Enabling Metrics with the AWS SDK for Java](https://java.awsblog.com/post/Tx3C0RV4NRRBKTG/Enabling-Metrics-with-the-AWS-SDK-for-Java).
- New Client Execution timeout feature to set a limit spent across retries, backoffs, ummarshalling, etc. This new timeout can be specified at the client level or per request.
  Also included in this release is the ability to specify the existing HTTP Request timeout per request rather than just per client.
## Release notes highlights from 1.10.33 for S3
- Added support for RequesterPays for all operations.
- Ignore the 'Connection' header when generating S3 responses.
- Allow users to generate an AmazonS3URI from a string without using URL encoding.
- Fixed issue that prevented creating buckets when using a client configured for the s3-external-1 endpoint.
- Amazon S3 bucket lifecycle configuration supports two new features: the removal of expired object delete markers and an action to abort incomplete multipart uploads.
- Allow TransferManagerConfiguration to accept integer values for multipart upload threshold.
- Copy the list of ETags before sorting aws/aws-sdk-java#589.
- Option to disable chunked encoding aws/aws-sdk-java#586.
- Adding retry on InternalErrors in CompleteMultipartUpload operation. aws/aws-sdk-java#538
- Deprecated two APIs : AmazonS3#changeObjectStorageClass and AmazonS3#setObjectRedirectLocation.
- Added support for the aws-exec-read canned ACL. Owner gets FULL_CONTROL. Amazon EC2 gets READ access to GET an Amazon Machine Image (AMI) bundle from Amazon S3.
## Release notes highlights from 1.10.33 for EC2
- Added support for referencing security groups in peered Virtual Private Clouds (VPCs). For more information see the service announcement at https://aws.amazon.com/about-aws/whats-new/2016/03/announcing-support-for-security-group-references-in-a-peered-vpc/ .
- Fixed a bug in AWS SDK for Java - Amazon EC2 module that returns NPE for dry run requests.
- Regenerated client with new implementation of code generator.
- This feature enables support for DNS resolution of public hostnames to private IP addresses when queried over ClassicLink. Additionally, you can now access private hosted zones associated with your VPC from a linked EC2-Classic instance. ClassicLink DNS support makes it easier for EC2-Classic instances to communicate with VPC resources using public DNS hostnames.
- You can now use Network Address Translation (NAT) Gateway, a highly available AWS managed service that makes it easy to connect to the Internet from instances within a private subnet in an AWS Virtual Private Cloud (VPC). Previously, you needed to launch a NAT instance to enable NAT for instances in a private subnet. Amazon VPC NAT Gateway is available in the US East (N. Virginia), US West (Oregon), US West (N. California), EU (Ireland), Asia Pacific (Tokyo), Asia Pacific (Singapore), and Asia Pacific (Sydney) regions. To learn more about Amazon VPC NAT, see [New - Managed NAT (Network Address Translation) Gateway for AWS](https://aws.amazon.com/blogs/aws/new-managed-nat-network-address-translation-gateway-for-aws/)
- A default read timeout is now applied when querying data from EC2 metadata service.
# Add `use_throttle_retries` repository setting

Defaults to `false` (AWS SDK default value).
If anyone is having trouble, you could enable it with `repositories.s3.use_throttle_retries: true` in `elasticsearch.yml` file or set it per repository.
</description><key id="148654235">17784</key><summary>Update aws sdk to 1.10.69 and add use_throttle_retries repository setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>upgrade</label><label>v5.0.0-alpha4</label></labels><created>2016-04-15T12:58:45Z</created><updated>2016-06-17T15:28:57Z</updated><resolved>2016-06-17T15:28:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-28T14:04:26Z" id="215433673">@tlrx Could you review this change please?
</comment><comment author="tlrx" created="2016-05-06T07:36:01Z" id="217371091">Left a comment
</comment><comment author="dadoonet" created="2016-05-19T14:58:55Z" id="220350828">@tlrx I added new commits. I also changed the title and the description of the PR.
</comment><comment author="tlrx" created="2016-05-25T13:52:56Z" id="221582836">Left some more comments
</comment><comment author="dadoonet" created="2016-05-27T08:16:01Z" id="222087661">@tlrx I added a new commit to address your comments. Thanks a lot!

BTW, I'm changing the description of the PR to conform with those new changes.
</comment><comment author="dadoonet" created="2016-06-13T06:24:27Z" id="225498150">ping @tlrx 
</comment><comment author="tlrx" created="2016-06-14T11:28:39Z" id="225853667">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex: throttle times returning negative numbers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17783</link><project id="" key="" /><description>**Elasticsearch version**:
master

**JVM version**:
1.8.0_51

**OS version**:
OSX

**Description of the problem including expected versus actual behavior**:
I'm getting negative numbers back for "throttled time" in reindex/update-by-query.

**Steps to reproduce**:

``` sh
#Make a pile of docs.
for i in $(seq 1 10000); do curl -XPOST localhost:9200/test/test -d'{"test": "test"}'; echo; done
curl -XPOST localhost:9200/_refresh

# Kick off a reindex and loo
curl -XPOST 'localhost:9200/test/_update_by_query?wait_for_completion=false'
while curl -XGET 'localhost:9200/_tasks/?pretty&amp;detailed=true'; do sleep .1; done
```

That returned throttled times that were negative numbers. Small negative numbers, like `-56` and stuff. That doesn't make any sense.
</description><key id="148649267">17783</key><summary>Reindex: throttle times returning negative numbers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T12:35:50Z</created><updated>2016-04-15T21:03:12Z</updated><resolved>2016-04-15T21:03:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-15T12:36:01Z" id="210447584">Note that I wasn't throttling the request at all.
</comment><comment author="nik9000" created="2016-04-15T20:37:23Z" id="210635725">It looks like I was wrong, it is the "throttled_until" times:

```
 "throttled_until_millis" : -5
```

Still, those are wrong.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Init scripts option switches not working -Ees.xxxx</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17782</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_77

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
The init scripts in the packaging repo attempt to set the DAEMON_OPTS using a "-E" switch. It seems that Elasticsearch is expecting a "-D" switch to set a property value.

**Steps to reproduce**:
elasticsearch --help

OPTIONS

```
-h,--help                    Shows this message

-p,--pidfile &lt;pidfile&gt;       Creates a pid file in the specified path on start

-d,--daemonize               Starts Elasticsearch in the background

-Dproperty=value             Configures an Elasticsearch specific property, like -Dnetwork.host=127.0.0.1

--property=value             Configures an elasticsearch specific property, like --network.host 127.0.0.1
--property value

NOTE: The -d, -p, and -D arguments must appear before any --property arguments.
```

Once I edited my local /etc/init.d/elasticsearch file that service started as expected.
</description><key id="148632428">17782</key><summary>Init scripts option switches not working -Ees.xxxx</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">CliveJL</reporter><labels /><created>2016-04-15T11:15:24Z</created><updated>2016-04-15T14:17:43Z</updated><resolved>2016-04-15T13:59:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-15T11:33:06Z" id="210425366">Are you sure that is what is going on here? Maybe you installed 5.0.0-alpha1 and that's the init script that is still in place? The `-Ees.` is a recent change in master, not integrated into the 2.x series at all; I strongly suspect that you have an init script from 5.0.0-alpha1. Can you share the full contents of the init script that you had (before you edited it) here? The 2.x init scripts do not have `-E` in them (otherwise Elasticsearch 2.3.1 would be horrifically broken right now). 

``` bash
07:22:21 [jason:~/src/elastic/elasticsearch-2.x] bd98092 &#177; ack "DAEMON_OPTS"
distribution/deb/src/main/packaging/init.d/elasticsearch
102:DAEMON_OPTS="-d -p $PID_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
170:    start-stop-daemon -d $ES_HOME --start -b --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS

distribution/deb/target/generated-packaging/deb/init.d/elasticsearch
102:DAEMON_OPTS="-d -p $PID_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.conf=$CONF_DIR"
170:    start-stop-daemon -d $ES_HOME --start -b --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
```

And it works fine for me:

``` bash
$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.2 LTS
Release:        14.04
Codename:       trusty
$ apt-cache policy elasticsearch
elasticsearch:
  Installed: 2.3.1
  Candidate: 2.3.1
  Version table:
 *** 2.3.1 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
        100 /var/lib/dpkg/status
     2.3.0 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.2.2 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.2.1 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.2.0 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.1.2 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.1.1 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.1.0 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.0.2 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.0.1 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.0.0 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.0.0~rc1 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.0.0~beta2 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
     2.0.0~beta1 0
        500 http://packages.elastic.co/elasticsearch/2.x/debian/ stable/main amd64 Packages
$ sudo service elasticsearch start
 * Starting Elasticsearch Server
   ...done.
$ curl -XGET localhost:9200
{
  "name" : "Projector",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "2.3.1",
    "build_hash" : "bd980929010aef404e7cb0843e61d0665269fc39",
    "build_timestamp" : "2016-04-04T12:25:05Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
$ 
```
</comment><comment author="CliveJL" created="2016-04-15T13:59:17Z" id="210473673">I think I must have got my init scripts from the ES 5 branch, sorry!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo polygon searches are slow. But they go about three times faster with profile: true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17781</link><project id="" key="" /><description>Elasticsearch version: 
2.2.0

JVM version: 
java version "1.8.0_74"
Java(TM) SE Runtime Environment (build 1.8.0_74-b02)
Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)

OS version:
CentOS Linux release 7.2.1511 (Core) 

We have approximately 500k items in our index over 15 shards (we expect many more documents over time). The documents are relatively small (approx 5k). The Elastic cluster is spread over 12 boxes, each with 90G RAM and 16 cores running 1 node per box. Each shard has two replicas. No other applications run on the boxes so it should be pretty well specced. 

We have a query as follows:

```
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "bool": {
                "should": [
                  {
                    "bool": {
                      "must": [
                        {
                          "terms": {
                            "metadata.queries": [
                              53
                            ]
                          }
                        },
                        {
                          "range": {
                            "metadata.timestamp": {
                              "from": 1365984000000
                            }
                          }
                        }
                      ]
                    }
                  },
                  {
                    "bool": {
                      "must": [
                        {
                          "terms": {
                            "metadata.queries": [
                              2034,
                              3106,
                              3107,
                              3108,
                              3109
                            ]
                          }
                        },
                        {
                          "range": {
                            "metadata.timestamp": {
                              "from": 1366070400000
                            }
                          }
                        }
                      ]
                    }
                  }
                ]
              }
            }
          ]
        }
      },
      "query": {
        "bool": {
          "must": [
            {
              "range": {
                "core.date.timestamp": {
                  "gte": "2016-04-08T09:16:33.443Z",
                  "lte": "2016-04-15T09:19:05.251Z"
                }
              }
            },
            {
              "terms": {
                "metadata.queries": [
                  53,
                  2034,
                  3106,
                  3107,
                  3108,
                  3109
                ]
              }
            },
            {
              "bool": {
                "must": [
                  {
                    "bool": {
                      "must": [
                        {
                          "or": [
                            {
                              "geo_polygon": {
                                "activity.location.location": {
                                  "points": [
                                    [
                                      -56.07421875,
                                      -24.206889622398023
                                    ],
                                    [
                                      -56.07421875,
                                      64.84893726357947
                                    ],
                                    [
                                      112.67578124999999,
                                      64.84893726357947
                                    ],
                                    [
                                      112.67578124999999,
                                      -24.206889622398023
                                    ],
                                    [
                                      -56.07421875,
                                      -24.206889622398023
                                    ]
                                  ]
                                }
                              }
                            }
                          ]
                        }
                      ]
                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  "size": 0
}
```

This query matches 407 of our 500k documents however with the geo polygon block this query goes from taking approx 10 milliseconds to anywhere between 1.5 and 4.5 seconds. Even more weird is that when I set:

```
profile: true
```

on the query it completes in about 500ms. 

My questions therefore are:
1) Why does adding the geo block above make the query so slow?
2) Why does adding profile: true make it go so much faster?

The mapping of the field is shown below:

```
"location": {
                              "type": "geo_point",
                              "lat_lon": true,
                              "geohash": true,
                              "geohash_precision": 8
                           }
```

If we change to geo_bounding_box the query is fast again but we can't always do this as we need to support arbitrary polygons. Also adding profile:true to this query makes it go about twice as fast again.
</description><key id="148631754">17781</key><summary>Geo polygon searches are slow. But they go about three times faster with profile: true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">motherhubbard</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2016-04-15T11:12:07Z</created><updated>2016-04-19T08:28:14Z</updated><resolved>2016-04-19T08:28:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-18T10:10:19Z" id="211310405">@motherhubbard was this index created on 2.2.0?  I'd try upgrading to 2.3.1 as there was a big speed improvement for multi geo points.

@nknize anything to add here? Also, any reason why the query is faster with profiling than without?
</comment><comment author="motherhubbard" created="2016-04-18T10:48:23Z" id="211324796">@clintongormley Thanks for getting back to me. This index was indeed created on 2.2.0

I'll try the same test on 2.3.1 and get back to you.

Cheers
</comment><comment author="motherhubbard" created="2016-04-18T12:14:15Z" id="211353340">The query is fast again on 2.3.1.

I'm happy for this to be closed (unless you are interested in working out the funny with the profiling flag?)
</comment><comment author="clintongormley" created="2016-04-19T08:28:14Z" id="211793003">thanks for letting us know @motherhubbard 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add MatchNoDocsQuery, a query that matches no documents and prints the reason why in the toString method.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17780</link><project id="" key="" /><description /><key id="148627391">17780</key><summary>Add MatchNoDocsQuery, a query that matches no documents and prints the reason why in the toString method.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T10:49:44Z</created><updated>2016-04-26T18:20:52Z</updated><resolved>2016-04-26T10:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-04-15T10:50:37Z" id="210413183">@colings86 this is the query we talked about. It can be useful for the validate API because the query would output something readable for the user (instead of ""). For the search explain I don't think we can do anything here. I've added a Explanation.noMatch but we only print the matching clause in the explain message.
</comment><comment author="colings86" created="2016-04-15T12:09:40Z" id="210440168">Maybe (probably not in this PR though) we should add a `noHitsExecute()` method to `FetchSubPhase` which we could then use in the `ExplainFetchSubPhase` to populate the `Explanation` in `SearchContext` with the `Explanation.noMatch()` information?
</comment><comment author="jimczi" created="2016-04-15T12:12:58Z" id="210441407">Good idea. I'll try to do that in a followup.
</comment><comment author="colings86" created="2016-04-15T12:16:12Z" id="210442532">Actually I'm not sure that would work as explain needs a docId to execute on. At least even with this fix as is, using the Explain API with an indexed document (https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-explain.html) should explain why it didn't match?
</comment><comment author="jimczi" created="2016-04-15T12:43:22Z" id="210449390">Yes, it will print the reason why the clause did not match.
</comment><comment author="jpountz" created="2016-04-20T07:30:13Z" id="212298580">LGTM
</comment><comment author="cbuescher" created="2016-04-26T10:25:13Z" id="214695529">As a follow up, we should probably use this new query in the MatchNoneQueryBuilder, which currently just produces an empty BooleanQuery. I'll open an issue to track this.
</comment><comment author="jimczi" created="2016-04-26T10:42:36Z" id="214698929">Thanks @cbuescher.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for documented byte/size units and for micros as a time unit in _cat API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17779</link><project id="" key="" /><description>We advertise in our documentation that byte units are like `kb`, `mb`... But we actually only support the simple notation `k` or `m`.
This commit adds support for the documented form and keeps the non documented options to avoid any breaking change.

It also adds support for `micros` as a time unit in `_cat` API.

Documentation updated accordingly.
</description><key id="148622530">17779</key><summary>Add support for documented byte/size units and for micros as a time unit in _cat API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-15T10:26:32Z</created><updated>2016-04-29T16:19:38Z</updated><resolved>2016-04-29T13:14:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-15T13:30:50Z" id="210464719">I updated the PR with a new commit.
- Remove the support for `b` as a SizeValue unit. Actually, for numbers, when using raw numbers without unit, there is no text to add/parse after the number. For example, you don't write `10` as `10b`.
- Size values don't have `b` suffix. So We revert the previous change in `renderValue()` when it comes to `SizeValue` and we support option like `size=` which means that we want to display raw data without unit (singles).
- Add support for `nanos` and `d` for `TimeValue`
- Update the documentation accordingly
</comment><comment author="dadoonet" created="2016-04-15T17:04:02Z" id="210548449">@nik9000 @ywelsch I added 2 new commits.

One for doc
The other one consist of adding a new test (and fix missing options in another one). Note that we are probably missing those global options in every CAT API REST description file.
</comment><comment author="dadoonet" created="2016-04-15T18:16:16Z" id="210572508">Thanks guys. Updated.
</comment><comment author="nik9000" created="2016-04-15T18:27:41Z" id="210577372">LGTM!
</comment><comment author="dadoonet" created="2016-04-16T06:46:13Z" id="210748527">Interesting. So I launched the full tests with `gradle check` and apparently it now fails in QA with a lot of timeout issues...

```
Suite: org.elasticsearch.backwards.MultiNodeBackwardsIT
  2&gt; REPRODUCE WITH: gradle :qa:backwards-5.0:integTest -Dtests.seed=CFB477C2B2E2A7B7 -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=indices.stats/14_groups/Groups - search metric}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=nl -Dtests.timezone=Europe/Belgrade
FAILURE 60.7s | MultiNodeBackwardsIT.test {p0=indices.stats/14_groups/Groups - search metric} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [index] returned [503 Service Unavailable] [{"error":{"root_cause":[{"type":"unavailable_shards_exception","reason":"[test1][3] primary shard is not active Timeout: [1m], request: [index {[test1][bar][1], source[{\"bar\":\"bar\",\"baz\":\"baz\"}]}]"}],"type":"unavailable_shards_exception","reason":"[test1][3] primary shard is not active Timeout: [1m], request: [index {[test1][bar][1], source[{\"bar\":\"bar\",\"baz\":\"baz\"}]}]"},"status":503}]
   &gt;    at __randomizedtesting.SeedInfo.seed([CFB477C2B2E2A7B7:47E048181C1ECA4F]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:107)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:387)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: gradle :qa:backwards-5.0:integTest -Dtests.seed=CFB477C2B2E2A7B7 -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=indices.open/20_multiple_indices/All indices}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=nl -Dtests.timezone=Europe/Belgrade
FAILURE 30.1s | MultiNodeBackwardsIT.test {p0=indices.open/20_multiple_indices/All indices} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [cluster.health] returned [408 Request Timeout] [{"cluster_name":"qa_backwards-5.0_integTest","status":"red","timed_out":true,"number_of_nodes":2,"number_of_data_nodes":2,"active_primary_shards":0,"active_shards":0,"relocating_shards":0,"initializing_shards":0,"unassigned_shards":15,"delayed_unassigned_shards":0,"number_of_pending_tasks":0,"number_of_in_flight_fetch":0,"task_max_waiting_in_queue_millis":0,"active_shards_percent_as_number":0.0}]
   &gt;    at __randomizedtesting.SeedInfo.seed([CFB477C2B2E2A7B7:47E048181C1ECA4F]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:107)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:387)
   &gt;    at java.lang.Thread.run(Thread.java:745)
...
```

Does this ring a bell to anyone?

I can reproduce it with: 

``` sh
gradle :qa:backwards-5.0:integTest -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=indices.stats/14_groups/Groups - search metric}" -Des.logger.level=DEBUG
```

Here is what I can see in DEBUG:

```
Suite: org.elasticsearch.backwards.MultiNodeBackwardsIT
  1&gt; [2016-04-16 08:49:58,473][INFO ][org.elasticsearch.test.rest.client] REST client initialized [http://[::1]:64492], elasticsearch version: [5.0.0]
  1&gt; [2016-04-16 08:49:58,474][DEBUG][org.elasticsearch.test.rest] resetting client, response and stash
  1&gt; [2016-04-16 08:49:58,480][INFO ][org.elasticsearch.test.rest.client] REST client initialized [http://[::1]:64492], elasticsearch version: [5.0.0]
  1&gt; [2016-04-16 08:49:58,481][DEBUG][org.elasticsearch.test.rest] resetting client, response and stash
  1&gt; [2016-04-16 08:49:58,481][INFO ][org.elasticsearch.backwards] start setup test [indices.stats/14_groups/Groups - search metric]
  1&gt; [2016-04-16 08:49:58,549][DEBUG][org.elasticsearch.test.rest.client] calling api [index]
  1&gt; [2016-04-16 08:50:59,494][DEBUG][org.elasticsearch.test.rest.client] calling api [indices.delete]
  1&gt; [2016-04-16 08:50:59,518][DEBUG][org.elasticsearch.test.rest.client] calling api [indices.delete_template]
  1&gt; [2016-04-16 08:50:59,520][DEBUG][org.elasticsearch.test.rest.client] calling api [snapshot.delete_repository]
  1&gt; [2016-04-16 08:50:59,523][DEBUG][org.elasticsearch.test.rest.client] calling api [tasks.list]
  1&gt; [2016-04-16 08:50:59,610][INFO ][org.elasticsearch.backwards] Stash dump on failure [{
  1&gt;   "stash" : { }
  1&gt; }]
```
</comment><comment author="ywelsch" created="2016-04-18T18:16:20Z" id="211509291">@dadoonet I cannot reproduce the issue. Neither `gradle clean check` nor `gradle :qa:backwards-5.0:integTest -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=indices.stats/14_groups/Groups - search metric}" -Des.logger.level=DEBUG` fail for me.
</comment><comment author="dadoonet" created="2016-04-18T18:21:00Z" id="211512414">Interesting. So I should may be push the changes as is?
</comment><comment author="ywelsch" created="2016-04-18T18:22:48Z" id="211513513">try running it again (remember to do the gradle clean), and provide me the seed of the failing test.
</comment><comment author="dadoonet" created="2016-04-18T20:03:10Z" id="211552411">I tried again tonight with `gradle clean check` from the root directory.

```
Suite: org.elasticsearch.backwards.MultiNodeBackwardsIT
  2&gt; REPRODUCE WITH: gradle :qa:backwards-5.0:integTest -Dtests.seed=6B2AE5C58B795DD0 -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=mget/15_ids/IDs}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=ar-SA -Dtests.timezone=Pacific/Ponape
FAILURE 60.2s | MultiNodeBackwardsIT.test {p0=mget/15_ids/IDs} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [index] returned [503 Service Unavailable] [{"error":{"root_cause":[{"type":"unavailable_shards_exception","reason":"[test_1][3] primary shard is not active Timeout: [1m], request: [index {[test_1][test][1], source[{\"foo\":\"bar\"}]}]"}],"type":"unavailable_shards_exception","reason":"[test_1][3] primary shard is not active Timeout: [1m], request: [index {[test_1][test][1], source[{\"foo\":\"bar\"}]}]"},"status":503}]
   &gt;    at __randomizedtesting.SeedInfo.seed([6B2AE5C58B795DD0:E37EDA1F25853028]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:107)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:395)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Got this report:

```
Completed [1/1] in 2408.32s, 81 tests, 45 failures, 2 errors, 1 skipped &lt;&lt;&lt; FAILURES!

Tests with failures (first 25 out of 47):
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=mget/15_ids/IDs}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.get_field_mapping/50_field_wildcards/Get field mapping should work using '_all' for indices and types}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=update/60_refresh/Refresh}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.get_field_mapping/50_field_wildcards/Get field mapping with t* for fields}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=bulk/20_list_of_strings/List of strings}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=termvectors/40_versions/Versions}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.stats/12_level/Level - shards}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=delete/20_internal_version/Internal version}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=percolate/19_nested/Percolate existing docs}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=bulk/10_basic/Array of objects}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=get_source/10_basic/Basic}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=delete/26_external_gte_version/External GTE version}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.get_field_mapping/10_basic/Get field mapping by index only}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.stats/11_metric/Metric - one}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=mget/70_source_filtering/Source filtering -  true/false}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=update/20_doc_upsert/Doc upsert}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=suggest/20_completion/Multiple Completion fields should work}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.get_field_mapping/50_field_wildcards/Get field mapping should work using '*' for indices and types}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=cat.segments/10_basic/Test cat segments output}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=cluster.state/20_filtering/Filtering the cluster state by indices should work in routing table and metadata}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=index/60_refresh/Refresh}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=cluster.state/20_filtering/Filtering the cluster state using _all for indices and metrics should work}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=suggest/20_completion/Simple suggestion array should work}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=indices.stats/10_index/Index - pattern}
  - org.elasticsearch.backwards.MultiNodeBackwardsIT.test {p0=cluster.state/30_expand_wildcards/Test ignore_unavailable parameter}

Slow Tests Summary:
2408.32s | org.elasticsearch.backwards.MultiNodeBackwardsIT

JUnit4 test failed, ant output was:
   [junit4] &lt;JUnit4&gt; says kaixo! Master seed: 6B2AE5C58B795DD0
   [junit4] JVM J0:     0.27 ..  2411.13 =  2410.86s
   [junit4] Execution time total: 40 minutes 11 seconds
   [junit4] Tests summary: 1 suite, 81 tests, 1 suite-level error, 1 error, 45 failures, 1 ignored (1 assumption)
```
</comment><comment author="ywelsch" created="2016-04-19T09:00:43Z" id="211812281">I cannot reproduce this. @nik9000 what about you?
</comment><comment author="dadoonet" created="2016-04-29T12:54:28Z" id="215703097">@ywelsch So if this works for do you think I can merge it?
</comment><comment author="ywelsch" created="2016-04-29T12:57:49Z" id="215703776">@dadoonet yes, go ahead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to registered Settings for all IndexingMemoryController settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17778</link><project id="" key="" /><description>I just fixed IMC to create `Settting` objects for all the settings it uses ... but I was unsure of some things, and I'm a bit nervous because I'm not familiar enough with the new settings design:

I'm passing `Property.NodeScope` for its settings ... is that correct?

I also added these settings onto the long list in `ClusterSettings.BUILD_IN_CLUSTER_SETTINGS` so they are registered ... is that ok?

Closes #17442 
</description><key id="148620032">17778</key><summary>Switch to registered Settings for all IndexingMemoryController settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T10:12:56Z</created><updated>2016-04-18T10:23:41Z</updated><resolved>2016-04-15T17:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-15T11:17:43Z" id="210421353">&gt; I'm passing Property.NodeScope for its settings ... is that correct?

Yes.

&gt; I also added these settings onto the long list in ClusterSettings.BUILD_IN_CLUSTER_SETTINGS so they are registered ... is that ok?

Yes.

Left some comments, otherwise looks great.
</comment><comment author="mikemccand" created="2016-04-15T13:44:40Z" id="210469368">Thanks for the review @jasontedor ... I folded in your feedback ... I think it's ready!
</comment><comment author="jasontedor" created="2016-04-15T14:40:37Z" id="210487378">LGTM.
</comment><comment author="mikemccand" created="2016-04-15T17:14:21Z" id="210551229">Thanks @jasontedor.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back range support to `ip` fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17777</link><project id="" key="" /><description>`ip` fields currently fail range queries when either bound is inclusive. This
commit makes ranges also work in the exclusive case to be consistent with other
data types.

Relates to #17971
</description><key id="148601485">17777</key><summary>Add back range support to `ip` fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T08:59:40Z</created><updated>2016-06-28T09:28:06Z</updated><resolved>2016-04-22T07:58:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-15T15:47:46Z" id="210515089">LGTM
</comment><comment author="rmuir" created="2016-04-17T11:04:05Z" id="211001082">this looks correct to me (for ipv4 and ipv6). we encode into internal format (which is always 128 bits) and treat it as a 128-bit number, adding or subtracting one to the endpoints. 

maybe it would be good to test out some of the real corner cases (address already all zeros or all 1s, addresses on the ipv4-mapped boundary). Its not clear to me what will happen, i think probably a strange exception from numericutils.
</comment><comment author="jpountz" created="2016-04-18T14:29:30Z" id="211404393">@rjernst @rmuir I pushed more tests
</comment><comment author="rmuir" created="2016-04-18T15:10:27Z" id="211422232">Thanks for the endpoints test. I'm not sure if its worth trying to improve the exception to be more understandable by the user (maybe the generic one could be improved, but it would still be limited to generic terms like underflow/overflow). 

As far as the "boundaries" I meant more the cases around `::ffff:0.0.0.0` and `::ffff:255.255.255.255`. I can see from your code that should work fine, because you increment/decrement in 128-bit space. But bugs around this area (where addresses change from 4 bytes to 16 bytes) seem likely if the code is refactored.
</comment><comment author="rmuir" created="2016-04-18T15:14:26Z" id="211424780">Also, I strongly dislike booleans and nulls in the lucene api, but if we are going to support this thing, maybe we should consider methods in InetAddressPoint.java like:

```
public static InetAddress nextUp(InetAddress address);
public static InetAddress nextDown(InetAddress address);
```

These could have better error messages for overflow and anyone wanting to do this can do it in a less fragile way.
</comment><comment author="rmuir" created="2016-04-18T15:20:49Z" id="211428019">Also, it is worth thinking about consistency for these ranges across all types as much as possible. Do integer/long fields throw exceptions in these cases or will Integer.MAX_VALUE exclusive still return Integer.MAX_VALUE? 

For double/float fields the behavior may really need to be different, depends if you think -Infinity exclusive will still include -Infinity and so on.
</comment><comment author="jpountz" created="2016-04-20T07:35:04Z" id="212299929">&gt; Do integer/long fields throw exceptions in these cases or will Integer.MAX_VALUE exclusive still return Integer.MAX_VALUE?

They return a MatchNoDocsQuery in that case since there is no greator integer than MAX_VALUE. We could do the same for ip addresses.

&gt; For double/float fields the behavior may really need to be different, depends if you think -Infinity exclusive will still include -Infinity and so on.

Right, this is what is happening right now. If you pass POSITIVE_INFINITY as a lower bound and it is exclusive, then we just call Math.nextUp which returns POSITIVE_INFINITY. So POSITIVE_INFINITY will be included in practice.
</comment><comment author="jpountz" created="2016-04-20T08:57:22Z" id="212337480">&gt; maybe we should consider methods in InetAddressPoint.java

I opened https://issues.apache.org/jira/browse/LUCENE-7234.
</comment><comment author="rmuir" created="2016-04-20T14:03:05Z" id="212439071">&gt; Right, this is what is happening right now. If you pass POSITIVE_INFINITY as a lower bound and it is exclusive, then we just call Math.nextUp which returns POSITIVE_INFINITY. So POSITIVE_INFINITY will be included in practice.

Technically i think NaN is better here (it sorts after POSITIVE_INFINITY). In all cases I don't know what to suggest: this is why i really think having an api taking booleans and nulls is unintuitive and too hard to reason about. 

All of these problems are artificially created by a bad query DSL.
</comment><comment author="jpountz" created="2016-04-20T15:04:28Z" id="212466506">I pushed one more commit. I had to fork some Lucene code in order to be able to leverage the new nextUp/nextDown functions and have the fix for `newPrefixQuery`. Hopefully Lucene 6.1 will be out before elasticsearch 5.0 and we will be able to remove this X class before the GA.

The range method now behaves consistently with integers: if the lower (resp. upper) bound is the maximum (resp. minimum) value and is not inclusive, then a MatchNoDocsQuery is returned.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apply the default operator on analyzed wildcard in simple_query_string builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17776</link><project id="" key="" /><description>This is a followup from https://github.com/elastic/elasticsearch/pull/17711 where we now apply the default operator on analyzed wildcard query in query_string builder.
</description><key id="148598681">17776</key><summary>Apply the default operator on analyzed wildcard in simple_query_string builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T08:46:17Z</created><updated>2016-04-18T10:29:18Z</updated><resolved>2016-04-15T20:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-15T14:37:13Z" id="210486219">I left one comment about testing. Otherwise it looks great.
</comment><comment author="jimczi" created="2016-04-15T19:43:37Z" id="210612461">@jpountz thanks for the review. I've added tests with synonyms. 
@cbuescher I tried to add the ability to register custom analyzer in AbstractQueryTestCase but I could not find an easy way. Instead I've added a standalone test for SimpleQueryParser where I force the analyzer for the query. 
</comment><comment author="jpountz" created="2016-04-15T19:49:41Z" id="210615744">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add points to SegmentStats.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17775</link><project id="" key="" /><description>This way points memory and disk usage will be reported in the stats API.

Closes #16974
</description><key id="148593296">17775</key><summary>Add points to SegmentStats.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T08:23:16Z</created><updated>2016-04-15T13:59:14Z</updated><resolved>2016-04-15T13:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-04-15T09:55:34Z" id="210396277">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove 'case' parameter from rest apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17774</link><project id="" key="" /><description>The current api allows for choosing which "case" response json keys are
written in. This has the options of camelCase or underscore. camelCase
is going to be deprecated from the query apis. However, with the case
api, it is not necessary to deprecate, as users who were using it in 2.x
can transition completely on 2.x before upgrading by simply using
the underscore option.

This change removes the 'case' option from rest apis.

see #8988
</description><key id="148563575">17774</key><summary>Remove 'case' parameter from rest apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:REST</label><label>breaking</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T06:09:56Z</created><updated>2016-04-19T09:13:57Z</updated><resolved>2016-04-18T21:27:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-15T07:07:07Z" id="210318957">&gt; users who were using it in 2.x can transition completely on 2.x before upgrading by simply using the underscore option

I think that in such cases, we would still like to have deprecation logging so that users have a chance to realize they are doing something that will go away before they upgrade. That said, this particular feature is a bit esoteric to me and I know that we do not use it for every field. So even if you request camelcase rendering, you will get some keys back that are in underscore. Moreover, the docs use underscore case all over the place. So I'm +1 to remove it now.
</comment><comment author="rjernst" created="2016-04-18T21:27:03Z" id="211589855">I added deprecation logging in 2.x in #17800, so I will merge this now for master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove validation errors from cluster health response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17773</link><project id="" key="" /><description>Cluster health responses have not shown validation errors, which are
retrieved from RoutingTable validations, in any production or testing
instances.  The code is unit tested well in this area and any issues are
exposed through the testing infrastructure, so this commit removes
reporting of validation errors in the cluster health response.

Closes #16979
</description><key id="148548773">17773</key><summary>Remove validation errors from cluster health response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>breaking</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-15T04:00:37Z</created><updated>2016-04-20T13:37:42Z</updated><resolved>2016-04-20T13:37:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-15T04:03:56Z" id="210275906">@bleskes FYI, or anyone else who can review it
</comment><comment author="bleskes" created="2016-04-15T06:42:02Z" id="210306964">@abeyad thx. changes so far look good to me. Can we make all the validate methods on the RoutingTables and friends package private and call them as an assertion on the relevant builders? 
</comment><comment author="abeyad" created="2016-04-15T13:45:14Z" id="210469519">@bleskes Not sure how we would call the validations in the relevant builders, unless we start passing in a `MetaData` or `IndexMetaData` object to the `build()` method to compare/validate against.  My understanding is that these validation methods are meant to ensure consistency with some external data structure (in this case, the metadatas), and we don't have those when `build()` is called internally.
</comment><comment author="bleskes" created="2016-04-15T15:33:41Z" id="210508818">&gt; and we don't have those when build() is called internally.

valid point. indeed we can't have it there. We can still change them to be package private and use them as an assertion from `AllocationService#buildChangedResult(MetaData, RoutingTable, RoutingNodes, RoutingExplanations)`

which is in the only place we use them (after this PR). We can also make them throw exceptions directly and not have to accumulate validations (but that will require more work in changing the tests. My goal here is to make them as simple as possible.
</comment><comment author="abeyad" created="2016-04-15T16:23:10Z" id="210528772">@bleskes `AllocationService#buildChangedResult` already calls the version which gets the validations and throws an exception if there are any by calling `RoutingTable#validateRaiseException`.  Is this what you meant?
</comment><comment author="abeyad" created="2016-04-15T16:26:46Z" id="210530067">Also, I've made the validation methods package private (and private if they're only used internally)
</comment><comment author="bleskes" created="2016-04-15T16:26:58Z" id="210530127">yes, this is what I meant - let's make this an assertion and only keep this path (remove the non throwing variant)
</comment><comment author="abeyad" created="2016-04-15T16:38:03Z" id="210537107">If we change:  

```
if (!validation.valid()) {
      throw new RoutingValidationException(validation);
}
```

to an `assert`, I think its better to keep the private non-throwing `validate()` in a separate method instead of unfolding it into the throwing method, because then we can change the throwing one to just:

```
public RoutingTable validateRaiseException(MetaData metaData) {
       assert validate(metaData).valid();
       return this;
}
```

what do you think?
</comment><comment author="abeyad" created="2016-04-15T16:40:24Z" id="210539127">Also, if we do that, I believe we can get rid of the `RoutingValidationException` entirely too
</comment><comment author="abeyad" created="2016-04-15T17:07:52Z" id="210549392">@bleskes pushed this commit which does exactly that: https://github.com/elastic/elasticsearch/pull/17773/commits/5e20416c24b4ff311e6f063c1fefaed737668ad3
</comment><comment author="bleskes" created="2016-04-19T19:58:34Z" id="212101841">thx @abeyad . Looking good - I made some minor suggestions to simplify things even further
</comment><comment author="bleskes" created="2016-04-19T20:01:38Z" id="212102865">thx @abeyad . Looking good - I made some minor suggestions to simplify things even further. Love the stats on this PR :)
</comment><comment author="abeyad" created="2016-04-20T00:02:11Z" id="212178661">@bleskes Just pushed a commit that removes what you suggested.  I have the `validate` method throwing `IllegalStateException` now, which I think is appropriate because there are other places in `AllocationService.buildChangedResult` that can throw `IllegalStateException`, so its consistent.  Another option is to just have `validate` return `true` or `false` instead.

Indeed the PR stats are looking even more beautiful now :)
</comment><comment author="bleskes" created="2016-04-20T08:44:49Z" id="212332816">LGTM. Left some minor nits. Feel free to push when ready.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/plugins throws NullPointerException on Tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17772</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: 1.8.0_31-b13

**OS version**: CentOS 7

**Description of the problem including expected versus actual behavior**:
We have `license` and `marvel` plugins installed (version 2.3.1) - we can see the plugins fine, the cat api works. Once the tribe connects to a cluster containing just a `kopf` plugin it starts throwing NPE. `_nodes` still contains correct info about the plugins and when we do `_cat/plugins` on the cluster the tribes connect to we see its plugins ans well as the plugins on the tribe node.

**Provide logs (if relevant)**:

```
[2016-04-15 01:23:02,525][WARN ][rest.suppressed          ] /_cat/plugins Params: {}
java.lang.NullPointerException
        at org.elasticsearch.rest.action.cat.RestPluginsAction.buildTable(RestPluginsAction.java:98)
        at org.elasticsearch.rest.action.cat.RestPluginsAction.access$000(RestPluginsAction.java:41)
        at org.elasticsearch.rest.action.cat.RestPluginsAction$1$1.buildResponse(RestPluginsAction.java:69)
        at org.elasticsearch.rest.action.cat.RestPluginsAction$1$1.buildResponse(RestPluginsAction.java:66)
        at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
        at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
        at org.elasticsearch.action.support.ThreadedActionListener$1.doRun(ThreadedActionListener.java:89)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="148531065">17772</key><summary>_cat/plugins throws NullPointerException on Tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:CAT API</label><label>:Tribe Node</label><label>adoptme</label><label>bug</label></labels><created>2016-04-15T01:49:27Z</created><updated>2016-04-15T08:10:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>MultiSearch floods ES console with EsRejectedExecutionException stack traces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17771</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.2 and 2.3.1

**JVM version**: 1.8.0_51 64-bit

**OS version**: Windows 10 64-bit

**Description of the problem including expected versus actual behavior**:

I am trying use the multisearch API to run a large number of independent searches in a batch. I expect all the searches to run on the localhost cluster and to return, as per the API description. Instead I get heaps of EsRejectedExecutionExceptions in my ES console window.

_I would expect that the multisearch API could manage its queue of requests so as to not flood Elasticsearch. The main reason I am trying to use this is to reduce the round-trip latency of each network request-response cycle. For even 1000 queries that adds up to a lot of waiting just for each transport back and forth._

**Steps to reproduce**:

My code (using the Java Transport client) just queries for the counts of documents containing 1000 different terms and returns those terms present in at least 5 docs each:

```
    List&lt;String&gt; terms = new ArrayList&lt;&gt;(1000);
    Random random = new Random();
    for (int i = 0; i &lt; 1000; i++)
    {
        terms.add(Integer.toString(random.nextInt()));
    }

    MultiSearchRequestBuilder requestBuilder = getEsClient().prepareMultiSearch();
    terms.forEach(term -&gt; {
        SearchRequestBuilder query = getEsClient().prepareSearch()
                                                  .setSize(0)
                                                  .setNoFields()
                                                  .setFetchSource(false)
                                                  .setTypes(ElasticSearchSchema.DOCS_TYPE)
                                                  .setQuery(termQuery(LuceneFields.CONTENT.getName(), term));
        requestBuilder.add(query);
    });

    MultiSearchResponse.Item[] responses = requestBuilder.get()
                                                         .getResponses();

    Set&lt;String&gt; result = new HashSet&lt;&gt;(1000);
    List&lt;Integer&gt; failedResponses = new ArrayList&lt;&gt;();
    for (int i = 0; i &lt; responses.length; i++)
    {
        MultiSearchResponse.Item response = responses[i];
        try
        {
            if (response.getResponse().getHits().getTotalHits() &gt;= 5)
            {
                result.add(terms.get(i));
            }
        }
        catch (NullPointerException e)
        {
            failedResponses.add(i);
        }
    }
    System.out.println("" + failedResponses.size() + " responses failed: " + Joiner.on(", ").join(failedResponses));
```

The request returns eventually but the logs are spammed with hundreds of failure messages (see below). Occasionally one of the queries will return a failure response like this that matches the ES cluster console output but the number of failures is tiny compared to the log spam:

```
EsRejectedExecutionException[rejected execution of org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2@5d935160 on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@25ba6eb[Running, pool size = 37, active threads = 37, queued tasks = 1000, completed tasks = 312998]]] 
```

After a typical run with hundreds of messages on the log only one query returned a failed result in the end:

```
1 responses failed: 847
```

**Provide logs (if relevant)**:

... but the ES console contains lots of entries like this, even if all the queries eventually succeeded:

```
[2016-04-15 11:00:29,114][DEBUG][action.search.type       ] [SYD-NAGAPPR-WS] [nuix-334f4c30941040ee894bde64ef3cc932][4], node[i_7E_ZZ8TYGZ9jXfnhaeig], [P], v[14], s[STARTED], a[id=_7pFXxtcSpir1j5nZAkLmQ]: Failed to execute [org.elasticsearch.action.search.SearchRequest@219198fc] lastShard [true]
RemoteTransportException[[SYD-NAGAPPR-WS][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@5b94e2b8 on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@25ba6eb[Running, pool size = 37, active threads = 37, queued tasks = 996, completed tasks = 129578]]];
Caused by: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@5b94e2b8 on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@25ba6eb[Running, pool size = 37, active threads = 37, queued tasks = 996, completed tasks = 129578]]]
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
    at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:346)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:310)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:282)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:142)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:85)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:166)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:148)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:64)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:53)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:44)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
    at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:63)
    at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:39)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
    at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:45)
    at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:41)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="148530122">17771</key><summary>MultiSearch floods ES console with EsRejectedExecutionException stack traces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnagappan</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-04-15T01:39:48Z</created><updated>2016-04-22T10:28:02Z</updated><resolved>2016-04-22T10:28:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-15T08:10:15Z" id="210347949">&gt; Occasionally one of the queries will return a failure response like this that matches the ES cluster console output but the number of failures is tiny compared to the log spam:

Did you check the shard failures in each search request? I bet there are lots of shard failures (which are responsible for the stack traces) and you're getting incomplete results.
</comment><comment author="rnagappan" created="2016-04-18T04:00:42Z" id="211184707">Yes, there are many shard failures for each request, so many matching failure messages like this below. The question remains though, why can't MultiSearch manage its own request queue so it does not flood the cluster like this?

```
shard [[UVvK8FU6TmGS6LDnVczdaA][nuix-334f4c30941040ee894bde64ef3cc932][3]], reason [RemoteTransportException[[SYD-NAGAPPR-WS][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@a81db35 on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@7bb429e0[Running, pool size = 37, active threads = 37, queued tasks = 990, completed tasks = 120883]]]; ], cause [EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@a81db35 on EsThreadPoolExecutor[search, queue capacity = 1000, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@7bb429e0[Running, pool size = 37, active threads = 37, queued tasks = 990, completed tasks = 120883]]]
at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
at org.elasticsearch.transport.TransportService.sendLocalRequest(TransportService.java:346)
at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:310)
at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:282)
at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:142)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:85)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:166)
at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.start(TransportSearchTypeAction.java:148)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:64)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:53)
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99)
at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:44)
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:63)
at org.elasticsearch.action.search.TransportMultiSearchAction.doExecute(TransportMultiSearchAction.java:39)
at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:45)
at org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:41)
at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:244)
at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:310)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="martijnvg" created="2016-04-22T10:28:01Z" id="213371050">Closing this issue in favour of #17926. 

@rnagappan There is room for improvement for the multi search api. Unfortunately today there isn't much that can be done to avoid rejected exceptions in your case other then lowering the amount of search requests in the multi search api or adding more nodes to your cluster.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Properly mark reindex's child tasks as child tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17770</link><project id="" key="" /><description>They weren't being marked before.
</description><key id="148499716">17770</key><summary>Properly mark reindex's child tasks as child tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T22:23:52Z</created><updated>2016-04-22T18:09:58Z</updated><resolved>2016-04-22T18:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T22:26:25Z" id="210184160">@imotov I'm wondering why all ActionRequests don't support setParentTaskId. It is kind of hard to know up front if your task is going to be part of a request.

If we did that we'd probably want to make the serialization for empty task ids smaller.
</comment><comment author="nik9000" created="2016-04-20T15:07:39Z" id="212468176">I'm going to rework this after #17872.
</comment><comment author="nik9000" created="2016-04-22T17:33:21Z" id="213521965">&gt; I'm going to rework this after #17872.

Done. Running tests against it now but all is looking good.
</comment><comment author="imotov" created="2016-04-22T17:55:44Z" id="213529603">LGTM
</comment><comment author="nik9000" created="2016-04-22T18:09:58Z" id="213536056">Thanks for reviewing @imotov !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Exclude gradle and eclipse build dirs from intellij</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17769</link><project id="" key="" /><description>Intellij has a model of "everything is a source dir unless you say
otherwise". This change fixes the intellij configuration to not think
the gradle or eclipse build dirs are source dirs.
</description><key id="148493728">17769</key><summary>Build: Exclude gradle and eclipse build dirs from intellij</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T21:56:41Z</created><updated>2016-04-14T22:16:06Z</updated><resolved>2016-04-14T22:16:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-14T21:59:48Z" id="210172450">LGTM
</comment><comment author="jasontedor" created="2016-04-14T22:03:01Z" id="210173929">I tested this locally and it worked great for me. Thanks for doing this. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix array parsing to remove its context when finished parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17768</link><project id="" key="" /><description>Handling of the current path when parsing a document is very sensitive.
This fixes a subtle bug in array parsing, where the path that was added
by parsing an array would not be cleared. It also adds a hard state
check at the end of parsing to ensure we ended with a clean path.
</description><key id="148491756">17768</key><summary>Fix array parsing to remove its context when finished parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T21:48:02Z</created><updated>2016-05-02T12:07:41Z</updated><resolved>2016-04-15T07:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-15T07:17:46Z" id="210325148">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Race conditions in Index Deletion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17767</link><project id="" key="" /><description>Some race conditions were found in shadow replica deletion, see #17695.  However, the following tests produce sporadic failures that indicate there are other race conditions when trying to delete the index contents of a deleted index (regular indices and shadow replicas).  
1. `IndexServiceTests#testBaseAsyncTask` (http://build-us-00.elastic.co/job/es_g1gc_master_metal/35910/testReport/junit/org.elasticsearch.index/IndexServiceTests/testBaseAsyncTask/)
2. `IndexShardTests#testSearcherWrapperWorksWithGlobaOrdinals` (https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=ubuntu/239/console)
3. `IndexServiceTests#testRefreshTaskIsUpdated` (https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+g1gc/20/console)

The stack traces look like:

```
java.io.IOException: Could not remove the following files (in the order of attempts):
   /home/jenkins/workspace/es_g1gc_master_metal/core/build/testrun/test/J1/temp/org.elasticsearch.index.IndexServiceTests_56B8EE9D1A7C1203-001/tempDir-001/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[6042546882828568297]-HASH=[15836EE647DB17]/nodes/0/indices/FRdrjAUiSb29Q33U4vlkXA/_state/state-1.st: java.io.IOException: access denied: /home/jenkins/workspace/es_g1gc_master_metal/core/build/testrun/test/J1/temp/org.elasticsearch.index.IndexServiceTests_56B8EE9D1A7C1203-001/tempDir-001/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[6042546882828568297]-HASH=[15836EE647DB17]/nodes/0/indices/FRdrjAUiSb29Q33U4vlkXA/_state/state-1.st
   /home/jenkins/workspace/es_g1gc_master_metal/core/build/testrun/test/J1/temp/org.elasticsearch.index.IndexServiceTests_56B8EE9D1A7C1203-001/tempDir-001/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[6042546882828568297]-HASH=[15836EE647DB17]/nodes/0/indices/FRdrjAUiSb29Q33U4vlkXA/_state: java.nio.file.DirectoryNotEmptyException: /home/jenkins/workspace/es_g1gc_master_metal/core/build/testrun/test/J1/temp/org.elasticsearch.index.IndexServiceTests_56B8EE9D1A7C1203-001/tempDir-001/data/single-node-cluster-CHILD_VM=[1]-CLUSTER_SEED=[6042546882828568297]-HASH=[15836EE647DB17]/nodes/0/indices/FRdrjAUiSb29Q33U4vlkXA/_state

    at org.apache.lucene.util.IOUtils.rm(IOUtils.java:323)
    at org.elasticsearch.gateway.MetaDataStateFormat.deleteMetaState(MetaDataStateFormat.java:378)
    at org.elasticsearch.indices.IndicesService.deleteIndexStore(IndicesService.java:579)
    at org.elasticsearch.indices.IndicesService.deleteIndexStore(IndicesService.java:554)
    at org.elasticsearch.indices.IndicesService.deleteClosedIndex(IndicesService.java:526)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedIndices(IndicesClusterStateService.java:239)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:182)
    at org.elasticsearch.cluster.service.ClusterService.runTasksForExecutor(ClusterService.java:652)
    at org.elasticsearch.cluster.service.ClusterService$UpdateTask.run(ClusterService.java:814)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:408)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:237)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:200)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

It is critical that the `_state` directory in the index dir gets deleted, so the index does not get re-imported as a dangling index.  

It is worth also keeping an eye out for these tests and seeing if #17265 solves the problem.  If the problem persists even after index tombstones are in, then an AwaitsFix should be added to the tests until the problem is resolved.
</description><key id="148488272">17767</key><summary>Race conditions in Index Deletion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>blocker</label><label>bug</label></labels><created>2016-04-14T21:34:59Z</created><updated>2016-09-13T11:12:33Z</updated><resolved>2016-09-13T11:12:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-05-12T10:02:22Z" id="218713585">@abeyad was there any discussion on this? re reading the ticket I'm not 100% clear on what the issue is.
</comment><comment author="abeyad" created="2016-05-12T13:18:54Z" id="218754129">@bleskes During test triage, I found that some of the tests sporadically failed throwing exceptions like the one shown above, indicating a delete was not completely successful in that files remained on the file system after the delete operation.  This was more to keep track of this issue and see if it continues to come up.  It may not be race conditions that are the problem, though that was my initial guess.  It would also be interesting to see if tombstones fixes this problem.  None of these tests have failed since I first opened this issue nearly a month ago.
</comment><comment author="clintongormley" created="2016-09-09T09:12:12Z" id="245861438">@abeyad any further news on this?
</comment><comment author="abeyad" created="2016-09-09T20:01:49Z" id="246023663">@clintongormley None of these tests have failed since this issue was reported on April 14.  I propose we close this unless something else manifests.  
</comment><comment author="clintongormley" created="2016-09-13T11:12:28Z" id="246649294">sounds good, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoBoundingBoxQueryBuilderTests#testExceptionOnMissingTypes CI test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17766</link><project id="" key="" /><description>Original failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=debian/258/console

```
Suite: org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=7236AD4BA5852E8 -Dtests.class=org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests -Dtests.method="testExceptionOnMissingTypes" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=fr-BE -Dtests.timezone=Asia/Hebron
FAILURE 0.10s J0 | GeoBoundingBoxQueryBuilderTests.testExceptionOnMissingTypes &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: Found no indexed geo query.
   &gt;    at __randomizedtesting.SeedInfo.seed([7236AD4BA5852E8:C0398645FCFA1C3D]:0)
   &gt;    at org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests.doAssertLuceneQuery(GeoBoundingBoxQueryBuilderTests.java:284)
   &gt;    at org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests.doAssertLuceneQuery(GeoBoundingBoxQueryBuilderTests.java:47)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.assertLuceneQuery(AbstractQueryTestCase.java:622)
   &gt;    at org.elasticsearch.index.query.AbstractQueryTestCase.testToQuery(AbstractQueryTestCase.java:547)
   &gt;    at org.elasticsearch.index.query.GeoBoundingBoxQueryBuilderTests.testExceptionOnMissingTypes(GeoBoundingBoxQueryBuilderTests.java:132)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Reproducibles on my machine. Most likely related to #17751.
</description><key id="148475334">17766</key><summary>GeoBoundingBoxQueryBuilderTests#testExceptionOnMissingTypes CI test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Query DSL</label><label>build</label><label>jenkins</label><label>test</label></labels><created>2016-04-14T20:37:36Z</created><updated>2016-04-15T09:11:30Z</updated><resolved>2016-04-15T09:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T09:11:14Z" id="210375572">Fixed by https://github.com/elastic/elasticsearch/commit/d863cbaa07d30c4e66f8ffe84b652ae7494d0851
</comment><comment author="colings86" created="2016-04-15T09:11:30Z" id="210375632">Thanks for raising this @imotov &#128516; 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut nested and reverse_nested aggregations to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17765</link><project id="" key="" /><description>and remove their PROTOTYPES.

Relates to #17085
</description><key id="148473290">17765</key><summary>Cut nested and reverse_nested aggregations to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T20:28:03Z</created><updated>2016-05-02T12:13:26Z</updated><resolved>2016-04-15T14:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T08:49:37Z" id="210368304">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Remove threadlocal from document parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17764</link><project id="" key="" /><description>The doc parser uses a context object to store the state of parsing,
namely the existing mappings, new mappings, and the parsed document.
Currently this uses a threadlocal which is "reset" for each doc parsed.
However, the thread local doesn't actually save anything, since
resetting is constructing new objects. This change removes the thread
local, which also simplifies the mapper service as it now does not need
to be closeable.

@mikemccand helped me with running a benchmark and there is no perf diff (this branch was 1% faster than master, just noise).
</description><key id="148469551">17764</key><summary>Internal: Remove threadlocal from document parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T20:11:58Z</created><updated>2016-04-14T22:17:45Z</updated><resolved>2016-04-14T22:17:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T20:29:44Z" id="210135254">Makes sense to me.
</comment><comment author="nik9000" created="2016-04-14T20:31:10Z" id="210135654">I removed `:Mapping` because the release scripts wants there to be one of those kind of tags (I think). I added enhancement because the release script wants there to be one of those kind of tags too (enhancement, bug, build, etc).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut the sampler and diversified_sampler aggregations to registerAggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17763</link><project id="" key="" /><description>and remove their PROTOTYPEs.

Relates to #17085
</description><key id="148468554">17763</key><summary>Cut the sampler and diversified_sampler aggregations to registerAggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T20:08:03Z</created><updated>2016-05-02T12:13:33Z</updated><resolved>2016-04-15T13:54:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T08:48:54Z" id="210368169">LGTM
</comment><comment author="nik9000" created="2016-04-15T13:56:57Z" id="210472766">Thanks for the review again @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut terms aggregation to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17762</link><project id="" key="" /><description>and remove its PROTOTYPE. This is the first aggregation builder that
serializes its targetValueType so ValuesSourceAggregatorBuilder had to
grow support for that.

Relates to #17085
</description><key id="148467884">17762</key><summary>Cut terms aggregation to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T20:05:19Z</created><updated>2016-05-02T12:13:42Z</updated><resolved>2016-04-15T13:13:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T08:47:52Z" id="210367710">left one minor comment but LGTM
</comment><comment author="nik9000" created="2016-04-15T13:13:33Z" id="210460113">Merged! Thanks for the review @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut filters aggregation to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17761</link><project id="" key="" /><description>Cut filters aggregation to registerAggregation and remove its PROTOTYPE.

Relates to #17085
</description><key id="148452619">17761</key><summary>Cut filters aggregation to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T19:00:05Z</created><updated>2016-05-02T12:13:46Z</updated><resolved>2016-04-15T13:05:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T08:44:56Z" id="210365505">LGTM: left one really minor comment but feel free to ignore it if you want
</comment><comment author="nik9000" created="2016-04-15T13:06:08Z" id="210458494">Merged! Thanks @colings86. I pulled the write of size out of the if statement like you asked.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut avg aggregation to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17760</link><project id="" key="" /><description>and remove its PROTOTYPE.

Relates to #17085
</description><key id="148448700">17760</key><summary>Cut avg aggregation to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T18:44:39Z</created><updated>2016-05-02T12:13:52Z</updated><resolved>2016-04-15T12:52:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T08:42:13Z" id="210363641">LGTM
</comment><comment author="nik9000" created="2016-04-15T12:52:52Z" id="210451908">Merged! Thanks for the review @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support dots in field names when mapping already exists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17759</link><project id="" key="" /><description>In 2.0 we began restricting fields to not contains dots in their names.
This change adds back part of dots in fieldnames support. Specifically,
it allows indexing documents that contain dots in the field names, when
the correct corresponding mappers exist. For example, if mappings
contain an object field `foo`, and a subfield `bar`, then indexing a
document with `foo.bar` will work.

see #15951
</description><key id="148446749">17759</key><summary>Support dots in field names when mapping already exists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T18:35:28Z</created><updated>2016-05-26T03:39:08Z</updated><resolved>2016-04-18T21:25:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-18T21:22:27Z" id="211586852">LGTM
</comment><comment author="djschny" created="2016-04-25T16:43:44Z" id="214435851">I noticed this is only tagged for `v5.0.0` however this is still holding people back from upgrading from `1.x` to `2.x` so therefore I would think this would be crucial because `5.x` won't be able to read Lucene 4 index formats which I believe was the newest version of Lucene that 1.x ES supported.

Thoughts or am I missing a way someone can upgrade straight from `1.x` -&gt; `5.x`?
</comment><comment author="clintongormley" created="2016-04-26T13:21:53Z" id="214741592">@djschny it's looking like it will not be possible to backport this feature to 2.x, as it relies on too many mapping changes that have happened in master.

&gt; Thoughts or am I missing a way someone can upgrade straight from 1.x -&gt; 5.x?

We will be building a reindex-from-remote solution which would allow importing indices from a 1.x cluster directly into 5.x
</comment><comment author="djschny" created="2016-04-26T13:27:03Z" id="214744356">&gt; We will be building a reindex-from-remote solution which would allow importing indices from a 1.x cluster directly into 5.x

OK so for folks that are anxiously waiting this change and cannot upgrade to 2.x because of it, our recommendation is for them to jump straight to 5.x and either re-index themselves or wait until a 5.x release has the planned "reindex-from-remote" solution?

I know you did a great job recently updating the migration plugin for 5.x, if installed in a 1.x cluster will it give recommendations on actions that need to be taken for an upgrade to 5.x?

I just want to make sure I advise folks out in the field appropriately since this comes up quite frequently.
</comment><comment author="nellicus" created="2016-05-17T14:14:50Z" id="219730715">+1 
wondering whether we can officially educate/advise around this for users who plan to be on 1.x when 5.0 will go GA thus avoiding the de-dot exercise pain
</comment><comment author="otisg" created="2016-05-26T03:34:38Z" id="221769279">&gt; We will be building a reindex-from-remote solution which would allow importing indices from a 1.x cluster directly into 5.x

@clintongormley Sounds good.  Is there an issue for that to which one could subscribe?
</comment><comment author="djschny" created="2016-05-26T03:39:08Z" id="221769753">@otisg See issue https://github.com/elastic/elasticsearch/issues/17447
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut derivative aggregation to registerPipelineAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17758</link><project id="" key="" /><description>and remove its PROTOTYPE.

Relates to #17085
</description><key id="148442898">17758</key><summary>Cut derivative aggregation to registerPipelineAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T18:19:42Z</created><updated>2016-05-02T12:13:58Z</updated><resolved>2016-04-15T12:36:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T18:19:50Z" id="210085626">@colings86 first pipeline aggregation.
</comment><comment author="colings86" created="2016-04-15T07:59:17Z" id="210345257">LGTM
</comment><comment author="nik9000" created="2016-04-15T12:37:06Z" id="210447978">Merged! Thanks for reviewing @colings86 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut range aggregations to registerAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17757</link><project id="" key="" /><description>and remove their PROTOTYPES. Does not remove the PROTOTYPEs from their
Range implementations which will have to wait for another commit.

Relates to #17085
</description><key id="148434257">17757</key><summary>Cut range aggregations to registerAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T17:42:03Z</created><updated>2016-05-02T12:14:07Z</updated><resolved>2016-04-15T12:25:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-15T07:41:37Z" id="210337043">This LGTM but should the geo_distance range query also be included here?
</comment><comment author="nik9000" created="2016-04-15T12:17:29Z" id="210442884">&gt; This LGTM but should the geo_distance range query also be included here?

It doesn't extend from AbstractRangeBuilder. I only grouped these because it was easy enough just to move all subclasses of that over. I'll get there!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ParseFieldMatcher from AbstractXContentParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17756</link><project id="" key="" /><description>Currently we are able to set a ParseFieldMatcher on XContentParsers, mainly to conveniently carry it around to be available where the
actual parsing happens. This was just recently introduced together with ObjectParser so that ObjectParser can make use of deprecation
logging and throwing errors while parsing.

This however is trappy because we create parsers in so many places in the code and it is easy to forget setting the right ParseFieldMatcher.
Instead we should hold the ParseFieldMatcher only in the parse contexts (e.g. QueryParseContext).

This PR removes the ParseFieldMatcher from XContentParser. ObjectParser can still make use of it because we can make the otherwise unbounded
`context` type to extend an interface that makes sure contexts used in ObjectParser can supply a ParseFieldMatcher. Contexts in ObjectParser
are now no longer optional, but it is sufficient to pass in a small lambda expression in places where no other context is available.

Relates to #17417
</description><key id="148422139">17756</key><summary>Remove ParseFieldMatcher from AbstractXContentParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T16:53:37Z</created><updated>2016-04-15T13:21:27Z</updated><resolved>2016-04-15T13:21:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T17:06:29Z" id="210052172">I genuinely preferred the old way but if it is too easy to break I can live with this.
</comment><comment author="javanna" created="2016-04-15T09:10:40Z" id="210375452">I like the change, it cleans a lot of things up, and actually unveils some more mess that will need to be cleaned up further, but that is good! left a few comments.
</comment><comment author="cbuescher" created="2016-04-15T10:18:11Z" id="210403128">@javanna thanks, I addressed your comments. The renaming of the getter from `parseFieldMatcher` to `getParseFieldMatcher` touches quiet a bit of code though. So I'm not sure if that should be a follow up, wdyt?
</comment><comment author="javanna" created="2016-04-15T10:24:51Z" id="210404984">LGTM, as for the rename, do what you prefer, I have no problem either way
</comment><comment author="cbuescher" created="2016-04-15T11:02:23Z" id="210416134">@javanna thanks, will merge after final test run
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use `&#181;s` as unit for microseconds instead of `micros`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17755</link><project id="" key="" /><description>Use `&#181;s` (or `mu`) as unit for microseconds instead of `micros`
</description><key id="148403556">17755</key><summary>Use `&#181;s` as unit for microseconds instead of `micros`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>discuss</label></labels><created>2016-04-14T15:47:00Z</created><updated>2016-04-15T10:28:03Z</updated><resolved>2016-04-15T10:27:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T15:54:53Z" id="210015286">I'm worried `&#181;s` will break some consoles and these units are all over the _cat APIs.
</comment><comment author="BobChao87" created="2016-04-14T18:29:39Z" id="210089924">To be clear, the suggestion is `&#181;s` or `mus`, or is it `&#181;s` or `mu`? Because I would find `mu` very confusing. It is not uncommon for the use of `us` in situations where the &#181; character isn't readily available.
</comment><comment author="nik9000" created="2016-04-14T18:31:45Z" id="210090531">&gt; `us`

I'd prefer that by default. 2016 or not, _cat API shouldn't depend on you having a decent font.
</comment><comment author="dadoonet" created="2016-04-15T10:27:45Z" id="210406427">We decided in Fix it friday meeting to close this issue as it does not bring really value and might be more confusing for users.
It would also be a non needed breaking change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapper.TypeParser instances can't deal with @Inject</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17754</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:  &gt;= 2.2.0

**JVM version**: 1.8

**OS version**: Windows 7

**Description of the problem including expected versus actual behavior**:
The current way to register custom type mappers is from _org.elasticsearch.indices.IndicesModule#registerMapper_, which receives instances of   _org.elasticsearch.index.mapper.Mapper.TypeParser_. 

The spot to handle Guice binding context for DI (register implementations, bindings, etc..) is inside _org.elasticsearch.common.inject.Module#config_.

The plugin bootstrap sequence first register the mappers calling onModule method related with IndicesModule, and them builds the DI context calling the module config method. 

How instances passed to _registerMapper_ will be able to solve @Inject bindings, if they can only be constructed by hand and can't be bound to the DI context as Singletons for example?

**Steps to reproduce**:
Does not apply.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="148385966">17754</key><summary>Mapper.TypeParser instances can't deal with @Inject</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cadu-goncalves</reporter><labels /><created>2016-04-14T14:48:47Z</created><updated>2016-04-14T15:07:50Z</updated><resolved>2016-04-14T15:07:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T15:07:50Z" id="209990542">Everything you register must be constructed by hand. Guice is **slowly** getting squashed out of Elasticsearch because it made it very hard to tell what was used where. It is being replaced by the deliberate "this depends on this" kind of behavior that you get from building things by hand.

If there is a thing that you need a reference to in a plugin and you can't get it then we can address that issue but the general direction that the code is going isn't an issue per say.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make Script Writeable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17753</link><project id="" key="" /><description>**Describe the feature**:
Working with the `Script` type is funky because it implements `Streamable` instead of `Writeable`. We should switch it to Writeable with all the cleanup that comes from that. I can think of at least these:
- It grows a StreamInput constructor.
- It loses its `SUPPLIER` member.
</description><key id="148377056">17753</key><summary>Make Script Writeable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-04-14T14:20:30Z</created><updated>2016-04-22T06:39:01Z</updated><resolved>2016-04-22T06:39:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Simplify ActionListenerResponseHandler by adding response supplier</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17752</link><project id="" key="" /><description>Based on comment in #16492
</description><key id="148363679">17752</key><summary>Simplify ActionListenerResponseHandler by adding response supplier</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T13:37:54Z</created><updated>2016-04-14T13:46:55Z</updated><resolved>2016-04-14T13:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-14T13:43:49Z" id="209948907">Nice simplification, removes a bunch of classes from the classloader. Left a comment, otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds `ignore_unmapped` option to geo queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17751</link><project id="" key="" /><description>The change adds a new option to the geo_\* queries: ignore_unmapped. If this option is set to false, the toQuery method on the QueryBuilder will throw an exception if the field specified in the query is unmapped. If the option is set to true, the toQuery method on the QueryBuilder will return a MatchNoDocsQuery. The default value is false so the queries work how they do today (throwing an exception on unmapped field)
</description><key id="148320082">17751</key><summary>Adds `ignore_unmapped` option to geo queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T10:39:35Z</created><updated>2016-05-02T12:04:39Z</updated><resolved>2016-04-14T14:29:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-14T14:09:53Z" id="209961895">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend field stats to include type, searchable, aggregatable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17750</link><project id="" key="" /><description>After much discussion in https://github.com/elastic/elasticsearch/issues/15728 we have agreed to make the following changes to the field stats API:
- `?fields` will accept wildcards, so that it can match all fields
- field stats will return the field `type`, and whether it is searchable (`index ==true`) and aggregatable (`doc_values==true || (index==true &amp;&amp; fielddata==true))`

When `?level=cluster` (the default), fields with the same name are collapsed into a single entry unless the fields have a different `type`, in which case the whole request returns an exception. This collapsing process should change as follows:
- An entry should be returned for fields with conflicting `type`, but with an `error` key which describes the error.
- `searchable` and `aggregatable` are collapsed to `true` if any of the instances of that field are searchable or aggregatable.

/cc @rashidkpc @spalger @jpountz @martijnvg 
</description><key id="148307946">17750</key><summary>Extend field stats to include type, searchable, aggregatable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha3</label></labels><created>2016-04-14T09:49:20Z</created><updated>2016-07-06T14:17:25Z</updated><resolved>2016-07-06T14:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-14T12:42:55Z" id="209919044">&gt; field stats will return the field type, and whether it is searchable (index ==true) and aggregatable (doc_values==true || (index==true &amp;&amp; fielddata==true))

This will be an issue for some virtual fields like `_id`, which is searchable even though it is not indexed, and `_index`, which is aggregatable even though it does not have doc values.

In order not to make it too complicated, I think we can check whether a field is aggregatable by checking whether `MappedFieldType.fielddataBuilder()` throws an exception since it is the method that throws an exception eg. when a keyword field does not have doc values or when a text field has `fielddata=false`.

I don't think we have anything similar for searchable fields, so we would probably have to start by checking `index=true`, plus a hardcoded list of fields that are known to be searchable even though not indexed like `_id` and `_index`.
</comment><comment author="jpountz" created="2016-04-14T14:29:41Z" id="209970572">&gt; index==true &amp;&amp; fielddata==true

This made me realize that it is a bug that you can set `fielddata=true` on a field which is not indexed, so I opened #17747.
</comment><comment author="rashidkpc" created="2016-04-14T14:56:03Z" id="209985805">&gt; When ?level=cluster (the default), fields with the same name are collapsed into a single entry unless the fields have a different type, in which case the whole request returns an exception.

This won't work for us, assuming it means that the entire request to _field_stats will fail. Ideally we would just exclude the field, perhaps with a way of listing the fields that were excluded due to any exception
</comment><comment author="spalger" created="2016-04-14T14:57:05Z" id="209986161">&gt; unless the fields have a different type, in which case the whole request returns an exception

I think this is a bit drastic. Ideally Kibana would be able to use this and not have to through some fatal error about the field types not matching. Could the conflicting field have "conflict" set for its type when types conflict? Perhaps we could do the same thing for searchable and aggregatable.
</comment><comment author="clintongormley" created="2016-04-14T16:09:20Z" id="210023168">&gt; in which case the whole request returns an exception.

this is what it does today, but the bullet points beneath that line explain what we need to change it to do:

&gt; - An entry should be returned for fields with conflicting type, but with an error key which describes the error.
</comment><comment author="jimczi" created="2016-04-18T13:26:42Z" id="211377969">&gt; field stats will return the field type, and whether it is searchable (index ==true) and aggregatable (doc_values==true || (index==true &amp;&amp; fielddata==true))

Currently the field stats API will not return any information if the field is not searchable or if there is no document indexed with a value in that field. It makes sense as none of the stats would be available in such cases. 
@clintongormley @jpountz should we add an entry for the "non-searchable/no value yet" fields ?
This could look like this:

```
      "fields": {
         "field_index_false": {
               "max_doc": 1000,
               "doc_count": -1, // not searchable
               "density": -1,
               "sum_doc_freq": -1,
               "sum_total_term_freq": -1,
               "searchable": false,
               "aggregatable": true,
               "min_value": null,
               "min_value_as_string": "",
               "max_value": null,
               "max_value_as_string": ""
            }
         },
          "field_index_true_no_value": {
               "max_doc": 1000,
               "doc_count": 0, // the field is searchable but no value has been indexed yet.
               "density": 0,
               "sum_doc_freq": 0,
               "sum_total_term_freq": 0,
               "searchable": true,
               "aggregatable": true,
               "min_value": null,
               "min_value_as_string": "",
               "max_value": null,
               "max_value_as_string": ""
            }
         }
```
</comment><comment author="clintongormley" created="2016-04-19T08:49:14Z" id="211805646">Hmm good question...  In fact, even if we can aggregate on a field because it has doc values, we can't return the min/max values unless it is also indexed (correct, @jpountz ?).  Today, the field stats API will ignore non-indexed fields.
</comment><comment author="jpountz" created="2016-04-19T08:54:46Z" id="211808543">&gt; even if we can aggregate on a field because it has doc values, we can't return the min/max values unless it is also indexed

This is correct: we need doc values to aggregate, but these stats either come from points or from the inverted index.

If a field is mapped, I agree there should be an entry regardless of whether it exists in the index or not.
</comment><comment author="Bargs" created="2016-07-02T14:07:53Z" id="230103408">@clintongormley I started working on the Kibana code to consume this new info and I've realized there was a miss with the PR that closed this issue. Field stats still doesn't include the type of each field, and that info is critical for Kibana.
</comment><comment author="jimczi" created="2016-07-04T07:30:33Z" id="230223772">@Bargs, sorry I missed this part in the initial PR. 
I submitted a new PR to fix this discrepancy: 
https://github.com/elastic/elasticsearch/pull/19241
Could you please verify that it contains the information that you need ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove FieldStats.Float.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17749</link><project id="" key="" /><description>Since doubles can represent all floats accurately, we can just use
FieldStats.Double instead.
</description><key id="148298760">17749</key><summary>Remove FieldStats.Float.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T09:11:52Z</created><updated>2016-04-14T13:04:13Z</updated><resolved>2016-04-14T13:04:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-14T09:14:10Z" id="209842035">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds `ignore_unmapped` option to nested and P/C queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17748</link><project id="" key="" /><description>The change adds a new option to the `nested`, `has_parent`, `has_children` and `parent_id` queries: `ignore_unmapped`. If this option is set to false, the `toQuery` method on the QueryBuilder will throw an exception if the type/path specified in the query is unmapped. If the option is set to true, the `toQuery` method on the QueryBuilder will return a MatchNoDocsQuery. The default value is `false`so the queries work how they do today (throwing an exception on unmapped paths/types)
</description><key id="148291532">17748</key><summary>Adds `ignore_unmapped` option to nested and P/C queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T08:38:34Z</created><updated>2016-05-02T12:04:47Z</updated><resolved>2016-04-14T09:38:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-14T08:49:19Z" id="209833938">A minor concern I have is that `ignore_unmapped: false` is a double negation, which could get confusing. Otherwise the change looks good and well tested.
</comment><comment author="martijnvg" created="2016-04-14T08:54:21Z" id="209835233">maybe we should name this setting `fail_unmapped`? (and then default to true)
</comment><comment author="colings86" created="2016-04-14T09:02:28Z" id="209837389">I'm ok with changing it to `fail_unmapped` although we seem to use `ignore_*` elsewhere in the api including `ignore_malformed` which is a bit of a double negation too. What do you think @jpountz and @clintongormley ?
</comment><comment author="clintongormley" created="2016-04-14T09:15:38Z" id="209842758">`ignore_unmapped` defaults to `false`,  so the only time the user will use this will be to set `ignore_unmapped: true`, which I think is OK.
</comment><comment author="clintongormley" created="2016-04-14T09:16:35Z" id="209843018">Docs LGTM
</comment><comment author="jpountz" created="2016-04-14T09:27:02Z" id="209846790">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disallow fielddata loading on text fields that are not indexed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17747</link><project id="" key="" /><description>If a field is not indexed, we cannot load fielddata for it.
</description><key id="148290536">17747</key><summary>Disallow fielddata loading on text fields that are not indexed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T08:34:34Z</created><updated>2016-04-14T16:11:48Z</updated><resolved>2016-04-14T16:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-14T15:10:54Z" id="209992116">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use the new points API to index numeric fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17746</link><project id="" key="" /><description>This makes all numeric fields including `date`, `ip` and `token_count` use
points instead of the inverted index as a lookup structure. This is expected
to perform worse for exact queries, but faster for range queries. It also
requires less storage.

Notes about how the change works:
- Numeric mappers have been split into a legacy version that is essentially
  the current mapper, and a new version that uses points, eg.
  LegacyDateFieldMapper and DateFieldMapper.
- Since new and old fields have the same names, the decision about which one
  to use is made based on the index creation version.
- If you try to force using a legacy field on a new index or a field that uses
  points on an old index, you will get an exception.
- IP addresses now support IPv6 via Lucene's InetAddressPoint and store them
  in SORTED_SET doc values using the same encoding (fixed length of 16 bytes
  and sortable).
- The internal MappedFieldType that is stored by the new mappers does not have
  any of the points-related properties set. Instead, it keeps setting the index
  options when parsing the `index` property of mappings and does
  `if (fieldType.indexOptions() != IndexOptions.NONE) { // add point field }`
  when parsing documents.

Known issues that won't fix:
- You can't use numeric fields in significant terms aggregations anymore since
  this requires document frequencies, which points do not record.
- Term queries on numeric fields will now return constant scores instead of
  giving better scores to the rare values.

Known issues that we could work around (in follow-up PRs, this one is too large
already):
- Range queries on `ip` addresses only work if both the lower and upper bounds
  are inclusive (exclusive bounds are not exposed in Lucene). We could either
  decide to implement it, or drop range support entirely and tell users to
  query subnets using the CIDR notation instead.
- Since IP addresses now use a different representation for doc values,
  aggregations will fail when running a terms aggregation on an ip field on a
  list of indices that contains both pre-5.0 and 5.0 indices.
- The ip range aggregation does not work on the new ip field. We need to either
  implement range aggs for SORTED_SET doc values or drop support for ip ranges
  and tell users to use filters instead. #17700

Closes #16751
Closes #17007
Closes #11513
</description><key id="148285954">17746</key><summary>Use the new points API to index numeric fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>das awesome</label><label>enhancement</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T08:12:55Z</created><updated>2016-04-14T15:57:10Z</updated><resolved>2016-04-14T15:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-04-14T11:55:16Z" id="209900998">&gt; Range queries on ip addresses only work if both the lower and upper bounds are inclusive (exclusive bounds are not exposed in Lucene). We could either decide to implement it, or drop range support entirely and tell users to query subnets using the CIDR notation instead.

That's not really true, its that lucene doesn't need horrible range APIs with tons of booleans and nulls.  if you really want to do this, you can adjust the endpoints with NumericUtils.add() and NumericUtils.subtract() yourself: these work on byte[].

But I would not do this without still deprecating this brokenness in the query DSL. Its just brain damage from the inverted index, this stuff does not make sense for network addresses!
</comment><comment author="rmuir" created="2016-04-14T12:06:28Z" id="209906145">&gt; IP addresses now support IPv6 via Lucene's InetAddressPoint and store them in SORTED_SET doc values using the same encoding (fixed length of 16 bytes and sortable).

I do think there is a potential trap here wrt global ordinals. We've got potentially large data sizes using this encoding, and global ordinals are not needed for anything it does, right? Sorting doesn't need it, and e.g. range faceting wouldn't need it (if implemented): it could just lookup range-&gt;ordinal per segment up front. Can we avoid building global ordinals for IP fields? :)
</comment><comment author="rmuir" created="2016-04-14T12:30:56Z" id="209915153">I took a pass through and this looks great. I had difficulty navigating these mapper apis the way they were structured before and I like how clean the integration now is. Its also good the legacy\* stuff is isolated without being entangled everywhere. I took a look through each of the data types: dates, ip addresses, primitives, and didn't spot any issues.
</comment><comment author="jpountz" created="2016-04-14T12:58:18Z" id="209928320">&gt; I do think there is a potential trap here wrt global ordinals. We've got potentially large data sizes using this encoding, and global ordinals are not needed for anything it does, right? Sorting doesn't need it, and e.g. range faceting wouldn't need it (if implemented): it could just lookup range-&gt;ordinal per segment up front. Can we avoid building global ordinals for IP fields?

We don't have range faceting implemented for SORTED_SET doc values yet (I opened #17700 about whether we should), but if we do it, I agree this should not use global ordinals. Sorting would not use global ordinals, only terms aggregations would (eg. figuring out the top recurring ip addresses that hit a web page). We could add the ability for terms aggs to not build global ordinals, but terms aggs already have lots of specializations so I am reluctant ot add one more. Additionally the cost of building global ords is the same as merging counts from segments in the end if the query matches most values, so I don't think it's too bad?
</comment><comment author="rmuir" created="2016-04-14T13:13:51Z" id="209936370">&gt; We don't have range faceting implemented for SORTED_SET doc values yet (I opened #17700 about whether we should), but if we do it, I agree this should not use global ordinals. Sorting would not use global ordinals, only terms aggregations would (eg. figuring out the top recurring ip addresses that hit a web page). We could add the ability for terms aggs to not build global ordinals, but terms aggs already have lots of specializations so I am reluctant ot add one more. Additionally the cost of building global ords is the same as merging counts from segments in the end if the query matches most values, so I don't think it's too bad?

Yeah, if we want to do a terms-agg by frequency, we need them. It is true there will be a merging cost: and this part will be done in uncompressed space which we can expect to be a turtle if cardinality is high (with java 9 apis we can make it better).

I'd hate for global ordinals to become common though for cases where its unnecessary: e.g. complicated aggregations/processing built off terms agg (demanding global ordinals) all trying to work around the lack of an efficient range faceting if that is really what is wanted. 
</comment><comment author="jpountz" created="2016-04-14T13:17:00Z" id="209937277">&gt; e.g. complicated aggregations/processing built off terms agg (demanding global ordinals) all trying to work around the lack of an efficient range faceting if that is really what is wanted.

If users do not have range faceting, I think the fallback will rather be on the `filters` aggregation than the `terms` aggregation. Which should work pretty well thanks to filter caching if there are not too many ranges and if ranges are always the same (which I think would be common).
</comment><comment author="rmuir" created="2016-04-14T13:49:31Z" id="209951782">OK. I don't mean for it to hold up the issue, it is just something to mention since we are using Sorted type for a "numeric-like" type here which is different than anything before. But the problem is probably also not unique: if we want to add a BigInteger type, i think we will need the same thing (and we can make even less assumptions about it)?
</comment><comment author="rmuir" created="2016-04-14T14:11:31Z" id="209962631">&gt; This is expected
&gt; to perform worse for exact queries

Can you explain this statement more? I find it a little odd: Mike has operations like KNN going at ~ 1500QPS against large datasets (LUCENE-7069), points has an efficient newSetQuery to replace "termsquery" type cases, etc. So i'm wondering exactly what operations we expect to be slower? 
</comment><comment author="jpountz" created="2016-04-14T14:26:16Z" id="209969017">Admittedly I haven't run any benchmark. This assertion was based on the fact that if some values have high document frequencies, then exact queries will have to visit all matching docs with points, while with the inverted index you can leverage the skip lists to skip over documents that do not match other required clauses. I can remove it if you think this is confusing.
</comment><comment author="rmuir" created="2016-04-14T14:27:53Z" id="209969591">I definitely think we should remove it. I think its better not to make assumptions here: its a different datastructure and the rules are different. So far we have only been able to make everything faster.
</comment><comment author="rmuir" created="2016-04-14T14:45:07Z" id="209978762">Also, it is true for some cases extreme cases (massive boolean AND of high-cardinality features) things could conceptually be slower than skiplist intersection. But in many such cases (e.g. AND of latitude and longitude), multiple dimensions may be the better/faster solution anyway. I definitely agree we should not go there here, but it is possible with these data types in lucene, we should keep it in our minds. 

For most common uses like date ranges, I think people will only see points as faster, so we shouldn't set ourselves up for failure.
</comment><comment author="mikemccand" created="2016-04-14T14:48:35Z" id="209981234">Wow, this change looks wonderful, thank you @jpountz!  You didn't hit any new bugs in points? :)

We can also now support larger integers (equivalent of "long long", 128 bits, is available in Lucene's sandbox `BigItegerPoint`), but in a separate issue: #17006.

I've also wondered about exact/set points performance vs TermQuery/TermsQuery but haven't run any tests yet...
</comment><comment author="rjernst" created="2016-04-14T15:01:06Z" id="209987577">LGTM
</comment><comment author="jpountz" created="2016-04-14T15:16:53Z" id="209997158">&gt; You didn't hit any new bugs in points?

Fortunately no, which is good news since the release is out already. :)

Thanks all for having a look. I will merge shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo correction heap_size.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17745</link><project id="" key="" /><description>"-Xms4000mb -Xmx4000mb" on example (Xms/Xmx instead of Xmx/Xmx)
</description><key id="148270345">17745</key><summary>Typo correction heap_size.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bloublou2014</reporter><labels><label>docs</label></labels><created>2016-04-14T07:07:25Z</created><updated>2016-04-14T18:37:37Z</updated><resolved>2016-04-14T18:37:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bloublou2014" created="2016-04-14T08:16:38Z" id="209822084">CLA signed
</comment><comment author="clintongormley" created="2016-04-14T09:08:37Z" id="209839771">Hiya @bloublou2014 

Thanks for the PR.  I've added one comment.  Also, I can't find the signed CLA... any chance you could forward it to me (clinton at elastic dot co)?
</comment><comment author="bloublou2014" created="2016-04-14T10:13:33Z" id="209864754">CLA mail confirmation fwd to clinton at elastic co
</comment><comment author="clintongormley" created="2016-04-14T11:05:17Z" id="209881367">thanks @bloublou2014 - got the CLA.  If you could push a fix for the comment I made, I'll be happy to merge this
</comment><comment author="bloublou2014" created="2016-04-14T18:17:19Z" id="210084802">Change effective for mb to m.
</comment><comment author="clintongormley" created="2016-04-14T18:37:30Z" id="210092230">thanks @bloublou2014 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Opening Indices blocks all indexing threads.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17744</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: Java(TM) SE Runtime Environment (build 1.8.0_71-b15)

**OS version**: CentOS release 6.6

**Description of the problem including expected versus actual behavior**:
Opening indices can cause indexing tasks to time out while processing put-mapping events.

We have about 200,000 indices on our cluster, but typically only have 100-300 indices open. We close them after a period of inactivity and re-open them the next time they're needed. If a large number of requests come in to open closed indices, this causes the put-mapping events generated by bulk indexing threads to hit the 30 second time out and fail.

From watching the pending_tasks list, I suspect (but could be very wrong here) that it's because the open requests insert cluster state tasks of 'shard-started' with URGENT priority. The indexing jobs insert cluster state tasks of 'put-mapping' with HIGH priority. It appears that the URGENT ones always process before the HIGH ones, effectively blocking any indexing jobs.

**Steps to reproduce**:
1. Create a bunch of indices. 1000's
2. Close all of them
3. Start a loop opening those closed indices one at a time
4. Start indexing a new index
5. Your indexing job will now fail with a timeout processing cluster event put-mapping

**Provide logs (if relevant)**:
ProcessClusterEventTimeoutException[failed to process cluster event (put-mapping [response]) within 30s]
        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:343)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
</description><key id="148247955">17744</key><summary>Opening Indices blocks all indexing threads.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ryanhadley</reporter><labels><label>:Cluster</label><label>feedback_needed</label></labels><created>2016-04-14T04:15:55Z</created><updated>2017-04-06T18:33:05Z</updated><resolved>2017-03-31T13:37:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-14T08:35:01Z" id="209828130">@bleskes could you take a look at this please?
</comment><comment author="bleskes" created="2016-04-14T11:54:12Z" id="209900469">Thx @ryanhadley . The reason why shard started messages are processed with high priority is that they are needed to process index and search operations and thus have a higher impact that a put mapping which may block some operations. That said they are batched and should be light, so I wonder why they talk long.  Also, it may be that something else is block the put mapping requests.

Since you say you looked at the pending task list, can you maybe share it? I would also be interested to see the output of the hot threads API of the master node while it processes this open index/put mapping requests
</comment><comment author="ryanhadley" created="2016-04-14T18:17:26Z" id="210084855">@bleskes, thank you! I'm trying to get you this additional data (can't believe I didn't save it from before... oops) but it might take me a bit. We worked around the problem on our Prod cluster by pooling open requests in a queue and only running the open requests once every 10 seconds. I have to get a test cluster up and running with enough large indices to close and then open to recreate this.

&gt; That said they are batched and should be light, so I wonder why they talk long.

They are definitely light and complete very fast. The open request typically takes less than a second per index. The only time it causes issues is when there are many open requests in a row. Which apparently our customers like to do in the early mornings when they show up to work and want to check their reports for new data.
</comment><comment author="ryanhadley" created="2016-04-14T20:15:20Z" id="210130638">OK, I got enough data in to a test cluster I spun up to recreate the issue.

This is a 4 server cluster and I got about 500 indices in to it of decent size (50,000 - 100,000 average document count per index). I close all but 200 of the indices and then ran this to start opening the closed indices one at a time:

for sindex in `curl 'http://localhost:9200/_cluster/state/blocks?pretty' 2&gt;/dev/null | grep "survey-" | awk '{print $1}' | sed -e 's/"//g'`; do echo $sindex; curl -XPOST "http://localhost:9200/$sindex/_open"; done

This was MUCH more stressful on the cluster than what our customers traffic would drive, but it results in the same put-mapping timeout errors while indexing and the same constant listing of URGENT priority tasks in the pending_tasks list while the put-mapping timeouts happen.

Attached are the results of pending_tasks and hot_threads.

[pending_tasks.json.txt](https://github.com/elastic/elasticsearch/files/219885/pending_tasks.json.txt)
[hot_threads.txt](https://github.com/elastic/elasticsearch/files/219887/hot_threads.txt)
</comment><comment author="lfkeitel" created="2016-05-03T20:22:40Z" id="216653290">I seem to have come across the same issue. Windows 10 running a single node during the hands-on training. Elasticsearch hung with no indication anything was wrong. When I stopped it, it output the following:

```
[2016-05-03 14:28:53,418][INFO ][cluster.metadata         ] [web-1] [.kibana] create_mapping [visualization]
[2016-05-03 14:38:07,643][INFO ][cluster.metadata         ] [web-1] [.kibana] update_mapping [index-pattern]
[2016-05-03 15:03:36,649][WARN ][rest.suppressed          ] /.kibana Params: {index=.kibana}
ProcessClusterEventTimeoutException[failed to process cluster event (delete-index [.kibana]) within 30s]
        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:349)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
[2016-05-03 15:03:30,643][WARN ][rest.suppressed          ] /.kibana Params: {index=.kibana}
ProcessClusterEventTimeoutException[failed to process cluster event (delete-index [.kibana]) within 30s]
        at org.elasticsearch.cluster.service.InternalClusterService$2$1.run(InternalClusterService.java:349)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
[2016-05-03 14:58:56,366][DEBUG][action.admin.indices.mapping.put] [web-1] failed to put mappings on indices [[.kibana]]
, type [dashboard]
```
</comment><comment author="bleskes" created="2016-07-07T18:48:52Z" id="231171717">@ryanhadley sorry for taking so long, but looking at the dumps you sent, it seemes the master is busy processing the refresh mapping commands it gets:

```
   79.7% (398.6ms out of 500ms) cpu usage by thread 'elasticsearch[ip-10-0-2-97-indexing-01][clusterService#updateTask][T#1]'
     2/10 snapshots sharing following 12 elements
       org.elasticsearch.index.mapper.MapperService.addMappers(MapperService.java:425)
       org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:318)
       org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:272)
       org.elasticsearch.cluster.metadata.MetaDataMappingService.executeRefresh(MetaDataMappingService.java:140)
       org.elasticsearch.cluster.metadata.MetaDataMappingService$RefreshTaskExecutor.execute(MetaDataMappingService.java:79)

```

Can you put your cluster on debug logging and shard the logs? I'm looking for messages that are produced by this line:

```
logger.debug("[{}] parsed mapping [{}], and got different sources\noriginal:\n{}\nparsed:\n{}", index, mappingType, mappingSource, mapperService.documentMapper(mappingType).mappingSource());
```
</comment><comment author="colings86" created="2017-03-31T13:37:24Z" id="290713961">No further feedback</comment><comment author="ryanhadley" created="2017-04-06T18:32:37Z" id="292267148">Sorry I didn't get back to you. But we discovered that we were just expecting things from Elasticsearch that we shouldn't have been expecting. Our cluster state was so large that things that should happen quickly were not happening quick enough. We solved this problem by drastically reducing our cluster state size with more condensed mapping and less indexes total.

The cluster state was so large. So large. We did bad things.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard level tasks in Bulk Action lose reference to their parent tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17743</link><project id="" key="" /><description>During the bulk action a hierachy of tasks is getting created: bulk-&gt;bulk[s](coord node) -&gt; bulk[s](primary shard node) -&gt; bulk[s][p] and bulk[s][r]. Due to a bug the first bulk[s] task didn't have bulk's task id is set as a parent id.  This commit fixes this bug.
</description><key id="148235193">17743</key><summary>Shard level tasks in Bulk Action lose reference to their parent tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T02:42:25Z</created><updated>2016-04-15T00:24:18Z</updated><resolved>2016-04-15T00:24:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-14T14:37:17Z" id="209974528">Left one comment, other than that, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add {} to ifs in org.apache.lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17742</link><project id="" key="" /><description /><key id="148233771">17742</key><summary>Add {} to ifs in org.apache.lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T02:29:52Z</created><updated>2016-04-14T11:41:10Z</updated><resolved>2016-04-14T11:40:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-14T02:32:52Z" id="209729793">I'm fine either way but I wonder why not just make all of these:

``` java
if (clauses.size() == 0) return null;
```
</comment><comment author="nik9000" created="2016-04-14T02:41:26Z" id="209731026">Habit. I'll pick it up in the morning and make them the short way.
On Apr 13, 2016 10:33 PM, "Jason Tedor" notifications@github.com wrote:

&gt; I'm fine either way but I wonder why not just make all of these:
&gt; 
&gt; if (clauses.size() == 0) return null;
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17742#issuecomment-209729793
</comment><comment author="danielmitterdorfer" created="2016-04-14T04:27:33Z" id="209756418">I wonder why not use `if (clauses.isEmpty())` instead? I think it is easier to read and describes the intention better.
</comment><comment author="jasontedor" created="2016-04-14T07:30:05Z" id="209804539">I'm fine with not changing it in a formatting PR.
</comment><comment author="nik9000" created="2016-04-14T11:41:10Z" id="209892536">Thanks for looking @jasontedor and @danielmitterdorfer! I did switch it to isEmpty() on a single line.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Small format changes in script and monitor packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17741</link><project id="" key="" /><description>Just three files - just adding {}s to if and for statements or making
them single line.
</description><key id="148229653">17741</key><summary>Small format changes in script and monitor packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T02:03:57Z</created><updated>2016-04-14T02:07:19Z</updated><resolved>2016-04-14T02:07:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-14T02:05:55Z" id="209721999">LGTM.
</comment><comment author="nik9000" created="2016-04-14T02:07:16Z" id="209722332">Thanks @jasontedor !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pass test JVM arguments to test nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17740</link><project id="" key="" /><description>This pull request modifies the integration test node spinup to pass
through the system property `tests.jvm.argline`. The main purpose of
passing through this `tests.jvm.argline` is so that we can continue to
test with G1GC on CI. Thus, also added is a test that if G1GC is
enabled, then `JvmInfo#useG1GC` returns the right thing on HotSpot.
</description><key id="148227559">17740</key><summary>Pass test JVM arguments to test nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-14T01:45:03Z</created><updated>2016-04-14T12:08:59Z</updated><resolved>2016-04-14T12:08:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-14T01:54:54Z" id="209719944">LGTM
</comment><comment author="jasontedor" created="2016-04-14T12:08:59Z" id="209906999">Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update stats.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17739</link><project id="" key="" /><description>fix grammar
</description><key id="148199050">17739</key><summary>Update stats.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheDeveloper</reporter><labels><label>docs</label><label>v2.4.0</label></labels><created>2016-04-13T22:17:47Z</created><updated>2016-04-14T08:23:35Z</updated><resolved>2016-04-14T08:23:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-04-14T08:23:24Z" id="209825165">Thanks for the fix. Looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove parser argument from methods where we already pass in a parse context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17738</link><project id="" key="" /><description>When we pass in both XContentParser and QueryParseContext to a method this can be trappy because we cannot make sure that the parser contained in the context and the parser passed as an argument are the same. This removes the parser argument from methods where we currently have both the parser and the parse context as arguments and instead retrieves the parse from the context inside the method.
</description><key id="148196422">17738</key><summary>Remove parser argument from methods where we already pass in a parse context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T22:04:25Z</created><updated>2016-04-14T15:09:52Z</updated><resolved>2016-04-14T15:09:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-14T09:30:13Z" id="209848644">left one tiny comment, LGTM otherwise
</comment><comment author="colings86" created="2016-04-14T10:50:10Z" id="209876890">I left a couple of comments but it looks pretty good. It would be worth coordinating with @nik9000 when you come to merge this so neither of you end up with nasty merge conflicts given you are touching the same code (aggregations parsing)
</comment><comment author="cbuescher" created="2016-04-14T12:08:31Z" id="209906901">@colings86 thanks, I removed the 'aggregations_binary' section you mentioned is unsupported now, can you have a quick look at the last commit if that's what you mean by it?
</comment><comment author="nik9000" created="2016-04-14T12:14:30Z" id="209909876">I'm fine with it. Theoretically it is backwards though. We're slowly moving towards parsing xcontent where "context" is a bunch of stuff used to help with parsing and it doesn't contain the parser. That is the vision for ObjectParser so far as I understand it.

I agree what we have is confusing - we shouldn't pass the parser in two ways. And I agree it'd be more work to go the other way. So, yeah, I'm fine with this.
</comment><comment author="colings86" created="2016-04-14T12:46:32Z" id="209921960">@cbuescher Yep thats what I meant :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix funny generics in request size limit test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17737</link><project id="" key="" /><description /><key id="148187688">17737</key><summary>Fix funny generics in request size limit test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Network</label><label>test</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T21:27:41Z</created><updated>2016-04-14T15:19:50Z</updated><resolved>2016-04-14T15:00:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T21:28:45Z" id="209653908">@danielmitterdorfer I was trying to debug a non-reproducable failure I had on this test locally (no node available while doing after test cleanup) and I found some funky stuff going on with the generics. Would you mind having a look?
</comment><comment author="danielmitterdorfer" created="2016-04-14T03:53:28Z" id="209749291">@nik9000 I wonder why the problem appeared in the first place. Do you use Eclipse? I did not have any problem in IDEA and on the command line with Oracle JDK 1.8.0_74-b02.

I left one comment, otherwise LGTM.

Can you please backport this to 2.x too as I just pushed the backport of #17133 where this change comes from?
</comment><comment author="nik9000" created="2016-04-14T11:49:06Z" id="209896809">&gt; I wonder why the problem appeared in the first place. Do you use Eclipse?

Yes I do, but Eclipse didn't catch it. I add type parameters to things that are missing them when I touch code out of habit and that caused a chain reaction. I suspect the that I hit is something you've already fixed by raising the request buffer but it never reproduced locally for me.
</comment><comment author="nik9000" created="2016-04-14T15:19:50Z" id="209998742">2.x: 9486bb04052f11a07e848fcba2c659672ba2e631 08bd55b7d4b946a04d0262e69f77d0788df37646
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>phrase prefix query returns different results when it is used in a query filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17736</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.3.2

**JVM version**: Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)

**OS version**: Ubuntu 12.04.5 LTS

**Description of the problem including expected versus actual behavior**: the same phrase prefix search returns different results depending on whether it is used in a query or in a query filter. We expect the same number of hits in both cases because the search condition is the same, although expressed in a different form.

**Steps to reproduce**:
1. `client.search index: my_index, search_type: 'count', body:{query:{match:{_all:{query:'construction document', type: :phrase_prefix}}}}`
2. `client.search index: my_index, search_type: 'count', body:{query:{filtered:{query:{match_all:{}},filter:{query:{match:{_all:{query:'construction document',type: :phrase_prefix}}}}}}}`

Expected: counts should match
Observed: counts do not match
</description><key id="148184336">17736</key><summary>phrase prefix query returns different results when it is used in a query filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aliakb</reporter><labels /><created>2016-04-13T21:11:03Z</created><updated>2016-04-14T08:30:57Z</updated><resolved>2016-04-14T08:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-14T08:30:57Z" id="209827231">Hi @aliakb 

I'm afraid you haven't given nearly enough information here.  I've tried to reproduce this and failed.  here's what I tried:

```
DELETE *

POST t/t/_bulk
{"index": {}}
{"text": "foo bar construction baz document"}
{"index": {}}
{"text": "foo bar construction document baz"}
{"index": {}}
{"text": "foo bar construction baz document"}
{"index": {}}
{"text": "foo bar construction document baz"}
{"index": {}}
{"text": "foo bar construction baz document"}
{"index": {}}
{"text": "foo bar construction document baz"}
{"index": {}}
{"text": "foo bar construction baz document"}
{"index": {}}
{"text": "foo bar construction document baz"}
{"index": {}}
{"text": "foo bar construction baz document"}
{"index": {}}
{"text": "foo bar construction document baz"}
{"index": {}}
{"text": "foo bar construction baz document"}
{"index": {}}
{"text": "foo bar construction document baz"}

GET _search?search_type=count
{
  "query": {
    "match": {
      "_all": {
        "query": "construction document",
        "type": "phrase_prefix"
      }
    }
  }
}

GET _search?search_type=count
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "query": {
          "match": {
            "_all": {
              "query": "construction document",
              "type": "phrase_prefix"
            }
          }
        }
      }
    }
  }
}
```

Both queries produce the same results.  I don't know which of your queries is correct and which incorrect, if you get consistent results when running the queries multiples times (maybe you're seeing a different number of docs on different shards?), etc.

Also, 1.3.2 is really old and won't be seeing any further releases.  I'm going to close this ticket, but if you manage to recreate the problem on a newer version, please open a new issue with the recreation

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch SearchAfterBuilder to writeGenericValue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17735</link><project id="" key="" /><description>and remove its PROTOTYPE.

Relates to #17085
</description><key id="148184163">17735</key><summary>Switch SearchAfterBuilder to writeGenericValue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T21:10:12Z</created><updated>2016-04-14T14:41:28Z</updated><resolved>2016-04-14T14:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T21:11:17Z" id="209648858">@cbuescher I figured it was safe to break SearchAfterBuilder's serialization before 5.0 in the name of using common code to send the generic value across the wire.
</comment><comment author="cbuescher" created="2016-04-14T08:55:26Z" id="209835469">@nik9000 took a look, I have one question where I'm not sure if allowing generic values will have some strange side effects. Maybe you have an opinion on that.
</comment><comment author="jimczi" created="2016-04-14T09:31:11Z" id="209849141">SearchAfterBuilder does not accept generic values and we rely on serialization/parsing to do the type checks. I agree that using writeGenericValues is cleaner but in that case could you please add a check in SearchAfterBuilder.setSortValues.
FYI the supported types are derived from org.elasticsearch.search.internalInternalSearchHit.sortValues which accepts only numerics and strings. 
</comment><comment author="nik9000" created="2016-04-14T12:45:58Z" id="209921365">@jimferenczi @cbuescher @javanna added validation to the setter.
</comment><comment author="jimczi" created="2016-04-14T14:22:15Z" id="209967464">Thanks @nik9000 
LGTM
</comment><comment author="nik9000" created="2016-04-14T14:41:28Z" id="209977237">Thanks for all the reviewing @cbuescher @jimferenczi @javanna !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove remaining PROTOTYPEs from org.elasticsearch.search.suggest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17734</link><project id="" key="" /><description>Relates to #17085
</description><key id="148181278">17734</key><summary>Remove remaining PROTOTYPEs from org.elasticsearch.search.suggest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T20:57:31Z</created><updated>2016-05-02T12:14:13Z</updated><resolved>2016-04-14T14:31:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-14T08:34:17Z" id="209827941">LGTM
</comment><comment author="nik9000" created="2016-04-14T14:31:03Z" id="209971333">Thanks for reviewing @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from ScriptField</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17733</link><project id="" key="" /><description>Relates to #17085
</description><key id="148170891">17733</key><summary>Remove PROTOTYPE from ScriptField</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T20:14:20Z</created><updated>2016-05-02T12:14:18Z</updated><resolved>2016-04-14T01:30:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-13T21:19:06Z" id="209651179">LGTM
</comment><comment author="nik9000" created="2016-04-13T21:26:11Z" id="209653133">Thanks for reviewing @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove remaining PROTOTYPEs from org.elasticsearch.index.query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17732</link><project id="" key="" /><description>Relates to #17085
</description><key id="148159138">17732</key><summary>Remove remaining PROTOTYPEs from org.elasticsearch.index.query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T19:24:06Z</created><updated>2016-05-02T12:14:25Z</updated><resolved>2016-04-13T20:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-13T19:50:44Z" id="209620072">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor BootstrapCheckTests to use expectThrows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17731</link><project id="" key="" /><description>This commit modifies all uses of the pattern try/catch/assert in
BootstrapCheckTests to use expectThrows/assert.
</description><key id="148157550">17731</key><summary>Refactor BootstrapCheckTests to use expectThrows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>non-issue</label><label>review</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T19:18:42Z</created><updated>2016-04-13T22:41:48Z</updated><resolved>2016-04-13T22:41:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-13T21:43:15Z" id="209661043">@nik9000 Thanks for reviewing, I pushed a commit responding to your feedback.
</comment><comment author="nik9000" created="2016-04-13T22:28:49Z" id="209674281">Lgtm
On Apr 13, 2016 5:43 PM, "Jason Tedor" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 Thanks for reviewing, I pushed a
&gt; commit responding to your feedback.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17731#issuecomment-209661043
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adjust line-length of transport related classes to coding standard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17730</link><project id="" key="" /><description /><key id="148157262">17730</key><summary>Adjust line-length of transport related classes to coding standard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T19:17:45Z</created><updated>2016-04-15T08:13:45Z</updated><resolved>2016-04-15T08:13:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-04-13T19:18:34Z" id="209604904">I have shortened the line-length of a few transport related classes. While I could have done that in #17133, the original PR was already big enough so I did this separately.
</comment><comment author="jpountz" created="2016-04-15T08:02:59Z" id="210346372">I found one minor oddity. Otherwise LGTM.
</comment><comment author="danielmitterdorfer" created="2016-04-15T08:05:19Z" id="210346769">@jpountz Thanks for the review! I'll check and merge afterwards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove StreamableReader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17729</link><project id="" key="" /><description>Relates to #17085
</description><key id="148154502">17729</key><summary>Remove StreamableReader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T19:08:39Z</created><updated>2016-04-14T14:22:55Z</updated><resolved>2016-04-14T14:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-14T08:38:25Z" id="209829993">LGTM, left one tiny comment. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add heap size bootstrap check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17728</link><project id="" key="" /><description>This commit adds a bootstrap check to ensure that the initial heap size
and max heap size are set equal to each other.

Closes #17490
</description><key id="148151524">17728</key><summary>Add heap size bootstrap check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T18:56:40Z</created><updated>2016-05-02T12:19:15Z</updated><resolved>2016-04-13T19:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T19:07:07Z" id="209598026">Seems fine with me. Left non-critical comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from filter aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17727</link><project id="" key="" /><description>and cut it to registerAggregation

Relates to #17085
</description><key id="148145322">17727</key><summary>Remove PROTOTYPE from filter aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T18:32:12Z</created><updated>2016-05-02T12:14:31Z</updated><resolved>2016-04-14T16:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-14T07:24:51Z" id="209802265">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from scripted_metric aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17726</link><project id="" key="" /><description>and cut it to registerAggregation.

Relates to #17085
</description><key id="148138027">17726</key><summary>Remove PROTOTYPE from scripted_metric aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T18:05:46Z</created><updated>2016-05-02T12:14:37Z</updated><resolved>2016-04-14T14:05:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-14T07:23:08Z" id="209801323">@nik9000 Left one comment but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix path to assembled packages in contributing doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17725</link><project id="" key="" /><description>This commit fixes the specification of the path to assembled packages in
the contributing doc.
</description><key id="148129266">17725</key><summary>Fix path to assembled packages in contributing doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T17:31:46Z</created><updated>2016-04-13T17:57:51Z</updated><resolved>2016-04-13T17:57:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-13T17:32:35Z" id="209556871">@RossLieberman Thank you for bringing this to my attention.
</comment><comment author="dakrone" created="2016-04-13T17:41:39Z" id="209559841">LGTM
</comment><comment author="RossLieberman" created="2016-04-13T17:52:03Z" id="209565653">Thanks! Found them now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apt-get (or dpkg) warns of weaks digest algorithm use by signature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17724</link><project id="" key="" /><description>**Elasticsearch version**: 2.3

**OS version**: Ubuntu 16.04

**Description of the problem including expected versus actual behavior**:
Warning from apt-get when installing ElasticSearch (and Logstash): `W: http://packages.elastic.co/elasticsearch/2.x/debian/dists/stable/Release.gpg: Signature by key 46095ACC8548582C1A2699A9D27D666CD88E42B4 uses weak digest algorithm (SHA1`

Notes:
- It installs just fine, it just warns.
- The same applies to Logstash.

**Steps to reproduce**:
1. Add elastic GPG key as explained in the documentation
2. Add elasticsearch repository to sources list and update: `apt-get update`
3. Run `apt-get install elasticsearch -y`
</description><key id="148125384">17724</key><summary>Apt-get (or dpkg) warns of weaks digest algorithm use by signature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">ThomasdOtreppe</reporter><labels><label>:Packaging</label><label>blocker</label><label>bug</label><label>feedback_needed</label></labels><created>2016-04-13T17:15:23Z</created><updated>2017-05-04T14:41:28Z</updated><resolved>2016-07-19T12:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-14T08:00:14Z" id="209815916">@drewr presumably this applies to anything signed by our key.  Any ideas here?
</comment><comment author="girirajsharma" created="2016-04-14T12:01:10Z" id="209904411">The release seems to be half-broken due to SHA1 removal by debian(apt) in newer OS versions. It has affected many repositories and they intend to shut off SHA1 completely on January 1, 2017.

The repository owner needs to pass `--digest-algo SHA512` or `--digest-algo SHA256` (or another SHA2 algorithm) to gpg when signing the file. Repositories with DSA keys need to be migrated to RSA first.
SHA1 support is not yet dropped, they merely do not consider it trustworthy.

Please check here for more info : https://wiki.debian.org/Teams/Apt/Sha1Removal
</comment><comment author="clintongormley" created="2016-04-14T12:06:03Z" id="209906073">thanks for this info @girirajsharma, very helpful!
</comment><comment author="spinscale" created="2016-05-06T13:59:51Z" id="217447976">so this is the Release.gpg file, which is created by the `deb-s3` tool on deployment... `deb-s3` supports an option to add arbitrary GPG parameters.

However I have no idea, if that would affect older distributions and if this is the only issue or just masking another one with the package itself.
</comment><comment author="s1monw" created="2016-07-18T14:33:36Z" id="233346457">@clintongormley @drewr @rjernst @spinscale any news on this
</comment><comment author="clintongormley" created="2016-07-18T14:43:54Z" id="233349518">this was fixed with the current release process by adding the following param to the `deb-s3` upload command:

```
--gpg-options="--digest-algo SHA512"
```

I'm not sure how @rjernst is planning on uploading deb packages though
</comment><comment author="rjernst" created="2016-07-18T18:39:07Z" id="233418912">This is already handled in unified release, by passing those gpg options when creating the deb signatures.
</comment><comment author="clintongormley" created="2016-07-19T12:49:56Z" id="233621846">Thanks @rjernst - closing
</comment><comment author="jonathanpmartins" created="2016-07-28T19:02:31Z" id="235992910">How do I fix the current warning that appears when I `apt-get update` ?
@ThomasdOtreppe @clintongormley @drewr @girirajsharma @jpountz @spinscale @rjernst @s1monw 

I'm using Ubuntu 16.04
</comment><comment author="sterago" created="2016-08-04T15:42:35Z" id="237593642">+1
I have the same problem, but with filebeat:
`W: https://packages.elastic.co/beats/apt/dists/stable/Release.gpg: Signature by key 46095ACC8548582C1A2699A9D27D666CD88E42B4 uses weak digest algorithm (SHA1)
`
</comment><comment author="ThomasdOtreppe" created="2016-08-09T16:57:42Z" id="238618147">I just tested again right now (since there was a release recently) and the issue is still present.
</comment><comment author="jonathanpmartins" created="2016-08-11T17:03:18Z" id="239224296">In 16.04.1 same thing...
</comment><comment author="clintongormley" created="2016-08-12T15:53:06Z" id="239484416">Please could you try this again - I've resigned the deb repository with SHA512
</comment><comment author="rabbitfang" created="2016-08-12T16:26:53Z" id="239493077">I am no longer getting this message on Ubuntu 16.04.1 for ElasticSearch.
</comment><comment author="ThomasdOtreppe" created="2016-08-12T17:21:03Z" id="239506827">Elasticsearch is fixed.

Logstash, Kibana and curator still have the issue (and I guess beats too but I haven't tested it). Should I open a bug in their respective repo?
</comment><comment author="clintongormley" created="2016-08-12T17:21:43Z" id="239506986">please do
</comment><comment author="rjernst" created="2016-08-12T17:31:39Z" id="239509534">There's no need to file issues with the other projects, this is fixed in the unified release, which should be used for beta1. 
</comment><comment author="clintongormley" created="2016-08-16T13:15:26Z" id="240097654">@rjernst i was thinking about the existing repositories which are currently a problem with ubuntu 16
</comment><comment author="failedguidedog" created="2016-08-31T13:09:37Z" id="243758369">@clintongormley Just to let you know, I'm having this issue with repositories for older elasticsearch releases (specifically 1.7). Unfortunately I can't use a newer version at this time, so a fix for those repos would be very much appreciated.
</comment><comment author="meteormatt" created="2016-10-20T07:32:55Z" id="255030646">It is still not fixed.

``` bash
W: https://artifacts.elastic.co/packages/5.x-prerelease/apt/dists/stable/Release.gpg: Signature by key 46095ACC8548582C1A2699A9D27D666CD88E42B4 uses weak digest algorithm (SHA1)
```
</comment><comment author="rjernst" created="2016-10-20T07:34:06Z" id="255030899">It's fixed in our release process (for real now) and will be in the next release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node names cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17723</link><project id="" key="" /><description>Fixes two node names contained trailing spaces that was causing cat.tasks test to fail.

Fixes #17718
</description><key id="148123612">17723</key><summary>Node names cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T17:06:46Z</created><updated>2016-04-13T17:10:08Z</updated><resolved>2016-04-13T17:10:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-13T17:08:16Z" id="209548396">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Fix extra backslash causing sed to hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17722</link><project id="" key="" /><description>This caused sed to hang and the output never to show for the smoke test
command
</description><key id="148119008">17722</key><summary>[TEST] Fix extra backslash causing sed to hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2016-04-13T16:49:33Z</created><updated>2016-04-13T18:45:03Z</updated><resolved>2016-04-13T18:45:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-13T16:51:46Z" id="209541792">@nik9000 I'm not sure if this is related to the CI failures or not (since it causes the command to hang), but can you take a look?
</comment><comment author="nik9000" created="2016-04-13T18:09:35Z" id="209572978">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable installing the rest-api-spec artifact</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17721</link><project id="" key="" /><description>Running `gradle install` on the rest-api-spec fails because there is no available install
task. This change applies the nexus plugin so that we can install and should also enable
publishing as part of the uploadArchives task.
</description><key id="148113640">17721</key><summary>Enable installing the rest-api-spec artifact</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T16:26:59Z</created><updated>2016-04-13T17:56:16Z</updated><resolved>2016-04-13T17:56:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-04-13T16:28:12Z" id="209532192">@rjernst can you take a look?
</comment><comment author="rjernst" created="2016-04-13T17:15:18Z" id="209550538">I made this same change locally yesterday in order to publish the rest api spec for alpha1. You'll also need this block:

```
extraArchive {
  javadoc = false
  sources = false
}
```
</comment><comment author="jaymode" created="2016-04-13T17:25:45Z" id="209554598">I think that block is already in the PR
</comment><comment author="rjernst" created="2016-04-13T17:46:57Z" id="209564169">Woah sorry I missed it this morning when looking quickly on my phone. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Copy checkstyle config to allow running from a jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17720</link><project id="" key="" /><description>The checkstyle configuration files were being accessed as resources within the project and
being converted from a URL to a File by gradle. This works when the build tools project is being
referenced as a project. However, when using the published jar the URL points to a resource
in the jar file that cannot be converted to a File object and caused the build to fail.

This change copies the files into a `checkstyle` directory in the project build folder and always uses
File objects pointing to the copied files.
</description><key id="148113535">17720</key><summary>Copy checkstyle config to allow running from a jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T16:26:34Z</created><updated>2016-04-14T17:39:56Z</updated><resolved>2016-04-14T17:39:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-04-13T16:27:52Z" id="209532091">@rjernst can you take a look?
</comment><comment author="jaymode" created="2016-04-14T11:27:49Z" id="209888977">@nik9000 @rjernst I reverted the change to how the files are copied and added a new commit that sets inputs/outputs so that the UP-TO-DATE checks work
</comment><comment author="nik9000" created="2016-04-14T14:12:00Z" id="209962780">I'm happy with it. @rjernst knows gradle much better than I do though.
</comment><comment author="rjernst" created="2016-04-14T15:36:28Z" id="210006607">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup nested, has_child &amp; has_parent query builders for inner hits construction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17719</link><project id="" key="" /><description>- Inner hits are now only provided and prepared in the constructors of the `nested`, `has_child` and `has_parent` queries. This will make fixing #11118 easier and then allow us to drop the top level inner hit syntax.
- Also made `score_mode` a required constructor parameter. (`fromXContent(...)` method maintain their defaults)
- Moved has_child's min_child/max_children validation from `doToQuery(...)` to a setter. (so we can fail sooner on the coordinating node)
</description><key id="148105267">17719</key><summary>Cleanup nested, has_child &amp; has_parent query builders for inner hits construction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T15:51:06Z</created><updated>2016-04-14T12:45:29Z</updated><resolved>2016-04-14T12:45:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-14T09:23:37Z" id="209845204">Apart from my comments about query builder default values this looks pretty good. Am I right in saying that these QueryBuilders are effectively immutable now? (i.e. they have no setters?). If so, I think https://github.com/elastic/elasticsearch/pull/17748 will change that as `ignore_unmapped` is not mandatory. I am also wondering why you needed to move score and InnerHitsBuilder to the constructor?Is there a reason they cannot be set after construction? I only ask as it is nice if only the mandatory parameters are specified in the constructor so the user doesn't have to worry about what the sensible defaults are.
</comment><comment author="martijnvg" created="2016-04-14T09:46:40Z" id="209854872">@colings86 Only score_mode will additionally be made a mandatory parameter. The other params remain optional.
</comment><comment author="colings86" created="2016-04-14T10:42:36Z" id="209874457">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] MultiNodeBackwardsIT.test {p0=cat.tasks/10_basic/Test cat tasks output} fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17718</link><project id="" key="" /><description>See: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=centos/243/console

Looks like it may be a simple regex issue:

```
Suite: org.elasticsearch.backwards.MultiNodeBackwardsIT
  1&gt; [2016-04-13 08:46:41,455][ERROR][org.elasticsearch.test.rest.client] Adding header Accept
  1&gt;  with value application/yaml
  1&gt; [2016-04-13 08:47:09,457][ERROR][org.elasticsearch.test.rest.client] Adding header Accept
  1&gt;  with value application/yaml
  2&gt; REPRODUCE WITH: gradle :qa:backwards-5.0:integTest -Dtests.seed=2C193B1BE603DD5E -Dtests.class=org.elasticsearch.backwards.MultiNodeBackwardsIT -Dtests.method="test {p0=cat.tasks/10_basic/Test cat tasks output}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=is-IS -Dtests.timezone=Mexico/General
FAILURE 0.02s | MultiNodeBackwardsIT.test {p0=cat.tasks/10_basic/Test cat tasks output} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: field [$body] was expected to match the provided regex but didn't
   &gt; Expected:  # action  task_id      parent_task_id      type    start_time   timestamp            running_time  ip                                  node
   &gt; ^(  \S+\s+  \S+\:\d+\s+  (?:\-|\S+\:\d+)\s+  \S+\s+  \d+\s+       \d\d\:\d\d\:\d\d\s+  \S+\s+        \d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}\s+  \S+(?:\s\S+)*\n)+$
   &gt;      but: was "cluster:monitor/tasks/lists    sEat2lQHTJ26PwkjuXqDXw:9695 -                           transport 1460537251333 08:47:31 83.2micros 127.0.0.1 Grasshopper \ncluster:monitor/tasks/lists[n] sEat2lQHTJ26PwkjuXqDXw:9696 sEat2lQHTJ26PwkjuXqDXw:9695 direct    1460537251333 08:47:31 21.7micros 127.0.0.1 Grasshopper \ncluster:monitor/tasks/lists[n] S8fYxQvLQwuEhXfQ2S5szw:4794 sEat2lQHTJ26PwkjuXqDXw:9695 netty     1460537251333 08:47:31 27.5micros 127.0.0.1 Mad-Dog\n"
   &gt;    at __randomizedtesting.SeedInfo.seed([2C193B1BE603DD5E:A44D04C148FFB0A6]:0)
```
</description><key id="148103776">17718</key><summary>[CI] MultiNodeBackwardsIT.test {p0=cat.tasks/10_basic/Test cat tasks output} fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2016-04-13T15:45:37Z</created><updated>2016-04-13T17:10:08Z</updated><resolved>2016-04-13T17:10:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from histogram aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17717</link><project id="" key="" /><description>Relates to #17085
</description><key id="148100135">17717</key><summary>Remove PROTOTYPE from histogram aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T15:33:28Z</created><updated>2016-05-02T12:14:43Z</updated><resolved>2016-04-14T15:34:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T15:34:40Z" id="209513908">@colings86 I decided not to try and change HistorgramParser beyond removing the methods we no longer use. Any cleanup in that area may as well wait for ObjectParserification at a later date.
</comment><comment author="colings86" created="2016-04-14T07:19:32Z" id="209799791">@nik9000 I left one comment but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setup jvm opts correctly for deb/rpm tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17716</link><project id="" key="" /><description>Also stops warning about JAVA_OPTS when it is an empty string.
</description><key id="148092661">17716</key><summary>Setup jvm opts correctly for deb/rpm tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T15:09:58Z</created><updated>2016-04-13T15:19:16Z</updated><resolved>2016-04-13T15:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T15:10:32Z" id="209499079">@jasontedor this was breaking on my machine with jna installed in a system location.
</comment><comment author="jasontedor" created="2016-04-13T15:16:38Z" id="209503271">LGTM.
</comment><comment author="nik9000" created="2016-04-13T15:19:16Z" id="209505503">Thanks for the review @jasontedor ! I wish it were simpler....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On restore, selecting concrete indices can select wrong index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17715</link><project id="" key="" /><description>While restoring a snapshot with multiple indices inside, selecting concrete indices can result in a wrong index being restored. This happens if at least 1 selected index is not available inside the snapshot while another being available but not the first one at the available list.

Example:
- Available indices in snapshot: ["foo", "bar" baz"]
- Selected indices to restore: ["bar", "not_available"]
- Resulting indices to be restored: ["foo"] &lt;-- WRONG, expected ["bar"]

This bug was also verified to appear at 1.7 and 2.x.
</description><key id="148074164">17715</key><summary>On restore, selecting concrete indices can select wrong index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seut</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T14:10:45Z</created><updated>2016-04-13T15:26:54Z</updated><resolved>2016-04-13T15:25:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-04-13T14:33:12Z" id="209476270">@seut LGTM. Thanks for contributing. I will also backport this to 2.3.2 (see version labels on the PR). Backports to 1.7 are only for the most critical bugs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify JVM options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17714</link><project id="" key="" /><description>This commit simplifies the default JVM options that ship with
Elasticsearch. In particular, expert settings that were previously
configurable via environment variables have been removed from the
default configuration file. Further, the heap size settings have been
moved to the top of the file with a clearer message that is in
concordance with their importance.

Relates #17675
</description><key id="148054948">17714</key><summary>Simplify JVM options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>docs</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T13:06:06Z</created><updated>2016-10-18T08:51:52Z</updated><resolved>2016-04-13T22:00:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T13:20:13Z" id="209437183">@jasontedor better, but I'd go further to make it really obvious:

```
## JVM configuration


####################################################
## IMPORTANT: JVM heap size                    
####################################################
##                                                  
##  You should always set the min and max JVM heap  
##  size to the same value.  For example, to set    
##  the heap to 4GB, set:                           
##                                                  
##  -Xms4g                                          
##  -Xmx4g                                          
##                                                  
## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html
## for more information
##
####################################################

# Xms represents the initial size of total heap space
# Xmx represents the maximum size of total heap space

-Xms${heap.min}
-Xmx${heap.max}


####################################################
## Expert settings
####################################################
##
## All settings below this section are considered 
## expert settings.  Don't tamper with them unless
## you understand what you are doing
##
####################################################
```
</comment><comment author="jasontedor" created="2016-04-13T16:54:11Z" id="209543268">@clintongormley Thanks. I pushed 5ed8ec914cf22caa9d49f6b5682759ebebd6d001.
</comment><comment author="jpountz" created="2016-04-13T21:53:54Z" id="209664306">LGTM
</comment><comment author="jasontedor" created="2016-04-13T22:00:52Z" id="209665994">Thanks @clintongormley and @jpountz!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms-aggregation.asciidoc tiny edit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17713</link><project id="" key="" /><description /><key id="148052383">17713</key><summary>terms-aggregation.asciidoc tiny edit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels><label>docs</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T12:56:53Z</created><updated>2016-04-14T10:46:11Z</updated><resolved>2016-04-13T22:50:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-13T22:50:56Z" id="209681406">Merged, thanks!
</comment><comment author="golubev" created="2016-04-14T10:46:10Z" id="209875579">Thanks, @dakrone!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove getQueryPrototype from QueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17712</link><project id="" key="" /><description>It isn't used any more.
</description><key id="148047717">17712</key><summary>Remove getQueryPrototype from QueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T12:42:31Z</created><updated>2016-05-02T12:14:57Z</updated><resolved>2016-04-13T13:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T12:42:44Z" id="209411074">@cbuescher one last one in queries I think
</comment><comment author="cbuescher" created="2016-04-13T13:05:38Z" id="209427296">LGTM
</comment><comment author="nik9000" created="2016-04-13T13:29:34Z" id="209441295">Thanks for reviewing @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apply the default operator on analyzed wildcard in query_string builder:</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17711</link><project id="" key="" /><description>- Tokens in the same position are grouped in a SynonymQuery.
- The default operator is applied on tokens in different positions.
- The wildcard is applied to the terms in the last position only.

Fixes #2183
</description><key id="148047048">17711</key><summary>Apply the default operator on analyzed wildcard in query_string builder:</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Query DSL</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T12:39:31Z</created><updated>2016-07-20T09:32:54Z</updated><resolved>2016-04-13T13:43:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-13T13:00:01Z" id="209423348">I left some (minor) comments.
</comment><comment author="jimczi" created="2016-04-13T13:34:55Z" id="209443089">Thanks @jpountz . I've pushed another commit to address your comments.
</comment><comment author="jpountz" created="2016-04-13T13:37:46Z" id="209443996">LGTM
</comment><comment author="lukapor" created="2016-07-12T07:09:55Z" id="231956043">Will be this fix (change request) merged in 2.4?
</comment><comment author="jpountz" created="2016-07-13T10:46:50Z" id="232321119">Hmm the labels say it was merged in 2.4 but I can't find the commit. @jimferenczi did you forget to backport?
</comment><comment author="clintongormley" created="2016-07-13T12:32:02Z" id="232340865">@jimferenczi I wonder if the 02992ae9d6a4fc42f086e8fab7b2487f868235b4 commit went only into 2.4 and not into 2.x - we replaced the 2.4 branch with 2.x
</comment><comment author="jimczi" created="2016-07-13T14:49:30Z" id="232379313">I don't remember :(. I'll check after my vacation next week. Sorry for the delay.
</comment><comment author="jimczi" created="2016-07-20T09:32:54Z" id="233900055">I am not sure where the initial merge went but thanks to @lukapor @jpountz and @clintongormley the fix **is** in 2.4:
https://github.com/elastic/elasticsearch/commit/02de9cf0bb599b9fe551805c010f3f49e48605f1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate Indices query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17710</link><project id="" key="" /><description>This is in favour of just searching the _index field

Closes #12017
</description><key id="148042239">17710</key><summary>Deprecate Indices query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T12:19:32Z</created><updated>2016-08-26T13:14:13Z</updated><resolved>2016-04-14T16:09:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-13T12:26:30Z" id="209404498">This looks good to me. Let's maybe add a concrete example to the docs that shows how an IndicesQuery can be replaced with a BooleanQuery using the `_index` field?
</comment><comment author="clintongormley" created="2016-04-13T12:46:54Z" id="209413730">Hmmm...  not quite replaceable.   Queries in the indices query are only parsed on the listed indices - the same is not true for bool, eg, this example creates a parent mapping on one index, but not the other:

```
DELETE one,two

PUT one 
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT two
```

The `indices` query allows you to query two indices with different mappings:

```
GET _search
{
  "query": {
    "indices": {
      "indices": [
        "one"
      ],
      "query": {
        "has_child": {
          "type": "child",
          "query": {"match_all": {}}
        }
      },
      "no_match_query": {"match_all": {}}
    }
  }
}
```

While this query will throw an exception about missing the parent type:

```
GET _search
{
  "query": {
    "bool": {
      "should": [
        {
          "bool": {
            "must": [
              {
                "term": {
                  "_index": "one"
                }
              },
              {
                "has_child": {
                  "type": "child",
                  "query": {
                    "match_all": {}
                  }
                }
              }
            ]
          }
        },
        {
          "bool": {
            "must_not": [
              {
                "term": {
                  "_index": "one"
                }
              }
            ]
          }
        }
      ]
    }
  }
}
```

That's why this change depends on https://github.com/elastic/elasticsearch/issues/12016
</comment><comment author="colings86" created="2016-04-14T14:34:03Z" id="209972920">@clintongormley now we have https://github.com/elastic/elasticsearch/pull/17751 and https://github.com/elastic/elasticsearch/pull/17748 to deal with the above, are you ok for me to merge this?
</comment><comment author="clintongormley" created="2016-04-14T16:07:27Z" id="210022056">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support Java 8's Streams API in Groovy restrictions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17709</link><project id="" key="" /><description>Hi! 

Currently, the only one way to process collections in scripts is to use Groovy's extension methods for collections - but they are not lazy - each invocation will create an intermediate collection. This is where Java 8's Streams API might be very useful, but currently they fail because of java.util.Function usage + Collectors class is restricted. 

Would be great if these classes will be whitelisted by default!

Thanks
</description><key id="148028700">17709</key><summary>Support Java 8's Streams API in Groovy restrictions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bsideup</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2016-04-13T11:25:36Z</created><updated>2016-04-21T16:19:33Z</updated><resolved>2016-04-21T16:19:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-15T09:41:27Z" id="210387707">@rmuir Do you have any opinions?
</comment><comment author="rmuir" created="2016-04-15T17:54:27Z" id="210564481">Well i'd start with the use case. What kind of collection processing is being done in scripts that requires e.g. streams api? This does not sound efficient whatsoever to be doing per-document.

In general creating/processing collections per-doc just seems very trappy to me.  I think things should be organized so this is not necessary: hell, I think even arrays are trappy, e.g. why even whitelist Arrays.sort?

Separately, if the decision is made to just allow more horribly trappy performance here, fine, but be careful. Some of these APIs (like parallelStreams) try to create/interact with threads and stuff. That is never ok :)
</comment><comment author="bsideup" created="2016-04-15T18:58:19Z" id="210596797">Hey @rmuir,

Generally: our use case - replace **existing**, **non-efficient** Groovy collection-based methods with Streams. That said, **existing** ones :)

Of course we know about performance and memory impact of such operations. But there are some rare cases where we really need to do scripted aggregations.

Also, we're not processing collections per-doc, we process them in combine and reduce scripts ( https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-scripted-metric-aggregation.html ). 

Even example from that page might be replaced:
`IntStream.of(_aggs).sum()`
instead of
`profit = 0; for (a in _aggs) { profit += a }; return profit`

in fact, for some heavy computations, even parallelStream will make sense, and actually will be faster than sequential processing. Of couse we will not use it everywhere, but parallelStream **was** designed to handle parallel computation of non-IO stuff. 
</comment><comment author="rmuir" created="2016-04-15T19:47:35Z" id="210614884">parallelstream is not an option for scripts. that is why i said it is never ok. you can whitelist it all you want: it will fail.
</comment><comment author="jasontedor" created="2016-04-21T16:19:33Z" id="212992923">There's no difference between your two sums examples other than the brevity of the code but they have the same effect.

Regarding the parallel streams, I think what is missing from this conversation is that to execute in parallel, threads are needed. Those threads have to come from a fork-join pool. We aren't going to provide one, so they will come from the common fork-join pool but we aren't going to allow that either. This is what @rmuir was getting at.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error installing elasticsearch plugin </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17708</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha1

**JVM version**: 1.8.0_51

**OS version**: CentOS 6.6

**Description of the problem including expected versus actual behavior**:
I try to install elasticsearch plugin such as elasticsearch-head and elasticsearch-kopf. The example below occurs on both elasticsearch-head and elasticsearch-kopf.

I run `./bin/elasticsearch-plugin install mobz/elasticsearch-head`, and I got MalformedURLException: no protocol. It seems that elasticsearch-5.0.0-alpha1 asking for full URL.

If I supply another full URL to install: 
`./bin/elasticsearch-plugin install https://github.com/mobz/elasticsearch-head`
I got this error message:
`ERROR: `elasticsearch` directory is missing in the plugin zip`

Looks like there are some changes in the elasticsearch plugin module. Is this a bug or expected behavior? Does the package require specific folder or specific stuffs to be compatible to elasticsearch-5.0.0-alpha1?

Thanks!
</description><key id="148022050">17708</key><summary>Error installing elasticsearch plugin </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">philipskokoh</reporter><labels /><created>2016-04-13T10:57:26Z</created><updated>2017-07-24T22:39:45Z</updated><resolved>2016-04-13T11:54:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-13T11:54:28Z" id="209391876">Duplicates #17637.
</comment><comment author="dadoonet" created="2016-04-13T11:57:44Z" id="209394579">Site plugins are not supported anymore. See https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_plugins.html#_site_plugins_removed
</comment><comment author="bonzofenix" created="2016-11-08T15:16:36Z" id="259163841">Working link: 
https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_plugins.html#_site_plugins_removed
</comment><comment author="jasontedor" created="2016-11-08T15:19:04Z" id="259164574">That link will grow stale as new versions are released and current is incremented. A permanent link is https://www.elastic.co/guide/en/elasticsearch/reference/5.0/breaking_50_plugins.html#_site_plugins_removed
</comment><comment author="UppalaPreetham" created="2017-03-01T23:14:41Z" id="283502812">I tried bin\elasticsearch.bat

giving me the fallowing error 
The system cannot find the path specified.
</comment><comment author="Whally01" created="2017-05-15T19:23:20Z" id="301577388">**OS: Windows 10** 
With all of the version I have an error:
- `ERROR:Unknown plugin mobz/elasticsearch-head` (+:ver) 
OR if use
 - `bin&gt; elasticsearch-plugin install https://github.com/mobz/elasticsearch-head `

`ERROR: elasticsearch directory is missing in the plugin zip`</comment><comment author="nik9000" created="2017-05-15T19:31:34Z" id="301579505">&gt; Site plugins are not supported anymore.

</comment><comment author="yao23" created="2017-07-24T22:39:45Z" id="317575284">the documentation states that this plugin is intended to be installed on ES 2.3.x, not on ES 5.x. It's a checkup BEFORE you upgrade to 5.x

https://github.com/elastic/elasticsearch-migration/issues/64</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Significant_terms aggregation Java API returns unformatted strings for Dates and IPs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17707</link><project id="" key="" /><description>The `date` and `ip` field types return unformatted strings for Bucket.getKeyAsString() in the `signficant_terms` aggregation responses. These values, like the REST api should return formatted values.
</description><key id="148021906">17707</key><summary>Significant_terms aggregation Java API returns unformatted strings for Dates and IPs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2016-04-13T10:56:41Z</created><updated>2016-04-22T11:02:56Z</updated><resolved>2016-04-22T10:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2016-04-22T10:59:14Z" id="213378974">This issue may have become irrelevant. 
As a result of the https://github.com/elastic/elasticsearch/pull/17746 change it is not possible to to run significant_terms aggs on numeric-based fields and users will need to define a special string-based mapping for significant_terms analysis e.g:

```
        "myIpField": {
           "type": "ip",
           "fields":{
               "asKeyword":{
                   "type":"keyword"
               }
           }
        }
```

..and then run sig terms analysis on the `myIpField.asKeyword` version of the field.

We may return to this formatting issue if we figure out a way to reintroduce support for significant_terms on numeric types with their new scheme for Lucene indexing.
</comment><comment author="markharwood" created="2016-04-22T11:02:56Z" id="213380597">For the record, the formatting solution was simple - it was copying the format.format() call found in `SignificantLongTerms.Bucket.toXContent` method for the key_as_string JSON property and applying it to the corresponding `getKeyAsString()` method used by Java clients.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>histogram-aggregation.asciidoc: tiny edit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17706</link><project id="" key="" /><description /><key id="148021469">17706</key><summary>histogram-aggregation.asciidoc: tiny edit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">golubev</reporter><labels><label>docs</label></labels><created>2016-04-13T10:54:03Z</created><updated>2016-04-13T12:20:33Z</updated><resolved>2016-04-13T12:18:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T12:18:40Z" id="209402100">thanks @golubev 
</comment><comment author="golubev" created="2016-04-13T12:20:33Z" id="209402601">Thanks, @clintongormley!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms aggregations parse failure for date and ip fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17705</link><project id="" key="" /><description>The `include` clause on the `terms` and `significant_terms` aggregations has a parse failure for string values provided for date and ip field types.
Steps to reproduce:

```
PUT test
{
   "settings": {
      "number_of_replicas": 0,
      "number_of_shards":1
   },
   "mappings": {
      "test": {

         "properties": {
            "stringField": {
               "type": "string"
            },
            "dateTimeField": {
               "type": "date",
                "format": "date_time"
            },
            "ipField": {
               "type": "ip"
            }
         }
      }
   }
}

POST test/test
{"stringField":"bar","ipField":"192.168.1.0", "dateTimeField":"2016-04-11T14:02:39.593Z"}

GET test/_search
{
    "size":0,
   "aggs": {
      "this_works": {
         "terms": {
            "field": "stringField",
            "include":["bar"]            
         }
      }
   }
}
GET test/_search
{
    "size":0,
   "aggs": {
      "this_does_not_work": {
         "terms": {
            "field": "ipField",
            "include":["192.168.1.0"]

         }
      }
   }
}
GET test/_search
{
    "size":0,
   "aggs": {
      "this_does_not_work": {
         "terms": {
            "field": "dateTimeField",
            "include":["2016-04-11T14:02:39.593Z"]

         }
      }
   }
}
```

The error returned is of the following type

```
{
   "error": {
      "root_cause": [
         {
            "type": "number_format_exception",
            "reason": "For input string: \"2016-04-11T14:02:39.593Z\""
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query_fetch",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test",
            "node": "Qz_UtnGzTOeYEDLeOgTQLw",
            "reason": {
               "type": "number_format_exception",
               "reason": "For input string: \"2016-04-11T14:02:39.593Z\""
            }
         }
      ]
   },
   "status": 400
}
```
</description><key id="148020974">17705</key><summary>Terms aggregations parse failure for date and ip fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2016-04-13T10:51:37Z</created><updated>2016-05-18T15:23:48Z</updated><resolved>2016-05-18T15:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2016-04-22T16:52:00Z" id="213508570">As part of fixing this I've also discovered that the formatting of IP bucket keys for terms agg has changed in master. Previously we had a `key` value that was a long and a formatted 'key_as_string' - now we just have a single `key` value with an ugly encoding as can be seen in this screenshot:
![sense - a json aware interface to elasticsearch](https://cloud.githubusercontent.com/assets/170925/14748517/0d379772-08b2-11e6-8f35-35af68740675.jpg)
This is a side-effect of switching over to byte representations for IPV6 I expect and consequently the shift to StringTerms for the buckets.
I'm not sure how much of a problem the ugly `key` is is but I will add back a friendlier "key_as_string" addition for readable IP addresses.
</comment><comment author="clintongormley" created="2016-04-25T18:47:23Z" id="214476578">@jpountz only ip address fields still return `key_as_string` but this seems to be missing for point fields.
</comment><comment author="jpountz" created="2016-04-27T08:31:36Z" id="215010531">@clintongormley @markharwood I opened #18003.
</comment><comment author="markharwood" created="2016-04-27T15:18:04Z" id="215117644">I started this work on formatting include/exclude agg criteria appropriately but it's in the same areas Adrien is currently fixing with formatting agg results appropriately in https://github.com/elastic/elasticsearch/pull/18003

So blocked on 18003 for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve glossary to not refer to types as "like a table"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17704</link><project id="" key="" /><description>Closes #17673
</description><key id="148018693">17704</key><summary>Improve glossary to not refer to types as "like a table"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>review</label></labels><created>2016-04-13T10:39:41Z</created><updated>2016-04-13T12:29:48Z</updated><resolved>2016-04-13T12:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T12:29:25Z" id="209407235">LGTM
</comment><comment author="clintongormley" created="2016-04-13T12:29:43Z" id="209407329">thanks @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds serialisation of sigma to extended_stats_bucket pipeline aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17703</link><project id="" key="" /><description>Previously the sigma variable in the `extended_stats_bucket` pipeline aggregation was not being serialised in `ExtendedStatsBucketPipelineAggregator`. This PR fixes that.

It also corrects the initial value of sumOfSquares to be 0.

Closes #17701
</description><key id="148011782">17703</key><summary>Adds serialisation of sigma to extended_stats_bucket pipeline aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T10:07:30Z</created><updated>2016-05-02T12:06:34Z</updated><resolved>2016-04-14T07:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-04-13T13:49:38Z" id="209448262">LGTM.  Thanks for the fix :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Let ingest intercept bulk requests at the bulk shard transport level.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17702</link><project id="" key="" /><description>Today ingest intercepts bulk requests at the bulk transport level. All the bulk items get preprocessed one by one at the coordinating node and there is no parallelism. After that the bulk api splits the requests up in shard bulk requests and then bulk indexing happens in parallel. It would be nice if ingest also preprocesses bulk requests in parallel.

I have been playing around with this idea and come up with this PR for discussion.
Without this change the amount of time spent on ingestion is pretty high chuck on time comparing it to the total time spent (when indexing into indices with a few shards and higher). With this change the relative ingest time compare to the total time spent drops significantly. The change does look sane to me, but perhaps I overlooked something.
</description><key id="148011005">17702</key><summary>Let ingest intercept bulk requests at the bulk shard transport level.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>discuss</label></labels><created>2016-04-13T10:04:51Z</created><updated>2016-06-08T15:46:00Z</updated><resolved>2016-06-08T15:46:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-14T07:28:58Z" id="209804237">One problem I foresee with this... what happens if the pipeline changes the `_id`, `_routing`, `_index`, or `_parent`?  
</comment><comment author="martijnvg" created="2016-04-14T08:16:17Z" id="209821956">@clintongormley Good point, I didn't take this into account when thinking about this optimization.

My goal is to achieve parallelism by using existing infrastructure. This approach fits nicely with how things already work. I need to think a bit more how can increase the parallelism for bulk requests and  also make this work when `_routing`, `_index`, etc. is changed.
</comment><comment author="bleskes" created="2016-04-14T10:36:46Z" id="209872828">I share @clintongormley concerns and some more. This parallelism is only needed where people are using a single client. On top of that, the client does round robin on multiple nodes but rather sends all traffic to a single ES node. IMO this is a very specialized situation. Things like our clients, beats and logstash do not fall under category. Sadly simple benchmarks do. All in all, I'm not sure we should embark on this adventure until we see some real ingest usage and evaluate if it's needed.

Second, if we do want to this, I suggest introducing the parallelism in `PipelineExecutionService` and how it interacts with the bulk thread pool. @martijnvg and I discussed some options there which will allow us to keep all the flexibility we have today (but are still tricky, mostly around deciding things like queue size, parallelism levels etc.) 
</comment><comment author="martijnvg" created="2016-04-14T10:47:51Z" id="209876233">&gt; Second, if we do want to this, I suggest introducing the parallelism in PipelineExecutionService and how it interacts with the bulk thread pool. @martijnvg and I discussed some options there which will allow us to keep all the flexibility we have today (but are still tricky, mostly around deciding things like queue size, parallelism levels etc.)

My hope was that intercepting requests at the bulk shard level would solve this problem with introducing any complexity elsewhere. (both for clients sending data to a single node and official clients that round robin)

&gt; All in all, I'm not sure we should embark on this adventure until we see some real ingest usage and evaluate if it's needed.

Agreed, we should be cautious adding this. I'm in no rush at all to push this. I see this PR as a small experiment on how potentially things can be changed to improve things in certain scenarios. The official clients already to the right thing and this change wouldn't improve things as much as for the single client case in a multi node cluster. 
</comment><comment author="martijnvg" created="2016-06-08T15:46:00Z" id="224632213">Closing, we shouldn't push for this single bulk request optimization, how things are okay as they are now, when clients send bulk requests is parallel
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing buckets produce incorrect results in extended_stats_bucket aggregation when using _count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17701</link><project id="" key="" /><description>Run the following sense script against a cluster running on master:

```
PUT test/doc/1
{
  "i": 1
}

PUT test/doc/2
{
  "i": 2
}

PUT test/doc/3
{
  "i": 4
}

PUT test/doc/4
{
  "i": 4
}

PUT test/doc/5
{
  "i": 5
}

PUT test/doc/6
{
  "i": 6
}

GET test/_search
{
  "size": 0,
  "aggs": {
    "histo": {
      "histogram": {
        "field": "i",
        "interval": 1
      },
      "aggs": {
        "moving_avg": {
          "moving_avg": {
            "buckets_path": "_count"
          }
        }
      }
    },
    "ext_stats_1": {
      "extended_stats_bucket": {
        "buckets_path": "histo&gt;moving_avg",
        "sigma": 1.0
      }
    },
    "ext_stats_2": {
      "extended_stats_bucket": {
        "buckets_path": "histo&gt;moving_avg",
        "sigma": 2.0
      }
    },
    "ext_stats_3": {
      "extended_stats_bucket": {
        "buckets_path": "histo&gt;moving_avg",
        "sigma": 3.0
      }
    }
  }
}
```

Note that there is no document with `"i": 3` and 2 documents with `"i": 4`. The request asks for 3 `extended_stats_bucket` aggregations with sigma values of 1, 2 and 3. The response you get for the `extended_stats_bucket` aggregations is:

```
    "ext_stats_1": {
      "count": 4,
      "min": 0.6666666666666666,
      "max": 1,
      "avg": 0.9166666666666666,
      "sum": 3.6666666666666665,
      "sum_of_squares": 4.444444444444445,
      "variance": 0.2708333333333335,
      "std_deviation": 0.5204164998665334,
      "std_deviation_bounds": {
        "upper": 0.9166666666666666,
        "lower": 0.9166666666666666
      }
    },
    "ext_stats_2": {
      "count": 4,
      "min": 0.6666666666666666,
      "max": 1,
      "avg": 0.9166666666666666,
      "sum": 3.6666666666666665,
      "sum_of_squares": 4.444444444444445,
      "variance": 0.2708333333333335,
      "std_deviation": 0.5204164998665334,
      "std_deviation_bounds": {
        "upper": 0.9166666666666666,
        "lower": 0.9166666666666666
      }
    },
    "ext_stats_3": {
      "count": 4,
      "min": 0.6666666666666666,
      "max": 1,
      "avg": 0.9166666666666666,
      "sum": 3.6666666666666665,
      "sum_of_squares": 4.444444444444445,
      "variance": 0.2708333333333335,
      "std_deviation": 0.5204164998665334,
      "std_deviation_bounds": {
        "upper": 0.9166666666666666,
        "lower": 0.9166666666666666
      }
    }
```

Note that `std_deviation_bounds.upper` and `std_deviation_bounds.lower` are the same for all three aggs and are all equal to the `avg`.
</description><key id="147996424">17701</key><summary>Missing buckets produce incorrect results in extended_stats_bucket aggregation when using _count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T09:11:46Z</created><updated>2016-04-14T07:29:58Z</updated><resolved>2016-04-14T07:29:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>IP range aggregation will not work anymore once points are integrated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17700</link><project id="" key="" /><description>The ip range aggregation uses the numeric representation of ip addresses to compute the ranges. However once points are integrated, ip addresses will be stored in doc values using a SORTED_SET (binary) representation. So we have two options:
- remove the ip range aggregation and tell users to use filters aggregations instead
- add support for range aggregations on SORTED_SET doc values
</description><key id="147994853">17700</key><summary>IP range aggregation will not work anymore once points are integrated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>blocker</label><label>v5.0.0-alpha3</label></labels><created>2016-04-13T09:07:02Z</created><updated>2016-05-13T15:23:37Z</updated><resolved>2016-05-13T15:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T12:09:28Z" id="209399350">I've never heard of anybody using the ip range agg.  @rashidkpc I see that this is supported in kibana, but do you have any idea if it is actually used?
</comment><comment author="rashidkpc" created="2016-04-13T14:32:07Z" id="209475736">It was asked for, so I assume someone needed it. I can certainly see its use it operations applications.
</comment><comment author="jpountz" created="2016-04-14T13:51:58Z" id="209953154">Discussions on #17746 also suggest that we might want to implement it. This will also be useful for big integers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>InternalTestCluster.getRandomNodeAndClient is not random</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17699</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.1

**JVM version**: 1.7.0_25

**OS version**: OS X 10.9.5 (13F34)

**Description of the problem including expected versus actual behavior**:
Expected: `InternalTestCluster.getRandomNodeAndClient` randomise the `NodeAndClient`
Actual: `InternalTestCluster.getRandomNodeAndClient` is not random. When calling this method in a test the same `NodeAndClient` is returned (since the nodes are sorted and the last one is always the last one)

**Steps to reproduce**:
1. Run an ESIntegTestCase test with few nodes
2. Call `internalTestCluster.smartClient()` many times
3. Always get the same `NodeAndClient`
</description><key id="147990808">17699</key><summary>InternalTestCluster.getRandomNodeAndClient is not random</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">bbrutman</reporter><labels><label>test</label></labels><created>2016-04-13T08:52:57Z</created><updated>2016-04-13T13:46:02Z</updated><resolved>2016-04-13T13:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-13T11:59:37Z" id="209395384">I just checked the code in master and 2.2 - the implementation looks a bit strange but I believe it should return a random entry from the list of nodes that satisfies the predicate. It picks a random number between 0 and the size of the list of nodes that pass the predicate and then walks the list. I haven't tried running it and checking the debugger but it sure looks random to me. Are you sure you are getting the same node when you have more than one node that matches the predicate?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rebalancing policy shouldn't prevent hard allocation decisions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17698</link><project id="" key="" /><description>#14259 added a check to honor rebalancing policies (i.e., rebalance only on green state) when moving shards due to changes in allocation filtering rules. The rebalancing policy is there to make sure that we don't try to even out the number of shards per node when we are still missing shards. However, it should not interfere with explicit user commands (allocation filtering) or things like the disk threshold wanting to move shards because of a node hitting the high water mark.
#14259 was done to address #14057 where people reported that using allocation filtering caused many shards to be moved at once. This is however a none issue - with 1.7 (where the issue was reported) and 2.x, we protect recovery source nodes by limitting the number of concurrent data streams they can open (i.e., we can have many recoveries, but they will be throttled). In 5.0 we came up with a simpler and more understandable approach where we have a hard limit on the number of outgoing recoveries per node (on top of the incoming recoveries we already had).
</description><key id="147985131">17698</key><summary>Rebalancing policy shouldn't prevent hard allocation decisions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Allocation</label><label>bug</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-13T08:31:32Z</created><updated>2017-01-26T14:54:19Z</updated><resolved>2016-04-13T18:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-13T08:31:43Z" id="209302023">@ywelsch can you take a look?
</comment><comment author="ywelsch" created="2016-04-13T08:39:33Z" id="209305950">Left one comment to document decision. Feel free to push afterwards. LGTM. Thanks @bleskes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>log4j 1.2.17 has reached End of life. Plans of upgrade?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17697</link><project id="" key="" /><description>**Elasticsearch version**: 2.0.2/2.1.0

As the version of log4j 1.2.17 lib used by ES has reached end of life as on Aug ,2015(https://logging.apache.org/log4j/1.2/index.html), and is recommend to migrate to log4j 2.X version what are the plans of ES team to use new versions of log4j. 
In what releases of ES can we expect new versions of log4j. If any security vulnerability are reported on log4j 1.2.17 lib in the future, what are the plans of mitigating the risks so that affects would be minimal. 
</description><key id="147947814">17697</key><summary>log4j 1.2.17 has reached End of life. Plans of upgrade?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">SKumarMN</reporter><labels><label>:Logging</label><label>blocker</label><label>enhancement</label><label>v5.0.0-beta1</label></labels><created>2016-04-13T05:11:41Z</created><updated>2016-09-14T14:44:25Z</updated><resolved>2016-09-01T03:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-15T09:53:06Z" id="210395130">Discussed in FixitFriday: we need to upgrade indeed. Since the configuration format will likely have some minor differences, this is probably a good opportunity to remove the layer we have on top of log4j to use a yaml configuration file. This would be more user friendly as the log4j documentation would be directly applicable.
</comment><comment author="nik9000" created="2016-04-15T11:37:20Z" id="210427437">If we upgrade do we go to log4j 2 or something like that?
On Apr 15, 2016 5:53 AM, "Adrien Grand" notifications@github.com wrote:

&gt; Discussed in FixitFriday: we need to upgrade indeed. Since the
&gt; configuration format will likely have some minor differences, this is
&gt; probably a good opportunity to remove the layer we have on top of log4j to
&gt; use a yaml configuration file. This would be more user friendly as the
&gt; log4j documentation would be directly applicable.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/17697#issuecomment-210395130
</comment><comment author="jasontedor" created="2016-04-15T11:56:45Z" id="210437373">&gt; If we upgrade do we go to log4j 2 or something like that?

Yeah, the proposal is:
- upgrade to log4j 2
- drop our custom log4j configuration and just enable regular log4j configuration
- target a major release, probably not 5.0
- fork log4j 1 if the need arises
</comment><comment author="dadoonet" created="2016-04-15T12:02:15Z" id="210438463">I spoke some years ago with Log4J (v1) author about this and he told me that we should consider [logback](http://logback.qos.ch/) instead. Do we consider benchmarking one or the other solutions around? May be there are no difference and it's a no brainer though?
</comment><comment author="rjernst" created="2016-04-15T15:50:30Z" id="210516611">&gt; drop our custom log4j configuration and just enable regular log4j configuration

I don't think we should do that. It would force us to always stay with log4j, and would have to reimplement log4j configuration in order to eg try using java logging.
</comment><comment author="nik9000" created="2016-04-15T16:32:28Z" id="210532809">&gt; I don't think we should do that. It would force us to always stay with log4j, and would have to reimplement log4j configuration in order to eg try using java logging.

We have that problem already. Our logging configuration now is just a thin wrapper around log4j so we'd have to reimplement it if we wanted continuity. I think we're better off telling users "the logging configuration is log4j 2.x" or "the logging configuration is jdk built in" and then telling them about the 6 or 8 interesting loggers. Then they can use the massive wealth of knowledge on the internet about configuring logging in those environments. They'll still need to know a bit about customizing it for elasticsearch but it won't be as specific as what we have now.
</comment><comment author="jmoney8080" created="2016-04-17T04:54:50Z" id="210956287">Instead of the "custom" logging layer why isn't slf4j used. Logback and log4j2 both support the api. There are bridges too that allow log4j 1.x to delegate to the slf4j api which can then be backed by log4j2 to "bridge" a migration. 

If a logging refactor is too happen might I suggest that be part of it to decouple future logging implementation upgrades. 
</comment><comment author="jasontedor" created="2016-04-17T07:40:29Z" id="210971287">The simplest argument against is that means we add a dependency (slf4j plus a binding that we have to ship with for a net addition of a dependency), and we don't achieve our goal of keeping the number of logging configurations that we support to a minimum. We are going to keep it simple here.

Relates #16585
</comment><comment author="ppf2" created="2016-06-30T19:18:47Z" id="229760983">One popular request is to provide an appender that will compress the ES logs _and_ have MaxFileSize + MaxBackUpIndex.  Currently, we have a commented out appender users can potentially use for gzipping of the logs.  With log4j2, there may be an opportunity here to update the extrasRollingFile appender example commented out in the logging.yml so that it will not just compress the logs, but allow users to set a file size limit + number of backups.  Thx!
</comment><comment author="rmuir" created="2016-07-14T16:10:16Z" id="232712921">Its not just about security releases, for example some features of log4j 1.x are broken with java 9, we have a hack around that: https://github.com/elastic/elasticsearch/blob/f01f15d3b8592db4210f725f0c37baff4a554a35/core/src/main/java/org/apache/log4j/Java9Hack.java

I think the should elevate the priority of this. This is not the place to bikeshed loggers (which are some of the largest bikesheds on the planet, by definition), instead we should simply move to something that is actively supported, and bikeshed on a separate issue.

Or we can use System.out.println for all logging, I am fine with that, it is supported method by the JDK.
</comment><comment author="s1monw" created="2016-07-18T08:54:30Z" id="233276834">I think we should prioritize this even for 5.0? 

&gt; target a major release, probably not 5.0

@jasontedor any reasons?

@clintongormley I wonder if this should be a blocker?
</comment><comment author="jasontedor" created="2016-07-18T09:49:16Z" id="233287484">&gt; any reasons?

That comment was made in April and is out of date now. We are now intending to target 5.0.0.
</comment><comment author="s1monw" created="2016-07-18T10:26:56Z" id="233294411">thanks @jasontedor 
</comment><comment author="bosinm" created="2016-08-05T11:26:17Z" id="237827220">Another common request is to have the option of combining MaxBackUPIndex from RollingFileAppender with DailyRollingFileAppender.  The idea is to have automatic deletion of the rolled files after X number of days.
So where the logger finally selected should be able to combine both functionalities . Thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch children aggregation to registerAggregation and remove its PROTOTYPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17696</link><project id="" key="" /><description>This required some invasive surgery on the serialization but it ought to give us a way to do one aggregation at a time.
</description><key id="147897664">17696</key><summary>Switch children aggregation to registerAggregation and remove its PROTOTYPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T23:00:03Z</created><updated>2016-05-02T12:15:03Z</updated><resolved>2016-04-13T13:40:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-13T10:16:29Z" id="209353929">LGTM. I left one minor comment. The `usesNewStyleSerialization` stuff isn't so nice but I know why you had to do it that way and I'm fine with it as a temporary fix to enable use to migrate the aggs one at a time. It will be good when all the aggregations are moved over and we can remove this.
</comment><comment author="nik9000" created="2016-04-13T13:33:20Z" id="209442425">Thanks for the review @colings86 ! I'm going to merge this and try and do one of the more complicated ones next.
</comment><comment author="colings86" created="2016-04-13T13:36:04Z" id="209443504">Cool, I would suggest doing something like the Range or Histogram Aggregation next if you want one of the more complex ones (although hopefully it will still be quite simple to do)
</comment><comment author="nik9000" created="2016-04-13T13:39:58Z" id="209444823">&gt; Range or Histogram Aggregation

Cool! I'm just waiting on the tests to pass one last time. This is one of those changes where one misplaced `}` could cause sneaky problems. The `usesNewStyleSerialization` hack would be unpleasant to keep for very long....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shadow Replica indexes do not delete properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17695</link><project id="" key="" /><description>This issue is to document deletion problems with shadow replica indices that were found while working on #17265.  A separate PR #17638 that improves the naming of methods in the `IndicesService` also contains tests or added assertions to existing tests that reveal the issues below and must be enabled as part of any PR that fixes the issues.

**No. 1**
The index file deletion logic that is triggered in `IndicesService#deleteIndexStore(String reason, Index index, IndexSettings indexSettings` checks before deleting files to see if the index is not a shadow replica, or if it is, ensure that it has been closed before (so that no other nodes are holding resources to it).  An issue with this is that it is too strict of a check, so that if a shadow replica index is deleted, if it was not previously closed, the index folder itself is not deleted and remains on the file system (an empty folder).   So one of the issues that needs fixing is to ensure index directories are deleted even on shadow replica index deletes.  The following tests have commented out assertions to test this behavior once fixed:
- `IndexWithShadowReplicaIT#testIndexWithShadowReplicasCleansUp` 
- `IndexWithShadowReplicaIT#testShadowReplicaNaturalRelocation` 

Note that shared shard data is cleaned up properly in a shadow replica index that is not closed, as the shard data is deleted by the `StoreCloseListener`.  This is verified in the tests with the `assertPathHasBeenCleared` assert.

**No. 2**
The issue with deleting a shadow replica index that was previously closed is that all of the index and shard data are potentially deleted simultaneously by each node that receives the delete operation and invokes `NodeEnvironment#deleteIndexDirectorySafe`.  This can lead to race conditions where a node is trying to delete a file that was deleted by another node as both are walking the file system simultaneously (using Lucene's `IOUtils.rm`).  This ends up logged as a warning in `IndicesService#deleteIndexStore(String reason, Index index, IndexSettings indexSettings` and the deletion is put on the pending queue.  
</description><key id="147889808">17695</key><summary>Shadow Replica indexes do not delete properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Shadow Replicas</label><label>bug</label></labels><created>2016-04-12T22:16:10Z</created><updated>2017-01-12T10:42:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-13T07:36:42Z" id="209281692">&gt; none of its index files are deleted

To be clear - shard level folders are deleted and the index metadata is deleted as well. The problem is that we leave an empty folder behind.

Re No 2.:

Do we have specific issue with master nodes or is it specifically about nodes that do not host a shard of the shard from the shadow index? (i.e., master nodes but also some data nodes)
</comment><comment author="abeyad" created="2016-04-13T16:07:05Z" id="209525625">&gt; To be clear - shard level folders are deleted and the index metadata is deleted as well. The problem is that we leave an empty folder behind.

Yes, that is correct.  I changed the description to be more clear on this.

&gt; Do we have specific issue with master nodes or is it specifically about nodes that do not host a shard of the shard from the shadow index? (i.e., master nodes but also some data nodes)

I think our tests prove that this isn't a specific issue, but only a general one as it relates to index folders not getting cleaned up.  Since we are dropping the `testDeletingIndexWithDedicatedMasterNodes`, I will remove this point from the description as well.

I believe the fundamental problems with shadow replica deletion are 1. the leaving behind of these (empty) index folders and 2. when a shadow replica has been closed and then deleted, all nodes try to delete the same shared shard data folder, in which case, the first node succeeds, but the remaining nodes try to delete already deleted files (with the potential for race conditions here if multiple nodes are deleting at the same time).  This throws a warning in the logs and puts the delete in the pending queue.  

I will update the description of this issue accordingly.
</comment><comment author="bleskes" created="2016-05-12T10:03:09Z" id="218713755">@abeyad was there any discussion on this ? if not we should pick it and make a plan.
</comment><comment author="abeyad" created="2016-05-12T13:31:39Z" id="218757655">@bleskes nothing further has been done on this, its worth a discussion to see how best to handle the aforementioned scenarios.
</comment><comment author="abeyad" created="2016-05-12T14:30:35Z" id="218775181">Upon discussion with Boaz, these issues are known and have been there for a long time.  They don't cause incorrect behavior, they just prevent shadow replicas from deleting cleanly.  It remains to be discussed if this is important to tackle and if so, what the appropriate solutions are.

@bleskes @clintongormley FYI
</comment><comment author="dakrone" created="2016-08-08T19:53:44Z" id="238355950">@abeyad @clintongormley how do we feel about this being a blocker for 5.0? It's currently marked as a blocker due to a `// norelease` in `IndexWithShadowReplicasIT` pointing to this issue
</comment><comment author="abeyad" created="2016-08-08T20:16:57Z" id="238362862">I'm inclined to say these are not blockers, because they result in improper full cleanup (empty directories laying around) or simultaneous deletes that are logged as such without tripping any errors.  Also, adding the failed deletes to the pending queue just means they will get executed later and essentially be no-ops because there will be nothing to delete (as the deletion was already done successfully by one of the nodes).  

So, its not great form, but given that shadow replicas aren't a ubiquitous feature and this issue doesn't actually cause incorrect behavior, I don't feel its a blocker.

If @clintongormley agrees, I can remove the the `// norelease` from the test.

@dakrone what do you think?
</comment><comment author="clintongormley" created="2016-08-11T17:04:09Z" id="239224557">+1
</comment><comment author="abeyad" created="2016-08-11T17:09:53Z" id="239226089">I removed the blocker label and removed the `//norelease` in 50b31ce6202683917bd007fde022d5019a501453
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No map collisions on FiltersTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17694</link><project id="" key="" /><description>Adds randomUnique to generate unique things and uses it to make unique
keys.

The offending seed was 81AE616FEAD10F17.
</description><key id="147886759">17694</key><summary>No map collisions on FiltersTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>review</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T21:59:28Z</created><updated>2016-04-13T12:40:39Z</updated><resolved>2016-04-13T12:40:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-12T22:19:16Z" id="209126612">@cbuescher I remember you did a thing like this recently and I found another place where it is needed so I made a utility.
</comment><comment author="cbuescher" created="2016-04-13T09:11:50Z" id="209324760">I like it, left one comment to be more clear about the limitations of the requested set size in comment and naming of the arguments, other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic setting of &#8220;cluster.routing.allocation.same_shard.host: true&#8221; is not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17693</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.0

**JVM version**: jre1.8.0_73

**OS version**: Windows Server 2012 R2

**Description of the problem including expected versus actual behavior**:
Update "cluster.routing.allocation.same_shard.host": "true" successfully in Sense, but when doing a GET cluster settings, the "cluster.routing.allocation.same_shard.host": "true" does not show up in either persistent or transient section, so I have no idea if the settings are working or not.
More details: https://discuss.elastic.co/t/dynamic-setting-of-cluster-routing-allocation-same-shard-host-true-is-not-working/45644
**Steps to reproduce**:
1. curl -XPUT http://localhost:9200/_cluster/settings -d '{ "persistent" : {"cluster.routing.allocation.same_shard.host": "true"}}'
2. curl -XGET http://localhost:9200/_cluster/settings

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="147885424">17693</key><summary>Dynamic setting of &#8220;cluster.routing.allocation.same_shard.host: true&#8221; is not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anhlqn</reporter><labels /><created>2016-04-12T21:54:39Z</created><updated>2016-10-06T05:56:54Z</updated><resolved>2016-04-12T21:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-12T21:58:59Z" id="209121270">Hi @anhlqn,

The reason for this is that is `"cluster.routing.allocation.same_shard.host"` is not a dynamic setting, it needs to be set at startup time currently. The settings infrastructure in 5.0.0 will help with identifying settings like these
</comment><comment author="anhlqn" created="2016-04-12T22:03:47Z" id="209122756">@dakrone does it mean I have to put the setting in elasticsearch.yml of all ES nodes including master and data?
</comment><comment author="dakrone" created="2016-04-13T01:34:47Z" id="209182043">@anhlqn yes that's correct
</comment><comment author="Sartner" created="2016-10-06T05:56:54Z" id="251873747">@dakrone 
Can I only update master node config and rolling restart (only the masters)?
Because restart all nodes will take a long time

Or how to do it with out restart?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Grok processor documentation is incorrect or out of date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17692</link><project id="" key="" /><description>The docs say the default grok patterns can be found in a file at `$ES_HOME/config/ingest/grok/patterns`, but I'm not seeing that file in the alpha1 distribution. This might be extra confusing if the user goes to add their own custom patterns and the directory doesn't exist.
</description><key id="147883241">17692</key><summary>Grok processor documentation is incorrect or out of date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bargs</reporter><labels><label>:Ingest</label><label>docs</label></labels><created>2016-04-12T21:44:53Z</created><updated>2016-04-13T21:24:47Z</updated><resolved>2016-04-13T10:38:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-13T09:46:12Z" id="209341740">@Bargs That should have been removed. Custom patterns can only be defined inside the processor under `pattern_definitions` setting. 
</comment><comment author="Bargs" created="2016-04-13T14:25:17Z" id="209472937">Thanks @martijnvg. I noticed there's one other mention of it too: https://github.com/elastic/elasticsearch/blame/0f00c14afc8428a2a72c0b766d2171029dc8f6e1/docs/reference/ingest/ingest-node.asciidoc#L917
</comment><comment author="martijnvg" created="2016-04-13T20:52:06Z" id="209642774">@Bargs you're right! I removed that too: https://github.com/elastic/elasticsearch/commit/16fa3e546e172d3d194c2711654824d4851d69f8
</comment><comment author="Bargs" created="2016-04-13T21:24:47Z" id="209652752">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reformat DirectCandidateGenerator's equals method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17691</link><project id="" key="" /><description>We'd like to have checkstyle complain about if statements without braces
for all non-single-line if statements.
</description><key id="147871513">17691</key><summary>Reformat DirectCandidateGenerator's equals method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T21:01:33Z</created><updated>2016-05-02T12:15:11Z</updated><resolved>2016-04-12T21:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-12T21:13:19Z" id="209105250">LGTM
</comment><comment author="jasontedor" created="2016-04-12T21:24:08Z" id="209108761">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Skip RPM testing on SLES-11 but run all vagrant tests on SLES-11</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17690</link><project id="" key="" /><description>SLES-11's ancient version of rpm doesn't support the signing that we do in Elasticsearch 5.0. Rather than bend over backwards to support it we should remove the RPM testing on SLES-11. Other than that, though, SLES-11 should get .tar.gz testing.
</description><key id="147867316">17690</key><summary>Skip RPM testing on SLES-11 but run all vagrant tests on SLES-11</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>adoptme</label><label>build</label><label>test</label></labels><created>2016-04-12T20:42:34Z</created><updated>2016-04-13T11:10:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add the shard's store status to the explain API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17689</link><project id="" key="" /><description>This adds information similar to what is from the [shard stores
API](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/indices-shards-stores.html)
to the cluster allocation explanation API (in fact, internally it uses
that API).

This means when you have a decision that otherwise could indicate that a
shard can go somewhere, you now have more information:

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "QzoKda9aQCG_hCaZQ18GEg",
    "id" : 0,
    "primary" : true
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "CLUSTER_RECOVERED",
    "at" : "2016-04-11T20:58:04.088Z"
  },
  "allocation_delay" : "0s",
  "allocation_delay_ms" : 0,
  "remaining_delay" : "0s",
  "remaining_delay_ms" : 0,
  "nodes" : {
    "24Qmw4tdRTuVOtjAdtmr5Q" : {
      "node_name" : "Vampire by Night",
      "node_attributes" : { },
      "final_decision" : "YES",
      "weight" : 7.0,
      "decisions" : [ ],
      "store" : {
        "allocation_id" : "aC6qVWA7TT2pgsalYxxUJQ",
        "store_exception" : "IndexFormatTooOldException[Format version is not supported (resource BufferedChecksumIndexInput(SimpleFSIndexInput(path=\"/home/hinmanm/scratch/elasticsearch-5.0.0-alpha1-SNAPSHOT/data/elasticsearch/nodes/0/indices/QzoKda9aQCG_hCaZQ18GEg/0/index/segments_1\"))): -1906795950 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 5.0 and later.]",
        "allocation" : "UNUSED"
      }
    }
  }
}
```

The "store" section is the new section, and will include allocation, id,
and the exception if there is one.

Relates to #17372
</description><key id="147826554">17689</key><summary>Add the shard's store status to the explain API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-12T17:53:19Z</created><updated>2016-05-01T20:21:21Z</updated><resolved>2016-04-28T20:19:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-13T07:28:28Z" id="209273468">This is an awesome extension. I'm a bit worried by the output, if you don't know the internals - this sounds like we have decided to allocation the shard (due to the final decision), but we won't because the store is corrupted (unless this is a replica):

```
"final_decision" : "YES",
      "weight" : 7.0,
      "decisions" : [ ],
      "store" : {
        "allocation_id" : "aC6qVWA7TT2pgsalYxxUJQ",
        "store_exception" : "IndexFormatTooOldException[Format version is not supported (resource BufferedChecksumIndexInput(SimpleFSIndexInput(path=\"/home/hinmanm/scratch/elasticsearch-5.0.0-alpha1-SNAPSHOT/data/elasticsearch/nodes/0/indices/QzoKda9aQCG_hCaZQ18GEg/0/index/segments_1\"))): -1906795950 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 5.0 and later.]",
        "allocation" : "UNUSED"
      }
```

Also, I'm not sure the allocation section of the shard stores is relevant here. 
</comment><comment author="dakrone" created="2016-04-13T13:27:19Z" id="209440665">@bleskes with the output though, I didn't want to conflate the two (deciders and store) in the output because they aren't conflated in ES (completely separate things).

&gt; Also, I'm not sure the allocation section of the shard stores is relevant here. 

I can take it out if desired, but I thought it would be good to show as much info as possible, it can at least show the allocation status of the store if there are multiple unused allocations (makes it more clear that the store is in a bad state if it is marked as unused and has an exception)
</comment><comment author="clintongormley" created="2016-04-14T07:52:50Z" id="209813446">&gt; this sounds like we have decided to allocation the shard (due to the final decision), but we won't because the store is corrupted (unless this is a replica):

you're right.  it does sound like that.  what does it actually mean?
</comment><comment author="ywelsch" created="2016-04-14T08:10:55Z" id="209819807">&gt;  I didn't want to conflate the two (deciders and store) in the output because they aren't conflated in ES (completely separate things).

I feel that just putting shard stores information here is not sufficient. The explain API needs to explain why a shard is unallocated (e.g. no shard data found or no shard data with matching allocation id found, i.e. only stale copies). The good thing about allocation deciders is that they explicitly state what the issue is.

Maybe the explanation API can abstract from the implementation details of allocation deciders and just show how the steps that lead to an allocation decision to assign / not assign a shard. Such a step can be a decision from an allocation decider as well as other checks.
</comment><comment author="bleskes" created="2016-04-14T13:21:49Z" id="209938689">&gt;  what does it actually mean?

Not sure exactly, which part qualifies as "it" here, so I hope I clarify it but saying that the allocation deciders will not block the assignment on that node but a primary can not be assigned to it because the data copy on it is not valid.

&gt; Maybe the explanation API can abstract from the implementation details

It feels like this is indeed required in order to come up with an output that makes sense from this API's perspective alone. An alternative route it to have the decision made by the primary shard allocator wrapped in a allocation decider format.
</comment><comment author="dakrone" created="2016-04-14T14:42:02Z" id="209977539">Thanks for taking a look @bleskes, @ywelsch, and @clintongormley - I'll try to re-work the output to be friendlier for people not familiar with the concept of a store but still keep the good debugging information. I think I may use the `final_decision` as well for this (perhaps with an "ERROR" option when the store is corrupted)
</comment><comment author="dakrone" created="2016-04-14T20:04:39Z" id="210126011">I changed the output for this -

Happy case (meaning no corruption) output:

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "P4WksCZdS2ukiiNa6eF1Gw",
    "id" : 0,
    "primary" : false
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "CLUSTER_RECOVERED",
    "at" : "2016-04-14T19:55:02.019Z"
  },
  "allocation_delay" : "0s",
  "allocation_delay_ms" : 0,
  "remaining_delay" : "0s",
  "remaining_delay_ms" : 0,
  "nodes" : {
    "bG9sYnmJQiGuBz-dy_PJ2A" : {
      "node_name" : "Andrew Chord",
      "node_attributes" : { },
      "store" : {
        "shard_copy" : "PRIMARY"
      },
      "final_decision" : "NO",
      "weight" : 5.0,
      "decisions" : [ {
        "decider" : "same_shard",
        "decision" : "NO",
        "explanation" : "the shard cannot be allocated on the same node id [bG9sYnmJQiGuBz-dy_PJ2A] on which it already exists"
      } ]
    }
  }
}
```

This now only indicates the copy of the data that the node is using (in the
`shard_copy`), either primary or replica (or unused if there is an extra copy of
the data around that has not been removed).

Sad case output:

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "P4WksCZdS2ukiiNa6eF1Gw",
    "id" : 0,
    "primary" : true
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "CLUSTER_RECOVERED",
    "at" : "2016-04-14T19:57:25.753Z"
  },
  "allocation_delay" : "0s",
  "allocation_delay_ms" : 0,
  "remaining_delay" : "0s",
  "remaining_delay_ms" : 0,
  "nodes" : {
    "SWw_UzplSx-OlkshdZBw5A" : {
      "node_name" : "Letha",
      "node_attributes" : { },
      "store" : {
        "store_exception" : "CorruptIndexException[misplaced codec footer (file extended?): remaining=36, expected=16, fp=303 (resource=BufferedChecksumIndexInput(SimpleFSIndexInput(path=\"/home/hinmanm/scratch/elasticsearch-5.0.0-alpha1-SNAPSHOT/data/elasticsearch/nodes/0/indices/P4WksCZdS2ukiiNa6eF1Gw/0/index/segments_2\")))]",
        "shard_copy" : "UNUSED"
      },
      "final_decision" : "STORE_ERROR",
      "weight" : 6.0,
      "decisions" : [ ]
    }
  }
}
```

The exception is still show, as well as the shard copy being "UNUSED" (because
it's corrupt), but now the `final_decision` has changed to `STORE_ERROR` because
the shard store is corrupt and can't be used at all.

Allocation id and legacy id are gone from both outputs (these aren't really
useful for humans to know)
</comment><comment author="clintongormley" created="2016-04-15T07:59:56Z" id="210345456">Better, easier to understand.

&gt; The exception is still show, as well as the shard copy being "UNUSED" (because it's corrupt), but now the final_decision has changed to STORE_ERROR because the shard store is corrupt and can't be used at all.

I assume this means that there is no valid copy of the shard available on any node?  Wondering if `STORE_ERROR` should be `NO_PRIMARY_AVAILABLE` or something like that?
</comment><comment author="dakrone" created="2016-04-15T14:36:46Z" id="210485980">&gt; I assume this means that there is no valid copy of the shard available on any node?

`final_decision` is still per-node, so only on that node is there a STORE_ERROR

&gt; Wondering if STORE_ERROR should be NO_PRIMARY_AVAILABLE or something like that?

I don't think this makes sense, since not having a primary and having a primary but it's got an error are two different things
</comment><comment author="bleskes" created="2016-04-15T15:17:49Z" id="210501746">I think the new version is clearer as well. Thx Lee. Double checking - what happens when the shard we're explaining is a replica? In this case we may still allocated it to that node (but prefer others good copies to avoid data migration).

&gt; Wondering if STORE_ERROR should be NO_PRIMARY_AVAILABLE or something like that?

I would prefer STORE_ERROR as well. This is a per-node output and NO_PRIMARY_AVAILABLE sounds like a global cluster wide things (no node has a valid copy).

Last, when designing the output, I think it will be useful to consider what Yannick already mentioned - it may be that we found valid good stores, the allocation deciders are all good to go and still we don't allocated a shard because none of the allocation IDs match the previously active allocation ids in the meta data (i.e., the found copies are stale). How are we planning to reflect that? it might be as simple as saying `STALE_COPY` ?
</comment><comment author="dakrone" created="2016-04-15T17:23:22Z" id="210553774">&gt; what happens when the shard we're explaining is a replica? In this case we may still allocated it to that node (but prefer others good copies to avoid data migration).

The first example I posted was explaining a replica, the `shard_copy` is `PRIMARY` because that node currently has the primary copy of that shard, if other nodes had a replica copy it would have `"shard_copy": "REPLICA"` if it could be used or `"shard_copy": "UNUSED"` if there was a reason it couldn't be used (corruption). If there are no copies of the data, it'll just be `"store": {}`.

&gt; it may be that we found valid good stores, the allocation deciders are all good to go and still we don't allocated a shard because none of the allocation IDs match the previously active allocation ids in the meta data (i.e., the found copies are stale). How are we planning to reflect that? it might be as simple as saying `STALE_COPY` ?

Yes, I added a commit to track stale allocation ids and it will report `STALE_COPY` if a stale copy is found
</comment><comment author="dakrone" created="2016-04-19T15:16:21Z" id="211974271">@ywelsch since @bleskes is on vacation, could you review this? I'd like to get it in for the 5.0 release
</comment><comment author="ywelsch" created="2016-04-20T09:58:50Z" id="212359078">Had a first go at this. Two observations I would like to clarify first before diving deeper:

I think that using the allocation status of the store status `storeStatus.getAllocationStatus()` does not add anything to the output. This information repeats what is already captured by `shard.primary` and `assigned`. In addition it can be inconsistent with `shard.primary` and `assigned` as it might be based on a different cluster state (the one that's the most recent on the node from which we fetch the shard store information which can be a different than the one on the coordinating node), making it quite confusing.

In my opinion store information should convey exactly the following information:
1. Whether a shard copy is available or not
2. Whether it is a corrupt copy
3. Whether it is a stale copy

For uniformity I would prefer that the `shard_copy` field captures all these cases:
- For 1) I would prefer we explicitly mark `shard_copy` as `NONE` if no data is available on disk.
- For 2) I would like us to distinguish whether the copy is corrupt (checksums mismatch) or we cannot read the disk properly (Hopefully there is a way to distinguish this based on exception type). Then mark `shard_copy` as `CORRUPT` or `IO_ERROR` depending on the situation and add the `store_exception` (as you did).
- For 3) I would mark `shard_copy` as `STALE`.
- In case data is available and neither corrupt nor stale I would mark "shard_copy" as AVAILABLE.

The final decision should then simply be "YES", "NO", or "ALREADY_ASSIGNED".
In case of "NO" it should also have a human readable explanation (e.g. "shard copy is stale")

Second observation:
ClusterAllocationExplanation also has too much logic in `toXContent`. By using the Java API I would like to get the same information. This means that there should be an object reflecting the store information with a getter to access it.
</comment><comment author="dakrone" created="2016-04-25T18:44:42Z" id="214475758">Hi @ywelsch, thanks for the great feedback! I totally agree!

I've changed the `ClusterAllocationExplanation` to have no logic in `toXContent` and make everything available via the Java API, I've also added the final decision and explanation as well as the store copy output, here's what it looks like now:

Happy Case (non-corrupt):

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "uPnPR2pPSHOCU5bG-KVB-g",
    "id" : 0,
    "primary" : false
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "INDEX_CREATED",
    "at" : "2016-04-25T18:26:42.946Z"
  },
  "allocation_delay" : "0s",
  "allocation_delay_ms" : 0,
  "remaining_delay" : "0s",
  "remaining_delay_ms" : 0,
  "nodes" : {
    "Ocyk1ql4QaejNBP6UvMZhA" : {
      "node_name" : "Vegas",
      "node_attributes" : {
        "testattr" : "test"
      },
      "store" : {
        "shard_copy" : "AVAILABLE"
      },
      "final_decision" : "NO",
      "final_explanation" : "the shard cannot be assigned because one or more allocation decider returns a 'NO' decision",
      "weight" : 5.0,
      "decisions" : [ {
        "decider" : "same_shard",
        "decision" : "NO",
        "explanation" : "the shard cannot be allocated on the same node id [Ocyk1ql4QaejNBP6UvMZhA] on which it already exists"
      } ]
    }
  }
}
```

Corrupt case:

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "WxAuxGpdSX6ezspqgtsMyQ",
    "id" : 0,
    "primary" : true
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "CLUSTER_RECOVERED",
    "at" : "2016-04-25T18:36:03.208Z"
  },
  "allocation_delay" : "0s",
  "allocation_delay_ms" : 0,
  "remaining_delay" : "0s",
  "remaining_delay_ms" : 0,
  "nodes" : {
    "uyixwqV1Rfuwmot-gDL8IQ" : {
      "node_name" : "Shiver Man",
      "node_attributes" : { },
      "store" : {
        "shard_copy" : "CORRUPT",
        "store_exception" : "CorruptIndexException[misplaced codec footer (file extended?): remaining=32, expected=16, fp=179 (resource=BufferedChecksumIndexInput(SimpleFSIndexInput(path=\"/home/hinmanm/scratch/elasticsearch-5.0.0-SNAPSHOT/data/elasticsearch/nodes/0/indices/WxAuxGpdSX6ezspqgtsMyQ/0/index/segments_2\")))]"
      },
      "final_decision" : "NO",
      "final_explanation" : "the copy of data in the shard store is corrupt",
      "weight" : 6.0,
      "decisions" : [ ]
    }
  }
}
```

Please take another look and let me know what you think!
</comment><comment author="ywelsch" created="2016-04-26T14:19:35Z" id="214760743">@dakrone I left more comments. The trickiest part is probably how store information influences (or does not influence) final decision. I also wonder in what kind of situations we can run into `activeAllocationIds.empty() == true` and whether we can provide a more helpful message in that case.
</comment><comment author="dakrone" created="2016-04-26T19:10:59Z" id="214853789">Thanks again @ywelsch, I pushed more commits with your feedback.

&gt; I also wonder in what kind of situations we can run into activeAllocationIds.empty() == true and whether we can provide a more helpful message in that case.

So this occurs when recovering an older index that doesn't have allocation ids yet, I changed this to have a separate `StoreCopy` status and added a TODO to get the store info the "old way" in the future. This PR is already getting big so I'll leave it for the next one :)
</comment><comment author="ywelsch" created="2016-04-27T12:32:05Z" id="215067650">I like the new structure! I added more comments about `calculateNodeExplanation`, the method that encapsulates now the essential logic of this PR. Can you add a few more unit tests for that method? I also wonder whether we should separate the testing of JSON serialization and generation of explanation result (now that these classes are separated). For example, `testStaleShardExplanation` does a little bit of both.
</comment><comment author="dakrone" created="2016-04-27T16:47:53Z" id="215144552">@ywelsch okay, I added more tests for the node explanation calculation. For the JSON serialization, I test the serialization of the both the CAE and NE in there, I'm not sure if needs further explanation? (there is also an integ test that tests serialization).

Let me know what you think, I think it's getting close!
</comment><comment author="ywelsch" created="2016-04-28T08:42:29Z" id="215354166">@dakrone left two more minor comments regarding node explanation calculation, otherwise this LGTM. Thanks for adding this feature, it's much appreciated!
</comment><comment author="clintongormley" created="2016-05-01T17:09:34Z" id="216055932">@dakrone looks like the docs still need writing?
</comment><comment author="dakrone" created="2016-05-01T20:21:21Z" id="216069524">@clintongormley yep, I'm going to open a separate PR for the docs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add back the Version.V_5_0_0 constant.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17688</link><project id="" key="" /><description>This also skips alpha releases for backward-compatibility tests, otherwise I would have had to generate a bw index for the 5.0.0-alpha1 release.
</description><key id="147814241">17688</key><summary>Add back the Version.V_5_0_0 constant.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T17:02:47Z</created><updated>2016-04-13T08:02:46Z</updated><resolved>2016-04-13T08:01:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-12T23:15:14Z" id="209145661">LGTM
</comment><comment author="bleskes" created="2016-04-13T07:58:21Z" id="209292001">LGTM2
</comment><comment author="jpountz" created="2016-04-13T08:02:46Z" id="209292995">Thanks @rjernst  and @bleskes  for the second pair of eyes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Support for Deprecated REST Handler</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17687</link><project id="" key="" /><description>As we change our REST Endpoints, particularly with simple examples like going from `_optimize` to `_forcemerge`, we should use the deprecation logger to announce the deprecation to the user.

Hopefully it's as simple as adding something along the lines of 

``` java
controller.registerDeprecatedHandler(POST, "/_optimize", this);
controller.registerDeprecatedHandler(POST, "/{index}/_optimize", this);
```

I'm interested to hear what people might want to see as part of the deprecation notice though. I wouldn't want to make it too fancy, but it might be nice to say something along the lines of "see _forcemerge".

What do you think @clintongormley?
</description><key id="147813770">17687</key><summary>Add Support for Deprecated REST Handler</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>:REST</label><label>v5.0.0-alpha5</label></labels><created>2016-04-12T17:00:40Z</created><updated>2016-07-06T23:53:08Z</updated><resolved>2016-07-06T23:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2016-04-12T17:11:56Z" id="209011585">align the message with the deprecated settings message?

```
"Endpoint [xxxxxx] is deprecated, use [yyyyyy] instead"
```

do we also want:

``` java
controller.registerRemovedEndpoint(POST, "/_optimize");
```

that will emit:

```
"Endpoint [xxxxxx] has been removed, use [yyyyyy] instead"
```
</comment><comment author="jasontedor" created="2016-04-12T17:37:28Z" id="209021824">As a bonanza, adding an HTTP header containing the same might be useful (the client might not have access to the server logs where the deprecation logging would be emitted)?
</comment><comment author="clintongormley" created="2016-04-13T11:00:33Z" id="209367170">++

&gt; As a bonanza, adding an HTTP header containing the same might be useful (the client might not have access to the server logs where the deprecation logging would be emitted)?

I was going to comment that the clients don't expose headers at all, so this info would be hidden.  But actually, the clients all have logging facilities. I think the clients should look for these headers and warn if found.  This would improve the discoverability of deprecations (https://github.com/elastic/elasticsearch/issues/8963) which currently require deprecation logging to be specifically enabled.  It'd also make it easier to figure out which requests are using deprecated syntax.
</comment><comment author="pickypg" created="2016-04-13T17:56:37Z" id="209567657">Is there a preferred, existing header for this?

As a side note, I took a look for a better response code, and [there does exist `299` as a warn code](http://www.iana.org/assignments/http-warn-codes/http-warn-codes.xhtml#warn-codes) in the 2xx line of response codes. The other [2xx response codes seem more exact in nature](http://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml).

Along with 299, it also brings along some other headers that seem highly relevant: https://tools.ietf.org/html/rfc7234#section-5.5

&gt;  The "Warning" header field is used to carry additional information
&gt;    about the status or transformation of a message that might not be
&gt;    reflected in the status code.  This information is typically used to
&gt;    warn about possible incorrectness introduced by caching operations or
&gt;    transformations applied to the payload of the message.
&gt; 
&gt;   Warnings can be used for other purposes, both cache-related and
&gt;    otherwise.  The use of a warning, rather than an error status code,
&gt;    distinguishes these responses from true failures.

This would allow us to signal a valid response without breaking anyone that checks for a `[200, 300)` response code and provide it in a standardized format. What do you think?

I think that the way it should be handled is:

```
if response code is 200 or 201, then
   change to 299
end

add warning header info
```

What do we think?
</comment><comment author="jasontedor" created="2016-04-13T18:06:02Z" id="209571885">&gt; Is there a preferred, existing header for this?

I think that we can use the `Warning:` header.

I do not think that we should change the status code. I especially do not think that we should use 299 from RFC 7234; while RFC 7234 is published on the Standards Track it is still in [Proposed Standard](http://www.rfc-editor.org/info/rfc7234) status (as of 2016-04-13). I doubt that clients will properly handle this.
</comment><comment author="clintongormley" created="2016-04-14T08:08:54Z" id="209818671">@elastic/es-clients see https://github.com/elastic/elasticsearch/issues/17687#issuecomment-209571885
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Infer heap size at startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17686</link><project id="" key="" /><description>Today we rely on the user to set the JVM heap size. But we also have very strong heap size recommendations (stay below zero-based compressed oops, less than half the physical RAM) and we can use these recommendations to start Elasticsearch with a recommended heap size. The heap will still be configurable and if it is configured we will not use our ergonomics to size the heap.
</description><key id="147812077">17686</key><summary>Infer heap size at startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label></labels><created>2016-04-12T16:53:53Z</created><updated>2016-05-19T15:19:31Z</updated><resolved>2016-05-19T15:19:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T07:28:36Z" id="209273620">How do we avoid killing the user's laptop by taking half their RAM?  What happens when the user starts three instances to try things out?  Relying on `localhost` to indicate production mode isn't sufficient here.
</comment><comment author="clintongormley" created="2016-04-13T07:31:04Z" id="209275889">While we're on it, the change in https://github.com/elastic/elasticsearch/pull/17675 which removes `ES_HEAP_SIZE` I think is dangerous.  Way back in the past, before we had `ES_HEAP_SIZE`, users would set min/max to different values all the time.  It was a real problem, and the reason that `ES_HEAP_SIZE` was added.  

Instead of sizing the heap automatically, I'd much prefer:
- to have one setting for min/max
- to complain in production mode if that setting is not set
</comment><comment author="jasontedor" created="2016-05-19T15:19:30Z" id="220357689">We have decided to not go this route, opting instead for the defaults added in #18311 and otherwise requiring manual configuration for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should we have a method to randomly build content for search source `ext` in tests?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17685</link><project id="" key="" /><description>This was originally a comment in SearchSourceBuilderTests that in #17622 we decided to have a separate issue for. This is it.
https://github.com/elastic/elasticsearch/blob/9567f154e6fe470b7a4fa34a1784db493cd4e55f/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java#L438
</description><key id="147808873">17685</key><summary>Should we have a method to randomly build content for search source `ext` in tests?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>test</label></labels><created>2016-04-12T16:40:41Z</created><updated>2016-09-16T19:05:55Z</updated><resolved>2016-09-16T19:05:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Create a random aggregation builder method for tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17684</link><project id="" key="" /><description>This was originally a comment in SearchSourceBuilderTests that in #17622 we decided to have a separate issue for. This is it.
https://github.com/elastic/elasticsearch/blob/9567f154e6fe470b7a4fa34a1784db493cd4e55f/core/src/test/java/org/elasticsearch/search/builder/SearchSourceBuilderTests.java#L434
</description><key id="147808133">17684</key><summary>Create a random aggregation builder method for tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>test</label></labels><created>2016-04-12T16:38:37Z</created><updated>2017-03-21T15:13:44Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Incorrect exception message for geoip fields options.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17683</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master/5.0

**JVM version**: ALL

**OS version**: ALL

**Description of the problem including expected versus actual behavior**:

The ingest-geoip plugin does not distinguish supported `fields` values between city and country 
database types. The error message should only provide suggested valid fields that are compatible with 
the selected database type. Currently, all fields are treated as City-related fields.

The expected behavior should properly distinguish between which fields belong to which database type and provide the appropriate valid fields for the selected database.

Also, `LATITUDE` and `LONGITUDE` are no longer valid fields, these should be removed from the Field enum.
**Steps to reproduce**:
1. PUT a pipeline definition that includes the following geoip processor definition (`GeoLite2-City.mmdb`):

```
{
  "geoip": {
    "database_file": "GeoLite2-City.mmdb",
    "fields": [ "foo" ]
  }
}
```

you will see the following error message:

```
[fields] illegal field option [test]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]
```
1. PUT a pipeline definition that include the following geoip processor definition (`GeoLite2-Country.mmdb`):

```
{
  "geoip": {
    "database_file": "GeoLite2-Country.mmdb",
    "fields": [ "foo" ]
  }
}
```

you will see the following error message:

```
[fields] illegal field option [foo]. valid values are [[IP, COUNTRY_ISO_CODE, COUNTRY_NAME, CONTINENT_NAME, REGION_NAME, CITY_NAME, TIMEZONE, LATITUDE, LONGITUDE, LOCATION]]
```

You'll notice that both yield the same error, even though the country database only supports the following fields: [`IP`, `COUNTRY_ISO_CODE`, `COUNTRY_NAME`, `CONTINENT_NAME`].
</description><key id="147802071">17683</key><summary>Incorrect exception message for geoip fields options.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>:Plugin Ingest GeoIp</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-04-12T16:16:27Z</created><updated>2016-04-29T14:58:27Z</updated><resolved>2016-04-29T14:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Proposal: make StreamInput#readNamedWriteable and StreamOutput#readNamedWriteable public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17682</link><project id="" key="" /><description>Now that we have NamedWriteable we keep finding things that had little implementations of it like `AllocationCommand`s and aggregation's streams. Each time we convert those to a named writeable we create new methods in StreamInput and StreamOutput to write those objects. The methods are really simple, like:

``` java
    public SuggestionBuilder&lt;?&gt; readSuggestion() throws IOException {
        return readNamedWriteable(SuggestionBuilder.class);
    }
```

So, why do we make these methods rather than just use `readNamedWriteable` directly? I mean, now `readNamedWriteable` is protected so we **can't**. But why not just make it public?

The argument for keeping it protected is that it forces us to be intentional about each new named writable. You can't make a new type of NamedWriteable in a plugin because you'd have to modify StreamInput and StreamOutput.

The argument for making it public is that we can drop all of the `readFooBar` methods and StreamInput and StreamOutput no longer need to know about all of the classes that they are reading.

So let's discuss! Regardless of the outcome this issue will be closed by a PR - either one that makes the methods public or one that adds some javadoc summarizing the outcome of the conversation.
</description><key id="147799996">17682</key><summary>Proposal: make StreamInput#readNamedWriteable and StreamOutput#readNamedWriteable public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2016-04-12T16:07:45Z</created><updated>2016-04-18T20:30:59Z</updated><resolved>2016-04-18T20:30:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-12T16:38:39Z" id="208997686">+1 on making the read/writeNamedWriteable methods public
</comment><comment author="cbuescher" created="2016-04-12T16:50:00Z" id="209003198">+1 I think I already wanted to do this back when we added Shapes to the family of ReadWriteable but was voted down at that point: https://github.com/elastic/elasticsearch/pull/16312. Might be worth considering the objections in that issue again though, but things might have changed since then.
</comment><comment author="jpountz" created="2016-04-12T17:10:41Z" id="209011201">+1 I suspect we just did not foresee that we would end up wanting to serialize that many different classes.
</comment><comment author="ywelsch" created="2016-04-12T19:09:13Z" id="209057631">(Repeating my comment from #17661 here):
+1 on removing these specialized methods and making write/readNamedWriteable public. We should  strive to modularize our code better.
</comment><comment author="jpountz" created="2016-04-15T09:58:59Z" id="210396991">Discussed in FixitFriday and we agreed to make these methods public.
</comment><comment author="nik9000" created="2016-04-18T18:26:34Z" id="211515488">Since no one else's grabbed this and I'm in the neighborhood.....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc: improve the read/search sentence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17681</link><project id="" key="" /><description /><key id="147798384">17681</key><summary>doc: improve the read/search sentence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thefourtheye</reporter><labels><label>docs</label><label>non-issue</label></labels><created>2016-04-12T16:02:00Z</created><updated>2016-04-12T21:18:18Z</updated><resolved>2016-04-12T21:18:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-12T21:18:13Z" id="209106748">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing unused parse method from SortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17680</link><project id="" key="" /><description>Just found this unused method parsing the sort element and setting it to SearchContext. I think this is all handled by fromXContent() in SearchSourceBuilder now.
</description><key id="147797472">17680</key><summary>Removing unused parse method from SortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label></labels><created>2016-04-12T15:58:35Z</created><updated>2016-04-13T10:52:31Z</updated><resolved>2016-04-12T16:10:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-12T15:59:58Z" id="208980850">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch registration to key first</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17679</link><project id="" key="" /><description>We've been reworking registration of things (query, score function, other stuff) so you register a ParseField, a parser, and reader. Right now the order is `reader, parser, parseField`. It'd be more like a map if it were `parseField, reader, parser`. Should we do that?
</description><key id="147790995">17679</key><summary>Switch registration to key first</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-12T15:39:55Z</created><updated>2016-04-15T09:59:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-12T15:46:39Z" id="208970975">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SimpleQueryParser should call MappedFieldType.termQuery when appropriate.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17678</link><project id="" key="" /><description>This will allow to use it on virtual fields like `_index` or `_id` which do not
exist in the index.
</description><key id="147782356">17678</key><summary>SimpleQueryParser should call MappedFieldType.termQuery when appropriate.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T15:10:40Z</created><updated>2016-04-12T15:48:18Z</updated><resolved>2016-04-12T15:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-12T15:35:41Z" id="208965366">LGTM, nice simplification
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add EC2 discovery tests to check permissions of AWS Java SDK</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17677</link><project id="" key="" /><description>Similar to the test for GCE (#16881), this adds a test for EC2 discovery that checks that the right permissions are in place for the AWS Java Client SDK.

Relates to #17318 
</description><key id="147771112">17677</key><summary>Add EC2 discovery tests to check permissions of AWS Java SDK</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Plugin Discovery EC2</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T14:34:34Z</created><updated>2016-04-13T08:01:59Z</updated><resolved>2016-04-13T08:01:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-13T07:26:54Z" id="209270942">I just tested your branch. This is so nice that you added this! 
LGTM. thanks for adding this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add rewrite phase to Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17676</link><project id="" key="" /><description>It would be useful to have a rewrite phase on aggregations so we can:
1. optimise an aggregation to make it run better (e.g. rewrite the `QueryBuilder`s in the filter/filters aggregation to enable better request caching)
2. Add syntactic sugar around aggregations (see https://github.com/elastic/elasticsearch/issues/17631 for an example)

We could add a rewrite method to `AggregationBuilder` which would be called from the `SearchSourceBuilder#rewrite` method.
</description><key id="147763666">17676</key><summary>Add rewrite phase to Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2016-04-12T14:10:34Z</created><updated>2017-07-04T15:47:48Z</updated><resolved>2017-07-04T15:47:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-12T17:12:26Z" id="209011745">This looks like an easy win to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add JVM options configuration file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17675</link><project id="" key="" /><description>This commit adds a new configuration file jvm.options to centralize and
simplify management of JVM options. This separates the configuration of
the JVM from the packaging scripts (bin/elasticsearch*, bin/service.bat,
and init.d/elasticsearch) simplifying end-user operational management of
custom JVM options.

Closes #17121 
</description><key id="147756547">17675</key><summary>Add JVM options configuration file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>breaking</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T13:47:03Z</created><updated>2016-04-13T11:18:03Z</updated><resolved>2016-04-13T03:07:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-12T15:27:55Z" id="208962010">I'm sad that this removes support for the `JAVA_OPTS` env variable, since that is a standard for most JVM software and it'd be nice to keep in line with all the other java servers that do support it.
</comment><comment author="dakrone" created="2016-04-12T15:28:31Z" id="208962230">Also, you're going to hate me for this, but I find the changes to the Windows stuff to be suuuper dangerous, I figure there isn't a great way to vagrant test those, but is there some way we can test this stuff in CI?
</comment><comment author="jasontedor" created="2016-04-12T15:31:14Z" id="208963160">&gt; I'm sad that this removes support for the `JAVA_OPTS` env variable, since that is a standard for most JVM software and it'd be nice to keep in line with all the other java servers that do support it.

It's not standard. Please see my comments on #17121 justifying removing `JAVA_OPTS`.
</comment><comment author="jasontedor" created="2016-04-12T15:33:33Z" id="208964153">&gt; Also, you're going to hate me for this, but I find the changes to the Windows stuff to be suuuper dangerous, I figure there isn't a great way to vagrant test those, but is there some way we can test this stuff in CI?

The `elasticsearch.bat` script will be implicitly tested on Windows when we start up external nodes for integration tests. I also tested it manually.

I manually tested the service.bat extensively, which I think that is all that we can do right now. My understanding is that there are plans to put a different service installer in place so I don't think we should invest any effort into making the current one testable under CI.
</comment><comment author="dakrone" created="2016-04-12T15:39:29Z" id="208967327">&gt; It's not standard. Please see my comments on #17121 justifying removing JAVA_OPTS.

Okay, since you're removing it, what do you think of the warning Clint mentioned if it was set and ES_JAVA_OPTS is not set?
</comment><comment author="jasontedor" created="2016-04-12T15:53:51Z" id="208976418">&gt; Okay, since you're removing it, what do you think of the warning Clint mentioned if it was set and ES_JAVA_OPTS is not set?

@dakrone I pushed 7743c422dbd14bcebdfbe1d253bf2d4e20445689 to warn about `JAVA_OPTS`
</comment><comment author="jasontedor" created="2016-04-12T16:40:35Z" id="208999237">@dakrone Thanks for the review; I responded to all of your feedback. &#128516; 
</comment><comment author="kimchy" created="2016-04-12T16:46:23Z" id="209002128">@jasontedor when someone upgrades to 5.0 with this change, and they rely on the environment variables, they will get a silent startup with wrong settings, right? I am concerned about that, I imagine someone upgrading to 5.0, not understanding why the system fails, and taking time to figure out that a system property no longer applies. Maybe we can log a warning or not startup if this is set, at least for 5.x?
</comment><comment author="kimchy" created="2016-04-12T16:49:58Z" id="209003189">also, for a separate issue, I always wished we could, maybe in certain OSes, figure out the memory allocated, and set the defaults OOB to be the one we typically recommend (less than 26gb, 50% of OS).

UPDATE: I remembered seeing this done before, example here: https://github.com/apache/cassandra/blob/trunk/conf/cassandra-env.sh#L17
</comment><comment author="jasontedor" created="2016-04-12T16:50:31Z" id="209003357">&gt; Maybe we can log a warning or not startup if this is set, at least for 5.x?

@kimchy Assuming this PR is integrated, my plan was to open a second PR for 2.x to add warnings for all the environment variables that are being removed here but I think that is a very good idea. I'll push another commit soon.
</comment><comment author="jasontedor" created="2016-04-12T16:54:05Z" id="209004617">&gt; also, for a separate issue, I always wished we could, maybe in certain OSes, figure out the memory allocated, and set the defaults OOB to be the one we typically recommend (less than 26gb, 50% of OS).

Yes, this is something that we can do. I opened #17686.
</comment><comment author="jasontedor" created="2016-04-12T17:37:51Z" id="209021926">@kimchy I pushed d4cf0f021a28544700af543299d878cec8db4438 to fail if any of the now-unsupported environment variables are set.
</comment><comment author="kimchy" created="2016-04-12T17:55:45Z" id="209029182">@jasontedor awesome, thanks!
</comment><comment author="kimchy" created="2016-04-12T18:03:49Z" id="209033322">@jasontedor I had another thought, I am on the fence on it, so would love your opinion. This is a change where someone has to make for 5.0, without an opportunity to have both options (the new one, using the right way to set it, and the old way, being deprecated). What do you think about "converting" the old env variables to `ES_JAVA_OPTS`, and add a deprecation warning for 5.x? Is it even possible (I am thinking of things like collisions between conflicting settings, in which case we just need to make this change as it is)?
</comment><comment author="jasontedor" created="2016-04-12T18:12:18Z" id="209037842">&gt; I am thinking of things like collisions between conflicting settings, in which case we just need to make this change as it is

@kimchy It's exactly because of this that I think that we need to take the change as is. Either:
- the user will have conflicting settings which cause the JVM to fail to start in which case they will be forced to convert to the new way
- the user will have conflicting settings which do not cause the JVM to fail to start, but we will be silently ignoring one of their settings (e.g., imagine a user that has set `ES_HEAP_SIZE`, we convert it to `ES_JAVA_OPTS` and then they later modify the heap size setting in `jvm.options`: then currently their old `ES_HEAP_SIZE` settings will still take precedence); for these users the best option is also to convert to the new way

I do not think there is a _nice_ way for the two ways to sit side-by-side.
</comment><comment author="kimchy" created="2016-04-12T18:17:17Z" id="209039406">&gt; I do not think there is a nice way for the two ways to sit side-by-side.

@jasontedor aye, I hear you... . I suspect many users will fail when they upgrade to 5.0, and need to change to use the new mechanism. Not a big change, but still there. Can we maybe give them examples in the error message, specifically around the most common setting which I suspect is setting the heap size (i.e. show an example how to set it using ES_JAVA_OPTS).
</comment><comment author="jasontedor" created="2016-04-12T19:30:27Z" id="209066354">&gt; Can we maybe give them examples in the error message, specifically around the most common setting which I suspect is setting the heap size (i.e. show an example how to set it using ES_JAVA_OPTS).

@kimchy I pushed bdfa063946d4c5f3ab30918b9c628442c21be27c. This provides error messages like this on startup:

```
$ ES_HEAP_SIZE=4g ES_DIRECT_SIZE=1g ~/elasticsearch/elasticsearch-5.0.0-alpha1-SNAPSHOT/bin/elasticsearch
Error: encountered environment variables that are no longer supported
Use jvm.options or ES_JAVA_OPTS to configure the JVM
ES_HEAP_SIZE=4g: set -Xms4g and -Xmx4g in jvm.options or add "-Xms4g -Xmx4g" to ES_JAVA_OPTS
ES_DIRECT_SIZE=1g: set -XX:MaxDirectMemorySize=1g in jvm.options or add "-XX:MaxDirectMemorySize=1g" to ES_JAVA_OPTS
```
</comment><comment author="kimchy" created="2016-04-12T19:40:53Z" id="209072730">@jasontedor you rock, thanks!
</comment><comment author="dakrone" created="2016-04-12T20:09:43Z" id="209083528">LGTM
</comment><comment author="jasontedor" created="2016-04-13T03:08:53Z" id="209206302">Thanks for a patient and helpful review @dakrone and great feedback @kimchy!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow opting out of strict JSON field names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17674</link><project id="" key="" /><description>Since https://github.com/elastic/elasticsearch/pull/15351, Elasticsearch will now throw an exception if field names are unquoted.  This will break a number of APIs for users who have documents without quotes around their field names.

We should add a setting to disable the requirement for strict quoting, and log to the deprecation logs when this setting is enabled.
</description><key id="147747664">17674</key><summary>Allow opting out of strict JSON field names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:REST</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T13:14:38Z</created><updated>2016-04-19T16:03:09Z</updated><resolved>2016-04-19T16:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-12T13:21:14Z" id="208901619">Since this code is statically configured, we might want to go with a system property rather than a setting.
</comment><comment author="jasontedor" created="2016-04-12T13:49:23Z" id="208915149">&gt; Since this code is statically configured, we might want to go with a system property rather than a setting.

Yeah, this is one of those rare cases when a system property is the best option. If #17675 gets in, it will be easy to ship with a commented out configuration of this property.
</comment><comment author="kimchy" created="2016-04-12T14:14:06Z" id="208926271">we could also have a setting, and Bootstrap reads the setting and set the system property that is being read. I know it is a hack, but since it is something that we plan to remove down the road, it might be simplest.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation - Glossary misunderstandings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17673</link><project id="" key="" /><description>In official [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/glossary.html#glossary):

&gt; **index**
&gt; An index is like a database in a relational database. 
&gt; 
&gt; **type**
&gt; A type is like a table in a relational database.

I think, this definition is wrong. Because every type in elasticsearch index shares fields.
Since ES 2.0 an index is a big table and type is an restricted view of bit table. You cannot update field types or delete fields from table. You have to create new index(table) and reindex your index.
</description><key id="147715609">17673</key><summary>Documentation - Glossary misunderstandings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">khakulov</reporter><labels><label>docs</label></labels><created>2016-04-12T11:21:03Z</created><updated>2016-04-13T12:29:47Z</updated><resolved>2016-04-13T12:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T10:39:56Z" id="209361495">I've opened https://github.com/elastic/elasticsearch/pull/17704 - what do you think?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation is not informative</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17672</link><project id="" key="" /><description>I am trying to learn Elastic, and in many places the documentation does not explain anything.

Simple example: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-fielddata-fields.html

&gt; Allows to return the field data representation of a field for each hit.

After reading this, I know absolutely nothing about what a "field data representation" is...
</description><key id="147714864">17672</key><summary>Documentation is not informative</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nasht00</reporter><labels><label>docs</label></labels><created>2016-04-12T11:18:05Z</created><updated>2016-05-07T15:09:43Z</updated><resolved>2016-05-07T15:09:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-12T11:37:38Z" id="208858404">This is covered in the [definitive guide](https://www.elastic.co/guide/en/elasticsearch/guide/current/fielddata.html) and in the [reference docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/fielddata.html). Does that help?

Please note though that Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.
</comment><comment author="nasht00" created="2016-04-12T12:23:07Z" id="208877329">Yes thank you, that's better.

My suggestions:
- Add a link from that page (https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-fielddata-fields.html).
- Add an example of a result set. That's in general. When you give an example request, it is very useful to see an example response.

Thanks! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On restore, selecting concrete indices can select wrong index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17671</link><project id="" key="" /><description>While restoring a snapshot with multiple indices inside, selecting concrete indices can result in a wrong index being restored. This happens if at least 1 selected index is not available inside the snapshot while another being available but not the first one at the available list.

Example:
- Available indices in snapshot: ["foo", "bar" baz"]
- Selected indices to restore: ["bar", "not_available"]
- Resulting indices to be restored: ["foo"] &lt;-- WRONG, expected ["bar"]
</description><key id="147696132">17671</key><summary>On restore, selecting concrete indices can select wrong index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">seut</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>review</label></labels><created>2016-04-12T09:51:56Z</created><updated>2016-04-13T14:11:48Z</updated><resolved>2016-04-13T14:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seut" created="2016-04-12T13:18:29Z" id="208900785">I've just verified that this bug also exists at the current 2.x and master branches.
Afaik this commit can be cherry-picked into these branches, let me know if you'll do this or if I should create dedicated PR's..
</comment><comment author="clintongormley" created="2016-04-13T10:43:03Z" id="209362596">@ywelsch would you review please?
</comment><comment author="ywelsch" created="2016-04-13T13:58:33Z" id="209454955">@seut Thanks for the contribution. Pull requests should always be based on the master branch. Can you create a new PR against master and close this one? I will take care of the cherry-picking to older branches. Ping me when the new PR is ready.
</comment><comment author="seut" created="2016-04-13T14:01:01Z" id="209457248">@ywelsch Sure, will do.
</comment><comment author="seut" created="2016-04-13T14:11:48Z" id="209464249">@ywelsch Here is the new PR against master: https://github.com/elastic/elasticsearch/pull/17715
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update field-mapping.asciidoc  Correcting a spelling mistake</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17670</link><project id="" key="" /><description>Correcting a spelling mistake
</description><key id="147690881">17670</key><summary>Update field-mapping.asciidoc  Correcting a spelling mistake</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LeonardGC</reporter><labels><label>docs</label></labels><created>2016-04-12T09:31:39Z</created><updated>2016-04-15T07:22:13Z</updated><resolved>2016-04-15T07:22:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T10:28:04Z" id="209358773">Hi @LeonardGC 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="LeonardGC" created="2016-04-13T14:34:22Z" id="209476766">The CLA was signed.
</comment><comment author="clintongormley" created="2016-04-14T07:56:23Z" id="209814538">Hi @LeonardGC 

Sorry, but I can't find any CLA containing Leonard or `leonardgc` 
</comment><comment author="LeonardGC" created="2016-04-14T15:20:40Z" id="209999283">Hi @clintongormley 
Sorry to hear this. What can I do to resolve this? 

In the history of the pdf I see that:
Signed document emailed to CLA Committers (committers@elasticsearch.com)...
04/13/2016 - 7:30:11 PDT
</comment><comment author="clintongormley" created="2016-04-14T17:12:48Z" id="210055085">Hi @LeonardGC 

Could you email it to me please? (clinton at elastic dot co)
</comment><comment author="clintongormley" created="2016-04-15T07:22:13Z" id="210326247">thanks @LeonardGC 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PercolatorQueryBuilder cleanup by using MemoryIndex#fromDocument(...) helper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17669</link><project id="" key="" /><description>Also by using `MemoryIndex#fromDocument(...)`  the `position_increment_gap` is taken into account, which fixes #9386.
</description><key id="147665496">17669</key><summary>PercolatorQueryBuilder cleanup by using MemoryIndex#fromDocument(...) helper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-12T07:56:35Z</created><updated>2016-04-12T08:51:36Z</updated><resolved>2016-04-12T08:51:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-12T08:19:54Z" id="208776812">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch2.1.1 cluster do not work using Shield plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17668</link><project id="" key="" /><description>I am using the ES 2.1.1. I have created a cluster with name "wisesearch", which included two node "wisesearch-node1" and "wisesearch-node2", it had worked fine until I installed Shield plugin.

After start node1, the node1's log show "[wisesearch-node1] failed to send join request to master" error. And the other hand, the node2's log show "java.net.NoRouteToHostException: No route to host". The ES will show "cluster_block_exception" error with query index:
{
"error" : {
"root_cause" : [ {
"type" : "cluster_block_exception",
"reason" : "blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];"
} ],
"type" : "cluster_block_exception",
"reason" : "blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];"
},
"status" : 503
}

Node1's log:
[2016-04-11 17:35:05,870][INFO ][discovery.zen ] [wisesearch-node1] failed to send join request to master [{wisesearch-node2}{WSGMaGHSQEyrHZmONb8J6w}{10.47.211.106}{10.47.211.106:9300}], reason [RemoteTransportException[[wisesearch-node2][10.47.211.106:9300][internal:discovery/zen/join]]; nested: ConnectTransportException[[wisesearch-node1][10.47.211.95:9300] connect_timeout[30s]]; nested: NotSerializableExceptionWrapper[No route to host]; ]

Node2's log:
[2016-04-11 17:35:37,562][WARN ][shield.transport.netty ] [wisesearch-node2] exception caught on transport layer [[id: 0x14feb6ff]], closing connection
java.net.NoRouteToHostException: No route to host
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)

The Node1's elasticsearch.yum file:
cluster.name: wisesearch
node.name: wisesearch-node1
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: ["10.47.211.106"]
discovery.zen.minimum_master_nodes: 2
shield.audit.enabled: true

The Node2's elasticsearch.yum file:
cluster.name: wisesearch
node.name: wisesearch-node2
network.host: 0.0.0.0
discovery.zen.ping.unicast.hosts: ["10.47.211.95"]
discovery.zen.minimum_master_nodes: 2
shield.audit.enabled: true
</description><key id="147599742">17668</key><summary>ElasticSearch2.1.1 cluster do not work using Shield plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin820224</reporter><labels /><created>2016-04-12T00:49:09Z</created><updated>2016-04-13T12:16:36Z</updated><resolved>2016-04-13T10:28:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-04-13T10:28:58Z" id="209358983">This was asked and answered in our forums https://discuss.elastic.co/t/after-use-shield-the-es-cluster-do-not-work/47061 so I am closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex should gracefully handle when _source is disabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17667</link><project id="" key="" /><description>Closes #17666
</description><key id="147585796">17667</key><summary>Reindex should gracefully handle when _source is disabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T23:29:31Z</created><updated>2016-04-13T12:27:14Z</updated><resolved>2016-04-13T12:27:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-12T06:24:46Z" id="208727494">LGTM
</comment><comment author="colings86" created="2016-04-13T10:18:35Z" id="209355090">Thanks @nik9000 LGTM now &#128516; 
</comment><comment author="nik9000" created="2016-04-13T11:43:32Z" id="209385759">Thanks for complaining about confusing things. It is the only way we make the software less confusing!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better handling when _source is disabled for reindex api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17666</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**Description of the problem including expected versus actual behavior**:

When _source is disabled, reindex api throws a NPE. 

**Steps to reproduce**:
1.  Create an index with a type that has _source disabled
2.  Create a document in the index
3.  Use the reindex api and you will see:

```
{
   "error": {
      "root_cause": [
         {
            "type": "null_pointer_exception",
            "reason": null
         }
      ],
      "type": "null_pointer_exception",
      "reason": null
   },
   "status": 500
}
```

**Provide logs (if relevant)**:

```
[2016-04-11 15:39:45,057][WARN ][rest.suppressed          ] /_reindex Params: {}
java.lang.NullPointerException
    at org.elasticsearch.common.compress.lzf.LZFCompressor.isCompressed(LZFCompressor.java:54)
    at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:74)
    at org.elasticsearch.common.compress.CompressorFactory.uncompressIfNeeded(CompressorFactory.java:118)
    at org.elasticsearch.search.internal.InternalSearchHit.sourceRef(InternalSearchHit.java:200)
    at org.elasticsearch.index.reindex.TransportReindexAction$AsyncIndexBySearchAction.buildIndexRequest(TransportReindexAction.java:152)
    at org.elasticsearch.index.reindex.AbstractAsyncBulkIndexByScrollAction.buildBulk(AbstractAsyncBulkIndexByScrollAction.java:90)
    at org.elasticsearch.index.reindex.AbstractAsyncBulkByScrollAction$2.doRun(AbstractAsyncBulkByScrollAction.java:199)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

While the reindex api requires _source, it will be nice to handle this situation better.
</description><key id="147575765">17666</key><summary>Better handling when _source is disabled for reindex api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>:Reindex API</label><label>enhancement</label></labels><created>2016-04-11T22:41:17Z</created><updated>2016-04-13T22:18:44Z</updated><resolved>2016-04-13T12:27:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T22:45:02Z" id="208597035">Fair enough!
On Apr 11, 2016 6:41 PM, "Pius" notifications@github.com wrote:

&gt; _Elasticsearch version_: 2.3.1
&gt; 
&gt; _Description of the problem including expected versus actual behavior_:
&gt; 
&gt; When _source is disabled, reindex api throws a NPE.
&gt; 
&gt; _Steps to reproduce_:
&gt; 1. Create an index with a type that has _source disabled
&gt; 2. Create a document in the index
&gt; 3. Use the reindex api and you will see:
&gt; 
&gt; {
&gt;    "error": {
&gt;       "root_cause": [
&gt;          {
&gt;             "type": "null_pointer_exception",
&gt;             "reason": null
&gt;          }
&gt;       ],
&gt;       "type": "null_pointer_exception",
&gt;       "reason": null
&gt;    },
&gt;    "status": 500
&gt; }
&gt; 
&gt; _Provide logs (if relevant)_:
&gt; 
&gt; [2016-04-11 15:39:45,057][WARN ][rest.suppressed          ] /_reindex Params: {}
&gt; java.lang.NullPointerException
&gt;     at org.elasticsearch.common.compress.lzf.LZFCompressor.isCompressed(LZFCompressor.java:54)
&gt;     at org.elasticsearch.common.compress.CompressorFactory.compressor(CompressorFactory.java:74)
&gt;     at org.elasticsearch.common.compress.CompressorFactory.uncompressIfNeeded(CompressorFactory.java:118)
&gt;     at org.elasticsearch.search.internal.InternalSearchHit.sourceRef(InternalSearchHit.java:200)
&gt;     at org.elasticsearch.index.reindex.TransportReindexAction$AsyncIndexBySearchAction.buildIndexRequest(TransportReindexAction.java:152)
&gt;     at org.elasticsearch.index.reindex.AbstractAsyncBulkIndexByScrollAction.buildBulk(AbstractAsyncBulkIndexByScrollAction.java:90)
&gt;     at org.elasticsearch.index.reindex.AbstractAsyncBulkByScrollAction$2.doRun(AbstractAsyncBulkByScrollAction.java:199)
&gt;     at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
&gt;     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;     at java.lang.Thread.run(Thread.java:745)
&gt; 
&gt; While the reindex api requires _source, it will be nice to handle this
&gt; situation better.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/17666
</comment><comment author="ppf2" created="2016-04-13T22:18:43Z" id="209671905">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from SearchSourceBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17665</link><project id="" key="" /><description>Relates to #17085
</description><key id="147570668">17665</key><summary>Remove PROTOTYPE from SearchSourceBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T22:13:00Z</created><updated>2016-05-02T12:15:16Z</updated><resolved>2016-04-12T15:58:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-12T10:37:47Z" id="208840084">@nik9000 left one minor comment about naming, but other than that LGTM.
</comment><comment author="nik9000" created="2016-04-12T15:58:29Z" id="208980029">Thanks @cbuescher !
</comment><comment author="nik9000" created="2016-04-12T15:58:35Z" id="208980090">Thanks @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.4.1 indexing with few errors when index can't be created</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17664</link><project id="" key="" /><description>Hello,

I experienced the following issue. I use file index templates. I moved file with synonyms to other directory and forgot to update the config, then I deleted the index and started indexing to a new index which should have been created. I expected that indexing process will fall down and no doc will be reported as indexed. Surprisingly only **part** of requests fallen down with error like this

```
   {
    'index': {
    '_id': '5660f26e5d47ea351743a6bc', '_index': 'contacts_v3.0__1', '_type': 'contact',
    'error': 'RemoteTransportException[[Sprite][inet[/10.0.102.142:9300]][indices:admin/create]]; nested: IndexCreationException[[contacts_v3.0__1] failed to create index]; nested: FailedToResolveConfigException[Failed to resolve config path [templates/synonym.txt], tried file path [templates/synonym.txt], path file [/etc/elasticsearch/templates/synonym.txt], and classpath]; ',
    'status': 500
    }
}
```

I index in butches of 1k docs using bulk api of pyes lib. From 300000 indexed only 7000 were reported as failed.
</description><key id="147569720">17664</key><summary>ES 1.4.1 indexing with few errors when index can't be created</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">serj-p</reporter><labels /><created>2016-04-11T22:09:14Z</created><updated>2016-04-13T10:18:06Z</updated><resolved>2016-04-13T10:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="serj-p" created="2016-04-11T23:23:00Z" id="208613508">I've checked, ES responses with 200, so this is not lib problem
</comment><comment author="clintongormley" created="2016-04-13T10:18:05Z" id="209354848">Hi @serj-p 

I've tested this out on 1.4.1 and, as expected, every entry in the bulk response reported the same exception.  (a 200 response is expected for the bulk API - you have to check the responses to the individual items to know whether they succeeded or not).

I suggest checking the successful responses to see what index they went to.

Either way, 1.4.1 is very old and we won't be doing any further releases of that branch.  If you do find something that looks like a bug, please could you try to confirm this in a more recent version.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportNodesListGatewayStartedShards should fall back to disk based index metadata if not found in cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17663</link><project id="" key="" /><description>When an index is recovered from disk it's metadata is imported first and the master reaches out to the nodes looking for shards of that index. Sometimes those requests reach other nodes before the cluster state is processed by them. At the moment, that situation disables the checking of the store, which requires the meta data (indices with custom path need to know where the data is). When corruption hits this means we may assign a shard to node with corrupted store, which will be caught later on but causes confusion. Instead we can try loading the meta data from disk in those cases.

Relates to #17630 
</description><key id="147565625">17663</key><summary>TransportNodesListGatewayStartedShards should fall back to disk based index metadata if not found in cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T21:51:56Z</created><updated>2016-04-13T10:54:59Z</updated><resolved>2016-04-12T16:41:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-12T15:41:06Z" id="208968237">LGTM, thanks Boaz
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cli: Add verbose output with zip url when installing plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17662</link><project id="" key="" /><description>closes #17529
</description><key id="147546896">17662</key><summary>Cli: Add verbose output with zip url when installing plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T20:34:42Z</created><updated>2016-05-02T12:04:10Z</updated><resolved>2016-04-22T16:26:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-22T16:24:28Z" id="213498049">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make AllocationCommands NamedWriteables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17661</link><project id="" key="" /><description>The cluster reroute API had a copy of NamedWriteableRegistry's behavior
inside it in the form of AllocationCommands#registerFactory and
AllocationCommands#lookupFactorySafe. There isn't a reason to duplicate
that effort. So this replaces all of AllocationCommand#Factory with
query-like registration in NetworkModule.
</description><key id="147536443">17661</key><summary>Make AllocationCommands NamedWriteables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T19:54:31Z</created><updated>2016-04-12T21:30:22Z</updated><resolved>2016-04-12T21:29:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T19:58:23Z" id="208530537">@ywelsch is this a thing you can review? It makes the allocation commands use the NamedWriteable infrastructure that we have for queries and other similar things rather than it's own thing. It removes the whole Factory interface in favor of the (now standard) Writeable.Reader interface and an AllocationCommand.Parser interface for the xcontent parsing.

@cbuescher this is an interesting detour/side effect of my PROTOTYPE hunt. In this case these didn't have explicit prototypes but they implemented StreamableReader which is a red flag for me.
</comment><comment author="nik9000" created="2016-04-11T20:04:20Z" id="208532221">Note to whoever: I did some work in #17653 to make the registries and registration less copy-and-paste-tastic and left a node in this to use that once it is available.
</comment><comment author="ywelsch" created="2016-04-12T20:00:08Z" id="209079281">@nik9000 Left some suggestions that don't need to be addressed as part of this PR. Thanks for cleaning up the codebase. LGTM.
</comment><comment author="nik9000" created="2016-04-12T20:24:32Z" id="209088388">@ywelsch I rebased and adapted it to use ParseField registry which saves some code and gets rid of some odd method names.
</comment><comment author="ywelsch" created="2016-04-12T20:32:22Z" id="209090669">++ looks good.
</comment><comment author="nik9000" created="2016-04-12T21:30:22Z" id="209110651">Thanks for all the review @ywelsch ! Be sure to tune in over at #17596 for more fun!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested field does not allow a Multifield definition </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17660</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: Java(TM) SE Runtime Environment (build 1.8.0_31-b13)

**OS version**: Mac 10.11.3

**Description of the problem including expected versus actual behavior**:
Mappings of a field in an object doesn't respect multi-field definition. This behavior worked until 1.4.5 but the mappings were defined with the field dot notations. 

create an index with the following mappings (assume case_insensitive_sort is defined):

```
{
    "my_type": {
        "properties": {
            "object_field": {
                "properties": {
                    "name": {
                        "type": "string",
                        "fields": {
                            "lower_case_sort": {
                                "type": "string",
                                "analyzer": "case_insensitive_sort"
                            }
                        }
                    }
                }
            }
        }
    }
}
```

The mappings after the data is indexed does not show `lower_case_sort` field and it is not available at query time. Response when inspecting the mappings of the index:

```
"browse_bits": {
    "properties": {     
        "name": {
            "type": "string"
        }
    }
}
```

**Steps to reproduce**:
1. Index a document specifying the mappings indicated above
2. Inspect mappings after indexing a document, the nested field is not respected.
</description><key id="147529587">17660</key><summary>Nested field does not allow a Multifield definition </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">miskander</reporter><labels /><created>2016-04-11T19:30:55Z</created><updated>2016-04-11T22:11:48Z</updated><resolved>2016-04-11T22:11:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="miskander" created="2016-04-11T22:11:48Z" id="208586698">Closing, this is actually not happening. The issue was the index was being deleted by a different operation and then re-created without the mappings. Sorry for the false issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Default value for a field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17659</link><project id="" key="" /><description>**Describe the feature**:
Currently, we have null_value, which works fine as long as the value is declared to be null at index time.  The `set` processor overwrites values if they already exist.  It'd be nice to have the ability to have the ability to set a value in the missing case.  To me, it seems like it'd be nice to add to the ingest processors (perhaps a simple `no_override` field in `set`?) so we can have different defaults depending on the pipeline

Related to https://github.com/elastic/elasticsearch/issues/12998.  Has been requested on discuss as well a few times:
https://discuss.elastic.co/t/is-it-possible-to-index-a-default-value-when-a-document-property-is-undeclared/9849
https://discuss.elastic.co/t/setting-missing-field-to-default-value-based-on-a-master-template/39317
https://discuss.elastic.co/t/setting-a-default-value-for-missing-fields-during-indexing/45021
https://discuss.elastic.co/t/field-null-value-doesnt-seem-to-have-any-effect/22015
</description><key id="147517690">17659</key><summary>Default value for a field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">eskibars</reporter><labels><label>:Ingest</label><label>feature</label></labels><created>2016-04-11T18:46:39Z</created><updated>2016-04-28T21:00:29Z</updated><resolved>2016-04-28T21:00:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-13T09:44:32Z" id="209340971">I can definitely see this being useful as an ingest processor.  Let's do it
/cc @talevy @martijnvg 
</comment><comment author="talevy" created="2016-04-13T23:08:54Z" id="209685438">@clintongormley, I can get on that. I think it would be nice as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove XGeoPointDistanceRangeQuery in favor of GeoPointDistanceSort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17658</link><project id="" key="" /><description>[LUCENE-7126](https://issues.apache.org/jira/browse/LUCENE-7126) removes XGeoPointDistanceRangeQuery due to its inability to support multi-valued points but ES still requires this Query. This issue will remove `XGeoPointDistanceRangeQuery` in favor of  `GeoPointDistanceSort`, from [LUCENE-7180](https://issues.apache.org/jira/browse/LUCENE-7180), with `searchAfter`.  
</description><key id="147507449">17658</key><summary>Remove XGeoPointDistanceRangeQuery in favor of GeoPointDistanceSort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>:Query Refactoring</label><label>FAKENEWS</label></labels><created>2016-04-11T18:04:22Z</created><updated>2017-03-20T21:31:00Z</updated><resolved>2017-03-20T21:31:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-10-18T07:50:33Z" id="254432951">@nknize is this still required?
</comment><comment author="nknize" created="2017-03-20T21:31:00Z" id="287904509">This is no longer required. Closing as FAKENEWS</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene 6 release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17657</link><project id="" key="" /><description>This PR updates ES to use the latest Lucene 6.0.0 release. 

Notable changes in this PR:
- updates to geo api changes
  - adds `GeoPointDistanceRangeQuery` as `XGeoPointDistanceRangeQuery`
  - cuts over to `GeoHashUtils` refactored from Lucene to ES
</description><key id="147490412">17657</key><summary>Upgrade to lucene 6 release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Core</label><label>release highlight</label><label>review</label><label>upgrade</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T16:58:43Z</created><updated>2016-04-13T10:26:55Z</updated><resolved>2016-04-11T22:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-11T21:13:53Z" id="208564069">LGTM
</comment><comment author="jpountz" created="2016-04-12T09:16:00Z" id="208809644">Thanks @nknize !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Isolate StreamableReader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17656</link><project id="" key="" /><description>Makes Writeable not depend on StreamableReader. Keeps the default readFrom
implementation for backwards compatibility during the PROTOTYPE removal
but that'll go when those are gone.

Makes Diffable not extend StreamableReader. Instead Diffable has a readFrom
method. The PROTOTYPE removal will not get to cluster state for a long time
so that method will stay.

Now only a few things implement StreamableReader. They will be addressed
individually and then we'll remove StreamableReader.

Relates to #17085
</description><key id="147468341">17656</key><summary>Isolate StreamableReader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T15:38:40Z</created><updated>2016-04-12T20:16:56Z</updated><resolved>2016-04-12T20:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T15:38:50Z" id="208406754">More in the PROTOTYPE vein.
</comment><comment author="nik9000" created="2016-04-11T16:23:06Z" id="208433531">@cbuescher it'd be cool if you got review time to review this one because there is a little more work behind this before I can remove StreamableReader.
</comment><comment author="nik9000" created="2016-04-11T20:04:59Z" id="208532388">Or maybe it doesn't matter. The stuff I was thinking about doing is already in #17661. Once that and this are in `StreamableReader` can go entirely!
</comment><comment author="cbuescher" created="2016-04-12T12:58:47Z" id="208892800">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove phrase prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17655</link><project id="" key="" /><description>I see several issues with this query:
- It feels like the right way to run auto completion when in fact the completion suggester should be used instead.
- It behaves non intuitively: for instance it seems reasonable to assume that "new y" would match more doruments than "new yo" but since the match prefix query only takes the first 50 (default) terms that start with `y` in the first case, it is quite likely that `york` is not part of these terms, so the first query will probably try to match on "new yacht" but not on "new york".
</description><key id="147458832">17655</key><summary>Remove phrase prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>adoptme</label><label>breaking</label></labels><created>2016-04-11T15:06:02Z</created><updated>2016-04-22T10:45:38Z</updated><resolved>2016-04-22T10:45:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T15:07:23Z" id="208391691">&gt; It behaves non intuitively:

this is why I don't like it.
</comment><comment author="mattweber" created="2016-04-11T16:56:00Z" id="208448574">-1 this is really one of the only queries that do simple analyzed wildcards (not a fan of `query_string`).

Maybe replace with a  "match_span" query that generates spans from analyzed terms?  I personally would love a simple analyzed query that generates spans. Something like this (trailing wildcard) could easily be an ordered `span_near` with no slop and the last term and/or any wildcards being represented with a `span_multi`.
</comment><comment author="clintongormley" created="2016-04-13T09:36:44Z" id="209337639">&gt; It feels like the right way to run auto completion when in fact the completion suggester should be used instead.

The completion suggester has a very different purpose from `match_phrase_prefix`, which people use as a poor-man's autocomplete with phrases which can appear anywhere within a text field, not just the beginning.

I'm not a huge fan of the query, but it is really easy for the newbie to get going with it, without having to understand what analyzers and edge ngrams are and how to set them up etc.  Later, when the user knows more and wants to improve their search, they can move on to more complex setups.

If we had a simpler way to provide the auto-complete experience, I'd be for it but currently...
</comment><comment author="jpountz" created="2016-04-22T10:03:47Z" id="213365451">Discussed in Fixit Friday: we agreed to keep it but will add a warning to the docs about the behaviour of this query discussed above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make registerScoreFunction look like registerQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17654</link><project id="" key="" /><description>The order of the first two arguments was different on the two register
functions. This makes them consistent.

Relates to #17085
</description><key id="147447884">17654</key><summary>Make registerScoreFunction look like registerQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T14:29:07Z</created><updated>2016-04-13T10:50:15Z</updated><resolved>2016-04-12T15:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T14:29:35Z" id="208374655">@cbuescher a smaller one for you this time - I just got the order backwards on this method so I'm making it line up.
</comment><comment author="cbuescher" created="2016-04-12T10:00:18Z" id="208827202">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create registration methods for aggregations similar to those for queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17653</link><project id="" key="" /><description>This also creates some methods to remove the copy-and paste nature of doing the same style of thing for aggregations, score functions, and queries.

This is step 1 of the rework of #17389.

Relates to #17085
</description><key id="147446536">17653</key><summary>Create registration methods for aggregations similar to those for queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T14:24:53Z</created><updated>2016-04-12T19:41:47Z</updated><resolved>2016-04-12T19:41:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-12T08:08:31Z" id="208771430">@nik9000 I left some comments
</comment><comment author="nik9000" created="2016-04-12T15:31:52Z" id="208963354">@colings86 and @javanna - I pushed a commit that makes ParseFieldRegistry which I think is what you wanted.

I wasn't able to remove IndicesQueriesRegistry because it is `@Inject`ed  and fixing that is well beyond the scope of this PR.
</comment><comment author="colings86" created="2016-04-12T15:56:58Z" id="208978880">@nik9000 LGTM. When you do the actual refactoring of the aggregations to make use of this can you do a couple aggs at a time (at the most) so the PRs are small and managable?
</comment><comment author="nik9000" created="2016-04-12T15:59:26Z" id="208980610">&gt; When you do the actual refactoring of the aggregations to make use of this can you do a couple aggs at a time (at the most) so the PRs are small and managable?

I can flood your inbox with one PR per aggregation. I think this might be the best way.
</comment><comment author="nik9000" created="2016-04-12T17:32:15Z" id="209020216">&gt; I can flood your inbox with one PR per aggregation. I think this might be the best way.

Or not. I'll send you one tonight and when that one looks good we can do one per.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sub aggregations can overwrite bucket fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17652</link><project id="" key="" /><description>Consider the following sense script:

```
GET test/_search
{
  "aggs": {
    "terms": {
      "terms": {
        "field": "i",
        "size": 10
      },
      "aggs": {
        "key": {
          "max": {
            "field": "i"
          }
        },
        "doc_count": {
          "bucket_script": {
            "buckets_path": "_count",
            "script": {
              "inline": "_value",
              "lang": "expression"
            }
          }
        }
      }
    }
  }
}
```

The sub aggregations to the terms aggregation are named `key` and `doc_count`. In the response these sub-aggregations overwrite the `key` and `doc_count` fields of the bucket (for comparison the normal response without any sub aggregations is at the bottom of this bug report):

```
{
  ...
  "aggregations": {
    "terms": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": {
            "value": 2
          },
          "doc_count": {
            "value": 1
          }
        }
      ]
    }
  }
```

Sub aggregations should not be able to overwrite fields in the bucket. Note that this affects all bucket aggregations and other bucket aggregations use other fields. More generally a bucket aggregation has (and should have) no restrictions on the fields it can use in each bucket response.

We should not fix this by not allowing aggregations to be named `doc_count` or `key`, etc.  That would be a hacky fix that would be hard to maintain (if a bucket aggregation defines a new field in the bucket we would have to remember to add it to the exceptions which is easily missed).

I think we should solve this by name-spacing the sub aggregations under an `aggregations` object in the bucket. This would make the output safer (and avoid this bug), and would match the request format better (where sub aggregations are nested under an `aggs`/`aggregations` object). This would obviously be a breaking change but I think it is an important change to make. If we implement https://github.com/elastic/elasticsearch/issues/11184 we could effectively deprecate the old response format by having it output the new format by default but output the old format if the version parameter is specified on the request (with a version number relevant for the old format). Then we could remove the old format in the next major version.

Response with no sub-aggregations:

```
{
  ...
  "aggregations": {
    "terms": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": 2,
          "doc_count": 1
        }
      ]
    }
  }
}
```
</description><key id="147441638">17652</key><summary>sub aggregations can overwrite bucket fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>breaking</label><label>bug</label><label>PITA</label><label>v6.0.0</label></labels><created>2016-04-11T14:08:19Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-06-03T09:42:22Z" id="223536399">To change this we would require https://github.com/elastic/elasticsearch/issues/11184 to be done first
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix EC2 Discovery settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17651</link><project id="" key="" /><description>The settings refactoring in #16602 introduced two bugs (currently only caught by thirdparty tests).
</description><key id="147435137">17651</key><summary>Fix EC2 Discovery settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Plugin Discovery EC2</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T13:46:20Z</created><updated>2016-04-11T14:17:56Z</updated><resolved>2016-04-11T14:17:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-11T14:08:30Z" id="208364856">Definitely the way to go! LGTM.
Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store indexed scripts in the cluster state instead of the `.scripts` index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17650</link><project id="" key="" /><description>This PR also cleans up the cyclic dependency `ScriptService` has at the moment. By moving scripts to the cluster state, the `Client` dependency was no longer needed.

Open questions:
- How to deal with scripts already stored in the `.scripts` index? Do we want to provide support for moving indexed scripts to the cluster state? Upon upgrading to 5.x the `.scripts` index will not be removed. We can provide a script that can be executed just after the upgrade. Or do we want to provide something that automatically moves scripts from the `.script` index to the cluster state, if so how?
- The `script.indexed.*` settings haven't been renamed yet to `script.stored.*`. Are we okay with making a hard break here? Or do we want to deprecate `script.indexed.*` settings first. How everything is coded right now this doesn't seem to be straight forward.
  (ScriptType.INDEXED enum has been renamed, but not its script type parameter)

PR for #16651
</description><key id="147431263">17650</key><summary>Store indexed scripts in the cluster state instead of the `.scripts` index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Indexed Scripts/Templates</label><label>breaking</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T13:34:46Z</created><updated>2016-05-19T08:58:11Z</updated><resolved>2016-04-22T11:44:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-18T10:55:25Z" id="211326952">I also added a soft limit for scripts based on @colings86 suggestion.
</comment><comment author="martijnvg" created="2016-04-18T12:20:34Z" id="211355545">&gt; The script.indexed.\* settings haven't been renamed yet to script.stored._. Are we okay with making a hard break here? Or do we want to deprecate script.indexed._ settings first. How everything is coded right now this doesn't seem to be straight forward. (ScriptType.INDEXED enum has been renamed, but not its script type parameter)

A node will not start if there are unknown settings, so during upgrading or test driving 5.x this will bubble up quickly.
</comment><comment author="clintongormley" created="2016-04-19T08:32:44Z" id="211795382">i'm OK with a hard break here, as long as we provide a utility that the user can run to move scripts from indexed to stored.  This could even be a shell script that the user can copy and paste from the migration docs.
</comment><comment author="colings86" created="2016-04-19T09:03:22Z" id="211813432">@martijnvg I left a couple of consistency comments but otherwise I think it looks good.
</comment><comment author="martijnvg" created="2016-04-20T12:47:27Z" id="212409728">@colings86 I've addressed your comment and added docs.
</comment><comment author="colings86" created="2016-04-21T07:32:18Z" id="212786308">@martijnvg I left some more comments but I think its very close
</comment><comment author="martijnvg" created="2016-04-21T09:35:18Z" id="212830881">@colings86 I've updated the PR and addressed your comments.
</comment><comment author="colings86" created="2016-04-21T10:52:16Z" id="212854414">LGTM
</comment><comment author="Mpdreamz" created="2016-05-01T20:45:23Z" id="216071297">@martijnvg small request the current exception message when `es.script.indexed=true` is passed is:

&gt; System.Exception: Exception in thread "main" java.lang.IllegalArgumentException: unknown setting [script.indexed] did you mean any of [script.inline, script.ingest]?

Where `es.script.stored=true` is the more logical equivalent to suggest (this of the latest snapshot, have not tested HEAD of master).
</comment><comment author="clintongormley" created="2016-05-02T11:12:40Z" id="216206111">@Mpdreamz yeah i had this as well, it's because of the fuzzy matching we're using on incorrect settings.  That said, I think we should add a hardcoded rule when `script.indexed` is used to recommend `script.stored` instead.
</comment><comment author="martijnvg" created="2016-05-19T08:58:10Z" id="220266473">@Mpdreamz @clintongormley I like the idea of a hard coded rule, but there is no infrastructure for that. Adding that doesn't seem to be worth the effort for this breaking change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query_string does not respect default_operator when a wildcard query produces multiple tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17649</link><project id="" key="" /><description>query string does not return right hits when you use query with wildcard and it is tokenized. Example

 "query_string": {
                  "query": "luka-por-si*",
                  "default_operator": "and",
                  "allow_leading_wildcard": false,
                  "analyze_wildcard": true,
                  "minimum_should_match": "100%",
                  "fields": [
                    "name"
                  ],
                  "use_dis_max": false
                }

query should return hits with luka AND por AND si_. On es version &lt; 2.3 that works ok, now on version 2.3 and 2.3.1 query returns luka OR por OR si_. I attached files to reproduce this scenario. 

**Elasticsearch version**:&gt;=2.3

**JVM version**:8

**OS version**:any (tested on win and Linux ubuntu)

**Steps to reproduce**:
1. create index (createindexmapping.txt)
2. fill data (insertdata.txt)
3. get results (searchdata.txt)
   [createindexmapping.txt](https://github.com/elastic/elasticsearch/files/212986/createindexmapping.txt)
   [insertdata.txt](https://github.com/elastic/elasticsearch/files/212988/insertdata.txt)
   [searchdata.txt](https://github.com/elastic/elasticsearch/files/212987/searchdata.txt)
</description><key id="147421377">17649</key><summary>query_string does not respect default_operator when a wildcard query produces multiple tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukapor</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-04-11T13:07:30Z</created><updated>2016-06-03T09:05:04Z</updated><resolved>2016-06-03T09:05:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-04-11T14:17:28Z" id="208368722">@lukapor the behavior is the same for 2.3 and &lt;= 2.3. The behavior is described here: https://github.com/elastic/elasticsearch/issues/1539
Though I agree that the query_string parser should respect the default operator when it deals with prefix query on analyzed fields. Could you please rename your issue with something like:
"query_string does not respect default_operator when a wildcard query produces multiple tokens"
</comment><comment author="clintongormley" created="2016-04-13T09:04:19Z" id="209319527">Duplicate of https://github.com/elastic/elasticsearch/issues/2183
</comment><comment author="lukapor" created="2016-04-14T07:01:04Z" id="209789269">@clintongormley I don't think that is duplicate issue.
Attached example returns on es version 2.2.0 (&lt; 2.3.0) only one document, on lastest es version (2.3.0, 2.3.1) return all (3) documents.
</comment><comment author="clintongormley" created="2016-04-14T08:45:58Z" id="209833161">OK - it seems this change in behaviour is due to https://github.com/elastic/elasticsearch/pull/16155

The explanation in 2.3 is:

```
"explanation": "(+(+(name:luka* name:por* name:si*)))"
```

while in 2.2 it is:

```
"explanation": "(+(+(name:luka* name:por* name:si*)))~3"
```

This happens even if you remove the `minimum_should_match` parameter.

I'm not sure what the correct behaviour should be in this situation.  @jimferenczi what do you think?
</comment><comment author="jimczi" created="2016-04-14T11:28:34Z" id="209889096">Yes this is a side effect of the way we handle analyzed wildcard. In 2.x coords are disabled on analyzed wildcard query that produces multiple tokens. I did not check this behavior when I've merged https://github.com/elastic/elasticsearch/pull/16155. 
IMO the 2.3 and 2.2 query are wrong and the real fix is in the master branch: https://github.com/elastic/elasticsearch/pull/17711.
... which produces:

```
+(+(+name:luka +name:por +name:si*))
```

... and with "default_operator": "or":

```
+(+((name:luka name:por name:si*)~3))
```

I can backport this fix to 2.x if needed, @clintongormley ?
</comment><comment author="clintongormley" created="2016-04-14T11:51:46Z" id="209898739">yes please @jimferenczi 
</comment><comment author="lukapor" created="2016-04-14T12:47:02Z" id="209922473">Nice @jimferenczi 

@clintongormley have you any clue, when it will be the next release with this patch?
</comment><comment author="clintongormley" created="2016-06-03T09:05:03Z" id="223528517">Closed by #17711
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update terms-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17648</link><project id="" key="" /><description>user id of tweet should exist in the `followers`, otherwise the search result is empty
</description><key id="147415998">17648</key><summary>Update terms-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chengpohi</reporter><labels><label>docs</label></labels><created>2016-04-11T12:48:17Z</created><updated>2016-04-22T16:55:39Z</updated><resolved>2016-04-22T16:55:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-11T13:40:25Z" id="208349484">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="chengpohi" created="2016-04-11T14:30:34Z" id="208375234">Hi @dakrone , I have signed the **CLA**, can you check it? I don't know why it still display I need **CLA**.
</comment><comment author="dakrone" created="2016-04-22T16:55:31Z" id="213509942">@chengpohi I see it now, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[5.0.0~alpha1] package installed path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17647</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:5.0.0~alpha1

**JVM version**:1.8.0_77-b03

**OS version**:ubuntu 14.04 64bits

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.dpkg -L elasticsearch | grep elasticsearch-plugin
 2.dpkg -L kibana | grep kibana-plugin
 3.dpkg -L logstash | grep logstash-plugin

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
Refering to https://www.elastic.co/downloads/x-pack 
we can use `elasticsearch-plugin` and `kibana-plugin` to install x-pack.

I use `dpkg -i *.deb` to install ELK stack.
However these commands are installed in, `opt` and `/usr/share`, separately
Would it be better to install ELK stack in consistent path and link bin files to `/usr/bin`?
Moreover, the steps in https://www.elastic.co/downloads/x-pack seems only good for tar file users now.
</description><key id="147375810">17647</key><summary>[5.0.0~alpha1] package installed path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">changsijay</reporter><labels><label>docs</label></labels><created>2016-04-11T09:57:30Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-12T18:47:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-12T18:44:26Z" id="209048652">Thanks for reporting a 5.0 bug!  I'm adding the [Pioneer Program](https://www.elastic.co/blog/elastic-pioneer-program) label to the issue to make you eligible for a gift pack.

&gt; Would it be better to install ELK stack in consistent path and link bin files to /usr/bin?

Agreed - we should be consistent, and we have a separate internal issue to track this cross-stack.

&gt; Moreover, the steps in https://www.elastic.co/downloads/x-pack seems only good for tar file users now.

Agreed - I'll open a ticket to get that page updated.

thanks
</comment><comment author="clintongormley" created="2016-04-12T18:47:33Z" id="209050054">Issue opened - closing this one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Propagate DocValueFormat to all terms aggs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17646</link><project id="" key="" /><description>TL;DR This commit should not have any impact on terms aggs, it will just make
supporting ipv6 easier.

Currently only the numeric terms aggs propagate the DocValueFormat instance since
we use numerics to represent also dates or ip addresses. Since string terms aggs
are only used for text/keyword/string fields, they do not use the format and just
call toUt8String(). However when we support ipv6, ip addresses as well will be
encoded in sorted doc values (just like strings) so we will need to use the
DocValueFormat to format the keys.
</description><key id="147363143">17646</key><summary>Propagate DocValueFormat to all terms aggs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T09:08:04Z</created><updated>2016-04-11T14:34:13Z</updated><resolved>2016-04-11T14:34:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-11T13:38:31Z" id="208348127">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Eclipse gradle build ElasticSearch problem (Not found JAVA_HOME)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17645</link><project id="" key="" /><description>@nik9000 I fixed #17554.
</description><key id="147361725">17645</key><summary>Eclipse gradle build ElasticSearch problem (Not found JAVA_HOME)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilly</reporter><labels><label>build</label><label>v5.0.0-beta1</label><label>v6.0.0-alpha1</label></labels><created>2016-04-11T09:02:10Z</created><updated>2016-12-08T10:45:51Z</updated><resolved>2016-09-12T22:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-11T12:34:24Z" id="208318398">@rjernst do you have an opinion on this? My understanding is that it gets the Eclipse gradle plugin's launcher working properly in the same way that IntelliJ's works. I've never encountered this because I, and presumably all the other Eclipse folks, run gradle on the command line. I don't think it can hurt but we don't want to spend forever supporting some unsupported way of running gradle when there is a perfectly good other way (the console) otoh we already do this for IntelliJ and it is simple so why not?
</comment><comment author="rjernst" created="2016-04-11T20:13:46Z" id="208538739">I'm fine with adding the same check for eclipse, but note that when I tried buildship many months ago (shortly after it was released, when first working on the gradle migration) there were lots of problems.  There is really no way for us to test this check (for eclipse or intellij) so it will always be at risk of "breaking".
</comment><comment author="rjernst" created="2016-04-11T20:14:41Z" id="208539787">Also note that eclipse should be a better user of java and actually set JAVA_HOME. It is ridiculous that IntelliJ does not. 
</comment><comment author="chilly" created="2016-04-12T11:38:27Z" id="208858605">It is not Eclipse or Intellij's mistake for JAVA_HOME. It may be a bug of gradle plugin. When I run java application to print System.getenv("JAVA_HOME") in eclipse or Intellij, it will not be null. But when I execute gradle plugin, groovy script only get null from System.getenv("JAVA_HOME"). 
</comment><comment author="dakrone" created="2016-09-12T21:32:01Z" id="246500899">@rjernst do you know if this still applies and if this should be merged?
</comment><comment author="rjernst" created="2016-09-12T21:46:19Z" id="246504857">I don't use eclipse so I'm not the best to check this.
</comment><comment author="dakrone" created="2016-09-12T22:18:56Z" id="246513352">Okay, I _think_ @nik9000 uses Eclipse, Nik do you know?
</comment><comment author="nik9000" created="2016-09-12T22:36:45Z" id="246517472">&gt; Okay, I think @nik9000 uses Eclipse, Nik do you know?

I do indeed use Eclipse. I don't use Eclipse's built in launcher for gradle though. I just tried and indeed it fails because it doesn't have JAVA_HOME.
</comment><comment author="nik9000" created="2016-09-12T22:37:51Z" id="246517723">&gt; I just tried and indeed it fails because it doesn't have JAVA_HOME.

And when I apply this patch it works.

I don't expect to start using the launcher more now that it isn't broken, but I'm happy to merge this now that I know it helps.
</comment><comment author="nik9000" created="2016-09-12T22:42:37Z" id="246518783">Merged! Thanks for fixing it @chilly. I'm sorry this took so long to get to!

Master: 4cf4683a64b26bf12365dbe46abac177fe66efbd
5.x: 88fa4840cb385dd78b7702e53918d6d4efeec796
5.0: c59fd49d053e15c30fb0b0803dfa19d162871376
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent dynamic default depending on how a field gets added</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17644</link><project id="" key="" /><description>**Elasticsearch version**:
1.x, 2.x and master

**Description of the problem including expected versus actual behavior**:
Create an index and specify `dynamic: false` (or `strict`) for the only type it holds. Create a subfield of type object and specify `dynamic: true` for it. When creating a new object field under the dynamic one, it gets a different dynamic default depending on whether it was added via put mapping api (apparently taken from the main dynamic behaviour of the type) or dynamically created through index api (`true`, maybe ok because it was created dynamically). I would expect the new field to get the same default regardless of how it got added in the first place. I am not sure if its default value should be taken from the type of from its ancestor field.

**Steps to reproduce**:
- Create the index

```
curl -XPUT localhost:9200/index1 -d '{
    "mappings": {
      "strict_type": {
        "dynamic": "strict",
        "properties": {
          "dynamic_field": {
            "dynamic": "true",
            "type": "object"
          }
        }
      }
    }
}'
```
- Add a new field under `dynamic_field` using the put mapping api:

```
curl -XPOST localhost:9200/index1/strict_type/_mapping -d '{
    "properties": {
        "dynamic_field": {
            "properties": {
                  "subobject": {
                    "type" : "object"
                  }
            }
        }
    }
}
'
```
- Retrieve the mapping and verify that `subobject` gets the default dynamic behaviour of the type.

```
curl localhost:9200/_mapping?pretty

{
  "index1" : {
    "mappings" : {
      "strict_type" : {
        "dynamic" : "strict",
        "properties" : {
          "dynamic_field" : {
            "dynamic" : "true",
            "properties" : {
              "subobject" : {
                "type" : "object"
              }
            }
          }
        }
      }
    }
  }
}
```
- Also try and index a document with a new field under `subobject`, it gets rejected due strict mapping:

```
curl -XPUT localhost:9200/index1/strict_type/1 -d '{
    "dynamic_field" : {
        "subobject" : {
              "field2" : 123 
        }
    }
}
'

{"error":"StrictDynamicMappingException[mapping set to strict, dynamic introduction of [field2] within [dynamic_field.subobject] is not allowed]","status":400}
```
- Index a new document containing a new subobject field under `dynamic_field`:

```
curl -XPUT localhost:9200/index1/strict_type/1 -d '{
    "dynamic_field" : {
        "subobject2" : {
        }
    }
}
'
```
- Retrieve the mapping and verify that `subobject2` has `dynamic` set to `true`, omitting its `dynamic` behaviour

```
curl localhost:9200/_mapping?pretty

{
  "index1" : {
    "mappings" : {
      "strict_type" : {
        "dynamic" : "strict",
        "properties" : {
          "dynamic_field" : {
            "dynamic" : "true",
            "properties" : {
              "subobject" : {
                "type" : "object"
              },
              "subobject2" : {
                "type" : "object",
                "dynamic" : "true"
              }
            }
          }
        }
      }
    }
  }
}
```
</description><key id="147359746">17644</key><summary>Inconsistent dynamic default depending on how a field gets added</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T08:53:35Z</created><updated>2016-04-20T16:21:31Z</updated><resolved>2016-04-20T16:21:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-11T14:30:00Z" id="208374868">I suspect this is done on purpose since 99% of the time, if you add a dynamic object to your mappings, you will have a field below it.

That said I agree it is confusing that the default depends on the root as opposed to the parent.
</comment><comment author="clintongormley" created="2016-04-13T09:12:12Z" id="209324860">However the sub-object gets added, if `dynamic` is not explicitly specified at the sub-object level, it should inherit from its direct parent.
</comment><comment author="clintongormley" created="2016-04-13T09:20:26Z" id="209327530">This raises another problem...  The `dynamic` setting is updatable.  If we update the setting for a parent, the children should inherit the new setting.  I think this will not work today as the inherited dynamic setting is resolved when the field is added.
</comment><comment author="jpountz" created="2016-04-14T13:13:18Z" id="209936225">&gt;  The dynamic setting is updatable. If we update the setting for a parent, the children should inherit the new setting. I think this will not work today as the inherited dynamic setting is resolved when the field is added.

I think that could work actually. The way that the dynamic setting is resolved is the following:
1. if `dynamic` is set on the object, then use it
2. otherwise if `dynamic` is set on the root object, then use it
3. otherwise use the default which is `true`

So if we replace (2) with "otherwise recursively check if `dynamic` is set on the parent object, and use it" and dynamically add objects with `null` values for `dynamic` then things should work?
</comment><comment author="javanna" created="2016-04-20T16:21:25Z" id="212499406">Just tested this on master, it is fixed. No inconsistency anymore depending on how the field gets added. We look at the parent and take the `dynamic` value from it. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Turn RestChannel into an interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17643</link><project id="" key="" /><description>In #17133 we introduce request size limit handling and need a custom
channel implementation. In order to ensure we delegate all methods
it is better to have this channel implement an interface instead of
an abstract base class (so changes on the interface turn into
compile errors).

Relates #17133
</description><key id="147359215">17643</key><summary>Turn RestChannel into an interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T08:51:37Z</created><updated>2016-04-13T13:29:16Z</updated><resolved>2016-04-11T09:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-11T08:59:26Z" id="208239753">LGTM. Left one little request
</comment><comment author="danielmitterdorfer" created="2016-04-11T09:02:10Z" id="208240484">Thanks for the quick review. I'll add a Javadoc comment and then merge the PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in search api - fields doc. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17642</link><project id="" key="" /><description>remove extra `"`  in search api doc.
</description><key id="147348925">17642</key><summary>Fix typo in search api - fields doc. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PareshGupta</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-04-11T08:11:05Z</created><updated>2016-04-12T19:16:01Z</updated><resolved>2016-04-12T19:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-11T13:35:18Z" id="208346152">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="PareshGupta" created="2016-04-11T14:07:22Z" id="208363887">@dakrone Hi, I have already signed the Contributor License Agreement.
</comment><comment author="clintongormley" created="2016-04-12T19:16:01Z" id="209060326">thanks @PareshGupta 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to index docx file with mapper-attachments plugin that comes with Elasticsearch 2.3.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17641</link><project id="" key="" /><description>I run ES with the following setup:
- ES 2.3.1 and Mapper-attachments plugin that comes with it
- Oracle java 8
- Ubuntu 14.04

I saw the following error when I tried to index docx files:

```
[2016-04-11 15:01:37,521][DEBUG][mapper.attachment        ] Failed to extract [100000] characters of text for [null]: [Unexpected RuntimeException from org.apache.tika.parser.microsoft.ooxml.OOXMLParser@3e3917f2]
[2016-04-11 15:01:37,521][TRACE][mapper.attachment        ] exception caught
org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.microsoft.ooxml.OOXMLParser@3e3917f2
    at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:282)
    at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
    at org.apache.tika.Tika.parseToString(Tika.java:537)
    at org.elasticsearch.mapper.attachments.TikaImpl$1.run(TikaImpl.java:94)
    at org.elasticsearch.mapper.attachments.TikaImpl$1.run(TikaImpl.java:91)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.mapper.attachments.TikaImpl.parse(TikaImpl.java:91)
    at org.elasticsearch.mapper.attachments.AttachmentMapper.parse(AttachmentMapper.java:481)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:309)
    at org.elasticsearch.index.mapper.DocumentParser.parseValue(DocumentParser.java:436)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:262)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:580)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnReplica(IndexShard.java:569)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnReplica(TransportIndexAction.java:185)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:170)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnReplica(TransportIndexAction.java:66)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncReplicaAction.doRun(TransportReplicationAction.java:392)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:291)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReplicaOperationTransportHandler.messageReceived(TransportReplicationAction.java:283)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.poi.openxml4j.exceptions.InvalidOperationException: Can't open the specified file: '/tmp/apache-tika-2211524028819836878.tmp'
    at org.apache.poi.openxml4j.opc.ZipPackage.&lt;init&gt;(ZipPackage.java:112)
    at org.apache.poi.openxml4j.opc.OPCPackage.open(OPCPackage.java:225)
    at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:69)
    at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)
    at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
    ... 27 more
Caused by: java.util.zip.ZipException: error in opening zip file
    at java.util.zip.ZipFile.open(Native Method)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:220)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:150)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:164)
    at org.apache.poi.openxml4j.util.ZipSecureFile.&lt;init&gt;(ZipSecureFile.java:105)
    at org.apache.poi.openxml4j.opc.internal.ZipHelper.openZipFile(ZipHelper.java:175)
    at org.apache.poi.openxml4j.opc.ZipPackage.&lt;init&gt;(ZipPackage.java:110)
    ... 31 more

```

I thought ES 2.3.1 should have fixed the indexing issue for docx files, but it's still no luck for me.
</description><key id="147339808">17641</key><summary>Failed to index docx file with mapper-attachments plugin that comes with Elasticsearch 2.3.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clarencechan</reporter><labels><label>:Plugin Mapper Attachment</label><label>feedback_needed</label></labels><created>2016-04-11T07:30:59Z</created><updated>2016-04-18T10:49:51Z</updated><resolved>2016-04-17T19:01:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-11T07:54:16Z" id="208211713">Could you update your description and paste the full stacktrace using 3 ticks ( ` ) at the beginning and end of the stacktrace?

See https://guides.github.com/features/mastering-markdown/ (Syntax highlighting).
</comment><comment author="clarencechan" created="2016-04-11T09:00:05Z" id="208240046">Updated.
</comment><comment author="clintongormley" created="2016-04-12T18:35:39Z" id="209045350">For some reason, java zip can't open the file.  Is there any more exception to shed light on that? Are you sure the file is readable?  

What happens if you run elasticsearch as:

```
./bin/elasticsearch --security.manager.enabled false
```
</comment><comment author="clarencechan" created="2016-04-15T08:10:25Z" id="210348041">&gt; Is there any more exception to shed light on that?

I could not find any other exceptions.

&gt; ./bin/elasticsearch --security.manager.enabled false

Same result

&gt; Are you sure the file is readable?

I've done a few more tests with mixed results. Some docx were ok while some were not. But I have no problem opening all the docx from Microsoft Word. Hence, they are all readable.
</comment><comment author="clintongormley" created="2016-04-15T08:18:58Z" id="210354456">Any chance you could make the document available to us so we could try it out?
</comment><comment author="clarencechan" created="2016-04-15T08:42:31Z" id="210363857">[test1.docx](https://github.com/elastic/elasticsearch/files/220587/test1.docx) is NOT ok
[test2a.docx](https://github.com/elastic/elasticsearch/files/220589/test2a.docx) is ok
</comment><comment author="clarencechan" created="2016-04-15T09:39:20Z" id="210386349">Sry, I messed up with my test files. test1.docx is actually ok. I will upload the files once again after i sorted out my test files.
</comment><comment author="clarencechan" created="2016-04-17T19:01:13Z" id="211083931">I found the problem. It's the golang driver that caused the problem. It's not an ES bug.
</comment><comment author="clintongormley" created="2016-04-18T10:49:51Z" id="211325037">thanks for letting us know @clarencechan 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update documentation around order/formatting of JSON when Source filt&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17640</link><project id="" key="" /><description>&#8230;ering

Update documentation to indicate that the order and formatting of the JSON returned
may not match the original passed in at index time when Source filtering is performed.

Closes https://github.com/elastic/elasticsearch/issues/17639
</description><key id="147321943">17640</key><summary>update documentation around order/formatting of JSON when Source filt&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>docs</label></labels><created>2016-04-11T05:43:45Z</created><updated>2016-04-12T22:01:12Z</updated><resolved>2016-04-12T18:11:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-11T05:55:19Z" id="208175361">LGTM
</comment><comment author="jpountz" created="2016-04-11T06:15:23Z" id="208180084">I am slightly concerned that it implies that the ordering and formatting is preserved otherwise. Maybe the docs should say that there is no guarantee at all about ordering and formatting? For instance, the update API is also known to change ordering/formatting and more generally having such guarantees may block future improvements to store the _source more efficiently.
</comment><comment author="dadoonet" created="2016-04-11T08:11:32Z" id="208218849">&gt; I am slightly concerned that it implies that the ordering and formatting is preserved otherwise.

Indeed. I agree.
</comment><comment author="clintongormley" created="2016-04-12T18:11:49Z" id="209037691">There are tons of places where the order will change - anytime the `_source` has to be parsed the order can change which includes source filtering, ingest, update, conversion from YAML/CBOR/SMILE to JSON or vice versa.  In fact, most languages specifically ensure the keys in their hashmaps will have a different order on every run to avoid denial of service attacks using hash collisions.

On top of that, we may change the internals of how the source is stored in the future (eg https://github.com/elastic/elasticsearch/issues/9034)

The JSON spec says:

&gt; An object is an unordered set of name/value pairs. 

We use JSON. I don't think we need to change the docs at all here.
</comment><comment author="djschny" created="2016-04-12T22:01:12Z" id="209122044">Looks like this was closed prematurely before I had time to update the PR so that documents were updated more globally on the `_source` page as opposed to the individual feature pagers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_source_exclude changes order of fields returned in _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17639</link><project id="" key="" /><description>**Elasticsearch version**: 2.3

**JVM version**: jdk1.8.0_74

**OS version**: Mac 10.11.4

**Description of the problem including expected versus actual behavior**: When using `_source_exclude` it appears the return json for `_source` is being modified and fields are being returned out of order

**Steps to reproduce**:
1. Index the following example document:

```
PUT /testindex/doc/1
{ "fielda": "one", "fieldb": "two", "fieldc": "three" }
```
1. Perform a search with `_source_exclude` with both a missing and existing field and notice the order of the json returned as changed:

**Request**

```
GET /testindex/_search?_source_exclude=nonexistantField
```

**Response**

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "testindex",
        "_type": "doc",
        "_id": "1",
        "_score": 1,
        "_source": {
          "fielda": "one",
          "fieldc": "three",
          "fieldb": "two"
        }
      }
    ]
  }
}
```

**Request**

```
GET /testindex/_search?_source_exclude=fielda
```

**Response**

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 1,
    "hits": [
      {
        "_index": "testindex",
        "_type": "doc",
        "_id": "1",
        "_score": 1,
        "_source": {
          "fieldc": "three",
          "fieldb": "two"
        }
      }
    ]
  }
}
```
</description><key id="147300726">17639</key><summary>_source_exclude changes order of fields returned in _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels /><created>2016-04-11T02:53:50Z</created><updated>2016-04-13T11:15:32Z</updated><resolved>2016-04-12T18:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-11T04:58:01Z" id="208163072">Field order does not matter in JSON.
And yes with source filtering we have to generate on the fly a new source.

So I don't see the issue here TBH.
</comment><comment author="djschny" created="2016-04-11T05:07:19Z" id="208164411">While it doesn't make a difference from a JSON spec standpoint, we have always made the advertisement that `_source` is the exact JSON string you sent in saved and returned to you. Our docs even state this:

https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html

&gt; The _source field contains the original JSON document body that was passed at index time.

Since we've always advertised this (and to be honest in training a I even say you can "curl up" a json document you have in a file, and then request the _source, write it to a file and do a diff and they should be equal) I feel it's important we either adhere to it, or we need to update documentation to reflect that in situations where Elasticsearch needs to parse the `_source` field and generate a new one (in the case of filtering like you mention), that the ordering can be different.

I'd be happy to help with documentation updates if we decide to go that route.
</comment><comment author="dadoonet" created="2016-04-11T05:16:33Z" id="208166726">Yes. We also modify the source IIRC when you use exclude in mapping.

That's the only features IMO where we do that.

As a user, I'm expecting a modification of the source because I explicitly ask to modify the source :)

But I agree that we should probably add this in documentation.
</comment><comment author="djschny" created="2016-04-11T05:44:34Z" id="208172930">I added notes where it seemed applicable in the docs. See referenced PR.
</comment><comment author="clintongormley" created="2016-04-12T18:12:24Z" id="209037871">Closing.  See https://github.com/elastic/elasticsearch/pull/17640#issuecomment-209037691
</comment><comment author="djschny" created="2016-04-12T22:00:16Z" id="209121807">Why close this? I'm happy to update the document to be more accurate based upon the discussion in this thread. I feel it is very important to be clear to folks about this as opposed to stating `The _source field contains the original JSON document body that was passed at index time.` we can add clarification that when _source is returned may be different from what was passed in?

How can the extra clarification hurt?
</comment><comment author="clintongormley" created="2016-04-13T11:15:32Z" id="209372601">&gt; How can the extra clarification hurt?

Extra clarification can hurt when it obscures more important and more relevant information.  Too much information is as bad as not enough, so I'd rather not clutter the docs with something that I don't think needs clarifying.  We use JSON, so the assumption is that keys are unordered.  Why do we need to repeat this statement?  We don't state that JSON has to be UTF8 encoded, because the JSON spec already states this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improvements to the IndicesService class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17638</link><project id="" key="" /><description>This commit contains the following improvements/fixes:
1. Renaming method names and variables to better reflect the purpose
   of the method and the semantics of the variable.
2. For deleting indexes, replace the `closed` parameter passed to the delete index/store methods with obtaining the index's state from the `IndexSettings` that is already passed in.
3. Adding tests to the `IndexWithShadowReplicaIT` suite, some of which show issues in the shadow replica delete process that are captured in #17695 
</description><key id="147299938">17638</key><summary>Improvements to the IndicesService class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Shadow Replicas</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T02:49:00Z</created><updated>2016-05-02T12:06:12Z</updated><resolved>2016-04-14T15:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-12T18:33:45Z" id="209044662">@abeyad left some initial feedback
</comment><comment author="abeyad" created="2016-04-13T02:32:08Z" id="209199372">@bleskes PR has been updated based on your feedback and our discussion.  Mainly, I've just added some tests to `IndexWithShadowReplicaIT`.
</comment><comment author="bleskes" created="2016-04-13T07:54:51Z" id="209290784">Looking good. Left some comments w.r.t tests.
</comment><comment author="abeyad" created="2016-04-13T18:36:38Z" id="209587432">@bleskes Updated the PR, and also added this test: https://github.com/elastic/elasticsearch/pull/17638/files#diff-2bb1232b0406b77c64e5285178afb63aR204
</comment><comment author="bleskes" created="2016-04-14T14:24:11Z" id="209968219">LGTM. Left one little ask. no need for another review..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>installing plugin fails with java.net.MalformedURLException in ES 5.0 alpha 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17637</link><project id="" key="" /><description>Hello, i just deployed ES 5 alpha 1 on my test machine an ran into this error:

**Elasticsearch version**:

{
  "name" : "magrathea",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0-alpha1",
    "build_hash" : "7d4ed5b",
    "build_date" : "2016-04-04T10:39:25.841Z",
    "build_snapshot" : false,
    "lucene_version" : "6.0.0"
  },
  "tagline" : "You Know, for Search"
}

**JVM version**:
[root@magrathea elasticsearch]# java -version
openjdk version "1.8.0_77"
OpenJDK Runtime Environment (build 1.8.0_77-b03)
OpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode)

**OS version**:
[root@magrathea elasticsearch]# cat /etc/redhat-release 
Fedora release 23 (Twenty Three)

**Description of the problem including expected versus actual behavior**:
Plugin installation fails, the steps are taken from https://www.elastic.co/guide/en/elasticsearch/plugins/master/installation.html  

[root@magrathea elasticsearch]# bin/elasticsearch-plugin install lmenezes/elasticsearch-kopf
-&gt; Downloading lmenezes/elasticsearch-kopf
Exception in thread "main" java.net.MalformedURLException: no protocol: lmenezes/elasticsearch-kopf
    at java.net.URL.&lt;init&gt;(URL.java:593)
    at java.net.URL.&lt;init&gt;(URL.java:490)
    at java.net.URL.&lt;init&gt;(URL.java:439)
    at org.elasticsearch.plugins.InstallPluginCommand.downloadZip(InstallPluginCommand.java:219)
    at org.elasticsearch.plugins.InstallPluginCommand.download(InstallPluginCommand.java:214)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:175)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:163)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:57)
</description><key id="147287148">17637</key><summary>installing plugin fails with java.net.MalformedURLException in ES 5.0 alpha 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rtznprmpftl</reporter><labels><label>docs</label><label>v5.0.0-alpha2</label></labels><created>2016-04-11T01:16:58Z</created><updated>2017-05-09T08:16:13Z</updated><resolved>2016-04-12T17:28:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-11T02:46:26Z" id="208131470">Thank you for reporting.
We need to update the guide.

Note that from 5.0, site plugins are not supported anymore.
</comment><comment author="clintongormley" created="2016-04-12T17:28:39Z" id="209018848">Closed in https://github.com/elastic/elasticsearch/commit/098b2e03b54054e1df53a47d33be6fc6e6f6e082
</comment><comment author="rajasekaran07" created="2017-01-18T13:28:16Z" id="273474857">Still I am seeing this issue. 

I am using elasticsearch 5.1.1 and java 1.8

when i am trying to install license plugin i am getting below message.

root@treselle-VirtualBox:/usr/share/elasticsearch# bin/elasticsearch-plugin install https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.4.4/marvel-agent-2.4.4.zip
-&gt; Downloading https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/marvel-agent/2.4.4/marvel-agent-2.4.4.zip
[=================================================] 100%&#194;&#160;&#194;&#160;
ERROR: `elasticsearch` directory is missing in the plugin zip

-Raja</comment><comment author="dadoonet" created="2017-01-18T13:39:32Z" id="273477348">@rajasekaran07 Ask questions on discuss.elastic.co.

Marvel 2.4.4 is not compatible with elasticsearch 5.x. Read about x-pack now.</comment><comment author="rajasekaran07" created="2017-01-18T14:27:42Z" id="273488659">@dadoonet thanks for your response may I know how to get access logs for my elasticsearch.

Regards,
Raja</comment><comment author="dadoonet" created="2017-01-18T15:01:25Z" id="273498438">@rajasekaran07 Ask questions on discuss.elastic.co.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index not removed from the alias after snapshot restore.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17635</link><project id="" key="" /><description>Elasticsearch version : 1.7.2
1. Create two cluster : A and B
2. Add index i1 in A
3. Add an alias i for i1 in A
4. Then take a full snapshot of A with default settings 
5. Add index i1 and index i2 in B
6. Add an alias i for both i1 and i2 in B
7. Delete i1 and i2 in B
8. Restore the snapshot of A in B with default settings

*_Expected *_
Alias i should only contain i1 in B after restore.

*_Actual  *_
Alias i contains i1 and i2 in B, even though i2 didn't exist in B. 
</description><key id="147076018">17635</key><summary>Index not removed from the alias after snapshot restore.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JeffreyZZ</reporter><labels><label>:Snapshot/Restore</label><label>feedback_needed</label></labels><created>2016-04-09T01:16:49Z</created><updated>2016-04-19T07:42:07Z</updated><resolved>2016-04-12T17:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-12T17:52:03Z" id="209027569">Hi @JeffreyZZ 

I've tried this out and am unable to replicate this, which makes sense given that aliases are stored per index.  So if `i2` existed in B after the restore, then the index i2 must either be in the snapshot or in the cluster (or imported as a dangling index from a node joining later).

Here's what I did:

On cluster A:

```
DELETE *

PUT i1

PUT i1/_alias/i

PUT /_snapshot/my_backup
{
  "type": "fs",
  "settings": {
    "location": "/Users/clinton/workspace/servers/foo/"
  }
}

PUT /_snapshot/my_backup/snapshot_1?wait_for_completion=true
```

On cluster B:

```
PUT i1

PUT i2

PUT i1,i2/_alias/i

GET _alias

DELETE i1,i2

PUT /_snapshot/my_backup
{
  "type": "fs",
  "settings": {
    "location": "/Users/clinton/workspace/servers/foo/"
  }
}

POST /_snapshot/my_backup/snapshot_1/_restore

GET _alias
```

this returns: 

```
{
  "i1": {
    "aliases": {
      "i": {}
    }
  }
}
```
</comment><comment author="clintongormley" created="2016-04-12T17:52:17Z" id="209027653">Feel free to reopen if you can come up with a recreation
</comment><comment author="JeffreyZZ" created="2016-04-18T23:02:23Z" id="211621508">Sorry. For Step 7, instead of deleting the i1 and i2 in B, please close i1 and i2 in B. 
1. Close i1 and i2 in B
</comment><comment author="clintongormley" created="2016-04-19T07:42:07Z" id="211778218">Hi @JeffreyZZ 

This is expected.  index i2 is not removed during the restore process, and aliases are stored in the index metadata, so alias `i` ends up pointing to the existing `i2` and the newly restored `i1`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging tests fail and package installation is dependent on user's umask</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17634</link><project id="" key="" /><description>When running the packages on my development machines, they fail due to incorrect expected permissions, for example, my `umask` permissions are 002, which means that directories are created with 775 permissions, however, our tests compare the directories from the _user_ with the ones generated by a _root_ install, which has a different umask (022).

Thus, you get:

```
# Expected privileges: 775, found 755
not ok 10 [TAR PLUGINS] install jvm-example plugin with a custom CONFIG_DIR
# (from function `assert_file' in file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash, line 181,
#  from function `install_jvm_example' in file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/plugins.bash, line 87,
#  in test file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/25_tar_plugins.bats, line 143)
#   `CONF_DIR="$ESCONFIG" install_jvm_example' failed
```

And

```
# Expected privileges: 644, found 664
not ok 27 [TAR PLUGINS] check lang-expression module
# (from function `assert_file' in file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash, line 181,
#  from function `assert_module_or_plugin_file' in file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/packaging_test_utils.bash, line 208,
#  from function `check_module' in file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/modules.bash, line 35,
#  from function `check_secure_module' in file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/modules.bash, line 43,
#  in test file /elasticsearch/qa/vagrant/src/test/resources/packaging/scripts/25_tar_plugins.bats, line 242)
#   `check_secure_module lang-expression antlr4-runtime-*.jar asm-5.0.4.jar asm-commons-*.jar asm-tree-*.jar lucene-expressions-*.jar' failed
# Should exist: /tmp/elasticsearch/modules/lang-expression
```

(and many others)

I think there are multiple options for fixing this:
1. Set the umask bats will expect in the tests before these tests are run
2. Change the tests to hardcode the correct "expected" permissions instead of
   inferring them from the user's directory
3. Change our packaging to set the permissions correctly instead of relying on
   umask

Personally, I think we should go with number 3.

Number 1 I'm not actually sure would work, I haven't tested it, but we would be
fooling ourselves if anyone actually changed their umask in the field.

Number 2 would be semi-okay, but again, if the umask changes or a user installs
the package as non-root (yes, with `dpkg --force-not-root`) the permissions will
be different because root's umask will not be used.

I really don't like the variance of number 1 and number 2, we have a lot of
checks to tell users when their settings may be wrong, this is a situation where
we should set the permissions to the exact values they should be to prevent
unintended errors. Hence, I believe we should go with number 3.
</description><key id="147056738">17634</key><summary>Packaging tests fail and package installation is dependent on user's umask</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2016-04-08T22:38:57Z</created><updated>2016-04-21T19:27:27Z</updated><resolved>2016-04-21T19:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-14T19:14:08Z" id="210105806">I think that we should set the permissions to what we think they should be rather than relying on the end-user umask. If the end-user has their umask wrong, we could expose permissions on directories that we don't want to exposed. While a valid claim can be made that it is the end-users responsibility, and that going against the umask possibly violates the principle of least astonishment, I would rather us choose the more secure option here. Thus, I favor option 3.
</comment><comment author="nik9000" created="2016-04-18T21:35:44Z" id="211593056">&gt; I think that we should set the permissions to what we think they should be rather than relying on the end-user umask.

I think we should set the permissions to what we think they should be. To me the experience with the deb and rpm is paramount - all the permissions should "just work". 644, root owned, whatever. Just set the permissions to what we think they should be. If the user doesn't like them I expect they'll change them with puppet/chef/ansible/bash scripts/duct tape/bubblegum.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17633</link><project id="" key="" /><description>Remove an unnecessary 's' in master.
</description><key id="147047722">17633</key><summary>Fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnowif</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-04-08T21:45:23Z</created><updated>2016-04-12T16:28:00Z</updated><resolved>2016-04-12T16:28:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-08T21:53:06Z" id="207622046">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="rnowif" created="2016-04-09T08:16:36Z" id="207743144">Hi,

I signed it right after my submission. Should I do something more like send it to you ?
</comment><comment author="clintongormley" created="2016-04-12T16:27:54Z" id="208991823">found it @rnowif - thanks, merging
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPEs from QueryBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17632</link><project id="" key="" /><description>Now that we've changed queries to be registered by their parsers and readers instead of PROTOTYPEs we can remove the PROTOTYPEs entirely. This does that.

Relates to #17085
</description><key id="147041896">17632</key><summary>Remove PROTOTYPEs from QueryBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T21:17:02Z</created><updated>2016-04-12T15:34:21Z</updated><resolved>2016-04-12T15:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-12T09:59:03Z" id="208826843">Left one minor comment, rest LGTM.
</comment><comment author="nik9000" created="2016-04-12T11:40:43Z" id="208859037">@javanna fixed the line breaks and removed the toString error reporting so I can make a new PR with it.
</comment><comment author="javanna" created="2016-04-12T11:50:18Z" id="208862492">@nik9000 no need for another review, thanks for addressing
</comment><comment author="nik9000" created="2016-04-12T12:19:03Z" id="208875635">OK! Merging!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc_count metric agg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17631</link><project id="" key="" /><description>This might sound entirely trivial, but this would be a real boon for consistency in our aggregation parsing code. `doc_count` is currently a property of each bucket in a bucket agg, which makes it an exception when walking the aggregation tree. 

If we could get a dedicated `doc_count` metric agg we could handle it the same way we handle every other metric agg, even if its only purpose is to duplicate the `doc_count` property of the bucket.
</description><key id="147035756">17631</key><summary>doc_count metric agg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rashidkpc</reporter><labels><label>:Aggregations</label><label>adoptme</label></labels><created>2016-04-08T20:47:03Z</created><updated>2016-11-03T17:13:33Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2016-04-08T20:47:23Z" id="207599247">@kimchy here's that issue we talked about
</comment><comment author="kimchy" created="2016-04-08T21:10:23Z" id="207607646">thanks @rashidkpc, @clintongormley is this something we can see if we can do, @colings86, it doesn't sound terribly tricky? It will help a lot for a potential improvement to visualize in Kibana 
</comment><comment author="jpountz" created="2016-04-09T12:52:21Z" id="207787180">Aren't there concerns about increasing the size of the response with duplicate data? For instance a terms&gt;doc_count aggregation would have twice the same amount of json as the terms aggregation alone even though it doesn't add information.
</comment><comment author="kimchy" created="2016-04-09T18:34:41Z" id="207827911">It will increase it, but on the other hand it will mean much simplified and more generic code for handling things in tools like Kibana. To me it is a question of consistency. We can reduce it heavily with compression for example, and CBOR (or smile if we are lucky). 
</comment><comment author="jpountz" created="2016-04-09T19:15:25Z" id="207836353">I understand the consistency argument. I just want to ensure it will be useful in the long term and that we won't go back to having special handling of the `doc_count` property in 6 months because the extra ease of consumption of the response is not considered worth the increased size of the response (either because of network trafic or load on the parser on client side).
</comment><comment author="kimchy" created="2016-04-09T20:09:25Z" id="207846907">Just so I understand, for now, I don't suggest removing the doc count property, just adding the count aggregation. I remember that we wanted to reduce the size of the response for something common as doc count, I think we can rethink if it is needed once we have the count agg?
</comment><comment author="colings86" created="2016-04-11T10:11:22Z" id="208267354">@rashidkpc Are you looking for the response format of `doc_count` to be the same as the other aggregations or do you need the request format to be the same as well? If it's just the response format, could we not just change the format for how we output doc count so instead of:

```
"doc_count": 200
```

We output:

```
"doc_count": {
  "value": 200
}
```

But this would still be done in the bucket itself rather than adding a new aggregation type for it?

If you need it to be the same on the request side too obviously this would not work.
</comment><comment author="rashidkpc" created="2016-04-11T15:39:29Z" id="208407180">@colings86 We need it to function like a real metric agg. We need it to be nameable, but we also need to be able to use it at the top of the aggregation tree, eg, outside of a bucket agg. 

The other option is to use a `filter` agg and just not stick anything under it. Is there a performance drawback there?

```
GET /usagov*/_search
{
 "size": 0,
 "aggs": {
  "doc_count": {
   "filter": {"match_all": {}}
  }
 }
}
```

In this case we would end with the property containing the value being called `doc_count`, but we already need different handling for most metric aggs, so its not a big deal. 
</comment><comment author="colings86" created="2016-04-11T16:24:10Z" id="208433822">Another possible alternative that will not require additional memory on the shards is to use the `bucket_script` pipeline aggregation. An example of this would be:

```
GET test/_search
{
  "size": 0, 
  "aggs": {
    "terms": {
      "terms": {
        "field": "i",
        "size": 10
      },
      "aggs": {
        "doc_count_agg": {
          "bucket_script": {
            "buckets_path": "_count",
            "script": {
              "inline": "_value",
              "lang": "expression"
            }
          }
        }
      }
    }
  }
}
```

Note that I have named the bucket_script aggregation `doc_count_agg` to avoid clashing with `doc_count` due to https://github.com/elastic/elasticsearch/issues/17652
</comment><comment author="rashidkpc" created="2016-04-11T22:49:53Z" id="208599677">Given that, would it made sense to add the syntactic sugar to just expose that bucket script as a dedicated doc_count agg?
</comment><comment author="colings86" created="2016-04-12T14:06:09Z" id="208923453">We don't currently have the ability to add the syntactic sugar for this but we could add a rewrite phase to the AggregatorBuilders so we can rewrite aggregations like this. We would have to do this differently for the top-level doc_count than sub agg doc_counts since `bucket_script` only works on buckets and not at the top-level. For the top-level we'll need to implement it as a MatchAllDocs filter agg instead.
</comment><comment author="colings86" created="2016-04-12T14:10:54Z" id="208925091">I opened https://github.com/elastic/elasticsearch/issues/17676 for the syntactic sugar feature
</comment><comment author="costin" created="2016-11-03T17:13:33Z" id="258209796">+1 from my side.
As a side note,  value_count could potentially be optimized to return the doc count by using a shortcut such as `value_count("_id")` (which currently doesn't work since `_id` does not support field data) or `value_count(1)`. This is basically similar `COUNT(1)` or `COUNT(*)` in SQL.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Primary shard failures should not block other primary shard recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17630</link><project id="" key="" /><description>On master, if I have an index with 5 primary shards, and I corrupt the `0` and
`1` shards, once ES hits a corrupt shard, it will not attempt to recovery the
next primary shard.

**Steps to reproduce**:
1. Create an index with 5 shards
2. Corrupt the 0 and 1 shards (I used https://github.com/joshsegall/corrupt
   to add bytes to the end of the `segments_#` files)
3. Start up ES

Here's what I get:

```
[2016-04-08 14:11:35,850][WARN ][indices.cluster          ] [Captain Zero] [[i][1]] marking and sending shard failed due to [failed recovery]
[i/QzoKda9aQCG_hCaZQ18GEg][[i][1]] IndexShardRecoveryException[failed to fetch index version after copying it over]; nested: IndexShardRecoveryException[shard allocated for local recovery (post api), should exist, but doesn't, current files: [segments_3, write.lock]]; nested: IndexFormatTooOldException[Format version is not supported (resource BufferedChecksumIndexInput(NIOFSIndexInput(path="/home/hinmanm/scratch/elasticsearch-5.0.0-alpha1-SNAPSHOT/data/elasticsearch/nodes/0/indices/QzoKda9aQCG_hCaZQ18GEg/1/index/segments_3"))): 317397653 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 5.0 and later.];
    at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:208)
    at org.elasticsearch.index.shard.StoreRecovery.lambda$recoverFromStore$0(StoreRecovery.java:78)
    at org.elasticsearch.index.shard.StoreRecovery.executeRecovery(StoreRecovery.java:123)
    at org.elasticsearch.index.shard.StoreRecovery.recoverFromStore(StoreRecovery.java:76)
    at org.elasticsearch.index.shard.IndexShard.recoverFromStore(IndexShard.java:1092)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$2(IndicesClusterStateService.java:636)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:408)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: [i/QzoKda9aQCG_hCaZQ18GEg][[i][1]] IndexShardRecoveryException[shard allocated for local recovery (post api), should exist, but doesn't, current files: [segments_3, write.lock]]; nested: IndexFormatTooOldException[Format version is not supported (resource BufferedChecksumIndexInput(NIOFSIndexInput(path="/home/hinmanm/scratch/elasticsearch-5.0.0-alpha1-SNAPSHOT/data/elasticsearch/nodes/0/indices/QzoKda9aQCG_hCaZQ18GEg/1/index/segments_3"))): 317397653 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 5.0 and later.];
    at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:193)
    ... 9 more
Caused by: org.apache.lucene.index.IndexFormatTooOldException: Format version is not supported (resource BufferedChecksumIndexInput(NIOFSIndexInput(path="/home/hinmanm/scratch/elasticsearch-5.0.0-alpha1-SNAPSHOT/data/elasticsearch/nodes/0/indices/QzoKda9aQCG_hCaZQ18GEg/1/index/segments_3"))): 317397653 (needs to be between 1071082519 and 1071082519). This version of Lucene only supports indexes created with release 5.0 and later.
    at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:295)
    at org.apache.lucene.index.SegmentInfos.readCommit(SegmentInfos.java:284)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:441)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:438)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:685)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:637)
    at org.apache.lucene.index.SegmentInfos.readLatestCommit(SegmentInfos.java:443)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:123)
    at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:191)
    at org.elasticsearch.index.store.Store.readLastCommittedSegmentsInfo(Store.java:176)
    at org.elasticsearch.index.shard.StoreRecovery.internalRecoverFromStore(StoreRecovery.java:184)
```

Which is great, and expected (I corrupted the files).

What I do expect is that ES will skip this shard (shard 1) since it failed and
try shard 0, instead, it gives up and never gets that far, leaving it unassigned
and un-tried:

```
index shard prirep state      docs store ip        node
i     4     p      STARTED       0  130b 127.0.0.1 Captain Zero
i     4     r      UNASSIGNED
i     3     p      STARTED       0  130b 127.0.0.1 Captain Zero
i     3     r      UNASSIGNED
i     2     p      STARTED       0  130b 127.0.0.1 Captain Zero
i     2     r      UNASSIGNED
i     1     p      UNASSIGNED
i     1     r      UNASSIGNED
i     0     p      UNASSIGNED
i     0     r      UNASSIGNED
```

This might not sound like a big deal, however, for the shard that was tried
(shard 1), `UnassignedInfo` will contain the `IndexShardRecoveryException` with
all the details about the file being corrupt and all that. For the shard that
was never tried (shard 0), it doesn't have those details (even though I
corrupted the shard).

It would be great if both shards' `UnassignedInfo` had any exception that
occurred during recovery.
</description><key id="147031600">17630</key><summary>Primary shard failures should not block other primary shard recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2016-04-08T20:29:13Z</created><updated>2016-04-11T22:06:25Z</updated><resolved>2016-04-11T22:06:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-11T10:02:47Z" id="208265337">In order to assign primaries the master goes out and asks the node if they have data for those shards. With 2.x we actually try to [read the lucene index](org.elasticsearch.index.store.Store#tryOpenIndex) before returning a response. If the nodes fail to open the index the shard will not be assigned to them and will be left unassigned (assuming no other copy is available) and nothing will change in the unassigned info (at least today).  Currently you can use the shard stores API to get the same information the master uses (and this should be added IMO to the new allocation explain API ).

What surprise me in this report is that shard 1 actually managed to be assigned. It seems the exceptions comes from reading the segments info, which is also read by the [shard data fetching](TransportNodesListGatewayStartedShards) action mentioned above. Do you have any idea what this may have happened?
</comment><comment author="dakrone" created="2016-04-11T13:30:21Z" id="208342681">&gt; Do you have any idea what this may have happened?

The only thing I could think of is that the shard store fetching returned successfully and then it tried to open the actual index. What's interesting is that the same corruption was added to both the 0 and 1 shards
</comment><comment author="bleskes" created="2016-04-11T21:54:00Z" id="208578469">@dakrone I found the source of the race condition and opened #17663 . I'm not sure it fully closes this issue as I think part of the problem is proper reporting. I think the allocation explain API should cover this. Do you agree and there an issue we can close this in favor of?
</comment><comment author="dakrone" created="2016-04-11T22:06:25Z" id="208583619">&gt; I think the allocation explain API should cover this. Do you agree and there an issue we can close this in favor of?

Yeah, I've already started integrating the shard stores API into the allocation explain API, so I'll close this for now. Thanks for opening the PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Output JAVA_HOME during builds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17629</link><project id="" key="" /><description>This commit enhances the build logging output to also output JAVA_HOME
(and, if applicable, org.gradle.java.home).

Relates #17628 
</description><key id="147007075">17629</key><summary>Output JAVA_HOME during builds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>build</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T18:49:26Z</created><updated>2016-04-08T18:53:05Z</updated><resolved>2016-04-08T18:53:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-08T18:50:32Z" id="207554069">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't build due to "update your JDK to at least"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17628</link><project id="" key="" /><description>**Elasticsearch version**: sha df8a971966b25d9b7025a6421e92f1b42685992c

**JVM version**: java version "1.8.0_77"

**OS version**: Mac OS X 10.11.3 (15D21)

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. gradle build

**Provide logs (if relevant)**:

```
FAILURE: Build failed with an exception.

* Where:
Build file '***/elastic/elasticsearch/core/build.gradle' line: 24

* What went wrong:
A problem occurred evaluating project ':core'.
&gt; Failed to apply plugin [id 'elasticsearch.build']
   &gt; JDK Oracle Corporation 1.8.0 has compiler bug JDK-8052388, update your JDK to at least 8u40

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED
```
</description><key id="147001522">17628</key><summary>Can't build due to "update your JDK to at least"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Randgalt</reporter><labels /><created>2016-04-08T18:22:47Z</created><updated>2016-04-11T16:40:28Z</updated><resolved>2016-04-11T16:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-08T18:36:36Z" id="207550123">I suspect that your `JAVA_HOME` is still pointing to an older version? Can you provide the logs that really shows this is running against 8u77? I'm running 8u77 and I do not have this issue.

``` bash
$ echo $JAVA_HOME
/Library/Java/JavaVirtualMachines/jdk1.8.0_77.jdk/Contents/Home
$ gradle build
.
.
.
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.12
  OS Info               : Mac OS X 10.11.4 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_77 [Java HotSpot(TM) 64-Bit Server VM 25.77-b03]
.
.
.
$
```

so it builds successfully for me.
</comment><comment author="Randgalt" created="2016-04-11T16:36:14Z" id="208438356">That fixed it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide more information about open contexts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17627</link><project id="" key="" /><description>Sometimes we get a test failure caused by search contexts left open.
The tests include a stack trace of the call that opened the context
but nothing else about the context. This adds more information about
the context that has been left open like what query it was running,
what shard it targeted, and whether or not it was a scroll.

Relates to #17582
</description><key id="146976960">17627</key><summary>Provide more information about open contexts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T16:34:34Z</created><updated>2016-04-11T01:03:48Z</updated><resolved>2016-04-11T01:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-08T16:34:49Z" id="207505608">@jpountz is this a thing you can review?
</comment><comment author="jpountz" created="2016-04-08T16:43:27Z" id="207508098">I left minor comments, but +1 to making it easier to debug!
</comment><comment author="nik9000" created="2016-04-10T00:58:54Z" id="207892581">@jpountz pushed a new commit.
</comment><comment author="jpountz" created="2016-04-10T17:58:38Z" id="208032569">LGTM
</comment><comment author="nik9000" created="2016-04-11T01:03:48Z" id="208105563">Thanks for the review @jpountz !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest does not close its factories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17626</link><project id="" key="" /><description>When you implement an ingest factory which implements `Closeable`:

``` java
public static final class Factory extends AbstractProcessorFactory&lt;MyProcessor&gt; implements Closeable {
    @Override
    public void close() throws IOException {
        logger.debug("closing my processor factory");
    }
}
```

The `close()` method is never called which could lead to some leak threads when we close a node.

The `ProcessorsRegistry#close()` method exists though and seems to do the right job:

``` java
@Override
public void close() throws IOException {
    List&lt;Closeable&gt; closeables = new ArrayList&lt;&gt;();
    for (Processor.Factory factory : processorFactories.values()) {
        if (factory instanceof Closeable) {
            closeables.add((Closeable) factory);
        }
    }
    IOUtils.close(closeables);
}
```

But apparently this method is never called in `Node#stop()`.

Closes #17625.
</description><key id="146973578">17626</key><summary>Ingest does not close its factories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T16:20:44Z</created><updated>2016-04-11T08:06:16Z</updated><resolved>2016-04-11T08:01:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-04-08T20:01:06Z" id="207579868">I left a comment. Looks good, but we need a test for this. What I think what would work is if we write a test that extends from `ESIntegTestCase` that registers a test processor that keeps track if close is invoked. In that test we start just one node and then stop it and then check is the test processor's close has been invoked.

(`IngestClientIT` is an example to look at for how to register a test processor)
</comment><comment author="dadoonet" created="2016-04-08T20:15:10Z" id="207585511">&gt; What I think what would work is if we write a test that extends from ESIntegTestCase that registers a test processor that keeps track if close is invoked. In that test we start just one node and then stop it and then check is the test processor's close has been invoked.

That's exactly what I initially started to do but I did not find how to manually start and stop a node. 
I'll dig into this more. Thanks!
</comment><comment author="rjernst" created="2016-04-08T20:28:54Z" id="207590774">&gt; I did not find how to manually start and stop a node

Use an `ESSingleNodeTestCase`?
</comment><comment author="martijnvg" created="2016-04-08T20:41:37Z" id="207595750">&gt; Use an ESSingleNodeTestCase?

That is even better. I forgot that via this base test has `getPlugins()` which tests can override.
</comment><comment author="dadoonet" created="2016-04-09T11:15:06Z" id="207771440">Thanks guys. I added a new commit.
</comment><comment author="martijnvg" created="2016-04-11T07:40:12Z" id="208204417">@dadoonet Left one minor comment. LGTM.
</comment><comment author="dadoonet" created="2016-04-11T08:06:16Z" id="208215627">Thanks @martijnvg!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest does not close its factories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17625</link><project id="" key="" /><description>When you implement an ingest factory which implements `Closeable`:

``` java
public static final class Factory extends AbstractProcessorFactory&lt;MyProcessor&gt; implements Closeable {
    @Override
    public void close() throws IOException {
        logger.debug("closing my processor factory");
    }
}
```

The `close()` method is never called which could lead to some leak threads when we close a node.

The `ProcessorsRegistry#close()` method exists though and seems to do the right job:

``` java
@Override
public void close() throws IOException {
    List&lt;Closeable&gt; closeables = new ArrayList&lt;&gt;();
    for (Processor.Factory factory : processorFactories.values()) {
        if (factory instanceof Closeable) {
            closeables.add((Closeable) factory);
        }
    }
    IOUtils.close(closeables);
}
```

But apparently this method is never called in `Node#stop()`.
</description><key id="146961126">17625</key><summary>Ingest does not close its factories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Ingest</label><label>bug</label></labels><created>2016-04-08T15:32:08Z</created><updated>2016-04-11T08:01:27Z</updated><resolved>2016-04-11T08:01:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Handle empty query bodies at parse time and remove EmptyQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17624</link><project id="" key="" /><description>As stated in #17540 we currently support empty query bodies like the filter in

```
"constant_score" : {  "filter" : { } }
```

in the query DSL. How these empty query bodies are handled depends on the query they are contained in. Upstream they are usually either ignored, converted to match all or no documents or bubbled up further in the query tree. While parsing they are currently represented as EmptyQueryBuilders, which serve no other purpose as to signal callers that it is the result of parsing an empty query body. 
When not handled anywhere else, it needs to be checked for on the shard when building the lucene query. This is trappy, so this PR changes the parsing of compound queries. Instead of returning QueryBuilder, the core query parsing method QueryShardContext#parseInnerQueryBuilder() now return an Optional&lt;QueryBuilder&gt; which is empty in the case of queries with empty body. This has the advantage of forcing caller code to deal with this sooner or later. when encountering empty Optionals, compound query builders now have the choice to ignore them, pass them on or rewrite to a different query, depending on context. 

Closes #17541
</description><key id="146955437">17624</key><summary>Handle empty query bodies at parse time and remove EmptyQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha4</label></labels><created>2016-04-08T15:11:19Z</created><updated>2016-06-28T09:33:24Z</updated><resolved>2016-06-02T16:40:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-05-16T08:57:49Z" id="219379839">This looks good to me but I wonder if we should get another person to look at it (maybe @martijnvg @jpountz or @javanna ?) to get their opinion on the approach. The reason for this is that this is quite a big change to the query parsing (touches every query) to deal with a case that we are going to remove in 6.0 anyway. So although this is an improvement for 5.x to deal with the `{}` we are probably going to reverse this change in 6.0 and not return Optional from the query parse method since in 6.0 the parse method will always need to return a QueryBuilder and will not be allow to return null or anything that represents `{}`.
</comment><comment author="jpountz" created="2016-05-16T13:52:44Z" id="219430077">Agreed: I would rather like a soon-to-be-deprecated feature to be a bit buggy than to have deep implications on our internal  APIs.
</comment><comment author="cbuescher" created="2016-05-17T16:26:07Z" id="219773622">@colings86 @jpountz thanks for taking a look. I would still like to argue for getting this in: we only introduced the EmptyQueryBuilder as a means to handle the empty clauses on master to avoid having to deal with null values during the query refactoring. This is not really a bug, but it is really just a crutch that we would have avoided if we had had Optional&lt;T&gt; when we introduced it (@javanna might agree here). Now that we can represent optional values differently, I think we should do that rather than introduce a query builder that can get missused. Even if we plan to remove it in 6.0, it will stick around for a shile, but I think Optional&lt;QueryBuilder&gt; is a much better choice with Java 8. I also think the changes to query builder method signatures are rather small in most cases (just wrapping the builder in an Optional), but we don't have to transport those objects across the wire anymore if we deal with empty clauses on the coordinating node already.
</comment><comment author="javanna" created="2016-05-24T09:49:33Z" id="221220221">I had a look at this and I like it. I agree that we would have used `Optional` from the beginning if we could, rather than introducing `EmptyQueryBuilder` to the transport layer, which can also be used from the java api. With this solution we handle the empty query case as soon as possible while parsing, we make it more explicit and that makes sure we don't have null lucene queries generated down the road. This is a good improvement and even if we remove this "feature" in 6.x this code will live for the whole 5.x series and make it better.

&gt; Agreed: I would rather like a soon-to-be-deprecated feature to be a bit buggy than to have deep implications on our internal APIs.

@jpountz by deep implication you mean the fact that we use `Optional` in `fromXContent`, which will go away in 6.x ? I find it better than having `EmptyQueryBuilder` that gets serialized over the transport, which generates null lucene queries down the road and that is not explicit at all, so one has to remember to handle the null case. Optional solves it sooner and makes it more explicit. I think it's worth to get this in, but I may be missing some of the downsides. What do you think?
</comment><comment author="cbuescher" created="2016-05-25T09:48:40Z" id="221525625">@javanna thanks for the review, I rebased and added a commit that removes the handling of null queries in the `toQuery` part where applicable. I'd still like to get this into 5.0 so we don't have to introduce EmptyQueryBuilder, even if we remove empty clauses altogether later. That is, if @colings86 and @jpountz don't have strong objections against it.
</comment><comment author="jpountz" created="2016-05-25T14:57:21Z" id="221603132">I am fine with this pull request either if we merge it to 5.x after master becomes 6.x, or if we make sure to move the API back to its current state once the branch is cut.
</comment><comment author="cbuescher" created="2016-05-31T09:21:47Z" id="222636190">@javanna I left one comment about possivle null return values in `QueryShardContext#toFilter()` and adressed your previous comments in the last commit. Mind to take another look?
</comment><comment author="cbuescher" created="2016-05-31T13:06:18Z" id="222682167">@javanna I changed the thing in IndexService#parse(), also opened https://github.com/elastic/elasticsearch/pull/18656 to remove the potential `null`return value in CommonTermsQueryBuilder.
</comment><comment author="javanna" created="2016-06-01T08:13:21Z" id="222923281">I had another look and left a few questions but this looks very good!
</comment><comment author="cbuescher" created="2016-06-02T10:33:10Z" id="223255093">@javanna thanke, I hope I answered and adressed all of your review comments above and in the last commit.
</comment><comment author="javanna" created="2016-06-02T13:34:55Z" id="223292937">I left a few more comments on your newly added commits, but still looks good ;)
</comment><comment author="cbuescher" created="2016-06-02T14:21:41Z" id="223306792">@javanna I adressed the last two comments and pushed an update of the last commit.
</comment><comment author="javanna" created="2016-06-02T14:47:08Z" id="223314831">LGTM
</comment><comment author="cbuescher" created="2016-06-02T16:39:35Z" id="223349186">@javanna thanks, will merge this then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Insert into elastic search from a partitioned table throws error- elasticsearch-hadoop-2.2.0-rc1.jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17623</link><project id="" key="" /><description>Hello,

I have noticed that Selecting  data from a partitioned hive table and inserting into elastic search does not work very will and the map reduce job ends in the following error.

```

URL:
  http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1458893148211_0019&amp;tipid=task_1458893148211_0019_m_000000
-----
Diagnostic Messages for this Task:
Error: java.io.IOException: java.lang.reflect.InvocationTargetException
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:265)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:139)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)
        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:251)
        ... 11 more
Caused by: java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.rangeCheck(ArrayList.java:635)
        at java.util.ArrayList.get(ArrayList.java:411)
        at org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.getProjectedGroupFields(DataWritableReadSupport.java:110)
        at org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.getSchemaByName(DataWritableReadSupport.java:155)
        at org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.init(DataWritableReadSupport.java:221)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:256)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&lt;init&gt;(ParquetRecordReaderWrapper.java:95)
        at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.&lt;init&gt;(ParquetRecordReaderWrapper.java:81)
        at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)
        at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.&lt;init&gt;(CombineHiveRecordReader.java:66)
        ... 16 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-0: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec
WARN: The method class org.apache.commons.logging.impl.SLF4JLogFactory#release() was invoked.
WARN: Please see http://www.slf4j.org/codes.html#release for an explanation.
```

 I have tested similar scenarios by using the different source tables ( Stored as parquet, stored as parquet and snappy compressed) and it works fine. But when i use partitioned hive table as my source table, the job fails with the above error. 

I have used **Cloudera 5.5 VM for hadoop, elasticsearch-2.2.1 and elasticsearch-hadoop-2.2.0-rc1.jar** for my tests.

I attach a zip file with two HQL scripts and the ES-Hadoop jar for reproducing this issue.

[Hive-ES.zip](https://github.com/elastic/elasticsearch/files/210365/Hive-ES.zip)

Thanks and Regards
Sa'M
</description><key id="146954113">17623</key><summary>Insert into elastic search from a partitioned table throws error- elasticsearch-hadoop-2.2.0-rc1.jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">smang1</reporter><labels /><created>2016-04-08T15:05:31Z</created><updated>2016-04-08T15:58:39Z</updated><resolved>2016-04-08T15:58:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-08T15:58:36Z" id="207492721">There is a dedicated repository for [elasitcsearch-hadoop](https://github.com/elastic/elasticsearch-hadoop).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove norelease comments in test classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17622</link><project id="" key="" /><description>We added these comments to enhance our randomized test infrastructure, but since they are in tests they don't seem to serve their purpose of blocking a release. I propose we should turn these into issues rather than leave them in the code base.
</description><key id="146948218">17622</key><summary>Remove norelease comments in test classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>discuss</label><label>non-issue</label><label>test</label></labels><created>2016-04-08T14:45:28Z</created><updated>2016-04-12T17:50:33Z</updated><resolved>2016-04-12T17:50:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-08T14:46:30Z" id="207460307">@colings86 @javanna opened this mainly for a brief discussion.
</comment><comment author="javanna" created="2016-04-08T14:52:46Z" id="207465502">++ thanks @cbuescher 
</comment><comment author="colings86" created="2016-04-08T15:16:05Z" id="207473922">I have no problem with opening issues for these but I am surprised that NORELEASE comments in tests don't affect the release. They probably should since the lack of something in tests may be a legitimate reason for not being able to release (I'm not saying in this case it should stop a release but I can imagine we might want that to happen at some point). Maybe we should open an issue for that too?
</comment><comment author="cbuescher" created="2016-04-08T15:24:51Z" id="207477481">This is probably the main issue: https://github.com/elastic/elasticsearch/issues/14414

There's a mini-solution to at least be able to test from a small set of random queries, but I haven't looked into it for some time now. Maybe we should close this and start a fresh attempt: https://github.com/elastic/elasticsearch/pull/16055

Random Aggregations and random Ext probably need separate issues, or does something like this exist already?
</comment><comment author="cbuescher" created="2016-04-12T16:44:06Z" id="209001367">I created two new issues regarding the randomized tests that would be nice to have for SearchSourceBuilder. @javanna @colings86 any objections that I merge this then and remove the comments in the test code now?
</comment><comment author="javanna" created="2016-04-12T17:14:14Z" id="209012292">I am good ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GatewayIndexStateIT#testRecoverMissingAnalyzer CI test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17621</link><project id="" key="" /><description>This is a test failure that doesn't reproduce for me even after running the reproduction step in a while loop for 20 minutes.

```
gradle :core:integTest -Dtests.seed=F2E5DD72931B795F -Dtests.class=org.elasticsearch.gateway.GatewayIndexStateIT -Dtests.method="testRecoverMissingAnalyzer" -Des.logger.level=DEBUG -Dtests.security.manager=true -Dtests.nightly=false -Dtests.heap.size=732m -Dtests.jvm.argline="-server -XX:+UseSerialGC -XX:-UseCompressedOops" -Dtests.locale=fr-FR -Dtests.timezone=America/Chicago
```

**Elasticsearch version**:
0d8e39978121def4a4fa77d1dd6787b7a46843a4

**Provide logs (if relevant)**:
http://build-us-00.elastic.co/job/es_core_master_window-2008/3492/consoleText

**OS**:
Windows 2008
</description><key id="146929508">17621</key><summary>GatewayIndexStateIT#testRecoverMissingAnalyzer CI test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Analysis</label><label>adoptme</label><label>jenkins</label><label>test</label></labels><created>2016-04-08T13:28:28Z</created><updated>2017-06-16T16:59:44Z</updated><resolved>2017-06-16T16:59:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-06-16T16:59:44Z" id="309079483">given that this issue had no activity in more than a year, I would close it. We can always reopen if we encounter the same failure again.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename reindex's timeout to shard_timeout or something less missleading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17620</link><project id="" key="" /><description>**Describe the feature**:
Reindex and update_by_query support a `timeout` parameter. It is just piped through to the bulk requests that they make. So it isn't a timeout on the reindex at all - it is a timeout on shards to become available for each individual bulk request. So the name is confusing.
</description><key id="146928081">17620</key><summary>Rename reindex's timeout to shard_timeout or something less missleading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label></labels><created>2016-04-08T13:21:16Z</created><updated>2017-03-14T00:48:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove Settings.settingsBuilder.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17619</link><project id="" key="" /><description>We have both `Settings.settingsBuilder` and `Settings.builder` that do exactly
the same thing, so we should keep only one. I kept `Settings.builder` since it
has my preference but also it is the one that we use in examples of the Java API.
</description><key id="146924017">17619</key><summary>Remove Settings.settingsBuilder.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Settings</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T13:03:48Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-04-08T16:10:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-08T13:04:47Z" id="207423831">&gt; Settings.builder

+
</comment><comment author="jpountz" created="2016-04-08T13:04:50Z" id="207423843">Note to reviewers: the change is purely mechanical.
</comment><comment author="nik9000" created="2016-04-08T13:25:05Z" id="207431510">In the name of moving forward, LGTM. _but_ we really really shouldn't be statically importing `Settings.builder` because it makes the call sites confusing. I'm fine with merging this as is and creating another PR myself to remove those imports if you want.
</comment><comment author="dadoonet" created="2016-04-08T14:23:01Z" id="207451038">++ Thanks for doing this Adrien! This always confuses me when my IDE propose the 2 methods... :) 
</comment><comment author="jpountz" created="2016-04-08T14:57:13Z" id="207467020">@nik9000 I agree these static imports are not nice. I pushed a new commit.
</comment><comment author="nik9000" created="2016-04-08T15:08:32Z" id="207471166">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Loosing file.content on _update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17618</link><project id="" key="" /><description>Hi,

I have a problem linked on the mapping I use : 

```
PUT documents/myDoc/_mapping
{
    "myDoc": {
        "_source": {
            "excludes": [
                "file"
            ]
        },
        "properties": {
           "title": {
                  "type": "string"
           },
            "file": {
                "type": "attachment",
                "fields": {
                    "content": {
                        "type": "string",
                        "term_vector": "with_positions_offsets",
                        "store": true
                    }
            }
       }
}
```

I'm doing this in order to not storing the file content and to be able to retrieve highlighted text from the extracted text. 
I set the `file._content` with the document content and then I can query on the extracted text with `file.content`. It works fine.

However, I have an issue when I try to update an entry. If I do, for instance

```
POST documents/myDoc/123456789/_update
{
  "doc": { "title": "my new title"}
}
```

After this update, I will have an empty value for `file.content`. From what I have understood, this is the standard behavior as the `file` is excluded from the `_source` and because file.content is also not in the `_source`. On partial updates, Elasticsearch looses the properties which are not on the `_source`.

I know I can re-upload the document on every update but it is not convenient at all. My question is : Is there a way to force the `file.content` (the extracted text) to be part of the `_source`, so I will not loose the extracted text on a partial update ?

Thanks in advance for your answer.

Elasticsearch version : 2.2
JVM version : 1.7.0_71
OS version : Windows 2012 RC2
</description><key id="146912655">17618</key><summary>Loosing file.content on _update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ptrussart</reporter><labels><label>:Plugin Mapper Attachment</label></labels><created>2016-04-08T12:14:49Z</created><updated>2016-04-12T18:27:32Z</updated><resolved>2016-04-12T18:27:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-11T07:56:10Z" id="208212493">Comment I put on https://github.com/elastic/elasticsearch-mapper-attachments/issues/209#issuecomment-207363491

&gt; And for now I don't believe the update API could work with attachments.

For the very least, we don't have any test to cover this use case.
</comment><comment author="clintongormley" created="2016-04-12T18:27:32Z" id="209042661">What is for sure is that the update API can't work as expected when fields are excluded from the `_source`.  The  `_source` is the only source of data - what you are doing is the equivalent of creating a new document without passing the `file` field.

This is documented here: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html#include-exclude
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex confict: "proceed" throws error when casting malformed data to `geo_point`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17617</link><project id="" key="" /><description>When reindexing malformed object to  `geo_point` the `conflict` parameter is ignorred and stops the task

example doc

``` javascript
{"geo":{"lat":null,"lon":null}}
```

with mapping:

``` javascript
{"properties":{"geo":{"properties":{"lat":{"type":long}, "lon":{"type":long}}}}
```

reindexing to 

``` javascript
{"properties":{"geo":{"type":"geo_point", "ignore_malformed": true}}}
```

exception:

``` javascript
{
   "took": 24483,
   "timed_out": false,
   "total": 5211,
   "updated": 699,
   "created": 2100,
   "batches": 28,
   "version_conflicts": 0,
   "noops": 0,
   "retries": 0,
   "failures": [
      {
         "index": "search.locations",
         "type": "location",
         "id": "86d20ff9-ea24-448c-95a4-f060a30a80ad",
         "cause": {
            "type": "mapper_parsing_exception",
            "reason": "failed to parse",
            "caused_by": {
               "type": "parse_exception",
               "reason": "latitude must be a number"
            }
         },
         "status": 400
      }
   ]
}
```

**Elasticsearch version**:
docker:latest (2.3)

example query

``` javascript
POST /_reindex
{
   "conflicts": "proceed",
   "source": {
      "index": "raw.locations"
   },
   "dest": {
      "index": "search.locations"
   }
}
```
</description><key id="146893304">17617</key><summary>Reindex confict: "proceed" throws error when casting malformed data to `geo_point`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">g00fy-</reporter><labels><label>:Reindex API</label><label>adoptme</label><label>docs</label></labels><created>2016-04-08T10:58:04Z</created><updated>2016-04-12T14:24:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-08T11:34:24Z" id="207393761">That doesn't look like a bug to me. Maybe a documentation bug.
`conflicts=proceed` only works on version conflicts.

If you want to skip the ones with invalid Geo points maybe you can craft
the query to do so?  You could probably also use a script to try to make
them valid. But reindex doesn't have support for skipping arbitrary errors.
On Apr 8, 2016 6:58 AM, "Piotrek Majewski" notifications@github.com wrote:

&gt; When reindexing malformed object to geo_point the conflict parameter is
&gt; ignorred and stops the task
&gt; 
&gt; example doc
&gt; 
&gt; {"geo":{"lat":null,"lon":null}}
&gt; 
&gt; with mapping:
&gt; 
&gt; {"properties":{"geo":{"properties":{"lat":{"type":long}, "lon":{"type":long}}}}
&gt; 
&gt; reindexing to
&gt; 
&gt; {"properties":{"geo":{"type":"geo_point", "ignore_malformed": true}}}
&gt; 
&gt; exception:
&gt; 
&gt; {
&gt;    "took": 24483,
&gt;    "timed_out": false,
&gt;    "total": 5211,
&gt;    "updated": 699,
&gt;    "created": 2100,
&gt;    "batches": 28,
&gt;    "version_conflicts": 0,
&gt;    "noops": 0,
&gt;    "retries": 0,
&gt;    "failures": [
&gt;       {
&gt;          "index": "search.locations",
&gt;          "type": "location",
&gt;          "id": "86d20ff9-ea24-448c-95a4-f060a30a80ad",
&gt;          "cause": {
&gt;             "type": "mapper_parsing_exception",
&gt;             "reason": "failed to parse",
&gt;             "caused_by": {
&gt;                "type": "parse_exception",
&gt;                "reason": "latitude must be a number"
&gt;             }
&gt;          },
&gt;          "status": 400
&gt;       }
&gt;    ]
&gt; }
&gt; 
&gt; _Elasticsearch version_:
&gt; docker:latest (2.3)
&gt; 
&gt; example query
&gt; 
&gt; POST /_reindex
&gt; {
&gt;    "conflicts": "proceed",
&gt;    "source": {
&gt;       "index": "raw.locations"
&gt;    },
&gt;    "dest": {
&gt;       "index": "search.locations"
&gt;    }
&gt; }
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/17617
</comment><comment author="eskibars" created="2016-04-08T14:59:07Z" id="207467578">We probably should clarify a bit further in the documentation what "conflicts" means.  Its a bit implicit by the fact that the return result says "version_conflicts", but I could see how somebody could miss that and assume this will skip more problem types
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use `mmapfs` by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17616</link><project id="" key="" /><description>I case any problem was discovered, you can still enable the legacy `default`
directory instead. But the plan is to get rid of it in 6.0.

Closes #16983
</description><key id="146885371">17616</key><summary>Use `mmapfs` by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Store</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T10:21:12Z</created><updated>2016-05-04T06:54:22Z</updated><resolved>2016-04-08T18:25:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-08T11:21:31Z" id="207386017">Left one comment, otherwise LGTM.
</comment><comment author="jpountz" created="2016-04-08T12:50:25Z" id="207420484">@jasontedor I pushed a new commit.
</comment><comment author="jasontedor" created="2016-04-08T12:58:24Z" id="207422432">LGTM.
</comment><comment author="dakrone" created="2016-04-08T15:11:29Z" id="207472021">@jpountz want to add a note to the migration guide about this also?
</comment><comment author="jpountz" created="2016-04-08T16:28:53Z" id="207503076">@dakrone I added a few lines about it in the last commit. Is it good?
</comment><comment author="dakrone" created="2016-04-08T17:12:41Z" id="207521076">sounds good, thanks Adrien
</comment><comment author="uschindler" created="2016-04-08T18:44:41Z" id="207552237">The reference to my blog post gets more and more a must-read standard document :)

Thanks for taking care! Long overdue to fix this.
</comment><comment author="mikemccand" created="2016-04-08T19:34:50Z" id="207571488">Thank you @jpountz!
</comment><comment author="rpedela" created="2016-05-01T23:08:00Z" id="216080310">After reading @uschindler excellent blog post, I am curious if the Java heap size recommendations change for ES now that mmapfs is the default? He mentions that heap size should be set to 1/4 memory for Lucene/Solr. Is it the same for ES now?
</comment><comment author="uschindler" created="2016-05-02T06:49:51Z" id="216117921">Hi @rpedela: As always, the answer to this type of question is "it depends"! :-)

The change here does not change the heap space requirements of Elasticsearch, because it does not matter if the directory implementation is NIOFS or MMAP or a combination of both. The change from NIO to MMAP only helps with speed for random access to docvalues, because Lucene does not need to do costly buffered I/O during sorting or aggregations when accessing the docvalues fields in a random access manner. And the keyword docvalues is the most important thing here:

The 1/4 of availabe memory was referring to the "typical Lucene use case" (the inverted index). As previous versions of Elasticsearch were using a lot of additional caches (e.g. uninverted fielddata, caches) the heap usage was higher. But since Elasticsearch uses docvalues, this changed. If you mapping is using doc values and no longer the old inverted fielddata, further reducing heap space helps, but this is unrelated to this issue.

In addition, it should always be tested: Check heap usage of ES during load and reduce until you see OutOfMemoryErrors in the logs. The real heap usage depends on features used, size of cluster, size of indexes,...
</comment><comment author="clintongormley" created="2016-05-02T14:33:34Z" id="216251545">@jpountz the docs here https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules-store.html should also be updated
</comment><comment author="jpountz" created="2016-05-04T06:54:22Z" id="216760703">@clintongormley done in 51a53c55cb2b30a3db85cd3342b3e3a4e5520903
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.security.AccessControlException in ESIntegTestCase when using Mockito</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17615</link><project id="" key="" /><description>May be related to #16459.

**Elasticsearch version**: 2.3.1

**JVM version**:

```
java version "1.8.0_60"
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
```

**OS version**:
OS X 10.11.4

**Description of the problem including expected versus actual behavior**:
When initializing a Mockito mock inside a test class inhering from `ESIntegTestCase`, I get a AccessControlException. Running the test with `-Dtests.security.manager=false` prevents the exception.

**Steps to reproduce**:

```
import static org.mockito.Mockito.mock;

import java.util.List;

import org.elasticsearch.test.ESIntegTestCase;
import org.junit.Test;

public class SecurityManagerTest extends ESIntegTestCase {

    @Test
    public void test() throws Exception {
        final List mock = mock(List.class);
    }
}

```

**Provide logs (if relevant)**:

```
java.lang.ExceptionInInitializerError
    at __randomizedtesting.SeedInfo.seed([7DFCAB452E627517:F5A8949F809E18EF]:0)
    at org.mockito.cglib.core.KeyFactory$Generator.generateClass(KeyFactory.java:167)
    at org.mockito.cglib.core.DefaultGeneratorStrategy.generate(DefaultGeneratorStrategy.java:25)
    at org.mockito.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:217)
    at org.mockito.cglib.core.KeyFactory$Generator.create(KeyFactory.java:145)
    at org.mockito.cglib.core.KeyFactory.create(KeyFactory.java:117)
    at org.mockito.cglib.core.KeyFactory.create(KeyFactory.java:109)
    at org.mockito.cglib.core.KeyFactory.create(KeyFactory.java:105)
    at org.mockito.cglib.proxy.Enhancer.&lt;clinit&gt;(Enhancer.java:70)
    at org.powermock.api.mockito.repackaged.ClassImposterizer.createProxyClass(ClassImposterizer.java:95)
    at org.powermock.api.mockito.repackaged.ClassImposterizer.imposterise(ClassImposterizer.java:57)
    at org.powermock.api.mockito.repackaged.ClassImposterizer.imposterise(ClassImposterizer.java:49)
    at org.powermock.api.mockito.repackaged.CglibMockMaker.createMock(CglibMockMaker.java:24)
    at org.powermock.api.mockito.internal.mockmaker.PowerMockMaker.createMock(PowerMockMaker.java:46)
    at org.mockito.internal.util.MockUtil.createMock(MockUtil.java:33)
    at org.mockito.internal.MockitoCore.mock(MockitoCore.java:59)
    at org.mockito.Mockito.mock(Mockito.java:1285)
    at org.mockito.Mockito.mock(Mockito.java:1163)
    at com.compuware.apm.elasticsearch.util.SecurityManagerTest.test(SecurityManagerTest.java:14)

...

Caused by: java.security.AccessControlException: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.reflect.AccessibleObject.setAccessible(AccessibleObject.java:128)
    at org.mockito.cglib.core.ReflectUtils$2.run(ReflectUtils.java:57)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.mockito.cglib.core.ReflectUtils.&lt;clinit&gt;(ReflectUtils.java:47)

```
</description><key id="146882074">17615</key><summary>java.security.AccessControlException in ESIntegTestCase when using Mockito</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tkaiser</reporter><labels /><created>2016-04-08T10:03:51Z</created><updated>2016-04-08T10:08:34Z</updated><resolved>2016-04-08T10:08:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-08T10:08:26Z" id="207358929">We provide [securemock](https://github.com/elastic/securemock) for mocking under the security manager.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for sorting terms aggregation by ascending count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17614</link><project id="" key="" /><description>We try to be as flexible as possible when it comes to sorting terms aggregations. However, sorting by anything but by _term or descending _count makes it very hard to return the correct top buckets and counts, which is disappointing to users. For this reason we should remove the ability to sort the terms aggregation by ascending count (split out from https://github.com/elastic/elasticsearch/issues/17588)
</description><key id="146878174">17614</key><summary>Remove support for sorting terms aggregation by ascending count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>breaking</label><label>discuss</label></labels><created>2016-04-08T09:44:22Z</created><updated>2017-07-21T18:29:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rashidkpc" created="2016-04-08T15:36:10Z" id="207482889">If and when you decide if you're going to do this, please open an issue on kibana. We currently support this in the UI so we'll need to issue a deprecation notice well in advance
</comment><comment author="rashidkpc" created="2016-04-08T19:40:35Z" id="207573400">Oh, also, I'm +1 on this, its confusing and rarely useful anyway. If we're going to deprecate it pre-5.0.0, which I'd prefer, let me know.
</comment><comment author="jimczi" created="2016-06-17T15:35:05Z" id="226802533">Since this requires some changes in Kibana I've reverted the removal. Though it is now deprecated in 2.x/2.4.
</comment><comment author="jccq" created="2016-08-23T12:37:25Z" id="241716709">Frankly i know people who were using this actually i'd say rely on this e.g. to spot anomalies,  if the only problem is that it wasnt working very well, could you just not have clarified in the docs? i mean so many things in Elasticsearch dont really return the right number
</comment><comment author="jccq" created="2016-08-23T12:42:56Z" id="241717946">unless there is an alternative for that use cases of course, is there? e.g. what are the least frequent MD5s executed across the logs
</comment><comment author="djschny" created="2016-08-30T14:19:01Z" id="243454352">For folks that have small datasets this is still useful and does not hurt anything to my knowledge. I do not understand why this is being removed as well. I plead to have this kept and enhance the docs and/or add a check based upon the size or cardinality of the field perhaps.
</comment><comment author="nich07as" created="2016-09-01T02:01:39Z" id="243955399">I've worked with a few customers who uses the ascending count to determine what are the least popular items and also the occasional outliers. Agree that it might be inaccurate over large datasets but would be helpful on smaller samples and doesn't hurt to keep it around as @djschny suggests.
</comment><comment author="jccq" created="2016-09-01T06:01:50Z" id="243984104">The point is.. what is the replacement really? sure its inaccurate but is
there any other way to achieve something like this? i dont think so. Thus
the big damage in removing vs... simply explaining the limitation.

On Thu, Sep 1, 2016 at 3:02 AM, nich07as notifications@github.com wrote:

&gt; I've worked with a few customers who uses the ascending count to determine
&gt; what are the least popular items and also the occasional outliers. Agree
&gt; that it might be inaccurate over large datasets but would be helpful on
&gt; smaller samples and doesn't hurt to keep it around as @djschny
&gt; https://github.com/djschny suggests.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/17614#issuecomment-243955399,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AEuGyLkuCM3oiAkuqValXtEn93axFcJnks5qljJGgaJpZM4IC2Tl
&gt; .

## 

Giovanni Tummarello
CEO - SIREn Solutions
</comment><comment author="jayswan" created="2016-09-01T14:20:50Z" id="244093885">We use this extensively to find outliers in log data. Usually in large data sets you can filter out the most common items before performing the aggregation, so loss of accuracy isn't a big problem.

Removing this feature is a huge problem for logging use cases -- nearly crippling, IMO.
</comment><comment author="jayswan" created="2016-09-01T15:12:28Z" id="244110791">Furthermore: in exploratory log analysis the exact content of the tails often doesn't matter as much as the general _kinds_ of things in the tails.

As an example, I just used this feature to reduce the set "interesting" documents in an index of about 6 million logs from 1.5 million to about 100 over the course of 5 minutes by iteratively excluding categories of things I found in the tails.
</comment><comment author="henrikjohansen" created="2016-09-01T15:13:24Z" id="244111106">@colings86 @jimferenczi ... this is a rather poor decision for a number of use-cases where finding rare occasions is important. It severely impacts security analytics for example.
</comment><comment author="colings86" created="2016-09-01T15:22:35Z" id="244114209">&gt; very hard to return the correct top buckets and counts

I want to explain this a bit more as I don't think its really clear on the description above (apologies for that)

The problem here isn't that the counts can be wrong, the problem is that there is currently no bound on how wrong the counts can be (and no way to know what the error might be). To explain this consider the following example.

Imagine we are looking for the top 5 terms in a field across 3 shards ordered by ascending count. The terms aggregation goes and retrieves the top 5 terms from each shard and then merges them together (in practice it actually retrieves more than the top 5 from each shard but for the purposes of this example lets assume `size` and `shard_size` are the same). The shards might return the following:

| Shard 1 | Shard 2 | Shard 3 |
| --- | --- | --- |
| a (1) | a (1) | a (1) |
| b (1) | c (1) | b (1) |
| d (1) | d (1) | f (1) |
| g (2) | f (2) | i (1) |
| h (9) | g (2) | j (2) |

When merged on the reduce node this will produce the final list of:
- c (1)
- i (1)
- d (2)
- b (2)
- j (2)
- a (3)
- f (3)
- g (4)
- h (9)

So the final top 5 will be:
- c (1)
- i (1)
- d (2)
- b (2)
- j (2)

Which seems great until you look into the results from the shards a bit closer.

The counts returned from each shard are 100% accurate so if a shard says it has 1 document with the term `a` it only has one document with the term `a`. But its the information thats not returned from the shard that leads to issues. From the shard results above we can see that `a` is returned from every shard so we know that the document count for `a` is completely accurate. But if we now look at `d` we can see that it was only returned form shards 1 and 2. We don't know whether Shard 3 doesn't have any documents containing `d` or whether `d` just didn't make it into the top 5 terms on Shard 3 (i.e. whether there are 0 or &gt; 2 documents containing `d` on the shard). It could be that shard 3 happens to have 100 documents containing `d` but it just wasn't returned in the top N list.

The [`terms` aggregation documentation](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-approximate-counts) tries to explain how with descending count ordering we can calculate the worst case error in the doc_count by using the doc_count of the last term returned on each shard. But in the case of ascending order the doc_count a terms could have on a shard that didn't return it could be anything, all we know is that it's either 0 or greater than or equal to the doc_count of the last term returned by each shard.
</comment><comment author="djschny" created="2016-09-01T15:27:58Z" id="244115995">This is still relevant and accurate when you are searching an index with only 1 shard correct?
</comment><comment author="clintongormley" created="2016-09-01T15:31:19Z" id="244117138">@djschny we're not building a product that only works on one shard and breaks at scale.  This is a really important part of how we make decisions: whatever we build must scale.
</comment><comment author="nik9000" created="2016-09-01T15:37:44Z" id="244119248">&gt; We don't know whether Shard 3 doesn't have any documents containing d or whether d just didn't make it into the top 5 terms on Shard 3

Maybe rename `ascending` to `ascending_candidates` to make it super clear that these _might_ be rare events? I think given this explanation this could still be useful but pretty trap-ish as named.

I think it is interesting that you could use `_routing` to make this properly accurate again. You'd have to use it when indexing the data and the data has to be amenable to it, but it is a thing.
</comment><comment author="colings86" created="2016-09-01T15:51:41Z" id="244123780">&gt; Maybe rename ascending to ascending_candidates to make it super clear that these might be rare events? I think given this explanation this could still be useful but pretty trap-ish as named.

Personally I think it's trap-ish regardless of what we name it. If the index has 20 shards and 19 of them have 1000 docs containing a particular term but 1 has only 1 document then the term is _likely_ to be returned as rare even though it is in fact not even slightly rare. That seems very trappy to me whatever we call the option. Consider also in that case you could get into the situation where the term appears in top N list for both doc_count descending and doc_count ascending because with doc_count descending with a doc count of 19000 it could be one of the most popular terms.

On time based indices this situation could be more likely if a term first appears at the end of a indexes time period but is actually very popular. In that case when searching over a period that rides over this period you could be returned the term as rare when in fact it's not rare at all in the context you are looking at
</comment><comment author="djschny" created="2016-09-01T19:33:59Z" id="244187351">&gt; @djschny we're not building a product that only works on one shard and breaks at scale. This is a really important part of how we make decisions: whatever we build must scale.

I can completely understand that for investing time into new features. But this feature is pre-existing and it does have usefulness. If we follow this at such a strict line then we should deprecate and remove terms aggregation in general and also cardinality because they have similar behavior to what is discussed here in regards to accuracy and scalability (as defined here). However I assume (or hope) that those are not in consideration for removal as well.

@colings86 thank you for the detailed explanation, however the behavior described is not what is in question. The topic at hand here is that yes, most people are perfectly fine with all the pitfalls of this. This feature is not perfect and that is fine. It still helps solve problems. And we don't have an alternative to it either.

This is not an issue where it has the potential to affect the health of the cluster (unless I missed that point) like other scalability items (fielddata, large mappings, etc.) do. So to consider this a scalability issue I just don't see.

The most practical thing here seems like a combination of what people have mentioned in this thread. For example:
- renaming the sort to be `ascending_candidates` name as @nik9000 mentions
- increasing warnings in the documentation as @jccq mentions
- and perhaps keeping Kibana warnings there that talk about inaccuracy

I'm all for removing things that are truly dangerous for the health of the system/cluster, but want to make sure we do not cut too far. IMO this is a cut too far.

I hope accurately was able to articulate the reasons behind my earlier terse remarks.
</comment><comment author="jpountz" created="2016-09-02T08:54:17Z" id="244320361">@djschny Even though these aggs are potentially inaccurate, there is a fine line between returning a good estimation and a number that is not unlikely to be completely off. The motivation here was to remove a trap. Now, there seems to be significant push back so I'd be fine to reconsider and solve it by documentation, but please please please do not put in into the same bucket as sorting by descending count or the cardinality aggregation, these are very different issues.
</comment><comment author="mlawler004" created="2016-10-17T12:38:38Z" id="254195586">Another vote for keeping this functionality available in some shape or form. There are lots of different uses cases for ES. We use ES on enterprise scale (million(s) of records, not billions), and we use this for outlier detection in some circumstances, We're ok with it not being accurate some times.
</comment><comment author="anhlqn" created="2016-12-08T01:06:28Z" id="265623256">+1 for keeping this feature around. We use it to look for the few emails that come from random countries that should not be in the picture. There are also many other use cases in log monitoring and analysis.</comment><comment author="epixa" created="2017-05-20T15:34:48Z" id="302880330">@clintongormley @colings86 @jimczi Are there any plans to continue with this for 6.0?  It looks stalled at the moment, but I want to make sure we remove the feature from Kibana if it is being removed from Elasticsearch as well.</comment><comment author="jimczi" created="2017-05-22T06:48:15Z" id="303013380">@epixa there is no plan and it seems that this functionality is important for some use cases.
As @jpountz said we can solve this with documentation, explaining that ascending count sort is not accurate and does not give any hint regarding the level of errors.</comment><comment author="epixa" created="2017-05-22T13:57:19Z" id="303107541">@jimczi Thanks for the update.  Any reason why we can't close this then?</comment><comment author="IdanWo" created="2017-07-18T20:03:13Z" id="316180942">Hey, @colings86 . Can you please explain why increasing `shard_size` isn't good enough for ordering in all kinds of ways? I know that ElasticSearch can't tell about the error bounds in some cases, but if I don't rely on it and make my own tests and know how many documents I need to take from each shard by using `shard_size` - then will I be able to always be 100% accurate? For example: making the `shard_size` equal to the `size` (on which `sum_other_doc_count` is always 0). To be concrete, we make `terms aggregations`, which is ordered by a sub `reverse_nested` aggregation and then sub `value_count` aggregation.

In addition, why using the `shard_size` isn't scaled horizontally? If I need to take a lot of documents from each shard, I can split the index to more shards and assign them to more nodes (at setup time, of course). The shard_size would be the same, that's true, but each shard would be less in size/documents count.</comment><comment author="colings86" created="2017-07-19T09:37:59Z" id="316329809">@IdanWo Increasing `shard_size` is only good enough if you can guarantee that the `shard_size` is big enough that all of the terms are returned from each shard. Although this may work for low cardinality fields and/or when the number of shards is relatively small, it does not scale well with number of shards or cardinality of the field. It is true though that in the single shard case and in the case where `shard_size &gt; number_terms_buckets_created` on every shard the results will be 100% accurate with any ordering. 

Although you can indeed split the data across shards you still need to return `number_of_shards * shard_size` buckets to the coordinating node for the reduce phase in order to get the final result. This means that even though the work on the shards is decreased by splitting the work across more of them, for the same `shard_size` more shards means that the coordinating node has to hold more in memory (the response form each shard) and do more work during the reduce phase.</comment><comment author="IdanWo" created="2017-07-21T11:38:58Z" id="316979109">@colings86 , thanks for the excellent explanation! I understand the circumstances, BUT I believe that something isn't right with the design decisions made: I don't understand why `terms aggregation` is considered a memory intensive operation where as `cross cluster search` - which is potentially much memory intensive since it obviously involves multiple indices with multiple shards that return responses to the coordinating node, is considered okay. In cross cluster search the design decision wasn't to limit the request (the request's query or the number of involved shards in the request), but to return batched results to the coordinating node. Why can't we make something similar here? And why actually this improvement doesn't help solving the current issue with a large `shard_size` in terms aggregation?

Therefore, it seems that there is a motivation to support cross cluster search but a low motivation to support full terms aggregations - although technologically they are quite the same in aspects of performance issues. It seems to me that increasing the `shard_size` in a request to a single index, is by far less dangerous than making a request to unlimited number of shards at once. How come sometimes the default is unlimited (`action.search.shard_count.limit` is unlimited) and sometime its nothing and has to be configured (`size:0` in terms aggregations is deprecated). Making a terms aggregation in 1000 shards for `size:300` is worse than a terms aggregation with `size: 0` on 1 index with 10 shards and 200 unique buckets only.

This is taken from the [Elasticsearch 5.4.0 released ](https://www.elastic.co/blog/elasticsearch-5-4-0-released) blog post (talking about [#23946](https://github.com/elastic/elasticsearch/pull/23946)):
&gt; That said, it is quite easy to reach the 1,000 shard limit, especially with the recent release of Cross Cluster Search. As of 5.4.0, Top-N search results and aggregations are reduced in batches of 512, which puts an upper limit on the amount of memory used on the coordinating node, which has allowed us to set the shard soft limit to unlimited by default.

This is taken from the [Tribe Nodes &amp; Cross-Cluster Search](https://www.elastic.co/blog/tribe-nodes-and-cross-cluster-search-the-future-of-federated-search-in-elasticsearch) blog post (pay attention to what is considered a good user experience here):
&gt; Now with the addition of cross cluster search where we are emphasizing searches across many, many shards, having such a soft limit isn&#8217;t providing a good user experience after all. In order to eliminate the impact of querying a large number of shards, the coordinating node now reduces aggregations in batches of 512 results down to a single aggregation object while it&#8217;s waiting for more results to arrive. This upcoming improvement was the initial step to eventually remove the soft limit altogether. The upcoming 5.4.0 release will also allow to reduce the top-N documents in batches. With these two major memory consumers under control, we now default the action.search.shard_count.limit setting to unlimited. This allows users to still limit and protect their searches in the number of shards while  providing a good user experience for other users. When you perform a search request, the node which receives the request becomes the coordinating node which is in charge of forwarding the shard-level requests to the appropriate data nodes, collecting the results, and merging them into a single result set. The memory use on the coordinating node varies according to the number of involved shards. Previous, we had added a 1,000 shard soft limit to try to prevent coordinating nodes from using too much memory.

**Remark:**
I can agree that sometimes it's better to stop the use of a bad-practice, instead of enabling it and support its consequences.  Personally, I sort terms in descending order but via a sub aggregation - which is also discouraged.  But, I really have to do it. So I i'm keep using `shard_size:  20000`, depending on the terms field, which acts okay until now in our environment (600-800 ms at most for a query, but in most of the times the real number of buckets is considerately lower than 20,000, and is more like 300).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify AllEntries, AllField and AllFieldMapper:</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17613</link><project id="" key="" /><description>- Create one AllField field per field eligible for _all.
- Add a positionIncrementGap (with a size of 100, not configurable) between
  each entry in order to distinguish fields when doing phrase query on _all.
</description><key id="146866800">17613</key><summary>Simplify AllEntries, AllField and AllFieldMapper:</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T09:00:00Z</created><updated>2016-04-11T10:13:48Z</updated><resolved>2016-04-11T10:13:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-09T13:22:55Z" id="207788735">This looks like a great cleanup! Does this change have any implications at runtime (I am wondering for instance if highlighting still works the same way)?
</comment><comment author="jimczi" created="2016-04-11T07:29:59Z" id="208201536">thanks @jpountz 
Highlighting would work exactly like a multi-valued string field. Though the _all field is not stored by default (and is not stored in _source either).
</comment><comment author="jpountz" created="2016-04-11T09:16:59Z" id="208245215">I played a bit with highlighting. We used to get something like this before:

```
{
  "took": 31,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.49999997,
    "hits": [
      {
        "_index": "index",
        "_type": "t",
        "_id": "1",
        "_score": 0.49999997,
        "_source": {
          "foo": "the quick fox jumps over the lazy dog",
          "bar": "the lazy dog ate the big cat"
        },
        "highlight": {
          "_all": [
            "the quick fox jumps over the &lt;em&gt;lazy&lt;/em&gt; &lt;em&gt;dog&lt;/em&gt; the &lt;em&gt;lazy&lt;/em&gt; &lt;em&gt;dog&lt;/em&gt; ate the big cat "
          ]
        }
      }
    ]
  }
}
```

and now we have:

```
{
  "took": 59,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 1,
    "max_score": 0.49999997,
    "hits": [
      {
        "_index": "index",
        "_type": "t",
        "_id": "1",
        "_score": 0.49999997,
        "_source": {
          "foo": "the quick fox jumps over the lazy dog",
          "bar": "the lazy dog ate the big cat"
        },
        "highlight": {
          "_all": [
            "the quick fox jumps over the &lt;em&gt;lazy&lt;/em&gt; &lt;em&gt;dog&lt;/em&gt;",
            "the &lt;em&gt;lazy&lt;/em&gt; &lt;em&gt;dog&lt;/em&gt; ate the big cat"
          ]
        }
      }
    ]
  }
}
```

which I think is much nicer.

LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a test for forced values in mapper-attachments plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17612</link><project id="" key="" /><description>This PR just adds a new test where we check that we forcing a value in the JSON document actually works as expected:

``` json
{
     "file": {
        "_content": "BASE64",
        "_name": "12-240.pdf",
        "_language": "en",
        "_content_type": "pdf"
    }
}
```

Note that we don't support forcing all values. So sending:

``` json
{
     "file": {
        "_content": "BASE64",
        "_name": "12-240.pdf",
        "_title": "12-240.pdf",
        "_keywords": "Div42 Src580 LGE Mechtech",
        "_language": "en",
        "_content_type": "pdf"
    }
}
```

Will have absolutely no effect on fields `title` and `keywords`.

Note that when `_language` is set, it only works if `index.mapping.attachment.detect_language` is set to `true`.

Related to https://discuss.elastic.co/t/mapper-attachments/46615/4
</description><key id="146851354">17612</key><summary>Add a test for forced values in mapper-attachments plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Mapper Attachment</label><label>review</label><label>test</label><label>v5.0.0-alpha3</label></labels><created>2016-04-08T08:10:03Z</created><updated>2016-04-29T13:06:02Z</updated><resolved>2016-04-29T13:03:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-29T13:06:02Z" id="215705920">I pushed this change as it's only a new test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better IPv6 support for installing plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17611</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: elasticsearch-2.2.1

**JVM version**: openjdk version "1.8.0_71"

**OS version**: CentOS Linux release 7.1.1503 (Core)

**Description of the problem including expected versus actual behavior**:
Having problems installing plugins on a server with a private IPv4 address and a global/public IPv6 address - because bin/plugin probably tries IPv4 first, then IPv6. If the server has a AAAA-record, the plugin-installer should try IPv6 first (if not link-local), then IPv4, or at least provide configuration-options for use IPv4 or IPv6, or prefer IPv6 over IPv4 and vice versa.

https://tools.ietf.org/html/rfc3484

**Steps to reproduce**:
1. Install Elasticsearch 2.2.1 on a system with private IPv4 address and a public IPv6 address
2. Install a plugin `/usr/share/elasticsearch/bin/plugin install --batch discovery-multicast`
3. Fails with `Failed: ConnectException[Network is unreachable]`

**Provide logs (if relevant)**:

```
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether 00:xx:xx:xx:xx:xx brd ff:ff:ff:ff:ff:ff
    inet 192.168.100.20/24 scope global ens160
       valid_lft forever preferred_lft forever
    inet6 yyyy:yyy:yyyy:yyyy:yyy::yy/64 scope global
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:xxxx:xxxx:952c/64 scope link
       valid_lft forever preferred_lft forever
```

```
dig download.elastic.co AAAA
...
;; ANSWER SECTION:
download.elastic.co.    60      IN      CNAME   dualstack.download-colb-770446651.us-east-1.elb.amazonaws.com.
dualstack.download-colb-770446651.us-east-1.elb.amazonaws.com. 60 IN AAAA 2406:da00:ff00::b848:f489
dualstack.download-colb-770446651.us-east-1.elb.amazonaws.com. 60 IN AAAA 2406:da00:ff00::b849:ab32
dualstack.download-colb-770446651.us-east-1.elb.amazonaws.com. 60 IN AAAA 2406:da00:ff00::b849:ba57
...
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="146848824">17611</key><summary>Better IPv6 support for installing plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aholen</reporter><labels /><created>2016-04-08T07:59:47Z</created><updated>2016-04-08T12:33:20Z</updated><resolved>2016-04-08T11:47:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-08T10:21:23Z" id="207362227">@aholen I think that you can solve this already with

```
$ ./bin/plugin -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true install --batch discovery-multicast
```

Would you be able to try this and report back?

(Please note that the discovery-multicast plugin is deprecated in 2.x and has been removed for 5.x.)
</comment><comment author="aholen" created="2016-04-08T11:47:54Z" id="207396863">@jasontedor Thanks for the tip - that didn't work, but this worked on both Oracle's 1.8.0_40 and openjdk 1.8.0_71:

```
./bin/plugin install -Djava.net.preferIPv6Addresses=true  --batch discovery-multicast
```
</comment><comment author="jasontedor" created="2016-04-08T12:19:31Z" id="207408956">I don't think that that should make any difference, but I would like to understand if and why it does. The reason that I do not think it should make any difference is because the `-D` parameters are parsed by the plugin script and passed as arguments when starting the JVM.

``` bash
$ bash -x ./bin/plugin install -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true --batch discovery-multicast
.
.
.
/Library/Java/JavaVirtualMachines/jdk1.8.0_74.jdk/Contents/Home/bin/java -client -Delasticsearch -Des.path.home=/usr/local/Cellar/elasticsearch/2.3.1/libexec -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Des.default.path.conf=/usr/local/Cellar/elasticsearch/2.3.1/libexec/config -cp '/usr/local/Cellar/elasticsearch/2.3.1/libexec/lib/*' org.elasticsearch.plugins.PluginManagerCliParser install --batch discovery-multicast
-&gt; Installing discovery-multicast...
.
.
.
```

and

``` bash
$ bash -x ./bin/plugin -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true install --batch discovery-multicast
.
.
.
/Library/Java/JavaVirtualMachines/jdk1.8.0_74.jdk/Contents/Home/bin/java -client -Delasticsearch -Des.path.home=/usr/local/Cellar/elasticsearch/2.3.1/libexec -Djava.net.preferIPv4Stack=false -Djava.net.preferIPv6Addresses=true -Des.default.path.conf=/usr/local/Cellar/elasticsearch/2.3.1/libexec/config -cp '/usr/local/Cellar/elasticsearch/2.3.1/libexec/lib/*' org.elasticsearch.plugins.PluginManagerCliParser install --batch discovery-multicast
-&gt; Installing discovery-multicast...
.
.
.
```

Showing that they both cause exactly the same JVM arguments to be used. As far as `-Djava.net.preferIPv4Stack=false`, that is the [default value](https://docs.oracle.com/javase/7/docs/api/java/net/doc-files/net-properties.html) but I included it to be explicit.
</comment><comment author="aholen" created="2016-04-08T12:30:15Z" id="207412339">My mistake @jasontedor - I didn't see your `-Djava.net.preferIPv6Addresses=true` in your first post, and tried only with `-Djava.net.preferIPv4Stack=false` - naturally that didn't make any difference. I agree with you that the order of the parameteres don't make any difference, it is the `-Djava.net.preferIPv6Addresses=true`  that is the vital part here.
</comment><comment author="jasontedor" created="2016-04-08T12:33:20Z" id="207413446">@aholen Perfect, thank you for clarifying! &#128516; 

&gt; it is the `-Djava.net.preferIPv6Addresses=true`  that is the vital part here.

Yes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation are not returned even though logs match in ES 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17610</link><project id="" key="" /><description>**Elasticsearch version**: 

```
2.0.2, 2.2.2, 2.3.0, 2.3.1
```

**JVM version**:

```
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**:

```
Linux defae922887a 4.0.9-boot2docker #1 SMP Thu Sep 10 20:39:20 UTC 2015 x86_64 GNU/Linux
```

**Description of the problem including expected versus actual behavior**:

Running the following query, which includes `range` and `query` filters under Filter Bool doesn't return aggregations:

```
POST /_search?search_type=count
{
  "query": {
    "bool": {
      "filter": {
        "range": {
          "logTime": {
            "from": 1459486745156,
            "to": 1460091545156
          }
        },
        "query": {
          "query_string": {
            "default_field": "body",
            "default_operator": "AND",
            "lowercase_expanded_terms": false,
            "query": "projectName:project*test"
          }
        }
      }
    }
  },
  "sort": [
    {
      "logTime": {
        "order": "desc",
        "ignoreUnmapped": true
      }
    }
  ],
  "aggs": {
    "projectName": {
      "terms": {
        "field": "projectName",
        "size": 100
      }
    },
    "host": {
      "terms": {
        "field": "host",
        "size": 100
      }
    },
    "logLevel": {
      "terms": {
        "field": "logLevel",
        "size": 100
      }
    },
    "projectVersion": {
      "terms": {
        "field": "projectVersion",
        "size": 100
      }
    }
  }
}
```

Response is (notice the `total = 2800`, there are matching logs, but no `aggregations`):

```
{
   "took": 32,
   "timed_out": false,
   "_shards": {
      "total": 16,
      "successful": 16,
      "failed": 0
   },
   "hits": {
      "total": 2800,
      "max_score": 0,
      "hits": []
   }
}
```

The following query works however after converting `filter` into an array:

```
POST /_search?search_type=count
{
  "query": {
    "filtered": {
      "filter": [
        {
            "range": {
              "logTime": {
                "from": 1459486745156,
                "to": 1460091545156
              }
            }
        },
        {
            "query": {
              "query_string": {
                "default_field": "body",
                "default_operator": "AND",
                "lowercase_expanded_terms": false,
                "query": "projectName:project*test"
              }
            }
        }
      ]
    }
  },
  "sort": [
    {
      "logTime": {
        "order": "desc",
        "ignoreUnmapped": true
      }
    }
  ],
  "aggs": {
    "projectName": {
      "terms": {
        "field": "projectName",
        "size": 100
      }
    },
    "host": {
      "terms": {
        "field": "host",
        "size": 100
      }
    },
    "logLevel": {
      "terms": {
        "field": "logLevel",
        "size": 100
      }
    },
    "projectVersion": {
      "terms": {
        "field": "projectVersion",
        "size": 100
      }
    }
  }
}
```

The response is:

```
{
   "took": 105,
   "timed_out": false,
   "_shards": {
      "total": 60,
      "successful": 60,
      "failed": 0
   },
   "hits": {
      "total": 2800,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "logLevel": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "INFO",
               "doc_count": 1459
            },
            {
               "key": "FATAL",
               "doc_count": 1341
            }
         ]
      },
      "host": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "host1504",
               "doc_count": 2800
            }
         ]
      },
      "projectName": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "project-ios-test",
               "doc_count": 1400
            },
            {
               "key": "project-test",
               "doc_count": 1400
            }
         ]
      },
      "projectVersion": {
         "doc_count_error_upper_bound": 0,
         "sum_other_doc_count": 0,
         "buckets": [
            {
               "key": "2.0.0",
               "doc_count": 2800
            }
         ]
      }
   }
}
```

**Steps to reproduce**:

The index has the following mapping:

```
{
   "log-2016-04-08": {
      "mappings": {
         "project-test": {
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "analyzed",
                        "omit_norms": true,
                        "type": "string",
                        "fields": {
                           "raw": {
                              "ignore_above": 256,
                              "index": "not_analyzed",
                              "type": "string",
                              "doc_values": true
                           }
                        }
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               }
            ],
            "date_detection": false,
            "properties": {
               "DmpData": {
                  "type": "string",
                  "index": "no",
                  "norms": {
                     "enabled": false
                  }
               },
               "SDK": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "Platform": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "body": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "host": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "index": {
                  "properties": {
                     "_id": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "_index": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "_routing": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "_type": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     }
                  }
               },
               "logLevel": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "logSource": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "logTime": {
                  "type": "long"
               },
               "logType": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "projectName": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "projectVersion": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "toList": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               }
            }
         },
         "project-ios-test": {
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "analyzed",
                        "omit_norms": true,
                        "type": "string",
                        "fields": {
                           "raw": {
                              "ignore_above": 256,
                              "index": "not_analyzed",
                              "type": "string",
                              "doc_values": true
                           }
                        }
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               }
            ],
            "date_detection": false,
            "properties": {
               "DmpData": {
                  "type": "string",
                  "index": "no",
                  "norms": {
                     "enabled": false
                  }
               },
               "SDK": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "Platform": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "body": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "host": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "index": {
                  "properties": {
                     "_id": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "_index": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     },
                     "_type": {
                        "type": "string",
                        "norms": {
                           "enabled": false
                        },
                        "fields": {
                           "raw": {
                              "type": "string",
                              "index": "not_analyzed",
                              "ignore_above": 256
                           }
                        }
                     }
                  }
               },
               "logLevel": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "logSource": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "logTime": {
                  "type": "long"
               },
               "logType": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               },
               "projectName": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "projectVersion": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "toList": {
                  "type": "string",
                  "norms": {
                     "enabled": false
                  },
                  "fields": {
                     "raw": {
                        "type": "string",
                        "index": "not_analyzed",
                        "ignore_above": 256
                     }
                  }
               }
            }
         },
         "_default_": {
            "dynamic_templates": [
               {
                  "string_fields": {
                     "mapping": {
                        "index": "analyzed",
                        "omit_norms": true,
                        "type": "string",
                        "fields": {
                           "raw": {
                              "ignore_above": 256,
                              "index": "not_analyzed",
                              "type": "string",
                              "doc_values": true
                           }
                        }
                     },
                     "match": "*",
                     "match_mapping_type": "string"
                  }
               }
            ],
            "date_detection": false,
            "properties": {
               "DmpData": {
                  "type": "string",
                  "index": "no",
                  "norms": {
                     "enabled": false
                  }
               },
               "host": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "logLevel": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "logTime": {
                  "type": "long"
               },
               "projectName": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               },
               "projectVersion": {
                  "type": "string",
                  "index": "not_analyzed",
                  "ignore_above": 256
               }
            }
         }
      }
   }
}
```

**Provide logs (if relevant)**:

A sample log is:

```
{
               "index": {
                  "_index": "log-2016-04-08",
                  "_type": "project-test",
                  "_id": "d490daea5bf23010560017",
                  "_routing": "project-test"
               },
               "SDK": "0.9.0",
               "Platform": "Windows Server 2008 R2(6.1)",
               "body": "[2014/04/22 17:21:31.704] [ItemCode:12402][ItemCount:1]\n",
               "host": "host1504",
               "logLevel": "INFO",
               "logSource": "openapi",
               "logTime": 1460091528472,
               "logType": "mocha",
               "projectName": "project-test",
               "projectVersion": "2.0.0",
               "toList": "[NA11607, NA10214]"
            }
```
</description><key id="146829175">17610</key><summary>Aggregation are not returned even though logs match in ES 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kadishmal</reporter><labels /><created>2016-04-08T06:36:34Z</created><updated>2016-04-12T14:08:12Z</updated><resolved>2016-04-12T14:08:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-12T14:08:12Z" id="208924138">Hi @kadishmal 

The `bool` query doesn't have a `query` clause.  Use `must` instead. Unfortunately, the parser then skipped over the rest of your request, ignoring the aggs.  The whole query DSL parsing is much stricter in 5.0, which means that you would have found your issue immediately:

```
"reason": "[bool] query does not support [query]"
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Assert names of read writeables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17609</link><project id="" key="" /><description>Adds an assertion that when reading a NamedWriteable it has the same name
you read. It'd be _super_ weird if it didn't.
</description><key id="146784596">17609</key><summary>Assert names of read writeables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-08T01:12:25Z</created><updated>2016-04-08T12:39:17Z</updated><resolved>2016-04-08T12:39:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-08T01:12:38Z" id="207159234">@colings86 we talked about this a while back, I'm just getting around to doing it.
</comment><comment author="colings86" created="2016-04-08T07:25:14Z" id="207268636">LGTM, thanks for adding this check
</comment><comment author="nik9000" created="2016-04-08T12:39:17Z" id="207416274">Thanks! Merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove registerQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17608</link><project id="" key="" /><description>We've fully cut over to registerQuery!
</description><key id="146771318">17608</key><summary>Remove registerQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T23:34:09Z</created><updated>2016-04-08T12:29:25Z</updated><resolved>2016-04-08T12:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-08T07:05:44Z" id="207262232">LGTM
</comment><comment author="nik9000" created="2016-04-08T12:29:25Z" id="207412142">Closed by d349de71aafbaf7734a43e673aaecc9cc94b94c4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Additional "moving" pipeline aggregations </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17607</link><project id="" key="" /><description>Similar to the "moving average" pipeline agg, it could be useful to be able to calculate moving max, moving min, moving variance, etc. Possibly allowing the user to configure which metric agg contributed to the moving value (min, max, avg, etc).

(This arose in the context of a moving median pipeline, although calculating median in the first place can be difficult).
</description><key id="146768476">17607</key><summary>Additional "moving" pipeline aggregations </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">kurtado</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2016-04-07T23:11:28Z</created><updated>2016-09-27T19:08:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="l8liu" created="2016-09-27T19:08:52Z" id="249966429">Moving standard deviation is useful
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove hostname from NetworkAddress.format (2.x)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17606</link><project id="" key="" /><description>This removes the inconsistent output of IP addresses. The format was parsing-unfriendly and it makes it hard to reason about API responses, such as to `_nodes`.

With this change in place, it will never print the hostname as part of the default format, which has the
added benefit that it can be used consistently for URIs, which was not the case when the hostname might appear at the front with "hostname/ip:port".

This is the 2.x port of #17601.

Closes #17604 (for 2.x)
</description><key id="146767969">17606</key><summary>Remove hostname from NetworkAddress.format (2.x)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Core</label><label>enhancement</label><label>v2.4.0</label></labels><created>2016-04-07T23:07:38Z</created><updated>2016-04-12T13:59:56Z</updated><resolved>2016-04-11T15:52:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-08T07:40:13Z" id="207277550">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Swapping when mlockall is set to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17605</link><project id="" key="" /><description>**Elasticsearch version**: 1.5.2

**JVM version**: 1.8u60

**OS version**: CentOS 7.1

**Description of the problem including expected versus actual behavior**: Appears to be related to https://github.com/elastic/elasticsearch/issues/7504. 

On a 128GB server with 24 cores using a hard disk drive in a 5 node cluster.

Running `sudo fgrep Swap /proc/PID/status` reports `VmSwap:     1980 kB`

**Steps to reproduce**:
1. systemd service file configured `LimitNOFILE=65535`
2. systemd service file configured `LimitMEMLOCK=infinity`
3. Configured to use non-standard tmp directory due to 'nonexec' using `-Djna.tmpdir`

**Provide logs (if relevant)**:

Attached. They are a bit sanitized, but should be good.
[nodes_stats.txt](https://github.com/elastic/elasticsearch/files/209319/nodes_stats.txt)
[nodes.txt](https://github.com/elastic/elasticsearch/files/209320/nodes.txt)
</description><key id="146757298">17605</key><summary>Swapping when mlockall is set to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdelgad</reporter><labels /><created>2016-04-07T22:15:10Z</created><updated>2016-04-10T15:45:48Z</updated><resolved>2016-04-08T00:35:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-08T00:35:15Z" id="207149090">When [Elasticsearch locks memory using `mlockall`](https://github.com/elastic/elasticsearch/blob/e0cde29a685efcf1487b008379bbe4b8f39bf115/core/src/main/java/org/elasticsearch/bootstrap/JNANatives.java#L65), it locks with the flag [`MCL_CURRENT`](http://linux.die.net/man/2/mlockall). This means that only pages that are currently mapped into the address space will be locked, and future pages that are mapped will not be locked. This means that any pages that are not mapped into the address space are eligible for swapping, and thus having `mlockall` enabled and successfully executed is not inconsistent with seeing pages swapped by the virtual memory subsystem.

There are a few circumstances under which Elasticsearch will map additional pages:
- if min heap size (`Xms`) and max heap size (`Xmx`) are not equal to each other, then the heap size can change from its size at startup to its steady-state size as Elasticsearch executes requests
- if using a collector that releases memory back to the operating system (the serial collector and the garbage first collector both do this but note that we [recommend _not_](https://www.elastic.co/blog/a-heap-of-trouble) using either of these collectors with Elasticsearch and their use with Elasticsearch is unsupported)
- if index files are mapped into memory via `mmap`

It's this last case that likely hits everyone because we recommend setting `Xms` equal to `Xmx` (and you get this for free if you use `ES_HEAP_SIZE` to set the heap size), and we recommend _not_ using either of the collectors that release memory back to the operating system. Thus, only the third case remains.

You see, `mlockall` is really just a last resort for minimizing swapping. It's not completely effective for reducing swapping.

If you want to disable swapping, then disable swapping using [`swapoff -a`](http://linux.die.net/man/2/swapoff). If for some reason this is not an option for you, then minimize swapping by setting the [kernel parameter `vm.swappiness`](https://en.wikipedia.org/wiki/Swappiness) to `1` (using `sysctl` and please do not set to `0` because the [behavior](http://gitorious.ti.com/ti-linux-kernel/ti-linux-kernel/commit/fe35004fbf9eaf67482b074a2e032abb9c89b1dd?format=patch) of this value changed in the 3.x kernel series). The absolute last resort is `mlockall` because at least this prevents the JVM itself (meaning the code, data and stack of the JVM) and parts of the heap from being swapped out.

A bad thing that can happen is part of the JVM's heap gets swapped out to disk and then the JVM goes to run a full garbage collection cycle and touches the entire heap leading to the disk thrashing during a garbage collection cycle. This is why ideally we want to be able to lock the entire heap in memory but with `mlockall` this is only possible if `Xms` is equal to `Xmx` and we are not using a releasing collector.

You might be wondering why we do not also lock memory with the flag `MCL_FUTURE`? This would lock memory mapped pages into physical memory but that's horrific because most user's data size exceeds their physical memory size. Further, it completely negates the amazing job the operating system does at managing memory mapped pages.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_nodes API prints hostname inconsistently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17604</link><project id="" key="" /><description>ES 2.x / 5.0

APIs that depend on the printing of `NetworkAddress.format` for `TransportAddress` objects, such as `_nodes/transport` currently print out the local node versus non-local nodes (e.g., `n - 1` nodes) inconsistently.

If the published address involved a hostname, then the local node will have resolved its hostname and it will report its hostname alongside its IP address and port. However, since it does not know/care about the hostname of _most_ other nodes, then it will not have resolved their hostname and it will not print it out. For example: if you start two nodes with

``` shell
$ bin/elasticsearch -Des.network.publish_host=`hostname` -Des.network.bind_host=127.0.0.1,`hostname`
```

Then try to find the transport address of those nodes:

``` shell
$ curl -XGET localhost:9201/_nodes/transport?pretty
```

Then you'll get a response like (ignoring node uid and name, which are random):

``` json
{
  "cluster_name": "elasticsearch",
  "nodes": {
    "cnPqK1eyQUykNZkhRXCeJQ": {
      "name": "Icemaster",
      "transport_address": "Chriss-MBP.home/192.168.1.6:9301",
      "host": "192.168.1.6",
      "ip": "192.168.1.6",
      "version": "2.3.0",
      "build": "8371be8",
      "http_address": "Chriss-MBP.home/192.168.1.6:9201",
      "transport": {
        "bound_address": [
          "127.0.0.1:9301",
          "[fe80::9425:ff:fe76:2429]:9301",
          "[fe80::82e6:50ff:fe0a:e71a]:9301",
          "192.168.1.6:9301"
        ],
        "publish_address": "Chriss-MBP.home/192.168.1.6:9301",
        "profiles": {}
      }
    },
    "_yVpRl1pSRa26EHBbr8ZHA": {
      "name": "Masque",
      "transport_address": "192.168.1.6:9300",
      "host": "192.168.1.6",
      "ip": "192.168.1.6",
      "version": "2.3.1",
      "build": "bd98092",
      "http_address": "Chriss-MBP.home/192.168.1.6:9200",
      "transport": {
        "bound_address": [
          "127.0.0.1:9300",
          "[fe80::9425:ff:fe76:2429]:9300",
          "[fe80::82e6:50ff:fe0a:e71a]:9300",
          "192.168.1.6:9300"
        ],
        "publish_address": "192.168.1.6:9300",
        "profiles": {}
      }
    }
  }
}
```

If you instead invoke the same API from the node running at port `9200`, then its `transport_address` will be reported properly with its hostname, while the other node will lose the hostname. The "problem" is that the `TransportAddress` address used by `n - 1` of the nodes is serialized and its hostname gets forgotten on other nodes, so when it gets printed, we avoid doing a hostname lookup and end up with inconsistent results.

The inconsistency can lead to parsing failure when unexpected and it leads to inconsistent handling/behavior. The solution is pretty straight forward: stop printing the hostname. This has the added benefit that it means that the format is always _directly_ usable within a URI without any parsing.
</description><key id="146743856">17604</key><summary>_nodes API prints hostname inconsistently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Core</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T21:13:11Z</created><updated>2016-04-11T16:23:02Z</updated><resolved>2016-04-11T15:52:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2016-04-07T23:09:24Z" id="207132821">Reopened for the #17606 version since it wasn't just a cherry-pick.
</comment><comment author="pickypg" created="2016-04-11T15:52:56Z" id="208416355">Closed by #17601 (master) #17606 (2.x).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggester: Fix loss of precision of completion weights</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17603</link><project id="" key="" /><description>Currently suggester scores are treated as floats, which may lead to loss of precision
when using when using int weights that are greater than 2^23 in Completion Suggester.

This change treats the int weights as doubles rather than floats to ensure no loss in precision

Stalled: lucene-level change required

closes #11392
</description><key id="146743116">17603</key><summary>Completion Suggester: Fix loss of precision of completion weights</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>stalled</label><label>v5.4.4</label></labels><created>2016-04-07T21:09:43Z</created><updated>2017-06-27T10:28:18Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-12T21:30:42Z" id="246500543">@areek any idea if the Lucene changes for this are in now that we're on 6.2?
</comment><comment author="dakrone" created="2017-04-07T23:12:57Z" id="292673110">@areek is this still stalled?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add bwc support for reading  pre-5.0 completion index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17602</link><project id="" key="" /><description>This adds support for reading and querying 2.x completion index in 5.0. 

WIP: documentation and more tests
</description><key id="146723093">17602</key><summary>Add bwc support for reading  pre-5.0 completion index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T19:40:36Z</created><updated>2016-05-02T11:56:49Z</updated><resolved>2016-04-26T01:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove hostname from NetworkAddress.format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17601</link><project id="" key="" /><description>This removes the inconsistent output of IP addresses. The format was parsing-unfriendly and it makes it hard to reason about API responses, such as to `_nodes`. The difficultly lies in the fact that the `TransportAddress` is serialized across nodes, so the hostname is never resolved on those nodes as a result, and the format therefore never prints the hostname.

With this change in place, it will never print the hostname as part of the format, which has the
added benefit that it can be used consistently for URIs, which was not the case when the hostname might appear at the front with "hostname/ip:port".

An example of the response from my local machine after starting two nodes via without this change:

``` shell
$ bin/elasticsearch -Des.network.publish_host=`hostname` -Des.network.bind_host=127.0.0.1,`hostname`
```

``` http
GET /_nodes/transport
{
  "cluster_name": "elasticsearch",
  "nodes": {
    "cnPqK1eyQUykNZkhRXCeJQ": {
      "name": "Icemaster",
      "transport_address": "Chriss-MBP.home/192.168.1.6:9301",
      "host": "192.168.1.6",
      "ip": "192.168.1.6",
      "version": "2.3.0",
      "build": "8371be8",
      "http_address": "Chriss-MBP.home/192.168.1.6:9201",
      "transport": {
        "bound_address": [
          "127.0.0.1:9301",
          "[fe80::9425:ff:fe76:2429]:9301",
          "[fe80::82e6:50ff:fe0a:e71a]:9301",
          "192.168.1.6:9301"
        ],
        "publish_address": "Chriss-MBP.home/192.168.1.6:9301",
        "profiles": {}
      }
    },
    "_yVpRl1pSRa26EHBbr8ZHA": {
      "name": "Masque",
      "transport_address": "192.168.1.6:9300",
      "host": "192.168.1.6",
      "ip": "192.168.1.6",
      "version": "2.3.1",
      "build": "bd98092",
      "http_address": "Chriss-MBP.home/192.168.1.6:9200",
      "transport": {
        "bound_address": [
          "127.0.0.1:9300",
          "[fe80::9425:ff:fe76:2429]:9300",
          "[fe80::82e6:50ff:fe0a:e71a]:9300",
          "192.168.1.6:9300"
        ],
        "publish_address": "192.168.1.6:9300",
        "profiles": {}
      }
    }
  }
}
```

If I were to request it from the node at 9200, then it would flip which node has the hostname as part of the response.

Of interest, `http_address` is always in the same format because it's added to the `NodeService` as a string attribute at startup rather than deserialized from its `TransportAddress` between nodes.

Closes #17604
</description><key id="146720939">17601</key><summary>Remove hostname from NetworkAddress.format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Core</label><label>enhancement</label><label>PITA</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T19:30:36Z</created><updated>2016-04-12T13:59:50Z</updated><resolved>2016-04-07T21:28:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-07T20:37:36Z" id="207078688">I think this LGTM, but I'm worried about this going into older versions since it's technically breaking the format of output :-/

Is this a bugfix? Is there an original bug we can associate with this?
</comment><comment author="rmuir" created="2016-04-07T20:44:31Z" id="207080623">Thanks for cleaning this up @pickypg !

Originally the crazy behavior here was just a path of least resistance: we removed arbitrary DNS lookups happening at unpredictable times but did not change the formatting to keep things simpler. If the user passed in a hostname and we resolved it anyway, we still tried to show it.

But this is seriously confusing and inconsistent, and also the inconsistencies between `format()` and `formatAddress()` have caused real accidental bugs/test-failures before. If someone is trying to parse this output i can see them easily running into the same trap: hostnames are only very rarely included and they don't expect that.
</comment><comment author="pickypg" created="2016-04-07T21:00:41Z" id="207085498">This will only be backported to the 2.x branch to avoid breaking anyone that depends on the inconsistent behavior.
</comment><comment author="jasontedor" created="2016-04-07T21:20:50Z" id="207092568">LGTM.
</comment><comment author="pickypg" created="2016-04-07T23:08:39Z" id="207132554">Opened #17606 for the 2.x version since I had to do it by hand rather than a cherry-pick.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms order aggregation name cannot contain a period</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17600</link><project id="" key="" /><description>I'm running on 1.7.5 and the following works:

```
  "aggs": {
    "terms": {
      "terms": {
        "field": "stack_id",
        "size": 21,
        "order": {
          "distinct_user_raw": "desc"
        }
      },
      "aggs": {
        "distinct_user_raw": {
          "cardinality": {
            "field": "user.raw",
            "precision_threshold": 100
          }
        }
      }
    }
  }
```

however the following throws an error:

```
  "aggs": {
    "terms": {
      "terms": {
        "field": "stack_id",
        "size": 21,
        "order": {
          "distinct_user.raw": "desc"
        }
      },
      "aggs": {
        "distinct_user.raw": {
          "cardinality": {
            "field": "user.raw",
            "precision_threshold": 100
          }
        }
      }
    }
  }
```

with the error:

```
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][0]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][1]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][2]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][3]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][4]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][5]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][6]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][7]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][8]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}{[vsKZW_avSS6E_Ci72eNoFg][events-v1-201604][9]: AggregationExecutionException[Invalid term-aggregator order path [distinct_user.raw]. Unknown aggregation [distinct_user]]}]",
   "status": 500
}
```

Please note that if I remove the order part it works.
</description><key id="146685847">17600</key><summary>Terms order aggregation name cannot contain a period</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niemyjski</reporter><labels><label>:Aggregations</label><label>stalled</label></labels><created>2016-04-07T17:05:14Z</created><updated>2016-06-17T09:52:15Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T17:15:02Z" id="207000748">Yes.  Don't do that :)

I tried out the bucket path syntax (eg using `[bar.baz]._count`) but that isn't supported here.  Perhaps we should.
</comment><comment author="niemyjski" created="2016-04-07T17:17:40Z" id="207002890">The fact that you can have aggregations with periods but not sort by it seems like a bug. On a side note I changed my code to remove the periods. But it should be all or nothing (you can sort by it, or aggregation names should be allowed to have a .)
</comment><comment author="jpountz" created="2016-04-08T09:10:54Z" id="207340377">Related to #9059.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove MappedFieldType.useTermQueryWithQueryString() and isNumeric().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17599</link><project id="" key="" /><description>In both cases, what elasticsearch is really interested in is whether the field
is an analyzed string field. So it can just check `tokenized()` instead.
</description><key id="146683563">17599</key><summary>Remove MappedFieldType.useTermQueryWithQueryString() and isNumeric().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T16:56:20Z</created><updated>2016-04-12T06:46:40Z</updated><resolved>2016-04-12T06:46:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T18:23:51Z" id="208486156">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove MappedFieldType.isSortable().</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17598</link><project id="" key="" /><description>It is not really needed since it is pretty equivalent to "supports fielddata or doc values". So we can just remove it and expect an exception when calling `MappedFieldType.fielddataBuilder()` if the field does not support fielddata.
</description><key id="146667210">17598</key><summary>Remove MappedFieldType.isSortable().</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T15:53:52Z</created><updated>2016-04-12T06:57:40Z</updated><resolved>2016-04-12T06:57:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T18:11:37Z" id="208482234">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should range aggregations support the `missing` option?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17597</link><project id="" key="" /><description>Background for the discussion can be found at https://discuss.elastic.co/t/is-the-missing-value-option-officially-supported-in-date-range-aggregations/45855/4. The use-case seems to be about grouping missing values with uninteresting values.
</description><key id="146666785">17597</key><summary>Should range aggregations support the `missing` option?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label><label>test</label></labels><created>2016-04-07T15:52:34Z</created><updated>2017-03-13T07:34:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-08T09:29:05Z" id="207347582">We discussed this on FixIt friday and although this feature works today it's totally unsupported (we don't test it, we don't document it).

Pasting here a typical request:

``` js
GET myindex/user/_search
{
  "aggs": {
    "age_groups": {
      "date_range": {
        "field": "user_date_of_birth",
        "missing": "1899-12-31",
        "ranges": [
          {
              "key": "Generation Z",
              "from": "2000"
            },
            {
              "key": "Generation Y",
              "from": "1980",
              "to": "2000"
            },
            {
              "key": "Generation X",
              "from": "1965",
              "to": "1980"
            },
            {
              "key": "Baby Boomer",
              "from": "1946",
              "to": "1965"
            },
            {
              "key": "Silent Generation",
              "from": "1925",
              "to": "1946"
            },
            {
              "key": "Greatest Generation",
              "from": "1900",
              "to": "1925"
            },
            {
              "key": "Other",
              "to": "1900"
            }
        ]
      }
    }
  }
}
```

Everybody without any age or born before 1900 will fall in the `Other` bucket.

There might be interesting use cases coming from the UI team. @rashidkpc WDYT? Should we support officially that kind of feature or simply close this as a non supported/non documented feature we might remove at any time?
</comment><comment author="rashidkpc" created="2016-04-08T14:52:19Z" id="207465182">I'd keep this, its definitely useful. And from what i understand `missing` is valid on most (all?) other bucket aggs
</comment><comment author="ChineseElectricPanda" created="2017-03-13T07:34:21Z" id="286033962">Hi, I am new to GitHub public projects, so if possible I would like to make this my first contribution to the project to get my feet wet. Seems like this just involves writing some tests for this feature and documenting it? If there is any additional related information please let me know.
Thanks.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ConstructingObjectParser adapts ObjectParser for ctor args</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17596</link><project id="" key="" /><description>ObjectParser makes parsing XContent 95% easier. No more nested loops.
No more forgetting to use ParseField. Consistent handling for arrays.
Awesome. But ObjectParser doesn't support building things objects whose
constructor arguments are mixed in with the rest of its properties.
Enter ConstructingObjectParser! ConstructingObjectParser queues up
fields until all of the constructor arguments have been parsed and
then sets them on the target object.

Closes #17352
</description><key id="146653171">17596</key><summary>ConstructingObjectParser adapts ObjectParser for ctor args</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-07T15:06:11Z</created><updated>2016-04-27T18:16:21Z</updated><resolved>2016-04-27T18:16:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-07T15:08:22Z" id="206945221">I labeled this discuss rather than review because it screws with error messages a bit too much. I can fix that if it integrate it more tightly into ObjectParser - either by hacking ObjectParser to expose something funky for ObjectSpooler, or, probably better, by extracting 90% of ObjectParser into an abstract base class and making ObjectSpooler inherit from that. In fact, I might just do that right now and see.
</comment><comment author="nik9000" created="2016-04-07T21:29:05Z" id="207095044">OK - I was able to really really improve this. No more broken error messaging and cleaner syntax! I did it by pulling a big chunk of ObjectParser out into a superclass which both ObjectParser and ObjectSpooler extend. Then I modified it's implementation to ally ObjectSpooler to wrap all the consumers cleanly. Not all of ObjectParser made it to the interface because it isn't compatible, but most of it is.

I also sped it up a bit, removing a hash lookup from constructor arguments and removing the hash required.
</comment><comment author="nik9000" created="2016-04-07T21:52:19Z" id="207104892">@colings86 can you review this? I'm not sure who the right person is but we talked about it earlier today so maybe it is you? Feel free to kick it back to me if not.
</comment><comment author="colings86" created="2016-04-08T09:09:54Z" id="207339958">@nik9000 I left some comments but it looks good so far. I have assumed that apart from extracting the super class and changing method and variable order (as you put in comments) you have not changed the `ObjectParser` class and so have not really reviewed it deeply. If that is not a good assumption let me know and I will review that class.
</comment><comment author="nik9000" created="2016-04-10T02:14:04Z" id="207897993">&gt; If that is not a good assumption let me know and I will review that class.

Sorry! That isn't actually true.

The order of stuff in that class was really wacky and I tried not to change it _that_ much but my eyes hurt seeing fields buried in the object. Those aren't big changes though.

The biggest change to ObjectParser is that it now has two declareField methods. One is the original for backward compatibility/"special" things. The other is compatible with the changes to the superclass. What I did was extract the superclass and realize that ObjectParser wanted to parse the field and set it to the object all in one go - and that was ok if I didn't want good error reporting. But to get nice error reporting I had to give ObjectSpooler a good way to wrap the consumer. So I created that second declareField method that takes a consumer and a ContextParser. This separated the two phases so that ObjectSpooler could do the wrapping it needed and ObjectParser just immediately applies the consumer to the result of the parse.

Then I had to change all the declareString, declareInt, declareXXX methods that I could do use the new declareField method. I don't think this change incurs any additional overhead and I think it actually makes it easier to read in the long run, but it is more than just extracting the methods.

Also that means the ObjectSpooler, or whatever we call it, doesn't support all the stuff that ObjectParser does - the things that relied on the original declareField method: declareNamedObjects and declareValue. I don't _think_ that is going to be a problem. Or, at least, I think if that is a problem we can fix it later.
</comment><comment author="jasontedor" created="2016-04-25T18:27:19Z" id="214470945">I think it looks great @nik9000, and solves a use-case I encountered earlier today! I left some comments.
</comment><comment author="nik9000" created="2016-04-26T20:08:33Z" id="214870634">@jasontedor I believe I've addressed your comments. This will probably need some special love when I rebase it - specifically you can't use Void as a context any more.
</comment><comment author="jasontedor" created="2016-04-27T16:15:16Z" id="215135325">&gt; I believe I've addressed your comments.

@nik9000 I've reviewed it again. I left one more code-review comment and we need to wrap up the discussion around the poor man's repeat. I don't like the name `ConstructingObjectParser` but I can't come up with anything better.
</comment><comment author="jasontedor" created="2016-04-27T17:37:54Z" id="215164894">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bootstrapping bootstrap checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17595</link><project id="" key="" /><description>This pull request offers two enhancements to the bootstrap checks.

The first is to execute the bootstrap checks in a way that we can detect
whether or not we our bound to a non-loopback interface or publishing to
a non-loopback interface (instead of merely checking whether or not any
of the network settings are set but not what their value is). This is
accomplished by moving the execution of the bootstrap checks to after
the networking services are started; as a bonanza, this enables us to
output the node name with the failing checks, a capability that did not
exist before when these messages were logged before the node had even
started.

The second is a modification to the bootstrap checks to output all
failing checks instead of forcing the user to resolve them one-by-one.

Closes #17474, #17570 
</description><key id="146636033">17595</key><summary>Bootstrapping bootstrap checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-07T14:01:56Z</created><updated>2016-04-13T13:05:32Z</updated><resolved>2016-04-13T13:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-07T14:06:15Z" id="206922831">Now we have this:

``` bash
$ ~/elasticsearch/elasticsearch-5.0.0-alpha1-SNAPSHOT/bin/elasticsearch -E es.network.host=192.168.1.8
[2016-04-07 10:02:46,394][INFO ][node                     ] [Texas Twister] version[5.0.0-alpha1-SNAPSHOT], pid[37835], build[e60cf58/2016-04-07T13:34:52.575Z]
[2016-04-07 10:02:46,395][INFO ][node                     ] [Texas Twister] initializing ...
[2016-04-07 10:02:46,664][INFO ][plugins                  ] [Texas Twister] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-04-07 10:02:46,686][INFO ][env                      ] [Texas Twister] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [109.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-04-07 10:02:46,686][INFO ][env                      ] [Texas Twister] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-04-07 10:02:48,245][INFO ][node                     ] [Texas Twister] initialized
[2016-04-07 10:02:48,248][INFO ][node                     ] [Texas Twister] starting ...
[2016-04-07 10:02:48,342][INFO ][transport                ] [Texas Twister] publish_address {192.168.1.8:9300}, bound_addresses {192.168.1.8:9300}
Exception in thread "main" java.lang.RuntimeException: bootstrap checks failed
max size virtual memory [35184372088832] for user [jason] likely too low, increase to [unlimited]
please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:96)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:63)
        at org.elasticsearch.node.Node.start(Node.java:333)
        at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:200)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
[2016-04-07 10:02:48,399][INFO ][node                     ] [Texas Twister] stopping ...
[2016-04-07 10:02:48,410][INFO ][node                     ] [Texas Twister] stopped
[2016-04-07 10:02:48,411][INFO ][node                     ] [Texas Twister] closing ...
[2016-04-07 10:02:48,421][INFO ][node                     ] [Texas Twister] closed
```

where I have two failing checks with some additional detail in the logs

```
[2016-04-07 10:02:48,346][ERROR][bootstrap                ] [Texas Twister] Exception
java.lang.RuntimeException: bootstrap checks failed
max size virtual memory [35184372088832] for user [jason] likely too low, increase to [unlimited]
please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:96)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:63)
        at org.elasticsearch.node.Node.start(Node.java:333)
        at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:200)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
        Suppressed: java.lang.IllegalStateException: max size virtual memory [35184372088832] for user [jason] likely too low, increase to [unlimited]
                at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:85)
                ... 10 more
        Suppressed: java.lang.IllegalStateException: please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
                at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:85)
                ... 10 more
```

Note that we no longer fail if we set `es.network.host` to a loopback address:

``` bash
&#177; ~/elasticsearch/elasticsearch-5.0.0-alpha1-SNAPSHOT/bin/elasticsearch -E es.network.host=_local_
[2016-04-07 10:04:32,032][INFO ][node                     ] [Forgotten One] version[5.0.0-alpha1-SNAPSHOT], pid[38016], build[e60cf58/2016-04-07T13:34:52.575Z]
[2016-04-07 10:04:32,033][INFO ][node                     ] [Forgotten One] initializing ...
[2016-04-07 10:04:32,303][INFO ][plugins                  ] [Forgotten One] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-04-07 10:04:32,327][INFO ][env                      ] [Forgotten One] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [109.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-04-07 10:04:32,328][INFO ][env                      ] [Forgotten One] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-04-07 10:04:33,867][INFO ][node                     ] [Forgotten One] initialized
[2016-04-07 10:04:33,868][INFO ][node                     ] [Forgotten One] starting ...
[2016-04-07 10:04:33,944][INFO ][transport                ] [Forgotten One] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2016-04-07 10:04:33,945][WARN ][bootstrap                ] [Forgotten One] max size virtual memory [35184372088832] for user [jason] likely too low, increase to [unlimited]
[2016-04-07 10:04:33,945][WARN ][bootstrap                ] [Forgotten One] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
[2016-04-07 10:04:36,984][INFO ][cluster.service          ] [Forgotten One] new_master {Forgotten One}{eg0mPFkAQ7qrAGGNKy4-Ow}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-04-07 10:04:37,004][INFO ][http                     ] [Forgotten One] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2016-04-07 10:04:37,004][INFO ][node                     ] [Forgotten One] started
[2016-04-07 10:04:37,011][INFO ][gateway                  ] [Forgotten One] recovered [0] indices into cluster_state
```
</comment><comment author="bleskes" created="2016-04-12T18:57:01Z" id="209053007">I like the change. Left some minor suggestions
</comment><comment author="jasontedor" created="2016-04-12T21:22:15Z" id="209108209">@bleskes I've responded to your feedback.
</comment><comment author="bleskes" created="2016-04-13T07:05:30Z" id="209263996">LGTM with one minor exception (&amp; one suggestion) - now it's illegal to call node.start() twice. I'm not saying that's good, but I don't think we should change it here. We don't have to go through another round once fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest Test failure: indices.stats/10_index/Index when testing using zip distributions on debian</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17594</link><project id="" key="" /><description>Build URL:

```
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=debian/213
```

Reproduce Command (note: this does not reproduce locally for me):

```
gradle :distribution:integ-test-zip:integTest -Dtests.seed=C508E06E98CA79E -Dtests.class=org.elasticsearch.test.rest.RestIT -Dtests.method="test {p0=indices.stats/10_index/Index - one index}" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=el-CY -Dtests.timezone=Poland
```

Failure:

```
java.lang.AssertionError: expected [2xx] status code but api [index] returned [503 Service Unavailable] [{"error":{"root_cause":[{"type":"process_cluster_event_timeout_exception","reason":"failed to process cluster event (put-mapping [bar]) within 30s"}],"type":"process_cluster_event_timeout_exception","reason":"failed to process cluster event (put-mapping [bar]) within 30s"},"status":503}]
   &gt;    at __randomizedtesting.SeedInfo.seed([C508E06E98CA79E:8404B1DC4770CA66]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:107)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:387)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="146624708">17594</key><summary>Rest Test failure: indices.stats/10_index/Index when testing using zip distributions on debian</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Index APIs</label><label>jenkins</label></labels><created>2016-04-07T13:28:05Z</created><updated>2017-06-16T17:00:00Z</updated><resolved>2017-06-16T17:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-06-16T17:00:00Z" id="309079554">given that this issue had no activity in more than a year, I would close it. We can always reopen if we encounter the same failure again.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop returning additional HTTP headers on exceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17593</link><project id="" key="" /><description>When an `ElasticsearchException` is bubbled up to HTTP, there are additional headers being returned.

``` bash
# curl -X DELETE localhost:9200/test
{"acknowledged":true}

# curl -X PUT localhost:9200/test -v
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; PUT /test HTTP/1.1
&gt; Host: localhost:9200
&gt; User-Agent: curl/7.43.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 21
&lt;
* Connection #0 to host localhost left intact
{"acknowledged":true}


# curl -X PUT localhost:9200/test -v
*   Trying 127.0.0.1...
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; PUT /test HTTP/1.1
&gt; Host: localhost:9200
&gt; User-Agent: curl/7.43.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 400 Bad Request
&lt; es.index: test
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 203
&lt;
* Connection #0 to host localhost left intact
{"error":{"root_cause":[{"type":"index_already_exists_exception","reason":"already exists","index":"test"}],"type":"index_already_exists_exception","reason":"already exists","index":"test"},"status":400}
```

If those headers are not needed on HTTP, maybe it does not make any sense to return them? All the information looks available to me in the HTTP response body, but I am possibly missing some cases.
</description><key id="146621071">17593</key><summary>Stop returning additional HTTP headers on exceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:REST</label><label>bug</label></labels><created>2016-04-07T13:17:15Z</created><updated>2017-01-24T15:12:46Z</updated><resolved>2017-01-24T15:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-08T09:28:09Z" id="207347269">@clintongormley do you know if this feature is used by the clients?
</comment><comment author="clintongormley" created="2016-04-12T14:12:02Z" id="208925571">@jpountz i'm unaware of anybody using this feature.
</comment><comment author="clintongormley" created="2016-06-17T09:54:06Z" id="226729274">Let's remove these headers
</comment><comment author="javanna" created="2017-01-18T13:56:02Z" id="273481003">These headers are printed out as part of the response body already, I don't think there's any need to send them back as response headers too. They were introduced to prevent having to add custom exceptions to our codebase whenever we need to throw an exception that has to hold some additional metadata that ElasticsearchException doesn't support. Maybe the headers naming should be changed as well to something like metadata? @s1monw @tlrx  what do you think?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.mapper.dynamic is not honored during auto-creation of an index from template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17592</link><project id="" key="" /><description>**Elasticsearch version**:  2.3.1

**JVM version**: 1.8.0_66

**OS version**: CentOS 7

**Description of the problem including expected versus actual behavior**:
When a new index is auto-created (by indexing a document) using an index template, the setting `index.mapper.dynamic` is not honored: If the document has an unmapped type, it gets indexed nevertheless and the additional type is added to the index.

_Expected result:_ The indexing operation should be rejected as dynamic mapping is disabled for the index created from the template.

**Steps to reproduce**:

Create template:

```
curl -XPUT localhost:9200/_template/test?pretty -d'
{
  "template": "test_*",
  "settings": {
    "number_of_shards": 1,
    "index.mapper.dynamic": false
  },
  "mappings": {
    "foo": {
      "properties": {
        "name": {
          "type": "string"
        }
      }
    }
  }
}'
```

Index data that does not match the mapping:

```
curl -XPOST localhost:9200/test_1/bar?pretty -d'
{
    "abc": "def"
}'
```

Now, an index with an additional type "bar" has been created from the template, although dynamic mapping is set to false.

```
curl -XGET localhost:9200/test_1?pretty
{
  "test_1" : {
    "aliases" : { },
    "mappings" : {
      "foo" : {
        "properties" : {
          "name" : {
            "type" : "string"
          }
        }
      },
      "bar" : {
        "properties" : {
          "abc" : {
            "type" : "string"
          }
        }
      }
    },
    "settings" : {
      "index" : {
        "mapper" : {
          "dynamic" : "false"
        },
        "creation_date" : "1460026755596",
        "number_of_shards" : "1",
        "number_of_replicas" : "1",
        "uuid" : "EGWEI-iARECmY__EEa9jHg",
        "version" : {
          "created" : "2030199"
        }
      }
    },
    "warmers" : { }
  }
}
```
</description><key id="146588636">17592</key><summary>index.mapper.dynamic is not honored during auto-creation of an index from template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">tom-mi</reporter><labels><label>:Mapping</label><label>adoptme</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-04-07T11:32:16Z</created><updated>2016-07-19T07:03:46Z</updated><resolved>2016-07-19T07:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T16:06:03Z" id="206971465">Related to https://github.com/elastic/elasticsearch/issues/17561

The difference is that, in this example, `index.mapper.dynamic` is set at the index level rather than in the config file in #17561.
</comment><comment author="clintongormley" created="2016-04-07T16:34:48Z" id="206983315">Probably caused by https://github.com/elastic/elasticsearch/pull/15424
</comment><comment author="clintongormley" created="2016-05-05T08:03:11Z" id="217097498">Delete previous comment - wrong issue
</comment><comment author="clintongormley" created="2016-05-05T08:08:30Z" id="217099025">Turns out this has always worked this way, it isn't a regression but it is a bug and could be improved.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>30500m is safer than 31g to avoid long pointers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17591</link><project id="" key="" /><description>Changed maximum memory from 31g to 30500m to be sure to avoid long pointers
</description><key id="146575526">17591</key><summary>30500m is safer than 31g to avoid long pointers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robin13</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2016-04-07T10:34:38Z</created><updated>2016-04-12T19:09:46Z</updated><resolved>2016-04-12T19:09:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-07T11:37:06Z" id="206826901">Are you seeing a system (both OS and JVM version) where you're not able to get compressed oops with 31g as 31g should be _very_ safe?

On my laptop running OS X 10.11.4:

``` bash
$ JAVA_HOME=`/usr/libexec/java_home -v 1.7` java -Xmx31g -XX:+PrintFlagsFinal -version | grep UseCompressedOops
     bool UseCompressedOops                        := true            {lp64_product}      
java version "1.7.0_80"
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
```

On my workstation running Fedora 23:

``` bash
$ ./jdk1.7.0_79/bin/java -Xmx31g -XX:+PrintFlagsFinal -version | grep UseCompressedOops
     bool UseCompressedOops                        := true            {lp64_product}      
java version "1.7.0_79"
Java(TM) SE Runtime Environment (build 1.7.0_79-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)
```

In fact, you can take it _really_ close to the 32g boundary.

Again, on OS X 10.11.4:

``` bash
&#177; JAVA_HOME=`/usr/libexec/java_home -v 1.7` java -Xmx`echo "2^35 - 84 * 1024 * 1024" | bc` -XX:+PrintFlagsFinal -version | grep UseCompressedOops
     bool UseCompressedOops                        := true            {lp64_product}      
java version "1.7.0_80"
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
```

On Fedora 23:

``` bash
$ ./jdk1.7.0_79/bin/java -Xmx`echo "2^35 - 84 * 1024 * 1024" | bc` -XX:+PrintFlagsFinal -version | grep UseCompressedOops
     bool UseCompressedOops                        := true            {lp64_product}      
java version "1.7.0_79"
Java(TM) SE Runtime Environment (build 1.7.0_79-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)
```

Note that here I'm taking it up to 32g minus a small amount (the small amount has to do with leaving room for the null page, and the JVM making a very conservative estimate based on the OS virtual memory page size: [`max_heap_for_compressed_oops`](http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/87ee5ee27509/src/share/vm/runtime/arguments.cpp#l1407)).

See also: [A Heap of Trouble](https://www.elastic.co/blog/a-heap-of-trouble).
</comment><comment author="robin13" created="2016-04-11T13:31:49Z" id="208343888">I've been looking, but unfortunately cannot find... I remember about a year ago there was a lot of discussion, and _somebody_ managed to get long pointers at 31g, but 30.5g was considered safe.
Maybe it would be best to remove the hard number of 32, 31, 31,5 etc. from the configuration all together, and [reference that](https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html)

&gt; Beginning with Elasticsearch v2.2.0, the startup log will actually tell you if your JVM is using compressed OOPs or not.

With a note saying something like 

&gt; If you want to be really safe, set to max 30g, otherwise 32g should usually be ok, but check the startup messages (every time you startup) to be sure.
</comment><comment author="clintongormley" created="2016-04-12T19:09:45Z" id="209057808">Closing in favour of https://github.com/elastic/elasticsearch/pull/17675
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range or bucket defining pipeline aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17590</link><project id="" key="" /><description>Let's say the data set is transactions made by users and the question we want to ask is "Which users have transactions totalling between 100 - 200, 200 - 300, etc.?"

At the moment, there doesn't seem to be a way to do this in one query.

Given documents like this:

```
"user": user1,
"credit": 50
```

we can answer questions such as "Show me only the users that have transactions totalling more than 100" by using a `bucket_selector` like this:

```
"aggs": {
  "users": {
    "terms": {
      "field": "user"
    },
    "aggs": {
      "sum_credit": {
        "sum": {
          "field": "credit"
        }
      },
      "the_pipeline_agg": {
        "bucket_selector": {
          "buckets_path": {
            "sumCredit": "sum_credit"
          },
          "script": "sumCredit &gt; 100"
        }
      }
    }
  }
}
```

And now we could form ranges on the client side by running several of these queries in parallel.

But to answer the original question we would need some form of range pipeline aggregation like:

```
"bucket_range": {
  "buckets_path": {
    "sumCredit": "sum_credit"
  },
  "ranges": [
    { "to": 100 },
    { "from": 100, "to": 200 }
  ]
}
```

Or maybe even a more generic "bucket defining" pipeline agg, e.g.:

```
"bucket_definer": {
  "buckets_path": {
    "sumCredit": "sum_credit"
  },
  "bucket_script": "floor(sumCredit / 100)"
}
```

This would define automatically define buckets for 0-100, 100-200, and so on.
</description><key id="146560296">17590</key><summary>Range or bucket defining pipeline aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwurm</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>feature</label><label>high hanging fruit</label></labels><created>2016-04-07T09:30:43Z</created><updated>2017-04-03T23:26:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-08T09:36:16Z" id="207349634">We discussed this in FixItFriday and thought it could be quite a useful aggregation but we would want to implement it in a generic way (so it can be used to pivot on any aggregation rather than alway producing ranges). This will make the API possibly fairly tricky so will need some thinking about
</comment><comment author="phoenixgao" created="2016-08-17T08:30:31Z" id="240347513">Any updates on this topic?
</comment><comment author="hisuwh" created="2016-09-26T15:31:45Z" id="249605336">What is happening with this?  I really need something like this
</comment><comment author="yadnesh-aaxis" created="2017-04-03T23:26:49Z" id="291314396">We need this feature as well.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove min_doc_count option on histogram aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17589</link><project id="" key="" /><description>Always assume min_doc_count=0.
</description><key id="146551507">17589</key><summary>Remove min_doc_count option on histogram aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>discuss</label></labels><created>2016-04-07T08:54:57Z</created><updated>2016-04-08T14:44:14Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-08T09:37:23Z" id="207349864">@rashidkpc @spalger  Am I right in saying Kibana now uses `min_doc_count: 1`?
</comment><comment author="rashidkpc" created="2016-04-08T14:43:49Z" id="207459126">Kibana does use `min_doc_count: 1`, though I'm fine getting rid of this setting and switching back to min_doc_count: 0  (or rather, dropping `min_doc_count` altogether) as it lets us get rid of some 0 filling code. Just log a ticket in Kibana with a reference to this one when you do it so we know to re-introduce `extended_bounds`.

I don't honestly see much reason to dump this though. From what I understand from @colings86 we can't take advantage of certain caching strategies with `extended_bounds`. Getting rid of the ability to strip empty buckets from the middle of the set, thus reducing response size, while still requiring users to manually fill to the edges of their desired range feels odd? Maybe the caching thing can be fixed?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify ordering support on terms aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17588</link><project id="" key="" /><description>We try to be as flexible as possible when it comes to sorting terms aggregations. However, sorting by anything but by `_term` or descending `_count` makes it very hard to return the correct top buckets and counts, which is disappointing to users. Instead, I suggest that we only allow `order` options that result in
reasonably accurate results:
- remove the ability to sort by ascending count
- remove ordering by sub aggregations entirely, or only allow when the leaf of the path is a `min` or `max` aggregation: in that case counts will not be accurate but I believe that the top buckets will be correct.
</description><key id="146549470">17588</key><summary>Simplify ordering support on terms aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>breaking</label><label>discuss</label><label>stalled</label></labels><created>2016-04-07T08:47:24Z</created><updated>2017-02-13T12:40:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T10:22:41Z" id="206800118">Is this something we can possibly support with two-phase aggregations? https://github.com/elastic/elasticsearch/issues/12316
</comment><comment author="jpountz" created="2016-04-07T12:48:47Z" id="206876249">With multiple phases, we could get accurate results when sorting by min/max aggs (the first round could compute the top buckets and the second could ask all shards only for these values). This would also help refine counts when sorting by descending count since we can assume that shards have similar number of occurrences of each term. But we cannot make such assumptions for sub aggregations. For instance if you sort a terms aggregation by a sub avg aggregation, you could still get very different top terms for each shard eg. if the field that you compute the avg on has outliers.
</comment><comment author="rashidkpc" created="2016-04-07T21:07:19Z" id="207088455">Please open an issue on the kibana repo if and when you start moving on this as it will be a breaking change for many of our users.
</comment><comment author="colings86" created="2016-04-08T09:45:18Z" id="207352287">Discussed in FixItFriday and we agreed to split out removing the ascending count option from this issue so the conversation can be separated from removing support for sorting by sub aggregations (see https://github.com/elastic/elasticsearch/issues/17614)
</comment><comment author="CristianWeiland" created="2017-02-13T12:35:01Z" id="279377174">I had this problem (described in [#23108](https://github.com/elastic/elasticsearch/issues/23108#issuecomment-278943327)), and increasing _size_ of terms aggregation gives a more accurate result. Increasing _size_ to a value bigger than the number of documents seems to give the correct result.
Why does this happen?</comment><comment author="clintongormley" created="2017-02-13T12:40:19Z" id="279378779">@CristianWeiland you can read about it in the documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-approximate-counts</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ordering support on `histogram` aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17587</link><project id="" key="" /><description>I propose that we always return histograms sorted by ascending bucket. I suspect that ordering is very rarely used on histograms, and even it it were needed, it would be easy to either do it on client side since histograms always return all buckets, or to use a terms aggregation instead.
</description><key id="146543172">17587</key><summary>Remove ordering support on `histogram` aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>breaking</label><label>v6.0.0</label></labels><created>2016-04-07T08:16:47Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-08T09:43:18Z" id="207351509">We discussed it in FixitFriday and agreed to remove this feature.
</comment><comment author="colings86" created="2016-04-08T10:29:41Z" id="207365502">This will need to be removed in 6.0 and deprecated in 5.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update resliency page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17586</link><project id="" key="" /><description>#14252 , #7572 , #15900, #12573, #14671, #15281 and #9126 have all been closed/merged and will be part of 5.0.0.
</description><key id="146540174">17586</key><summary>Update resliency page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>docs</label></labels><created>2016-04-07T08:02:29Z</created><updated>2016-04-07T10:17:13Z</updated><resolved>2016-04-07T10:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T10:16:12Z" id="206798696">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClassNotFoundException: org.apache.log4j.PropertyConfigurator [2.3.1]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17585</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1
**JVM version**: 1.8
**OS version**: Ubuntu 14.04
**Description of the problem including expected versus actual behavior**:
We have a custom build of elaticsearch with embedded plugins, which we build as a single JAR (using scala &amp; sbt-assembly). Now, if we try to upgrade elasticsearch dependency from `2.2.0` to `2.3.1` we will get `java.lang.ClassNotFoundException: org.apache.log4j.PropertyConfigurator` on startup.

```
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/log4j/PropertyConfigurator
Likely root cause: java.lang.ClassNotFoundException: org.apache.log4j.PropertyConfigurator
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:128)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:243)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Everything works in `2.2.0` so I assume there is something fishy with the maven declarations?
</description><key id="146539601">17585</key><summary>ClassNotFoundException: org.apache.log4j.PropertyConfigurator [2.3.1]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Pyppe</reporter><labels /><created>2016-04-07T08:00:06Z</created><updated>2016-04-27T13:41:32Z</updated><resolved>2016-04-07T08:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-07T08:29:22Z" id="206758522">I believe that you have to include it explicitly as it's marked as optional: https://github.com/elastic/elasticsearch/blob/2.x/core/pom.xml#L166-L175
</comment><comment author="Pyppe" created="2016-04-07T08:53:16Z" id="206766346">Yeah, but why is it marked optional if you cannot to start the program without it?
</comment><comment author="dadoonet" created="2016-04-07T11:04:26Z" id="206817693">Because you can provide another implementation if you wish.
</comment><comment author="rmannibucau" created="2016-04-27T10:01:49Z" id="215037152">Hi got the same issue with 2.3.2,

https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java#L243 being called whatever config you do, https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java#L128 is triggered and load log4j so it is not optional so shouldn't be marked as such in the pom IMO.

Having a es.log flag == jul could allow to skip it and avoid this noclassdeffounderror.
</comment><comment author="nik9000" created="2016-04-27T13:41:32Z" id="215086455">As of 2.3.something log4j isn't optional in the server. In the client it should be. The problem is that the client shares the same pom and jar as the server. Both of those things are known bad but not actively being worked on other than the initiative to provide a REST based java client.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unsupported major.minor version 52.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17584</link><project id="" key="" /><description>I tried elasticsearch 5.0-alpha this morning (on Ubuntu 14.04) , both the deb package and the tar file give me a " Unsupported major.minor version 52.0" error.

I have the latest openjdk installed.

See Error , os and java below.

root@sin-cpt-sl-es-development:~/elasticsearch-5.0.0-alpha1# lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.4 LTS
Release:    14.04
Codename:   trusty
root@sin-cpt-sl-es-development:~/elasticsearch-5.0.0-alpha1# ./bin/elasticsearch
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 52.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:803)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)
root@sin-cpt-sl-es-development:~/elasticsearch-5.0.0-alpha1# java -version
java version "1.7.0_95"
OpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-0ubuntu0.14.04.2)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)
root@sin-cpt-sl-es-development:~/elasticsearch-5.0.0-alpha1# 
</description><key id="146533536">17584</key><summary>Unsupported major.minor version 52.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pagenbag</reporter><labels /><created>2016-04-07T07:31:31Z</created><updated>2017-06-08T23:11:36Z</updated><resolved>2016-04-07T07:44:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-07T07:44:02Z" id="206742088">Thanks for trying 5.0 aplha1. ES 5.0 will require Java8 as a minimum. See "https://www.elastic.co/guide/en/elasticsearch/reference/master/_installation.html#_installation"

I wish the error message was clearer, but as it comes from the JVM I don't think we have control over it. 
</comment><comment author="pagenbag" created="2016-04-07T08:00:47Z" id="206748484">Ah , thanx - I saw a similar issue on a previous version , so I suspected java. But since I did update to the latest (available) version I thought maybe it was something else. 

Turns out ubuntu 14.04 just doesnt have openjdk 8 in the repos - so installed from ppa and now everything seems to be working.
</comment><comment author="danm" created="2016-08-14T00:56:08Z" id="239649810">@bleskes can we add Java8 requirement to this doc https://www.elastic.co/guide/en/elasticsearch/reference/master/rpm.html
to be consistent to other v5 docs such as logstash and help this these issues?
</comment><comment author="clintongormley" created="2016-08-16T13:32:48Z" id="240102375">@calbatron I've opened #20005
</comment><comment author="abhiphanse" created="2016-09-29T08:12:46Z" id="250400056">Just install java 8.
</comment><comment author="Tomket" created="2017-02-06T17:52:15Z" id="277759184">C:\elasticsearch-5.2.0\bin&gt;elasticsearch.bat
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 52.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:482)

C:\elasticsearch-5.2.0\bin&gt;java -version
java version "1.8.0_121"
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)

C:\elasticsearch-5.2.0\bin&gt;</comment><comment author="LaszloHont" created="2017-02-10T08:46:29Z" id="278888886">@bleskes Shouldn't the upgrade checker plugin check this?</comment><comment author="bleskes" created="2017-02-10T10:31:10Z" id="278910173">@LaszloHont I think it should but it doesn't now. I opened https://github.com/elastic/elasticsearch-migration/issues/102</comment><comment author="clintongormley" created="2017-02-10T11:14:57Z" id="278918929">[v2.0.4](https://github.com/elastic/elasticsearch-migration/releases/tag/v2.0.4) of the migration plugin checks the JVM version now</comment><comment author="DariosP" created="2017-03-16T07:15:53Z" id="286976191">In Ubuntu use in terminal commands:
sudo add-apt-repository ppa:webupd8team/java
sudo apt update
sudo apt install oracle-java9-installer</comment><comment author="eaglefinds" created="2017-03-17T04:13:40Z" id="287262500">When I set JAVA_HOME  to jdk 1.8 in command prompt it worked without any issue. Thanks for your posting.</comment><comment author="swapnilgangrade01" created="2017-03-17T07:44:23Z" id="287288259">Elasticsearch will not pick java from path environment variable in windows. It check for JAVA_HOME set up in Environment Variable , Please make sure to put correct java for example using 5.2.2 version setup JAVA_HOME as java 8 JDK/JRE.</comment><comment author="shahkaushal94" created="2017-06-08T22:04:48Z" id="307240360">I am following the documentation of ES (https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-index.html). I am getting the following error:

Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/common/transport/TransportAddress : Unsupported major.minor version 52.0
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)
	at java.lang.Class.getMethod0(Class.java:2774)
	at java.lang.Class.getMethod(Class.java:1663)
	at sun.launcher.LauncherHelper.getMainMethod(LauncherHelper.java:494)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:486)

Here is a part of screenshot of my config of eclipse:
java.awt.graphicsenv=sun.awt.CGraphicsEnvironment
java.awt.printerjob=sun.lwawt.macosx.CPrinterJob
java.class.path=/Users/JohnDoe/eclipse/java-neon/Eclipse.app/Contents/MacOS//../Eclipse/plugins/org.eclipse.equinox.launcher_1.3.201.v20161025-1711.jar
java.class.version=52.0
java.endorsed.dirs=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/endorsed
java.ext.dirs=/Users/JohnDoe/Library/Java/Extensions:/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/ext:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java
java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre


I have spent a lot of time trying to get the version of JAVA used. From the above config file, it seems it is Java 8. How do I resolve this issue?</comment><comment author="dadoonet" created="2017-06-08T22:24:38Z" id="307243990">@shahkaushal94 feel free to ask on discuss.elastic.co where we can give better support.
Here, it's clear that you still have a java 1.7 version somewhere.

Check your `JAVA_HOME` and what gives `java -version`.</comment><comment author="shahkaushal94" created="2017-06-08T22:27:58Z" id="307244598">$java -version
java version "1.8.0_131"
Java(TM) SE Runtime Environment (build 1.8.0_131-b11)
Java HotSpot(TM) 64-Bit Server VM (build 25.131-b11, mixed mode)

$ echo $JAVA_HOME
/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home

Sure, I'll ask questions on discuss.elastic.co. Sorry.
But why is my java version showing 1.8 then?</comment><comment author="dadoonet" created="2017-06-08T22:33:23Z" id="307245604">You can see that something is inconsistent here:

&gt; java version "1.8.0_**131**"

and

&gt; /Library/Java/JavaVirtualMachines/jdk1.8.0_**45**.jdk/Contents/Home

</comment><comment author="shahkaushal94" created="2017-06-08T23:11:36Z" id="307252036">Thanks! I did a complete reinstall of java and it worked. I have more questions! Will post on discuss.elastic.co</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test failure in FunctionScoreQueryBuilderTests.testMustRewrite</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17583</link><project id="" key="" /><description>Build URL:

```
http://build-us-00.elastic.co/job/es_g1gc_master_metal/35311
```

Reproduce command:

```
gradle :core:test -Dtests.seed=35D37728EEA24403 -Dtests.class=org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilderTests -Dtests.method="testMustRewrite" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=he-IL -Dtests.timezone=America/Caracas
```

Failure:

```
java.lang.IllegalArgumentException: function_score : scale must be &gt; 0.0.
    at __randomizedtesting.SeedInfo.seed([35D37728EEA24403:813C041596F54283]:0)
    at org.elasticsearch.index.query.functionscore.DecayFunctionBuilder$AbstractDistanceScoreFunction.&lt;init&gt;(DecayFunctionBuilder.java:504)
    at org.elasticsearch.index.query.functionscore.DecayFunctionBuilder$NumericFieldDataScoreFunction.&lt;init&gt;(DecayFunctionBuilder.java:415)
    at org.elasticsearch.index.query.functionscore.DecayFunctionBuilder.parseDateVariable(DecayFunctionBuilder.java:321)
    at org.elasticsearch.index.query.functionscore.DecayFunctionBuilder.parseVariable(DecayFunctionBuilder.java:202)
    at org.elasticsearch.index.query.functionscore.DecayFunctionBuilder.doToFunction(DecayFunctionBuilder.java:183)
    at org.elasticsearch.index.query.functionscore.ScoreFunctionBuilder.toFunction(ScoreFunctionBuilder.java:137)
    at org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder.doToQuery(FunctionScoreQueryBuilder.java:327)
    at org.elasticsearch.index.query.AbstractQueryBuilder.toQuery(AbstractQueryBuilder.java:78)
    at org.elasticsearch.index.query.AbstractQueryTestCase.testMustRewrite(AbstractQueryTestCase.java:981)
    at org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilderTests.testMustRewrite(FunctionScoreQueryBuilderTests.java:611)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)

```
</description><key id="146533267">17583</key><summary>Test failure in FunctionScoreQueryBuilderTests.testMustRewrite</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label></labels><created>2016-04-07T07:29:48Z</created><updated>2016-09-16T19:18:46Z</updated><resolved>2016-09-16T19:18:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-07T07:30:59Z" id="206737850">@nik9000 could this be due to your changes yesterday?
</comment><comment author="javanna" created="2016-09-16T19:18:40Z" id="247684527">I believe this was fixed by https://github.com/elastic/elasticsearch/commit/cd982ade99ce2db939b7ed9278ba199010641131 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex and Update by Query tests fail due to in flight contexts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17582</link><project id="" key="" /><description>Build URL: 
http://build-us-00.elastic.co/job/es_core_master_window-2008/3478/

Failed tests:
org.elasticsearch.index.reindex.BulkByScrollTaskTests.testDelayAndRethrottle
org.elasticsearch.index.reindex.BulkByScrollTaskTests.testBasicData
org.elasticsearch.index.reindex.BulkByScrollTaskTests.testProgress
org.elasticsearch.index.reindex.BulkByScrollTaskTests.testStatusHatesNegatives
org.elasticsearch.index.reindex.ReindexFailureTests.testResponseOnSearchFailure
org.elasticsearch.index.reindex.ReindexFailureTests.testFailuresCauseAbortDefault
org.elasticsearch.index.reindex.ReindexFailureTests.testSettingTimestampIsValidationFailure
org.elasticsearch.index.reindex.ReindexFailureTests.testAbortOnVersionConflict
org.elasticsearch.index.reindex.UpdateByQueryMetadataTests.testTTL
org.elasticsearch.index.reindex.UpdateByQueryMetadataTests.testTimestampIsCopied
org.elasticsearch.index.reindex.UpdateByQueryMetadataTests.testRoutingIsCopied
org.elasticsearch.index.reindex.UpdateByQueryWithScriptTests.testChangeSource
org.elasticsearch.index.reindex.UpdateByQueryWithScriptTests.testScriptAddingJunkToCtxIsError
org.elasticsearch.index.reindex.UpdateByQueryWithScriptTests.testModifyingCtxNotAllowed
</description><key id="146531862">17582</key><summary>Reindex and Update by Query tests fail due to in flight contexts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label><label>stalled</label></labels><created>2016-04-07T07:24:03Z</created><updated>2016-07-29T19:54:34Z</updated><resolved>2016-07-29T19:54:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-07T07:24:16Z" id="206734608">@nik9000 would you be able to take a look at these?
</comment><comment author="nik9000" created="2016-04-08T01:13:46Z" id="207159371">I'm tired and this hasn't reproduced for that I've seen so I'm probably going to leave this for the morning.
</comment><comment author="nik9000" created="2016-04-08T13:40:27Z" id="207435645">Reproduces with `gradle :modules:reindex:test -Dtests.seed=5A143CB0A60E2250`. You can't run any of the individual reproduction lines though.
</comment><comment author="nik9000" created="2016-04-08T13:53:30Z" id="207441141">Er - I lied. It doesn't reproduce. But if I add a delay in the right spot I can get it to reproduce.
</comment><comment author="nik9000" created="2016-04-08T16:36:42Z" id="207506177">I've opened a PR to get more information the next time this happens but I can't reproduce locally.
</comment><comment author="nik9000" created="2016-04-08T16:37:04Z" id="207506281">I think the right thing is to close this and wait until it happens again on CI?
</comment><comment author="colings86" created="2016-04-11T10:05:06Z" id="208265862">I think we should add information to debug this and wait for it to happen again (as you have done, thanks :) ) but I don;t think it's right to close this issue. This was a valid test failure and hasn't been fixed so IMO the issue should remain open. We should then add further information to this issue when it fails again on CI.
</comment><comment author="nik9000" created="2016-04-11T12:31:35Z" id="208317424">Reopening but marking stalled as we're waiting for another failure of this kind so we can use the extra information we added to the failure message to debug it.
</comment><comment author="nik9000" created="2016-07-29T19:23:24Z" id="236271159">I think we tracked this one down a while back. Closing. If we see it again we can reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No correct phrase highlighting with plain highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17581</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.2

**JVM version**: Java(TM) SE Runtime Environment (build 1.8.0_40-b27)

**OS version**: OSX 10.11.3

**Description of the problem including expected versus actual behavior**:
The plain highlighter highlights single terms of phases that do not completely match. My expected behavior would be that a given search phrase must completely match, before it is used for highlighting.

**Steps to reproduce**:
1. Create a new document
   `PUT sample/doc/1
   {
   "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed sed hendrerit ante." 
   }`
2. Do a query_string query with a quoted phrase
   `GET sample/doc/_search
   {
   "query": {
    "filtered": {
      "query": {
        "query_string": {
          "default_field": "content",
          "query": "\"lorem something\" OR ipsum" 
        }
      }
   }
   },
   "highlight": {
    "fields": {
      "content": {}
   }
   }
   }`
   _lorem_ gets highlighted, but I do not expect that since the phrase _lorem something_ does not exist in the document
3. Do an equivalent bool query
   `GET sample/doc/_search
   {
   "query": {
    "bool": {
      "should": [
        {
          "match_phrase": {
            "content": "lorem something" 
          }
        },
        {
          "match": {
            "content": "ipsum" 
          }
        }
      ]
   }
   },
   "highlight": {
    "fields": {
      "content": {}
   }
   }
   }`
   Same result.
4. Switch the elements of the bool query
   `GET sample/doc/_search
   {
   "query": {
    "bool": {
      "should": [
         {
          "match": {
            "content": "ipsum" 
          }
        },
        {
          "match_phrase": {
            "content": "lorem something" 
          }
        }
      ]
   }
   },
   "highlight": {
    "fields": {
      "content": {}
   }
   }
   }`

Whats that? Just _ipsum_ gets highlighted (as expected). That seems like a bug to be. I did not try to reproduce that with the other highlighters.
</description><key id="146516629">17581</key><summary>No correct phrase highlighting with plain highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">schoch</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2016-04-07T05:57:40Z</created><updated>2016-07-11T07:30:39Z</updated><resolved>2016-04-12T18:30:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-04-11T08:23:33Z" id="208223846">The bug is in Lucene and has been fixed for the 6.x series here:
https://issues.apache.org/jira/browse/LUCENE-7112

@jpountz should we backport the fix for Elasticsearch 2.x ?
</comment><comment author="clintongormley" created="2016-04-12T18:30:24Z" id="209043592">thanks for looking @jimferenczi.  Let's not backport - the fix is already in 5.0
</comment><comment author="havran" created="2016-07-11T07:30:38Z" id="231661053">Seems this bug is fixed in Lucene 5.5.1 but Elasticsearch 2.x still use Lucene 5.5.0. No plans for using Lucene 5.5.1?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue after upgrading ES from 1.x to 2.x </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17580</link><project id="" key="" /><description>After upgrading ES version from 1.7 to 2.2.0 we have start receiving errors.Please see below log for the same.

`we use virtual environment with EMC storage`

We have three nodes all as master=true and data=true.

Here is our configuration

```
cluster.name: cluster1
node.name: "node01
node.master: true
node.data: true
index.query.bool.max_clause_count: 50100
indices.fielddata.cache.size: 40%
action.disable_delete_all_indices: true
indices.cluster.send_refresh_mapping: false
index.cache.field.type: soft
indices.breaker.fielddata.limit: 60%
path.data: E:/Share/Elastic_Search/data
path.logs: \\PC1\Elasticsearch\Logs\node01
bootstrap.mlockall: true
network.host: node01
indices.recovery.max_bytes_per_sec: 100mb
indices.recovery.concurrent_streams: 5
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: [node01,node02,node03]
```

And log messages

```
[2016-04-06 12:01:52,496][DEBUG][action.admin.indices.stats] [node01] [indices:monitor/stats] failed to execute operation for shard [[Products][3], node[L_mbnAHGRHiwIyi3of0GKw], [P], v[209], s[STARTED], a[id=FOWy8JKfRWeiFLmhvX05bQ]]
ElasticsearchException[failed to refresh store stats]; nested: AccessDeniedException[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\Products\3\index\_9zmi_Lucene54_0.dvd];
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1534)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1519)
  at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
  at org.elasticsearch.index.store.Store.stats(Store.java:293)
  at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:665)
  at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
  at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
  at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:409)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:388)
  at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:375)
  at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.AccessDeniedException: E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\Products\3\index\_9zmi_Lucene54_0.dvd
  at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileSystemProvider.readAttributes(Unknown Source)
  at java.nio.file.Files.readAttributes(Unknown Source)
  at java.nio.file.Files.size(Unknown Source)
  at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:209)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1543)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1532)
  ... 15 more
[2016-04-06 12:30:41,867][DEBUG][action.admin.cluster.node.stats] [node01] failed to execute on node [L_mbnAHGRHiwIyi3of0GKw]
RemoteTransportException[[node01][node01/10.250.200.104:9300][cluster:monitor/nodes/stats[n]]]; nested: ElasticsearchException[failed to refresh store stats]; nested: AccessDeniedException[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\Products\1\index\_a0it.nvd];
Caused by: ElasticsearchException[failed to refresh store stats]; nested: AccessDeniedException[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\Products\1\index\_a0it.nvd];
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1534)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1519)
  at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
  at org.elasticsearch.index.store.Store.stats(Store.java:293)
  at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:665)
  at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
  at org.elasticsearch.indices.IndicesService.stats(IndicesService.java:253)
  at org.elasticsearch.node.service.NodeService.stats(NodeService.java:157)
  at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:82)
  at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
  at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
  at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
  at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.AccessDeniedException: E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\Products\1\index\_a0it.nvd
  at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileSystemProvider.readAttributes(Unknown Source)
  at java.nio.file.Files.readAttributes(Unknown Source)
  at java.nio.file.Files.size(Unknown Source)
  at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:209)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1543)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1532)
  ... 16 more
[2016-04-06 13:27:45,837][DEBUG][action.admin.cluster.node.stats] [node01] failed to execute on node [L_mbnAHGRHiwIyi3of0GKw]
RemoteTransportException[[node01][node01/10.250.200.104:9300][cluster:monitor/nodes/stats[n]]]; nested: ElasticsearchException[failed to refresh store stats]; nested: AccessDeniedException[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\anthem_review\0\index\_t7.fnm];
Caused by: ElasticsearchException[failed to refresh store stats]; nested: AccessDeniedException[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\anthem_review\0\index\_t7.fnm];
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1534)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1519)
  at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
  at org.elasticsearch.index.store.Store.stats(Store.java:293)
  at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:665)
  at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
  at org.elasticsearch.indices.IndicesService.stats(IndicesService.java:253)
  at org.elasticsearch.node.service.NodeService.stats(NodeService.java:157)
  at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:82)
  at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
  at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
  at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
  at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.AccessDeniedException: E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\anthem_review\0\index\_t7.fnm
  at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileSystemProvider.readAttributes(Unknown Source)
  at java.nio.file.Files.readAttributes(Unknown Source)
  at java.nio.file.Files.size(Unknown Source)
  at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:209)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1543)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1532)
  ... 16 more
[2016-04-06 18:26:11,254][DEBUG][action.admin.cluster.node.stats] [node01] failed to execute on node [BR9PT4QVRuy7VqE5iTlJLQ]
RemoteTransportException[[LDESMETA03][10.250.200.29:9300][cluster:monitor/nodes/stats[n]]]; nested: ElasticsearchException[failed to refresh store stats]; nested: NotSerializableExceptionWrapper[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\mma_review\0\index\_rbj.cfe];
Caused by: ElasticsearchException[failed to refresh store stats]; nested: NotSerializableExceptionWrapper[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\mma_review\0\index\_rbj.cfe];
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1534)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1519)
  at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
  at org.elasticsearch.index.store.Store.stats(Store.java:293)
  at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:665)
  at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
  at org.elasticsearch.indices.IndicesService.stats(IndicesService.java:253)
  at org.elasticsearch.node.service.NodeService.stats(NodeService.java:157)
  at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:82)
  at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:44)
  at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
  at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
  at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
  at java.lang.Thread.run(Unknown Source)
Caused by: NotSerializableExceptionWrapper[E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\mma_review\0\index\_rbj.cfe]
  at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
  at sun.nio.fs.WindowsFileSystemProvider.readAttributes(Unknown Source)
  at java.nio.file.Files.readAttributes(Unknown Source)
  at java.nio.file.Files.size(Unknown Source)
  at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:209)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
  at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1543)
  at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1532)
  ... 16 more
```
</description><key id="146513260">17580</key><summary>Issue after upgrading ES from 1.x to 2.x </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2016-04-07T05:28:00Z</created><updated>2017-03-07T06:26:09Z</updated><resolved>2016-10-07T09:24:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T10:07:01Z" id="206795244">Try changing your `path.data` to:

```
path.data: E:\Share\Elastic_Search\data
```
</comment><comment author="dobariya" created="2016-04-07T10:21:17Z" id="206799844">yes before i had the same but after reading some same issues online, found to change the path from backward slash to forward slash which i did.But still its not working.
</comment><comment author="clintongormley" created="2016-04-07T10:29:42Z" id="206801988">Are you sure that path is accessible by the user running Elasticsearch, and that the file mentioned actually exists and has a non-zero size?
</comment><comment author="dobariya" created="2016-04-07T10:33:15Z" id="206802758">yes its accessible and on that path we have almost 500 indices so it is non-zero
</comment><comment author="clintongormley" created="2016-04-07T10:45:21Z" id="206805817">@Mpdreamz do you have any ideas?
</comment><comment author="Mpdreamz" created="2016-04-07T11:47:02Z" id="206829905">I don't think its a `path.data` configuration issue, permissions of  that are checked on startup and should prevent the node from starting. 

It might miss a specific extended attributes permission file trying to get the `size()` of that file or the attached EMC drive might not like that. Is `E:\Share` a network share? Does EMC use SMB(2) or CIFS?

Whats the output of calling `icacls E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\mma_review\0\index\_rbj.cfe]` look like?

Are you running Elasticsearch as a service and if so under what account?
</comment><comment author="dobariya" created="2016-04-07T12:18:48Z" id="206847579">we are using virtual machine so `E:\Share` is a local drive of that machine.
`EMC use SMB(2)`
There is no file available  like `E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\mma_review\0\index\_rbj.cfe`.
No we are not running ES as a service but directly from bin/elasticsearch.bat under administrator account
</comment><comment author="Mpdreamz" created="2016-04-07T13:04:47Z" id="206887749">This is a wild guess but you could be hit by https://support.microsoft.com/en-us/kb/3035936 possibly?

can you give me the icacls output of any of the parent folders that does exist?
</comment><comment author="dobariya" created="2016-04-07T13:17:55Z" id="206896005">E:\Share\Elastic_Search\data\cluster1\nodes\0\indices\mma_review

`BUILTIN\Administrators:(F)
 BUILTIN\Administrators:(I)(OI)(CI)(F)
 NT AUTHORITY\SYSTEM:(I)(OI)(CI)(F)
 CREATOR OWNER:(I)(OI)(CI)(IO)(F)
 BUILTIN\Users:(I)(OI)(CI)(RX)
 BUILTIN\Users:(I)(CI)(AD)
 BUILTIN\Users:(I)(CI)(WD)`

Successfully processed 1 files; Failed processing 0 files
</comment><comment author="dobariya" created="2016-04-08T03:25:40Z" id="207194831">FYI,
First we were using EMC storage but after installing elastic hq we found some slow IO errors in HQ so we moved ES to [pure storage](https://www.purestorage.com/) which is blazing fast.Even after moving to pure storage we are receiving these errors.So is pure storage have anything to do with smb2?
</comment><comment author="TinLe" created="2016-04-08T03:37:24Z" id="207196557">FYI.  I am testing pure storage, but using iSCSI, and I do not see these errors.   The difference is that I am using Linux RHEL 6.6.   So this look like a Windows specific bug.
</comment><comment author="dobariya" created="2016-04-08T04:38:14Z" id="207209642">So do we have to try this [https://support.microsoft.com/en-us/kb/3035936] for pure storage as well?
</comment><comment author="dobariya" created="2016-04-19T03:58:03Z" id="211718910">We even tried above Microsoft solution but still it says AccessDeniedException on store refresh.
Please look into this.
</comment><comment author="dobariya" created="2016-04-27T10:48:12Z" id="215046728">Please provide me updates on this
</comment><comment author="daveslee" created="2016-09-25T20:42:25Z" id="249445417">Was this ever resolved? We're seeing a similar issue after upgrading from 1.x to 2.x with ES running on Windows.
</comment><comment author="Mpdreamz" created="2016-09-26T06:08:00Z" id="249486606">@daveslee it was not, can you provide some more details about what kind of disks and filesystem you are using? Shared not shared? And if so over what protocol. Also the output of the afformentioned `icacls` would be useful. 

Cc @elastic/microsoft 
</comment><comment author="sss13579" created="2016-09-29T07:20:45Z" id="250390139">have the same issue.
Elastic 2.3.3 on Windows Server 2012 R2. java 1.8.0_91. 3 master nodes (2 of  them with data). ssd hdd. 
there are several errors per day:

ElasticsearchException[failed to refresh store stats]; nested: AccessDeniedException[E:_db\elasticsearch-shorteners\nodes\0\indices\shtype-s-160928\1\index_k98.cfe];
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1532)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1517)
    at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
    at org.elasticsearch.index.store.Store.stats(Store.java:293)
    at org.elasticsearch.index.shard.IndexShard.storeStats(IndexShard.java:702)
    at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:134)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:165)
    at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:47)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.onShardOperation(TransportBroadcastByNodeAction.java:420)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:399)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.AccessDeniedException: E:_db\elasticsearch-shorteners\nodes\0\indices\shtype-s-160928\1\index_k98.cfe
    at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
    at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
    at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
    at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
    at sun.nio.fs.WindowsFileAttributeViews$Basic.readAttributes(Unknown Source)
    at sun.nio.fs.WindowsFileSystemProvider.readAttributes(Unknown Source)
    at java.nio.file.Files.readAttributes(Unknown Source)
    at java.nio.file.Files.size(Unknown Source)
    at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:210)
    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
    at org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:67)
    at org.elasticsearch.index.store.Store$StoreStatsCache.estimateSize(Store.java:1541)
    at org.elasticsearch.index.store.Store$StoreStatsCache.refresh(Store.java:1530)
    ... 17 more
</comment><comment author="s1monw" created="2016-09-29T07:29:46Z" id="250391790">I don't see how this is an Elasticsearch problem. `java.nio.file.AccessDeniedException` is not coming form anything we do. It's NOT an security manager exception. The filesystem doesn't allow to access this file. Somebody or something must be changing permissions on that file while ES is running. I don't think we can do anything about this? @clintongormley @Mpdreamz can we close this?
</comment><comment author="dobariya" created="2016-09-29T07:35:17Z" id="250392794">@s1monw : I disagree with you because there was no issue in ES1.x and it started on ES2.x. Our OS is same for both version.
</comment><comment author="sss13579" created="2016-09-29T07:47:21Z" id="250395078">&gt; "Somebody or something must be changing permissions on that file while ES is running"

then return it back?
I understand your opinion, but it's strange, that we have sporadic errors. Before them and after there are no errors with these files. cluster is ok, node is ok, data is consistent.
It looks like something locks the file for a moment.
</comment><comment author="ashitpupu" created="2017-03-07T00:01:24Z" id="284575556">Is there any solution to this issue ?? I am encountering the same issue don't know how to proceed.
If any solution please suggest . Using ES 2.3.3 , error started to happen once I have updated my tokenizer to Ngram tokenizer. 
Please let me know if any solution available.</comment><comment author="sss13579" created="2017-03-07T06:26:09Z" id="284633768">It has been fixed in version 2.4.2</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Modified scrolling docs to match with the REST API docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17579</link><project id="" key="" /><description>Since `SearchType.SCAN` has been deprecated, the Java API docs needs to be modified to reflect that.
Closes #17555
</description><key id="146509778">17579</key><summary>Java API: Modified scrolling docs to match with the REST API docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">consulthys</reporter><labels><label>:Java API</label><label>docs</label></labels><created>2016-04-07T04:56:44Z</created><updated>2016-04-07T08:42:15Z</updated><resolved>2016-04-07T08:23:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-07T05:17:31Z" id="206701005">That's great! Can you sign the CLA please?
</comment><comment author="consulthys" created="2016-04-07T05:31:37Z" id="206704309">As a matter of fact, I already did sign the CLA yesterday and got the document back via email.
Do I need to do anything else?
</comment><comment author="dadoonet" created="2016-04-07T06:55:49Z" id="206722700">Did you sign the CLA with the same email address you used for your commits?
I can't see your name or github account in our CLA checker.
</comment><comment author="dadoonet" created="2016-04-07T06:56:59Z" id="206723163">Could you send me your email address at david at elastic dot co so I can better check that?
May be we have a synchronisation issue on our end though...
</comment><comment author="dadoonet" created="2016-04-07T08:36:59Z" id="206760464">Thank you! I also pushed it in `2.x` and `2.3` branches
</comment><comment author="consulthys" created="2016-04-07T08:41:13Z" id="206761624">Nice, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incomplete results when using geo_distance for large distances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17578</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.2, 2.3.1, 5.0.0-alpha1

**JVM version**: 1.7.0_95 (Ubuntu 14.04), 1.8.0_77 (Windows 8.1)

**Description of the problem including expected versus actual behavior**:
Starting with version 2.2.2, using the _geo_distance_ query does not appear to be accurate when using large distance values. In the included reproduction, documents are added that are placed from 1 to 10,000 miles away from the point at _47.60621, -122.33207_. Executing a query for matches up to 10,000 miles away only returns the documents from 1 to 5,000 miles away.

If I profile the query, it appears that somewhere around a 6,000mi distance, the bounding box generated by the GeoPointDistanceQueryImpl query maxes out at Lower Left: [-180.0,-37.181370025500655] Upper Right: [180.0,90.0]. It also appears that the max distance in the _geo_distance_ query becomes less the closer to a pole that you get.

Running the same test with 2.1.2 returns all expected results up to 10,000 miles away.

``` bash
curl -XDELETE localhost:9200/distance-test

curl -XPUT localhost:9200/distance-test -d '{
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "index.queries.cache.type": "none"
    }
}'

curl -XPUT localhost:9200/distance-test/document/_mapping -d '{
    "properties": {
        "name": {
            "type": "string"
        },
        "location": {
            "type": "geo_point",
            "lat_lon": true
        }
    }
}'

curl -XPUT localhost:9200/distance-test/document/_bulk -d '{ "index": { "_id": 1 } }
{ "name": "1 mile", "location": "47.606208, -122.310628" }
{ "index": { "_id": 2 } }
{ "name": "10 miles", "location": "47.60601, -122.117647" }
{ "index": { "_id": 3 } }
{ "name": "100 miles", "location": "47.586236, -120.188375" }
{ "index": { "_id": 4 } }
{ "name": "1000 miles", "location": "45.654944, -101.405702" }
{ "index": { "_id": 5 } }
{ "name": "2000 miles", "location": "40.275895, -83.006558" }
{ "index": { "_id": 6 } }
{ "name": "3000 miles", "location": "32.469747, -67.847419" }
{ "index": { "_id": 7 } }
{ "name": "4000 miles", "location": "23.156323, -55.315033" }
{ "index": { "_id": 8 } }
{ "name": "5000 miles", "location": "12.986677, -44.486196" }
{ "index": { "_id": 9 } }
{ "name": "6000 miles", "location": "2.40577, -34.530192" }
{ "index": { "_id": 10 } }
{ "name": "7000 miles", "location": "-8.24639, -24.729009" }
{ "index": { "_id": 11 } }
{ "name": "8000 miles", "location": "-18.648019, -14.38816" }
{ "index": { "_id": 12 } }
{ "name": "9000 miles", "location": "-28.413215, -2.7355" }
{ "index": { "_id": 13 } }
{ "name": "10000 miles", "location": "-36.996556, 11.128372" }
'
```

Search query:

``` http
GET localhost:9200/distance-test/document/_search 
{
    "size": 0,
    "from": 0,
    "profile": true,
    "_source": ["name"],
    "query": {
        "geo_distance": {
            "distance": "10000mi",
            "location": "47.60621, -122.33207"
        }
    },
    "sort": [{
        "_geo_distance": {
            "location": "47.60621, -122.33207",
            "order": "desc",
            "unit": "mi"
        }
    }]
}
```
</description><key id="146475965">17578</key><summary>Incomplete results when using geo_distance for large distances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">jweber</reporter><labels><label>:Geo</label><label>bug</label><label>v5.0.0-alpha5</label></labels><created>2016-04-07T00:43:41Z</created><updated>2017-05-09T08:16:12Z</updated><resolved>2016-07-08T17:15:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T09:57:04Z" id="206792132">@nknize please could you take a look?
</comment><comment author="jweber" created="2016-05-03T17:10:00Z" id="216598389">I just tried this out in 5.0.0-alpha2 and it's still showing the same behavior.
</comment><comment author="nknize" created="2016-05-04T13:36:03Z" id="216866894">&gt; it appears that somewhere around a 6,000mi distance, the bounding box generated by the GeoPointDistanceQueryImpl query maxes out

hmmm... This sounds like could be a units issue with `maxRadialDistance`?  I'll try to reproduce and see what's going on.
</comment><comment author="jweber" created="2016-05-04T14:57:24Z" id="216891474">If I shift the query origin further north then the max distance become less before the bounding box stops growing. For example, when using location _71.284992,-156.9219567_ (Barrow, AK) the bounding box stops changing at a query distance of around 2,600 miles.

I'm also assuming that the _Lower Left: [x,y], Upper Right: [x,y]_ being reported by the GeoPointDistanceQueryImpl lucene query when running the query profiler is the bounding box. Hopefully I'm not wrong on that!
</comment><comment author="nknize" created="2016-05-04T15:32:11Z" id="216903254">Ah. This was a radius restriction bug that was fixed in https://issues.apache.org/jira/browse/LUCENE-7137.
</comment><comment author="jweber" created="2016-05-04T16:57:05Z" id="216931091">@nknize: Any chance of that fix getting applied to a 2.3.x release?
</comment><comment author="nknize" created="2016-05-04T21:56:59Z" id="217016001">Possibly 2.3.x, that might be a stretch. At minimum it should really make 5.0 but as it stands right now the fix isn't available until Lucene 6.1 which means 5.x. I'll dig and keep this issue updated.
</comment><comment author="jweber" created="2016-07-05T15:10:13Z" id="230507198">@nknize I noticed that alpha4 is using Lucene 6.1 so I re-ran this test. It's still exhibiting the same unexpected behavior.
</comment><comment author="nknize" created="2016-07-08T17:15:22Z" id="231418073">@jweber I still had the radial restriction in ES. I just pushed a fix that will be in the next release.
</comment><comment author="bubchi89" created="2016-08-21T00:48:52Z" id="241232317">Do you guys have any suggestions for a workaround for this in 2.3.x? Or is the best we can do is validate the user's query before passing it along to ES? Does it sound reasonable to validate that GeoUtils.maxRadialDistance() doesn't shrink the input distance?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in templates.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17577</link><project id="" key="" /><description>The doc mentions match_path in one place but the correct syntax is path_match which is mentioned everywhere else. Using the wrong string leads to errors because the mapping becomes too greedy, and matches things it shouldn't.
</description><key id="146451916">17577</key><summary>Fix typo in templates.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ibrahima</reporter><labels><label>docs</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T22:22:57Z</created><updated>2016-04-06T22:44:39Z</updated><resolved>2016-04-06T22:39:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T22:37:01Z" id="206598670">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="dakrone" created="2016-04-06T22:37:23Z" id="206598743">Oh weird, it just updated and looks like you already did :)
</comment><comment author="dakrone" created="2016-04-06T22:40:53Z" id="206599573">Merged, thanks @ibrahima!
</comment><comment author="ibrahima" created="2016-04-06T22:44:39Z" id="206600666">No problem! Glad to help out others.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Update instructions for running on Windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17576</link><project id="" key="" /><description>- Updated the customizing service section and listed all the available environment variables for running as a service (including new ones added in #17312)
- Added instructions for passing ES settings to elasticsearch.bat
- Updated manager GUI png

cc @elastic/microsoft @clintongormley 

Closes #16455
</description><key id="146437553">17576</key><summary>[DOCS] Update instructions for running on Windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>docs</label><label>review</label></labels><created>2016-04-06T21:15:32Z</created><updated>2016-04-07T22:01:42Z</updated><resolved>2016-04-07T22:01:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T09:52:29Z" id="206790849">thanks @gmarz - i've added some minor comments, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut over remaining queries to registerQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17575</link><project id="" key="" /><description>After this we can remove the old registration method!
</description><key id="146429084">17575</key><summary>Cut over remaining queries to registerQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T20:43:00Z</created><updated>2016-05-02T12:15:23Z</updated><resolved>2016-04-07T23:02:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-07T08:34:11Z" id="206759780">LGTM
</comment><comment author="nik9000" created="2016-04-07T23:02:53Z" id="207131359">Closed by e0cde29a685efcf1487b008379bbe4b8f39bf115
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No coherent error message when https connect attempt made to http server</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17574</link><project id="" key="" /><description>**Elasticsearch version**:

2.2.0 

**JVM version**:

JDK8

**OS version**:

Windows10

**Description of the problem including expected versus actual behavior**:

When I connect to ES through the http port but accidentally use https,  I get a crazy stack trace  that does not clearly explain the problem

**Steps to reproduce**:
1. curl https://localhost:PORT/

**Provide logs (if relevant)**:

[2016-04-05 20:20:55,498][INFO ][node                     ] [Madame Masque] version[2.2.0], pid[9832], build[8ff36d1/
6-01-27T13:32:39Z]
[2016-04-05 20:20:55,498][INFO ][node                     ] [Madame Masque] initializing ...
[2016-04-05 20:20:55,999][INFO ][plugins                  ] [Madame Masque] modules [lang-expression, lang-groovy], p
ins [siren-join], sites []
[2016-04-05 20:20:56,030][INFO ][env                      ] [Madame Masque] using [1] data paths, mounts [[Windows7_O
C:)]], net usable_space [63.3gb], net total_space [476.6gb], spins? [unknown], types [NTFS]
[2016-04-05 20:20:56,047][INFO ][env                      ] [Madame Masque] heap size [910.5mb], compressed ordinary
ect pointers [true]
[2016-04-05 20:20:58,456][INFO ][node                     ] [Madame Masque] initialized
[2016-04-05 20:20:58,456][INFO ][node                     ] [Madame Masque] starting ...
[2016-04-05 20:20:58,888][INFO ][transport                ] [Madame Masque] publish_address {127.0.0.1:9330}, bound_a
esses {127.0.0.1:9330}, {[::1]:9330}
[2016-04-05 20:20:58,888][INFO ][discovery                ] [Madame Masque] kibi-demo/zC1nmMAmRTyR9ZhYeaW9vA
[2016-04-05 20:21:01,941][INFO ][cluster.service          ] [Madame Masque] new_master {Madame Masque}{zC1nmMAmRTyR9Z
aW9vA}{127.0.0.1}{127.0.0.1:9330}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-04-05 20:21:02,094][INFO ][gateway                  ] [Madame Masque] recovered [6] indices into cluster_state
[2016-04-05 20:21:02,357][INFO ][http                     ] [Madame Masque] publish_address {127.0.0.1:9220}, bound_a
esses {127.0.0.1:9220}, {[::1]:9220}
[2016-04-05 20:21:02,357][INFO ][node                     ] [Madame Masque] started
[2016-04-05 20:21:03,508][INFO ][cluster.routing.allocation] [Madame Masque] Cluster health status changed from [RED]
 [YELLOW](reason: [shards started [[.kibi][0]] ...]).
[2016-04-06 15:46:00,784][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0xf70f119f, /127.0.0.1:27656 =&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: invalid version format: &#8745;&#9488;&#199;+&#8745;&#9488;&#199;/&#8745;&#9488;&#238;&#8745;&#9563;&#8976;&#8745;&#9488;&#238;&#8745;&#9563;&#191;&#8745;&#9488;&#238;&#8745;&#9488;&#238;&#8745;&#9488;&#199;
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:94)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:00,841][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0xf70f119f, /127.0.0.1:27656 :&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: invalid version format: &#8745;&#9488;&#199;+&#8745;&#9488;&#199;/&#8745;&#9488;&#238;&#8745;&#9563;&#8976;&#8745;&#9488;&#238;&#8745;&#9563;&#191;&#8745;&#9488;&#238;&#8745;&#9488;&#238;&#8745;&#9488;&#199;
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:94)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:554)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelin
nk.java:81)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:

```
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
```

line.java:779)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
line.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:
)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.jboss.netty.channel.Channels.close(Channels.java:812)
        at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:206)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.exceptionCaught(NettyHttpServerTransport.java:379)
        at org.elasticsearch.http.netty.HttpRequestHandler.exceptionCaught(HttpRequestHandler.java:72)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
        at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
        at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:00,842][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0xf76ea191, /127.0.0.1:27657 =&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:01,044][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0xf76ea191, /127.0.0.1:27657 :&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:554)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelin
nk.java:81)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:

```
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
```

line.java:779)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
line.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:
)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.jboss.netty.channel.Channels.close(Channels.java:812)
        at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:206)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.exceptionCaught(NettyHttpServerTransport.java:379)
        at org.elasticsearch.http.netty.HttpRequestHandler.exceptionCaught(HttpRequestHandler.java:72)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
        at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
        at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:01,046][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0x610e0f93, /127.0.0.1:27658 =&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:01,261][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0x610e0f93, /127.0.0.1:27658 :&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:554)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelin
nk.java:81)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:

```
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
```

line.java:779)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
line.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:
)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.jboss.netty.channel.Channels.close(Channels.java:812)
        at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:206)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.exceptionCaught(NettyHttpServerTransport.java:379)
        at org.elasticsearch.http.netty.HttpRequestHandler.exceptionCaught(HttpRequestHandler.java:72)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
        at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
        at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:01,263][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0x1484f32b, /127.0.0.1:27660 =&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:01,461][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0x1484f32b, /127.0.0.1:27660 :&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:554)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelin
nk.java:81)
        at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:

```
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
```

line.java:779)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelP
line.java:784)
        at org.jboss.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
        at org.jboss.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:
)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.jboss.netty.channel.Channels.close(Channels.java:812)
        at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:206)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.exceptionCaught(NettyHttpServerTransport.java:379)
        at org.elasticsearch.http.netty.HttpRequestHandler.exceptionCaught(HttpRequestHandler.java:72)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
        at org.jboss.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
        at org.jboss.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-06 15:46:01,581][WARN ][http.netty               ] [Madame Masque] Caught exception while handling client ht
traffic, closing connection [id: 0x806f282d, /127.0.0.1:27661 =&gt; /127.0.0.1:9220]
java.lang.IllegalArgumentException: empty text
        at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:89)
        at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
        at org.jboss.netty.handler.codec.http.HttpRequestDecoder.createMessage(HttpRequestDecoder.java:75)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:191)
        at org.jboss.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:102)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPip
ne.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</description><key id="146414981">17574</key><summary>No coherent error message when https connect attempt made to http server</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paulhoule</reporter><labels><label>:REST</label></labels><created>2016-04-06T19:51:15Z</created><updated>2016-04-08T10:06:54Z</updated><resolved>2016-04-08T10:06:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T09:35:25Z" id="206783814">I'm not sure what we can do here.  Making an https request sends a lot of gobbledygook to a server which is listening for an http request.  Not sure how we would differentiate this from just plain random bytes.
</comment><comment author="danielmitterdorfer" created="2016-04-08T09:50:23Z" id="207353793">We discussed this in Fix-It Friday. The problem occurs when we try to determine the HTTP version:

```
java.lang.IllegalArgumentException: invalid version format: &#8745;&#9488;&#199;+&#8745;&#9488;&#199;/&#8745;&#9488;&#238;&#8745;&#9563;&#8976;&#8745;&#9488;&#238;&#8745;&#9563;&#191;&#8745;&#9488;&#238;&#65533;&#8745;&#9488;&#238;&#65533;&#8745;&#9488;&#199;
at org.jboss.netty.handler.codec.http.HttpVersion.(HttpVersion.java:94)
at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
...
```

So in the absence of a valid HTTP version something is fishy anyway and we can at least detect that this is an invalid protocol. 

Obviously on protocol level there is not much we can do except to close the connection but we can maybe log a bit better exception.
</comment><comment author="jasontedor" created="2016-04-08T10:05:03Z" id="207357696">There's nothing that we can do here. Note that I can produce the exact same stack trace with this request:

```
$ curl -XGET "localhost:9200/ foo"
```

which produces:

```
java.lang.IllegalArgumentException: invalid version format: FOO HTTP/1.1
    at org.jboss.netty.handler.codec.http.HttpVersion.&lt;init&gt;(HttpVersion.java:94)
    at org.jboss.netty.handler.codec.http.HttpVersion.valueOf(HttpVersion.java:62)
```

Bad requests are bad.
</comment><comment author="danielmitterdorfer" created="2016-04-08T10:06:49Z" id="207358281">As discussed, in that case it does not make to much sense to try to distinguish invalid HTTP requests and invalid protocols if the behavior is identical.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Add vagrant.boxes gradle property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17573</link><project id="" key="" /><description>This change makes specifying which boxes to run vagrant tests on a
little easier. Previously there were two tasks, checkPackages and
checkPackagesAllDistros. With this change, there is a single
packagingTest task. The boxes to run on are specified using the
gradle property vagrant.boxes, which can be easily specified on the
command line, or in a gradle properties file. There are also two
alias names, 'sample' for a yum and apt box, and 'all' for all boxes.
</description><key id="146411450">17573</key><summary>Tests: Add vagrant.boxes gradle property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T19:35:45Z</created><updated>2016-04-06T23:09:45Z</updated><resolved>2016-04-06T20:33:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-06T19:53:08Z" id="206531680">Mostly LGTM - left a question about whether we should generate the tasks always and use the list only for generating dependencies.
</comment><comment author="rjernst" created="2016-04-06T20:09:35Z" id="206538049">Thanks @nik9000 I pushed commits to address your comments.
</comment><comment author="nik9000" created="2016-04-06T20:32:44Z" id="206549726">LGTM
</comment><comment author="dakrone" created="2016-04-06T21:23:21Z" id="206576548">@rjernst I think TESTING.asciidoc also needs to be updated as it points to the old (removed) gradle tasks now?
</comment><comment author="rjernst" created="2016-04-06T23:09:44Z" id="206613356">@dakrone Good catch, I updated the docs in d1cfe0e.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reassigning aliases to the newly created indexes using Aliases API (Java Client)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17572</link><project id="" key="" /><description>Hi folks, I&#180;m using ES 1.7.3 and client Java the same version.

The alias API accept a list of add/remove actions which are executed atomically in one call. Java-wise you can do the following:

``` java
client.admin().indices().prepareAliases().removeAlias("old_index", "my_alias").addAlias("new_index", "my_alias").execute().actionGet();
```

But, in a short time, the other client request a new search on ES and the Exception `[my_alias] missing` happens. What can I do to fix it?

Thanks,
Andr&#233; Rezende
</description><key id="146411119">17572</key><summary>Reassigning aliases to the newly created indexes using Aliases API (Java Client)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">andremrezende</reporter><labels /><created>2016-04-06T19:34:01Z</created><updated>2016-06-30T06:56:20Z</updated><resolved>2016-06-30T06:56:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-07T10:01:47Z" id="206794076">@andremrezende Thank you for reporting this.
Is this something you can reproduce with a Java test?

Could you share the full code?

Asking that because IMO this is not supposed to happen.
The `client` is not aware that the configuration on the server has changed. Or may be you are using a NodeClient which could potentially explain this?

So any information you could add would help a lot.

And BTW, are you seeing that with elasticsearch 2.3? 
</comment><comment author="andremrezende" created="2016-04-07T11:03:09Z" id="206816593">The method to add alias is:

```
 private void addAlias(final IndicesAdminClient indices, final String indexName, long cdCatalog) throws DBRElasticsearchException {
    IndicesAliasesResponse aliasReponse = null;
    try {
        //Removes the alias from the old index at the same time as we add it to the new index. The change needs to be atomic.
        if(lastIndexName!=null &amp;&amp; !"".equals(lastIndexName)) {
            aliasReponse = indices.prepareAliases().removeAlias(lastIndexName, ALIAS.PRODUCTS.toString(cdCatalog)).addAlias(indexName, ALIAS.PRODUCTS.toString(cdCatalog)).execute().actionGet();
            //indices.prepareAliases().removeAlias(lastIndexName, ALIAS.PRODUCTS.toString(cdCatalog)).execute();
        } else {
            aliasReponse = indices.prepareAliases().addAlias(indexName, ALIAS.PRODUCTS.toString(cdCatalog)).execute().actionGet();
        }
        if (aliasReponse!=null &amp;&amp; !aliasReponse.isAcknowledged()) {
            throw new DBRElasticsearchException("Alias: %s error to create: " + ALIAS.PRODUCTS.toString(cdCatalog));
        }
    } catch (InvalidAliasNameException e) {
             ...
    }
}
```

The indices parameter is loading on constructor method and shared with other methods.

Thanks in advanced,
Andr&#233; Rezende
</comment><comment author="dadoonet" created="2016-04-07T11:05:34Z" id="206818369">What kind of client are you using?
</comment><comment author="andremrezende" created="2016-04-07T11:06:18Z" id="206818921">The property type is: IndicesAdminClient, the java lib is on 1.7.3
</comment><comment author="dadoonet" created="2016-04-07T11:41:17Z" id="206828123">I might be unclear.

How do you build this client please?
</comment><comment author="andremrezende" created="2016-04-07T11:48:51Z" id="206830284">```
        Settings settings = ImmutableSettings.settingsBuilder()
                    .put("cluster.name", prop.getProperty("clusetername"))
                    .put("client.transport.ping_timeout", "100s")
                    .put("client.transport.sniff", true)
                    .put("discovery.zen.ping.multicast.enabled", false)
                    .put("discovery.zen.ping.unicast.enabled", true)
                    .put("discovery.zen.multicast.enabled", false)
                    .put("discovery.zen.unicast.enabled", true).build();
       client = new TransportClient(settings);
       client.addTransportAddress(new InetSocketTransportAddress(
                        'IP_ADDRESS', 9300));
```
</comment><comment author="dadoonet" created="2016-04-07T11:57:57Z" id="206833936">Ok. So it's a `TransportClient`. So I don't understand how this can happen.

Are you able to reproduce this in a test case?
Also, can you reproduce with 2.x version?

Asking that because lot of changes have been done in the meantime in 2.x series. For example #11930 might fix what you described.
</comment><comment author="andremrezende" created="2016-04-07T12:06:02Z" id="206839352">So, I tried to reproduce it using a test case, but I was not able to get exceptions, just on prodction enviroment. :(
I did NOT upgrade my ES to 2 because the java api lib is different, and I must change the source code.
I would like to see any document or reference about this upgrade, change the ES server is easier, but the ES client I don&#180;t think so.

thanks
</comment><comment author="dadoonet" created="2016-04-07T14:24:33Z" id="206929922">&gt; change the ES server is easier

Well. It depends. You will have to check what the migration plugin says and also go through all the braking changes.

&gt; but the ES client I don&#180;t think so.

Yes and no. It depends on the result of what I just said ^^^ 

&gt; So, I tried to reproduce it using a test case, but I was not able to get exceptions, just on prodction enviroment. :(

May be you are doing things a bit differently than in your use case?
</comment><comment author="andremrezende" created="2016-04-07T15:25:21Z" id="206952065">There are 2 systems, an ERP to update data in ES and another system to "consume" data. The exception occurs on consumer when the ERP is removing and adding alias on the same time.
</comment><comment author="dadoonet" created="2016-06-30T06:56:20Z" id="229575654">I'm going to close this one. I believe it has been fixed in 2.x with #11930 TBH. 
If you can come with a test, like a load test which change aliases all the time in a process and query the alias in another process and fails, that could be awesome.
Even more, if you can do it in 1.7 then upgrade to 2.3 and check that it does not occur anymore... That'd be fantastic.

Feel free to reopen or comment if you have any more information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut CustomQueryParserIT to registerQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17571</link><project id="" key="" /><description>This is the last place inside of ES other than SearchModule.
</description><key id="146399742">17571</key><summary>Cut CustomQueryParserIT to registerQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T18:41:52Z</created><updated>2016-05-02T12:15:27Z</updated><resolved>2016-04-06T20:41:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-06T19:12:50Z" id="206518176">LGTM
</comment><comment author="nik9000" created="2016-04-06T20:40:12Z" id="206555900">Merged! Thanks @cbuescher !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node requires `discovery.zen.minimum_master_nodes` even though bound to only _local_</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17570</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: v5.0.0-alpha1

**JVM version**: Java(TM) SE Runtime Environment (build 1.8.0_74-b02)

**OS version**: OSX 10.11.4

**Description of the problem including expected versus actual behavior**: Node requires `discovery.zen.minimum_master_nodes` even though `es.network.host=_local_`. It appears that https://github.com/elastic/elasticsearch/pull/17288 was taken too far and is not checking that we are overriding to a public interface.

**Steps to reproduce**:
1. Start elasticsearch as shown below

**Provide logs (if relevant)**:

```
djschny:elasticsearch-5.0.0-alpha1 djschny$ bin/elasticsearch -E es.network.host=_local_
Exception in thread "main" java.lang.RuntimeException: please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:79)
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:60)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:187)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:263)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
```
</description><key id="146391812">17570</key><summary>Node requires `discovery.zen.minimum_master_nodes` even though bound to only _local_</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">djschny</reporter><labels /><created>2016-04-06T18:08:25Z</created><updated>2016-04-13T13:05:32Z</updated><resolved>2016-04-13T13:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-06T19:22:12Z" id="206521371">&gt; It appears that #17288 was taken too far and is not checking that we are overriding to a public interface.

It wasn't, this is currently how all of the the bootstrap checks operate by design. If you keep the enforced settings at their default, then the checks do not error. If you set any of them, then the checks do error. 

It's not just a simple matter of checking whether or not we are bound to localhost only. For example, we definitely want to consider binding to localhost but publishing to an external interface that nginx or another proxy is listening on to be production mode.

That said, I have proof-of-concept improvement to this that I want to noodle on a little longer before I turn it into a formal pull request. The main thing that I want think about is whether or not we are failing too late (because with my proof-of-concept we have to startup most services including network services!). This requires caution.

I have it to the point where we can _explicitly_ bind to localhost and pass:

``` bash
&#177; ~/elasticsearch/elasticsearch-5.0.0-alpha1-SNAPSHOT/bin/elasticsearch -E es.network.host=_local_
[2016-04-06 15:18:55,165][INFO ][node                     ] [Mr. M] version[5.0.0-alpha1-SNAPSHOT], pid[9512], build[57059f1/2016-04-06T19:13:15.556Z]
[2016-04-06 15:18:55,165][INFO ][node                     ] [Mr. M] initializing ...
[2016-04-06 15:18:55,437][INFO ][plugins                  ] [Mr. M] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-04-06 15:18:55,459][INFO ][env                      ] [Mr. M] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [106.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-04-06 15:18:55,459][INFO ][env                      ] [Mr. M] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-04-06 15:18:56,992][INFO ][node                     ] [Mr. M] initialized
[2016-04-06 15:18:56,992][INFO ][node                     ] [Mr. M] starting ...
[2016-04-06 15:18:57,065][INFO ][transport                ] [Mr. M] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2016-04-06 15:18:57,066][WARN ][node                     ] [Mr. M] please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
[2016-04-06 15:19:00,107][INFO ][cluster.service          ] [Mr. M] new_master {Mr. M}{8Zg2JgGfTwu13BtKjw6LRw}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-04-06 15:19:00,126][INFO ][http                     ] [Mr. M] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2016-04-06 15:19:00,126][INFO ][node                     ] [Mr. M] started
[2016-04-06 15:19:00,130][INFO ][gateway                  ] [Mr. M] recovered [0] indices into cluster_state
```

but bind to localhost and publish externally and fail:

``` bash
&#177; ~/elasticsearch/elasticsearch-5.0.0-alpha1-SNAPSHOT/bin/elasticsearch -E es.network.host=_local_ -E es.network.publish_host=_en0_
[2016-04-06 15:20:32,521][INFO ][node                     ] [Ebon Seeker] version[5.0.0-alpha1-SNAPSHOT], pid[9780], build[57059f1/2016-04-06T19:13:15.556Z]
[2016-04-06 15:20:32,521][INFO ][node                     ] [Ebon Seeker] initializing ...
[2016-04-06 15:20:32,778][INFO ][plugins                  ] [Ebon Seeker] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-04-06 15:20:32,800][INFO ][env                      ] [Ebon Seeker] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [106.8gb], net total_space [232.6gb], spins? [unknown], types [hfs]
[2016-04-06 15:20:32,800][INFO ][env                      ] [Ebon Seeker] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-04-06 15:20:34,181][INFO ][node                     ] [Ebon Seeker] initialized
[2016-04-06 15:20:34,181][INFO ][node                     ] [Ebon Seeker] starting ...
[2016-04-06 15:20:34,256][INFO ][transport                ] [Ebon Seeker] publish_address {192.168.1.8:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
Exception in thread "main" java.lang.RuntimeException: please set [discovery.zen.minimum_master_nodes] to a majority of the number of master eligible nodes in your cluster.
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:79)
        at org.elasticsearch.node.Node.start(Node.java:331)
        at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:202)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:265)
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
        at org.elasticsearch.cli.Command.main(Command.java:53)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
[2016-04-06 15:20:34,259][INFO ][node                     ] [Ebon Seeker] stopping ...
[2016-04-06 15:20:34,267][INFO ][node                     ] [Ebon Seeker] stopped
[2016-04-06 15:20:34,267][INFO ][node                     ] [Ebon Seeker] closing ...
[2016-04-06 15:20:34,274][INFO ][node                     ] [Ebon Seeker] closed
```
</comment><comment author="djschny" created="2016-04-06T21:40:43Z" id="206583296">Do we really think this will help people properly configure? I'm afraid this will cause more harm than good. For example:
- Person sets the first non-prod cluster (typically 1-3 nodes) that is not bound to localhost and finds this issue when starting ES
- They hopefully [Google the setting](https://www.google.com/webhp?sourceid=chrome-instant&amp;ion=1&amp;espv=2&amp;ie=UTF-8#q=%22discovery.zen.minimum_master_nodes%22%20elasticsearch) and then arrive at the [Zen Discovery Module page](https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-discovery-zen.html). 

Do we feel confident this will actually help or will people just configure it quickly to get it working and then forget about the setting as they add more nodes, etc.

I'm thinking in the docs we should have explicit yes/no configurations for the simple one, two, or three node configurations that most folks new to Elasticsearch will setup.

This essentially makes `discovery.zen.minimum_master_nodes` a required setting for any cluster. 

I'd be happy to help with documentation additions to that page too if you would like.
</comment><comment author="dakrone" created="2016-04-06T21:45:24Z" id="206584461">@djschny #15625 should help with this also or instead of
</comment><comment author="jasontedor" created="2016-04-06T22:12:52Z" id="206592581">&gt; Do we really think this will help people properly configure?

Yes, and note that it's exactly for the reasons that you outlined. People have to set it, and the error message suggests that they set it to a majority of their master-eligible nodes. You said they will search for it and this helps too. It's not perfect, but it's an incremental improvement over what we have today where it's too easy to not even know to set this setting at all. This setting is too important to not try to do something.

&gt; Do we feel confident this will actually help or will people just configure it quickly to get it working and then forget about the setting as they add more nodes, etc.

Yes, I truly think that it will help. But note that we also have #15625 from @ywelsch that detects the number of master eligible nodes and warns when `discovery.zen.minimum_master_nodes` is not set to a quorum of that detected number.

&gt; This essentially makes `discovery.zen.minimum_master_nodes` a required setting for any cluster.

As it should be.

&gt; I'd be happy to help with documentation additions to that page too if you would like.

That would be awesome.
</comment><comment author="bleskes" created="2016-04-07T08:19:14Z" id="206755581">&gt; Do we feel confident this will actually help or will people just configure it quickly to get it working and then forget about the setting as they add more nodes, etc.

I think we'd be in a better situation as opposed to where people never heard of the setting.

&gt; This essentially makes discovery.zen.minimum_master_nodes a required setting for any cluster.

Exactly. This is a tension between people trying out ES on the laptop and having a good OOB experience versus making sure that we the setup a proper cluster they'll think about this and other crucial environment settings (like file descriptors). If run a single node local cluster all you need to do is set the setting to 1. 

&gt; I'm thinking in the docs we should have explicit yes/no configurations for the simple one, two, or three node configurations that most folks new to Elasticsearch will setup.

I think there is some value in an example 3 node cluster setup (next to a dedicated master setup) but I think it's valid for more than just the min master nodes settings. Thinking of things like node names and expected_nodes (for the dedicated master setup).

&gt; I'd be happy to help with documentation additions to that page too if you would like.

Of course! 
</comment><comment author="jasontedor" created="2016-04-07T15:56:32Z" id="206967847">I opened #17595.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException from IndexingMemoryController when a version conflict happens during recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17569</link><project id="" key="" /><description>@dakrone hit this:

During recovery, if a version conflict happens, it's considered harmless (no exception thrown by the engine), because it just means Lucene already has a newer version of the document indexed, e.g. because translog has different order-of-operations.

But `IndexingMemoryController` hits an NPE in that case, because no `Translog.Location` will be set on the engine operation, and IMC uses that to estimate ram bytes used by that operation.

Lee's amazingly simple test for some reason tickles this; I included it here.

I believe this only affects 5.0.0.
</description><key id="146384004">17569</key><summary>NullPointerException from IndexingMemoryController when a version conflict happens during recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T17:39:10Z</created><updated>2016-04-06T22:23:20Z</updated><resolved>2016-04-06T17:47:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-06T17:44:25Z" id="206484418">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail if an object is added after a field with the same name.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17568</link><project id="" key="" /><description>Today we fail if you try to add a field and an object from another type already
has the same name. However, we do NOT fail if you insert the field first and the
object afterwards. This leads to bad bugs since mappings are not necessarily
parsed in the same order at recovery time, so a mapping update could succeed and
then you would fail to reopen the index.

Closes #17567
</description><key id="146383649">17568</key><summary>Fail if an object is added after a field with the same name.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T17:37:23Z</created><updated>2016-04-07T08:37:00Z</updated><resolved>2016-04-06T18:16:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-06T18:08:28Z" id="206496179">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Conflicted Mapping causes problems during shard initialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17567</link><project id="" key="" /><description>This issue reproduces in ES 2.3.

``` http
POST /my-index/_bulk
{"index":{"_type":"type1"}}
{"title":"I am just a string"}
{"index":{"_type":"type2"}}
{"title":{"field":"I am a string in an object"}}

POST /my-index/_close
POST /my-index/_open
```

After opening the index, you'll get something like this in the logs:

```
[2016-04-06 10:24:20,677][WARN ][cluster.action.shard     ] [Anthropomorpho] [my-index][0] received shard failed for target shard [[my-index][0], node[dC8bCuUMRHWqd0dmyqxgwg], [P], v[88], s[INITIALIZING], a[id=HdFXs5PdQI6iT4G5HFyF3w], unassigned_info[[reason=ALLOCATION_FAILED], at[2016-04-06T14:24:20.505Z], details[failed to update mappings, failure IllegalArgumentException[Field [title] is defined as a field in mapping [type1] but this name is already used for an object in other types]]]], indexUUID [LiIeIKE-RvSNnfaBUi9NWw], message [failed to update mappings], failure [IllegalArgumentException[Field [title] is defined as a field in mapping [type1] but this name is already used for an object in other types]]
java.lang.IllegalArgumentException: Field [title] is defined as a field in mapping [type1] but this name is already used for an object in other types
    at org.elasticsearch.index.mapper.MapperService.checkObjectsCompatibility(MapperService.java:473)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:336)
    at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:289)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.processMapping(IndicesClusterStateService.java:387)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyMappings(IndicesClusterStateService.java:348)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:164)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:610)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:772)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
```

Relates to #15243
</description><key id="146371929">17567</key><summary>Conflicted Mapping causes problems during shard initialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-04-06T16:50:51Z</created><updated>2016-07-26T12:32:54Z</updated><resolved>2016-04-07T08:37:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T08:37:16Z" id="206760520">Closed by #17568
</comment><comment author="damienalexandre" created="2016-04-14T18:07:13Z" id="210080377">I got this bug on production today, with mapping creation based on templates (logstash :heart:).

I had a field which was a string in 99% of my logs, and another "type", only getting event once an hour, with this field as an object - I didn't notice, as everything was working smoothly for weeks. Until we decided to install Marvel :neckbeard: 

Marvel is fine, but we restarded the cluster to start the agent... our 70+ shards were not able to start anymore! IllegalArgumentException on all shards, about this field not mapped properly.

I had to close all the indexes, no more logs history :sob: and since 2.0 removed the possibility to delete a Type, no possibility to clean up those bad indexes, I'm stuck with gigabyte of data I can't use because shard won't boot. Anyway.

Glad this is now fixed, and waiting for ES 2.3.2 :neckbeard: :sparkling_heart: 
</comment><comment author="cherweg" created="2016-07-26T12:32:53Z" id="235253505">Is there any way to recovery the data of an specific type inside of such an index?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing comma</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17566</link><project id="" key="" /><description>Added missing comma
</description><key id="146365989">17566</key><summary>Add missing comma</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Dmitrii-I</reporter><labels><label>docs</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T16:29:20Z</created><updated>2016-04-06T22:21:15Z</updated><resolved>2016-04-06T20:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T17:42:22Z" id="206483409">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="Dmitrii-I" created="2016-04-06T20:18:43Z" id="206542109">Done!

On Wed, Apr 6, 2016 at 7:43 PM, Lee Hinman notifications@github.com wrote:

&gt; Hi, thanks for the submission!
&gt; 
&gt; Could I ask you to sign the CLA
&gt; https://www.elastic.co/contributor-agreement/ so we can merge this in?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you authored the thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17566#issuecomment-206483409
</comment><comment author="dakrone" created="2016-04-06T20:41:44Z" id="206556848">Merged into 2.3, 2.x, and master, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error while Bulk Index Operations !!!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17565</link><project id="" key="" /><description>Hi ES Team,

we are using ES 2.2 build (2.2.0) for document storage. 

while executing bulk index request having a few docs around 10-50  docs at every 5 sec interval, getting a weird error in afterbulk callback of bulk processor.
**
RemoteTransportException[[Hairbag][127.0.0.1:9300][indices:data/write/bulk]]; nested: NullPointerException[id must not be null];**

in result of, lots of documents are not being indexed and most of our bulk request got failed !!!

we are clueless what's going underneath ??? can you guys throw some lights on it ??? 
</description><key id="146359294">17565</key><summary>Error while Bulk Index Operations !!!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adhamelia</reporter><labels><label>:Bulk</label><label>feedback_needed</label></labels><created>2016-04-06T16:01:14Z</created><updated>2016-05-24T10:34:52Z</updated><resolved>2016-05-24T10:34:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T16:05:59Z" id="206443517">Please provide a short but complete reproduction of the issue, including creating the index, specifying the mappings, indexing the docs, and provide the full exception with stack trace.  It will greatly help us to understand what the problem is.
</comment><comment author="clintongormley" created="2016-05-24T10:34:52Z" id="221230545">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make delete-by-query a module and integrate with task mgmt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17564</link><project id="" key="" /><description>Now that the task management API is in place, we should move delete-by-query back into core as a module, and make it use the task management API as done by reindex.
</description><key id="146356611">17564</key><summary>Make delete-by-query a module and integrate with task mgmt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugin Delete By Query</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-06T15:50:30Z</created><updated>2016-05-02T12:03:49Z</updated><resolved>2016-04-06T15:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-06T15:54:59Z" id="206439834">Dupe of #16883 I believe.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change next major version from 3.0.0 to 5.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17563</link><project id="" key="" /><description /><key id="146345616">17563</key><summary>Change next major version from 3.0.0 to 5.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raf64flo</reporter><labels><label>docs</label></labels><created>2016-04-06T15:15:25Z</created><updated>2016-04-07T08:13:29Z</updated><resolved>2016-04-07T08:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T08:13:29Z" id="206751693">thanks @raf64flo - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improving parsing of sigma param for Extended Stats Bucket Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17562</link><project id="" key="" /><description>Also improving parsing of percents param for Percentiles Bucket Aggregation
Fix for #17499
</description><key id="146341751">17562</key><summary>Improving parsing of sigma param for Extended Stats Bucket Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v5.0.0-alpha3</label></labels><created>2016-04-06T15:00:24Z</created><updated>2016-05-10T14:12:30Z</updated><resolved>2016-05-10T14:11:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-07T08:30:09Z" id="206758683">@alexshadow007 Thanks for raising this PR. I left a couple of comments but I think this looks pretty good so far.
</comment><comment author="clintongormley" created="2016-05-07T14:34:11Z" id="217641396">@colings86 could review this again please?
</comment><comment author="colings86" created="2016-05-10T10:52:11Z" id="218123759">@alexshadow007 Sorry its taken a while for me to get back to you on this. It looks good to me but I wonder if you could update the branch with the latest master so it can be merged?
</comment><comment author="alexshadow007" created="2016-05-10T13:01:37Z" id="218150085">@colings86 I updated branch with latest master.
</comment><comment author="colings86" created="2016-05-10T13:27:55Z" id="218156708">@alexshadow007 Thanks. I'm just going to run through a build again to check there is nothing unexpected and then I'll merge it in
</comment><comment author="colings86" created="2016-05-10T14:12:30Z" id="218169708">@alexshadow007 merged into master. Thanks for the contribution. Sorry it took so long to get reviewed and merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>'no such index' exception when index.mapper.dynamic: false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17561</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.0

**JVM version**: Oracle 1.8.0_60 x86_64

**OS version**: CentOS 6.7 (Final)

**Description of the problem including expected versus actual behavior**:
I have an index template that matches template `persons*`:

```
{
    "order" : 0,
    "template" : "persons*",
    "settings" : {
        "index" : {
            "number_of_shards" : "1",
            "number_of_replicas" : "0"
        }
    },
    "mappings" : {
        "person" : {
            "dynamic" : "strict",
            "_all" : {
                "enabled" : false
            },
            "properties" : {
                "name" : {
                    "index" : "not_analyzed",
                    "type" : "string"
                }
            }
        }
    }
}
```

I index a document into an index `persons-1970-01-01`:

```
{
    "name" : "Henk"
}
```

Before 2.3.0, the new index would be created with the supplied document. With 2.3.0 it doesn't work any longer. The response is:

```
{
    "error": {
        "root_cause": [
            {
                "type": "index_not_found_exception",
                "reason": "no such index",
                "resource.type": "index_expression",
                "resource.id": "persons-1970-01-01",
                "index": "persons-1970-01-01"
            }
        ],
        "type": "index_not_found_exception",
        "reason": "no such index",
        "resource.type": "index_expression",
        "resource.id": "persons-1970-01-01",
        "index": "persons-1970-01-01"
    },
    "status": 404
}
```

See below for ES stacktrace.

**Steps to reproduce**:
1. Make sure you have `index.mapper.dynamic: false` configured in your `config/elasticsearch.yml` file.
2. Create the template: `curl -s -XPUT localhost:13011/_template/my-template -d@persons-template.json`. This should work.
3. Index the doc: `curl -s -XPUT localhost:13011/persons-1970-01-01/person/henk -d@person-doc.json`. You'll get the above mentioned error.
4. Now, comment the `index.mapper.dynamic: false` setting.
5. Repeat steps 2 and 3, but this time, the doc will be indexed as expected.

**Provide logs (if relevant)**:
Elasticsearch stacktrace when indexing the doc with `index.mapper.dynamic: false`:

```
[2016-04-06 16:11:06,386][WARN ][action.index             ] [rob] unexpected error during the primary phase for action [indices:data/write/index], request [index {[persons-1970-01-01][person][henk], source[{    "name" : "Henk"}]}]
[persons-1970-01-01] IndexNotFoundException[no such index]
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:151)
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:95)
    at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteSingleIndex(IndexNameExpressionResolver.java:208)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$ReroutePhase.doRun(TransportReplicationAction.java:451)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction.doExecute(TransportReplicationAction.java:131)
    at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:135)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:119)
    at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:66)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
    at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
    at org.elasticsearch.client.support.AbstractClient.index(AbstractClient.java:371)
    at org.elasticsearch.rest.action.index.RestIndexAction.handleRequest(RestIndexAction.java:102)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:205)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
    at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
    at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:449)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:61)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="146332787">17561</key><summary>'no such index' exception when index.mapper.dynamic: false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robth</reporter><labels><label>:Mapping</label><label>adoptme</label><label>regression</label></labels><created>2016-04-06T14:30:48Z</created><updated>2016-05-05T08:07:03Z</updated><resolved>2016-05-05T08:06:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-07T16:07:18Z" id="206971806">See related bug in https://github.com/elastic/elasticsearch/issues/17592

In this issue, `index.mapper.dynamic` is set in the config file instead of in the template.  The ability to set this in the config file will go away in 5.x.
</comment><comment author="clintongormley" created="2016-04-07T16:34:58Z" id="206983383">Probably caused by https://github.com/elastic/elasticsearch/pull/15424
</comment><comment author="clintongormley" created="2016-05-05T08:06:57Z" id="217098396">Actually investigating this further, this setting in the config file never worked correctly.  For instance, the following:

```
PUT _template/test
{
  "order": 0,
  "template": "persons*",
  "settings": {
    "index": {
      "number_of_shards": "1",
      "number_of_replicas": "0"
    }
  },
  "mappings": {
    "person": {
      "dynamic": "strict",
      "_all": {
        "enabled": false
      },
      "properties": {
        "name": {
          "index": "not_analyzed",
          "type": "string"
        }
      }
    }
  }
}
```

Now put a document with type `personx`, which should have been prevented:

```
PUT persons_1970/personx/1
{
    "name" : "Henk"
}
```

The `personx` type is added.  If the index already existed at the time we tried to add `personx`, then this would fail.  Setting `index.*` settings in the config file is no longer supported in 5.0 so I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `.percolator` type in favour of `percolator` field type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17560</link><project id="" key="" /><description>- The `.percolator` type is now forbidden. (just like any type that starts with a `.`)
- Added an extra `query_field` parameter to the `percolator` query to indicate what percolator field should be used. This must be an existing field in the mapping of type `percolator`.

This only applies for new indices created on 5.0.x and later. Indices created on previous versions the .percolator type is still allowed to exist.
The new `percolator` field type isn't active in such indices and the `PercolatorQueryCache` knows how to load queries from these legacy indices.
The `PercolatorQueryBuilder` will not enforce that the `query_field` is of type `percolator`.
</description><key id="146327053">17560</key><summary>Remove `.percolator` type in favour of `percolator` field type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>breaking</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T14:14:57Z</created><updated>2016-04-19T10:35:52Z</updated><resolved>2016-04-19T09:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T14:28:51Z" id="206400734">How about just `field` instead of `query_field`, to be consistent with other queries.  Also wondering if we should change the percolator query to have a similar structure to eg the `match` query:

```
"query": {
  "percolator": {
    "my_query_field": {
      ...
}}}
```

Might a percolator query use queries from multiple fields, like `multi_match`?  In which case, perhaps `fields` instead?

Also thinking that we should change the query name to `percolate` instead of `percolator` - feels more natural I think
</comment><comment author="martijnvg" created="2016-04-06T14:33:44Z" id="206403478">&gt; How about just field instead of query_field,

+1

&gt; Also wondering if we should change the percolator query to have a similar structure to eg the match query

I'm not a fan of that structure and it complicates query parsing which is simple now.

&gt; Might a percolator query use queries from multiple fields, like multi_match? In which case, perhaps fields instead?

No.

&gt; Also thinking that we should change the query name to percolate instead of percolator

I'm ok with changing the name to `percolate`. I thought that it would consistent that the field type and query have the same name. Not sure what others think about this. 
</comment><comment author="jpountz" created="2016-04-08T08:22:38Z" id="207307389">I left some comments.
</comment><comment author="martijnvg" created="2016-04-11T09:38:16Z" id="208254901">@jpountz Thx! I've updated the pr based on your comments.
</comment><comment author="martijnvg" created="2016-04-18T09:24:59Z" id="211293056">@jpountz I've addressed the comments you made last Friday.
</comment><comment author="jpountz" created="2016-04-18T21:38:37Z" id="211593809">There is one thing I'd like to fix in the mapping validation but otherwise LGTM.
</comment><comment author="jpountz" created="2016-04-19T08:03:42Z" id="211784193">LGTM
</comment><comment author="martijnvg" created="2016-04-19T09:32:53Z" id="211824802">@clintongormley I'll rename the `percolator` query to `percolate` query in a follow up PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid shift value in prefixCoded bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17559</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**JVM version**: 8-jre

**OS version**: docker

**Description of the problem including expected versus actual behavior**:

Sorting fails after field with the same name is inserted in a _different_ type with a different mapping within the same index. Expected: results from specified type.

**Steps to reproduce**:

```
PUT test

PUT test/_mapping/foo
{
  "properties": {
    "id": {
      "type": "integer"
    }
  }
}

POST test/foo
{
  "id": 1
}

# Works
GET test/foo/_search
{
  "sort": "id"
}

# Breaks sorting by id in ANY type within test index
POST test/bar
{
  "id": 2
}

# Doesn't work
GET test/foo/_search
{
  "sort": "id"
}

# Doesn't work either
GET test/bar/_search
{
  "sort": "id"
}
```

**Provide logs (if relevant)**:

```
[2016-04-06 11:33:43,399][DEBUG][action.search.type       ] [Umbo] [test][4], node[utxVpAQfT_GXAYpH1nq3kQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3fcf2407] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [test][4]: query[ConstantScore(cache(_type:foo))],from[0],size[10],sort[&lt;custom:"id": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@6676d6dc&gt;]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:301)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:312)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchException: java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:80)
    at org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource$1.getLongValues(LongValuesComparatorSource.java:67)
    at org.apache.lucene.search.FieldComparator$LongComparator.setNextReader(FieldComparator.java:716)
    at org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:612)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:581)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:533)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:510)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:150)
    ... 8 more
Caused by: org.elasticsearch.common.util.concurrent.UncheckedExecutionException: java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:167)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:74)
    ... 18 more
Caused by: java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)
    at org.apache.lucene.util.NumericUtils.getPrefixCodedIntShift(NumericUtils.java:209)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$2.accept(OrdinalsBuilder.java:445)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:244)
    at org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:82)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:472)
    at org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:109)
    at org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:49)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:180)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:167)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 22 more
```
</description><key id="146288843">17559</key><summary>Invalid shift value in prefixCoded bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s12v</reporter><labels /><created>2016-04-06T11:48:11Z</created><updated>2016-06-27T10:11:29Z</updated><resolved>2016-04-06T12:00:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T12:00:07Z" id="206338596">This is a known issue with mappings in 1.x.  In 2.0 you are no longer allowed to have fields with the same name but different mappings in different types. See #8870
</comment><comment author="mohdnavedshaikh" created="2016-06-27T10:11:28Z" id="228706027">Hi @clintongormley ,

I have some what same kind of issue.Kindly, help me to resolve the issue i am recieving .

I have below aggregation query:

{
  "query": {
    "bool": {
      "must": [
        {
          "bool": {
            "must": [
              {
                "nested": {
                  "query": {
                    "term": {
                      "skus.rack_status": "Y"
                    }
                  },
                  "path": "skus"
                }
              },
              {
                "terms": {
                  "boutique_id": [
                    "14404"
                  ]
                }
              }
            ]
          }
        },
        {
          "nested": {
            "filter": {
              "bool": {
                "must": {
                  "match_all": {}
                }
              }
            },
            "path": "skus"
          }
        }
      ]
    }
  },
  "aggregations": {
    "presaleFilter": {
      "terms": {
        "field": "is_presale"
      }
    },
    "filterCondition": {
      "filter": {
        "bool": {
          "must": {
            "nested": {
              "filter": {
                "bool": {
                  "must": [
                    {
                      "match_all": {}
                    },
                    {
                      "range": {
                        "skus.inv": {
                          "from": 0,
                          "to": null,
                          "include_lower": false,
                          "include_upper": true
                        }
                      }
                    }
                  ]
                }
              },
              "path": "skus"
            }
          }
        }
      },
      "aggregations": {
        "topCategoryFilter": {
          "terms": {
            "field": "category_id",
            "size": 2147483647
          },
          "aggregations": {
            "subCategoryFilter": {
              "terms": {
                "field": "sub_category_id",
                "size": 2147483647
              }
            }
          }
        },
        **"brandFilter": {
          "terms": {
            "field": "brand_id",
            "size": 2147483647
          }**
        }, "productClass": {
          "terms": {
            "field": "product_class_id",
            "size": 2147483647
          }
        },
        "skuFilter": {
          "nested": {
            "path": "skus"
          },
          "aggregations": {
            "sizeValues": {
              "nested": {
                "path": "skus.attribute"
              },
              "aggregations": {
                "attributeId": {
                  "terms": {
                    "field": "skus.attribute.attribute_id",
                    "size": 2147483647
                  },
                  "aggregations": {
                    "attributeValue": {
                      "terms": {
                        "field": "skus.attribute.attribute_value",
                        "size": 2147483647
                      }
                    }
                  }
                }
              }
            },
            "fromAge": {
              "terms": {
                "field": "skus.from_age"
              },
              "aggregations": {
                "toAge": {
                  "terms": {
                    "field": "skus.to_age",
                    "size": 2147483647
                  }
                },
                "from_to_age": {
                  "reverse_nested": {}
                }
              }
            },
            "gender": {
              "terms": {
                "field": "skus.gender",
                "size": 2147483647
              },
              "aggregations": {
                "gender_to_skus": {
                  "reverse_nested": {}
                }
              }
            },
            "min_price": {
              "min": {
                "field": "skus.sale_price"
              }
            },
            "max_price": {
              "max": {
                "field": "skus.sale_price"
              }
            }
          }
        }
      }
    }
  },
  "from": 0,
  "size": 56
}

The highlighted if commented out then i get response else i get below given  exception : 

org.elasticsearch.ElasticsearchException: java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:80)
    at org.elasticsearch.search.aggregations.support.ValuesSource$MetaData.load(ValuesSource.java:88)
    at org.elasticsearch.search.aggregations.support.AggregationContext.numericField(AggregationContext.java:159)
    at org.elasticsearch.search.aggregations.support.AggregationContext.valuesSource(AggregationContext.java:137)
    at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.create(ValuesSourceAggregatorFactory.java:53)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createAndRegisterContextAware(AggregatorFactories.java:53)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createSubAggregators(AggregatorFactories.java:71)
    at org.elasticsearch.search.aggregations.Aggregator.&lt;init&gt;(Aggregator.java:191)
    at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.&lt;init&gt;(BucketsAggregator.java:39)
    at org.elasticsearch.search.aggregations.bucket.SingleBucketAggregator.&lt;init&gt;(SingleBucketAggregator.java:32)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.&lt;init&gt;(FilterAggregator.java:45)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator$Factory.create(FilterAggregator.java:86)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createAndRegisterContextAware(AggregatorFactories.java:53)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:157)
    at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:79)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:100)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:301)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:312)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.common.util.concurrent.UncheckedExecutionException: java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2203)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:167)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:74)
    ... 23 more
Caused by: java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)
    at org.apache.lucene.util.NumericUtils.getPrefixCodedIntShift(NumericUtils.java:209)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$2.accept(OrdinalsBuilder.java:445)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:244)
    at org.elasticsearch.index.fielddata.RamAccountingTermsEnum.next(RamAccountingTermsEnum.java:82)
    at org.elasticsearch.index.fielddata.ordinals.OrdinalsBuilder$3.next(OrdinalsBuilder.java:472)
    at org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:109)
    at org.elasticsearch.index.fielddata.plain.PackedArrayIndexFieldData.loadDirect(PackedArrayIndexFieldData.java:49)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:180)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$1.call(IndicesFieldDataCache.java:167)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)
    at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)
    at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)
    ... 27 more

Thank You in advance.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable script access to `_source` by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17558</link><project id="" key="" /><description>Accessing the `_source` field within a script is very slow and can hurt a cluster's performance. That said, it can be a useful way of accessing some not-indexed value for one-off uses without having to reindex.

As a [safeguard](https://github.com/elastic/elasticsearch/issues/11511), let's disable script access to `_source` by default (other than update/reindex scripts, obviously) and allow the user to re-enable access with a dynamic index setting.
</description><key id="146269802">17558</key><summary>Disable script access to `_source` by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Scripting</label><label>adoptme</label><label>breaking</label></labels><created>2016-04-06T10:36:47Z</created><updated>2016-10-18T07:51:00Z</updated><resolved>2016-10-18T07:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-06T13:56:43Z" id="206383622">I was against this when I read the title but I'm ok with it if we make it enable-able with a dynamic setting. _source is genuinely useful but really slow. It'd be nice to document exactly why when we write the docs for the dynamic setting. Maybe this is a thing we should enable on the request instead? It feels like a dynamic setting is too broad. Like, when I ran _source things at WMF I really only ran them on a handful of requests. All hand written, all ones I was willing to wait for. OTOH allowing anyone to override it feels funky. If core ES had users and permissions I'd say "this should be a permission a user has".

Point of clarification: this only applies to searches. It'll be ok by default to put it in `_update` and the mutation script in `_update_by_query` or `_reindex`.
</comment><comment author="clintongormley" created="2016-04-06T14:20:58Z" id="206395596">&gt; Maybe this is a thing we should enable on the request instead? It feels like a dynamic setting is too broad.

Think of this from the viewpoint of the sysadmin who wants to protect their cluster from abuse by users. Like the other [safeguards](https://github.com/elastic/elasticsearch/issues/11511), this would be a setting the sysadmin could use to turn slow/dangerous stuff off
</comment><comment author="clintongormley" created="2016-10-18T07:50:57Z" id="254433046">This has been done in Painless.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove MappedFieldType.value.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17557</link><project id="" key="" /><description>This commit removes `MappedFieldType.value` and simplifies
`MappedFieldType.valueforSearch`. `valueforSearch` was used to post-process
values that come for stored fields (eg. to convert a long back to a string
representation of a date in the case of a date field) and also values that
are extracted from the source but only in the case of GET calls: it would
not be called when performing source filtering on search requests.

`valueforSearch` is now only called for stored fields, since values that are
extracted from the source should already be formatted as expected.
</description><key id="146261854">17557</key><summary>Remove MappedFieldType.value.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T10:08:16Z</created><updated>2016-04-12T07:13:51Z</updated><resolved>2016-04-12T07:13:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-06T10:10:02Z" id="206285100">I think we should consider renaming `valueForSearch` but I'd like to do it in a follow-up PR as it could make the merge a bit painful with other branches I am working on (especially one that tries to integrate points).
</comment><comment author="rjernst" created="2016-04-11T18:10:30Z" id="208481934">LGTM, nice clean up!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes NPE when no window is specified in moving average request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17556</link><project id="" key="" /><description>This PR fixes a bug where a NPE was thrown when parsing a moving average pipeline aggregation request which did not specify a window size.

Closes #17516
</description><key id="146218988">17556</key><summary>Fixes NPE when no window is specified in moving average request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-06T07:42:03Z</created><updated>2016-05-02T12:06:39Z</updated><resolved>2016-04-06T14:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-04-06T14:54:10Z" id="206414265">LGTM!

Tangentially and for future thought, it's a shame that we have to pass these settings into the model (and then set the model on the factory), feels a bit circular.  But since HoltWinters needs to check the window length, I'm not really sure if there's any way to refactor it cleanly.  

Perhaps we could run the model's Parser, then call a `validate()` method on the model which gets a copy of the current settings?  Doesn't feel any better tbh.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Scroll search still uses deprecated SCAN search type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17555</link><project id="" key="" /><description>In the Java client API documentation, when [using scrolls in Java](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-search-scrolling.html), the sample code still uses `SearchType.SCAN` even though this was [deprecated in 2.1](https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_21_search_changes.html#_literal_search_type_scan_literal_deprecated).

I guess the sample code needs to be revised to match what the [REST documentation on scrolling](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-request-scroll.html) states, namely that a sort on `_doc` should simply be added instead of using the `SCAN` search type.

```
SearchResponse scrollResp = client.prepareSearch(test)
        .setScroll(new TimeValue(60000))
        .setQuery(qb)
        .setSize(100)
        .addSort("_doc", SortOrder.ASC)
        .execute().actionGet(); //100 hits per shard will be returned for each scroll
```
</description><key id="146218736">17555</key><summary>Java API: Scroll search still uses deprecated SCAN search type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">consulthys</reporter><labels><label>:Java API</label><label>adoptme</label><label>docs</label></labels><created>2016-04-06T07:40:47Z</created><updated>2016-04-07T08:23:04Z</updated><resolved>2016-04-07T08:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-06T07:51:14Z" id="206190903">Thanks for bringing this up @consulthys . Would you be up for sending a PR that fixes the docs?
</comment><comment author="consulthys" created="2016-04-06T07:56:39Z" id="206194800">Sure thing, I will do that today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Eclipse gradle build ElasticSearch problem (Not found JAVA_HOME)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17554</link><project id="" key="" /><description>In my console, echo $JAVA_HOME is correct path.
I changed 
buildSrc/src/main/groovy/org/elasticsearch/gradle/BuildPlugin.groovy

eclipse can not set JAVA_HOME, like intellij. but the following code only judge intellij. is it a bug?

```
+                   if (System.getProperty("idea.active") != null) {
+                       // intellij doesn't set JAVA_HOME, so we use the jdk gradle was run with
+                   } else {
+                       throw new GradleException('JAVA_HOME must be set to build Elasticsearch')
+                   }
```
</description><key id="146195493">17554</key><summary>Eclipse gradle build ElasticSearch problem (Not found JAVA_HOME)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilly</reporter><labels><label>build</label><label>discuss</label></labels><created>2016-04-06T06:03:00Z</created><updated>2016-04-06T14:45:31Z</updated><resolved>2016-04-06T14:45:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-06T14:45:21Z" id="206410348">I use Eclipse but I don't use the Eclipse gradle plugin. I think that is how most folks who do Elasticsearch development with Eclipse do it. The docs (CONTRIBUTING.md I think) talk about how to do project generation for Eclipse. Sadly, it is too much work to support both the Eclipse gradle plugin and the gradle eclipse plugin. So we went with the gradle eclipse plugin.

I'm going to close this because I don't think it is a bug. This is just "how it is done" but I'm happy to reopen if you disagree or think we need more docs somewhere. If you were interested in contributing and want to hack on the docs then please do. You are in a better position to do that than I am because I've been doing Elasticsearch development with Eclipse for years and have internalized a lot of my workflow to the point where I'm liable to skip things in the documentation because it has become muscle memory.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut a few more queries over to registerQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17553</link><project id="" key="" /><description>Basically just copy-and-paste the fromXContent method and remove the old class.
</description><key id="146126625">17553</key><summary>Cut a few more queries over to registerQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T22:37:08Z</created><updated>2016-05-02T12:15:32Z</updated><resolved>2016-04-06T16:13:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-06T09:20:52Z" id="206252381">pretty much same as https://github.com/elastic/elasticsearch/pull/17552#issuecomment-206236303 LGTM based on trust besides two minor comments on testing
</comment><comment author="nik9000" created="2016-04-06T16:13:13Z" id="206445740">Merged! Thanks for the tip on the testing @javanna ! I see light at the end of this tunnel.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cut a dozen or so queries to registerQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17552</link><project id="" key="" /><description>Basically just copy-and-paste the fromXContent method and remove the old class.
</description><key id="146126446">17552</key><summary>Cut a dozen or so queries to registerQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T22:36:31Z</created><updated>2016-05-02T12:15:36Z</updated><resolved>2016-04-06T15:34:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-06T09:00:19Z" id="206236303">I think reviewing this change is only a matter of trust. I trust that you moved the `fromXContent` method and all the parse fields from each query parser to its corresponding query builder (cut and paste work), while making no other changes meanwhile. In that case this LGTM :)
</comment><comment author="nik9000" created="2016-04-06T12:34:42Z" id="206350603">Fair enough.
</comment><comment author="nik9000" created="2016-04-06T15:35:09Z" id="206432659">Merged on pure trust.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add _cat/tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17551</link><project id="" key="" /><description>Adds new _cat endpoint that lists all tasks
</description><key id="146120557">17551</key><summary>Add _cat/tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:CAT API</label><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T22:16:13Z</created><updated>2016-04-11T19:49:41Z</updated><resolved>2016-04-07T15:30:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-06T00:22:36Z" id="206045154">LGTM
</comment><comment author="drewr" created="2016-04-11T19:49:41Z" id="208526652">:joy_cat: :heart: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard stuck in STARTED state when taking snapshot.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17550</link><project id="" key="" /><description>Elasticsearch version: 
1.6.0

JVM version:
1.8.0_45-internal

Description of the problem including expected versus actual behavior:
When taking a snapshot (using an s3 repository), one of my shards is being snapshotted extremely slowly compared to the other shards in the index (and they are of comparable size). I've tried deleting the snapshot and trying again, but the problem persists. 

Provide logs (if relevant):
Here is the output of `GET /_snapshot/snapshot_prod/snapshot_16/_status` (node shard 2 in "prod"):

```
{
   "snapshots": [
      {
         "snapshot": "snapshot_16",
         "repository": "snapshot_prod",
         "state": "STARTED",
         "shards_stats": {
            "initializing": 0,
            "started": 1,
            "finalizing": 0,
            "done": 15,
            "failed": 0,
            "total": 16
         },
         "stats": {
            "number_of_files": 437,
            "processed_files": 366,
            "total_size_in_bytes": 89068328004,
            "processed_size_in_bytes": 65917540329,
            "start_time_in_millis": 1459881018320,
            "time_in_millis": 682146
         },
         "indices": {
            "qa": {
               "shards_stats": {
                  "initializing": 0,
                  "started": 0,
                  "finalizing": 0,
                  "done": 5,
                  "failed": 0,
                  "total": 5
               },
               "stats": {
                  "number_of_files": 8,
                  "processed_files": 8,
                  "total_size_in_bytes": 13196,
                  "processed_size_in_bytes": 13196,
                  "start_time_in_millis": 1459881018320,
                  "time_in_millis": 808
               },
               "shards": {
                  "0": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018320,
                        "time_in_millis": 474
                     }
                  },
                  "1": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 4,
                        "processed_files": 4,
                        "total_size_in_bytes": 12880,
                        "processed_size_in_bytes": 12880,
                        "start_time_in_millis": 1459881018324,
                        "time_in_millis": 489
                     }
                  },
                  "2": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018325,
                        "time_in_millis": 458
                     }
                  },
                  "3": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018320,
                        "time_in_millis": 448
                     }
                  },
                  "4": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018322,
                        "time_in_millis": 806
                     }
                  }
               }
            },
            "prod": {
               "shards_stats": {
                  "initializing": 0,
                  "started": 1,
                  "finalizing": 0,
                  "done": 2,
                  "failed": 0,
                  "total": 3
               },
               "stats": {
                  "number_of_files": 421,
                  "processed_files": 350,
                  "total_size_in_bytes": 89068309560,
                  "processed_size_in_bytes": 65917521885,
                  "start_time_in_millis": 1459881018328,
                  "time_in_millis": 682138
               },
               "shards": {
                  "0": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 71,
                        "processed_files": 71,
                        "total_size_in_bytes": 6022934925,
                        "processed_size_in_bytes": 6022934925,
                        "start_time_in_millis": 1459881018328,
                        "time_in_millis": 682138
                     }
                  },
                  "1": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 123,
                        "processed_files": 123,
                        "total_size_in_bytes": 4443650351,
                        "processed_size_in_bytes": 4443650351,
                        "start_time_in_millis": 1459881018328,
                        "time_in_millis": 494598
                     }
                  },
                  "2": {
                     "stage": "STARTED",
                     "stats": {
                        "number_of_files": 227,
                        "processed_files": 156,
                        "total_size_in_bytes": 78601724284,
                        "processed_size_in_bytes": 55450936609,
                        "start_time_in_millis": 1459881018332,
                        "time_in_millis": 0
                     },
                     "node": "ItiaPMQhQiOVl3IMqabfeQ"
                  }
               }
            },
            "library": {
               "shards_stats": {
                  "initializing": 0,
                  "started": 0,
                  "finalizing": 0,
                  "done": 3,
                  "failed": 0,
                  "total": 3
               },
               "stats": {
                  "number_of_files": 3,
                  "processed_files": 3,
                  "total_size_in_bytes": 4853,
                  "processed_size_in_bytes": 4853,
                  "start_time_in_millis": 1459881018323,
                  "time_in_millis": 1474
               },
               "shards": {
                  "0": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 1635,
                        "processed_size_in_bytes": 1635,
                        "start_time_in_millis": 1459881018983,
                        "time_in_millis": 814
                     }
                  },
                  "1": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 1739,
                        "processed_size_in_bytes": 1739,
                        "start_time_in_millis": 1459881018323,
                        "time_in_millis": 818
                     }
                  },
                  "2": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 1479,
                        "processed_size_in_bytes": 1479,
                        "start_time_in_millis": 1459881018327,
                        "time_in_millis": 670
                     }
                  }
               }
            },
            "edmarket_dev": {
               "shards_stats": {
                  "initializing": 0,
                  "started": 0,
                  "finalizing": 0,
                  "done": 5,
                  "failed": 0,
                  "total": 5
               },
               "stats": {
                  "number_of_files": 5,
                  "processed_files": 5,
                  "total_size_in_bytes": 395,
                  "processed_size_in_bytes": 395,
                  "start_time_in_millis": 1459881018322,
                  "time_in_millis": 911
               },
               "shards": {
                  "0": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018322,
                        "time_in_millis": 721
                     }
                  },
                  "1": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018322,
                        "time_in_millis": 911
                     }
                  },
                  "2": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018322,
                        "time_in_millis": 697
                     }
                  },
                  "3": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018322,
                        "time_in_millis": 744
                     }
                  },
                  "4": {
                     "stage": "DONE",
                     "stats": {
                        "number_of_files": 1,
                        "processed_files": 1,
                        "total_size_in_bytes": 79,
                        "processed_size_in_bytes": 79,
                        "start_time_in_millis": 1459881018322,
                        "time_in_millis": 655
                     }
                  }
               }
            }
         }
      }
   ]
}
```

It continues to snapshot more bytes each time I re-run the `_status` command, but the # changes extremely slowly, so I want to make sure nothing is wrong with this node. Happy to provide more info &amp; log output as necessary.

Thanks!
</description><key id="146104283">17550</key><summary>Shard stuck in STARTED state when taking snapshot.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshreback</reporter><labels /><created>2016-04-05T20:45:33Z</created><updated>2016-04-06T11:42:45Z</updated><resolved>2016-04-06T11:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-04-05T22:46:44Z" id="206022691">@joshreback for whatever reason (custom routing, perhaps?) this shard is more than 10 times larger than other 2 shards. Shard 0 is 4g, shard 1 is 6g and shard 2 is 78g. It takes time to snapshot 78g. If your node is in AWS and has good network connection you can try changing throttling, but be careful not to overload the nodes in your cluster.
</comment><comment author="clintongormley" created="2016-04-06T11:42:45Z" id="206329074">Sounds like this issue can be closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Root rest api delegates to a transport action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17549</link><project id="" key="" /><description>This change makes the root (/) rest api delegate to a transport action to get the
data for the response. This aligns this rest api with all of the other apis, which
delegate to one or more actions.

In doing this, unit tests were added to provide coverage of the RestMainAction
and the associated classes.
</description><key id="146066420">17549</key><summary>Root rest api delegates to a transport action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T18:13:25Z</created><updated>2016-04-07T14:05:38Z</updated><resolved>2016-04-07T14:05:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-06T00:11:34Z" id="206042933">Left one comment, otherwise LGTM.
</comment><comment author="javanna" created="2016-04-06T07:47:32Z" id="206189815">left a couple of comments, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove BulkIndexByScrollResponseContentListener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17548</link><project id="" key="" /><description>Its causes the reindex and update_by_query to be the only API that returns a body on errors thats not a  structured error.

It also tries to do the right thing by setting the status code to the highest statuscode of all the individual failures which I don't think is terribly useful to consumers either. 

It be nicer if these API's behaved like the `_bulk` and return a 200 if the request was received properly and each response has to handle individual failures (whether conflicts=proceed or not) with an `errors` boolean.
</description><key id="146055533">17548</key><summary>Remove BulkIndexByScrollResponseContentListener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Reindex API</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-05T17:27:55Z</created><updated>2016-07-27T17:57:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-05T19:42:50Z" id="205963140">&gt; Its causes the reindex and update_by_query to be the only API that returns a body on errors thats not a structured error.

This I'm happy to fix but I don't really understand.

&gt; It also tries to do the right thing by setting the status code to the highest statuscode of all the individual failures which I don't think is terribly useful to consumers either.

This I really really really want to keep. Not only does it handle important cases like "your query failed for some reason" but I really like returning a REST status code when the request terminated early that explains why. Returning 200 when we bailed early isn't cool.
</comment><comment author="Mpdreamz" created="2016-04-05T20:16:19Z" id="205973849">Most responses on a bad status code (400, 409, 500 etcetera) will return the throwable as structured `error` instead of the same response as on a 200.

&gt; Returning 200 when we bailed early isn't cool.

But is exactly what we do for `_bulk`, `_mget` `_msearch` and the likes. They return a `200` with a `valid json response strucure` with`"errors": true`. Not doing this for `reindex` and `update by query` makes them the odd ones out and also the only ones that'll return a `valid json response structure` when the http status code is not 200. 

I personally agree that a status code indicating a (partial) failure for **all** of these API's would be an improvement but we'd need to settle on a single status code (e.g 400) rather then stealing a status code from one of the individual sub request failures. 
</comment><comment author="nik9000" created="2016-04-05T20:33:30Z" id="205978858">I don't think `_reindex` and `_update_by_query` can just be like `_bulk`, _`mget`, and `_msearch`. The latter APIs still attempt all the things you asked them to do even if there was a failure. The former don't. Personally I'm not happy that we return 200 when those APIs only partially succeed but that is a battle for another time.

Is the trouble the reindex doesn't return a single error field like the other APIs? If it did that would it make everything work better? We could cook up something like "aborted due to other errors error" if it'd make things fit.

I think it is a good thing reindex returns its progress when it aborts due to an error because that data is useful. Well, not all of it is, but stuff like the total hits and how many of each operation in performed is.
</comment><comment author="Mpdreamz" created="2016-04-05T21:14:34Z" id="205986471">&gt; I don't think _reindex and _update_by_query can just be like _bulk, _mget, and _msearch. The latter APIs still attempt all the things you asked them to do even if there was a failure. 

That does not mean _reindex and update_by_query stops at the first error, a batch can have multiple failures and it that sense is exactly like the `_bulk` etcerera in my opinion. 

And because it can have multiple failures picking the `highest` is just as meaningless as `200`. 

&gt; Is the trouble the reindex doesn't return a single error field like the other APIs? If it did that would it make everything work better?

I rather we make it behave the same as mget and family, either by return a 200 with `"errors" : true` or a common single http status code for all of these API's with (partial) failures

&gt; I think it is a good thing reindex returns its progress when it aborts due to an error because that data is useful. Well, not all of it is

I agree!, the exception that is rendered on `"error"` can include more data by overriding `innerToXContent` [like the SearchPhaseExecutionException does](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java#L137)

From a pure RESTful perspective i'd agree the current API is better, right now though these are the only API's I have to special case. In our client we have a dedicated error reader for bad responses that'll shortcircuit deserialization alltogether. Whilst not something we can not work around I rather treat all elasticsearch responses the same.
</comment><comment author="clintongormley" created="2016-07-08T10:26:13Z" id="231327241">Discussed in Fix it Friday.  A bulk or mget operation is a wrapper of individual items, so it makes sense for a bulk operation with errors to return 200 (with errors listed in the body) because the coordinating node has done its job correctly.

Reindex on the other hand aborts when it encounters an error, and should return an error code.  I agree with @Mpdreamz that this should be a `400` or a `500` (depending on the error that was thrown) but it shouldn't just choose the highest error code.  These error codes have meaning for the clients (eg should i retry this request) and just because one error code happens to be `429` or `503` doesn't mean that the client should retry the reindexing request.

Regarding the structured body...  I think the information that is passed back is useful, but it is inconsistent with some responses (eg bulk) but inconsistent with others.  I considered wrapping the info inside a structured `error` body, but that feels forced.

My Perl client looks for a structured `error` but, failing that, falls back to including the whole body in the exception.  I'm not sure what other clients do here.
</comment><comment author="nik9000" created="2016-07-08T16:22:33Z" id="231404656">So what is the change you want? Change the response code calculations to be "return the code if all errors have the same code, if two errors have different codes and any are 500-level then return 500. Otherwise return 400?" 
</comment><comment author="clintongormley" created="2016-07-08T16:43:47Z" id="231410317">No, return 500 if any errors are 50*, or 400 otherwise.  (open to discussion here).
</comment><comment author="nik9000" created="2016-07-08T16:46:14Z" id="231410889">Ok, so:
0. If there are no errors return 200.
1. If there are any 500 level statuses then return 500.
2. Return 400.
</comment><comment author="clintongormley" created="2016-07-08T16:49:02Z" id="231411557">i think this makes sense.  @Mpdreamz ?
</comment><comment author="nik9000" created="2016-07-20T16:59:04Z" id="234012225">@Mpdreamz, do you like my proposal in https://github.com/elastic/elasticsearch/issues/17548#issuecomment-231410889 ?
</comment><comment author="Mpdreamz" created="2016-07-20T17:32:42Z" id="234021847">+1 my main remaining concern is that the response in case of an error (500/400) will not be in the form of a serialized exception.

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/action/search/SearchPhaseExecutionException.java#L137

That exception also attaches useful data, is there anyway we can introduce a general `ReindexFailedException` of some sorts that gets thrown that also sets all the related useful data? 

That would bring the behaviour in line with existing API's
</comment><comment author="clintongormley" created="2016-07-21T11:03:30Z" id="234222834">@Mpdreamz what would this exception look like?

I tried to reorganise the response to be a structured exception while maintaining the useful info that is returned, but didn't like what I ended up with.  Can you suggest something?
</comment><comment author="Mpdreamz" created="2016-07-21T14:11:10Z" id="234265369">How individual bulk items failures response looks like (409 lifted from failures)

``` json
{
  "took" : 654,
  "timed_out" : false,
  "total" : 2,
  "updated" : 0,
  "created" : 0,
  "deleted" : 0,
  "batches" : 1,
  "version_conflicts" : 2,
  "noops" : 0,
  "retries" : {
    "bulk" : 0,
    "search" : 0
  },
  "throttled_millis" : 0,
  "requests_per_second" : "unlimited",
  "throttled_until_millis" : 0,
  "failures" : [
    {
      "index" : "nest-09ffd83a-clone",
      "type" : "test",
      "id" : "1",
      "cause" : {
        "type" : "version_conflict_engine_exception",
        "reason" : "[test][1]: version conflict, document already exists (current version [1])",
        "index_uuid" : "dwwlqnkfTfaq1zefjq0XnA",
        "shard" : "3",
        "index" : "nest-09ffd83a-clone"
      },
      "status" : 409
    },
    {
      "index" : "nest-09ffd83a-clone",
      "type" : "test",
      "id" : "2",
      "cause" : {
        "type" : "version_conflict_engine_exception",
        "reason" : "[test][2]: version conflict, document already exists (current version [1])",
        "index_uuid" : "dwwlqnkfTfaq1zefjq0XnA",
        "shard" : "2",
        "index" : "nest-09ffd83a-clone"
      },
      "status" : 409
    }
  ]
}
```

vs reindexing into the same index (400)

``` json
{
  "error" : {
    "root_cause" : [
      {
        "type" : "action_request_validation_exception",
        "reason" : "Validation Failed: 1: reindex cannot write into an index its reading from [nest-9aced9c5];"
      }
    ],
    "type" : "action_request_validation_exception",
    "reason" : "Validation Failed: 1: reindex cannot write into an index its reading from [nest-9aced9c5];"
  },
  "status" : 400
}
```

vs making a script error (400)

```
{
  "error" : {
    "root_cause" : [
      {
        "type" : "action_request_validation_exception",
        "reason" : "Validation Failed: 1: reindex cannot write into an index its reading from [nest-6893097f];"
      }
    ],
    "type" : "action_request_validation_exception",
    "reason" : "Validation Failed: 1: reindex cannot write into an index its reading from [nest-6893097f];"
  },
  "status" : 400
}
```

Now for all elasticsearch requests we can shortcircuit bad http status codes to a generic error handling mechanism e.g pseudo code

``` F#
type elasticsearch_response = error | response
let response = match status
| bad_response -&gt; error
| _ -&gt; response
```

In our client `response` can be a fully deserialized type but folks can even request the unititialized response stream as `response` so this short circuiting on status code is very very useful.

Except for this special case within the `_reindex` we'd now have to peek wether it has an `error` and if not still return a `response`. Something which for us would be expensive because we deserialize straight from a forward readonly (tcp) stream and can not seek backwards forcing us to do a double read on this API.

Going back on my original +1 for setting a fixed 400 or 500 and still maintain aligning it with bulk would be better, personal preference and agreement that https://github.com/elastic/elasticsearch/issues/17548#issuecomment-231410889 is sementically sound aside.

The only other alternative would be to wrap it in an exception and attach all the data as metadata (which we already do in some other cases as linked earlier. 

``` json
{
  "error" : {
    "root_cause" : [
      {
        "type" : "reindex_conflict_exception",
        "reason" : "Reindexing some items failed"
      }
    ],
    "type" : "reindex_conflict_exception",
    "reason" : "Reindexing some items failed"
    "status" : 400
    "took" : 654,
    "timed_out" : false,
    "total" : 2,
    "updated" : 0,
    "created" : 0,
    "deleted" : 0,
    "batches" : 1,
    "version_conflicts" : 2,
    "noops" : 0,
    "retries" : {
      "bulk" : 0,
      "search" : 0
    },
    "throttled_millis" : 0,
    "requests_per_second" : "unlimited",
    "throttled_until_millis" : 0,
    "failures" : [
      {
        "index" : "nest-09ffd83a-clone",
        "type" : "test",
        "id" : "1",
        "cause" : {
          "type" : "version_conflict_engine_exception",
          "reason" : "[test][1]: version conflict, document already exists (current version [1])",
          "index_uuid" : "dwwlqnkfTfaq1zefjq0XnA",
          "shard" : "3",
          "index" : "nest-09ffd83a-clone"
        },
        "status" : 409
      },
      {
        "index" : "nest-09ffd83a-clone",
        "type" : "test",
        "id" : "2",
        "cause" : {
          "type" : "version_conflict_engine_exception",
          "reason" : "[test][2]: version conflict, document already exists (current version [1])",
          "index_uuid" : "dwwlqnkfTfaq1zefjq0XnA",
          "shard" : "2",
          "index" : "nest-09ffd83a-clone"
        },
        "status" : 409
      }
    ]
  },
}
```

Which I agree @clintongormley feels very icky :/ 

Returning `200` ala bulk introduce an `errors: false|true` seems like the less of two _evils_ in this case and unifies error handling for these related multi document requests.

As mentioned my personal preference would actually also be for the bulk and the likes to return a 400/500 in case of individual item failures. Having to check for `errors: true` is often forgotten but not sure if too open that can of worms on this ticket :smile:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add shuffling xContent to aggregation tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17547</link><project id="" key="" /><description>This adds shuffling of xContent similar to #17521 to the aggregation and pipeline aggregation base test.  This additional shuffling uncovered that some aggregation builders internally store some properties in a way that made the equals() testing fail when the xContent is shuffled. 
For TopHitsAggregatorBuilder, the internal `scriptFields` parameter was changed to a set because the order they appear in the xContent should not matter. For FiltersAggregatorBuilder, the internal list of KeyedFilters is sorted by key now. As a side effect, the keys in the aggregation response are now not always in the same order as the `filters` in the query, but sorted by key as well (unless they are anonymous).
</description><key id="146053780">17547</key><summary>Add shuffling xContent to aggregation tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T17:19:23Z</created><updated>2016-04-06T12:34:15Z</updated><resolved>2016-04-06T12:09:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-05T17:19:47Z" id="205906084">@colings86 since this touches some aggregations, maybe you can take a look?
</comment><comment author="colings86" created="2016-04-06T08:06:51Z" id="206202931">This is a great change. It LGTM but might be worth waiting for @jpountz to make sure he is happy with the latest commit?
</comment><comment author="jpountz" created="2016-04-06T08:58:53Z" id="206235320">LGTM too!
</comment><comment author="cbuescher" created="2016-04-06T10:08:34Z" id="206283357">@colings86 @jpountz thanks, added another small update after your last comments. 
One last question before I merge: I'm not sure if the change in Filter FiltersAggregatorBuilder needs additional documentation somewhere, since currently the order of bucket keys in the response seems to match the order of the filters in the request, but now they are orderes by key. I checked https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-filters-aggregation.html but it only mentions "order as provided in the request" for the anonymous filters case, which we keep, so I think its good. Wdyt?
</comment><comment author="colings86" created="2016-04-06T12:06:35Z" id="206341845">I think it's fine without the note in documentation since we never claimed to do anything with ordering the keyed filter responses and IIRC order of maps is not guaranteed in JSON anyway so it was only a coincidence that this was the case before. The most I would do is add a note in the breaking changes doc but I think even this isn't really necessary.
</comment><comment author="cbuescher" created="2016-04-06T12:08:19Z" id="206342444">thanks @colings86 will merge then.
</comment><comment author="jpountz" created="2016-04-06T12:34:15Z" id="206350487">+1 to not document it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make MappedFieldType responsible for providing a parser/formatter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17546</link><project id="" key="" /><description>Aggregations need to perform instanceof calls on MappedFieldType instances in
order to know how they should be parsed or formatted. Instead, we should let
the field types provide a formatter/parser that can can be used.
</description><key id="146052305">17546</key><summary>Make MappedFieldType responsible for providing a parser/formatter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-04-05T17:13:15Z</created><updated>2016-04-07T14:59:51Z</updated><resolved>2016-04-07T14:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-06T10:45:40Z" id="206303499">@jpountz I left a few comments.
</comment><comment author="jpountz" created="2016-04-06T16:15:46Z" id="206446465">I pushed commits to address your comments.
</comment><comment author="colings86" created="2016-04-07T09:22:23Z" id="206779500">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support wait_for_completion=&lt;time_value&gt;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17545</link><project id="" key="" /><description>It would be super handy if we can supply a `time` value for `wait_for_completion` e.g `2m`

Right now the `reindex` and `update_by_query` are blocking by default and unbounded but most of the clients all have request timeouts. 

It would be awesome if we could specify a time unit after which a `task` property with a task id will be returned just as if we'd specified `wait_for_completion=false` from the get go.

This way clients won't recieve a timed out exception but a 200 that they can act on.
- indices upgrade
- task list
- snapshot_create
- snapshot_restore

Also expose a `wait_for_completion`
</description><key id="146050573">17545</key><summary>Support wait_for_completion=&lt;time_value&gt;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>:Task Manager</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-05T17:07:03Z</created><updated>2016-07-15T10:07:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-04-08T10:09:09Z" id="207359071">We discussed this in Fix-It Friday. First of all we agreed that if a timeout occurs we should never return a 200.
Regarding the wait_for_completion for reindex and update_by_query, we also agreed that wait_for_completion should always be false. This kind of operation can take hours or days so the reindex/update_by_query should answer right away and the TaskManager should be used to wait for the completion of the task (with a poll strategy ?).
</comment><comment author="nik9000" created="2016-04-08T12:26:46Z" id="207411446">&gt; First of all we agreed that if a timeout occurs we should never return a 200.

Timeout occurs where? Like if we supported `wait_for_completion=1m` we should return some non-200 code with the task id after a minute? Sure!

&gt; wait_for_completion should always be false

This is a pretty big reversal from the discussion during reindex's implementation. I wanted wait_for_completion to default to false. @clintongormley and @imotov wanted it to default to true. They argued that it'd be more consistent with the rest of the API - `wait_for_completion` is only supported in some places. Where it is supported you can set it to false and get immediate returns with a task id. But by default those APIs work just like every other API. That argument won me over.

There are plenty of cases where reindex and update_by_query make sense defaulting to false. If you are only modifying tens of thousands of documents, for instance. Or if you don't have a proxy between you and the server - like if you are running on localhost. I think that'll be pretty common actually.

Right now `wait_for_completion=false` is the only way to get the final result. Since it isn't stored anywhere. This is certainly a defect that we'll have to fix soon, but it is a consideration. So we can't "always" make it false.
</comment><comment author="nik9000" created="2016-04-08T12:28:31Z" id="207411905">&gt; Like if we supported wait_for_completion=1m we should return some non-200 code with the task id after a minute? Sure!

202 looks like the code we should use any time we return a task id and the task is still processing, regardless of if we support some timeout on waiting for completion. It fits super well.
</comment><comment author="jimczi" created="2016-04-08T14:07:29Z" id="207445525">&gt; This is a pretty big reversal from the discussion during reindex's implementation

Doesn't have to be ;) We discussed about long running tasks in general where wait_for_completion should/must always be set to false. That's all.

&gt; Timeout occurs where? Like if we supported wait_for_completion=1m we should return some non-200 code with the task id after a minute? Sure!

yep exactly.

&gt; There are plenty of cases where reindex and update_by_query make sense defaulting to false. If you are only modifying tens of thousands of documents, for instance. Or if you don't have a proxy between you and the server - like if you are running on localhost. I think that'll be pretty common actually.

I guess you mean defaulting to true ?
Yes for playground stuff it's simpler that way but it's not recommended ;) ?

&gt; Right now wait_for_completion=false is the only way to get the final result. Since it isn't stored anywhere. This is certainly a defect that we'll have to fix soon, but it is a consideration. So we can't "always" make it false.

wait_for_completion=true ? 
So if the sender does not wait for completion and checks the task manager afterward it would be impossible to determine if the task was successful or not ?
</comment><comment author="nik9000" created="2016-04-08T14:14:28Z" id="207447469">&gt; long running tasks in general where wait_for_completion should/must always be set to false

That is certainly different from how we'd talked about before. I can see the merit in defaulting long running tasks to false to "teach users what they'll need to do at scale" but I also see the merit in the argument that we should just be consistent and always wait unless asked to do otherwise.

I'm totally against taking away the option for people to set `wait_for_completion=true` if they want it.

&gt; playground stuff

I dunno. You can crank through a couple hundred thousand documents in well under the time most things would timeout. That is a fairly big playground. And if you don't have a proxy between you and Elasticsearch you can wait as long as you'd like. I think that is pretty common for this kind of thing.

&gt; So if the sender does not wait for completion and checks the task manager afterward it would be impossible to determine if the task was successful or not ?

All you know is "the task is gone" there is an open issue for it but I don't have it handy. We'll be doing _something_ to persist the results after completion. Probably also something to persist the results while it is running in case the node drops out.
</comment><comment author="bleskes" created="2016-04-08T14:43:07Z" id="207458875">&gt; I'm totally against taking away the option for people to set wait_for_completion=true if they want it.

+1 on keeping the option to wait. We assume too much about what people will do with this API. Remember that update by query can be used with great success on 10 docs as well.  This API doesn't add any complexity on our side.

Re the default  - I think we need to think about the inexperienced users first. Trying out the API for the first time, it's very confusing if it just "comes back". To me, waiting first and then looking at docs to see you don't have to wait is better learning curve. 

+1 on returning 202 on timeout.
</comment><comment author="jimczi" created="2016-04-08T14:45:35Z" id="207459834">&gt; I also see the merit in the argument that we should just be consistent and always wait unless asked to do otherwise.
&gt; I'm totally against taking away the option for people to set wait_for_completion=true if they want it.

I am not arguing against the default. I agree that the default should be true but for some use cases we should advise the user to manually set it to 'false'. Though maybe not now if the "the task is gone" issue is not resolved ;) 
</comment><comment author="nik9000" created="2016-04-08T14:55:32Z" id="207466480">&gt; for some use cases we should advise the user to manually set it to 'false'.

We should certainly have stronger language around that in the docs!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Preserve failures on reindex and update_by_query when conflicts=proceed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17544</link><project id="" key="" /><description>```
POST /nest-8af80b80/test/_update_by_query?pretty=true&amp;conflicts=proceed
```

will return a `200` with:

``` json
{
  "took" : 20,
  "timed_out" : false,
  "total" : 1,
  "updated" : 0,
  "batches" : 1,
  "version_conflicts" : 1,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ ]
}
```

while 

`POST /nest-41ca4794/test/_update_by_query?pretty=true`

will return a `409` with

``` json
{
  "took" : 3,
  "timed_out" : false,
  "total" : 1,
  "updated" : 0,
  "batches" : 1,
  "version_conflicts" : 1,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ {
    "index" : "nest-6fee3631",
    "type" : "test",
    "id" : "1",
    "cause" : {
      "type" : "version_conflict_engine_exception",
      "reason" : "[test][1]: version conflict, current [2], provided [1]",
      "shard" : "3",
      "index" : "nest-6fee3631"
    },
    "status" : 409
  } ]
}
```

It'd be nice if failures are exposed in both cases.
</description><key id="146048966">17544</key><summary>Preserve failures on reindex and update_by_query when conflicts=proceed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2016-04-05T17:01:32Z</created><updated>2016-04-05T19:57:02Z</updated><resolved>2016-04-05T19:57:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-05T19:39:06Z" id="205961931">We talked about this in the initial reindex implementation. We didn't want to do more than count the failures when `conflicts=proceed` because the number of failures can grow huge. It is possible to preserve the first N of them if we have a use case for it, but we'll never be able to preserve all of them.
</comment><comment author="Mpdreamz" created="2016-04-05T19:57:02Z" id="205968159">Makes sense
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exploit DiscoveryNode immutability in toString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17543</link><project id="" key="" /><description>DiscoveryNode is immutable yet we rebuild DiscoveryNode#toString on
every invocation. Most importantly, this just leads to unnecessary
allocations. This is most germane to ZenDiscovery and the processing of
cluster states where DiscoveryNode#toString is invoked when submitting
update tasks and processing cluster state updates.
</description><key id="146038756">17543</key><summary>Exploit DiscoveryNode immutability in toString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>non-issue</label></labels><created>2016-04-05T16:19:25Z</created><updated>2016-04-12T14:29:41Z</updated><resolved>2016-04-05T23:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-05T16:24:48Z" id="205882922">LGTM
</comment><comment author="bleskes" created="2016-04-05T17:03:23Z" id="205897177">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes Filter and FiltersAggregation to work with empty query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17542</link><project id="" key="" /><description>This fix ensures the filter and filters aggregation will not throw a NPE when `{}` is passed in as a filter. Instead `{}` is interpreted as a MatchAllDocsQuery.

Closes #17518
</description><key id="146021412">17542</key><summary>Fixes Filter and FiltersAggregation to work with empty query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T15:17:09Z</created><updated>2017-05-12T10:42:44Z</updated><resolved>2016-04-05T16:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-05T15:19:43Z" id="205854889">LGTM
</comment><comment author="Mpdreamz" created="2017-05-11T13:53:49Z" id="300796197">This might be a feature not a bug but `6.0.0-alpha1` started throwing on this again:

```json
{
  "aggs": {
    "empty_filter": {
      "filter": {}
    }
  }
}
```


```json
{
  "error" : {
    "root_cause" : [
      {
        "type" : "illegal_argument_exception",
        "reason" : "query malformed, empty clause found at [4:18]"
      }
    ],
    "type" : "illegal_argument_exception",
    "reason" : "query malformed, empty clause found at [4:18]"
  },
  "status" : 400
}
```






</comment><comment author="javanna" created="2017-05-12T10:42:44Z" id="301044644">@Mpdreamz it is indeed a feature :) See #22092 . Support for empty queries was deprecated in 5.0 and removed in 6.0.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for EmptyQueryBuilder at parse time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17541</link><project id="" key="" /><description>At the moment if `{}` is specified as a query we convert it to an `EmptyQueryBuilder` object and expect whatever calls `QueryBuilder#toQuery` to check for null and handle it appropriately. There are bugs such as https://github.com/elastic/elasticsearch/issues/17518 where we did not realise/remember to add this check. Depending on the outcome of https://github.com/elastic/elasticsearch/issues/17540 and at the very least during the deprecation period if it is accepted, it would be good to improve the handling by moving this check to the parsing stage.

When parsing compound queries and when parsing the top level query object we should check whether the inner query is an instance of `EmptyQueryBuilder` and ignore it, convert it or throw an exception at that point. We should also then make `EmptyQueryBuilder#toQuery()` throw an `UnsupportedOperationException` with an appropriate message so it is easy to see what the problem is if it does get called
</description><key id="146015084">17541</key><summary>Check for EmptyQueryBuilder at parse time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>discuss</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-04-05T14:56:19Z</created><updated>2016-06-17T10:00:42Z</updated><resolved>2016-06-17T10:00:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-05T15:20:57Z" id="205855302">+1 as I said in the other issue this seems logical and I am now wondering why we didn't do it in the first place. Let's see what we find on our way when trying again.
</comment><comment author="cbuescher" created="2016-06-17T10:00:42Z" id="226730682">Handling empty clauses at parse time was added with https://github.com/elastic/elasticsearch/pull/17624. This includes deprecation logging and throwing an exception for empty clauses when parsing is strict. This gives us the ability to remove empty clauses in 6.0 as discussed in https://github.com/elastic/elasticsearch/issues/17540.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate empty query bodies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17540</link><project id="" key="" /><description>The EmptyQueryBuilder deals with the case where a query is specified in JSON as `{}`. This is not actually a valid query and we handle it differently depending on what context it is used (it may be ignored, converted to MatchAllQuery, convert to MatchNoneQuery, or an exception thrown. 

IMO we should consider `{}` an invalid query and throw an exception when parsing to prevent the hacky logic we have that creates this placeholder object and forces the parent to decide what to do with it (creating issues like https://github.com/elastic/elasticsearch/issues/17518 when we forget to add the hack to handle it). So the following search request would be deprecated:

```
GET _search
{
    "query": {
    }
}
```

as would: 

```
GET _search
{
    "query": {
        "boolean": {
            "must": [
                {}
            ]
        }
    }
}
```

@javanna and @cbuescher mentioned that they thought it was included in the first place because of clients using templates which add a query object to the JSON without first checking if they have any query to put into it. I would have thought this can be easily fixed in most templating languages so it can check if there is an object to put inside the query object before creating it.

I would like to deprecate the EmptyQueryBuilder (which actually isn't part of the Java API anyway) and the use of `{}` as a query, but wanted to open the discuss up to see what others thought and if there are any other known reasons for keeping the EmptyQueryBuilder around
</description><key id="146012628">17540</key><summary>Deprecate empty query bodies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>v5.0.0-alpha4</label></labels><created>2016-04-05T14:49:14Z</created><updated>2016-11-29T18:54:57Z</updated><resolved>2016-11-29T18:54:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-05T15:18:45Z" id="205854518">Thanks for opening this, I am +1 on deprecating on 5.0 and removing on 6.0. We can introduce a deprecation warning (or fail in strict mode) when using `{}`. That still would leave us needing to better handle these situations for the 5.x series as the current behaviour is trappy. We discussed with Colin and Christoph that we could convert the empty brackets to something else at parse time already, so that we wouldn't have to serialize this weird query and keep track of the fact that there were empty brackets all the way to the shards where the query is converted to a lucene query. I am now wondering though why we didn't do that in the first place, maybe there were other obstacles...
</comment><comment author="nik9000" created="2016-04-05T19:34:37Z" id="205960674">I remember #13406 being similar to this discussion.
</comment><comment author="cbuescher" created="2016-04-05T21:21:50Z" id="205988046">I took a first look at this, it's going to be a bit more complicated but I think at this point we can get rid of the EmptyQueryBuilder while still beeing able to support the empty objects syntax `{ }` until we deprecate that at some point in the future. When we added EmptyQueryBuilder we were still on Java 7, with Java 8 we have Optional, which I think is very apropriate here. We can replace EmptyQueryBuilder by an empty Optional I think and then handle all we need to handle at parse time.
</comment><comment author="javanna" created="2016-04-05T21:24:37Z" id="205989217">@nik9000 note that `{}` currently means either match_all, match_none or ignore, which we don't have a query for, it all depends on the context.
</comment><comment author="nik9000" created="2016-04-05T21:38:07Z" id="205994960">&gt; means either match_all, match_none or ignore

Sounds crazy. I vote that people with weird templating concerns just emit match_all or match_none where appropriate. I believe those are always rewritten such that they are free. If they aren't then we should fix that.
</comment><comment author="colings86" created="2016-04-08T10:17:06Z" id="207360985">Discussed in FixItFriday and agreement was we should remove this but we wanted to get opinions form @rashidkpc (in terms of Kibana) and @clintongormley as well. We should also decide if we want to make this a hard break in 5.0 (since it leads to a buggy software) or deprecate in 5.0 and remove in 6.0.
</comment><comment author="javanna" created="2016-04-08T10:34:32Z" id="207367918">I think removing in 5.0 is too much for users, although it's bad practice `{}` is probably used in lots of applications out there. I would emit deprecation warnings instead (and fail on strict mode like we do in other cases) when using `{}` in 5.0, then remove support for it everywhere in 6.0. 
</comment><comment author="rashidkpc" created="2016-04-08T14:38:51Z" id="207456605">As far as I know, we don't do this anywhere in kibana, however I agree with @javanna, deprecate and log warnings in 5.0, remove in 6.0
</comment><comment author="clintongormley" created="2016-04-18T09:44:53Z" id="211300608">I'm actually leaning towards keeping this, as it makes building up the DSL easier on the client side, eg you start with:

```
{ 
  "bool": {
    "must": [],
    "should": [],
    "must_not": [],
    "filter": [],
  }
}
```

And then you can just push clauses onto (eg) `bool.must` or `bool.filter'  as needed.
</comment><comment author="javanna" created="2016-04-18T09:50:06Z" id="211303491">@clintongormley I think your example would still work, the behaviour you describe with empty arrays is very specific to the bool query. What we want to get rid of is the fact that an empty query has a meaning, which actually depends on where it is placed. For instance bool query will ignore the empty must clause completely when presented as follows:

```
{ 
  "bool": {
    "must": [
      {}
    ]
  }
}
```

We would like to instead return an error at all times when receiving `{}`. Makes sense?
</comment><comment author="cbuescher" created="2016-04-18T09:55:17Z" id="211304809">Also things like `"constant_score" : { "filter" : { } }` shouldn't work, this doesn't really make sense and is unnecessary. In each of these cases we should decide and document properly what an empty clause like this means (in this case maybe `match_all` ?) instead of having to delay and bubble up that decision to the parent query.
</comment><comment author="clintongormley" created="2016-04-18T11:13:41Z" id="211334100">Makes sense
</comment><comment author="cbuescher" created="2016-04-19T13:38:32Z" id="211925978">@colings86 @clintongormley if we lean towards keeping empty claues or deprecation in 5.0 + removal in 6.0, I'd like to propose to get https://github.com/elastic/elasticsearch/pull/17624 in. It removes the need for representing the empty clause internally but still keeps the empty clause syntax in the DSL, adding a way for logging or throwing errors if we want to deprecate it.
</comment><comment author="clintongormley" created="2016-04-20T11:35:13Z" id="212389813">@cbuescher sounds good to me
</comment><comment author="javanna" created="2016-11-29T18:54:57Z" id="263662971">The deprecation of empty query bodies went in with #17624 . We can close this.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split reindex and update by query failures exposed on the REST interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17539</link><project id="" key="" /><description>Right now we expose `BulkItemResponse.Failure` and `ShardSearchFailures` under a single `failures`  property but these classes are of different shapes.

 e.g one of the failures writes the throwable as `cause` the other as `reason`.

It would be nice if we can expose them under `index_failures` and `search_failures`, the way they are stored on the `BulkIndexByScrollResponse`. 

Or come up with a single `BulkIndexByScrollResponse.Failure`

I am wondering what the development guidelines/best practices are for exposing failures?  We have quite a few `Failure` inner classes that do not relate to one another?

cc @nik9000 
</description><key id="146003790">17539</key><summary>Split reindex and update by query failures exposed on the REST interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Reindex API</label><label>breaking</label><label>v6.0.0</label></labels><created>2016-04-05T14:17:29Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-05T19:51:56Z" id="205966444">I think we'd be better off standardizing on things like `cause` and `reason`. Then indexing failure would look "just like" a search failure with some extra data about the indexing operation that failed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Require executor name when calling scheduleWithFixedDelay</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17538</link><project id="" key="" /><description>The ThreadPool#scheduleWithFixedDelay method does not make it clear that all scheduled runnable instances will be run on the scheduler thread. This becomes problematic if the actions being performed include blocking operations since there is a single thread and tasks may not get executed due to a blocking task.

This change includes a few different aspects around trying to prevent this situation. The first is that
the scheduleWithFixedDelay method now requires the name of the executor that should be used to execute the runnable. All existing calls were updated to use Names.SAME to preserve the existing behavior.

The second aspect is the removal of using ScheduledThreadPoolExecutor#scheduleWithFixedDelay in favor of a custom runnable, ReschedulingRunnable. This runnable encapsulates the logic to deal with rescheduling a runnable with a fixed delay and mimics the behavior of executing using a ScheduledThreadPoolExecutor and provides a ScheduledFuture implementation that also mimics that of the type returned by a ScheduledThreadPoolExecutor.

Finally, an assertion was added to BaseFuture to detect blocking calls that are being made on the scheduler thread.
</description><key id="145982005">17538</key><summary>Require executor name when calling scheduleWithFixedDelay</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha5</label></labels><created>2016-04-05T13:03:55Z</created><updated>2017-04-10T09:00:04Z</updated><resolved>2016-07-19T17:03:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-08T10:28:17Z" id="207364755">Thx @jaymode . I left some comment and a suggestion
</comment><comment author="jaymode" created="2016-04-15T11:53:44Z" id="210436849">@bleskes I pushed a new commit to address your feedback
</comment><comment author="dakrone" created="2016-04-29T01:01:11Z" id="215604885">LGTM, left one suuuuper minor comment
</comment><comment author="bleskes" created="2016-05-11T21:48:01Z" id="218600466">@jaymode what's the state of this?
</comment><comment author="jasontedor" created="2016-05-12T12:01:07Z" id="218736718">&gt; @jaymode what's the state of this?

I'm taking over this PR.
</comment><comment author="jaymode" created="2016-05-20T13:17:51Z" id="220602893">@bleskes I merged master to bring this back up to date. I am waiting for a second review, maybe you can take a look to ensure I addressed all of your comments?
</comment><comment author="bleskes" created="2016-06-01T14:29:05Z" id="223010167">@jaymode the implementation looks great. The tests need some tightening up as discussed. Also, I know it was previous behavior, but I think make scheduleWithFixedDelay reschedule on error will make it simpler to use, do what we expect of it (we currently don't deal with errors right) and allows us to use it in more places and make things like refresh scheduling code simpler. Can we move to that model? Again, thanks a lot for picking this up.
</comment><comment author="jaymode" created="2016-06-09T11:34:48Z" id="224869603">@bleskes I made the changes we discussed. If an exception occurs, the task will still be rescheduled and tried to make the tests more evil
</comment><comment author="bleskes" created="2016-06-20T09:56:19Z" id="227099458">@jaymode looks great. Thx! I left some suggestions to improve testing and one semantic change (cancel on rejection)
</comment><comment author="jaymode" created="2016-07-18T10:51:50Z" id="233298550">@bleskes this is ready for review again. I addressed all of your previous feedback
</comment><comment author="bleskes" created="2016-07-19T13:02:34Z" id="233625218">LGTM! left one nit. Thanks so much Jay!
</comment><comment author="jaymode" created="2016-07-27T11:00:33Z" id="235553840">This was back-ported to the 2.4 branch in https://github.com/elastic/elasticsearch/commit/ebd900e955f0bb09bf2cec24a1816ed7728307fc
</comment><comment author="nithril" created="2017-04-10T07:55:32Z" id="292876050">My Spring scheduler thread prefix contains `scheduler` hence I get an assertion error from elastic. The comparison seems to me a bit too broad.  Maybe the elastic thread pool prefix can be more specific.</comment><comment author="bleskes" created="2017-04-10T08:40:22Z" id="292885677">@nithril can you elaborate more about your setup? are you using the transport client in your spring application? </comment><comment author="nithril" created="2017-04-10T08:54:25Z" id="292888996">Elastic 2.4.1
Java 1.8

I'm using a `NodeClient` (used for the integration testing). Actually, the exception occurs in my test after I renamed the Quartz scheduler. A scheduled task is executed which performs a blocking search.

The stack is the following: 
```
Caused by: java.lang.AssertionError: Expected current thread [Thread[website-scheduler_Worker-2,5,main]] to not be the scheduler thread. Reason: [Blocking operation]
	at org.elasticsearch.threadpool.ThreadPool.assertNotScheduleThread(ThreadPool.java:1089)
	at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:119)
	at java_util_concurrent_Future$get.call(Unknown Source)
```</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17537</link><project id="" key="" /><description>Hi guys,

we have upgraded ElasticSearch from 2.3.0 and reindexed our geolocations so the latitude and longitude are stored separately. We have noticed that some of our visualisation started to fail after we add a filter based on geolocation rectangle. However, map visualisation are working just fine. The problem occurs when we include actual documents. In this case, we get some failed shards (usually 1 out of 5) and error: Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?).

Details:
Our geolocation index is based on:

```
"dynamic_templates": [{
....
{
        "ner_geo": {
          "mapping": {
            "type": "geo_point",
            "lat_lon": true
          },
          "path_match": "*.coordinates"
        }
      }],
```

The ok query with the error is as follows. If we change the query size to 0 (map visualizations example), the query completes without problem.

```
{
  "size": 100,
  "aggs": {
    "2": {
      "geohash_grid": {
        "field": "authors.affiliation.coordinates",
        "precision": 2
      }
    }
  },
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "*"
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "geo_bounding_box": {
                "authors.affiliation.coordinates": {
                  "top_left": {
                    "lat": 61.10078883158897,
                    "lon": -170.15625
                  },
                  "bottom_right": {
                    "lat": -64.92354174306496,
                    "lon": 118.47656249999999
                  }
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "highlight": {
    "pre_tags": [
      "@kibana-highlighted-field@"
    ],
    "post_tags": [
      "@/kibana-highlighted-field@"
    ],
    "fields": {
      "*": {}
    },
    "require_field_match": false,
    "fragment_size": 2147483647
  }
}
```

Elasticsearch version**: 2.3.0
OS version**: Elasticsearch docker image with head plugin, marvel and big desk installed

Thank you for your help,
regards,
Jakub Smid
</description><key id="145978723">17537</key><summary>Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">jaksmid</reporter><labels><label>:Highlighting</label><label>bug</label><label>v2.3.3</label></labels><created>2016-04-05T12:52:21Z</created><updated>2016-09-19T10:27:35Z</updated><resolved>2016-09-19T10:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T11:08:13Z" id="206313197">@jaksmid could you provide some documents and the stack trace that is produced when you see this exception please?
</comment><comment author="clintongormley" created="2016-04-06T11:09:22Z" id="206313641">@jpountz given that this only happens with `size` &gt; 0, I'm wondering if this highlighting trying to highlight the geo field? Perhaps with no documents on a particular shard?

/cc @nknize 
</comment><comment author="rmuir" created="2016-04-07T07:17:50Z" id="206731331">I can reproduce something that looks just like this with a lucene test if you apply the patch on https://issues.apache.org/jira/browse/LUCENE-7185

I suspect it may happen with extreme values such as latitude = 90 or longitude = 180 which are used much more in tests with the patch. See seed:

```
  [junit4] Suite: org.apache.lucene.spatial.geopoint.search.TestGeoPointQuery
   [junit4] IGNOR/A 0.01s J1 | TestGeoPointQuery.testRandomBig
   [junit4]    &gt; Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4] IGNOR/A 0.00s J1 | TestGeoPointQuery.testRandomDistanceHuge
   [junit4]    &gt; Assumption #1: 'nightly' test group is disabled (@Nightly())
   [junit4]   2&gt; NOTE: reproduce with: ant test  -Dtestcase=TestGeoPointQuery -Dtests.method=testAllLonEqual -Dtests.seed=4ABB96AB44F4796E -Dtests.locale=id-ID -Dtests.timezone=Pacific/Fakaofo -Dtests.asserts=true -Dtests.file.encoding=US-ASCII
   [junit4] ERROR   0.35s J1 | TestGeoPointQuery.testAllLonEqual &lt;&lt;&lt;
   [junit4]    &gt; Throwable #1: java.lang.IllegalArgumentException: Illegal shift value, must be 32..63; got shift=0
   [junit4]    &gt;    at __randomizedtesting.SeedInfo.seed([4ABB96AB44F4796E:DBB16756B45E397A]:0)
   [junit4]    &gt;    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCodedBytes(GeoEncodingUtils.java:109)
   [junit4]    &gt;    at org.apache.lucene.spatial.util.GeoEncodingUtils.geoCodedToPrefixCoded(GeoEncodingUtils.java:89)
   [junit4]    &gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum$Range.fillBytesRef(GeoPointPrefixTermsEnum.java:236)
   [junit4]    &gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointTermsEnum.nextRange(GeoPointTermsEnum.java:71)
   [junit4]    &gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextRange(GeoPointPrefixTermsEnum.java:171)
   [junit4]    &gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.nextSeekTerm(GeoPointPrefixTermsEnum.java:190)
   [junit4]    &gt;    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:212)
   [junit4]    &gt;    at org.apache.lucene.spatial.geopoint.search.GeoPointTermQueryConstantScoreWrapper$1.scorer(GeoPointTermQueryConstantScoreWrapper.java:110)
   [junit4]    &gt;    at org.apache.lucene.search.Weight.bulkScorer(Weight.java:135)
   [junit4]    &gt;    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:644)
   [junit4]    &gt;    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)
   [junit4]    &gt;    at org.apache.lucene.search.BooleanWeight.optionalBulkScorer(BooleanWeight.java:231)
   [junit4]    &gt;    at org.apache.lucene.search.BooleanWeight.booleanScorer(BooleanWeight.java:297)
   [junit4]    &gt;    at org.apache.lucene.search.BooleanWeight.bulkScorer(BooleanWeight.java:364)
   [junit4]    &gt;    at org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.bulkScorer(LRUQueryCache.java:644)
   [junit4]    &gt;    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)
   [junit4]    &gt;    at org.apache.lucene.search.AssertingWeight.bulkScorer(AssertingWeight.java:68)
   [junit4]    &gt;    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:666)
   [junit4]    &gt;    at org.apache.lucene.search.AssertingIndexSearcher.search(AssertingIndexSearcher.java:91)
   [junit4]    &gt;    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:473)
   [junit4]    &gt;    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.verifyRandomRectangles(BaseGeoPointTestCase.java:835)
   [junit4]    &gt;    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.verify(BaseGeoPointTestCase.java:763)
   [junit4]    &gt;    at org.apache.lucene.spatial.util.BaseGeoPointTestCase.testAllLonEqual(BaseGeoPointTestCase.java:495)

```
</comment><comment author="jaksmid" created="2016-04-07T07:20:27Z" id="206732544">Hi @clintongormley, thank you for your message. 

The stack trace is as follows:
`RemoteTransportException[[elasticsearch_4][172.17.0.2:9300][indices:data/read/search[phase/fetch/id]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [cyberdyne_metadata.ner.mitie.model.DISEASE.tag]]]; nested: NumberFormatException[Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)];
Caused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [cyberdyne_metadata.ner.mitie.model.DISEASE.tag]]]; nested: NumberFormatException[Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)];
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)
    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:592)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:408)
    at org.elasticsearch.search.action.SearchServiceTransportAction$FetchByIdTransportHandler.messageReceived(SearchServiceTransportAction.java:405)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:300)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: Invalid shift value (65) in prefixCoded bytes (is encoded value really a geo point?)
    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)
    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)
    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)
    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)
    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)
    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)
    ... 12 more`

The field cyberdyne_metadata.ner.mitie.model.DISEASE.tag should not be a geopoint according to the dynamic template.
</comment><comment author="jpountz" created="2016-04-07T07:33:06Z" id="206738984">@rmuir oh, good catch
@clintongormley The stack trace indeed suggests that the issue is with highlighting on the geo field. Regardless of this bug, I wonder that we should fail early when highlighting on anything but text fields and/or exclude non-text fields from wildcard matching.
</comment><comment author="dadoonet" created="2016-04-07T08:40:43Z" id="206761380">&gt; I wonder that we should fail early when highlighting on anything but text fields and/or exclude non-text fields from wildcard matching.

+1 to fail early if the user explicitly defined a non text field to highlight on and exclude non text fields when using wildcards
</comment><comment author="rodgermoore" created="2016-04-17T08:09:07Z" id="210972692">I was running into this bug during a live demo... Yes I know, I've should have tested all demo scenario's after updating ES :grimacing: . Anyway, +1 for fixing this!
</comment><comment author="fclotet" created="2016-04-18T21:45:45Z" id="211595595">-I&#180;m having the same error. It's happends with doc having location and trying to use 
"highlight": {... "require_field_match": false ...}

thanks!
</comment><comment author="dellis23" created="2016-05-03T01:52:24Z" id="216414709">I'm unclear as to what exactly is going on here, but I'm running into the same issue.  I'm attempting to do a geo bounding box in Kibana while viewing the results in the Discover tab.  Disabling highlighting in Kibana fixes the issue, but I would actually like to keep highlighting enabled, since it's super useful otherwise.

It sounds from what others are saying that this should fail when querying on _any_ non-string field, but I am not getting the same failure on numeric fields.  Is it just an issue with geoip fields?  I suppose another nice thing would be to explicitly allow for configuration of which fields should be highlighted in Kibana.
</comment><comment author="vingrad" created="2016-05-03T10:40:19Z" id="216492263">Please fix this issue.
</comment><comment author="brwe" created="2016-05-04T17:50:50Z" id="216946525">I wrote two tests so that everyone can reproduce what happens easily: https://github.com/brwe/elasticsearch/commit/ffa242941e4ede34df67301f7b9d46ea8719cc22

In brief:
The plain highlighter tries to highlight whatever the BBQuery provides as terms in the text "60,120" if that is how the `geo_point` was indexed (if the point was indexed with `{"lat": 60, "lon": 120}` nothing will happen because we cannot even extract anything from the source). The terms in the text are provided to Lucene as a token steam with a keyword analyzer.
In Lucene, this token stream is converted this via a longish call stack into a terms enum. But this terms enum is pulled from the query that contains the terms that are to be highlighted. In this case we call `GeoPointMultiTermQuery.getTermsEnum(terms)` which wraps the term in a `GeoPointTermsEnum`. This enum tries to convert a prefix coded geo term back to something else but because it is really just the string  "60,120" it throws the exception we see. 

I am unsure yet how a correct fix would look like but do wonder why we try highlingting on numeric and geo fields at all? If anyone has an opinion let me know.
</comment><comment author="brwe" created="2016-05-04T17:57:01Z" id="216948517">I missed @jpountz comment:

&gt; Regardless of this bug, I wonder that we should fail early when highlighting on anything but text fields and/or exclude non-text fields from wildcard matching.

I agree. Will make a pr for that.
</comment><comment author="clintongormley" created="2016-05-05T08:17:58Z" id="217100888">@brwe you did something similar before: https://github.com/elastic/elasticsearch/pull/11364 - i would have thought that that PR should have fixed this issue?
</comment><comment author="brwe" created="2016-05-05T09:15:10Z" id="217109572">@clintongormley Yes you are right. #11364 only addresses problems one gets when the way text is indexed is not compatible with the highlighter used. I do not remember why I did not exclude numeric fields then. 
</comment><comment author="rodgermoore" created="2016-05-07T13:15:25Z" id="217634314">Great work. Tnx 

:sunglasses: 
</comment><comment author="rodgermoore" created="2016-05-19T07:09:10Z" id="220244710">This is not fixed in 2.3.3 yet, correct?
</comment><comment author="jpountz" created="2016-05-19T07:13:30Z" id="220245373">@rodgermoore It should be fixed in 2.3.3, can you still reproduce the problem?
</comment><comment author="rodgermoore" created="2016-05-19T11:44:32Z" id="220300797">Ubuntu 14.04-04
Elasticsearch 2.3.3
Kibana 4.5.1
JVM 1.8.0_66

I am still able to reproduce this error in Kibana 4.5.1. I have a dashboard with a search panel with highlighting enabled. On the same Dashboard I have a tile map and after selecting an area in this map using the select function (draw a rectangle) I got the "Invalid shift value (xx) in prefixCoded bytes (is encoded value really a geo point?)" error.

When I alter the json settings file of the search panel and remove highlighting the error does not pop-up.
</comment><comment author="brwe" created="2016-05-19T13:07:51Z" id="220318549">@rodgermoore I cannot reproduce this but I might do something different from you. Here is my dashboard:

![image](https://cloud.githubusercontent.com/assets/4320215/15393472/bd2b4cf2-1dcd-11e6-8ac1-cf6ba5e995b7.png)

Is that what you did?
Can you attach the whole stacktrace from the elasticsearch logs again? If you did not change the logging config the full search request should be in there. Also, if you can please add an example document.
</comment><comment author="rodgermoore" created="2016-05-19T13:12:50Z" id="220319750">I see you used "text:blah". I did not enter a search at all (so used the default wildcard) and then did the aggregation on the tile map. This resulted in the error. 
</comment><comment author="brwe" created="2016-05-19T13:16:46Z" id="220320747">I can remove the query and still get a result. Can you please attach the relevant part of the elasticsearch log? 
</comment><comment author="rodgermoore" created="2016-05-19T13:37:15Z" id="220326277">Here you go:

```
[2016-05-19 15:23:08,270][DEBUG][action.search            ] [Black King] All shards failed for phase: [query_fetch]
RemoteTransportException[[Black King][192.168.48.18:9300][indices:data/read/search[phase/query+fetch]]]; nested: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [tags.nl]]]; nested: NumberFormatException[Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)];
Caused by: FetchPhaseExecutionException[Fetch Failed [Failed to highlight field [tags.nl]]]; nested: NumberFormatException[Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)];
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:123)
    at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:140)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:188)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:480)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:392)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:389)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NumberFormatException: Invalid shift value (115) in prefixCoded bytes (is encoded value really a geo point?)
    at org.apache.lucene.spatial.util.GeoEncodingUtils.getPrefixCodedShift(GeoEncodingUtils.java:134)
    at org.apache.lucene.spatial.geopoint.search.GeoPointPrefixTermsEnum.accept(GeoPointPrefixTermsEnum.java:219)
    at org.apache.lucene.index.FilteredTermsEnum.next(FilteredTermsEnum.java:232)
    at org.apache.lucene.search.TermCollectingRewrite.collectTerms(TermCollectingRewrite.java:67)
    at org.apache.lucene.search.ScoringRewrite.rewrite(ScoringRewrite.java:108)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:220)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:227)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.extract(WeightedSpanTermExtractor.java:113)
    at org.apache.lucene.search.highlight.WeightedSpanTermExtractor.getWeightedSpanTerms(WeightedSpanTermExtractor.java:505)
    at org.apache.lucene.search.highlight.QueryScorer.initExtractor(QueryScorer.java:218)
    at org.apache.lucene.search.highlight.QueryScorer.init(QueryScorer.java:186)
    at org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:195)
    at org.elasticsearch.search.highlight.PlainHighlighter.highlight(PlainHighlighter.java:108)
    ... 12 more
```

We are using dynamic mapping and we dynamically analyse all string fields using the Dutch language analyzer. All string fields get a non analyzed field: "field.raw" and a Dutch analyzed field "field.nl". 
</comment><comment author="brwe" created="2016-05-19T13:46:59Z" id="220328929">Ah...I was hoping to get the actual request but it is not in the stacktrace after all. Can you also add the individual requests from the panels in your dashboard (in the spy tab) and a screenshot so I can see what the geo bounding box filter filters on? I could then try to reconstruct the request.

Also, are you sure you upgraded all nodes in the cluster? Check with `curl -XGET "http://hostname:port/_nodes"`. Would be great if you could add the output of that here too just to be sure. 
</comment><comment author="lemig" created="2016-05-19T14:17:36Z" id="220337833">I have got the exact same issue. I am running 2.3.3. All my nodes (1) are upgraded.
</comment><comment author="lemig" created="2016-05-19T14:42:02Z" id="220345434">&lt;img width="1676" alt="screen shot 2016-05-19 at 16 29 15" src="https://cloud.githubusercontent.com/assets/78766/15397413/7858191c-1de0-11e6-802b-773f4a7ecf79.png"&gt;
</comment><comment author="rodgermoore" created="2016-05-19T14:42:12Z" id="220345495">Here you go.

Tile Map Query:

```
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "analyze_wildcard": true,
          "query": "*"
        }
      },
      "filter": {
        "bool": {
          "must": [
            {
              "geo_bounding_box": {
                "SomeGeoField": {
                  "top_left": {
                    "lat": REMOVED,
                    "lon": REMOVED
                  },
                  "bottom_right": {
                    "lat": REMOVED,
                    "lon": REMOVED
                  }
                }
              },
              "$state": {
                "store": "appState"
              }
            },
            {
              "query": {
                "query_string": {
                  "query": "*",
                  "analyze_wildcard": true
                }
              }
            },
            {
              "range": {
                "@timestamp": {
                  "gte": 1458485686484,
                  "lte": 1463666086484,
                  "format": "epoch_millis"
                }
              }
            }
          ],
          "must_not": []
        }
      }
    }
  },
  "size": 0,
  "aggs": {
    "2": {
      "geohash_grid": {
        "field": "SomeGeoField",
        "precision": 5
      }
    }
  }
}
```

I'm using a single node cluster, here's the info:

```
{
  "cluster_name": "elasticsearch",
  "nodes": {
    "RtBthRfeSOSud1XfRRAkSA": {
      "name": "Black King",
      "transport_address": "192.168.48.18:9300",
      "host": "192.168.48.18",
      "ip": "192.168.48.18",
      "version": "2.3.3",
      "build": "218bdf1",
      "http_address": "192.168.48.18:9200",
      "settings": {
        "pidfile": "/var/run/elasticsearch/elasticsearch.pid",
        "cluster": {
          "name": "elasticsearch"
        },
        "path": {
          "conf": "/etc/elasticsearch",
          "data": "/var/lib/elasticsearch",
          "logs": "/var/log/elasticsearch",
          "home": "/usr/share/elasticsearch",
          "repo": [
            "/home/somename/es_backups"
          ]
        },
        "name": "Black King",
        "client": {
          "type": "node"
        },
        "foreground": "false",
        "config": {
          "ignore_system_properties": "true"
        },
        "network": {
          "host": "0.0.0.0"
        }
      },
      "os": {
        "refresh_interval_in_millis": 1000,
        "name": "Linux",
        "arch": "amd64",
        "version": "3.19.0-59-generic",
        "available_processors": 8,
        "allocated_processors": 8
      },
      "process": {
        "refresh_interval_in_millis": 1000,
        "id": 1685,
        "mlockall": false
      },
      "jvm": {
        "pid": 1685,
        "version": "1.8.0_66",
        "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
        "vm_version": "25.66-b17",
        "vm_vendor": "Oracle Corporation",
        "start_time_in_millis": 1463663018422,
        "mem": {
          "heap_init_in_bytes": 6442450944,
          "heap_max_in_bytes": 6372720640,
          "non_heap_init_in_bytes": 2555904,
          "non_heap_max_in_bytes": 0,
          "direct_max_in_bytes": 6372720640
        },
        "gc_collectors": [
          "ParNew",
          "ConcurrentMarkSweep"
        ],
        "memory_pools": [
          "Code Cache",
          "Metaspace",
          "Compressed Class Space",
          "Par Eden Space",
          "Par Survivor Space",
          "CMS Old Gen"
        ],
        "using_compressed_ordinary_object_pointers": "true"
      },
      "thread_pool": {
        "force_merge": {
          "type": "fixed",
          "min": 1,
          "max": 1,
          "queue_size": -1
        },
        "percolate": {
          "type": "fixed",
          "min": 8,
          "max": 8,
          "queue_size": 1000
        },
        "fetch_shard_started": {
          "type": "scaling",
          "min": 1,
          "max": 16,
          "keep_alive": "5m",
          "queue_size": -1
        },
        "listener": {
          "type": "fixed",
          "min": 4,
          "max": 4,
          "queue_size": -1
        },
        "index": {
          "type": "fixed",
          "min": 8,
          "max": 8,
          "queue_size": 200
        },
        "refresh": {
          "type": "scaling",
          "min": 1,
          "max": 4,
          "keep_alive": "5m",
          "queue_size": -1
        },
        "suggest": {
          "type": "fixed",
          "min": 8,
          "max": 8,
          "queue_size": 1000
        },
        "generic": {
          "type": "cached",
          "keep_alive": "30s",
          "queue_size": -1
        },
        "warmer": {
          "type": "scaling",
          "min": 1,
          "max": 4,
          "keep_alive": "5m",
          "queue_size": -1
        },
        "search": {
          "type": "fixed",
          "min": 13,
          "max": 13,
          "queue_size": 1000
        },
        "flush": {
          "type": "scaling",
          "min": 1,
          "max": 4,
          "keep_alive": "5m",
          "queue_size": -1
        },
        "fetch_shard_store": {
          "type": "scaling",
          "min": 1,
          "max": 16,
          "keep_alive": "5m",
          "queue_size": -1
        },
        "management": {
          "type": "scaling",
          "min": 1,
          "max": 5,
          "keep_alive": "5m",
          "queue_size": -1
        },
        "get": {
          "type": "fixed",
          "min": 8,
          "max": 8,
          "queue_size": 1000
        },
        "bulk": {
          "type": "fixed",
          "min": 8,
          "max": 8,
          "queue_size": 50
        },
        "snapshot": {
          "type": "scaling",
          "min": 1,
          "max": 4,
          "keep_alive": "5m",
          "queue_size": -1
        }
      },
      "transport": {
        "bound_address": [
          "[::]:9300"
        ],
        "publish_address": "192.168.48.18:9300",
        "profiles": {}
      },
      "http": {
        "bound_address": [
          "[::]:9200"
        ],
        "publish_address": "192.168.48.18:9200",
        "max_content_length_in_bytes": 104857600
      },
      "plugins": [],
      "modules": [
        {
          "name": "lang-expression",
          "version": "2.3.3",
          "description": "Lucene expressions integration for Elasticsearch",
          "jvm": true,
          "classname": "org.elasticsearch.script.expression.ExpressionPlugin",
          "isolated": true,
          "site": false
        },
        {
          "name": "lang-groovy",
          "version": "2.3.3",
          "description": "Groovy scripting integration for Elasticsearch",
          "jvm": true,
          "classname": "org.elasticsearch.script.groovy.GroovyPlugin",
          "isolated": true,
          "site": false
        },
        {
          "name": "reindex",
          "version": "2.3.3",
          "description": "_reindex and _update_by_query APIs",
          "jvm": true,
          "classname": "org.elasticsearch.index.reindex.ReindexPlugin",
          "isolated": true,
          "site": false
        }
      ]
    }
  }
}
```

Screenshot, I had to clear out the data:

![error_es](https://cloud.githubusercontent.com/assets/12231719/15397399/668a374c-1de0-11e6-903d-f929a2d9f0b2.PNG)
</comment><comment author="clintongormley" created="2016-05-19T14:45:25Z" id="220346518">@rodgermoore does the query you provided work correctly? You said that it started working once you deleted the highlighting and this query doesn't contain highlighting.  Could you provide the query that doesn't work?
</comment><comment author="rodgermoore" created="2016-05-19T14:51:27Z" id="220348346">It does has highlighting enabled. This is the json for the search panel: 

```
{
  "index": "someindex",
  "query": {
    "query_string": {
      "query": "*",
      "analyze_wildcard": true
    }
  },
  "filter": [],
  "highlight": {
    "pre_tags": [
      "@kibana-highlighted-field@"
    ],
    "post_tags": [
      "@/kibana-highlighted-field@"
    ],
    "fields": {
      "*": {}
    },
    "require_field_match": false,
    "fragment_size": 2147483647
  }
}
```

I can't show the actual data so I selected to show only the timestamp field in the search panel in the screenshot...

When I change the json of the search panel to:

```
{
  "index": "someindex",
  "filter": [],
  "query": {
    "query_string": {
      "query": "*",
      "analyze_wildcard": true
    }
  }
}
```

The error disappears.
</comment><comment author="dellis23" created="2016-05-19T14:54:44Z" id="220349441">If my understanding of the patch is correct, it shouldn't matter whether Kibana is including the highlighting field.  Elasticsearch should only be trying to highlight string fields, even if a wildcard is being used.
</comment><comment author="brwe" created="2016-05-19T16:23:44Z" id="220377621">Ok, I managed to reproduce it on 2.3.3. It happens with `"geohash": true` in the mapping. 

Steps are:

```
DELETE test
PUT test 
{
  "mappings": {
    "doc": {
      "properties": {
        "point": {
          "type": "geo_point",
          "geohash": true
        }
      }
    }
  }
}

PUT test/doc/1
{
  "point": "60.12,100.34"
}

POST test/_search
{
  "query": {
    "geo_bounding_box": {
      "point": {
        "top_left": {
          "lat": 61.10078883158897,
          "lon": -170.15625
        },
        "bottom_right": {
          "lat": -64.92354174306496,
          "lon": 118.47656249999999
        }
      }
    }
  },
  "highlight": {
    "fields": {
      "*": {}
    }
  }
}
```

Sorry, I did not think of that. I work on another fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update match query documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17536</link><project id="" key="" /><description>Now the `match` query has been split out into `match`, `match_phrase` and `match_phrase_prefix` we need to update the docs to remove the deprecated syntax
</description><key id="145970056">17536</key><summary>Update match query documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>docs</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T12:17:19Z</created><updated>2016-04-06T14:45:24Z</updated><resolved>2016-04-06T14:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-05T12:19:52Z" id="205774203">I left a question, mainly for @clintongormley I think
</comment><comment author="clintongormley" created="2016-04-06T11:01:49Z" id="206310895">Could we split these docs into a separate page for each of `match`, `match_phrase`, and `match_phrase_prefix`?  Will make them easier to digest.
</comment><comment author="colings86" created="2016-04-06T13:07:35Z" id="206361369">@clintongormley I pushed an update, let me know what you think
</comment><comment author="clintongormley" created="2016-04-06T14:14:07Z" id="206391108">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Specifying the search type dfs_query_then_fetch has no effect on nested queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17535</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.0
**JVM version**: 1.7.0_80
**OS version**: Windows 10

**Description of the problem including expected versus actual behavior**:
Specifying [search_type=dfs_query_then_fetch](https://www.elastic.co/blog/understanding-query-then-fetch-vs-dfs-query-then-fetch) does not work for nested queries; the scores returned are still based on local information in each shard.

**Steps to reproduce**:
Please check the gist [here](https://gist.github.com/mrkevinze/6e9dc23e98f4845db0b69316b95b636a).
</description><key id="145968691">17535</key><summary>Specifying the search type dfs_query_then_fetch has no effect on nested queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrkevinze</reporter><labels><label>:Nested Docs</label><label>:Search</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-04-05T12:10:18Z</created><updated>2016-10-24T17:09:07Z</updated><resolved>2016-06-23T09:49:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T11:00:00Z" id="206310324">@martijnvg could you take a look please?
</comment><comment author="martijnvg" created="2016-04-06T20:27:46Z" id="206546064">@kvnlte @clintongormley This is because the `ToParentBlockJoinQuery` which the `nested` query uses doesn't implement query extraction, which the dfs phase uses to extract terms from the query to compute distributed idf. This needs to be fixed in Lucene.
</comment><comment author="clintongormley" created="2016-04-07T09:37:46Z" id="206784661">thanks @martijnvg - is there a lucene issue we can link to?
</comment><comment author="martijnvg" created="2016-04-07T10:37:26Z" id="206804295">@clintongormley https://issues.apache.org/jira/browse/LUCENE-7187
</comment><comment author="sfcgeorge" created="2016-06-23T09:30:42Z" id="227997419">Just bit me. Spent ages figuring why queryNorm was varying widely between documents, found dfs_query_then_fetch which fixed it, then using nested docs broke it again so I guess I'll have to run on 1 shard :(

It looks like the Lucene issue was fixed just a week ago :) So I guess this could be fixed in ES now (and "stalled" label can be removed)?
</comment><comment author="martijnvg" created="2016-06-23T09:49:59Z" id="228002459">@sfcgeorge yes, it can be removed. It is already fixed in the upcoming alpha4 release, so it can be closed too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>http.cors ES 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17534</link><project id="" key="" /><description>**Elasticsearch version**:
2.3
**JVM version**:
1.8.0
**OS version**:
Windows 7
**Description of the problem including expected versus actual behavior**:
I have set cors conf in elasticsearch.yml like this
`http.cors.enabled : true
http.cors.allow-origin : "*"
http.cors.allow-methods : OPTIONS, HEAD, GET, POST, PUT, DELETE
http.cors.allow-headers : X-Requested-With,X-Auth-Token,Content-Type, Content-Length`

When requesting elasticsearch from file:///C:/elastic/index.html i got the following error
`Request header field Content-Type is not allowed by Access-Control-Allow-Headers`

I switched to 2.2 version and i set the same parameters and it works like a charm

**Steps to reproduce**:
 1.Set cors parameters in elasticsearch.yml
 2.Request elasticsearch from an url like this: file:///c:/..
</description><key id="145945223">17534</key><summary>http.cors ES 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chanserv</reporter><labels /><created>2016-04-05T10:14:30Z</created><updated>2016-04-06T11:12:47Z</updated><resolved>2016-04-06T11:12:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pciccarese" created="2016-04-05T13:05:58Z" id="205792730">I believe this is similar to this: 
https://github.com/elastic/elasticsearch/issues/17483
</comment><comment author="clintongormley" created="2016-04-06T11:12:47Z" id="206314864">Closing as duplicate of #17483.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/plugins throws NullPointerException on client nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17533</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.1

**JVM version**: OpenJDK Runtime Environment (build 1.8.0_71-b15)

**OS version**: Linux 2.6.32-573.12.1.el6.x86_64 #1 SMP

**Description of the problem including expected versus actual behavior**:

Calling _cat/plugins on client nodes throws NullPointerException:

`[2016-04-05 10:02:28,895][WARN ][rest.suppressed          ] /_cat/plugins Params: {}
java.lang.NullPointerException
        at org.elasticsearch.rest.action.cat.RestPluginsAction.buildTable(RestPluginsAction.java:98)
        at org.elasticsearch.rest.action.cat.RestPluginsAction.access$000(RestPluginsAction.java:41)
        at org.elasticsearch.rest.action.cat.RestPluginsAction$1$1.buildResponse(RestPluginsAction.java:69)
        at org.elasticsearch.rest.action.cat.RestPluginsAction$1$1.buildResponse(RestPluginsAction.java:66)
        at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
        at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
        at org.elasticsearch.action.support.ThreadedActionListener$1.doRun(ThreadedActionListener.java:89)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
`

The problem seems only to happen on client nodes, not data nodes.

**Steps to reproduce**:
1. curl http://localhost:9200/_cat/plugins
</description><key id="145940953">17533</key><summary>_cat/plugins throws NullPointerException on client nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">terjesannum</reporter><labels><label>:CAT API</label><label>adoptme</label><label>bug</label></labels><created>2016-04-05T09:58:08Z</created><updated>2016-05-05T08:14:30Z</updated><resolved>2016-05-05T08:14:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-05T08:14:26Z" id="217100422">This has been fixed by #16898
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not assume term queries use the inverted index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17532</link><project id="" key="" /><description>We have a couple places in the code base that assume that search is always done
on the inverted index. However with the new points API in Lucene 6, this is not
true anymore. This commit makes MappedFieldType.indexedValueForSearch protected
and fixes call sites to keep working for field types that use the inverted
index and either work differently ar throw an exception otherwise. For instance,
it will still be possible to run cross_fields multi match queries on numeric
fields, but the score contributions will not be blended as well as before, and
significant terms aggregations on long terms will not be possible anymore since
points do not record document frequencies.
</description><key id="145932291">17532</key><summary>Do not assume term queries use the inverted index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T09:21:05Z</created><updated>2016-04-12T07:48:18Z</updated><resolved>2016-04-12T07:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T17:54:08Z" id="208473955">LGTM, left a couple minor testing suggestions. I would also have someone else verify the blended term part, as I'm not that familiar with it.
</comment><comment author="jpountz" created="2016-04-12T07:45:47Z" id="208760909">&gt; I would also have someone else verify the blended term part, as I'm not that familiar with it.

I am not sure who is familiar with it besides Simon, who is off these days. I sent him a ping so that he can have a look when he is back and will merge in the meantime. Thanks Ryan for having a look!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Java api field  script encounter BigDecimal Exception in Elastic 2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17531</link><project id="" key="" /><description>**Elasticsearch 2.1.0**

**jdk1.7.0_79x64**

**Debian GNU/Linux 7.8 (wheezy)  64bit**

 1.I set up my Elastic 2.1.0 cluster and I do 

```
PUT  bucket/relation/1 
{
  "name":"Jake",
  "frozen":300000
}
```

 2.use elastic java api

```
    try {
        Settings settings = Settings.settingsBuilder().put("cluster.name", "my-application").build();
        Client client = TransportClient.builder().settings(settings).build().addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("10.166.224.199"), 9300));
    } catch (UnknownHostException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
    SearchRequestBuilder builder = client.prepareSearch("bucket").setTypes("relation");
    builder.addScriptField("money", new Script("doc['frozen'].value / 100"));
    SearchResponse searchResponse=builder.get();
```

 3.I get error

```
 Exception in thread "main" NotSerializableExceptionWrapper[Can't write type [class java.math.BigDecimal]]
    at org.elasticsearch.common.io.stream.StreamOutput.writeGenericValue(StreamOutput.java:416)
    at org.elasticsearch.search.internal.InternalSearchHitField.writeTo(InternalSearchHitField.java:112)
    at org.elasticsearch.search.internal.InternalSearchHit.writeTo(InternalSearchHit.java:728)
    at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:261)
    at org.elasticsearch.search.internal.InternalSearchHits.writeTo(InternalSearchHits.java:232)
    at org.elasticsearch.search.internal.InternalSearchResponse.writeTo(InternalSearchResponse.java:121)
    at org.elasticsearch.action.search.SearchResponse.writeTo(SearchResponse.java:207)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:97)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:75)
    at org.elasticsearch.action.support.HandledTransportAction$TransportHandler$1.onResponse(HandledTransportAction.java:49)
    at org.elasticsearch.action.support.HandledTransportAction$TransportHandler$1.onResponse(HandledTransportAction.java:45)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.doRun(TransportSearchQueryThenFetchAction.java:155)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

**But when I use sense:**

```
GET  bucket/relation/_search
{
 "script_fields" : {
"money" : {
"script" : "doc['frozen'].value / 100"
}
 }
}
```

it works.
</description><key id="145928257">17531</key><summary> Java api field  script encounter BigDecimal Exception in Elastic 2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">game365</reporter><labels><label>:Java API</label><label>:Scripting</label><label>discuss</label></labels><created>2016-04-05T09:05:49Z</created><updated>2017-05-28T05:30:15Z</updated><resolved>2017-05-28T05:30:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="prateekengineer" created="2016-04-26T10:38:16Z" id="214698106">This method **public void writeGenericValue(@Nullable Object value) throws IOException**   checks the type of Object while writing generic value .It has provided support for various types but 
not for object of BigDecimal type.

So it throws **IOException("Can't write type [" + type + "]")**
</comment><comment author="smartcat315" created="2016-12-15T03:44:39Z" id="267231081">This problem has been bothering me for a long time.Is the problem solved?</comment><comment author="rjernst" created="2017-05-28T05:30:15Z" id="304493986">Groovy has been removed for 6.0 and this was an issue with BigDecimal being accessible in groovy. Painless has BigDecimal whitelisted so this should not be an issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace problematic bwc indices for 2.3.1 with newly generated one.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17530</link><project id="" key="" /><description>Restore the test OldIndexBackwardsCompatibilityIT.testOldIndexes which now works fine with 2.3.1 bwc indices.
</description><key id="145919104">17530</key><summary>Replace problematic bwc indices for 2.3.1 with newly generated one.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T07:52:01Z</created><updated>2016-04-06T14:21:59Z</updated><resolved>2016-04-05T09:19:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-04-05T07:57:14Z" id="205708987">@clintongormley I've re-created the bwc indices for 2.3.1 and the test is ok now. It seems that the indices you've created do not contain payloads. Any idea why ? 
</comment><comment author="jpountz" created="2016-04-05T09:18:08Z" id="205723944">Maybe it is due to differences between the script that we have on master and 2.3?

LGTM
</comment><comment author="jimczi" created="2016-04-05T09:19:56Z" id="205724888">Thanks @jpountz 
</comment><comment author="clintongormley" created="2016-04-06T10:47:28Z" id="206305103">yes i may have created the indices from the wrong branch - thanks for fixing @jimferenczi 
</comment><comment author="nik9000" created="2016-04-06T13:58:28Z" id="206384107">I pushed a similar fix a week or two ago. I didn't just push the fix again
this time because I figured we should figure out how we made these in the
first place if it happened twice.

On Wed, Apr 6, 2016 at 6:47 AM, Clinton Gormley notifications@github.com
wrote:

&gt; yes i may have created the indices from the wrong branch - thanks for
&gt; fixing @jimferenczi https://github.com/jimferenczi
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17530#issuecomment-206305103
</comment><comment author="clintongormley" created="2016-04-06T14:21:59Z" id="206396277">I did this twice? :(  I'll have to be more careful...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verbose mode no longer works in elasticsearch-plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17529</link><project id="" key="" /><description>Although `elasticsearch-plugin` claims to support verbose mode, adding `-v` doesn't seem to do anything. In particular, it no longer prints out the URL of the plugin it is about to install, which is needed for offline installation.
</description><key id="145913841">17529</key><summary>Verbose mode no longer works in elasticsearch-plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T07:30:05Z</created><updated>2016-04-22T16:26:30Z</updated><resolved>2016-04-22T16:26:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T20:35:05Z" id="208548297">This isn't a bug with `-v`, it is simply something that is no longer printed. Here's a PR to add it: #17662
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename service.bat to elasticsearch-service.bat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17528</link><project id="" key="" /><description>with 5.0 we renamed the plugin executables to elasticsearch-plugin. I think we should do the same with service.bat

@Mpdreamz thoughts?
</description><key id="145912844">17528</key><summary>Rename service.bat to elasticsearch-service.bat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v5.0.0-beta1</label></labels><created>2016-04-05T07:26:24Z</created><updated>2016-09-17T09:02:50Z</updated><resolved>2016-09-15T13:53:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2016-04-05T08:07:21Z" id="205712445">:+1: from me, cc @elastic/microsoft
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Call out where we are making a setting change.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17527</link><project id="" key="" /><description>IMHO the original text here was incomplete. Adding the simple words 'in the index mapping' makes this sentence more clear. Perhaps a be more clear to make this a link.
</description><key id="145912406">17527</key><summary>Call out where we are making a setting change.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffklassen</reporter><labels><label>docs</label><label>v2.3.2</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T07:24:38Z</created><updated>2016-04-05T19:51:16Z</updated><resolved>2016-04-05T19:49:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeffklassen" created="2016-04-05T07:27:59Z" id="205695955">I have since signed the CLA, how do I get this check to update?
</comment><comment author="dakrone" created="2016-04-05T19:49:35Z" id="205965521">@jeffklassen I confirmed you signed, I'll merge this in, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slow shard allocation in Elastic 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17526</link><project id="" key="" /><description>Hi Team,

Recently I updated my elastic from 2.2.0 to 2.3.0. When I start elastic 2.3, It is taking almost 5 to 8 min to turn all index from RED to YELLOW status. If I use same data and start elastic 2.2, it is approximately taking  30 seconds. 

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="145874861">17526</key><summary>Slow shard allocation in Elastic 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pradeepg-telsiz</reporter><labels><label>:Recovery</label><label>feedback_needed</label></labels><created>2016-04-05T03:25:05Z</created><updated>2017-04-02T19:42:39Z</updated><resolved>2017-04-02T19:42:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-05T07:03:39Z" id="205686568">Thanks for reporting. Can you tell us the exact upgrade procedure you followed and the size of your cluster?
</comment><comment author="pradeepg-telsiz" created="2016-04-05T08:51:23Z" id="205719776">I have 11 index, total size adding up to 12GB. 

Below are the steps that I followed
1. Stop elasticsearch 2.2
2. Downloaded ES 2.3 and updated configurations to match with ES2.2
3. Start ES 2.3 
Takes minimum 5 to 7 min to get all index to YELLOW state. 
4. Revert back ES 2.3 to 2.2
5. Start ES 2.2 
Takes less than 40-40 sec to get all index to YELLOW state.
</comment><comment author="bleskes" created="2016-04-05T09:05:13Z" id="205721118">Sounds strange. Can you share your config? is there anything of note in the master logs? If you prefer you can send them to my first name at elastic.co 
</comment><comment author="pradeepg-telsiz" created="2016-04-05T17:00:49Z" id="205895784">Sent you configurations in mail. Did you get it? 
</comment><comment author="bleskes" created="2016-04-05T17:04:06Z" id="205897749">Yes. Didn&#8217;t have time to look at it yet. Will do asap. Thanks!

&gt; On 05 Apr 2016, at 19:01, Pradeep Gowda notifications@github.com wrote:
&gt; 
&gt; Sent you configurations in mail. Did you get it?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt; Reply to this email directly or view it on GitHub
</comment><comment author="pradeepg-telsiz" created="2016-04-11T17:38:35Z" id="208467353">Any update on this? 
</comment><comment author="bleskes" created="2016-04-12T08:41:56Z" id="208789215">I looked at the logs and there is nothing concrete I can take out of it. I do see a lot of node restarts (which are probably planned). I also some cluster state publishing that is slower than it should (see messages with `cluster state update task`). 

Can you repeat the experiment with logging set to DEBUG? Also, I'm not sure but it seems like you are indexing/searching while the cluster is recovering. If so, it will be better stop it while doing the experiment because it pollutes the logs..
</comment><comment author="clintongormley" created="2016-07-15T10:07:44Z" id="232912707">@pradeepg-telsiz any further info?
</comment><comment author="pradeepg-telsiz" created="2016-07-15T18:50:51Z" id="233036514">No improvement. Living with the same state and recovering each index manually. Where can I send the log? Can you please share email id? 
</comment><comment author="bleskes" created="2016-07-15T21:23:18Z" id="233073307">@pradeepg-telsiz same as before - my first name at elastic.co . Thx.
</comment><comment author="pradeepg-telsiz" created="2016-07-15T21:45:46Z" id="233077546">Sent mail with logs. Please let me know if you need any more information.
</comment><comment author="bleskes" created="2016-07-22T08:22:21Z" id="234483571">@pradeepg-telsiz thanks for the logs. I can see that some shards can not be assigned because they miss enough shard copies (you have 1 primary, 2 replicas which means that you need at least 2 copies to be found before a primary can be assigned, sadly only 1 is found - do you know why?). Once the shard is forcefully assigned via the API it recovers locally on node1 as you said. The node 2 is assigned a replica and it recovers from node1 and it looks it does so very quickly. Then there is nothing for a long time until node3 starts its recovery. It'ss hard to say why exactly as the logs you sent are only partially set to debug and I miss things. For example node3.txt has debug logs from some components (`discovery.zen.publish`) but not others (`cluster.service`). Can you check what's going on and resend with a complete log?
</comment><comment author="colings86" created="2017-03-31T13:41:13Z" id="290714922">@bleskes is this still an issue? could this be closed?</comment><comment author="bleskes" created="2017-04-02T19:42:39Z" id="291009756">I think we can close it... it's been a long while. Thanks for the ping @colings86 !</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes reading of CORS pre-flight headers and methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17525</link><project id="" key="" /><description>CORS headers and methods config parameters must be read as arrays. This
commit fixes the issue. It affects http.cors.allow-methods and
http.cors.allow-headers.

Backports #17523
</description><key id="145873280">17525</key><summary>Fixes reading of CORS pre-flight headers and methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>bug</label><label>v2.4.0</label></labels><created>2016-04-05T03:10:31Z</created><updated>2016-04-07T19:59:00Z</updated><resolved>2016-04-07T19:59:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-07T19:59:00Z" id="207066759">Closed by Commit [39253df](https://github.com/elastic/elasticsearch/commit/39253df704a8aa2039683acd145ece1135a6766a)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes reading of CORS pre-flight headers and methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17524</link><project id="" key="" /><description>CORS headers and methods config parameters must be read as arrays. This
commit fixes the issue. It affects http.cors.allow-methods and
http.cors.allow-headers.

Backports #17523
</description><key id="145866120">17524</key><summary>Fixes reading of CORS pre-flight headers and methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.2</label></labels><created>2016-04-05T02:30:27Z</created><updated>2016-06-13T18:34:06Z</updated><resolved>2016-04-07T20:00:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="skundrik" created="2016-04-05T06:48:37Z" id="205678669">Fixes #17483 Problems with http.cors.allow-methods
</comment><comment author="abeyad" created="2016-04-07T20:00:04Z" id="207067034">Closed by Commit [1ecc627](https://github.com/elastic/elasticsearch/commit/1ecc62778e2ae03937d6b498026652ec098f4fa2)
</comment><comment author="uschindler" created="2016-04-18T12:02:58Z" id="211350308">Thanks for fixing. I have seen the same on my server. But I was able to fix with reverse proxy, so I did not care too much.

In my case the issue came from the fact that I only allowed GET and POST in my configuration.

I will try 2.3.2 and report if it's fixed.
</comment><comment author="abeyad" created="2016-04-18T13:57:31Z" id="211391887">@uschindler Thank you!
</comment><comment author="uschindler" created="2016-04-26T14:06:49Z" id="214756674">I can confirm: 2.3.2 works again without reverse proxy hack
</comment><comment author="abeyad" created="2016-04-26T14:18:10Z" id="214760265">@uschindler Great, thanks for confirming
</comment><comment author="ieatbytes" created="2016-06-13T15:19:37Z" id="225613479">can you please guide me what to write in elasticsearch.yml? i used to work like 
network.host: 0.0.0.0 and it was working like charm
</comment><comment author="abeyad" created="2016-06-13T18:34:06Z" id="225668955">@ieatbytes I am not sure what you mean?  None of the properties in `elasticsearch.yml` related to CORS handling have changed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes reading of CORS pre-flight headers and methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17523</link><project id="" key="" /><description>CORS headers and methods config parameters must be read as arrays.  This
commit fixes the issue.  It affects http.cors.allow-methods and
http.cors.allow-headers.

Fixes #17483 
</description><key id="145861522">17523</key><summary>Fixes reading of CORS pre-flight headers and methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-05T02:01:37Z</created><updated>2016-04-07T19:56:56Z</updated><resolved>2016-04-07T19:56:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-05T21:10:09Z" id="205985458">@spinscale Do you have spare cycles to review?
</comment><comment author="dakrone" created="2016-04-07T17:03:25Z" id="206995676">this LGTM
</comment><comment author="abeyad" created="2016-04-07T19:56:56Z" id="207066183">Closed by Commit [763a659](https://github.com/elastic/elasticsearch/commit/763a659830d5011a4e944b5f863bc3c8b7e0382e)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Very large windows_size can cause node to run OOM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17522</link><project id="" key="" /><description>**Elasticsearch version**: 2.1.x, 2.3.1 (not reproducible using 2.0.x)

**Steps to reproduce**:
1.  Create an empty index (eg. curl -XPUT http://localhost:9200/foo)
2.  Then run the query:

```
curl -XGET "http://localhost:9200/foo/_search" -d' 
{
  "rescore": {
    "window_size": 10000000,
    "query": {
      "match_all": {}
    }
  }
}'
```

On 2.0.x, the query will return immediately.  On 2.1x and above (including 2.3.1), the query will go to town and use CPU and memory until it OOM.
</description><key id="145791431">17522</key><summary>Very large windows_size can cause node to run OOM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>resiliency</label></labels><created>2016-04-04T20:13:50Z</created><updated>2016-04-22T15:25:58Z</updated><resolved>2016-04-22T15:25:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2016-04-04T20:19:57Z" id="205477711">Might be a good candidate for the [safeguards](https://github.com/elastic/elasticsearch/issues/11511) we are adding to the product as well.
</comment><comment author="chrismwendt" created="2016-04-04T20:23:29Z" id="205478674">I loaded the heap profile and found lots of objects of type `org.apache.lucene.search.ScoreDoc`:

![image](https://cloud.githubusercontent.com/assets/1387653/14261982/1235d642-fa68-11e5-87a8-1132584c95d6.png)

![image](https://cloud.githubusercontent.com/assets/1387653/14261996/2380b6b0-fa68-11e5-86cd-6ccec5a77e4a.png)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more fromXContent() testing with shuffled field order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17521</link><project id="" key="" /><description>This adds random shuffling of fields for toXContent roundtrip tests for RescoreBuilder, ShapeBuilders, IngestMetadata, HighlightBuilder, SortBuilders and SuggestionBuilders. In addition to that, changes some of the base test classes to use a random xContent type in its xContent roundtrip testing.

Relates to #5831 
</description><key id="145787418">17521</key><summary>Add more fromXContent() testing with shuffled field order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T19:55:50Z</created><updated>2016-04-06T10:44:06Z</updated><resolved>2016-04-05T08:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-04T20:39:36Z" id="205483838">I made a comment about what looks like a testing leftover. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>string.asciidoc: fix for `position_increment_gap`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17520</link><project id="" key="" /><description>Remove  outdated and duplicate description for the `position_increment_gap` parameter.
</description><key id="145777874">17520</key><summary>string.asciidoc: fix for `position_increment_gap`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">golubev</reporter><labels><label>docs</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T19:15:14Z</created><updated>2016-04-06T09:50:29Z</updated><resolved>2016-04-05T20:20:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="golubev" created="2016-04-04T19:16:06Z" id="205453334">My company - LUN UA LLC - signed the CCLA and had listed me as a contributor. And I've signed the ICCLA under that one above.
</comment><comment author="nik9000" created="2016-04-05T20:12:52Z" id="205973020">@golubev the change looks good. Thanks for pointing me to the corporate CLA. I found it and synced it into the CLA checker. By making this comment I'm triggering it to check again.
</comment><comment author="nik9000" created="2016-04-05T20:24:50Z" id="205976676">Merged!
Master: 8430b379d889b7db6172f38548e5422044fd0766
2.x: ece74c048260050b4dbeb60a5a8a54140716f21b
2.3: 0ad79285422cfff11d9d6cd012e18daa32a21e1d

Thanks @golubev !
</comment><comment author="golubev" created="2016-04-06T09:50:29Z" id="206267650">Thanks @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disk Space full + unassigned nodes after close/open index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17519</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 
1.6.0

**JVM version**:
1.8.0_45-internal

**Description of the problem including expected versus actual behavior**:
I recently closed an index to make some changes to the available analyzers. After opening the index, I quickly started seeing disk space on all of our nodes fill up. My guess is that, as part of the failed recovery, Elasticsearch may have created another replica, since each shard has a replica which is not assigned to any node in the cluster (there are currently 3 shards and 2 replicas). I will paste relevant logs and configurations below. 

**Provide logs (if relevant)**:
Here are logs from the master node on the day that the index was closed: 

```
[2016-03-18 01:54:46,161][INFO ][cluster.metadata] [instance name] closing indices [[prod]]
[2016-03-18 01:54:46,161][INFO ][cluster.metadata] [instance name] opening indices [[prod]]
[2016-03-18 01:54:48,493][WARN ][cluster.routing.allocation.decider] [instance name] After allocating, node [nodename] would have less than the required 0b free bytes threshold (-28916726190 bytes free), preventing allocation
[2016-03-18 01:54:48,494][WARN ][cluster.routing.allocation.decider] [instance name] After allocating, node [nodename] would have less than the required 0b free bytes threshold  (-29217364398 bytes free), preventing allocation
```

... one of these lines for each of the ES nodes ...

Then, there is one of these exceptions for each node:

```
[2016-03-18 01:54:49,500][DEBUG][action.search.type] [instance name] All shards failed for phase: [query]
org.elasticsearch.transport.RemoteTransportException: instance name]][indices:data/read/search[phase/query]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: [prod][0] CurrentState[RECOVERING] operations only allowed when started/relocated
    at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:1000)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:793)
    at org.elasticsearch.index.shard.IndexShard.acquireSearcher(IndexShard.java:789)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:552)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:532)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:294)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:776)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:767)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Then:

```
[2016-03-18 01:56:39,891][INFO ][cluster.metadata] [instance name] [prod] create_mapping [mapping name]
.
.
.
[2016-03-18 02:05:18,993][WARN ][cluster.action.shard] [instance name] [prod][1] received shard failed for [prod][1], node[node name], [R], s[INITIALIZING], indexUUID [index id], reason [shard failure [failed recovery][RecoveryFailedException[[prod][1]: Recovery failed from [instance name]{master=true} into [instance name]{master=false}]; nested: RemoteTransportException[[instance name][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[prod][1] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[prod][1] Failed to transfer [0] files with total size of [0b]]; nested: IllegalStateException[try to recover [prod][1] from primary shard with sync id but number of docs differ: 217250828 (instance name, primary) vs 217250830(instance name)]; ]]
```

And then a bunch of low-disk/high-disk watermark errors on each of the nodes. These errors have continued happening since the time the open/close commands were run, and as such they are preventing new data from being indexed.

When I run /cat/_shards/prod, I see:

```
index shard prirep state             docs  store ip          node                                    
prod  0     p      STARTED      218452373 73.5gb 
prod  0     r      STARTED      218452373 73.5gb 
prod  0     r      UNASSIGNED                                                                        
prod  1     p      STARTED      217445482 73.1gb 
prod  1     r      STARTED      217445482 73.1gb 
prod  1     r      UNASSIGNED                                                                        
prod  2     r      INITIALIZING                  
prod  2     r      INITIALIZING                 
prod  2     p      STARTED      218665090 73.2gb
```

and note that one of the replica shards for shard 2 oscillates between an INITIALIZING and UNASSIGNED phase.

Is anyone able to consult on the best way to resolve these issues? My best guess is to scale the # of replicas down to 0, let Elasticsearch clean up the orphaned shards, and then scale back up to 1. But, I'm not really confident enough in my knowledge of ES to move forward quite yet. 

I'm also curious if anyone can provide some more info on why opening/closing an index leads to spikes in disk usage. The docs mention that closing an index can result in such a situation, but they don't give much other background.

Happy to provide any other info if helpful. Thanks a bunch in advance!
</description><key id="145776962">17519</key><summary>Disk Space full + unassigned nodes after close/open index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshreback</reporter><labels /><created>2016-04-04T19:11:20Z</created><updated>2016-04-07T09:56:02Z</updated><resolved>2016-04-06T10:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T10:28:37Z" id="206295441">HI @joshreback 

It looks like your disks are reporting free space incorrectly:

```
After allocating, node [nodename] would have less than the required 0b free bytes threshold (-28916726190 bytes free)
```

I remember that a change was made to ignore obviously incorrect reporting of disk size (but i can't find the PR right now, so not sure what version it was in).

I suggest disabling the disk threshold decider: https://www.elastic.co/guide/en/elasticsearch/reference/1.6/index-modules-allocation.html#disk

And I'd suggest upgrading to a newer version - lots of bugs have been fixed since 1.6
</comment><comment author="joshreback" created="2016-04-07T00:05:44Z" id="206627240">Thanks @clintongormley. Ultimately fixed the problem by cutting down the # of replicas from 2 to 1, and ES cleaned itself up just fine. Out of curiosity, though, were there recovery-related bugs that you remember in 1.6? (e.g, ES inadvertently creating another replica). Just trying to understand how this issue may have cropped up in the first place.
</comment><comment author="clintongormley" created="2016-04-07T09:56:02Z" id="206791617">@joshreback difficult to say :)  Just look at the long list of [`:Recovery`](https://github.com/elastic/elasticsearch/pulls?q=is%3Apr+label%3A%3ARecovery+is%3Aclosed) and [`:Allocation`](https://github.com/elastic/elasticsearch/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+label%3A%3AAllocation+is%3Aclosed+)  changes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter aggregation with no filter results in a NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17518</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master

**JVM version**: 1.8.0_74

**OS version**: Win 10

**Description of the problem including expected versus actual behavior**:

Not really a typical use case, but an issue nonetheless caught by our integration tests.  Specifying no filter in a filter agg results in a NPE.

**Steps to reproduce**:

```
{
  "aggs": {
    "empty_filter": {
      "filter": {}
    }
  }
}
```

Response:

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "null_pointer_exception",
      "reason" : null
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "project",
      "node" : "9QsiljMeTtKbZ2FijGofvw",
      "reason" : {
        "type" : "null_pointer_exception",
        "reason" : null
      }
    } ],
    "caused_by" : {
      "type" : "null_pointer_exception",
      "reason" : null
    }
  },
  "status" : 500
}
```

**Provide logs (if relevant)**:

```
Failed to execute [org.elasticsearch.action.search.SearchRequest@3218e268]
RemoteTransportException[[readonly-node-55e651][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: NullPointerException;
Caused by: java.lang.NullPointerException
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:684)
    at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:734)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:107)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregatorFactory.&lt;init&gt;(FilterAggregatorFactory.java:46)
    at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregatorBuilder.doBuild(FilterAggregatorBuilder.java:60)
    at org.elasticsearch.search.aggregations.AggregatorBuilder.build(AggregatorBuilder.java:120)
    at org.elasticsearch.search.aggregations.AggregatorFactories$Builder.build(AggregatorFactories.java:171)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:709)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:577)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:526)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:277)
    at org.elasticsearch.search.action.SearchTransportService$SearchQueryTransportHandler.messageReceived(SearchTransportService.java:369)
    at org.elasticsearch.search.action.SearchTransportService$SearchQueryTransportHandler.messageReceived(SearchTransportService.java:366)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:65)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:468)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="145766741">17518</key><summary>Filter aggregation with no filter results in a NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2016-04-04T18:28:34Z</created><updated>2016-04-05T16:21:20Z</updated><resolved>2016-04-05T16:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-05T15:18:13Z" id="205854237">@gmarz Thanks for flagging this up. I have opened #17542 to fix this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for default date format breaking change in migration plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17517</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.x

**Description of the problem including expected versus actual behavior**:

There is a breaking change related to default date formats: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/breaking_20_mapping_changes.html#_date_fields_and_unix_timestamps

While the migration plugin cannot tell what data values users will actually pass into the date field, it will still be nice to warn users about this breaking changes or as an advisory note for this can become a breaking change at runtime.
</description><key id="145763166">17517</key><summary>Check for default date format breaking change in migration plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2016-04-04T18:17:17Z</created><updated>2016-04-06T10:21:27Z</updated><resolved>2016-04-06T10:21:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T10:21:27Z" id="206292181">This issue was moved to elastic/elasticsearch-migration#54
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in moving average agg when window isn't specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17516</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master

**JVM version**: 1.8.0_74

**OS version**: Win 10

**Description of the problem including expected versus actual behavior**:

Moving average throws a NPE when the optional `window` parameter isn't specified.

**Steps to reproduce**:

```
{
  "size": 0,
  "aggs": {
    "projects_started_per_month": {
      "date_histogram": {
        "field": "startedOn",
        "interval": "month"
      },
      "aggs": {
        "commits": {
          "sum": {
            "field": "numberOfCommits"
          }
        },
        "commits_moving_avg": {
          "moving_avg": {
            "buckets_path": "commits",
            "gap_policy": "insert_zeros",
            "model": "linear"
          }
        }
      }
    }
  }
}
```

Response:

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "null_pointer_exception",
      "reason" : null
    } ],
    "type" : "null_pointer_exception",
    "reason" : null
  },
  "status" : 500
}
```

Changing the above request and specifying a window returns the expected response.

**Provide logs (if relevant)**:

```
java.lang.NullPointerException
    at org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgParser.parse(MovAvgParser.java:166)
    at org.elasticsearch.search.aggregations.pipeline.movavg.MovAvgParser.parse(MovAvgParser.java:38)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:204)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:185)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:109)
    at org.elasticsearch.search.builder.SearchSourceBuilder.parseXContent(SearchSourceBuilder.java:864)
    at org.elasticsearch.rest.action.search.RestSearchAction.parseSearchRequest(RestSearchAction.java:133)
    at org.elasticsearch.rest.action.search.RestSearchAction.handleRequest(RestSearchAction.java:95)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:51)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:214)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:174)
    at org.elasticsearch.http.HttpServer.dispatchRequest(HttpServer.java:101)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:487)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:65)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:85)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:83)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="145759763">17516</key><summary>NullPointerException in moving average agg when window isn't specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2016-04-04T18:04:55Z</created><updated>2016-04-06T14:57:08Z</updated><resolved>2016-04-06T14:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-06T07:42:28Z" id="206187434">@gmarz thanks for raising this. I've opened a pull request to fix it: https://github.com/elastic/elasticsearch/pull/17556
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show configured and remaining delay for an unassigned shard.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17515</link><project id="" key="" /><description>When a shard is delayed, we now show output like:

``` json
{
  "shard" : {
    "index" : "i",
    "index_uuid" : "QzoKda9aQCG_hCaZQ18GEg",
    "id" : 3,
    "primary" : false
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "NODE_LEFT",
    "at" : "2016-04-04T16:44:47.520Z",
    "details" : "node_left[HyRLmMLxR5m_f58RKURApQ]"
  },
  "allocation_delay" : "59.9s",
  "allocation_delay_ms" : 59910,
  "remaining_delay" : "38.9s",
  "remaining_delay_ms" : 38991,
  "nodes" : {
    "jKiyQcWFTkyp3htyyjxoCw" : {
      "node_name" : "Landslide",
      "node_attributes" : { },
      "final_decision" : "YES",
      "weight" : 1.0,
      "decisions" : [ ]
    },
    "9bzF0SgoQh-G0F0sRW_qew" : {
      "node_name" : "Caretaker",
      "node_attributes" : { },
      "final_decision" : "NO",
      "weight" : 2.0,
      "decisions" : [ {
        "decider" : "same_shard",
        "decision" : "NO",
        "explanation" : "the shard cannot be allocated on the same node id [9bzF0SgoQh-G0F0sRW_qew] on which it already exists"
      } ]
    }
  }
}
```

Where the new addition is this section:

```
  "allocation_delay" : "59.9s",
  "allocation_delay_ms" : 59910,
  "remaining_delay" : "38.9s",
  "remaining_delay_ms" : 38991,
```

Which shows the configured delay as well as the remaining delay until
the shard can be considered "assignable". This data is only shown if the
shard is unassigned.

Relates to #17372
</description><key id="145755456">17515</key><summary>Show configured and remaining delay for an unassigned shard.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Stats</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T17:50:31Z</created><updated>2016-05-02T11:58:47Z</updated><resolved>2016-04-07T19:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-07T18:35:20Z" id="207041023">Just left a couple comments, otherwise, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI Failure] OldIndexBackwardsCompatibilityIT.testOldIndexes fails b/c payloads not found in _all search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17514</link><project id="" key="" /><description>https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/237/console

Reproduces locally for me with:

```
gradle :core:integTest -Dtests.seed=DAF558AA44C953B8 -Dtests.class=org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT -Dtests.method="testOldIndexes" -Des.logger.level=INFO -Dtests.security.manager=true -Dtests.locale=en-IE -Dtests.timezone=Europe/Lisbon
```

Seems to fail when testing against 2.3.1 and an `_all` search.  Afraid I don't know much about the backcompat tests to investigate more.  @jpountz @jimferenczi Is this perhaps related to https://github.com/elastic/elasticsearch/pull/16899 ?

```
1&gt; [2016-04-04 13:32:19,670][INFO ][org.elasticsearch.bwcompat] --&gt; Testing old index index-2.3.1.zip
  1&gt; [2016-04-04 13:32:19,755][INFO ][org.elasticsearch.bwcompat] --&gt; injecting index [index-2.3.1] into multi data path
  1&gt; [2016-04-04 13:32:19,849][WARN ][org.elasticsearch.common.util] [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path1/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.1.2] no index state found - ignoring
  1&gt; [2016-04-04 13:32:19,851][WARN ][org.elasticsearch.common.util] [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path2/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.1.2] no index state found - ignoring
  1&gt; [2016-04-04 13:32:19,853][WARN ][org.elasticsearch.common.util] [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path1/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.2.1] no index state found - ignoring
  1&gt; [2016-04-04 13:32:19,853][WARN ][org.elasticsearch.common.util] [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path2/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.2.1] no index state found - ignoring
  1&gt; [2016-04-04 13:32:19,856][INFO ][org.elasticsearch.common.util] [index-2.3.1/_llJ2G6NQfmFpn-VuRXk5Q] upgrading [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path1/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.3.1] to new naming convention
  1&gt; [2016-04-04 13:32:19,857][INFO ][org.elasticsearch.common.util] [index-2.3.1/_llJ2G6NQfmFpn-VuRXk5Q] moved from [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path1/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.3.1] to [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path1/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/_llJ2G6NQfmFpn-VuRXk5Q]
  1&gt; [2016-04-04 13:32:19,858][WARN ][org.elasticsearch.common.util] [/Users/tongz/Documents/elasticsearch_gradle/elasticsearch/core/build/testrun/integTest/J0/temp/org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT_DAF558AA44C953B8-002/tempDir-002/multi-path2/TEST-CHILD_VM=[0]-CLUSTER_SEED=[-7196169852749911737]-HASH=[F4A1A95576A]-cluster/nodes/0/indices/index-2.3.1] no index state found - ignoring
  1&gt; [2016-04-04 13:32:19,867][INFO ][org.elasticsearch.gateway] [node_t2] [[index-2.3.1/_llJ2G6NQfmFpn-VuRXk5Q]] dangling index, exists on local file system, but not in cluster metadata, auto import to cluster state
  1&gt; [2016-04-04 13:32:19,872][WARN ][org.elasticsearch.index  ] [node_t0] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:19,875][INFO ][org.elasticsearch.gateway] [node_t0] auto importing dangled indices [[index-2.3.1/_llJ2G6NQfmFpn-VuRXk5Q]/OPEN] from [{node_t2}{xo2bfVhdSFepOjnC3EER5Q}{local}{local[3]}]
  1&gt; [2016-04-04 13:32:19,880][WARN ][org.elasticsearch.index  ] [node_t2] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:19,905][WARN ][org.elasticsearch.index  ] [node_t2] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:19,978][INFO ][org.elasticsearch.cluster.routing.allocation] [node_t0] Cluster health status changed from [RED] to [GREEN] (reason: [shards started [[index-2.3.1][0]] ...]).
  1&gt; [2016-04-04 13:32:19,979][WARN ][org.elasticsearch.index  ] [node_t1] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:19,996][WARN ][org.elasticsearch.index  ] [node_t0] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:20,020][INFO ][org.elasticsearch.bwcompat] --&gt; testing basic search
  1&gt; [2016-04-04 13:32:20,022][INFO ][org.elasticsearch.bwcompat] Found 2499 in old index
  1&gt; [2016-04-04 13:32:20,023][INFO ][org.elasticsearch.bwcompat] --&gt; testing basic search with sort
  1&gt; [2016-04-04 13:32:20,028][INFO ][org.elasticsearch.bwcompat] --&gt; testing exists filter
  1&gt; [2016-04-04 13:32:20,032][INFO ][org.elasticsearch.bwcompat] --&gt; testing _all search
  1&gt; [2016-04-04 13:32:20,041][INFO ][org.elasticsearch.bwcompat] [OldIndexBackwardsCompatibilityIT#testOldIndexes]: finished test
  1&gt; [2016-04-04 13:32:20,049][INFO ][org.elasticsearch.bwcompat] [OldIndexBackwardsCompatibilityIT#testOldIndexes]: cleaning up after test
  1&gt; [2016-04-04 13:32:20,079][WARN ][org.elasticsearch.index  ] [node_t1] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:20,080][WARN ][org.elasticsearch.index  ] [node_t1] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:20,120][WARN ][org.elasticsearch.index  ] [node_t0] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:20,121][WARN ][org.elasticsearch.index  ] [node_t0] [index-2.3.1] [index.merge.enabled] is set to false, this should only be used in tests and can cause serious problems in production environments
  1&gt; [2016-04-04 13:32:20,143][INFO ][org.elasticsearch.node   ] [node_t0] stopping ...
  1&gt; [2016-04-04 13:32:20,146][INFO ][org.elasticsearch.node   ] [node_t0] stopped
  1&gt; [2016-04-04 13:32:20,146][INFO ][org.elasticsearch.node   ] [node_t0] closing ...
  1&gt; [2016-04-04 13:32:20,157][INFO ][org.elasticsearch.node   ] [node_t0] closed
  1&gt; [2016-04-04 13:32:20,159][INFO ][org.elasticsearch.node   ] [node_t1] stopping ...
  1&gt; [2016-04-04 13:32:20,160][INFO ][org.elasticsearch.node   ] [node_t1] stopped
  1&gt; [2016-04-04 13:32:20,160][INFO ][org.elasticsearch.node   ] [node_t1] closing ...
  1&gt; [2016-04-04 13:32:20,164][INFO ][org.elasticsearch.node   ] [node_t1] closed
  1&gt; [2016-04-04 13:32:20,167][INFO ][org.elasticsearch.node   ] [node_t2] stopping ...
  1&gt; [2016-04-04 13:32:20,168][INFO ][org.elasticsearch.node   ] [node_t2] stopped
  1&gt; [2016-04-04 13:32:20,168][INFO ][org.elasticsearch.node   ] [node_t2] closing ...
  1&gt; [2016-04-04 13:32:20,176][INFO ][org.elasticsearch.node   ] [node_t2] closed
  1&gt; [2016-04-04 13:32:20,176][INFO ][org.elasticsearch.bwcompat] [OldIndexBackwardsCompatibilityIT#testOldIndexes]: cleaned up after test
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=DAF558AA44C953B8 -Dtests.class=org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT -Dtests.method="testOldIndexes" -Des.logger.level=INFO -Dtests.security.manager=true -Dtests.locale=en-IE -Dtests.timezone=Europe/Lisbon
FAILURE 25.3s | OldIndexBackwardsCompatibilityIT.testOldIndexes &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: Could not find payload boost in explanation
   &gt; 2.4764225 = sum of:
   &gt;   2.4764225 = weight(_all:36 in 0) [PerFieldSimilarity], result of:
   &gt;     2.4764225 = fieldWeight in 0, product of:
   &gt;       1.0 = tf(freq=1.0), with freq of:
   &gt;         1.0 = termFreq=1.0
   &gt;       4.952845 = idf(docFreq=47, docCount=2499)
   &gt;       0.5 = fieldNorm(doc=0)
   &gt;   0.0 = match on required clause, product of:
   &gt;     0.0 = # clause
   &gt;     0.20190416 = _type:doc, product of:
   &gt;       1.0 = boost
   &gt;       0.20190416 = queryNorm
   &gt;    at __randomizedtesting.SeedInfo.seed([DAF558AA44C953B8:FD2F1AFC0324C60F]:0)
   &gt;    at org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT.assertAllSearchWorks(OldIndexBackwardsCompatibilityIT.java:403)
   &gt;    at org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT.assertOldIndexWorks(OldIndexBackwardsCompatibilityIT.java:320)
   &gt;    at org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT.testOldIndexes(OldIndexBackwardsCompatibilityIT.java:305)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

```
org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT testOldIndexes
FAILED: 1
ERROR: 0
SKIPPED: 66
TOTAL: 6145

BUILD INFO

Build   20160404170608-D7238A4C
Log https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-intake/237/console
Duration    14m 54s (894774ms)
Started 2016-04-04T17:06:08.441Z
Ended   2016-04-04T17:21:03.215Z
Exit Code   1
Host    slave-1cdb68c6 (up 90 days)
OS  Fedora 23, Linux 4.2.8-300.fc23.x86_64
Specs   4 CPUs, 15.67GB RAM
java.version    1.8.0_65
java.vm.name    OpenJDK 64-Bit Server VM
java.vm.version 25.65-b01
java.runtime.version    1.8.0_65-b17
java.home   /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-13.b17.fc23.x86_64
```
</description><key id="145753924">17514</key><summary>[CI Failure] OldIndexBackwardsCompatibilityIT.testOldIndexes fails b/c payloads not found in _all search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>test</label></labels><created>2016-04-04T17:43:53Z</created><updated>2016-04-05T09:20:47Z</updated><resolved>2016-04-05T09:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-04-04T20:19:09Z" id="205477496">Seemed to be failing consistently so I went ahead and silenced the test for now to keep the noise down.
</comment><comment author="jimczi" created="2016-04-05T07:24:57Z" id="205694727">This is related to this commit https://github.com/elastic/elasticsearch/commit/b7fb34fed23976ba341873f6244ce982e0a79b9f
I'll investigate but I guess it is due to a misconfiguration during the creation of the 2.3.1 index. 
@clintongormley where can I find the procedure to create the bwc indices ?
</comment><comment author="jpountz" created="2016-04-05T07:32:31Z" id="205699262">@jimferenczi it is in `dev-tools/create_bwc_index.py`
</comment><comment author="jimczi" created="2016-04-05T08:19:05Z" id="205716563">Thanks @jpountz I opened https://github.com/elastic/elasticsearch/pull/17530
</comment><comment author="jimczi" created="2016-04-05T09:20:47Z" id="205725392">Fixed by https://github.com/elastic/elasticsearch/pull/17530
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a more descriptive example to Index Template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17513</link><project id="" key="" /><description>The current example in the documentation for Index Templates lacks any properties values. This is helpful to many devs that aren't sure how to take a regular Index Mapping and convert it to a template.
</description><key id="145750766">17513</key><summary>Add a more descriptive example to Index Template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lababidi</reporter><labels /><created>2016-04-04T17:29:17Z</created><updated>2016-04-06T10:19:02Z</updated><resolved>2016-04-06T10:18:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T10:19:01Z" id="206290958">thanks @lababidi - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail requests on deprecated syntax</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17512</link><project id="" key="" /><description>**Describe the feature**:
I'd be useful to have a switch that you could set that would fail a request that uses deprecated syntax. It'd be must useful for things like creating indexes, where the deprecated settings live for a long time. I talked to some folks who started using Elasticsearch on 1.x, wanted to migrate to 2.x, but were a bit stuck because their mappings contained syntax from 0.90. They never meant to use deprecated syntax, they just did on accident.
</description><key id="145729054">17512</key><summary>Fail requests on deprecated syntax</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>enhancement</label></labels><created>2016-04-04T16:03:24Z</created><updated>2016-08-04T11:15:44Z</updated><resolved>2016-07-27T15:00:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-04T17:11:04Z" id="205398183">We already have `index.query.parse.strict` but I agree we need something that applies to the whole request and not only queries.
</comment><comment author="jpountz" created="2016-04-04T17:12:22Z" id="205398555">See also #8963.
</comment><comment author="javanna" created="2016-04-04T17:13:32Z" id="205398881">I think the problem is just renaming the setting that turns this on. It's already used outside of queries as far as I can remember, pretty much applied to the whole REST layer (wherever we use ParseField). Plus we should switch more things to ParseField.
</comment><comment author="clintongormley" created="2016-04-06T10:09:40Z" id="206284666">The switch should upgrade anything going to the deprecated log to be an exception.
</comment><comment author="clintongormley" created="2016-04-06T10:10:53Z" id="206285820">This would be particularly useful for cross-stack integration tests - it would help alert the logstash, beats, and kibana teams early on to their use of deprecated syntax.
</comment><comment author="clintongormley" created="2016-07-27T15:00:37Z" id="235613167">I think we can close this now that https://github.com/elastic/elasticsearch/issues/17687 has been merged
</comment><comment author="javanna" created="2016-07-27T15:05:29Z" id="235614810">Sorry Clint, me again :) Wasn't #17687 about returning a warning header, while this issue was about failing requests completely? Is failing a request possible when it contains usage of any deprecated syntax?
</comment><comment author="jpountz" created="2016-07-27T16:29:20Z" id="235641747">I was initially more in favour of failing requests, but now that I think more about it I think a warning is good enough, then the client can deal with the warnings however it wants, similarly to failed or missing shards. This way, we also don't have to support yet another request parameter, we can just send warnings all the time and let clients deal with it.
</comment><comment author="javanna" created="2016-07-27T16:59:22Z" id="235650389">I am digging what you propose @jpountz , I like it. I think that some of our language clients don't expose headers though. I don't think there would be any problem other than that. Nothing needs to be enabled, you always get warnings, you just have to decide what to do with them. Sounds good to me.
</comment><comment author="nik9000" created="2016-07-27T17:02:57Z" id="235651436">What about `curl`? I guess if sense can be made to complain appropriately on those headers we can be in good shape?

I can put together something for the rest tests and docs to fail if they see this header unless it is explicitly marked as expected if we're truly ok with just the Warnings.

Also, if we're ok with warning headers then should we just remove the strict parsing entirely? Relates to https://github.com/elastic/elasticsearch/issues/19552
</comment><comment author="jpountz" created="2016-07-27T17:24:35Z" id="235657559">&gt; I guess if sense can be made to complain appropriately on those headers we can be in good shape?

This is a great idea!!

&gt; Also, if we're ok with warning headers then should we just remove the strict parsing entirely?

I think it's worth discussing indeed...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field Aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17511</link><project id="" key="" /><description>**Describe the feature**:
Say you have documents like

``` json
{
  "description": "lorem ipsum",
  "customer": {
    "id": 2134,
    "name": "Test Enterprises"
  }
}
```

But you really want to search like `customer:2134` to find this issue. It'd be useful to be able to define `customer` as an alias for `customer.id` or maybe `customer.id^10, customer.name`.

Traditionally this has been something Elasticsearch has left to the user but maybe it is time to talk about implementing this in Elasticsearch?
</description><key id="145727931">17511</key><summary>Field Aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-04T15:59:08Z</created><updated>2017-06-06T10:27:45Z</updated><resolved>2016-08-26T11:07:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T10:06:43Z" id="206281256">Hi @nik9000 

I closed a very similar issue recently (https://github.com/elastic/elasticsearch/issues/17163).  I'd much prefer not to clutter our mappings with something that seems to fit nicely into the application layer.  Why do you feel this is needed?
</comment><comment author="nik9000" created="2016-04-06T13:50:13Z" id="206381086">Sorry, I hadn't searched the issues before opening this. I should have done that. 20 days ago is super recent.

I agree it isn't a thing you'd expect from a database. It is more a feature you expect from a search interface which is why we've said "make it the application's problem". But now that we are doing ingest node, it feels like we are more willing to do things to make the "elasticsearch not wrapped in application code" experience nicer. This feels like an extension of that.
</comment><comment author="clintongormley" created="2016-04-06T14:18:29Z" id="206393104">With the ingest node, you can now rename the fields yourself.  This is why I'm wondering why we also need field aliases.  Also, `boost` in the mappings is now implemented as a query time boost instead of an index time boost.   If https://github.com/elastic/elasticsearch/issues/16920 makes progress, we could even allow updating the `boost` parameter on existing fields, in which case your `foo^10` example wouldn't be necessary either.
</comment><comment author="nik9000" created="2016-04-06T14:22:34Z" id="206396494">&gt; now rename

That is fine if Elasticsearch is just your search engine. But if it is your document store then renaming fields gets really confusing really fast.

I certainly wasn't thinking of aliases as a way to get updatable boosts. More of a way to get multiple fields. Kind of like a query time brother to copy_to.
</comment><comment author="ejsmith" created="2016-04-06T17:47:35Z" id="206485475">In our app we are exposing a search box that is just sent directly to query_string. We don't want to modify our documents because ES is our document store, but we want to give end users a friendly experience where they can type company:blah instead of data.company.id:blah

I understand that you guys wanted to make things simpler and got rid of just_name because it was confusing, but I think adding alias support could be intuitive and work great for things like this.

Alternatively, I will need to parse the users query myself and rewrite it. I feel like this has got to be a very common thing.

What is the downside of aliases?
</comment><comment author="clintongormley" created="2016-04-07T08:54:42Z" id="206767494">&gt; but we want to give end users a friendly experience where they can type company:blah instead of data.company.id:blah

Makes sense.

&gt; What is the downside of aliases?

Code complexity.  The mapping code is seriously complex and prone to bugs.  You just need to search for all the issues labelled `:Mapping` to get an idea of problematic they have been.  For this reason, I'm loathe to add further complexity.  

For instance, possibly the most obvious way of adding field aliases would be to add a field of type `alias`, which points to the real field.  But now you want to change or remove those aliases.  This is a huge change from the way we deal with mappings today: fields never change type and fields never get removed.  What happens if you index into an alias field? etc etc

I could see field aliases having all sorts of unintended consequences - this is why they concern me.  Yet they are really easy to implement application side.  There are so many other truly useful things that we'd like to add to Elasticsearch - things that can't easily be done in the application - that something like field aliases doesn't make the cut in my opinion.
</comment><comment author="ejsmith" created="2016-04-07T13:27:21Z" id="206901183">What if aliases were their own feature separate from mapping and were only applied to queries. The aliases would be listed separate from the field mappings so it would be a lot more straightforward to see what is going on vs using something like just_name which I could see would be very confusing.

It wouldn't be the end of the world if I had to implement this in my app, but it just feels like something that should've solved by ElasticSEARCH and seems like it would be a very common thing.
</comment><comment author="niemyjski" created="2016-04-08T14:43:07Z" id="207458867">I agree with @ejsmith 
</comment><comment author="gmoskovicz" created="2016-04-11T11:56:30Z" id="208307988">@ejsmith 

I think that this can be really useful as well, however it shouldn't be so hard to apply something to the search box before searching for something in order to translate a query to it's alias. 

If we add this to any structure, mapping or not, we should keep the aliases for each field and document and index in either the mapping or the cluster state. I believe that the infromation related to structures are **only** stored in the mapping, hence why if we add this it doesn't make sense to use another structure as it will break the concept of mapping, and why they exist. @clintongormley am i right?

The good thing about this is that there is an easy workaround: translating this at the app level, you just keep the structure in your end, and if you have an alias `a: a.id` and a query string `a:id_search` you need to translate this into `a.id:id_search`. 

Another complex workaround (i believe), could be creating a small plugin, but that can be tricky since you will need to find your way to store this.

Finally, you could use the [Mapping Metadata](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-meta-field.html) to save this. But then i believe you should load the information at app start time to avoid GET operations only to get this information from the mapping. @clintongormley shouldn't the meta field be an option to store this information about field "aliases"? That can be an option to not include this straight away in the mapping.
</comment><comment author="JackRyanson" created="2016-08-23T23:09:30Z" id="241908716">gentlemen, i would see a very important use case for this: on the fly mapping of an index to another

say i have LOG1 which contains IP addresses under a field name e.g. SRC_IP  and LOG2 that contains IP addresse under a completely different name e.g. .. LOGGEDIP  ..  would this feature not allow one to create an alias (index alias) which mapes the 2 indexes into one.. and then maps the 2 fields into what would look like the same field.. therefore allowing me to ask "top 10 IPs" etc.. across the 2 indexes.

I think this would be tremendously poweful and enabling. Thoughts? thanks for considering
</comment><comment author="clintongormley" created="2016-08-26T11:07:07Z" id="242703687">We discussed this in FixItFriday today.  The comment in https://github.com/elastic/elasticsearch/issues/17511#issuecomment-241908716 made me think that perhaps this could be a solution for https://github.com/elastic/elasticsearch/issues/18195#issuecomment-238841499

Turns out that adding field aliases would be massively complex, and wouldn't provide a proper solution.  This is more than just queries.  Field rewriting would need to happen in queries, aggs, highlighting, stored fields, docvalue_fields, suggesters, source filtering, security etc, both in requests and responses.  It wouldn't be clean or obvious, we would leak the real field names all over the place.   

We have decided against doing this.
</comment><comment author="niemyjski" created="2016-08-26T15:43:37Z" id="242771765">@clintongormley I don't think anyone here expects you guys to rewrite the response anywhere, we just want an easier way to query. The results should stay the same!
</comment><comment author="ejsmith" created="2016-08-26T17:58:28Z" id="242806384">Exactly. Really just for query string filters that users would type in directly
</comment><comment author="luckydonald" created="2017-06-06T10:27:45Z" id="306445648">You can write rather complex queries in there, and `query.replace("customer:", "customer.id:")` feels dirty for a reason.

One reason for doing this on elastic side is, you already doing some user input verification, and probably parse this way better as the user application anyway.

So maybe just a rewrite for the single  query, as part of `query_string`?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in BoolQueryBuilder.doXArrayContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17510</link><project id="" key="" /><description>Hi, i'm getting an error but only when i have a massive requests. Let me explain, I have a service that communicate with ES, and my service receives a lot of request per second, and in a random way i'm getting this error (please see the stacktrace) when I build the request to ES from the service, is not always the same request (i have a stack of request with performance test), for example, from 100 request 4 fail, and in another time 0 fails, it's random. When the tests run, are the unique application that is consuming the service. In average there are 10 concurrent user ( configuration with Gatling test)  requesting to the service.

For my case it's only with a BoolQueryBuilder (all queries are builded using this class)

My configuration is
- 1.7.2 ES version 
- 1.7.2 version of Java connector.
- Spring Boot 1.2.3.RELEASE version
- Java 1.8.0_25 version

This is my stacktrace

```
java.lang.NullPointerException: null
    at org.elasticsearch.index.query.BoolQueryBuilder.doXArrayContent(BoolQueryBuilder.java:181) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.index.query.BoolQueryBuilder.doXContent(BoolQueryBuilder.java:152) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.index.query.BaseQueryBuilder.toXContent(BaseQueryBuilder.java:66) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.index.query.BoolQueryBuilder.doXArrayContent(BoolQueryBuilder.java:181) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.index.query.BoolQueryBuilder.doXContent(BoolQueryBuilder.java:150) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.index.query.BaseQueryBuilder.toXContent(BaseQueryBuilder.java:66) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.action.support.QuerySourceBuilder.innerToXContent(QuerySourceBuilder.java:59) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.search.builder.SearchSourceBuilder.innerToXContent(SearchSourceBuilder.java:789) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.search.builder.SearchSourceBuilder.toXContent(SearchSourceBuilder.java:767) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:756) ~[elasticsearch-1.7.2.jar!/:na]
    ... 81 common frames omitted
Wrapped by: org.elasticsearch.search.builder.SearchSourceBuilderException: Failed to build search source
    at org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:759) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.action.search.SearchRequest.source(SearchRequest.java:253) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.action.search.SearchRequestBuilder.request(SearchRequestBuilder.java:1102) ~[elasticsearch-1.7.2.jar!/:na]
    at org.elasticsearch.action.search.MultiSearchRequestBuilder.add(MultiSearchRequestBuilder.java:57) ~[elasticsearch-1.7.2.jar!/:na]
    at com.search.repository.SearchRepository.executeQuery(SearchRepository.java:86) ~[search-io-service-sdk-1.1.0-SNAPSHOT.jar!/:na]
    at com.search.repository.SearchRepository$$FastClassBySpringCGLIB$$dcf9ea4e.invoke(&lt;generated&gt;) [spring-core-4.1.6.RELEASE.jar!/:na]
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204) [spring-core-4.1.6.RELEASE.jar!/:4.1.6.RELEASE]
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:717) [spring-aop-4.1.6.RELEASE.jar!/:4.1.6.RELEASE]
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) [spring-aop-4.1.6.RELEASE.jar!/:4.1.6.RELEASE]
    at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:136) [spring-tx-4.1.6.RELEASE.jar!/:4.1.6.RELEASE]
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) [spring-aop-4.1.6.RELEASE.jar!/:4.1.6.RELEASE]
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:653) [spring-aop-4.1.6.RELEASE.jar!/:4.1.6.RELEASE]
```

I need to know if this problem could be caused by the concurrent user requests to the service , or could be a ES issue. If you need more details, please let me know.

Thank you :)
</description><key id="145723462">17510</key><summary>NullPointerException in BoolQueryBuilder.doXArrayContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">diego522</reporter><labels /><created>2016-04-04T15:45:10Z</created><updated>2016-04-06T10:04:56Z</updated><resolved>2016-04-06T10:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T10:04:56Z" id="206279899">Hi @diego522 

You haven't given us any indication of what requests you are running  other than the fact that they're "massive", but I don't know what massive means here, or what your code looks like.  Also, you're on an old version. This code has been completely rewritten in later versions.  BoolQueryBuilder no longer exists in 5.0.

If you provide more info, somebody may be interested in looking at it, but given the changes that have happened subsequently it is unlikely.  I'd suggest testing out your code on a later version and reopening this bug (with more info) if you can still replicate the issue.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When considering the size of shadow replica shards, set size to 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17509</link><project id="" key="" /><description>Otherwise, when trying to calculate the amount of disk usage _after_ the
shard has been allocated, it has incorrectly subtracted the shadow
replica size.

Resolves #17460
</description><key id="145702906">17509</key><summary>When considering the size of shadow replica shards, set size to 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>bug</label><label>v2.2.2</label><label>v2.3.2</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T14:37:06Z</created><updated>2016-04-07T17:44:26Z</updated><resolved>2016-04-07T17:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-04-07T17:25:14Z" id="207009917">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Splits `phrase` and `phrase_prefix` in match query into `MatchPhraseQueryBuilder` and `MatchPhrasePrefixQueryBuilder`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17508</link><project id="" key="" /><description>The `phrase` and `phrase_prefix` options in the `MatchQueryBuilder` have been deprecated in favour of using the new `MatchPhraseQueryBuilder` and `MatchPhrasePrefixQueryBuilder`. This means that there is only one name for each query and also means that we can better validate the options which are compatible (for example you could previously define a `match_phrase` query and add fuzziness options which actually had no effect on the query)

This is not a breaking change since `MatchQueryBuilder` still supports `phrase` and `phrase_prefix` but this option will be removed from the `MatchQueryBuilder` in the future (probably in 6.0)

Relates to https://github.com/elastic/elasticsearch/pull/17458#discussion_r58351998
</description><key id="145694015">17508</key><summary>Splits `phrase` and `phrase_prefix` in match query into `MatchPhraseQueryBuilder` and `MatchPhrasePrefixQueryBuilder`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>deprecation</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T14:09:34Z</created><updated>2016-05-02T11:58:15Z</updated><resolved>2016-04-05T11:55:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-04T14:41:04Z" id="205327115">left a few comments, looks good though. I also wonder if it makes sense to split the MatchQuery at the lucene level, seems that we have a few switches and instanceof checks that depend on the selected type. That can wait for another PR though.
</comment><comment author="colings86" created="2016-04-04T14:47:59Z" id="205329970">Yeah it might be a good idea to split the MatchQuery too, haven\t looked at how easy/hard that would be yet though
</comment><comment author="javanna" created="2016-04-05T10:22:06Z" id="205744919">left a few more comments, it's close though
</comment><comment author="javanna" created="2016-04-05T11:15:46Z" id="205759401">LGTM besides those last minor comments I left.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate mlt, in and geo_bbox query name shortcuts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17507</link><project id="" key="" /><description>If we have a deprecated query name, at the moment we don't have a way to log any deprecation warning. With this PR we use ParseField, so that we can properly deprecate query synonyms and have one single name for each query. This PR effectively deprecates  `in`, `mlt` and `geo_bbox` in favour of their complete names `terms`, `more_like_this` and `geo_bounding_box`.
</description><key id="145690947">17507</key><summary>Deprecate mlt, in and geo_bbox query name shortcuts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T13:57:35Z</created><updated>2016-05-02T12:05:09Z</updated><resolved>2016-04-05T13:38:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-04T13:58:44Z" id="205306349">This does not even compile at the moment, it is just a prototype to share with @colings86 @nik9000 and @cbuescher can have a look before I go ahead with the change. I moved only the newer registration method to ParseField to minimize the work at this first stage.
</comment><comment author="colings86" created="2016-04-04T14:07:16Z" id="205308716">@javanna I left a couple of comments but I like it
</comment><comment author="cbuescher" created="2016-04-04T14:32:41Z" id="205323664">I also think this looks good, two minor comments, the second is also just idea maybe for future improvements, don't know if that would work currently.
</comment><comment author="javanna" created="2016-04-04T18:48:30Z" id="205444198">I updated the PR and removed the WIP label. I addressed all comments, don't get fooled by the size, it's the same small change all over the place, plus some line length problems addressed while I was at it. Most of the change is in SearchModule, where I made some of the changes that Nik made as part of #17458. We also have a test for registered queries in SearchModuleTests, let me know what you think!
</comment><comment author="nik9000" created="2016-04-04T19:18:25Z" id="205454064">I'm fine with the code. We'll need to add stuff to the breaking changes docs I think. As well as put the versions when things were deprecated in the docs.
</comment><comment author="cbuescher" created="2016-04-04T19:19:01Z" id="205454197">I took a look, this looks good to me.
</comment><comment author="javanna" created="2016-04-04T19:56:07Z" id="205468863">&gt; We'll need to add stuff to the breaking changes docs I think.

but nothing is breaking here :) I am not sure whether we have a deprecation list somewhere, I will ask around.
</comment><comment author="nik9000" created="2016-04-04T20:13:00Z" id="205475390">The breaking changes list is the deprecated list, iirc.
</comment><comment author="colings86" created="2016-04-05T08:56:44Z" id="205720323">@javanna I added one more comment but otherwise it LGTM
</comment><comment author="colings86" created="2016-04-05T13:05:45Z" id="205792646">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't use org.elasticsearch.common.geo.GeoDistance.ARC anymore in scripts: class is not public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17506</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.0

**JVM version**: openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)

**OS version**: https://github.com/docker-library/elasticsearch/blob/bceef593300c241757fe93737f0e4502a269f122/2.3/Dockerfile

**Description of the problem including expected versus actual behavior**:
In my scripts, I NEED to use `org.elasticsearch.common.geo.GeoDistance.ARC`.
For various reasons, I can't use the Groovy "document field distance" methods.

_It works with Elasticsearch 1.7 or 2.2._

**Steps to reproduce**:
Do this search:

```
{
  "script_fields": {
    "test": {
      "script": "return org.elasticsearch.common.geo.GeoDistance.ARC.calculate(0, 0, 1, 1, org.elasticsearch.common.unit.DistanceUnit.METERS)"
    }
  }
}
```

**Provide logs (if relevant)**:

```
{
"took": 6,
"timed_out": false,
"_shards": {
"total": 5,
"successful": 4,
"failed": 1,
"failures": [
{
"shard": 0,
"index": "xxx",
"node": "Fnsq89X8R8m7aeO8jqUjgw",
"reason": {
"type": "script_exception",
"reason": "failed to run inline script [return org.elasticsearch.common.geo.GeoDistance.ARC.calculate(0, 0, 1, 1, org.elasticsearch.common.unit.DistanceUnit.METERS)] using lang [groovy]",
"caused_by": {
"type": "groovy_bug_error",
"reason": "BUG! UNCAUGHT EXCEPTION: class is not public: org.elasticsearch.common.geo.GeoDistance$3.calculate(double,double,double,double,DistanceUnit)double/invokeSpecial, from org.codehaus.groovy.vmplugin.v7.IndyInterface",
"caused_by": {
"type": "illegal_access_exception",
"reason": "class is not public: org.elasticsearch.common.geo.GeoDistance$3.calculate(double,double,double,double,DistanceUnit)double/invokeSpecial, from org.codehaus.groovy.vmplugin.v7.IndyInterface"
}
}
}
}
]
},
"hits": {
"total": 147356,
"max_score": 1,
"hits": [ ]
}
}
```
</description><key id="145688604">17506</key><summary>Can't use org.elasticsearch.common.geo.GeoDistance.ARC anymore in scripts: class is not public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">pierrre</reporter><labels><label>:Geo</label><label>adoptme</label><label>enhancement</label></labels><created>2016-04-04T13:48:08Z</created><updated>2016-07-22T20:15:41Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pierrre" created="2016-04-05T09:55:12Z" id="205735338">By the way, my Elasticsearch config:

``` yaml
script.inline: on
```

And my java policy file:

```
grant {
    permission org.elasticsearch.script.ClassPermission "*"; // allow all (disables filtering basically)
};
```
</comment><comment author="pierrre" created="2016-04-05T09:56:44Z" id="205735712">I can "fix" the issue with `security.manager.enabled: false` in my Elasticsearch config, but it's deprecated....
</comment><comment author="clintongormley" created="2016-04-06T10:56:39Z" id="206309155">@nknize can you suggest anything?
</comment><comment author="spideyfusion" created="2016-04-27T11:42:11Z" id="215057398">Can we get some more feedback regarding this? I'm running into the exact same issue. As @pierrre mentioned everything works fine on version **2.2**.
</comment><comment author="nknize" created="2016-04-27T13:38:52Z" id="215085561">Sorry for the delay @spideyfusion and @pierrre. I'll dig today and see what, if any, security manager changes might have caused this. Its unrelated to any of the geo changes.
</comment><comment author="coldlamper" created="2016-04-28T18:06:42Z" id="215513289">I have the same problem after upgrading to 2.3.2 from 2.2.1.

``` javascript
{"took":543,"timed_out":false,"_shards":
{"total":5,"successful":0,"failed":5,"failures":     [{"shard":0,"index":"1457361018254","node":"T8SYOug2RemSSDCLclwGog","reason":{
"type":"script_exception",
"reason":"failed to run file script [closest_listing] using lang [groovy]",
"caused_by":{"type":"groovy_bug_error",
"reason":"BUG! UNCAUGHT EXCEPTION: class is not public: org.elasticsearch.common.geo.GeoDistance$3.calculate(double,double,double,double,DistanceUnit)double/invokeSpecial, from org.codehaus.groovy.vmplugin.v7.IndyInterface","caused_by":{
"type":"illegal_access_exception",
"reason":"class is not public: org.elasticsearch.common.geo.GeoDistance$3.calculate(double,double,double,double,DistanceUnit)double/invokeSpecial, from org.codehaus.groovy.vmplugin.v7.IndyInterface"}}}}]},"hits":{"total":4443,"max_score":6.40699,"hits":[]}}
```

My policy file looks like

```
grant {
    permission org.elasticsearch.script.ClassPermission "*"; // allow all (disables filtering basically)
};
```

Maybe this helps. If I change the policy file to 

```
grant {
      permission java.security.AllPermission;    
};
```

then everything works.
</comment><comment author="rjernst" created="2016-04-28T23:16:59Z" id="215590155">The problem here is a combination of groovy craziness with the GeoDistance class and how java handles enums.  Each enum value is a pkg protected anonymous class. Due to how groovy handles grabbing stuff when loading, it tries to get at this class, and that is where the error comes from.  In order to fix this, the GeoDistance class needs to not be so crazy. We shouldn't be doing crazy things like this with an enum. /cc @nknize 
</comment><comment author="pierrre" created="2016-04-29T12:37:40Z" id="215699863">I confirm, it works with:

```
grant {
      permission java.security.AllPermission;    
};
```

in java policy file and without modifying `security.manager.enabled`.

Thank you @coldlamper :+1: 
</comment><comment author="spideyfusion" created="2016-04-29T14:27:40Z" id="215735163">@coldlamper @pierrre Loosening the security policy like that is definitely something I wouldn't even consider doing in a production environment.
</comment><comment author="pierrre" created="2016-04-29T14:38:21Z" id="215739043">@spideyfusion I understand, but:
- my Elasticsearch server is not publicly available (ports 9200/9300 are not open)
- I don't run untrusted scripts

Is there a security issue that I don't see?
</comment><comment author="spideyfusion" created="2016-04-29T14:48:39Z" id="215741899">@pierrre You're basically crippling the security manager this way:

https://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html#AllPermission
</comment><comment author="pierrre" created="2016-04-29T14:54:44Z" id="215743833">@spideyfusion 

```
This permission should be used only during testing, 
in extremely rare cases where an application or applet is completely trusted
and adding the necessary permissions to the policy is prohibitively cumbersome.
```
- `where an application or applet is completely trusted` I "trust" Elasticsearch, and I don't run unstrusted scripts in my queries
- `adding the necessary permissions to the policy is prohibitively cumbersome` I just want to use the `GeoDistance` class. Do you know a safer way to use it?
</comment><comment author="rjernst" created="2016-05-15T04:05:00Z" id="219265151">@pierrre Have you tried using the distance methods available for fields?

```
{
  "script_fields": {
    "test": {
      "script": "doc['mygeofield'].arcDistance(someLat, someLon)"
    }
  }
}
```

However, as I alluded to in my previous comment here, this is a "bug" in that something about the craziness of how GeoDistance is implemented causes this problem. Pinging @nknize one more time to look into refactoring so it doesn't try to be so fancy, but I've also marked this as adoptme.
</comment><comment author="pierrre" created="2016-05-19T08:22:37Z" id="220258710">@rjernst I can't use `arcDistance()` because I have several geopoints in my document, and I want to sort/extract data with these geopoints.

My mapping looks like this (PHP representation of my JSON):

``` php
        return [
            'title' =&gt; [
                'type' =&gt; 'string',
                'analyzer' =&gt; 'plyce_french',
            ],
            'priority' =&gt; [
                'type' =&gt; 'integer',
            ],
            'stores' =&gt; [
                'type' =&gt; 'nested',
                'include_in_parent' =&gt; true,
                'properties' =&gt; [
                    'id' =&gt; [
                        'type' =&gt; 'string',
                        'index' =&gt; 'not_analyzed',
                    ],
                    'location' =&gt; [
                        'type' =&gt; 'geo_point',
                    ]
                ],
            ],
        ];
```

My sort script:

``` php
        $script = &lt;&lt;&lt;'EOT'
import org.elasticsearch.common.unit.DistanceUnit;
import org.elasticsearch.common.geo.GeoDistance;
distanceMin = null;
for (storeLocation in doc["stores.location"].values) {
    distance = GeoDistance.ARC.calculate(location.lat, location.lon, storeLocation.lat, storeLocation.lon, DistanceUnit.METERS);
    if (distanceMin == null || distance &lt; distanceMin) {
        distanceMin = distance;
    }
}
if (distanceMin == null) {
    distanceMin = 1000000000;
}
prioritySort = pow(2, doc.priority.value);
return distanceMin / (_score * prioritySort);
EOT;
        return [
            [
                '_script' =&gt; [
                    'script' =&gt; $script,
                    'type' =&gt; 'number',
                    'params' =&gt; [
                        'location' =&gt; $this-&gt;getParamLocation($params)-&gt;toArray(true),
                    ],
                    'order' =&gt; 'asc',
                ],
            ],
            '_uid', // stable sort
        ];
```

I could use `arcDistance()` if I was able to iterate over stored values instead of the document's values.

And my script field:

``` php
        $script = &lt;&lt;&lt;'EOT'
import org.elasticsearch.common.unit.DistanceUnit;
import org.elasticsearch.common.geo.GeoDistance;
nearestStore = null;
distanceMin = null;
for (store in _source.stores) {
    distance = GeoDistance.ARC.calculate(location.latitude, location.longitude, store.location.lat, store.location.lon, DistanceUnit.METERS);
    if (distanceMin == null || distance &lt; distanceMin) {
        distanceMin = distance;
        nearestStore = store.id;
    }
}
return nearestStore;
EOT;
        return [
            'script' =&gt; $script,
            'params' =&gt; [
                'location' =&gt; $this-&gt;getParamLocation($params)-&gt;toArray(),
            ],
        ];
```

This script is very ugly, because it uses the (inefficient) `_source`.
But it's actually very fast, because it's only called to process a script field.
</comment><comment author="clintongormley" created="2016-05-19T12:27:19Z" id="220309316">@pierrre You know that we provide this functionality for you out of the box?

```
PUT t 
{
  "mappings": {
    "t": {
      "properties": {
        "location": {
          "type": "geo_point"
        }
      }
    }
  }
}

PUT t/t/1
{
  "location": [
    {
      "lat": 0,
      "lon": 0
    },
    {
      "lat": 1,
      "lon": 0
    },
    {
      "lat": 2,
      "lon": 0
    },
    {
      "lat": 3,
      "lon": 0
    }
  ]
}

GET _search
{
  "sort": [
    {
      "_geo_distance": {
        "location": {
          "lat": 0,
          "lon": 1
        },
        "order": "asc",
        "unit": "km",
        "mode": "min",
        "distance_type": "sloppy_arc"
      }
    }
  ]
}
```

The distance which you're calculating in your script field is available to you in the `sort` parameter.
</comment><comment author="pierrre" created="2016-05-19T12:32:29Z" id="220310479">@clintongormley yes I know.
But I don't sort by distance.
I use a combination of: "minimum distance", "_score" and "priority"
</comment><comment author="rjernst" created="2016-06-14T22:27:48Z" id="226035324">@pierrre You can use `org.apache.lucene.util.SloppyMath.haversinMeters` (that is what GeoDistance.ARC uses).
</comment><comment author="Korikulum" created="2016-07-11T11:08:33Z" id="231705756">@rjernst Have you actually tried if this works? I tried it &amp; got the following error:
`No signature of method: static org.apache.lucene.util.SloppyMath.haversinMeters() is applicable for argument types: (java.lang.Double, java.lang.Double, java.lang.Double, java.lang.Double)`.

When I listed the available methods of the class (using `getMethods()`) I only got `haversin(double, double, double, double)`. As far as I can tell this is cause [Lucene 5](https://lucene.apache.org/core/5_0_0/core/org/apache/lucene/util/SloppyMath.html#haversin%28double, double, double, double%29) is being used while the method you mentioned is in [Lucene 6](https://lucene.apache.org/core/6_0_0/core/org/apache/lucene/util/SloppyMath.html#haversinMeters-double-double-double-double-).

So, I'm I doing something wrong here? Did `haversinMeters()` work for you? However, `haversin()` seems to work, so that's a plus.

I'm using Elasticsearch v2.3.4.
</comment><comment author="nknize" created="2016-07-11T14:14:46Z" id="231746918">&gt; I'm using Elasticsearch v2.3.4.

Then, yes, you need to use `SloppyMath.haversin()`
</comment><comment author="clintongormley" created="2016-07-21T12:22:36Z" id="234237572">@nknize did you see @rjernst 's comment about the problem with using enums in the GeoDistance class?  https://github.com/elastic/elasticsearch/issues/17506#issuecomment-215590155

What about adding distance methods to GeoPoints to make it easier to calculate distances on the GeoPoints returned by `doc['loc'].values`, or perhaps `doc['loc'].values` should return a wrapped GeoPoint with the extra functionality?  (no idea of the consequences here, just suggesting things)
</comment><comment author="nknize" created="2016-07-22T20:15:41Z" id="234645019">@clintongormley I did see the comment. Since we've deprecated `SloppyArc` and fixed up haversine in Lucene, I'd actually like to remove the `GeoDistance` class altogether. I like the idea of wrapping the GeoPoint with the extra functionality but I'll need to have a look at the side effects.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix locahost typo and distro clarifications</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17505</link><project id="" key="" /><description>As per the commit log:
1. Apart from `locahost` typo, the issue is that localhost is not 100% safe
   for all distros with IPv6.
   For example fedora23 defines localhost4 and localhost6 (among other
   aliases) so `curl localhost:9200` doesn't work.
2. clarify that dnf is mostly for Fedora &gt;=22
   RHEL7 and CentOS 7 are still on yum
</description><key id="145681478">17505</key><summary>Fix locahost typo and distro clarifications</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dliappis</reporter><labels><label>docs</label></labels><created>2016-04-04T13:17:42Z</created><updated>2016-04-05T16:24:33Z</updated><resolved>2016-04-05T16:24:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-04T13:38:25Z" id="205298732">99% of our documentation uses localhost so I'm a bit worried about 127.0.0.1 being safer.
</comment><comment author="nik9000" created="2016-04-04T13:39:27Z" id="205298992">The package manager part of the change LGTM.
</comment><comment author="dliappis" created="2016-04-04T13:53:02Z" id="205303943">I understand the concern @nik9000 and this maybe a bit of a last minute thing after all.
If we revert back to `localhost` we should just keep in mind that the curl command won't work on latest fedora releases.
</comment><comment author="dliappis" created="2016-04-04T13:56:46Z" id="205305254">Or what about adding a note there saying "curl 127.0.0.1:9200 on Fedora releases?" @nik9000 
</comment><comment author="jasontedor" created="2016-04-04T14:02:38Z" id="205307443">I'm confused, which Fedora releases does it not work on? My workstation is running Fedora 23 and I do not have any issues pinging localhost or curling localhost:9200 against a running Elasticsearch instance:

``` bash
$ uname -r
4.2.3-300.fc23.x86_64
$ ip a l lo
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
$ lsmod | grep ipv6
nf_reject_ipv6         16384  1 ip6t_REJECT
nf_conntrack_ipv6      20480  12
nf_defrag_ipv6         36864  1 nf_conntrack_ipv6
nf_nat_ipv6            16384  1 ip6table_nat
nf_nat                 28672  3 nf_nat_ipv4,nf_nat_ipv6,nf_nat_masquerade_ipv4
nf_conntrack          106496  9 nf_conntrack_netbios_ns,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_broadcast,nf_conntrack_ipv4,nf_conntrack_ipv6
$ curl -XGET localhost:9200
{
  "name" : "Iceman",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0-alpha1",
    "build_hash" : "baa2d51",
    "build_date" : "2016-03-31T20:40:01.961Z",
    "build_snapshot" : true,
    "lucene_version" : "6.0.0"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="dliappis" created="2016-04-04T15:16:54Z" id="205344030">@jasontedor do you have ipv6?
</comment><comment author="jasontedor" created="2016-04-04T15:20:48Z" id="205345888">&gt; do you have ipv6?

@dliappis Yes, I showed the output from `ip` and `lsmod`. I can also hit elasticsearch against localhost6:9200:

``` bash
$ curl -XGET localhost6:9200
{
  "name" : "Lizard",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "5.0.0-alpha1",
    "build_hash" : "baa2d51",
    "build_date" : "2016-03-31T20:40:01.961Z",
    "build_snapshot" : true,
    "lucene_version" : "6.0.0"
  },
  "tagline" : "You Know, for Search"
}
```

My instance binds to both:

```
[2016-04-04 11:19:34,526][INFO ][http                     ] [Lizard] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
```
</comment><comment author="dakrone" created="2016-04-04T15:21:00Z" id="205346043">Same here with multiple machines running Fedora, `curl localhost:9200` and `curl localhost6:9200` both work
</comment><comment author="dliappis" created="2016-04-04T16:06:56Z" id="205369551">I actually traced this down to a vagrant bug.
It appears that if you specify a hostname, vagrant overwrites `localhost`

From a systems perspective, `127.0.0.1` is what is guaranteed to always work as it is the address assigned to the loopback device as opposed to `localhost` being just an entry in /etc/hosts.

I already reverted the doc changes and pushed the doc/typo fixo showing up `localhost`.

Thanks @jasontedor and @dakrone 
</comment><comment author="dliappis" created="2016-04-05T14:53:16Z" id="205843232">is this mergable now @nik9000 / @jasontedor  ?
</comment><comment author="dakrone" created="2016-04-05T15:01:09Z" id="205847020">LGTM
</comment><comment author="jasontedor" created="2016-04-05T15:41:40Z" id="205863265">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ParseField#getAllNamesIncludedDeprecated to not return duplicate names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17504</link><project id="" key="" /><description /><key id="145677739">17504</key><summary>ParseField#getAllNamesIncludedDeprecated to not return duplicate names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T13:01:34Z</created><updated>2016-04-04T13:08:43Z</updated><resolved>2016-04-04T13:08:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-04T13:03:21Z" id="205287659">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index is a required url part for update by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17503</link><project id="" key="" /><description /><key id="145667217">17503</key><summary>index is a required url part for update by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.2</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T12:15:51Z</created><updated>2016-04-06T09:54:53Z</updated><resolved>2016-04-04T14:49:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-04T14:32:15Z" id="205323462">LGTM
</comment><comment author="nik9000" created="2016-04-04T14:32:29Z" id="205323572">Can you backport to 2.x?
</comment><comment author="Mpdreamz" created="2016-04-04T14:53:14Z" id="205333619">backported to both `2.x` and `2.3` 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle: wraps command line arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17502</link><project id="" key="" /><description>So that they are resolved at execution time, not configuration time
</description><key id="145640255">17502</key><summary>Gradle: wraps command line arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-04-04T10:09:58Z</created><updated>2016-04-05T13:59:34Z</updated><resolved>2016-04-04T16:30:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-04-04T10:11:17Z" id="205228569">Related to https://github.com/elastic/elasticsearch/pull/17475/files/662c34b8884ebd601c7693fb48b53dc3afb27769#r58229331
</comment><comment author="tlrx" created="2016-04-04T16:03:55Z" id="205368530">@rjernst Thanks for your review. I updated according to your comment
</comment><comment author="rjernst" created="2016-04-04T16:21:43Z" id="205374701">LGTM
</comment><comment author="tlrx" created="2016-04-04T16:30:52Z" id="205380096">Thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nodes leaving cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17501</link><project id="" key="" /><description>Hello,

I'm using Elasticsearch 2.3 with JVM 1.7 on CentOS 6.6.

I recently set up an Elasticsearch cluster  with 14 nodes but I met a little problem I can't understand.

Sometimes I have nodes leaving the cluster for no apparent reason and they won't come back unless I restart the instance.

It's kind of annoying because it messes up all the sharding and things like that.

I don't really understand because of every single instance, I filled 

discovery.zen.ping.unicast.hosts:

with all the instance's hosts.
</description><key id="145632522">17501</key><summary>Nodes leaving cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">NicoYUE</reporter><labels><label>feedback_needed</label></labels><created>2016-04-04T09:39:39Z</created><updated>2016-04-06T09:56:59Z</updated><resolved>2016-04-06T09:56:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-04-04T09:48:36Z" id="205217738">It think it's important to understand  why they nodes left the cluster. Do see anything in the logs like long GC pauzes? do you monitor the memory usage of the nodes? Also, when the need leaves, do you see anything of note in the node's logs?
</comment><comment author="NicoYUE" created="2016-04-04T10:30:39Z" id="205233529">I just checked my logs and I don't see anything specific for leaving the cluster. And I have to do something like a query or checking KOPF to realize they are missing.

I remember when I just started setting it up, the nodes would leave if they couldn't find an active host in:

discovery.zen.ping.unicast.hosts

About memory, I've set an ES_HEAP_SIZE of 32g but I don't know if it's relevant.

If one of node leave the cluster again, I'll try to get more info
</comment><comment author="NicoYUE" created="2016-04-04T13:01:42Z" id="205287162">Just happened again, this is what my logs says

```
[2016-04-04 14:43:59,950][WARN ][cluster.service          ] [hdp1.1.prod2.es.xxx] failed to disconnect to node [{hdp6.2.prod2.es.xxx}{-fIZEZwvTmmHFKGMgxXCdA}{192.168.10.6}{192.168.10.6:9301}]
java.lang.OutOfMemoryError: unable to create new native thread
[2016-04-04 14:44:05,318][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:05,322][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:05,324][DEBUG][action.admin.indices.get ] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:05,325][DEBUG][action.admin.cluster.health] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:09,901][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$ReconnectToNodes@58f4c6e4
java.lang.OutOfMemoryError: unable to create new native thread
[2016-04-04 14:44:16,089][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout@778292ad
java.lang.OutOfMemoryError: unable to create new native thread
[2016-04-04 14:44:29,442][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:29,442][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:35,320][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])
[2016-04-04 14:44:35,322][WARN ][rest.suppressed          ] /_cluster/state/master_node,routing_table,blocks/ Params: {metric=master_node,routing_table,blocks}
MasterNotDiscoveredException[null]
[2016-04-04 14:44:35,323][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])
[2016-04-04 14:44:35,324][WARN ][rest.suppressed          ] /_cluster/settings Params: {}
MasterNotDiscoveredException[null]
[2016-04-04 14:44:35,326][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout@3e9e7840
java.lang.OutOfMemoryError: unable to create new native thread
[2016-04-04 14:44:35,325][DEBUG][action.admin.indices.get ] [hdp1.1.prod2.es.xxx] timed out while retrying [indices:admin/get] after failure (timeout [30s])
[2016-04-04 14:44:35,327][WARN ][rest.suppressed          ] /_aliases Params: {index=_aliases}MasterNotDiscoveredException[null]

[2016-04-04 14:44:35,328][DEBUG][action.admin.indices.get ] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:35,332][DEBUG][action.admin.cluster.health] [hdp1.1.prod2.es.xxx] no known master node, scheduling a retry
[2016-04-04 14:44:59,443][DEBUG][action.admin.cluster.state] [hdp1.1.prod2.es.xxx] timed out while retrying [cluster:monitor/state] after failure (timeout [30s])
[2016-04-04 14:44:59,443][WARN ][threadpool               ] [hdp1.1.prod2.es.xxx] failed to run [threaded] org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout@189b1bb5
java.lang.OutOfMemoryError: unable to create new native thread

```

It tells me memory problem but I've set 32g which should be enough and from KOPF, i'm not using a lof of memory either.

Each of my instances have a discovery zen so I don't know why they can't find the master anymore which didnt leave my cluster.

Also, I noticed that the only nodes that are leaving everytime are those I had set on triple instances on one machine, with 32g for each instances, but I should have at least 100g unused RAM on these machines.
</comment><comment author="bleskes" created="2016-04-04T14:37:05Z" id="205325170">This line indicates you are indeed having memory problems:

```
java.lang.OutOfMemoryError: unable to create new native thread
```

Once a node goes out of memory it becomes unreliable and indeed needs to be restarted.  You mention KOPF indicates you have enough memory. Can you elaborate? 
</comment><comment author="jasontedor" created="2016-04-04T14:44:51Z" id="205328280">&gt; `java.lang.OutOfMemoryError: unable to create new native thread`

When you see the message "unable to create new native thread" this generally means that you have an issue limiting the number of processes that the elasticsearch user can create. The exact resolution varies from system to system, but in general on Linux you would look at `/etc/security/limits.conf` and `ulimit -u`. 
</comment><comment author="jasontedor" created="2016-04-04T14:49:17Z" id="205331019">&gt; About memory, I've set an ES_HEAP_SIZE of 32g but I don't know if it's relevant.

It's not relevant to the issue that you're seeing in the logs ("unable to create new native threads"). However, you'll actually see better results if you drop the heap slightly below 32g. On the version of Elasticsearch that you're running, when a node starts up you'll see a message that says

```
[2016-04-04 10:47:27,618][INFO ][env                      ] [Masked Marauder] heap size [31.9gb], compressed ordinary object pointers [false]
```

but if you drop the heap below 32g you'll be able to take advantage of compressed oops

```
[2016-04-04 10:48:20,133][INFO ][env                      ] [Brute II] heap size [31.1gb], compressed ordinary object pointers [true]
```

This actually gives you more useable heap, and the smaller pointers are friendlier to memory bandwidth and CPU caches.
</comment><comment author="NicoYUE" created="2016-04-04T15:29:39Z" id="205350728">On KOPF, I can check the HEAP usage where my instances usually are using around 1Gb for 31.81Gb Max, it's far from that.

That's why I find this really weird. i do have some nodes with a low max RAM but they're not those who leave my cluster and are actually quite stable

Haven't check limit.conf yet, will do it ASAP
</comment><comment author="clintongormley" created="2016-04-06T09:56:59Z" id="206272578">@jasontedor has already provided the answer, and given that this topic is more suited to the forums than github, I'm going to close
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Double wildcard in string query causes incorrect highlighting for plain and fast vectors highlighters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17500</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 1.5.2, 2.2.1

**JVM version**: java version "1.8.0_60" (Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode))

**OS version**: Windows 7 Enterprize Service Pack 1

**Description of the problem including expected versus actual behavior**:
Expected same highlight results for query `*` and queries `**`, `*?` but in fact they are totally different.

**Steps to reproduce**:
Create following mapping:

```
PUT http://localhost:9200/index/_mapping/sometype 
{
    "properties" : {
        "sometext" : {
            "type" : "string",
            "term_vector" : "with_positions_offsets"
        }
    }
}
```

Post data

```
POST http://localhost:9200/index/sometype
{
    "sometext" : "A supervisor is responsible for the productivity and actions of a small group of     employees. The supervisor has several manager-like roles, responsibilities, and powers. Two of the key differences between a supervisor and a manager are (1) the supervisor does not typically have hire and fire authority, and (2) the supervisor does not have budget authority."
}
```

Query data with highlithing

```
POST http://localhost:9200/index/sometype/_search
{
    "query" : {
        "query_string" : {
            "query" : "**",
            "fields" : ["sometext"]
        }
    },
    "highlight" : {
        "pre_tags" : ["&lt;em&gt;"],
        "post_tags" : ["&lt;/em&gt;"],
        "order" : "score",
        "require_field_match" : true,
        "fields" : {
            sometext : {
                "fragment_size" : 150,
                "number_of_fragments" : 1
            }
        }
    }
}
```

and you will got following (incorrect) highlighting:

```
"highlight" : {
    "sometext" : ["responsibilities, &lt;em&gt;and&lt;/em&gt; &lt;em&gt;powers&lt;/em&gt;. &lt;em&gt;Two&lt;/em&gt; &lt;em&gt;of&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;key&lt;/em&gt; &lt;em&gt;differences&lt;/em&gt; &lt;em&gt;between&lt;/em&gt; &lt;em&gt;a&lt;/em&gt; &lt;em&gt;supervisor&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;a&lt;/em&gt; &lt;em&gt;manager&lt;/em&gt; &lt;em&gt;are&lt;/em&gt; (&lt;em&gt;1&lt;/em&gt;) &lt;em&gt;the&lt;/em&gt; &lt;em&gt;supervisor&lt;/em&gt; &lt;em&gt;does&lt;/em&gt; &lt;em&gt;not&lt;/em&gt; &lt;em&gt;typically&lt;/em&gt; &lt;em&gt;have&lt;/em&gt; &lt;em&gt;hire&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;fire&lt;/em&gt; &lt;em&gt;authority&lt;/em&gt;, and"]
}
```

The same highlighting results are produced by query `*?`.
On plain highlighter (I just added `"type" : "plain"` to highlight) result looks a bit different (but still incorrect):

```
"highlight" : {
    "sometext" : [", &lt;em&gt;responsibilities&lt;/em&gt;, &lt;em&gt;and&lt;/em&gt; &lt;em&gt;powers&lt;/em&gt;. &lt;em&gt;Two&lt;/em&gt; &lt;em&gt;of&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;key&lt;/em&gt; &lt;em&gt;differences&lt;/em&gt; &lt;em&gt;between&lt;/em&gt; &lt;em&gt;a&lt;/em&gt; &lt;em&gt;supervisor&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;a&lt;/em&gt; &lt;em&gt;manager&lt;/em&gt; &lt;em&gt;are&lt;/em&gt; (&lt;em&gt;1&lt;/em&gt;) &lt;em&gt;the&lt;/em&gt; &lt;em&gt;supervisor&lt;/em&gt; &lt;em&gt;does&lt;/em&gt; &lt;em&gt;not&lt;/em&gt; &lt;em&gt;typically&lt;/em&gt; &lt;em&gt;have&lt;/em&gt; &lt;em&gt;hire&lt;/em&gt; &lt;em&gt;and&lt;/em&gt; &lt;em&gt;fire&lt;/em&gt; &lt;em&gt;authority&lt;/em&gt;, &lt;em&gt;and&lt;/em&gt; (&lt;em&gt;2&lt;/em&gt;) &lt;em&gt;the&lt;/em&gt; &lt;em&gt;supervisor&lt;/em&gt; &lt;em&gt;does&lt;/em&gt; &lt;em&gt;not&lt;/em&gt; &lt;em&gt;have&lt;/em&gt; &lt;em&gt;budget&lt;/em&gt; &lt;em&gt;authority&lt;/em&gt;."]
}
```

But when query is `*` - nothing returned by highlighter.
</description><key id="145610881">17500</key><summary>Double wildcard in string query causes incorrect highlighting for plain and fast vectors highlighters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">luckywin</reporter><labels><label>:Highlighting</label><label>discuss</label></labels><created>2016-04-04T07:51:24Z</created><updated>2016-11-25T15:53:58Z</updated><resolved>2016-11-25T15:53:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-06T09:41:56Z" id="206262695">`*` is mapped to a match-all query, so nothing is highlighted.  `**` and `*?` (and `?*` and `***` and `*?*?`) end up doing a wildcard query on all terms, so all terms are highlighted.  Not sure what we can really do about this.
</comment><comment author="nik9000" created="2016-04-06T13:43:05Z" id="206378992">It might be worth looking at \* just doing a wildcard match all for text and making sure that is rewritten sensibly. You can certainly use `highlight_query` in the mean time if you find searching for `**` to be slower than `*`. At a minimum we need to update the docs page for highlighting.

The "incorrect" highlighting you describe is actually the expected behavior of the fast vector highlighter. It identifies hits, picks the right snippet based on the hits, and then expands the snippet forwards and backwards looking for word breaks. It doesn't go back to the list of hits when it does the seeking. IIRC the experimental-highlighter did do that, but no one has updated it to work with 2.0 so, yeah.
</comment><comment author="clintongormley" created="2016-11-25T15:53:58Z" id="262984997">Closed in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extended Stats Bucket Aggregation returns parse exception when sigma is an Integer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17499</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 

2.3.0

**JVM version**:

java version "1.8.0_77"
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)

**OS version**: 

Windows 10

**Description of the problem including expected versus actual behavior**:

[The documentation for _Extended Stats Aggregation_ specifies that `sigma` can be any non-negative double](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-metrics-extendedstats-aggregation.html#CO49-1). In the example in the docs, `3` is passed as the value for sigma.

[_Extended Stats Bucket Aggregation_ also has a `sigma` parameter](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-aggregations-pipeline-extended-stats-bucket-aggregation.html) but the documentation does not specify what type `sigma` is. I assumed it is also a `double` and [confirmed this with a (failing) integration test for NEST](https://github.com/elastic/elasticsearch-net/blob/2.x/src/Tests/Aggregations/Pipeline/ExtendedStatsBucket/ExtendedStatsBucketAggregationUsageTests.cs); **a parse exception** is returned when passing an `integer` (i.e. no decimal places) as the value for `sigma`. **I would expect an integer to be valid value as an input for a `double`**, based on the example provided for _Extended Stats Aggregation_.

**Steps to reproduce**:
1. Failing request (from NEST integration test). Notice the value of `2` for `sigma`

``` javascript
{
  "size": 0,
  "aggs": {
    "projects_started_per_month": {
      "date_histogram": {
        "field": "startedOn",
        "interval": "month"
      },
      "aggs": {
        "commits": {
          "sum": {
            "field": "numberOfCommits"
          }
        }
      }
    },
    "extended_stats_commits_per_month": {
      "extended_stats_bucket": {
        "buckets_path": "projects_started_per_month&gt;commits",
        "sigma": 2
      }
    }
  }
}
```

returns:

``` javascript
{
  "error" : {
    "root_cause" : [ {
      "type" : "parse_exception",
      "reason" : "Parameter [sigma] must be a Double, type `Integer` provided instead"
    } ],
    "type" : "search_phase_execution_exception",
    "reason" : "all shards failed",
    "phase" : "query",
    "grouped" : true,
    "failed_shards" : [ {
      "shard" : 0,
      "index" : "project",
      "node" : "afZfSF7hQoqmZQip5n3-LQ",
      "reason" : {
        "type" : "parse_exception",
        "reason" : "Parameter [sigma] must be a Double, type `Integer` provided instead"
      }
    } ]
  },
  "status" : 500
}
```
1. Change the request from step 1, to change value of `sigma` from `2` to `2.0`:

``` javascript
{
  "size": 0,
  "aggs": {
    "projects_started_per_month": {
      "date_histogram": {
        "field": "startedOn",
        "interval": "month"
      },
      "aggs": {
        "commits": {
          "sum": {
            "field": "numberOfCommits"
          }
        }
      }
    },
    "extended_stats_commits_per_month": {
      "extended_stats_bucket": {
        "buckets_path": "projects_started_per_month&gt;commits",
        "sigma": 2.0
      }
    }
  }
}
```

successfully returns results:

``` javascript
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "hits" : {
    "total" : 100,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "projects_started_per_month" : {
      "buckets" : [ {
        "key_as_string" : "2015-04-01T00:00:00.000Z",
        "key" : 1427846400000,
        "doc_count" : 2,
        "commits" : {
          "value" : 796.0
        }
      }, {
        "key_as_string" : "2015-05-01T00:00:00.000Z",
        "key" : 1430438400000,
        "doc_count" : 12,
        "commits" : {
          "value" : 4982.0
        }
      }, {
        "key_as_string" : "2015-06-01T00:00:00.000Z",
        "key" : 1433116800000,
        "doc_count" : 13,
        "commits" : {
          "value" : 5847.0
        }
      }, {
        "key_as_string" : "2015-07-01T00:00:00.000Z",
        "key" : 1435708800000,
        "doc_count" : 7,
        "commits" : {
          "value" : 3693.0
        }
      }, {
        "key_as_string" : "2015-08-01T00:00:00.000Z",
        "key" : 1438387200000,
        "doc_count" : 7,
        "commits" : {
          "value" : 3848.0
        }
      }, {
        "key_as_string" : "2015-09-01T00:00:00.000Z",
        "key" : 1441065600000,
        "doc_count" : 8,
        "commits" : {
          "value" : 4421.0
        }
      }, {
        "key_as_string" : "2015-10-01T00:00:00.000Z",
        "key" : 1443657600000,
        "doc_count" : 9,
        "commits" : {
          "value" : 5119.0
        }
      }, {
        "key_as_string" : "2015-11-01T00:00:00.000Z",
        "key" : 1446336000000,
        "doc_count" : 1,
        "commits" : {
          "value" : 339.0
        }
      }, {
        "key_as_string" : "2015-12-01T00:00:00.000Z",
        "key" : 1448928000000,
        "doc_count" : 12,
        "commits" : {
          "value" : 4218.0
        }
      }, {
        "key_as_string" : "2016-01-01T00:00:00.000Z",
        "key" : 1451606400000,
        "doc_count" : 10,
        "commits" : {
          "value" : 4804.0
        }
      }, {
        "key_as_string" : "2016-02-01T00:00:00.000Z",
        "key" : 1454284800000,
        "doc_count" : 7,
        "commits" : {
          "value" : 4316.0
        }
      }, {
        "key_as_string" : "2016-03-01T00:00:00.000Z",
        "key" : 1456790400000,
        "doc_count" : 10,
        "commits" : {
          "value" : 5009.0
        }
      }, {
        "key_as_string" : "2016-04-01T00:00:00.000Z",
        "key" : 1459468800000,
        "doc_count" : 2,
        "commits" : {
          "value" : 1664.0
        }
      } ]
    },
    "extended_stats_commits_per_month" : {
      "count" : 13,
      "min" : 339.0,
      "max" : 5847.0,
      "avg" : 3773.5384615384614,
      "sum" : 49056.0,
      "sum_of_squares" : 2.21307799E8,
      "variance" : 2784084.325443786,
      "std_deviation" : 1668.55755832509,
      "std_deviation_bounds" : {
        "upper" : 7110.653578188641,
        "lower" : 436.42334488828146
      }
    }
  }
}
```
</description><key id="145594502">17499</key><summary>Extended Stats Bucket Aggregation returns parse exception when sigma is an Integer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">russcam</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-04-04T06:20:19Z</created><updated>2016-06-07T09:14:50Z</updated><resolved>2016-06-07T09:14:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-06T14:34:41Z" id="206404025">@russcam Are you sure this is an Elasticsearch issue rather than a NEST issue? I have just tested this on both master and 2.3 (see command line output below for what I did for 2.3) and have not been able to reproduce the issue. Could you maybe provide the stack trace from the Elasticsearch logs?

```
$ curl -XDELETE localhost:9200/test
{"acknowledged":true}% 

$ curl -XPOST 'localhost:9200/test/doc/1' -d '{ "i": 2 } '
{"_index":"test","_type":"doc","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}% 

$ curl -XGET 'localhost:9200/test/_search?pretty' -d '{
quote&gt;   "aggs": {
quote&gt;     "orders": {
quote&gt;       "extended_stats": {
quote&gt;         "field": "i",
quote&gt;         "sigma": 2
quote&gt;       }
quote&gt;     }
quote&gt;   }
quote&gt; }'
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0,
      "_source" : {
        "i" : 2
      }
    } ]
  },
  "aggregations" : {
    "orders" : {
      "count" : 1,
      "min" : 2.0,
      "max" : 2.0,
      "avg" : 2.0,
      "sum" : 2.0,
      "sum_of_squares" : 4.0,
      "variance" : 0.0,
      "std_deviation" : 0.0,
      "std_deviation_bounds" : {
        "upper" : 2.0,
        "lower" : 2.0
      }
    }
  }
}
```
</comment><comment author="colings86" created="2016-04-06T14:38:10Z" id="206406107">Also, from looking at the code in `ExtendedStatsParser` it should accept any number for sigma (see https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/search/aggregations/metrics/stats/extended/ExtendedStatsParser.java#L64-67)
</comment><comment author="russcam" created="2016-04-06T23:26:11Z" id="206617893">@colings86 I think you've made the same mistake I initially did in looking at `extended_stats` and _not_ `extended_stats_bucket` :smile:  The former works fine with any number but the latter has the issue. 

Looking at [ExtendedStatsBucketParser in 2.3](https://github.com/elastic/elasticsearch/blob/2.3/core/src/main/java/org/elasticsearch/search/aggregations/pipeline/bucketmetrics/stats/extended/ExtendedStatsBucketParser.java#L47-L53), it looks like the parsing logic for this should be brought in line with the logic used in `ExtendedStatsParser`.

I see @alexshadow007 has [opened a PR with a fix](https://github.com/elastic/elasticsearch/pull/17562/commits/eafa77c0bb5976466fd6a27d37354ecbb54c0fc0#diff-20dcf2ad6adedd04f75f9026ce05d5feL38); happy to submit a fix if needed.

NEST had an issue with modelling the Extended Stats Bucket Aggregation's `Sigma` property as a nullable int; that is now fixed :+1: 
</comment><comment author="colings86" created="2016-04-07T08:32:30Z" id="206759410">@russcam yep you are right, I missed that it was `extended_stats_bucket` we were talking about here.  The PR from @alexshadow007 looks pretty good so I'll work with him to address the few comments I have on it and get it merged
</comment><comment author="colings86" created="2016-06-07T09:14:49Z" id="224224524">Fixed by https://github.com/elastic/elasticsearch/pull/17562
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Cluster Allocation Filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17498</link><project id="" key="" /><description>Put more emphasis on the fact that multiple values can be specified
and move examples after explanation of settings.

Suggestion for #16997
</description><key id="145583094">17498</key><summary>Docs: Cluster Allocation Filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>docs</label><label>review</label></labels><created>2016-04-04T04:47:11Z</created><updated>2016-10-17T00:12:22Z</updated><resolved>2016-10-17T00:12:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-07T01:01:29Z" id="206639974">LGTM
</comment><comment author="clintongormley" created="2016-04-07T10:02:04Z" id="206794128">LGTM
</comment><comment author="dakrone" created="2016-07-11T17:02:28Z" id="231797442">@joshuar do you want to merge this?
</comment><comment author="dakrone" created="2016-09-12T21:29:48Z" id="246500291">@joshuar ping again about merging this (it should be rebased probably beforehand)
</comment><comment author="joshuar" created="2016-10-11T22:27:21Z" id="253065721">@dakrone yep, I'll rebase and update.
</comment><comment author="joshuar" created="2016-10-17T00:12:21Z" id="254087182">Superceded by #20958.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score not accurate when using with long values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17497</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**1.8.0_60**:

**Mac OsX**:

*_Expecting function score with boost mode 'replace', will replace the document score with the given field value factor. But the result is approximate with the given field value. Accuracy getting reduced with using with long values. *_:

**Steps to reproduce**:
1. Put some documents with current time in secs or millis as field value
2. Use function score with boost mode replace and set field value factor as given field.

**Query**

```
`curl -XGET 'localhost:9200/805/8102/_search?pretty&amp;_source=visited_time' -d'
{
  "query": {
    "function_score": {
      "boost_mode": "replace", // we need to replace document score with the result of the functions,
      "query": {
      },
      "functions": [
        {
          "field_value_factor": { // return `lastvisited` value as score
            "field": "visited_time"
          }
        }
      ]
    }
  },
   "size": 10
}'`

```

```
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.45974246E9,
    "hits" : [ {
      "_index" : "idx5",
      "_type" : "8102",
      "_id" : "null-8c24d76eccf94d769368c42cf5288f0e-1459742489206",
      "_score" : 1.45974246E9,
      "_routing" : "8c24d76eccf94d769368c42cf5288f0e",
      "_source" : {
        "visited_time" : 1459742464
      }
    }, {
      "_index" : "idx5",
      "_type" : "8102",
      "_id" : "null-8c24d76eccf94d769368c42cf5288f0e-1459742501669",
      "_score" : 1.45974246E9,
      "_routing" : "8c24d76eccf94d769368c42cf5288f0e",
      "_source" : {
        "visited_time" : 1459742464
      }
    }, {
      "_index" : "idx5",
      "_type" : "8102",
      "_id" : "null-669df757deeb1c347b3f467d35cff5d7-1459742224763",
      "_score" : 1.45974221E9,
      "_routing" : "669df757deeb1c347b3f467d35cff5d7",
      "_source" : {
        "visited_time" : 1459742208
      }
    }, {
      "_index" : "idx5",
      "_type" : "8102",
      "_id" : "null-669df757deeb1c347b3f467d35cff5d7-1459742073082",
      "_score" : 1.45974208E9,
      "_routing" : "669df757deeb1c347b3f467d35cff5d7",
      "_source" : {
        "visited_time" : 1459742080
      }
    } ]
  }
}
```
</description><key id="145579961">17497</key><summary>Function score not accurate when using with long values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bharathikannan-zarget</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-04-04T04:20:46Z</created><updated>2016-04-04T17:16:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-04T17:16:09Z" id="205400267">This is due to the fact that the score is stored into a float, which can only represent integers accurately up to 2&lt;sup&gt;24&lt;/sup&gt;. This is not something that we want to change, however I agree it can be surprising and we should document it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Quote path to java binary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17496</link><project id="" key="" /><description>This commit quotes the variable that contains the path to the java
binary. Without these quotes, when the arguments to eval are evaluated
the existing quotes will be removed leading to unquoted use of the path
to the java binary. If this path contains spaces, evaluation will fail.

Closes #17495
</description><key id="145574230">17496</key><summary>Quote path to java binary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-04-04T03:34:52Z</created><updated>2016-04-08T11:25:45Z</updated><resolved>2016-04-06T17:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-04T03:54:59Z" id="205123522">Code looks good but it would be great to have a vagrant test for this
</comment><comment author="jasontedor" created="2016-04-06T17:30:56Z" id="206478326">&gt; Code looks good but it would be great to have a vagrant test for this

@dakrone I pushed a test in eb7ecb729624e7b7226737f539af29063d3fee96.
</comment><comment author="dakrone" created="2016-04-06T17:41:04Z" id="206483007">LGTM, thanks for adding the test!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch-plugin doesn't quote the java path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17495</link><project id="" key="" /><description>**Description of the problem including expected versus actual behavior**: bin/elasticsearch-plugin doesn't quote the Java path in the same way that bin/elasticsearch does, which can cause problems when installing plugins and your JAVA_HOME has a space in it. 

Elasticsearch-plugin: https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch-plugin#L70

vs

https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch#L98

I admit this isn't likely to cause problems in the real world, because it should only be problematic when there are spaces in the JAVA_HOME path (bad idea!), but if it happens, you end up with `bin/elasticsearch` working, while `bin/elasticsearch-plugin` does not. 
</description><key id="145564961">17495</key><summary>elasticsearch-plugin doesn't quote the java path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skearns64</reporter><labels><label>:Plugins</label><label>bug</label></labels><created>2016-04-04T01:59:14Z</created><updated>2016-04-08T11:25:07Z</updated><resolved>2016-04-08T11:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-04T03:35:18Z" id="205119077">The quotes in the `elasticsearch` script are not part of the value of `JAVA` after the assignment. Thus, the [line](https://github.com/elastic/elasticsearch/blob/7d4ed5b19ebc25d608d70babab0fb08f0c9ea122/distribution/src/main/resources/bin/elasticsearch-plugin#L70) you point to in the `elasticsearch-plugin` script is not the problem.

The problem here is the [use of `eval` when invoking `java`](https://github.com/elastic/elasticsearch/blob/7d4ed5b19ebc25d608d70babab0fb08f0c9ea122/distribution/src/main/resources/bin/elasticsearch-plugin#L113) using `JAVA`. While `$JAVA` is quoted there, evaluation of the parameters causes those quotes to be removed (this is normal shell behavior)! Note that the `elasticsearch` script [uses `exec` instead](https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/elasticsearch#129), hence the difference.

A simple solution is to use single quotes or escaped quotes around `"$JAVA"` (as other arguments here already do). Note that changing to use `exec` is a much bigger change because of the quoting of arguments that is done earlier in the script.

I opened #17496.
</comment><comment author="jasontedor" created="2016-04-08T11:25:07Z" id="207387943">Closed by #17496.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 2.2.0 memory dump and node goes out of cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17494</link><project id="" key="" /><description>ElasticSearch version: 2.2.0

JVM version: 1.8

OS version: AWS Linux C3.4xlarge

Description of the problem including expected versus actual behavior: 

After we restart ElasticSearch run fine like for 2-3 days and then suddenly one of node in cluster goes out of cluster and on node it show ElasticSeach running  but leaves a memory dump on disk. 
It doesn't come back to cluster unless we restart this.

Memory dump is about 13 GB so let me know where i can keep so keep for you to watch.
</description><key id="145504241">17494</key><summary>ElasticSearch 2.2.0 memory dump and node goes out of cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lokeshhctm</reporter><labels /><created>2016-04-03T14:11:43Z</created><updated>2016-04-04T19:48:37Z</updated><resolved>2016-04-04T19:48:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lokeshhctm" created="2016-04-03T14:20:35Z" id="204984688">Please find the memory dump location:
http://admarvel.s3.amazonaws.com/uploads/lokesh1/java_pid26509.hprof.zip
</comment><comment author="clintongormley" created="2016-04-04T19:48:37Z" id="205465882">Hi @lokeshhctm 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only, and this sounds very much like misconfiguration or misuse, rather than a bug.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force change cluster's elected master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17493</link><project id="" key="" /><description>Hi,

Today I've been trying to increase the storage of my elasticsearch nodes. The nodes are hosted on AWS EC2 each with an attached EBS of 10 GB. I was trying to increase the EBS size to 20GB for each node and it all went fine until I went and restarted the cluster master.

It took about 30 seconds for the cluster to elect a new master and during that time all requests failed and all the other node gave me the 503 error when I tried to check their status.

I am wondering if there is a way to change the cluster master to a specific node instantly without having to wait for the nodes to elect a new master.

For example, lets say my cluster has three nodes:

Node1 (Cluster Master)
Node2
Node3

What I would like to do is change the size of the drive for lets say Node2, then once that node joins the cluster and all shards get reallocated, I force elect it as the cluster master, then I can safely change the configuration for the other two nodes.

Is this possible in ES? If not, how can I go about reducing the time it takes for the nodes to elect a new master?
</description><key id="145489504">17493</key><summary>Force change cluster's elected master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">redserpent7</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2016-04-03T12:37:10Z</created><updated>2017-03-22T13:02:26Z</updated><resolved>2016-04-05T06:49:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T19:47:52Z" id="205465512">You don't mention what version of Elasticsearch you're using.  @bleskes why would it take 30s to elect a master?
</comment><comment author="redserpent7" created="2016-04-05T06:32:41Z" id="205673389">@clintongormley  I am running 1.7.2, not sure why it took 30 secs the first time. I tried it some time later and it did not take that long to elect.

BTW, my PING timeout is set to 5s and the PING retry is set to the default, (not specified in yml)
</comment><comment author="bleskes" created="2016-04-05T06:49:31Z" id="205678969">It should take 3s for a clean restart. It might take longer if network is slow or nodes are so overloaded that they fail to process the master loss quickly. 

The only way to remove a master from it's position is to restart it. In theory it is possible to implement a clean mastership transfer but it's very tricky and there things we should do first. For now, I will close the issue.

@redserpent7 - if you keep running into a 30s master election  please open up an issue with the revelent details (logs, timing + the output of _cat/master on all the nodes. [this is a handy program for that](http://linux.die.net/man/1/pssh))
</comment><comment author="redserpent7" created="2016-04-05T07:51:15Z" id="205707315">@bleskes I did try restarting the master node several times and it did not take a long time for the other nodes to re-elect a new master, probably it had something to do with AWS EC2 at the time of my initial restart. 

Its a non issue for me really as the cluster in question is my testing environment while the production environment nodes have a big enough drives attached to them that should last long which makes the increase very infrequent. 

I would like though if you can consider implementing the clean mastership transfer in a future version.
</comment><comment author="bleskes" created="2016-04-05T08:07:11Z" id="205712371">Thank you for letting us know. 

&gt;  I would like though if you can consider implementing the clean mastership transfer in a future version.

I agree. We just have bigger fish to fry first.
</comment><comment author="munnerz" created="2017-03-22T12:16:47Z" id="288381093">Hey @bleskes - I'm currently working on some automation for running Elasticsearch in a clustered fashion on top of Kubernetes, and would love to be able to manually trigger a master re-election (or alternatively, disallow the current master from being master, similar to setting `cluster.routing.allocation.exclude`. Right now upon a scale down event involving the master node, the cluster can turn red for up to 30s (thus serving no requests).

Are there any plans to implement this yet? Is it something you'd consider adding still?

(FYI, I am using ES 5.2.2 here)</comment><comment author="bleskes" created="2017-03-22T12:57:50Z" id="288390162">@munnerz can you open a topic on discuss.elastic.co  and link it here? we can continue talking there. I have a feeling this will become a discussion ;)</comment><comment author="munnerz" created="2017-03-22T13:02:26Z" id="288391203">@bleskes done! Thanks for the prompt response :) https://discuss.elastic.co/t/gracefully-trigger-re-election-of-master-node/79587</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make JNA calls optional</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17492</link><project id="" key="" /><description>The introduction of max number of processes and max size virtual memory
checks inadvertently made JNA non-optional on OS X and Linux. This
commit wraps these calls in a check to see if JNA is available so that
JNA remains optional.

Closes #17476
</description><key id="145384451">17492</key><summary>Make JNA calls optional</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-04-02T15:24:44Z</created><updated>2016-04-02T17:29:54Z</updated><resolved>2016-04-02T16:14:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-02T15:51:55Z" id="204742489">LGTM
</comment><comment author="jasontedor" created="2016-04-02T16:16:05Z" id="204749403">Thanks @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document the Elasticsearch GC slow log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17491</link><project id="" key="" /><description>Elasticsearch has a built-in JVM GC monitoring service that logs slow GC activity in the main Elasticsearch log. The log lines produced by this monitoring service pack a lot of useful information but it's not immediately obvious what all of it means. The meaning of these log lines should be documented.
</description><key id="145376342">17491</key><summary>Document the Elasticsearch GC slow log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-04-02T14:06:43Z</created><updated>2016-11-05T16:58:05Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="samy-deghou" created="2016-04-03T20:09:00Z" id="205046261">Are you talking about all the classes from the package org.elasticsearch.monitor.jvm or specifically about the class org.elasticsearch.monitor.jvm.JvmGcMonitorService ?
Should the additional doc be part of the the main Elasticsearch log ?
</comment><comment author="jasontedor" created="2016-04-04T13:05:59Z" id="205288745">&gt; Are you talking about all the classes from the package org.elasticsearch.monitor.jvm or specifically about the class org.elasticsearch.monitor.jvm.JvmGcMonitorService ?

I'm referring to documenting the log lines that are put into the main Elasticsearch log by the JVM GC monitor service.

&gt; Should the additional doc be part of the the main Elasticsearch log ?

Possibly, or perhaps on its own separate page linked to from other logging documentation.
</comment><comment author="gmoskovicz" created="2016-10-24T15:57:13Z" id="255782684">++ on documenting this. The configuration is already shown in https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/monitor/jvm/JvmGcMonitorService.java but we need to document the defaults and the format to be user friendly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add bootstrap check for ES_HEAP_SIZE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17490</link><project id="" key="" /><description>Would it be possible to add a bootstrap check to ensure that ES_HEAP_SIZE has been configured?  This should be a required setting when moving to production.
</description><key id="145374641">17490</key><summary>Add bootstrap check for ES_HEAP_SIZE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>discuss</label><label>enhancement</label></labels><created>2016-04-02T13:43:49Z</created><updated>2016-04-13T19:11:17Z</updated><resolved>2016-04-13T19:11:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-02T13:44:03Z" id="204719517">/cc @jasontedor 
</comment><comment author="jasontedor" created="2016-04-02T13:54:33Z" id="204720075">@clintongormley There are other ways to set the heap (for example, via `ES_JAVA_OPTS`). Do you think the check should be for `ES_HEAP_SIZE` or just that the heap is not set to the current defaults (`Xms256m` and `Xmx1g`)?
</comment><comment author="clintongormley" created="2016-04-02T14:18:28Z" id="204724126">Yeah, tricky.  99% of users will use ES_HEAP_SIZE, but there's probably somebody out there using java opts for some custom reason.  If there is, I'd like to hear about why they're doing it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add some clarification regarding docs.count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17489</link><project id="" key="" /><description>Closes #17319 
</description><key id="145360958">17489</key><summary>Add some clarification regarding docs.count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hgfischer</reporter><labels><label>docs</label></labels><created>2016-04-02T10:36:19Z</created><updated>2016-04-07T13:02:45Z</updated><resolved>2016-04-07T09:15:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-02T17:23:01Z" id="204761697">@hgfischer Thanks, can you sign the CLA?
</comment><comment author="hgfischer" created="2016-04-04T07:23:42Z" id="205171435">@jasontedor I already signed the CLA, two times.
</comment><comment author="clintongormley" created="2016-04-04T20:01:58Z" id="205470627">@hgfischer the problem with the CLA was using a different email address from github.  I've connected the two now.
</comment><comment author="hgfischer" created="2016-04-04T20:27:01Z" id="205479643">@clintongormley Strange. I've signed with the same address from my profile. Nevertheless, this PR still show that I didn't signed.
</comment><comment author="clintongormley" created="2016-04-05T07:39:10Z" id="205703019">@hgfischer you signed with your gmail address, but here you used your .io address.  Anyway, CLA database updated to include both.
</comment><comment author="hgfischer" created="2016-04-05T12:35:32Z" id="205780650">Ah, ok. It was in my `.gitconfig` file. Good to know. Thanks
</comment><comment author="clintongormley" created="2016-04-06T11:02:45Z" id="206311183">@hgfischer will merge this in as soon as you've pushed a commit to address @jasontedor 's comments
</comment><comment author="hgfischer" created="2016-04-06T18:38:25Z" id="206507852">@clintongormley done.
</comment><comment author="clintongormley" created="2016-04-07T09:15:40Z" id="206777641">thanks @hgfischer - merged
</comment><comment author="hgfischer" created="2016-04-07T13:02:45Z" id="206886744">Great! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Smoke tester: Use portsfile to find out host/port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17488</link><project id="" key="" /><description>Instead of hardcoding localhost:9200, the smoke tester
now uses the portsfile's first entry to find out, which
host/port combination to test HTTP against.

Closes #17409
</description><key id="145342650">17488</key><summary>Smoke tester: Use portsfile to find out host/port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label></labels><created>2016-04-02T06:21:55Z</created><updated>2016-04-06T07:16:19Z</updated><resolved>2016-04-06T07:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-04-05T14:13:10Z" id="205828117">Left a minor comment, otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from some enums in query builders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17487</link><project id="" key="" /><description>Relates to #17085
</description><key id="145322906">17487</key><summary>Remove PROTOTYPE from some enums in query builders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-04-02T01:34:05Z</created><updated>2016-05-02T12:15:50Z</updated><resolved>2016-04-05T18:15:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-04T08:51:54Z" id="205199464">LGTM
</comment><comment author="nik9000" created="2016-04-05T18:31:55Z" id="205935163">Closed by 90f300bb719e5758b012ef8f7e17dff13c1be356.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Move Repositories section to be with Installation section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17486</link><project id="" key="" /><description>Installing our whole stack from the docs would be a lot easier if we were consistent in our docs.

I like that there's an "Installation" section in the "Getting Started" section of Elasticsearch reference.

But having another "Setup" section and "Repositories" under that is confusing.

The simplest improvement would just be to add a "Repositories" heading and link on https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html to https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html

A better change would be to agree on a template across all products and get just the installation sections consistent.

I think the Beats docs are laid out pretty nice;
![installationscreens](https://cloud.githubusercontent.com/assets/13542669/14220002/0ae21e1a-f823-11e5-8ff2-4cf49c56706d.png)
</description><key id="145286042">17486</key><summary>Docs: Move Repositories section to be with Installation section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LeeDr</reporter><labels /><created>2016-04-01T21:11:47Z</created><updated>2016-04-04T19:42:16Z</updated><resolved>2016-04-04T19:42:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T19:42:15Z" id="205463708">These docs have been completely rewritten in https://github.com/elastic/elasticsearch/commit/06604708d4bfc56eb397c6003567886bb1ce0114
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>analyzers on non-string fields do not migrate correctly to 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17485</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.7 or 1.8

**OS version**: OSX 10.9.5 but it does not really matter, issue is reproducible on various Linux flavors as well.

**Description of the problem including expected versus actual behavior**:
Looks like there was a change in 2.2 related to analyzers. 
In 2.0 you can specify analyzer and search_analyzer on any filed type. In 2.2 you can only do it for 'string' types. It maybe makes sense but it is not compatible with previous versions and many cases fail in different way.

**Steps to reproduce**:
_With 2.0.2, create a new index:_

```
PUT index_try123
{
  "mappings": {
    "doc": {
      "properties": {
        "prop1": {
          "type": "boolean",
          "analyzer": "standard"
        },
        "prop2": {
          "type": "string",
          "analyzer": "standard",
          "search_analyzer": "standard"
        }
      }
    }
  }
}
```

_Upgrade ES to 2.2.1_

_Try to update the mapping:_

```
PUT index_try123/_mapping/doc
{
  "properties": {
        "prop1": {
          "type": "boolean"
        },
        "prop2": {
          "type": "string",
          "analyzer": "standard"
        },
        "prop3": {
          "type": "string",
          "analyzer": "standard"
        }
      }
}

```

This fails with the error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "analyzer [_keyword] not found for field [prop1]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "analyzer [_keyword] not found for field [prop1]"
   },
   "status": 400
}
```

If you look at the index after you created it with 2.0.1, you will see "search_analyzer": "_keyword" added to prop1.

_Now, delete the index and try to create it again:_

```
DELETE index_try123

PUT index_try123
{
  "mappings": {
    "doc": {
      "properties": {
        "prop1": {
          "type": "boolean",
          "analyzer": "standard"
        },
        "prop2": {
          "type": "string",
          "analyzer": "standard",
          "search_analyzer": "standard"
        }
      }
    }
  }
}
```

This now fails with this error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "mapper_parsing_exception",
            "reason": "Mapping definition for [prop1] has unsupported parameters:  [analyzer : standard]"
         }
      ],
      "type": "mapper_parsing_exception",
      "reason": "Failed to parse mapping [doc]: Mapping definition for [prop1] has unsupported parameters:  [analyzer : standard]",
      "caused_by": {
         "type": "mapper_parsing_exception",
         "reason": "Mapping definition for [prop1] has unsupported parameters:  [analyzer : standard]"
      }
   },
   "status": 400
}
```

So, if you had a mapping with an analyzer set on non-string field in 2.0, and then migrate to 2.2, you can't update existing mappings,  nor you can create the same mapping on a fresh installation without modifying the put request.
</description><key id="145268917">17485</key><summary>analyzers on non-string fields do not migrate correctly to 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">plebedev</reporter><labels /><created>2016-04-01T19:49:01Z</created><updated>2016-04-04T19:40:24Z</updated><resolved>2016-04-04T19:40:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T19:40:01Z" id="205462686">Hi @plebedev 

Your issue is correct, but is a real edge case.  I think it's one for the migration tool rather than anything we plan to fix in elasticsearch.  I'll move it there.
</comment><comment author="clintongormley" created="2016-04-04T19:40:23Z" id="205462845">This issue was moved to elastic/elasticsearch-migration#53
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch-plugin.bat Java options fail on PowerShell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17484</link><project id="" key="" /><description>```
.\elasticsearch-plugin install -Des.plugins.staging=true x-pack
```

Results in the option being passed as `-Des=.plugins.staging` and causes

```
ERROR: Must supply a single plugin id argument
```

The script works fine in cmd.exe.
</description><key id="145267060">17484</key><summary>elasticsearch-plugin.bat Java options fail on PowerShell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">gmarz</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-04-01T19:38:49Z</created><updated>2016-04-04T08:58:33Z</updated><resolved>2016-04-04T08:58:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2016-04-04T08:58:33Z" id="205201199">`-Des.plugins.staging=true` always has to be quoted when called from PS.

Without it the `.plugins.staging=true` will be a `CommandArgument` and the  equals sign will be split on in the opts handling of the bat file itself

Causing this call to java to be constructed:

``` bat
"$JAVA_HOME" -client -Des.path.home="$ES_HOME" \ 
   "-Des=.plugins.staging" -cp "$ES_HOME/lib/*;" \
   "org.elasticsearch.plugins.PluginCli" \
   "install" "true" "x-pack"
```

Sending two arguments to install. 

For full reference here is the output of the powershell parsing

``` powershell
PS ~ [management.automation.psparser]::Tokenize(
    "elasticsearch-plugin.bat install -Des.plugins.staging=true x-pack",
     [ref]$null)


Content     : elasticsearch-plugin.bat
Type        : Command
Start       : 0
Length      : 24
StartLine   : 1
StartColumn : 1
EndLine     : 1
EndColumn   : 25

Content     : install
Type        : CommandArgument
Start       : 25
Length      : 7
StartLine   : 1
StartColumn : 26
EndLine     : 1
EndColumn   : 33

Content     : -Des
Type        : CommandParameter
Start       : 33
Length      : 4
StartLine   : 1
StartColumn : 34
EndLine     : 1
EndColumn   : 38

Content     : .plugins.staging=true
Type        : CommandArgument
Start       : 37
Length      : 21
StartLine   : 1
StartColumn : 38
EndLine     : 1
EndColumn   : 59

Content     : x-pack
Type        : CommandArgument
Start       : 59
Length      : 6
StartLine   : 1
StartColumn : 60
EndLine     : 1
EndColumn   : 66
```

``` powershell

PS ~ [management.automation.psparser]::Tokenize(
     'elasticsearch-plugin.bat install "-Des.plugins.staging=true" x-pack', 
    [ref]$null)


Content     : elasticsearch-plugin.bat
Type        : Command
Start       : 0
Length      : 24
StartLine   : 1
StartColumn : 1
EndLine     : 1
EndColumn   : 25

Content     : install
Type        : CommandArgument
Start       : 25
Length      : 7
StartLine   : 1
StartColumn : 26
EndLine     : 1
EndColumn   : 33

Content     : -Des.plugins.staging=true
Type        : String
Start       : 33
Length      : 27
StartLine   : 1
StartColumn : 34
EndLine     : 1
EndColumn   : 61

Content     : x-pack
Type        : CommandArgument
Start       : 61
Length      : 6
StartLine   : 1
StartColumn : 62
EndLine     : 1
EndColumn   : 68
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problems with http.cors.allow-methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17483</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.0
**JVM version**: 1.8.0_31
**OS version**: MAC OS X 10.10.5
**Description of the problem including expected versus actual behavior**:
Definition of an accepted a custom header does not seem to work.

**Steps to reproduce**:
1. Add the following lines to the configuration file

```
http.cors.enabled: true
http.cors.allow-origin: "*"
http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETE
http.cors.allow-headers: "X-Requested-With, Content-Type, Content-Length, X-User"
```
1. Ran an ajax POST request with X-User header.
2. Getting response "Request header field x-user is not allowed by Access-Control-Allow-Headers in preflight response."
</description><key id="145246162">17483</key><summary>Problems with http.cors.allow-methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">pciccarese</reporter><labels><label>:REST</label><label>bug</label><label>v2.3.1</label><label>v5.0.0-alpha2</label></labels><created>2016-04-01T18:07:55Z</created><updated>2016-04-07T19:35:20Z</updated><resolved>2016-04-07T19:35:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pciccarese" created="2016-04-04T13:51:32Z" id="205303554">I am afraid I will need to reopen this.

**Elasticsearch version**: 2.3.0
**JVM version**: 1.8.0_31
**OS version**: MAC OS X 10.10.5

**Use of custom header does not seem to work reliably.**

Here is the CORS section of my configuration (elasticsearch.yml) file:

```
http.cors.enabled: true
http.cors.allow-origin: "*"
http.cors.allow-headers: "X-Requested-With, Content-Type, Content-Length, X-User"
```

The client code is performing the following call:

```
jQuery.ajax({
    url: requrl,
    data: reqdata,
    type: 'POST',
    headers: {"X-User": user},
    success: function (result, status, xhr) {
        resolve(result);
    },
    error: function (xhr, status, error) {
        reject(error);
    }
});

```

Now, that request always goes through with Firefox 45.0.1.
However, it does not work for:
- Safari Version 9.1 (10601.5.17.4)
- Chrome Version 49.0.2623.110 (64-bit) 

When inspecting with Chrome I get:

```
XMLHttpRequest cannot load http://localhost:9200/myindex/mytype/_search. 
Request header field x-user is not allowed by Access-Control-Allow-Headers in preflight response.
```

These are the headers details I see in Chrome:

```
General
-----------
Request URL:http://localhost:9200/myindex/mytype/_search
Request Method:OPTIONS
Status Code:200 OK
Remote Address:[::1]:9200

Response Headers
--------------------------
Access-Control-Allow-Methods:
Access-Control-Allow-Origin:*
Access-Control-Max-Age:1728000
content-length:0
date:Mon, 04 Apr 2016 13:49:00 GMT

Request Headers
-----------------------
Accept:*/*
Accept-Encoding:gzip, deflate, sdch
Accept-Language:en-US,en;q=0.8,it;q=0.6
Access-Control-Request-Headers:accept, content-type, x-user
Access-Control-Request-Method:POST
Cache-Control:no-cache
Connection:keep-alive
Host:localhost:9200
Origin:http://localhost:3333
Pragma:no-cache
Referer:http://localhost:3333/
User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.110 Safari/537.36
```

**The very same configuration, index and client code works reliably in 2.2.0**.
After seeing in the release notes the entry: &#8220;More robust handling of CORS HTTP Access Control&#8221; I am wondering if that has anything to do with it.
</comment><comment author="skundrik" created="2016-04-04T17:40:17Z" id="205409727">I have experienced same issue which now breaks Grafana UI.
</comment><comment author="skundrik" created="2016-04-04T19:02:28Z" id="205449403">- The problem appears to be in `NettyHttpServerTransport.buildCorsConfig()` where first parameter to the `getAsArray()` function should be the setting name and not the value of the setting. 
- It affects `cors-allow-headers` as well.
- It's also specific to `2.3 branch` as the whole settings system seems to have been reworked on `master`.

The following code

``` java
        String[] strMethods = settings.getAsArray(settings.get(SETTING_CORS_ALLOW_METHODS, DEFAULT_CORS_METHODS), new String[0]);
        HttpMethod[] methods = new HttpMethod[strMethods.length];
        for (int i = 0; i &lt; methods.length; i++) {
            methods[i] = HttpMethod.valueOf(strMethods[i]);
        }
        return builder.allowedRequestMethods(methods)
                      .maxAge(settings.getAsInt(SETTING_CORS_MAX_AGE, DEFAULT_CORS_MAX_AGE))
                      .allowedRequestHeaders(settings.getAsArray(settings.get(SETTING_CORS_ALLOW_HEADERS, DEFAULT_CORS_HEADERS), new String[0]))
                      .shortCircuit()
                      .build();
```

should say

``` java
        String[] strMethods = settings.getAsArray(SETTING_CORS_ALLOW_METHODS, DEFAULT_CORS_METHODS);
        HttpMethod[] methods = new HttpMethod[strMethods.length];
        for (int i = 0; i &lt; methods.length; i++) {
            methods[i] = HttpMethod.valueOf(strMethods[i]);
        }
        return builder.allowedRequestMethods(methods)
                      .maxAge(settings.getAsInt(SETTING_CORS_MAX_AGE, DEFAULT_CORS_MAX_AGE))
                      .allowedRequestHeaders(settings.getAsArray(SETTING_CORS_ALLOW_HEADERS, DEFAULT_CORS_HEADERS))
                      .shortCircuit()
                      .build();

```
</comment><comment author="abeyad" created="2016-04-05T13:32:37Z" id="205806698">Thank you for reporting the issue.  We have PRs open to fix them for different versions.

#17523 #17524 #17525 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkProcessor exception in 2.3.0 - IllegalIndexShardStateException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17482</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.0 (official es docker image)

**JVM version**: OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)

**OS version**: Debian 8

**Description of the problem including expected versus actual behavior**: Starting from es 2.2 and still in 2.3, we get the following exception only while using the Java BulkProcessor:

```
[356019-20160321][[356019-20160321][0]] IllegalIndexShardStateException[CurrentState[POST_RECOVERY] operation only allowed when started/recovering, origin [PRIMARY]]
    at org.elasticsearch.index.shard.IndexShard.ensureWriteAllowed(IndexShard.java:1068)
    at org.elasticsearch.index.shard.IndexShard.create(IndexShard.java:538)
    at org.elasticsearch.index.engine.Engine$Create.execute(Engine.java:810)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:237)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:326)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:119)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:68)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Did we lose the records that would have been indexed, or does BulkProcessor wait for the index to get in a state where it can write and retry?  If the latter, are these exceptions "harmless" as stated in #15478 (there were multiple exceptions there so not sure if this was considered the harmless one).

**Steps to reproduce**:
1. Consume from Kafka topic with three consumer threads that share one ES BulkProcessor object with all default values.
</description><key id="145236107">17482</key><summary>BulkProcessor exception in 2.3.0 - IllegalIndexShardStateException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reflection</reporter><labels><label>:Bulk</label></labels><created>2016-04-01T17:24:06Z</created><updated>2016-04-04T23:31:03Z</updated><resolved>2016-04-04T23:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-01T21:03:50Z" id="204565153">&gt; official es docker image

Just so that there is no confusion: there is not an official Docker image provided by Elastic at this time. Any images that you currently obtain off of Docker Hub that are marked as "official" are not vetted by Elastic. (And I'm not saying that you're confused here, this is for clarity for anyone else reading this thread.)

&gt; Did we lose the records that would have been indexed, or does BulkProcessor wait for the index to get in a state where it can write and retry?

How did you configure the `BulkProcessor`? How are you committing message consumption back to Kafka?

&gt; If the latter, are these exceptions "harmless" as stated in #15478 (there were multiple exceptions there so not sure if this was considered the harmless one).

The situation in #15478 is quite different than your situation. The logging in #15478 was debug logging from stats requests where shards were being ignored because they were in a state not ready to receive such requests. A stats request missing a recovering shard is not a big deal. Your situation is concerning indexing into a primary shard and how you handle this exception is critically important to whether or not there is a data loss. Thus, the questions above about how you configured the `BulkProcessor` and how you're handling topic consumption.

I note that you truncated part of the exception message (in particular, the log level and the log prefix). Is there any chance that you can provide the remainder of the log message, including any additional pieces of the exception if there are any (there might not be other than the missing level and prefix)?
</comment><comment author="reflection" created="2016-04-04T23:28:56Z" id="205539986">Automatic offset committing in Kafka with the new consumer wasn't working the way I thought it would.  I switched back to the old high-level consumer, and these errors went away.

That being said, if we do see these kind of errors, [BulkProcessor](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html) looks like it would retry with whatever you set in its BackoffPolicy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from MLT.Item</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17481</link><project id="" key="" /><description>Relates to #17085
</description><key id="145230004">17481</key><summary>Remove PROTOTYPE from MLT.Item</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-04-01T16:53:38Z</created><updated>2016-04-02T02:34:52Z</updated><resolved>2016-04-02T02:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-01T18:02:42Z" id="204495047">this is small, I like it a lot :) LGTM
</comment><comment author="nik9000" created="2016-04-02T02:34:52Z" id="204632820">Thanks for the reviews tonight @javanna ! All merged in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from VersionType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17480</link><project id="" key="" /><description>Relates to #17085.
</description><key id="145228464">17480</key><summary>Remove PROTOTYPE from VersionType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-04-01T16:46:03Z</created><updated>2016-04-02T02:01:05Z</updated><resolved>2016-04-02T02:01:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-01T18:03:51Z" id="204496018">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error in decay function-score picture</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17479</link><project id="" key="" /><description>The picture for the decay function score is old and has **reference** instead of **origin**.

https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#_supported_decay_functions

https://www.elastic.co/guide/en/elasticsearch/reference/current/images/decay_2d.png

![decay_2d](https://cloud.githubusercontent.com/assets/361280/14208628/b3ed4732-f7f6-11e5-8c5a-78d718fe356f.png)
</description><key id="145184298">17479</key><summary>Error in decay function-score picture</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">pmusa</reporter><labels><label>docs</label></labels><created>2016-04-01T13:44:42Z</created><updated>2016-04-20T11:38:49Z</updated><resolved>2016-04-20T11:38:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T19:28:01Z" id="205458904">@brwe could you correct this image please?
</comment><comment author="pmusa" created="2016-04-19T13:52:50Z" id="211931097">&lt;img width="404" alt="screen shot 2016-04-19 at 2 49 27 pm" src="https://cloud.githubusercontent.com/assets/361280/14641467/53e0514e-063e-11e6-97e8-1bfcd037aa56.png"&gt;
</comment><comment author="clintongormley" created="2016-04-20T11:38:34Z" id="212391521">thanks @pmusa - fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In SLES 11, the chkconfig and service commands aren't on the path of &#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17478</link><project id="" key="" /><description>When installing the RPM in SLES 11 SP4, the post install script says:

```
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using chkconfig
 sudo chkconfig --add elasticsearch
### You can start elasticsearch service by executing
 sudo service elasticsearch start
```

But chkconfig and service aren't in the user's path, so these commands fail.  Instead, using `sudo -i` means that an initial login is simulated and sudo then uses root's path
</description><key id="145182828">17478</key><summary>In SLES 11, the chkconfig and service commands aren't on the path of &#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label></labels><created>2016-04-01T13:38:09Z</created><updated>2016-04-22T12:57:37Z</updated><resolved>2016-04-22T12:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-22T12:57:37Z" id="213418314">We can't support RPM installation on SLES11-SP4 (old version of RPM, see https://github.com/elastic/elasticsearch/issues/17470) so I'm going to close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added RPM metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17477</link><project id="" key="" /><description>When running the generated RPM through rpmrebuild, it complained about missing metadata such as License, Vendor, Distribution.

This PR adds these missing values
</description><key id="145180939">17477</key><summary>Added RPM metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-04-01T13:29:11Z</created><updated>2016-04-02T08:38:34Z</updated><resolved>2016-04-02T08:38:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-01T13:29:22Z" id="204396499">@rjernst please could you take a look
</comment><comment author="nik9000" created="2016-04-01T16:15:55Z" id="204456567">LGTM
</comment><comment author="rjernst" created="2016-04-01T19:56:25Z" id="204544347">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add jna as non optional within the test framework</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17476</link><project id="" key="" /><description>I'm using the test framework 5.0.0-alpha1 from an external plugin (see `pom.xml` snippet):

``` xml
        &lt;dependency&gt;
            &lt;groupId&gt;org.elasticsearch.test&lt;/groupId&gt;
            &lt;artifactId&gt;framework&lt;/artifactId&gt;
            &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
```

When implementing a simple IT like this:

``` java
public class BanaPluginTest extends ESIntegTestCase {
    public void testBanoPlugin() {    }
}
```

And launching, I'm getting this error:

```
ERROR   0.00s | BanaPluginTest (suite) &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.NoClassDefFoundError: com/sun/jna/Structure$ByReference
   &gt;    at java.lang.ClassLoader.defineClass1(Native Method)
   &gt;    at java.lang.ClassLoader.defineClass(ClassLoader.java:760)
   &gt;    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
   &gt;    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)
   &gt;    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)
   &gt;    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)
   &gt;    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)
   &gt;    at java.security.AccessController.doPrivileged(Native Method)
   &gt;    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)
   &gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
   &gt;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
   &gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
   &gt;    at org.elasticsearch.bootstrap.JNANatives.trySetMaxSizeVirtualMemory(JNANatives.java:131)
   &gt;    at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:138)
   &gt;    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:77)
   &gt;    at org.elasticsearch.test.ESTestCase.&lt;clinit&gt;(ESTestCase.java:111)
   &gt;    at java.lang.Class.forName0(Native Method)
   &gt;    at java.lang.Class.forName(Class.java:348)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:585)
   &gt; Caused by: java.lang.ClassNotFoundException: com.sun.jna.Structure$ByReference
   &gt;    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
   &gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
   &gt;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
   &gt;    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
   &gt;    ... 19 more
```

Actually `jna` is defined a an optional library in elasticsearch, which is fine, but the test framework uses it.

To fix that, I have to change my `pom.xml` and add:

``` xml
        &lt;dependency&gt;
            &lt;groupId&gt;net.java.dev.jna&lt;/groupId&gt;
            &lt;artifactId&gt;jna&lt;/artifactId&gt;
            &lt;version&gt;4.1.0&lt;/version&gt;
            &lt;scope&gt;test&lt;/scope&gt;
        &lt;/dependency&gt;
```

I think we should declare `jna` not optional in the test framework so it will come by default when anyone uses it.
</description><key id="145179921">17476</key><summary>Add jna as non optional within the test framework</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Core</label></labels><created>2016-04-01T13:24:22Z</created><updated>2016-05-04T14:26:23Z</updated><resolved>2016-05-04T14:26:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-01T13:24:42Z" id="204395399">@rjernst WDYT?
</comment><comment author="jimczi" created="2016-04-01T14:34:20Z" id="204418642">The direct calls to JNANatives trySetMaxNumberOfThreads and trySetMaxSizeVirtualMemory in bootstrap makes jna non optional on Linux and MAC_OS_X.
I don't think you should add jna for your tests nor declare jna non optional.
@jasontedor couldn't we use the Natives wrapper for those calls ?
</comment><comment author="jasontedor" created="2016-04-02T15:24:56Z" id="204737330">I agree with @jimferenczi and I opened #17492.
</comment><comment author="dadoonet" created="2016-04-02T16:06:50Z" id="204746110">I definitely prefer this idea as well! Thanks guys!
</comment><comment author="jasontedor" created="2016-04-02T16:15:29Z" id="204749222">Closed by #17492 via e85535724ef9a1cfa622fd79a3593d3b714e5570.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Command line arguments with comma must be quoted on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17475</link><project id="" key="" /><description>... with some log message cosmetics.
</description><key id="145168324">17475</key><summary>Command line arguments with comma must be quoted on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-04-01T12:28:38Z</created><updated>2016-04-01T12:46:47Z</updated><resolved>2016-04-01T12:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-04-01T12:35:01Z" id="204381472">LGTM
</comment><comment author="tlrx" created="2016-04-01T12:46:47Z" id="204386002">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Production startup checks should output all problems at once</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17474</link><project id="" key="" /><description>The production start up checks (e.g. minimum master nodes, max file descriptors) should check all the settings and output a single error containing all the problems rather than showing the first error and then waiting for it to be fixed and the node started again before showing the next error.
</description><key id="145146168">17474</key><summary>Production startup checks should output all problems at once</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-04-01T10:38:26Z</created><updated>2016-04-13T13:05:32Z</updated><resolved>2016-04-13T13:05:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-07T15:56:37Z" id="206967887">I opened #17595.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Network interface names containing colons can't be bound to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17473</link><project id="" key="" /><description>**Elasticsearch version**: 
2.3.0

**JVM version**:
java version "1.8.0_77"
Java(TM) SE Runtime Environment (build 1.8.0_77-b03)
Java HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)

**OS version**:
Ubuntu 14.04:4

**Steps to reproduce**:
1. Add a network interface that contains a colon
2. Add this interface to elasticsearch.yml as the network host or publish host like so: network.host: `_bond0:0_`
3. Restart elasticsearch

**Provide logs (if relevant)**:

```
[2016-04-01 11:59:04,363][INFO ][node                     ] [master_1] initialized
[2016-04-01 11:59:04,363][INFO ][node                     ] [master_1] starting ...
[2016-04-01 11:59:04,466][ERROR][bootstrap                ] [master_1] Exception
java.lang.IllegalArgumentException: No interface named 'bond0:0' found, got [name:lo (lo), name:bond0 (bond0), name:bond0:0 (bond0:0)]
        at org.elasticsearch.common.network.NetworkUtils.getAddressesForInterface(NetworkUtils.java:232)
        at org.elasticsearch.common.network.NetworkService.resolveInternal(NetworkService.java:262)
        at org.elasticsearch.common.network.NetworkService.resolveInetAddresses(NetworkService.java:209)
        at org.elasticsearch.common.network.NetworkService.resolveBindHostAddresses(NetworkService.java:122)
        at org.elasticsearch.transport.netty.NettyTransport.bindServerBootstrap(NettyTransport.java:424)
        at org.elasticsearch.transport.netty.NettyTransport.doStart(NettyTransport.java:321)
        at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:68)
        at org.elasticsearch.transport.TransportService.doStart(TransportService.java:170)
        at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:68)
        at org.elasticsearch.node.Node.start(Node.java:252)
        at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:221)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:287)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
[2016-04-01 11:59:04,486][INFO ][node                     ] [master_1] stopping ...
[2016-04-01 11:59:04,488][INFO ][node                     ] [master_1] stopped
```
</description><key id="145143475">17473</key><summary>Network interface names containing colons can't be bound to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karlra</reporter><labels><label>:Network</label><label>discuss</label></labels><created>2016-04-01T10:22:36Z</created><updated>2016-09-13T11:41:30Z</updated><resolved>2016-09-13T11:41:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="daschl" created="2016-07-20T17:00:34Z" id="234012670">I was curious so I dug in a bit. The problem is not the colon naming but rather that this is a virtual interface. `NetworkUtils.getAddressesForInterface` uses `NetworkInterface.getByName` of the JDK which returns null for virtual interfaces, even if the "name" is right. 

For your case `NetworkInterface.getByName("bond0").getSubInterfaces().nextElement();` would get you the right element. 

Now the thing is, its quite unpractical to split it up by colon itself and then match the subinterfaces name, but I think a compromise would be to just iterate over the result from `NetworkService .getInterfaces` and get the one where the name matches - because this includes all the physical and virtual interfaces (what the JDK refers to as sub interfaces).

If that makes sense I can get a PR up :)

@jasontedor wdyt?
</comment><comment author="daschl" created="2016-07-22T18:16:30Z" id="234617125">I'll rework the test case and resubmit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch reports fielddata memory for doc_values only index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17472</link><project id="" key="" /><description>**Elasticsearch version**:

```
# elasticsearch --version
Version: 2.2.0, Build: 8ff36d1/2016-01-27T13:32:39Z, JVM: 1.8.0_72-internal
```

**JVM version**:

```
# java -version
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**: Debian Jessie on kernel 4.1.3.

**Description of the problem including expected versus actual behavior**:

Elasticsearch reports fielddata memory for `doc_values` only index:

```
index                   fm   fcm    sm
raytracer-2016.03.31 6.5gb 2.5gb 4.6gb
```

Index mapping declares everything to be `doc_values` (and it should be default in 2.2 anyway):

``` json
{
  "template": "raytracer-*",
  "settings": {
    "number_of_shards": 50,
    "number_of_replicas": 1,
    "index.refresh_interval": "30s",
    "index.query.default_field": "id"
  },
  "mappings": {
    "_default_": {
      "_all": {
        "enabled": false
      },
      "dynamic_templates": [
        {
          "string_fields": {
            "match": "*",
            "match_mapping_type": "string",
            "mapping": {
              "type": "string",
              "index": "not_analyzed",
              "ignore_above": 256,
              "doc_values": true
            }
          }
        },
        {
          "long_fields": {
            "match": "*",
            "match_mapping_type": "long",
            "mapping": {
              "type": "long",
              "doc_values": true
            }
          }
        },
        {
          "double_fields": {
            "match": "*",
            "match_mapping_type": "double",
            "mapping": {
              "type": "double",
              "doc_values": true
            }
          }
        },
        {
          "date_fields": {
            "match": "*",
            "match_mapping_type": "date",
            "mapping": {
              "type": "date",
              "doc_values": true
            }
          }
        }
      ]
    }
  }
}
```
</description><key id="145134870">17472</key><summary>Elasticsearch reports fielddata memory for doc_values only index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-04-01T09:40:21Z</created><updated>2016-07-30T00:09:31Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-04-01T12:05:20Z" id="204369726">Do you perform terms aggregations on your string fields? If yes then this memory usage is due to global ordinals that elasticsearch builds on top of doc values.
</comment><comment author="bobrik" created="2016-04-01T12:11:33Z" id="204372325">Yes, I do. Is it documented somewhere?
</comment><comment author="jpountz" created="2016-04-01T14:24:52Z" id="204415907">It doesn't seem to be, at least not close to the docs of the stats APIs.
</comment><comment author="ppf2" created="2016-07-30T00:09:31Z" id="236323346">Let's document this since we will not be implementing separate reporting of ordinals' memory usage (https://github.com/elastic/elasticsearch/issues/9838) :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document task id's as string in the rest spec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17471</link><project id="" key="" /><description /><key id="145127969">17471</key><summary>Document task id's as string in the rest spec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>docs</label><label>v2.3.1</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-04-01T09:05:51Z</created><updated>2016-04-01T13:03:02Z</updated><resolved>2016-04-01T12:54:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-04-01T12:44:41Z" id="204384962">LGTM
</comment><comment author="Mpdreamz" created="2016-04-01T13:02:49Z" id="204390325">merged to `master`, `2.x` and `2.3`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Installing Elasticsearch 5.0.0-alpha1 rpm segfaults SLES11SP4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17470</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha1

**JVM version**: Oracle JDK 1.8.0_77-b03

**OS version**: SLES 11SP4

**Description of the problem including expected versus actual behavior**:

`elasticsearch 5.0.0-alpha1` rpm package is not installable on SLES11SP4. `rpm -i` fails complaining about the header and exits with a segmentation fault.
Tangentially to this, `gradle checkSles11` fails trying to install the created elasticsearch package.
Note that `elasticsearch-2.0.0.0.rpm` installs without problems.

Expected behavior is that `elasticsearch-5.0.0*.rpm` should be installable on SLES 11SP4 via `rpm` or `zypper`

**Steps to reproduce**:
1. clone [my forked branch](https://github.com/dliappis/elasticsearch/tree/add_sles11_support) containing the sles-11 vagrant additions
2. `gradle prepareTestRoot`
3. `vagrant up sles-11 &amp;&amp; vagrant ssh sles-11`
4. `cd $TESTROOT`
5. `sudo bats $BATS/*_rpm_*.bats`

**Provide logs (if relevant)**:
1. Download and install elasticsearch-5.0.0-alpha1.rpm
   
   ```
   sles-11:~$ sudo su -
   sles-11:~$ pwd
   /root
   sles-11:~$ rpm -qa | grep -i elastic
   sles-11:~$ wget http://download.elastic.co/elasticsearch/staging/5.0.0-alpha1-f27399d/org/elasticsearch/distribution/rpm/elasticsearch/5.0.0-alpha1/elasticsearch-5.0.0-alpha1.rpm
   --2016-04-01 08:15:11--  http://download.elastic.co/elasticsearch/staging/5.0.0-alpha1-f27399d/org/elasticsearch/distribution/rpm/elasticsearch/5.0.0-alpha1/elasticsearch-5.0.0-alpha1.rpm
   Resolving download.elastic.co... 184.72.244.137, 184.73.171.50, 184.73.186.87, ...
   Connecting to download.elastic.co|184.72.244.137|:80... connected.
   HTTP request sent, awaiting response... 200 OK
   Length: 26878246 (26M) [application/x-rpm]
   Saving to: `elasticsearch-5.0.0-alpha1.rpm'
   
   100%[=============================================================================&gt;] 26,878,246  1.80M/s   in 19s     
   
   2016-04-01 08:15:31 (1.37 MB/s) - `elasticsearch-5.0.0-alpha1.rpm' saved [26878246/26878246]
   
   sles-11:~$ rpm -ivh ./elasticsearch-5.0.0-alpha1.rpm 
   error: ./elasticsearch-5.0.0-alpha1.rpm: headerRead failed: Header sanity check: OK
   Segmentation fault
   ```
2. Trying to install 5.0.0-alpha1 with zypper
   
   ``` shell
   sles-11:~$ sudo zypper ar -f -t rpm-md https://download.elastic.co/elasticsearch/staging/5.0.0-alpha1-f27399d/repos/5.x/centos elasticsearch
   Adding repository 'elasticsearch' [done]
   Repository named 'elasticsearch' already exists. Please use another alias.
   sles-11:~$     sudo zypper install elasticsearch
   Loading repository data...
   Reading installed packages...
   Resolving package dependencies...
   
   The following NEW package is going to be installed:
   elasticsearch 
   
   The following package is not supported by its vendor:
   elasticsearch 
   
   1 new package to install.
   Overall download size: 25.6 MiB. After the operation, additional 28.6 MiB will be used.
   Continue? [y/n/? shows all options] (y): y
   Installing: elasticsearch-5.0.0-alpha1 [error]
   Installation of elasticsearch-5.0.0-alpha1 failed:
   (with --nodeps --force) Error: Subprocess failed. Error: RPM failed: error: /var/cache/zypp/packages/elasticsearch/elasticsearch-5.0.0-alpha1.rpm: headerRead failed: Header sanity check: OK
   ```
</description><key id="145120651">17470</key><summary>Installing Elasticsearch 5.0.0-alpha1 rpm segfaults SLES11SP4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">dliappis</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2016-04-01T08:34:39Z</created><updated>2016-04-22T12:58:15Z</updated><resolved>2016-04-22T12:58:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-01T09:57:08Z" id="204335438">This looks like a bug in the gradle-ospackage-plugin.  We're not alone: https://github.com/nebula-plugins/gradle-ospackage-plugin/issues/162
</comment><comment author="clintongormley" created="2016-04-01T13:07:41Z" id="204391452">OK I've got SLES11 working, but the RPM has to be rebuilt with http://rpmrebuild.sourceforge.net/ which looks like it is available on at least opensuse and ubuntu.  Looks like redline (used by gradle's ospackage plugin) doesn't like rpm 4.4 - see https://github.com/craigwblake/redline/issues/97

```
rpmrebuild -p elasticsearch-5.0.0-alpha1.rpm
```

rpmrebuild pointed out that we were missing some metadata eg License, which I'll open a PR to fix.

The init.d script required PR https://github.com/elastic/elasticsearch/pull/12555 in order to start.
</comment><comment author="clintongormley" created="2016-04-01T13:15:30Z" id="204393310">@rjernst see https://github.com/elastic/elasticsearch/issues/17470#issuecomment-204391452
</comment><comment author="rjernst" created="2016-04-05T23:26:31Z" id="206032065">I investigated this and commented on the redline issue. It looks like the problem is 4.4 only knew about md5 checksums, but redline writes md5 and sha1.  Using `rpmrebuild` works because it has special logic just for this case, where it writes the rpm file back out with only md5 digests.
</comment><comment author="nik9000" created="2016-04-08T13:58:03Z" id="207442492">@rjernst is this a thing you are actively working on? We've got SLES builds that constantly fail in Jenkins. Either we should fix them or disable them.
</comment><comment author="nik9000" created="2016-04-08T13:58:27Z" id="207442650">Example: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=sles/221/console
</comment><comment author="clintongormley" created="2016-04-22T12:58:11Z" id="213418616">We can't support RPM installation on SLES11-SP4 (old version of RPM) so I'm going to close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Add VM to test internal RC repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17469</link><project id="" key="" /><description>In order to test if the repo creation was successful when creating an RC, we should just fire up a deb/rpm based vagrant VM and download from those repos.

My idea would be to export `VERSION` and `HASH` into the host environment, and then run run the following install script on ubuntu

``` bash
wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
rm -fr /etc/apt/sources.list.d/elasticsearch.list
echo deb https://download.elastic.co/elasticsearch/staging/#{VERSION}-#{HASH}/repos/5.x/debian/ stable main &gt; /etc/apt/sources.list.d/elasticsearch.list
dpkg -P elasticsearch
apt-get update
apt-get -y install elasticsearch
/usr/share/elasticsearch/bin/plugin install -Des.plugins.staging=true analysis-kuromoji
/usr/share/elasticsearch/bin/plugin install -Des.plugins.staging=true x-pack
```

and this one on centos/whatever

``` bash
rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
rm -fr /etc/yum.repos.d/elasticsearch.repo

(
echo "[elasticsearch-5.x]"
echo "name=Elasticsearch repository for packages"
echo "baseurl=https://download.elastic.co/elasticsearch/staging/#{VERSION}-#{HASH}/repos/5.x/centos"
echo "gpgcheck=1"
echo "gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch"
echo "enabled=1"
) &gt; /etc/yum.repos.d/elasticsearch.repo

rpm -e elasticsearch || true
yum -y install elasticsearch
/usr/share/elasticsearch/bin/elasticsearch-plugin install -Des.plugins.staging=true analysis-kuromoji
/usr/share/elasticsearch/bin/elasticsearch-plugin install -Des.plugins.staging=true x-pack
```

Unfortunately we cannot seem to use the existing infra that uses the `extra` parameter as java is installed after that. 

I guess someone with deeper vagrant insights might have a better idea than I do (maybe just a small gradle task), so please suggest things here.
</description><key id="145120228">17469</key><summary>Release: Add VM to test internal RC repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>adoptme</label><label>build</label></labels><created>2016-04-01T08:32:03Z</created><updated>2016-04-01T08:32:15Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Increase default file descriptors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17468</link><project id="" key="" /><description>In `/etc/default/elasticsearch` we have `MAX_OPEN_FILES=65535`. If you uncomment that, but don't edit it, and start ES you get this in the logs;

```
[2016-04-01 05:02:50,128][WARN ][env                      ] [Achebe] max file descriptors [65535] for elasticsearch process likely too low, consider increasing to at least [65536]
```

Can we just increase the default by one to ensure our checks align with our default settings?
</description><key id="145087967">17468</key><summary>Increase default file descriptors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2016-04-01T05:06:32Z</created><updated>2016-04-01T08:30:26Z</updated><resolved>2016-04-01T08:30:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-01T08:30:26Z" id="204307156">Duplicate of #17430
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve error reporting for permissions errors on script directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17467</link><project id="" key="" /><description>Did an upgrade to 2.3 and restarted the process and saw;

```
# service elasticsearch start
 * Starting Elasticsearch Server
# tailf /var/log/elasticsearch/elasticsearch.log
#
# LICENSE EXPIRED ON [Friday, February 05, 2016]. IF YOU HAVE A NEW LICENSE, PLEASE
# UPDATE IT. OTHERWISE, PLEASE REACH OUT TO YOUR SUPPORT CONTACT.
#
[2016-04-01 04:45:57,713][INFO ][gateway                  ] [Nightwatch] recovered [38] indices into cluster_state
[2016-04-01 04:46:03,004][INFO ][cluster.routing.allocation] [Nightwatch] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).
[2016-04-01 04:46:39,207][INFO ][node                     ] [Nightwatch] stopping ...
[2016-04-01 04:46:39,485][INFO ][node                     ] [Nightwatch] stopped
[2016-04-01 04:46:39,485][INFO ][node                     ] [Nightwatch] closing ...
[2016-04-01 04:46:39,519][INFO ][node                     ] [Nightwatch] closed
```

But after a while there was no new logs showing the service starting, which was confirmed with;

```
# ps -ef|grep java
root     16365  1680  0 04:47 pts/0    00:00:00 grep --color=auto java
```

So I ran it manually;

```
# /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.0.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.conf=/etc/elasticsearch
Exception in thread "main" ElasticsearchException[Failed to load logging configuration]; nested: AccessDeniedException[/etc/elasticsearch/scripts];
Likely root cause: java.nio.file.AccessDeniedException: /etc/elasticsearch/scripts
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:427)
    at java.nio.file.Files.newDirectoryStream(Files.java:457)
    at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:300)
    at java.nio.file.FileTreeWalker.next(FileTreeWalker.java:372)
    at java.nio.file.Files.walkFileTree(Files.java:2706)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:142)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:103)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:243)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

Here's what it looks like;

```
# ll -d /etc/elasticsearch/scripts
total 24
drwxr-x---  2 root elasticsearch 4096 Dec 15 13:30 scripts/
```

I tried a `chown -R elasticsearch:elasticsearch scripts/` but it didn't help. Neither did a `chmod 755 scripts/`. Note that I did nothing to that directory, I just ran an `apt-get dist-upgrade`.

To fix it I had to delete the directory, recreate it, re-add the permissions and all was good. No idea what happened there...

Anyway the root cause of this being a permissions error would be easier to diagnose if there were something in the logs to that affect :)
</description><key id="145087721">17467</key><summary>Improve error reporting for permissions errors on script directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>enhancement</label></labels><created>2016-04-01T05:03:24Z</created><updated>2017-01-20T16:18:19Z</updated><resolved>2016-04-04T19:30:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-01T15:58:40Z" id="204449282">When you say you "ran it manually" you mean as your user, not the elasticsearch user? Other has no permissions, so it cannot read the directory.
</comment><comment author="clintongormley" created="2016-04-04T19:30:28Z" id="205459808">You didn't mention which version of ubuntu you're using, but i'm guessing that you're on a more recent version, which uses systemd, in which case you'd find logs using journalctl (which is now documented).
</comment><comment author="jasontedor" created="2016-04-05T00:33:25Z" id="205560154">The root cause indicated a [permissions issue](https://docs.oracle.com/javase/7/docs/api/java/nio/file/AccessDeniedException.html):

&gt; `Likely root cause: java.nio.file.AccessDeniedException: /etc/elasticsearch/scripts`
</comment><comment author="glmrenard" created="2017-01-20T16:18:19Z" id="274112683">Hello, I also encounteer this issue, but the folder does not even exist ...
Any help appreciated.

Thanks :)</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPEs from highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17466</link><project id="" key="" /><description>Relates to #17085
</description><key id="145041983">17466</key><summary>Remove PROTOTYPEs from highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T23:11:18Z</created><updated>2016-04-02T01:48:35Z</updated><resolved>2016-04-02T01:48:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T23:11:27Z" id="204166574">Should be a simple one.
</comment><comment author="nik9000" created="2016-03-31T23:14:11Z" id="204167226">I spoke too soon. Looks like this breaks TopHits....
</comment><comment author="nik9000" created="2016-04-01T12:43:44Z" id="204384198">I believe I just fixed TopHits.
</comment><comment author="javanna" created="2016-04-01T18:12:20Z" id="204499205">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hunspell Location setting broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17465</link><project id="" key="" /><description>**Elasticsearch version**:
elasticsearch-2.2.0-1.noarch

**JVM version**:
java version "1.8.0_66"
Java(TM) SE Runtime Environment (build 1.8.0_66-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.66-b17, mixed mode)

**OS version**:
CentOS 6.7

**Description of the problem including expected versus actual behavior**:
According to https://www.elastic.co/guide/en/elasticsearch/guide/current/hunspell.html#_installing_a_dictionary I should be able to set `indices.analysis.hunspell.dictionary.location` to wherever I want.  Unfortunately, the code says no: 
https://github.com/elastic/elasticsearch/blob/v2.2.2/core/src/main/java/org/elasticsearch/indices/analysis/HunspellService.java#L115-L121

The logic there is if OLD_HUNSPELL_LOCATION is set, cause an error - and OLD_HUNSPELL_LOCATION is set by the value of   `indices.analysis.hunspell.dictionary.location`.

The result of this is that if you use the setting in the docs, the service refuses to start.

**Steps to reproduce**:
1. Add hunspell directories to a directory outside of "config/" (in my case, /etc/elasticsearch/hunspell instead of /etc/elasticsearch/01/hunspell)
2. Set  `indices.analysis.hunspell.dictionary.location` to /etc/elasticsearch/hunspell
3. Attempt to restart the process

**Provide logs (if relevant)**:

```
[2016-03-31 15:10:43,540][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalArgumentException: please, put your hunspell dictionaries under config/hunspell !
    at org.elasticsearch.indices.analysis.HunspellService.resolveHunspellDirectory(HunspellService.java:118)
    at org.elasticsearch.indices.analysis.HunspellService.&lt;init&gt;(HunspellService.java:89)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:200)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</description><key id="145039207">17465</key><summary>Hunspell Location setting broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AshtonDavis</reporter><labels /><created>2016-03-31T22:52:06Z</created><updated>2016-04-06T10:31:31Z</updated><resolved>2016-04-04T19:14:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T19:14:40Z" id="205452945">Hi @ntent-ashton 

I'll move this issue to the def-guide repo.
</comment><comment author="clintongormley" created="2016-04-04T19:14:45Z" id="205452971">This issue was moved to elastic/elasticsearch-definitive-guide#495
</comment><comment author="AshtonDavis" created="2016-04-04T20:07:33Z" id="205472710">So this is a doc issue, not something that can be fixed in the code?
</comment><comment author="clintongormley" created="2016-04-06T10:31:31Z" id="206296684">Correct.  We have locked down the disk locations that Elasticsearch is allowed to access for security reasons
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose whether a task is cancellable in the _tasks list API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17464</link><project id="" key="" /><description>Closes #17369
</description><key id="145037890">17464</key><summary>Expose whether a task is cancellable in the _tasks list API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-31T22:43:50Z</created><updated>2016-04-06T02:52:43Z</updated><resolved>2016-04-06T02:52:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-31T23:13:24Z" id="204167103">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal: Remove AggregationStreams</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17463</link><project id="" key="" /><description>The functionality of AggregationStreams can be replaced
NamedWriteableRegistry. This proposes a change that eliminates most of
the class and replaces it with NamedWriteableRegistry while making minimal
changes the rest of aggregations. To really benefit from the cut over this
would have to be the first of many clean ups.
</description><key id="145036202">17463</key><summary>Proposal: Remove AggregationStreams</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>discuss</label><label>stalled</label></labels><created>2016-03-31T22:33:41Z</created><updated>2016-06-29T13:21:44Z</updated><resolved>2016-06-29T12:28:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-01T09:31:07Z" id="204327839">I like the idea, what do you think @colings86 ?
</comment><comment author="colings86" created="2016-04-04T10:26:04Z" id="205232143">I also like the idea. In theory I think this should not break compatibility on the wire protocol but I am a little worried we might do it without realising here. I'm not sure if we want to try to rush this in for 5.0 since it's a fairly big change at a late stage or wait for 6.0 but am open to others opinions
</comment><comment author="nik9000" created="2016-04-11T15:05:59Z" id="208390501">Stalling this until after the 6.0 branch cut. We'd prefer to stabilize 5.0 and this wouldn't help.
</comment><comment author="colings86" created="2016-06-24T09:28:33Z" id="228300653">Discussed this in FixItFriday and we agreed that we could push this to 5.0 if it is ready but it's also ok if its pushed to a later version.
</comment><comment author="nik9000" created="2016-06-24T13:32:05Z" id="228346141">I'll put reworking this on my queue then. I imagine I'll close this in favor of something that lets us do this incrementally so the review is crazy huge.
</comment><comment author="nik9000" created="2016-06-27T15:42:55Z" id="228785288">OK! I've picked this up again in a less-one-huge-PR fashion at #19097. So if you care about this feature look there instead.
</comment><comment author="clintongormley" created="2016-06-29T09:56:40Z" id="229311869">@nik9000 does that mean that this PR can be closed?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge Size and Count Inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17462</link><project id="" key="" /><description>**Elasticsearch version**:
1.7.5

**JVM version**:
1.8.0_40-64

**OS version**:
centos6

**Description of the problem including expected versus actual behavior**:
The current merge count is different between _cat/nodes and _stats/merges.  We show multiple merges being performed on the _cat/nodes api, however, _stats/merges shows no merges being performed for any index.  We are not sure what is the true merge activity on the cluster.  Also, is there a way to see what shards are bing merged?

![cat-nodes](https://cloud.githubusercontent.com/assets/6633418/14191693/f8057626-f75e-11e5-9a62-10c0c76de164.png)
![stats-merges](https://cloud.githubusercontent.com/assets/6633418/14191712/0a9e1cf2-f75f-11e5-85f9-27187f099f09.png)

**Steps to reproduce**:
 We are not sure how the cluster gets into this state.  We have experienced this multiple times.  We have not found a way to clear what we think are stuck merges without bouncing the nodes.

**Provide logs (if relevant)**:
</description><key id="145026448">17462</key><summary>Merge Size and Count Inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">texlnghorn</reporter><labels><label>:Stats</label><label>discuss</label></labels><created>2016-03-31T21:42:09Z</created><updated>2016-06-03T11:20:19Z</updated><resolved>2016-06-03T11:20:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T19:10:04Z" id="205451605">Hi @texlnghorn 

Are you saying that the merges reported by cat-nodes never change? While those from nodes-stats do? The problem is that 1.7.5 is really quite old now.  We're working towards releasing 5.0, and so much has changed since 1.7.5  We'd be interested if you were able to replicate this on a recent version.
</comment><comment author="texlnghorn" created="2016-04-04T19:52:53Z" id="205467793">1.7.5 was released 2/2/2016.  But I know that you are saying that the 1.7.x version is old.  Yes, it appears that merges are stuck querying cat-nodes but not stuck querying nodes-stats.  We have seen the same number of docs and same size reported in cat-nodes for more than 2 weeks.  If we bounce the node, then the merges info goes back to 0.  Eventually we see the same situation again after a few days.  Unfortunately we cannot upgrade to 2.x or 5.x for some time.  I had not seen anyone else report this and was wondering if others had experienced the same thing.   We don't know if we really have stuck merges or if something is not getting reset properly in cat-nodes.
</comment><comment author="clintongormley" created="2016-04-06T10:30:50Z" id="206296451">It sounds very much like an issue with stats reporting, but curious that it happens in cat-nodes and not in nodes-stats. I haven't heard other reports of this but that doesn't mean that it doesn't happen or that it has already been fixed.  Worth investigating I think
</comment><comment author="mkelkarbv" created="2016-05-31T16:46:36Z" id="222748460">We see the same issue on version 1.7.3. the _stats/merges reports no current merges, where as _cat/nodes API reports 3 on going merges on different nodes. 
</comment><comment author="s1monw" created="2016-05-31T18:16:02Z" id="222773919">@mkelkarbv can you provide a sample output what doesn't match? I can't see any example like a json snippet and a CAT line that is wrong.. hard to tell what's going on?!
</comment><comment author="thebearmayor" created="2016-05-31T20:27:07Z" id="222809617">@s1monw I can provide that.

```
# curl -s 'localhost:9200/_cat/nodes?v&amp;h=id,mc,mcd'
id   mc     mcd
VXvB  0       0
lnIR  0       0
_oF8  0       0
bfHD  0       0
sv91  2 1265846
EtuE  0       0
uPsj  0       0
TSPN  0       0
faYE  0       0
_K5Y  0       0
nz9W  0       0
piz6  1  347959
R6jG  0       0
QP9S  0       0
kG31  0       0
rncC  0       0
0WSl  0       0
-mEf  0       0
1zgM  0       0
```

```
curl -s 'localhost:9200/_cat/indices?v&amp;h=i,mc,mcd'
i                      mc mcd
&amp;part-reviewpartition7  0   0
&amp;part-reviewpartition2  0   0
&amp;part-reviewpartition6  0   0
&amp;part-reviewpartition5  0   0
&amp;part-reviewpartition8  0   0
&amp;part-reviewpartition4  0   0
&amp;part-reviewpartition9  0   0
percolators             0   0
&amp;part-reviewpartition0  0   0
&amp;part-reviewpartition3  0   0
&amp;part-reviewpartition1  0   0
```

These were taken at the same time. The 3 merges that show when we query _cat/nodes never go away. The info from _cat/indices seems to be correct -- merges come and go but currently there are none.
</comment><comment author="s1monw" created="2016-06-03T11:20:19Z" id="223554225">this is fixed by #13801 in 2.0 upwards.... The node stats include the stats of all shards ever on that node even the old ones. Yet, it was also counting the current stats not just totals. this is now fixed and this is essentially a duplicate of  #13386  

the stats in `curl -s 'localhost:9200/_cat/indices?v&amp;h=i,mc,mcd'` are accurate in 1.x 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Native script invoke file script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17461</link><project id="" key="" /><description>From: https://github.com/elastic/elasticsearch/issues/17448. It's impossible to retrieve native from plugin because of security problem.

Can I invoke groovy script from native ?

```
@Inject
public Factory(Settings settings, GroovyScriptEngineService service) {
    super(settings);
    this.service = service;
}
```

then in constructor : 

```
    @Override
    public Object run() {
        GroovyScriptEngineService abc = TestScript.Factory.service;
        CompiledScript combined = new CompiledScript(ScriptType.FILE, "init", "groovy", abc.compile("init"));
        logger.info(combined.getClass().toString());
        return abc.execute(combined, params);
    }
```

Result:

&gt; "reason": {
&gt;             "type": "script_exception",
&gt;             "reason": "failed to execute file script [init] using lang [groovy]",
&gt;             "caused_by": {
&gt;             "type": "missing_property_exception",
&gt;             "reason": "No such property: init for class: fd62812fbd9ec4c7f99aa4f6253fead2388eb238"
&gt;    }
&gt; }
</description><key id="145019238">17461</key><summary>Native script invoke file script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haingo-kenshjn</reporter><labels /><created>2016-03-31T21:09:04Z</created><updated>2016-04-01T17:11:50Z</updated><resolved>2016-04-01T17:11:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-01T14:45:03Z" id="204423376">some more explanation [here](https://discuss.elastic.co/t/elasticsearch-scripting-invoke-custom-script-from-plugin/45835/6)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DiskThresholdDecider adds shard size even if shard is on different filesystem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17460</link><project id="" key="" /><description>If the `DiskThresholdDecider` is enabled, it will attempt to see how much disk
space is used _after_ a shard has been allocated to a node (to know whether it
is over the high watermark).

However, in the case that a shard is using a custom data_path setting, the
shard's size _may_ not affect the amount of disk on the node's configured
`path.data`.

This can lead to weird things like shadow replicas not being relocated within a
cluster because all nodes think they don't have enough space for them, even
though they do because it's a different filesystem entirely.
</description><key id="145015685">17460</key><summary>DiskThresholdDecider adds shard size even if shard is on different filesystem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2016-03-31T20:54:23Z</created><updated>2016-04-07T17:44:26Z</updated><resolved>2016-04-07T17:44:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch proxy error when setting script.disable_dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17459</link><project id="" key="" /><description>**Elasticsearch version**: 2.3.0
**JVM version**: 1.8 latest
**OS version**: Windows 10 64bit

I'm upgrading my app from 1.7.5 and my elasticsearch.yml file had `script.disable_dynamic: false` set. When trying to launch with this setting I get the following error:

``` sh
C:\elasticsearch.2.3\bin&gt;elasticsearch
[2016-03-31 15:12:42,319][INFO ][node                     ] [ex-dev01] version[2.3.0], pid[6016], build[8371be8/2016-03-29T07:54:48Z]
[2016-03-31 15:12:42,324][INFO ][node                     ] [ex-dev01] initializing ...
[2016-03-31 15:12:42,801][INFO ][plugins                  ] [ex-dev01] modules [reindex, lang-expression, lang-groovy], plugins [license, marvel-agent, cloud-azure], sites []
[2016-03-31 15:12:42,824][INFO ][env                      ] [ex-dev01] using [1] data paths, mounts [[(C:)]], net usable_space [175.6gb], net total_space [255.6gb], spins? [unknown], types [NTFS]
[2016-03-31 15:12:42,830][INFO ][env                      ] [ex-dev01] heap size [910.5mb], compressed ordinary object pointers [true]
Exception in thread "main" java.lang.IllegalStateException: This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at com.sun.proxy.$Proxy13.settings(Unknown Source)
        at org.elasticsearch.client.FilterClient.&lt;init&gt;(FilterClient.java:41)
        at org.elasticsearch.marvel.shield.SecuredClient.&lt;init&gt;(SecuredClient.java:38)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:213)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:140)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)

```

This should be a better error message!
</description><key id="145005781">17459</key><summary>Elasticsearch proxy error when setting script.disable_dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niemyjski</reporter><labels /><created>2016-03-31T20:15:34Z</created><updated>2016-04-04T18:59:22Z</updated><resolved>2016-04-04T18:59:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="niemyjski" created="2016-03-31T20:18:28Z" id="204107553">I'm trying to find what this setting has been changed to. Ideas?
</comment><comment author="s1monw" created="2016-03-31T20:22:09Z" id="204108578">maybe this helps https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html#enable-dynamic-scripting
</comment><comment author="s1monw" created="2016-03-31T20:22:47Z" id="204109032">btw. upcoming 5.0 version has settigns validation it will tell you which setting is not valid etc.
</comment><comment author="clintongormley" created="2016-04-04T18:59:22Z" id="205448250">Yeah, this setting was changed quite a while ago, and as Simon says, the experience in 5.0 will be a whole lot better.  Nothing further to do here, so closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rework a query parser and improve registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17458</link><project id="" key="" /><description>By default queries now register using both their snake_case and camelCase
name. We no longer need to delay construction of the parser map so we can
throw out QueryRegistration and no longer need Suppliers.
</description><key id="144997758">17458</key><summary>Rework a query parser and improve registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-31T19:39:54Z</created><updated>2016-04-06T16:40:50Z</updated><resolved>2016-04-06T16:40:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T19:41:14Z" id="204095087">Another one for @javanna and @cbuescher. Two more queries down out of about 45.
</comment><comment author="cbuescher" created="2016-04-01T11:17:18Z" id="204351715">@nik9000 I left a small comment about a missing name constant and some general thought about whether we should keep some of the query identifiers in their respective classes for discussion.
</comment><comment author="nik9000" created="2016-04-01T16:12:24Z" id="204455238">@cbuescher, @javanna, @colings86 and I had a video chat about this and talked about this and we were uncomfortable with the way in which we supported query alternative names and how we just relied on the first name being the getWriteableName by convention.

I added a few patches to address those concerns. Hopefully this is better now!
</comment><comment author="javanna" created="2016-04-01T18:36:40Z" id="204507718">thanks a lot Nik I left my random thoughts, let's see what Colin and Christoph have to say ;)
</comment><comment author="javanna" created="2016-04-05T14:29:55Z" id="205833786">@nik9000 I think after recent changes this should easily become a simple MultiMatchQueryParser rework. regarding the test for alternate query names, I think that what we have in SearchModuleTests is good enough. ping me when you have rebased and I will review.
</comment><comment author="nik9000" created="2016-04-05T19:18:51Z" id="205955694">Rebased. Since #17507 incorporated the good parts of this the rebase left this just MultiMatchQueryParser rework, a few useful line wraps, and fixing a test.

Pinging @javanna as requested.
</comment><comment author="javanna" created="2016-04-06T08:17:22Z" id="206211388">left a tiny comment, LGTM though, no need for another review 
</comment><comment author="nik9000" created="2016-04-06T16:40:50Z" id="206457174">Closed by 1afc9b7e5632c097affec2745bf58fe267291729.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make TranslogConfig immutable and pass TranslogGeneration as a ctor arg to Translog</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17457</link><project id="" key="" /><description>This mutable state is confusing and is easily missed. By default this is null and
wipes all translog. This commit makes the TranslogGeneration mandatory on the Translog
constructor and removes the mutalbe state.
</description><key id="144994986">17457</key><summary>Make TranslogConfig immutable and pass TranslogGeneration as a ctor arg to Translog</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T19:28:27Z</created><updated>2016-04-04T19:20:35Z</updated><resolved>2016-04-01T10:18:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-03-31T21:19:49Z" id="204134148">LGTM
</comment><comment author="bleskes" created="2016-04-01T10:18:14Z" id="204340140">Great. Merging..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Chrome/Safari errors in Kibana 4.5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17456</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3.0

**JVM version**: 1.7.0_95

**OS version**: 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u4 (2016-02-29) x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
This issue appeared after upgrading from 4.3.2 to 4.5.0 for me.  I'm running debian 64bit with Elasticsearch 2.3.

UI does not load in Chrome or Safari.  It does in Firefox.

**Steps to reproduce**:
 1.Launch Browser
 2.Open Kibana

**Provide logs (if relevant)**:
`Oops!
Looks like something went wrong. Refreshing may do the trick.

Go back  or clear your session

 Fatal Error
Courier Fetch Error: unhandled courier request error: Authorization Exception
Version: 4.5.0
Build: 9889
Error: unhandled courier request error: Authorization Exception
    at handleError (http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78902:23)
    at DocRequest.AbstractReqProvider.AbstractReq.handleFailure (http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78822:15)
    at http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78716:18
    at Array.forEach (native)
    at http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78714:19
    at processQueue (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:42357:29)
    at http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:42373:28
    at Scope.$eval (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:43601:29)
    at Scope.$digest (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:43412:32)
    at Scope.$apply (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:43709:25)
 Fatal Error
Courier Fetch: unhandled courier request error: Authorization Exception
Version: 4.5.0
Build: 9889
Error: unhandled courier request error: Authorization Exception
    at handleError (http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78902:23)
    at DocRequest.AbstractReqProvider.AbstractReq.handleFailure (http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78822:15)
    at http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78716:18
    at Array.forEach (native)
    at http://elkarti.blank.com/bundles/kibana.bundle.js?v=9889:78714:19
    at processQueue (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:42357:29)
    at http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:42373:28
    at Scope.$eval (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:43601:29)
    at Scope.$digest (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:43412:32)
    at Scope.$apply (http://elkarti.blank.com/bundles/commons.bundle.js?v=9889:43709:25)
`

Request and response codes:
`POST /elasticsearch/_mget?timeout=0&amp;ignore_unavailable=true&amp;preference=1459445925363 HTTP/1.1
Host: elkarti.blank.com.ca:5601
Connection: keep-alive
Content-Length: 62
Accept: application/json, text/plain, _/_
Origin: http://elkarti.blank.com.ca:5601
kbn-version: 4.5.0
User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36
Content-Type: application/json;charset=UTF-8
Referer: http://elkarti.blank.com.ca:5601/app/kibana
Accept-Encoding: gzip, deflate
Accept-Language: en-US,en;q=0.8,da;q=0.6

HTTP/1.1 403 Forbidden, kbn-name: kibana, kbn-version: 4.5.0
cache-control: no-cache
Date: Thu, 31 Mar 2016 17:38:39 GMT
Connection: keep-alive
Transfer-Encoding: chunked`

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="144981125">17456</key><summary>Chrome/Safari errors in Kibana 4.5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jtryon</reporter><labels /><created>2016-03-31T18:29:34Z</created><updated>2016-12-05T10:24:06Z</updated><resolved>2016-03-31T18:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-31T18:31:41Z" id="204067346">Sorry that you're having an issue, but this would be better served if you opened this in the [Kibana repository](https://github.com/elastic/kibana).
</comment><comment author="jtryon" created="2016-03-31T18:31:47Z" id="204067374">Oops, wrong github
</comment><comment author="DuncanMacWeb" created="2016-12-05T10:24:06Z" id="264817703">For info: the corresponding issue in the Kibana repo is [elastic/kibana#6719](https://github.com/elastic/kibana/issues/6719).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>yum install command for older 2.x releases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17455</link><project id="" key="" /><description>All of the 2.x releases (all the way back to 2.0) have the same step for installing ES using yum:

https://www.elastic.co/guide/en/elasticsearch/reference/2.2/setup-repositories.html#_yum_dnf
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/setup-repositories.html#_yum_dnf

&gt; [elasticsearch-2.x]
&gt; name=Elasticsearch repository for 2.x packages
&gt; baseurl=http://packages.elastic.co/elasticsearch/2.x/centos
&gt; gpgcheck=1
&gt; gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
&gt; enabled=1
&gt; 
&gt; And your repository is ready for use. You can install it with:
&gt; yum install elasticsearch

But to install an older release, the yum command should be different, eg.

yum install elasticsearch-2.2.0
yum install elasticsearch-2.1.2

Maybe we can change the documentation to say something like:
- To install the latest 2.x version, use `yum install elasticsearch`.
- To install a specific (older) 2.x version, use `yum install elasticsearch-&lt;version&gt;`, eg. yum install elasticsearch-2.2.0
- You can use `yum list elasticsearch --showduplicates` to retrieve a list of available versions for the elasticsearch repo.
</description><key id="144969533">17455</key><summary>yum install command for older 2.x releases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Packaging</label><label>docs</label></labels><created>2016-03-31T17:42:35Z</created><updated>2016-03-31T17:42:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove MathUtils.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17454</link><project id="" key="" /><description>It has a single method: mod, which can be replaced with Math.floorMod since
we always coll it with a positive divisor.
</description><key id="144963992">17454</key><summary>Remove MathUtils.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T17:24:02Z</created><updated>2016-04-01T09:25:15Z</updated><resolved>2016-04-01T09:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-31T17:33:46Z" id="204042228">LGTM.
</comment><comment author="jpountz" created="2016-04-01T09:25:15Z" id="204326623">Merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add randomization of XContentBuilder output to query tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17453</link><project id="" key="" /><description>Currently our testing of parsing query builders is limited to the default order of the parameters that each builders toXContent() method produces. To better test real queries where the order of parameters can be different, this change adds a helper method to ESTestCase that takes a XContentBuilder and randomly shuffles the order of the fields inside an object. This is now used in AbstractQueryTestCase, but it can be used in other similar places in the future.
</description><key id="144962065">17453</key><summary>Add randomization of XContentBuilder output to query tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T17:17:50Z</created><updated>2016-04-05T11:06:04Z</updated><resolved>2016-04-01T13:02:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-01T08:28:28Z" id="204306764">@nik9000 @javanna thanks for the suggestions, I incorporated them into this PR.
</comment><comment author="cbuescher" created="2016-04-01T11:29:07Z" id="204355590">@martijnvg @javanna improved the comment you mentioned. Can you take another look and let me know if theres anything left to add?
</comment><comment author="javanna" created="2016-04-01T12:03:34Z" id="204368991">LGTM @cbuescher . I am just not sure it should close #5831 as it shuffles things only for queries, while the original issue was more generic. Not sure it is doable though more generic than this and indeed it would be nice to close that old issue :)
</comment><comment author="cbuescher" created="2016-04-01T12:27:45Z" id="204379343">@javanna thanks, I removed closing #5831, but then we should revisit that issue and define more clearly what is required to close it. I'm planning to use this for the other parts of xContent in the search source, maybe we can close it when we added this kind of randomization to aggregations, sorts, suggesters, shapes etc. (to be defined in 5831)? Anyway, will merge this then.
</comment><comment author="martijnvg" created="2016-04-01T12:29:25Z" id="204379723">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kstem filter doesn't work for "dogs" word</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17452</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.2

**JVM version**: 1.8.0_45

**OS version**: Windows 7 SP1 64 bit

**Description of the problem including expected versus actual behavior**:
kstem filter doesn't work for "dogs" word.

**Steps to reproduce**:
1) Create an index with custom analyzer with kstem filter

```
curl -XPUT "http://localhost:9200/test" -d'
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "type": "custom",
                    "tokenizer": "standard",
                    "filter": [ "kstem" ]
                }
            }
        }
    }
}'
```

2) Test the analyzer
`curl -XGET "http://localhost:9200/test/_analyze?analyzer=my_analyzer&amp;pretty=true" -d"dogs" | grep "token"`

3) The result token is still "dogs" rather than "dog". But if I try with another words like "birds", "cats", it works fine.

So what's wrong with the dogs?
</description><key id="144956058">17452</key><summary>Kstem filter doesn't work for "dogs" word</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s101d1</reporter><labels /><created>2016-03-31T16:52:42Z</created><updated>2016-04-01T08:48:42Z</updated><resolved>2016-04-01T08:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-04-01T08:48:26Z" id="204311631">Unfortunately Stemmers never work perfect, KStem tends to be less agressive than e.g. Porter Stemmer, so if you really need "dogs" stemmed, you could try that one. I just quickly dug this up for comparison. https://mail-archives.apache.org/mod_mbox/lucene-java-user/201211.mbox/%3C96A3937D727B4011B77FC7C6B6FAB829@JackKrupansky%3E
Also, please ask very use-specific questions like this in the form at https://discuss.elastic.co/ in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simple Get API requests sometimes slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17451</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

2.3.0

**JVM version**:

Java 8 Update 77

**OS version**:

Debian Jessie

**Description of the problem**:

I have intermittent but reproduceable slow requests when I get a record via /index/type/id and if there is a "larger" substructure (array of objects with multiple different fields and types) in the _source result. 1700 bytes (in JSON notation) is already enough to slow down a steady number of requests to above 100ms instead of the usual 1-2ms per request.

These slow requests disappear in my reproducible tests if I `_source_exclude` the substructure in my request, but they still appear randomly on the site I am using ES on (a simple ES GET then takes 200ms instead of 1-2ms). The slow requests also disappear from my reproducible tests when I start flooding ES with these simple GET requests and do additional requests in parallel. I have had slow requests of this kind since ES 1.4 and now still in ES 2.3.

I have described my experiences in detail on http://stackoverflow.com/questions/36272601/find-causes-for-slower-elasticsearch-responses - the server is idle with plenty of resources, which is why this vexes me so much. Either this is a bug, or something very strange is going on.

As far as I can tell the number of slow requests increases the larger the substructures are in the stored object, but only if they are not excluded in the request. Maybe it also has to do something with `"index": "no"`, because all the data which causes the problems is not indexed or stored, it is only in the _source.
</description><key id="144948641">17451</key><summary>Simple Get API requests sometimes slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iquito</reporter><labels /><created>2016-03-31T16:22:40Z</created><updated>2016-04-04T22:18:16Z</updated><resolved>2016-04-04T18:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T18:19:22Z" id="205430991">Hi @iquito 

Requests for any stored fields (including `_id`, `_source` etc) need to decompress at least one 16kB block to extract the desired fields.  As you say, after the first request (or second request, depending on how many shards you have) the stored fields will be in the file system cache and should be fast to access. The fact that you're seeing such variability points to either (a) issues with GC or (b) hardware issues, where something else is interfering with the request.

Either way, this doesn't sound like a bug in Elasticsearch.  The better forum to talk about issues like this is http://discuss.elastic.co/ - there you'll find people who have had to deal with similar problems.
</comment><comment author="iquito" created="2016-04-04T22:18:16Z" id="205520151">Thanks, I created a discussion there: https://discuss.elastic.co/t/get-api-requests-sometimes-slow-up-to-200ms-instead-of-2ms/46301

The symptoms I am experiencing are just so over the place and only concern ES (and I switched all servers around just to try it out - so the server hardware should not be a factor) that I do not know what else to try. I can even reproduce a version of these slow responses, but I do not know what else I can test to find the root cause. Nothing in the Elasticsearch documentation has any hint on how to "debug" something like this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch : Cannot create a client node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17450</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8

**OS version**: windows 7

**Description of the problem including expected versus actual behavior**:

I am trying to create a client node for my Elastic cluster, which has elasticsearch version 2.2.0. I have 4 nodes in my cluster and each of them can be data or master nodes. Now, i want to create a client node that will ping one of my other 4 nodes. My intention is that even if one of my 4 main nodes goes down, my client will be able to get data from some node that is functional.

Here is my elasticsearch.yml for client

```
cluster.name: myCluster
node.name: myClient
node.master: false
node.data: false
discovery.zen.ping.unicast.hosts: ["box1:9300","box2:9300","box3:9300","box4:9300"]
discovery.zen.fd.ping_timeout: 30s
discovery.zen.minimum_master_nodes: 3
discovery.zen.ping.multicast.enabled: false
```

each of the above boxes are configured as

```
cluster.name: myCluster
node.name: n2
path.data: /asd/thth/jtjut/elastic
path.logs: /var/log/eslogs
network.host: box2
node.master: true
node.data: true
index.number_of_replicas: 1
discovery.zen.ping.unicast.hosts: ["box1:9300","box2:9300","box3:9300","box4:9300"]
discovery.zen.fd.ping_timeout: 30s
discovery.zen.minimum_master_nodes: 3
discovery.zen.ping.multicast.enabled: false

action.disable_delete_all_indices: true

http.cors.enabled: true
http.cors.allow-origin: "*"
```

but when i start the elasticsearch service on the client, i get

```
[2016-03-31 10:24:07,576][INFO ][discovery.zen            ] [myClient] failed
to send join request to master [{n2}{5vlJHDZaQhGttqZsD5gVvA}{34.87.98.213}{34.87.98.213:9300}{master=true}], reason [RemoteTransportException[[n2] 34.87.98.213:9300][internal:discovery/zen/join]]; nested:ConnectTransportException[[myClient][127.0.0.1:9300] connect_timeout[30s]]; nested: NotSerializableExceptionWrapper[Connection refused: /127.0.0.1:9300]; ]
```

why am i getting connection refused exception? is there anything missing from my configurations?

**Steps to reproduce**:
1. Install elasticsearch on client machine
2. Update elasticsearch.yml to create client node
3. Run elasticsearch on client machine
</description><key id="144921767">17450</key><summary>Elasticsearch : Cannot create a client node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2016-03-31T14:42:15Z</created><updated>2016-04-04T18:09:59Z</updated><resolved>2016-04-04T18:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-04-04T18:09:59Z" id="205425891">Your client node needs `network.host` configured as well.  At the moment, it is bound to localhost.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests and docs for scroll_size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17449</link><project id="" key="" /><description>Closes #17381
</description><key id="144911705">17449</key><summary>Tests and docs for scroll_size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T14:04:30Z</created><updated>2016-04-04T18:20:59Z</updated><resolved>2016-03-31T16:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T14:04:49Z" id="203954599">I suspect the test wouldn't pass against 2.3 but it passes in master.
</comment><comment author="dakrone" created="2016-03-31T15:02:17Z" id="203977337">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0 scripting: Invoke custom script from plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17448</link><project id="" key="" /><description>If we defined native script in plugin: com.test.nativescript.TestScript, can we invoke that package inside another groovy script and create new instance ?

Ex: 
inside config/scripts/test.groovy:

```
import  com.test.nativescript.TestScript;

def testScript = new TestScript();
```

&gt; Result: "startup failed:\r\nc78a82bc96ca7751e83e256b4ee0c322cda13bea: 1: unable to resolve com.test.nativescript.TestScript"
</description><key id="144895661">17448</key><summary>Elasticsearch 2.0 scripting: Invoke custom script from plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haingo-kenshjn</reporter><labels /><created>2016-03-31T13:05:31Z</created><updated>2016-03-31T18:58:39Z</updated><resolved>2016-03-31T18:58:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T18:58:39Z" id="204078727">Hi @haingo-kenshjn 

No, you can't, and allowing this would open up a security hole.  Scripts are meant to be relatively simple.  Anything more and you should consider writing a native script in java.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex from remote cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17447</link><project id="" key="" /><description>The reindex API should be able to pull data from an index in a remote cluster.  This would allow importing an index in a 1.x cluster into a cluster running Elasticsearch 5 or later.

The syntax could look like this:

```
POST /_reindex
{
  "source": {
    "remote": {
      "host": "192.168.1.2:9200",
      "username": "foo",
      "password": "bar"
    },
    "index": "twitter",
    "query": {
      "match": {
        "user": "kimchy"
      }
    }
  },
  "dest": {
    "index": "new_twitter"
  }
}
```

The `username` and `password` and any other options within `remote` would be used to configure the HTTP client contacting the remote server, and the `query` etc would be passed to the remote server without parsing (which deals with any API changes).
</description><key id="144895215">17447</key><summary>Reindex from remote cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Reindex API</label><label>feature</label><label>release highlight</label></labels><created>2016-03-31T13:04:02Z</created><updated>2017-06-10T03:04:21Z</updated><resolved>2016-07-05T20:15:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T13:20:54Z" id="203933563">I like the syntax. I think we'll need the http based java API for it though.
</comment><comment author="clintongormley" created="2016-03-31T19:00:34Z" id="204079516">Not necessarily.  A simple HTTP client should be sufficient for this.  
</comment><comment author="ayushsangani" created="2016-05-04T20:00:17Z" id="216984153">+1 @nik9000  eagerly waiting for this to be released in 1.x
</comment><comment author="nik9000" created="2016-05-04T20:04:18Z" id="216985166">&gt; +1 @nik9000 eagerly waiting for this to be released in 1.x

It'll be a 5.0 or 5.1 feature depending on when I get time to really start it. You should be able to connect to 1.x with it though.
</comment><comment author="git-cambridge" created="2017-06-05T22:48:12Z" id="306332032">Is the _source._ingest feature working to get metadata fields from a remote source? I can't get it to work. Have tried posting in forum, no response.</comment><comment author="monikamaheshwari" created="2017-06-07T10:23:41Z" id="306754475">Could you tell what would happen to data that is created or updated when performing reindex api?</comment><comment author="nik9000" created="2017-06-10T02:14:53Z" id="307535592">&gt; Could you tell what would happen to data that is created or updated when performing reindex api?

Reindex sees a consistent view of the data. It won't notice the changes.</comment><comment author="monikamaheshwari" created="2017-06-10T02:55:13Z" id="307537422">So how to add that changes.</comment><comment author="nik9000" created="2017-06-10T03:04:21Z" id="307537790">&gt; So how to add that changes.

We don't have native support for that. If you ask around on discuss.elastic.co you might find people that have some solutions.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change upgrade API to reindex-in-place</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17446</link><project id="" key="" /><description>The upgrade API in its current form will not help users to migrate indices created in Elasticsearch 1.x to 5.x.

The upgrade API rewrites an index in the latest Lucene format, but it still retains the original data structures that were used when the index was first created. For instance:
- Doc-values on numeric fields used to use BinaryDocValues, but now use dedicated NumericDocValues.
- The parent-child feature has been completely rewritten to use a new data structure.
- Geo-point fields now require doc values and the Lucene index where, previously, they relied on in-memory calculations.

We should change the upgrade API to reindex-in-place, that is:
- Create `.my_index`using the mapping and settings from `my_index`
- Reindex the data in `my_index` to `.my_index`
- Atomically delete `my_index` and rename `.my_index` to `my_index`

This would work only on a read-only index (at least for now).  It depends on:
- [ ] The ability to rename an index #17426 
- [ ] The ability to retrieve a reindex job's status after completion #16911 
</description><key id="144893027">17446</key><summary>Change upgrade API to reindex-in-place</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Reindex API</label><label>:Upgrade API</label><label>feature</label></labels><created>2016-03-31T12:57:33Z</created><updated>2016-03-31T12:57:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add up-to-date example of cluster stats API output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17445</link><project id="" key="" /><description>While looking up the [cluster stats API](https://www.elastic.co/guide/en/elasticsearch/reference/master/cluster-stats.html) I noticed that example in the docs is very old. With this PR we update the example to the output of the latest master version.
</description><key id="144888359">17445</key><summary>Add up-to-date example of cluster stats API output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Stats</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T12:39:27Z</created><updated>2016-03-31T13:59:22Z</updated><resolved>2016-03-31T12:56:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T12:53:42Z" id="203920623">LGTM
</comment><comment author="danielmitterdorfer" created="2016-03-31T12:55:19Z" id="203921529">Thanks for the quick review!
</comment><comment author="javanna" created="2016-03-31T13:56:15Z" id="203952023">Thanks @danielmitterdorfer I am sure I have forgotten to update the docs after my recent changes to cluster stats.
</comment><comment author="danielmitterdorfer" created="2016-03-31T13:59:22Z" id="203952944">@javanna Oh, I don't think that. The output looked like it was created even before 2.0. But no worries, that's what team work is for. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document move of ClusterHealthStatus in breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17444</link><project id="" key="" /><description>Closes #17346
</description><key id="144868048">17444</key><summary>Document move of ClusterHealthStatus in breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>docs</label><label>v2.2.2</label></labels><created>2016-03-31T11:31:41Z</created><updated>2016-03-31T11:38:55Z</updated><resolved>2016-03-31T11:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-31T11:37:22Z" id="203890783">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix small reindex doc issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17443</link><project id="" key="" /><description /><key id="144861730">17443</key><summary>Fix small reindex doc issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>docs</label><label>review</label></labels><created>2016-03-31T11:00:10Z</created><updated>2016-03-31T12:40:16Z</updated><resolved>2016-03-31T12:40:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T12:36:55Z" id="203911806">LGTM. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexingMemoryController settings are not registered settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17442</link><project id="" key="" /><description>All settings in `IndexingMemoryController` are not registered settings but should be. Missing settings are:
- `indices.memory.min_index_buffer_size`
- `indices.memory.index_buffer_size`
- `indices.memory.max_index_buffer_size`
- `indices.memory.shard_inactive_time`
- `indices.memory.interval`
</description><key id="144851160">17442</key><summary>IndexingMemoryController settings are not registered settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>adoptme</label><label>blocker</label><label>v5.0.0-alpha2</label></labels><created>2016-03-31T10:07:29Z</created><updated>2016-04-15T17:14:12Z</updated><resolved>2016-04-15T17:14:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-03-31T10:09:58Z" id="203863521">Nice catch, I'll tackle this ...
</comment><comment author="s1monw" created="2016-03-31T10:13:58Z" id="203865170">thanks @mikemccand 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove RescoreParseElement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17441</link><project id="" key="" /><description>The refactoring of RescoreBuilder and QueryRescoreBuilder moved parsing previously in RescoreParseElement into the builders. This removes the left over parse element.
</description><key id="144836045">17441</key><summary>Remove RescoreParseElement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T09:10:14Z</created><updated>2016-03-31T12:38:56Z</updated><resolved>2016-03-31T12:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T12:35:08Z" id="203910501">Left a small question. LGTM otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster Health should run on applied states, even if waitFor=0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17440</link><project id="" key="" /><description>We already protect against making decisions based on an inflight cluster state if someone asks for a waitFor rule (like wait for green). We should do the same for normal health checks as well (unless timeout is set to 0) as it be trappy to debug failures when health says the cluster is in a certain state, but that state wasn't applied yet.

Example failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/270/console
</description><key id="144834114">17440</key><summary>Cluster Health should run on applied states, even if waitFor=0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T09:00:53Z</created><updated>2016-04-01T09:19:45Z</updated><resolved>2016-04-01T09:19:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-31T19:16:57Z" id="204086117">LGTM 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed unneeded refresh during post recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17439</link><project id="" key="" /><description>also removed an obsolete exception, `_percolator` type, which is `.percolator` since version 1.0
</description><key id="144824212">17439</key><summary>Removed unneeded refresh during post recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-31T08:22:21Z</created><updated>2016-04-06T20:08:18Z</updated><resolved>2016-04-06T20:08:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T16:38:29Z" id="206456167">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge multiple searches on aliases pointing to the same index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17438</link><project id="" key="" /><description>**Describe the feature**:
Most of my searches are on the last day or two.. so I use daily indexes (for logs - from logstash).

But I get 100 mill. + logs easily per day.. and some have the insane idea that they want to be allowed to search for f.ex. 1 year.. That eats up a lot of memory in index space.

So I'd like to be able to merge older indexes into larger indexes.. with the new reindex api and alias'es - thats easy.

Only issue left, is that kibana will still search on every daily index.. even though they would now be an alias for the same weekly (or monthly) index.. meaning I would get 30 searches run on the same index.. instead of just 1.. I asked on irc and Warkholm stated that ES does not in fact recognize when this happens..

I would like ES to actually detect this fact, when searching on aliases.. so 30 searches are merged into one in this case :)
</description><key id="144823970">17438</key><summary>Merge multiple searches on aliases pointing to the same index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KlavsKlavsen</reporter><labels /><created>2016-03-31T08:21:34Z</created><updated>2016-03-31T18:19:14Z</updated><resolved>2016-03-31T18:19:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T18:19:14Z" id="204063085">Hi @KlavsKlavsen 

&gt; Only issue left, is that kibana will still search on every daily index.. even though they would now be an alias for the same weekly (or monthly) index.. meaning I would get 30 searches run on the same index.. instead of just 1.. I asked on irc and Warkholm stated that ES does not in fact recognize when this happens..

I'm not sure where @markwalkom got that idea... aliases are reduced to a unique list of the corresponding indices

```
PUT t/t/1
{}

PUT t/_alias/x
PUT t/_alias/y

GET x,y/_validate/query?explain
```

returns:

```
"explanations": [
  {
    "index": "t",
    "valid": true,
    "explanation": "*:*"
  }
]
```

It even knows how to combine filtered aliases on the same index correctly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CASE_INSENSITIVE not working in ElasticSearch &gt; 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17437</link><project id="" key="" /><description>**Elasticsearch version**: 2.2 and 2.3 (it was working in ES 1.7)

**JVM version**: openjdk version "1.8.0_77"

**OS version**: centos 7.2

**Description of the problem including expected versus actual behavior**:CASE_INSENSITIVE flags in aggregation is not working

**Steps to reproduce**:

```
curl -XPOST -d '{"mappings":{"type":{"properties":{"name":{"type":"string", "index": "not_analyzed"}}}}}' localhost:9200/toto

curl -XPOST -d '{"name": "g1"}' localhost:9200/toto/type/1
curl -XPOST -d '{"name": "g2"}' localhost:9200/toto/type/2
curl -XPOST -d '{"name": "G3"}' localhost:9200/toto/type/3

curl  -XPOST localhost:9200/toto/_search?pretty -d '{
"size": 0,
  "aggs": {
    "2": {
      "terms": {
        "field": "name",
        "include": {
          "pattern": "g[0-9]",
          "flags": "CASE_INSENSITIVE"
        }
      }
    }
  }
}'
```

```
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "2" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "g1",
        "doc_count" : 1
      }, {
        "key" : "g2",
        "doc_count" : 1
      } ]
    }
  }
}
```

G3 is not displayed
</description><key id="144817572">17437</key><summary>CASE_INSENSITIVE not working in ElasticSearch &gt; 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Filirom1</reporter><labels /><created>2016-03-31T07:50:34Z</created><updated>2016-03-31T09:12:06Z</updated><resolved>2016-03-31T09:12:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-31T09:12:05Z" id="203837772">Thanks @Filirom1. This is documented in the breaking 2.0 aggregation changes here:
https://www.elastic.co/guide/en/elasticsearch/reference/2.3/breaking_20_aggregation_changes.html#_including_excluding_terms
The `flags` parameter is no longer supported but we have some leniency in the aggregation parsers.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>thirdPartyAudit build task spewes gazilion warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17436</link><project id="" key="" /><description>I'm not sure if this is an issue with CI, an issue with `thirdPartyAudit` logic or an actual problem, but we should look at it... 

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/270/consoleFull

```
[thirdPartyAudit] WARNING: The referenced class 'org.apache.commons.logging.Log' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.apache.commons.logging.Log' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.apache.commons.logging.Log' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.jboss.marshalling.ByteInput' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.jboss.marshalling.ByteInput' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.jboss.marshalling.ByteInput' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.jboss.marshalling.ByteInput' cannot be loaded. Please fix the classpath!
[thirdPartyAudit] WARNING: The referenced class 'org.jboss.marshalling.ByteInput' cannot be loaded. Please fix the classpath!
```
</description><key id="144806542">17436</key><summary>thirdPartyAudit build task spewes gazilion warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>build</label><label>v5.1.1</label></labels><created>2016-03-31T06:46:21Z</created><updated>2016-11-10T09:38:01Z</updated><resolved>2016-11-10T09:38:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T12:31:05Z" id="203908627">Is this a new thing or has it been doing this in CI the whole time? We interact with the code that does the third party audit by looking at the logger. Usually we capture the output, filter it, and interpret it.
</comment><comment author="bleskes" created="2016-04-01T08:56:22Z" id="204315191">&gt; Is this a new thing or has it been doing this in CI the whole time?

No idea. I just noticed it now. Maybe other people can chime in..
</comment><comment author="javanna" created="2016-11-10T09:38:01Z" id="259644097">Closed by #21443 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verbose dangling indices logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17435</link><project id="" key="" /><description>While investigating #17319, I had started two nodes built from master, and indexed some data. I thought I killed these nodes but one of them was still running. I started another node and it produced the following in its logs:

```
[2016-03-30 22:51:39,322][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,335][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,375][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,495][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,524][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,543][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,571][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:39,630][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:40,049][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:40,183][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:40,214][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:40,251][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
[2016-03-30 22:51:40,573][WARN ][gateway                  ] [Ani-Mator] [[i/CetR0D_FTdmYi5IcTE55EA]] can not be imported as a dangling index, as index with same name already exists in cluster metadata
```

I tried to reproduce this from a fresh cluster, but it did not immediately reproduce. I restored the shared data directory for this cluster from backup and the issue appeared again so there appears to be something special here.
1. Package the elasticsearch tar from master
2. Extract the tar into some location `/path/to/elasticsearch`
3. Extract the attached [data.tar.gz](https://github.com/elastic/elasticsearch/files/196920/data.tar.gz) into `/path/to/elasticsearch/data`
4. Start Elasticsearch twice from `/path/to/elasticsearch/bin/elasticsearch`
5. :boom: 
</description><key id="144777151">17435</key><summary>Verbose dangling indices logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-03-31T03:13:16Z</created><updated>2017-01-15T14:47:54Z</updated><resolved>2016-04-25T20:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-31T03:18:11Z" id="203733253">I've verified that this happens locally for me as well.  Will investigate.
</comment><comment author="abeyad" created="2016-03-31T16:15:28Z" id="204004924">@jasontedor These dangling indices import warnings are legitimate.  The issue is that in the data directory for node 1, there are 3 indices, two of which have the same name `i`.  One of them is `[i/bDwGVomkT3-CML0uG7o_tg]`, the other is `[i/CetR0D_FTdmYi5IcTE55EA]`.  The question is, how was it possible to get two indices of the same name in the cluster state for node 1 to begin with?
</comment><comment author="jasontedor" created="2016-03-31T16:30:00Z" id="204010403">&gt; The issue is that in the data directory for node 1, there are 3 indices, two of which have the same name `i`.

That's not good.

&gt; The question is, how was it possible to get two indices of the same name in the cluster state for node 1 to begin with?

That is a very good question, this looks bad. :frowning: 
</comment><comment author="abeyad" created="2016-03-31T16:41:21Z" id="204014161">I wonder if the index folder name upgrade that @areek worked on could have contributed?
</comment><comment author="jasontedor" created="2016-03-31T16:48:00Z" id="204016688">&gt; I wonder if the index folder name upgrade that @areek worked on could have contributed?

I'm wondering that too, but I'm not sure how. To be clear, this was a fresh cluster and so not upgraded.
</comment><comment author="abeyad" created="2016-03-31T19:04:27Z" id="204082130">The following scenario reproduces this issue:
1. Start a master node `M` and another node `D`.
2. Create an index named `idx`.
3. Shutdown node `D`.
4. Delete index `idx`.
5. Create again an index named `idx` (it will have a different uuid).
6. Start back up node `D`.

At this point, node `D` will show the log messages above, because it has the original `idx` index on its disk and its trying to import it as a dangling index because its not part of the cluster state.  It also leads to the same data directory as what @jasontedor linked to in this issue, with `M` having one index in its data directory and `D` having two.

This issue will be resolved by PR #17265 
</comment><comment author="celesteking" created="2016-12-07T14:26:27Z" id="265460241">Still happening on 5.0.2.</comment><comment author="Lifang13" created="2017-01-13T23:52:32Z" id="272579181">stilling happening 5.1.1</comment><comment author="jasontedor" created="2017-01-15T03:47:56Z" id="272671611">Showing up and only saying something still happening is not helpful. If you think you are experiencing a bug, please open a new issue with a complete reproduction. If you are unsure or have a question, ask on the [forum](https://discuss.elastic.co). Please note that what was reported here was on a pre-production snapshot of Elasticsearch 5.0.0 before a feature was added to handle situations like this more gracefully.</comment><comment author="abeyad" created="2017-01-15T14:47:54Z" id="272699762">I tried reproducing using the above steps and I could not get the above log messages to appear.  </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPEs from ingest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17434</link><project id="" key="" /><description>Relates to #17085
</description><key id="144765519">17434</key><summary>Remove PROTOTYPEs from ingest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T01:43:31Z</created><updated>2016-04-05T11:05:14Z</updated><resolved>2016-03-31T13:10:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T01:43:40Z" id="203711594">Another smaller one this time @javanna 
</comment><comment author="jpountz" created="2016-03-31T07:38:55Z" id="203800320">LGTM
</comment><comment author="javanna" created="2016-03-31T07:57:20Z" id="203804757">LGTM
</comment><comment author="nik9000" created="2016-03-31T13:10:22Z" id="203930080">Thanks for the reviews @jpountz and @javanna !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from BulkItemResponse.Failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17433</link><project id="" key="" /><description>Closes #17086
</description><key id="144763118">17433</key><summary>Remove PROTOTYPE from BulkItemResponse.Failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-31T01:32:15Z</created><updated>2016-03-31T13:35:44Z</updated><resolved>2016-03-31T13:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T01:32:22Z" id="203708324">@javanna this one should be easier.
</comment><comment author="jpountz" created="2016-03-31T07:39:38Z" id="203800632">LGTM
</comment><comment author="javanna" created="2016-03-31T07:58:22Z" id="203805044">how come @jpountz takes all the easy ones from me? ;)  LGTM too!
</comment><comment author="nik9000" created="2016-03-31T13:35:44Z" id="203939116">Thanks for the reviews @jpountz and @javanna !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include node and request body information in rest.suppressed messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17432</link><project id="" key="" /><description>When we throw these rest.suppressed messages, it will be helpful to indicate:
1. Which node is rejecting the rest request (even if it's the local node that is logging this message, it will be nice to explicitly indicate that as part of the log entry).
2.  The request body.  In this particular case, it is a particular _index referenced in the docs body of the request that requires permission.  So it will be helpful to include the request body as well.

```
[2016-03-29 10:26:41,607][INFO ][rest.suppressed          ] /_mget Params: {ignore_unavailable=true, preference=1459272400934, timeout=0}
ElasticsearchSecurityException[action [indices:data/read/mget] is unauthorized for user [user_name]]
    at org.elasticsearch.shield.support.Exceptions.authorizationError(Exceptions.java:45)
    at org.elasticsearch.shield.authz.InternalAuthorizationService.denialException(InternalAuthorizationService.java:294)
    at org.elasticsearch.shield.authz.InternalAuthorizationService.denial(InternalAuthorizationService.java:268)
```
</description><key id="144756035">17432</key><summary>Include node and request body information in rest.suppressed messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-31T00:32:46Z</created><updated>2016-04-01T01:44:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T17:32:17Z" id="204041030">Hi @ppf2 

I can see why having the request body (or at least the name of the problematic index) would be useful, but not following why adding the node name would be useful, if these requests are always logged locally?  (not sure whether they are or not)
</comment><comment author="ppf2" created="2016-04-01T01:44:34Z" id="204202755">@clintongormley Yah, I just assume that these are logged locally :)  But it will be nice to indicate it with the message somehow esp. in the tribe node case, it makes users wonder if it's complaining about its local request, or when running against one of the downstream clusters, etc..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set MAX_OPEN_FILES to 65536</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17431</link><project id="" key="" /><description>Relates to #17430
</description><key id="144750968">17431</key><summary>Set MAX_OPEN_FILES to 65536</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joehillen</reporter><labels><label>:Packaging</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T23:47:58Z</created><updated>2016-03-31T17:48:48Z</updated><resolved>2016-03-31T17:33:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-31T01:27:35Z" id="203707485">LGTM
</comment><comment author="javanna" created="2016-03-31T17:48:48Z" id="204049008">Should this close #17430 @joehillen ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch init scripts set max open files to 65535, but expects 65536</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17430</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0.0-alpha1 78ab6c5b7ff82da7f7d3c059b4a43d80bad188fb

**JVM version**: `OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)`

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:

Elasticsearch expects max open file descriptors to be set to 65536, but init scripts set it to 65535.

**Steps to reproduce**:
1. Launch elasticsearch using `/etc/init.d/elasticsearch`

**Provide logs (if relevant)**:

```
[2016-03-30 23:12:05,787][ERROR][bootstrap                ] Exception
java.lang.RuntimeException: max file descriptors [65535] for elasticsearch process likely too low, increase to at least [65536]
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:79)
    at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:60)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:264)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:111)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:106)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```
</description><key id="144750663">17430</key><summary>Elasticsearch init scripts set max open files to 65535, but expects 65536</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joehillen</reporter><labels><label>:Packaging</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T23:45:17Z</created><updated>2016-06-06T01:57:34Z</updated><resolved>2016-03-31T17:50:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajaybhatnagar" created="2016-06-02T20:11:52Z" id="223407568">I had the setting in /etc/security/limits.conf with alpha3 release and starting ES still throws the error 
elasticsearch   -       nofile  65536

and init file contained following lines:

# Run Elasticsearch as this user ID and group ID

ES_USER=elasticsearch
ES_GROUP=elasticsearch

# Maximum number of open files

MAX_OPEN_FILES=65536

Error logs:

[2016-06-02 20:05:44,637][ERROR][bootstrap                ] [es-tst-m01] Exception
java.lang.RuntimeException: bootstrap checks failed
max file descriptors [65535] for elasticsearch process likely too low, increase to at least [65536]
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:125)
        at org.elasticsearch.bootstrap.BootstrapCheck.check(BootstrapCheck.java:85)
</comment><comment author="jasontedor" created="2016-06-02T20:19:29Z" id="223409614">@ajaybhatnagar How are you starting Elasticsearch? What are the limits for the root user? Note that the limits for the root user must be at least as high as the elasticsearch user.
</comment><comment author="ajaybhatnagar" created="2016-06-02T20:44:18Z" id="223416395">Root and es user settings for nofile:
root            -       nofile  100000
elasticsearch   -       nofile  65536

ulimit -n output for ES user:
65536

Starting ES with service elasticsearch start
</comment><comment author="jasontedor" created="2016-06-02T22:18:11Z" id="223439534">@ajaybhatnagar I'm going to make some assumptions here, please correct if any of them are wrong.

&gt; Starting ES with service elasticsearch start

I'm assuming that you deliberately left `sudo` off here. From this, I'm assuming that you are thus logged in as the root user. I'm further assuming that you made the above changes to `/etc/security/limits.conf` and did not logout and log back in.

If these assumptions are correct, the solution to the problem is for you to just log out and log back in.
</comment><comment author="ajaybhatnagar" created="2016-06-03T13:42:36Z" id="223581944">Rebooted the node and still the same error. 

Init script has the line below,  yet startup is finding max file descriptors below the configured value.  Hardcoded value picked from somewhere else  or overwritten ?

# Maximum number of open files

MAX_OPEN_FILES=65536
Check on the node:
root@es-tst-m01:/es1/logs/elasticsearch# ulimit -n
100000
root@es-tst-m01:/es1/logs/elasticsearch# su - elasticsearch
elasticsearch@es-tst-m01:~$ ulimit -n
65536
[2016-06-03 13:35:51,324][ERROR][bootstrap                ] [es-tst-m01] Exception
java.lang.RuntimeException: bootstrap checks failed
max file descriptors [65535] for elasticsearch process likely too low, increase to at least [65536]
</comment><comment author="jasontedor" created="2016-06-03T13:56:36Z" id="223585366">You have a configuration error somewhere, it's just a matter of finding where. Can you check `cat /proc/sys/fs/file-max`? If it's too low, execute `sysctl -w fs.file-max=65536` or some other higher value to raise it. Consider putting this value in `/etc/sysctl.conf` so that it persists across system reboots. You will have to logout and log back in if you changed this value.
</comment><comment author="dajoen" created="2016-06-03T14:23:25Z" id="223592578">I ran into the same problem just now. And had to change the nofiles in /usr/lib/systemd/system/elasticsearch.service. After changing this to the same value as fs.file-max the message disappeared.
</comment><comment author="ajaybhatnagar" created="2016-06-03T15:56:59Z" id="223618572">Caused by setting in /etc/deafult/elasticsearch. Closed.
thx
</comment><comment author="X-Mars" created="2016-06-06T01:53:40Z" id="223853226">hi  
i have the same error
but i don't have  /etc/deafult/elasticsearch
can u help me 
thx
</comment><comment author="jasontedor" created="2016-06-06T01:57:00Z" id="223853511">@X-Mars This issue is closed, and was specific to how the defaults that Elasticsearch shipped with were inconsistent with a warning log message that it would produce. Please open a new post on the [Elastic Discourse forum](https://discuss.elastic.co) with details about your setup and debugging steps that you have already gone through.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix major version designator for 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17429</link><project id="" key="" /><description>This commit fixes the major version designator for 2.3. This designator
is used in docs/reference/setup/repositories.asciidoc to specify the URL
for the package repositories.

Closes #17423 
</description><key id="144743790">17429</key><summary>Fix major version designator for 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>docs</label><label>review</label></labels><created>2016-03-30T23:01:54Z</created><updated>2016-03-31T11:09:11Z</updated><resolved>2016-03-31T11:09:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-31T07:37:23Z" id="203799744">LGTM
</comment><comment author="clintongormley" created="2016-03-31T11:09:07Z" id="203881196">Fixed by e559ecaaf8d9836404d4b2c53a222ad7c560d239
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Painless Clean Up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17428</link><project id="" key="" /><description>- Removed all Java from the ANTLR grammar.
- Improved error messages for invalid types and identifiers.
- Pre-curser for unlimited generic types.
- Fixed _score to only be accessed if it's actually used instead of in all scripts.
- Score test added here (though this was a previous issue).
</description><key id="144743229">17428</key><summary>Painless Clean Up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T22:57:42Z</created><updated>2016-03-30T23:45:58Z</updated><resolved>2016-03-30T23:45:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-30T23:38:44Z" id="203686038">LGTM
</comment><comment author="jdconrad" created="2016-03-30T23:45:57Z" id="203687895">@rjernst Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>repository-s3 is not available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17427</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.3

**JVM version**: 1.8.0_77

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
Installation of repository-s3 failed
**Steps to reproduce**:
1. Follow documentation on https://www.elastic.co/guide/en/elasticsearch/plugins/master/repository-s3.html
2. Run /usr/share/elasticsearch/bin/plugin install repository-s3 --verbose
3. ERROR: failed to download out of all possible locations..., use --verbose to get detailed information

**Provide logs (if relevant)**:
-&gt; Installing repository-s3...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/repository-s3/2.3.0/repository-s3-2.3.0.zip ...
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/repository-s3/2.3.0/repository-s3-2.3.0.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/repository-s3/2.3.0/repository-s3-2.3.0.zip]; 
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
</description><key id="144736392">17427</key><summary>repository-s3 is not available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">passkey1510</reporter><labels /><created>2016-03-30T22:17:54Z</created><updated>2016-03-30T22:52:27Z</updated><resolved>2016-03-30T22:34:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-30T22:34:55Z" id="203668907">The S3 repository functionality for the 2.x series is available in the cloud-aws plugin (it is, however, split out for the 5.x series as repository-s3 versus discovery-ec2). You should be able to install the plugin with

```
$ ./bin/plugin install cloud-aws
```

and accepting the additional security permissions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_rename index feature request for a read-only index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17426</link><project id="" key="" /><description>Now that we have _reindex could we get a _rename for a closed index? 

Then I could do
_reindex a-&gt;b
_close b
DELETE a
_rename b-&gt;a
_open a

Yes this can kind of be done with aliases, but there are still some weirdness with aliases that this would make so much easier.
</description><key id="144736013">17426</key><summary>_rename index feature request for a read-only index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels><label>:Index APIs</label><label>adoptme</label><label>feature</label><label>high hanging fruit</label></labels><created>2016-03-30T22:15:24Z</created><updated>2016-06-15T10:49:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T12:51:30Z" id="203919449">I'll go one better.  I think that, with the work done in https://github.com/elastic/elasticsearch/pull/16442, https://github.com/elastic/elasticsearch/pull/17001, https://github.com/elastic/elasticsearch/pull/16217, and https://github.com/elastic/elasticsearch/pull/17048, we can now work on atomically renaming an open read-only index.
</comment><comment author="DrHyde" created="2016-06-14T15:02:59Z" id="225910785">I too would find it Really Useful to be able to rename an index via the API
</comment><comment author="clintongormley" created="2016-06-14T18:21:39Z" id="225970735">Update on this: the index name is way too entwined with the current code to let us do this easily.  We're going to work on moving the reliance on index names to index UUIDs instead, then maybe we can reconsider this change.  But this is a massive effort, so don't hold your breath :)
</comment><comment author="awick" created="2016-06-14T19:08:08Z" id="225984810">Ok what is the next best thing? :)

I know that I can shutdown ES, rename the directories and restart and it seems to work.  Is there anything in between?

Will _close, rename on disk, _open work?

If not, what about _close, rename on disk, rolling bounce, _open?
</comment><comment author="clintongormley" created="2016-06-15T08:39:52Z" id="226124336">&gt; I know that I can shutdown ES, rename the directories and restart and it seems to work. Is there anything in between?

This will no longer work in 5.0 because we now use the index UUID as the folder name, instead of the index name. (We will probably provide a command line tool which allows you to rename an index before starting the node)

&gt; Ok what is the next best thing? :)

Use aliases.  They can be switched atomically.
</comment><comment author="awick" created="2016-06-15T10:15:01Z" id="226146046">&gt; (We will probably provide a command line tool which allows you to rename an index before starting the node)

+1 

&gt; Use aliases. They can be switched atomically.

Ya, but they have some annoying properties.  Such as the _index returned is not the alias asked for, so you have to map them back, which is impossible if multiple aliases point to the same index.  This can be especially annoying in _stats where you ask for foo but the key into the map is bar.  (At least in 2.3, maybe this has all changed in 5)
</comment><comment author="clintongormley" created="2016-06-15T10:49:25Z" id="226153084">&gt; Ya, but they have some annoying properties. Such as the _index returned is not the alias asked for, so you have to map them back, which is impossible if multiple aliases point to the same index. This can be especially annoying in _stats where you ask for foo but the key into the map is bar. (At least in 2.3, maybe this has all changed in 5)

Agreed.  We've been thinking of making the index name just an alias, but marking it as the "main" alias somehow, so that it works as expected for stats, index apis, and logging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch request parsing in ClusterAllocationExplainRequest to ObjectParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17425</link><project id="" key="" /><description>Relates to #17305
</description><key id="144735894">17425</key><summary>Switch request parsing in ClusterAllocationExplainRequest to ObjectParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T22:14:42Z</created><updated>2016-03-31T01:44:53Z</updated><resolved>2016-03-31T01:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-30T22:16:16Z" id="203663586">@nik9000 I think I made a mistake with my earlier testing, I was trying to implement the test for https://github.com/elastic/elasticsearch/issues/17352 and wasn't able to reproduce,it, so I was doing something wrong :)
</comment><comment author="nik9000" created="2016-03-31T01:05:01Z" id="203701004">I left two "I'd do it a different way" comments but nothing worth holding the patch up for. LGTM.
</comment><comment author="nik9000" created="2016-03-31T01:44:53Z" id="203711728">I think you can make an object reference to the empty constructor from within the class itself, right?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start to rework query registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17424</link><project id="" key="" /><description>Changes QueryParser into a @FunctionalInterface and provides a way to
register queries using that. Cuts match a function_score queries over
to that registration method as a proof of concept.

Once all queries have been cut over we can remove their PROTOTYPES.
</description><key id="144733217">17424</key><summary>Start to rework query registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T22:02:53Z</created><updated>2016-04-04T18:50:42Z</updated><resolved>2016-03-31T17:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-30T22:03:41Z" id="203658389">@javanna I've learned from how huge my other attempts to remove PROTOTYPES were - something like this is a good way to start and to give us a way to cut the job into more manageable chunks.
</comment><comment author="nik9000" created="2016-03-31T14:48:53Z" id="203972002">@javanna I move the registration back.
</comment><comment author="javanna" created="2016-03-31T15:30:17Z" id="203987331">I like it, it's also along the lines of what we wanted to do at the end of the query refactoring, move the fromXContent to the query builders and get rid of all the query parser classes. Maybe @cbuescher wants to have a look too.
</comment><comment author="cbuescher" created="2016-03-31T17:13:15Z" id="204027987">Nice, I also took a quick look, merging the parsers into the builders this way is good I think. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Repo path should be 2.x not 2.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17423</link><project id="" key="" /><description>Looks like a typo has snuck into the docs with 2.3, [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-repositories.html), as we have moved away from specific minor version repos;

```
echo "deb http://packages.elastic.co/elasticsearch/2.3/debian stable main" | sudo tee -a /etc/apt/sources.list.d/elasticsearch-2.3.list
```

And also;

```
baseurl=https://packages.elastic.co/elasticsearch/2.3/centos
```

The `2.3` needs to be replaced with `2.x` for both. 

Reported [here](https://discuss.elastic.co/t/elasticsearch-2-3-installation-with-apt-fails-404/45848).
</description><key id="144728032">17423</key><summary>Repo path should be 2.x not 2.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2016-03-30T21:44:47Z</created><updated>2016-04-01T13:27:12Z</updated><resolved>2016-03-31T11:08:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T11:08:53Z" id="203881094">Fixed by e559ecaaf8d9836404d4b2c53a222ad7c560d239
</comment><comment author="s123klippel" created="2016-04-01T13:00:36Z" id="204389896">How does one then upgrade to a specific minor version to retain certain plugin compatibility? This move seems counter intuitive in conjunction with the 2.x move toward strict minor version plugin compatibility. 
</comment><comment author="jasontedor" created="2016-04-01T13:05:21Z" id="204390823">&gt; How does one then upgrade to a specific minor version to retain certain plugin compatibility?

@s123klippel Your package manager will let you specify a version that you want to upgrade to. Specifically for `apt-get` (assuming that you've updated your cache):

```
$ sudo apt-get install --only-upgrade elasticsearch=2.2.2
```

Here's a full log showing this in action, first installing version 2.2.1 of Elasticsearch and then upgrading to version 2.2.2.

``` bash
$ sudo apt-get install elasticsearch=2.2.1
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  elasticsearch
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 0 B/29.3 MB of archives.
After this operation, 32.8 MB of additional disk space will be used.
Selecting previously unselected package elasticsearch.
(Reading database ... 87653 files and directories currently installed.)
Preparing to unpack .../elasticsearch_2.2.1_all.deb ...
Unpacking elasticsearch (2.2.1) ...
Processing triggers for systemd (225-1ubuntu9.1) ...
Processing triggers for ureadahead (0.100.0-19) ...
Setting up elasticsearch (2.2.1) ...
Installing new version of config file /usr/lib/systemd/system/elasticsearch.service ...
$ sudo apt-get install --only-upgrade elasticsearch=2.2.2
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following packages will be upgraded:
  elasticsearch
1 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.
Need to get 0 B/29.3 MB of archives.
After this operation, 1,024 B of additional disk space will be used.
(Reading database ... 87722 files and directories currently installed.)
Preparing to unpack .../elasticsearch_2.2.2_all.deb ...
Unpacking elasticsearch (2.2.2) over (2.2.1) ...
Processing triggers for systemd (225-1ubuntu9.1) ...
Processing triggers for ureadahead (0.100.0-19) ...
Setting up elasticsearch (2.2.2) ...
Installing new version of config file /usr/lib/systemd/system/elasticsearch.service ...
```
</comment><comment author="s123klippel" created="2016-04-01T13:09:24Z" id="204391905">Thank you, I'm not an everyday linux user and didn't know that existed. 
</comment><comment author="jasontedor" created="2016-04-01T13:27:12Z" id="204396017">&gt; Thank you, I'm not an everyday linux user and didn't know that existed.

You're welcome. :smile: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move translog recover outside of the engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17422</link><project id="" key="" /><description>We changed the way we manage engine memory buffers to an
open model where each shard can essentially has infinite memory.
The indexing memory controller is responsible for moving memory to disk
when it's needed. Yet, this doesn't work today when we recover from store/translog
since the engine is not fully initialized such that IMC has no access to the engine,
neither to it's memory buffer nor can it move data to disk.

The biggest issue here is that translog recovery happends inside the Engine constructor
which is problematic by itself since it might take minutes and uses a not yet fully
initialzied engine to perform write operations on.

This change detaches the translog recovery and makes it the responsibility of the caller
to run it once the engine is fully constructed or skip it if not necessary.
</description><key id="144723995">17422</key><summary>Move translog recover outside of the engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T21:25:22Z</created><updated>2016-04-04T18:51:20Z</updated><resolved>2016-03-31T19:03:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-30T21:25:31Z" id="203641653">@mikemccand  FYI
</comment><comment author="mikemccand" created="2016-03-30T21:42:30Z" id="203646388">LGTM, thanks @s1monw ... I think it's best have a faster ctor for `Engine` and do heavy stuff (translog recovery) after it's created.

With this patch I see the power loss tester got through 4 crashes so far, and no `OutOfMemoryError`.  I ran with DEBUG logging and I see that `IndexingMemoryController` is indeed seeing indexing buffer used, and writing them to disk.
</comment><comment author="bleskes" created="2016-03-31T08:57:28Z" id="203831908">I think the essence of the change is good. I'm a bit concerned that we now make an important like forceNewTranslog mutable in engineConfig (so we can forget to reset it it). We already crossed this line with isCreate . I would prefer moving both to be explicit parameters to the engine constructor...
</comment><comment author="s1monw" created="2016-03-31T10:02:01Z" id="203860436">@bleskes pushed new commits
</comment><comment author="bleskes" created="2016-03-31T10:50:14Z" id="203876158">I like it very much. Left some comments.
</comment><comment author="mikemccand" created="2016-03-31T13:44:05Z" id="203945116">I love the new tests!  Thanks @s1monw.
</comment><comment author="bleskes" created="2016-03-31T14:32:48Z" id="203966609">LGTM. Left on comment about redundant commits. Thanks for the extra effort.
</comment><comment author="s1monw" created="2016-03-31T19:03:24Z" id="204081257">merged, thanks @mikemccand and @bleskes for the help here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard listener registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17421</link><project id="" key="" /><description>The merge #17398 adds back in the ability to register IndexingOperationListeners in a plugin.  However, the semantics are different and doesn't provide the same functionality as the 2.x implementation.

In 2.x a plugin would register a listener directly with an IndexShard after creation.  I have a 2.x plugin that uses information about the IndexShard when handling events in the listener.

In the new implementation, there is a single instance of a given listener used for all IndexShards for an Index.  I don't see a way to get the calling IndexShard in the listener anymore.

Was this an intentional change, or can we get this functionality back in?
I can provide a PR...
If you would like a PR for this, I see two ways to solve.  Register a listener factory in IndexModule, build and attach the listeners just after IndexShard creation.  Or pass the calling IndexShard to the listener on each event call.
</description><key id="144712642">17421</key><summary>Shard listener registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oillio</reporter><labels /><created>2016-03-30T20:36:31Z</created><updated>2016-03-30T21:31:16Z</updated><resolved>2016-03-30T21:31:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-30T20:43:40Z" id="203627630">what kind of infos are you using, I don't really think we should open up too much on this level of the system so I am hesitating here to bring back the large openness we had in 2.x
</comment><comment author="oillio" created="2016-03-30T21:31:16Z" id="203643208">Yeah, I was afraid of that.  I should be able to get what I need at the Index level, I didn't want to re-write it if I didn't have to.  Thanks for the quick reply.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a configuration option:  routing=my-local-shards-only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17420</link><project id="" key="" /><description>**Describe the feature**: Provide a configuration **routing: my-local-shards-only**

**Some background**: We have a 40 node storm cluster that runs 40 kafka-spouts and 80 ES-Bolts with LOCAL_OR_SHUFFLE_GROUPING. ES-cluster has 10 shards with 1 replica each and they run on 10 powerful machines with 100+GB RAM and 32 cores each.

**Problem**: Field-based routing has been removed in 2.x version and the latest elasticsearch-hadoop does not provide a way to route the messages yet ([ES-Hadoop-issue-715](https://github.com/elastic/elasticsearch-hadoop/issues/715)).

So the bulk request (we have 10k messages batch currently, each message is 0.7 kb roughly) from each bolt is distributed equally among all the shards and that makes it roughly ~1k messages / shard. This is killing the performance. If batch-size is made larger, number of failures increase in storm.

Even if ES-Hadoop implements routing and the bolts start appending a routing-value in their URLs, it **seems** unlikely (I am not sure though) that every bolt will be able to figure out an exact value of routing such that the ES-server that receives the requests process it locally only. This is so because currently the custom-routing is used in the following formula as per [the docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html):

```
 shard_num = hash(_routing) % num_primary_shards
```

So to make the ES-server process the bulk request locally, the routing-field value must be such that the _shard_num_ value comes out to be the same value as the current shard (ES-Hadoop sends data to primary shards only if its not configured for client-nodes.) This is difficult to achieve IMO (especially when there are several writers like storm's ES-Bolts).

An alternative could be to have a configuration **routing: my-local-shards-only** in elastic-search itself which makes ES-servers consume the entire bulk-request locally. On doing so, the clients need not implement any routing and just by ensuring that their request goes to the correct primary, they can achieve a good load balance. Benefits of this approach:
1. Client need not **guess** an exact value of the _routing_ field such that its consumed locally.
2. We are sure that there would never be a hop of bulk request from one primary-shard to another primary-shard because it's guaranteed to be consumed locally.
3. For several clients (like storm/hadoop), there is no code change to implement routing.

IMHO, the load-balancing achieved in this way is no different than the load-balancing achieved by specifying the routing field in the URL (I could be wrong though) because client is responsible to either provide a good routing-field or select the right primary-shards (latter seems to be easier).

One drawback I do see in the **routing: my-local-shards-only** approach is that lookup-by-ID becomes mostly impossible because clients would not know what routing-field to specify. But for many systems, this is not a concern because they will never do a lookup-by-ID (Example: log search use-case) and even for those cases, one can always search for ID like all other regular fields (scatter-gather)

Sorry for the long description :)

**Elasticsearch version**: 2.x
</description><key id="144691617">17420</key><summary>Provide a configuration option:  routing=my-local-shards-only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sachingsachin</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2016-03-30T19:08:49Z</created><updated>2017-03-14T11:55:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T11:03:46Z" id="203879027">Hi @sachingsachin 

&gt; Sorry for the long description :)

Not at all.  Helps to understand the context.

&gt; One drawback I do see in the routing: my-local-shards-only approach is that lookup-by-ID becomes mostly impossible because clients would not know what routing-field to specify. But for many systems, this is not a concern because they will never do a lookup-by-ID (Example: log search use-case) and even for those cases, one can always search for ID like all other regular fields (scatter-gather)

This is the problem I see with your suggestion. It's important for us at Elasticsearch to keep things consistent, to not introduce feature X which doesn't work with feature Y unless condition Z.... These things end up producing a lot of confusion and complexity, which makes Elasticsearch harder to understand and use.  I'm very much against adding non-generic changes like this, especially when we have a generic mechanism specifically for managing such issues, ie custom routing.

I think the solution needs to be implemented at a higher level, eg each bolt specifies a custom routing value on each message, which limits all of the messages in that bolt to a single shard.

&gt; So to make the ES-server process the bulk request locally, the routing-field value must be such that the shard_num value comes out to be the same value as the current shard (ES-Hadoop sends data to primary shards only if its not configured for client-nodes.) This is difficult to achieve IMO (especially when there are several writers like storm's ES-Bolts).

Actually it is pretty easy to figure out routing values that map to individual shards.  Here's what I did to come up with a list for 10 shards:

```
PUT t 
{
  "settings": {"number_of_shards":10}
}

POST t/t/_bulk
{"index": {"_routing": "01"}}
{}
{"index": {"_routing": "02"}}
{}
{"index": {"_routing": "03"}}
{}
{"index": {"_routing": "04"}}
{}
{"index": {"_routing": "05"}}
{}
{"index": {"_routing": "06"}}
{}
{"index": {"_routing": "07"}}
{}
{"index": {"_routing": "11"}}
{}
{"index": {"_routing": "13"}}
{}
{"index": {"_routing": "17"}}
{}


GET _stats?level=shards&amp;filter_path=**.shards.**.docs.count
```

Give one of each of these values to 4 kafka spouts and all 4 will then map to a single shard.
</comment><comment author="sachingsachin" created="2016-03-31T17:35:37Z" id="204043247">Thank you @clintongormley 

&gt; Here's what I did to come up with a list for 10 shards.

The JSON response I got from the above requests is as follows:

```
{
   "indices": {
      "t": {
         "shards": {
            "0": [
               {
                  "docs": {
                     "count": 1
                  }
               },
               {
                  "docs": {
                     "count": 1
                  }
               }
            ],
            "1": [
               {
                  "docs": {
                     "count": 1
                  }
               },
               {
                  "docs": {
                     "count": 1
                  }
               }
            ],
            ... 8 more blocks like the above
          }
        }
  }
}
```

&gt; Give one of each of these values to 4 kafka spouts and all 4 will then map to a single shard.

I think you meant 80 EsBolts and not 4 kafka spout.
But even if you meant EsBolt, I do not understand the above JSON response or how it can help my case.

For other indices I have, I get a pretty similar response without making any of the above POST requests.
For example, the stats on my storm index look very similar:

```
{
   "indices": {
      "storm": {
         "shards": {
            "0": [
               {
                  "docs": {
                     "count": 14467774
                  }
               },
               {
                  "docs": {
                     "count": 14467774
                  }
               }
            ]
         }
      }
  }
}
```

So what is the difference made by the above POST calls (sorry, I have only a few months experience with ES).

&gt; it's important for us at Elasticsearch to keep things consistent, to not introduce feature X which doesn't work with feature Y unless condition Z

I am inclined to think that this is how options work in general.
All features do not work with all the options.
For example, if someone uses custom routing (putting it in the URL of a client request), he is aware that lookup-by-ID will not work unless a correct routing value is not specified.

A quote from [the docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html):

&gt; When indexing documents specifying a custom _routing, the uniqueness of the _id is not guaranteed across all of the shards in the index. In fact, documents with the same _id might end up on different shards if indexed with different _routing values.
&gt; 
&gt; It is up to the user to ensure that IDs are unique across the index.

Thus, the user would be aware what he is doing.
If he chooses routing in the URL (feature X), he will not get unique results when doing a lookup-by-ID (feature Y) unless he ensures uniqueness of bulk requests to shards (condition Z).
Similarly, if a user chooses the feature requested here, I would assume that he knows what he is doing and a good documentation (or an answer on SO) should clarify this even further.

Also, I would appreciate your thoughts on the other benefits of the proposed option (listed in my first comment).

Thank you @clintongormley 
</comment><comment author="clintongormley" created="2016-04-04T18:33:59Z" id="205438877">&gt; I think you meant 80 EsBolts and not 4 kafka spout.
&gt; But even if you meant EsBolt, I do not understand the above JSON response or how it can help my case.

What I meant by the above was that here you have a list of routing values, each of which maps to a **single** shard (when you're using 10 shards).  This is the answer to:

&gt; it seems unlikely (I am not sure though) that every bolt will be able to figure out an exact value of routing such that the ES-server that receives the requests process it locally only.

 So you can assign one routing value to 4 kafka spouts, or to 8 EsBolts and they will target a single Elasticsearch shard.

&gt; Also, I would appreciate your thoughts on the other benefits of the proposed option (listed in my first comment).

While I see the benefits you mention, the problem I have with the proposal as a whole is as stated in 
https://github.com/elastic/elasticsearch/issues/17420#issuecomment-203879027.

I've left this open for discussion in case somebody has some better idea.  
</comment><comment author="sachingsachin" created="2016-04-05T21:22:39Z" id="205988341">Thanks for leaving it open for others to comment @clintongormley .
If more votes are skewed towards the benefits introduced by this option and lesser towards the problems, then we could try to implement the same.

Is there a way to seek opinion from couple more folks?
A quicker closure would be better than an indefinite one.
</comment><comment author="fmyblack" created="2017-03-14T11:55:47Z" id="286399504">I have a nearly-like problem.
**my background**: I have about 350GB data need write in `es2.3` using `elasticsearch-hadoop` before 6:00am each day. Every doc is about a single user profile (with a uuid and multi-tags) . 
**The problem**: The only thing I want is increasing index performance cause there be a cache api in query client. I want one partition data can be indexed in same shard, though I'm not care about if the route path is uuid.

I find the hash function in `es2.3` is `Murmur3HashFunction`, 
and in `elasticsearch-hadoop`:
```
int bucket = currentInstance % targetShards.size();
Shard chosenShard = orderedShards.get(bucket);
Node targetNode = targetShards.get(chosenShard);
```
thats means `partitionid % totalshards == chosenshard`, so if the data is in the same partition, is will send to same shard, I want the chosen shard will be the routing shard, So I create a Partitioner like this:
```
public class EsPartitioner extends Partitioner {

	int numPartitions;
	
	public EsPartitioner(int numPartitions) {
		this.numPartitions = numPartitions;
	}
	
	@Override
	public int getPartition(Object row) {
		// TODO Auto-generated method stub
		int hash = Murmur3HashFunction.hash("" + row) % this.numPartitions;
		if(hash &lt; 0) {
			hash += this.numPartitions;
		}
		return hash;
	}
	
	@Override
	public int numPartitions() {
		// TODO Auto-generated method stub
		return numPartitions;
	}

	/**
	 * Hash function based on the Murmur3 algorithm, which is the default as of
	 * Elasticsearch 2.0.
	 */
	static class Murmur3HashFunction {...// same like in es}
}
```

In my rdd, key is uuid:
```
int partitionNum = 10 * shardNum;
JavaPairRDD&lt;String, Profile&gt; users...;
users.partitionBy(new EsPartitioner(partitionNum));
```

last, I set `es.mapping.routing` --&gt; 'uuid' to make the es and spark use same hash function and same route field. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't run `mkdir` when $DATA_DIR contains a comma-separated list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17419</link><project id="" key="" /><description>Resolves #16992
Resolves https://github.com/elastic/cookbook-elasticsearch/issues/441
</description><key id="144689645">17419</key><summary>Don't run `mkdir` when $DATA_DIR contains a comma-separated list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Packaging</label><label>bug</label><label>v5.0.0-alpha3</label></labels><created>2016-03-30T19:01:33Z</created><updated>2016-05-17T20:07:17Z</updated><resolved>2016-05-17T19:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-30T19:55:32Z" id="203607169">If you change packaging without updating the Vagrant tests, you're going to have a bad time. :smile: 
</comment><comment author="dakrone" created="2016-03-30T19:58:23Z" id="203608109">&gt; If you change packaging without updating the Vagrant tests, you're going to have a bad time. :smile:

I'm trying to figure out the best way to test this still :) I won't merge it without tests, but I wanted to make sure the fix was uncontroversial also
</comment><comment author="nik9000" created="2016-03-30T20:38:57Z" id="203625343">Fix looks good to me. Enjoy the land of vagrant tests.
</comment><comment author="dakrone" created="2016-04-14T19:16:11Z" id="210106663">This is blocked on #17634 as my user's umask is "wrong" and prevents me from running the vagrant tests without a ton of bogus failures.
</comment><comment author="clintongormley" created="2016-05-07T14:34:41Z" id="217641424">@dakrone want to pick this one up again?
</comment><comment author="dakrone" created="2016-05-12T15:31:14Z" id="218794277">@nik9000 or @jasontedor I added a vagrant test for this, sorry for the force push, it was waaaaay outdated and the packaging tests wouldn't work until it was rebased.
</comment><comment author="jasontedor" created="2016-05-12T21:44:01Z" id="218894951">&gt; I added a vagrant test for this, sorry for the force push, it was waaaaay outdated and the packaging tests wouldn't work until it was rebased.

I left some questions.
</comment><comment author="jasontedor" created="2016-05-12T22:10:32Z" id="218900704">LGTM.
</comment><comment author="jasontedor" created="2016-05-17T19:50:04Z" id="219832861">I left a comment, otherwise LGTM.
</comment><comment author="dakrone" created="2016-05-17T19:58:28Z" id="219835040">Thanks for reviewing @jasontedor!
</comment><comment author="martinb3" created="2016-05-17T20:07:17Z" id="219837392">Thanks for taking care of this! The chef cookbook users are very grateful! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Block new index creation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17418</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Block create index**: Block new index creation at cluster level. One way to achieve this is to enable "cluster.blocks.read_only" but it blocks all write operations on cluster whereas we just want to block new index creation and allow other operations. Let me know the possibilities. 

I tried to implement above configuration in 1.5 branch which can be found [here](https://github.com/rishabhmaurya/elasticsearch/commit/dcbfbe3e4edb8506b543e8f2ffd800431c6d549e).
</description><key id="144686629">17418</key><summary>Block new index creation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rishabhmaurya</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2016-03-30T18:52:17Z</created><updated>2016-07-05T14:57:37Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-31T10:33:16Z" id="203872330">This can be done today by:
- Setting `action.auto_create_index` to `false`
- Blocking access to the `PUT /index` API
</comment><comment author="rishabhmaurya" created="2016-03-31T10:45:20Z" id="203875113">@clintongormley  Yes, this should work but I forgot to describe one use case, we want to trigger this setting dynamically. 
As `action.auto_create_index: false` is a configurable from `elasticsearch.yml`, so do you think the solution you suggested will work ? 
Also, is there some way in elasticsearch to restrict access to  API ?
</comment><comment author="clintongormley" created="2016-03-31T18:55:32Z" id="204077046">&gt; As action.auto_create_index: false is a configurable from elasticsearch.yml, so do you think the solution you suggested will work ? 

As you note, this is not yet a dynamic setting.  See https://github.com/elastic/elasticsearch/issues/7513

&gt; Also, is there some way in elasticsearch to restrict access to API ?

By putting a proxy in front of Elasticsearch, which you should have anyway, or by installing the Shield plugin
</comment><comment author="rishabhmaurya" created="2016-04-01T07:10:44Z" id="204278905">@clintongormley do you think its good to introduce block create index setting as well with #7513 ? We are not much comfortable in putting these setting in proxy or using shield plugin as it would add overhead to all requests.  
</comment><comment author="rishabhmaurya" created="2016-04-07T07:59:38Z" id="206747768">@clintongormley  As changes are in `TransportClusterUpdateSettingsAction`and `TransportCreateIndexAction`, do you think its possible to write plugin for [this ](https://github.com/rishabhmaurya/elasticsearch/commit/dcbfbe3e4edb8506b543e8f2ffd800431c6d549e)change and is it normally a good practice to override these classes in plugin ? Also, the change in `TransportClusterUpdateSettingsAction.java` is part of class `AckedClusterStateUpdateTask` which is passed as parameter. So don't think that this part of change is extensible to write a plugin or I have to maintain own version of this class.
</comment><comment author="rishabhmaurya" created="2016-07-05T14:57:37Z" id="230503232">@clintongormley Tried to write plugin for this setting. Looks like we cannot modify existing actions(in this case TransportCreateIndexAction) from es plugin(see https://github.com/elastic/elasticsearch/issues/2311).
We don't have any solution now other then to contribute it back to elasticsearch. This setting might be useful for others too. Scenarios where it can be useful - 
1.  If there is high memory consumption because of disk writes
2.  Running low disk space
One can trigger circuit breakers on exceeding memory/disk usage thresholds. Blocking new index creation will help in above cases.
Let me know what do you think in this regard ?
Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up QueryParseContext and don't hold it inside QueryRewrite/ShardContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17417</link><project id="" key="" /><description>This change cleans up a few things in QueryParseContext and QueryShardContext that make it hard to reason about the state of these context objects and are thus error prone and should be simplified.

Currently the parser that used in QueryParseContext can be set and reset any time from the outside, which makes reasoning about it hard.
This change makes the parser a mandatory constructor argument removes ability to later set a differen ParseFieldMatcher. If none is provided at construction time, the one set inside the parser is used. If a ParseFieldMatcher is specified at construction time, it is implicitely set on the parser that is beeing used. The ParseFieldMatcher is only kept inside the parser.

Also currently the QueryShardContext historically holds an inner QueryParseContext (in the super class QueryRewriteContext), that is mainly used to hold the parser and parseFieldMatcher. For that reason, the parser can also be reset, which leads to the same problems as above. This change removes the QueryParseContext from QueryRewriteContext and removes the ability to reset or retrieve it from the QueryShardContext. Instead, `QueryRewriteContext#newParseContext(parser)` can be used to create new parse contexts with the given parser from a shard context when needed.
</description><key id="144653243">17417</key><summary>Clean up QueryParseContext and don't hold it inside QueryRewrite/ShardContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-30T17:00:05Z</created><updated>2016-04-18T13:13:53Z</updated><resolved>2016-04-18T13:13:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-04-06T16:15:31Z" id="206446382">I like these changes a lot! I left some comments, mainly ideas to explore I think, but they don't need to be all implemented as part of this PR. Up to you which ones can be done here and which ones are bigger and deserve their own followup PR.

One more thing, can we make the ParseFieldMatcher constructor that takes a boolean argument private? 
</comment><comment author="cbuescher" created="2016-04-13T13:22:21Z" id="209438337">@javanna I rebased this PR on current master and added a commit that now keeps the parseFieldMatcher only in the parser and not storing it in the QueryParseContext. The `parseFieldMatch()` getter of the context just forwards to the parser. This however makes is necessary that we alway set the parseFieldMatcher in the parsers instead of now where we set it on the context. In the future we should try to make ParseFieldMatcher immutable on parsers and make it a mandatory constructor argument of AbstractXContentParser. But this is a big change and out of the scope of this PR.
</comment><comment author="cbuescher" created="2016-04-14T17:32:39Z" id="210067569">@javanna so I think after opening #17756 (and after that gets some traction) I'm going to rework this and remove the last commit.
</comment><comment author="cbuescher" created="2016-04-15T16:39:19Z" id="210538141">@javanna after merging #17756 I updated this and removed some more usages of the STRICT constant from core. I think this is much cleaner now.
</comment><comment author="javanna" created="2016-04-15T17:35:54Z" id="210558449">this looks much cleaner indeed! left a few minors but LGTM!
</comment><comment author="cbuescher" created="2016-04-18T12:48:36Z" id="211366292">@javanna thanks for the last round of comments, I adressed them in the last two commits. 
</comment><comment author="javanna" created="2016-04-18T12:55:41Z" id="211369553">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17416</link><project id="" key="" /><description>Name of the file was not coherent with previous name.
</description><key id="144650473">17416</key><summary>Typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">antilogos</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-03-30T16:49:59Z</created><updated>2016-03-31T18:22:00Z</updated><resolved>2016-03-31T18:21:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-30T18:08:17Z" id="203559349">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="antilogos" created="2016-03-31T08:36:20Z" id="203820878">No, find a way around, I only came here  to report this and didn't find any other way. This CLA stuff looks too bothersome.
</comment><comment author="clintongormley" created="2016-03-31T18:21:59Z" id="204064067">Sorry you feel that way @antilogos - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update `pom.xml` needed changes when using Java client 5.0.0.alpha1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17415</link><project id="" key="" /><description>When we are using a Lucene Snapshot version, the generated `pom.xml` does not refer at all to our internal Lucene snapshot.

So in our `pom.xml` we depend on:

``` xml
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt;
  &lt;artifactId&gt;lucene-core&lt;/artifactId&gt;
  &lt;version&gt;6.0.0-snapshot-f0aa4fc&lt;/version&gt;
  &lt;scope&gt;compile&lt;/scope&gt;
&lt;/dependency&gt;
```

But our users won't be able to find this version and won't be able to basically use our Java client.

They need to find where this repo lives (and TBH I did not find it yet...).

When we depend on an internal Lucene version like this, could it be possible to generate the link to this repo within the generated `pom.xml`? Like:

``` xml
    &lt;repositories&gt;
        &lt;repository&gt;
            &lt;id&gt;lucene-snapshot-f0aa4fc&lt;/id&gt;
            &lt;name&gt;Lucene Snapshot repo&lt;/name&gt;
            &lt;url&gt;http://download.elastic.co/xxxx&lt;/url&gt;
        &lt;/repository&gt;
    &lt;/repositories&gt;
```
</description><key id="144621538">17415</key><summary>Update `pom.xml` needed changes when using Java client 5.0.0.alpha1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label></labels><created>2016-03-30T15:03:10Z</created><updated>2016-04-29T16:10:30Z</updated><resolved>2016-04-29T16:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-30T15:03:26Z" id="203478792">@rjernst WDYT? Is that doable?
</comment><comment author="jpountz" created="2016-03-30T15:31:11Z" id="203489567">Do we need to do anything here? It would only be an issue for some alpha releases that we publish while Lucene is not out yet? Maybe it is fine to say that you have to wait until the first beta to test with the transport client?
</comment><comment author="dadoonet" created="2016-03-30T15:33:11Z" id="203490434">I'm fine with that as well. Just that it's need to be documented somewhere for early testers using the Java client.
</comment><comment author="rjernst" created="2016-03-30T15:37:27Z" id="203492843">While it is doable (here is patch which does it: https://gist.github.com/rjernst/65b13310c01e42d35bbbf12432ac5859), I agree with @jpountz that maybe it is not worthwhile to have this code for such an edge case that only applies during alpha releases. I actually think gradle should be doing it themselves (they should never generate a "broken" pom).
</comment><comment author="spinscale" created="2016-03-31T10:01:13Z" id="203860083">+1 for less magic and tell people to add it explicitely as long as we need
</comment><comment author="clintongormley" created="2016-03-31T10:13:58Z" id="203865171">agreed - i can add the required info to the blog post, if somebody can tell me what needs to be said?  @rjernst @dadoonet ?
</comment><comment author="dadoonet" created="2016-03-31T17:46:55Z" id="204047968">@clintongormley To use the Java client in a maven project, users have to add:

``` xml
&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;lucene-snapshot&lt;/id&gt;
        &lt;name&gt;Lucene Snapshot repo&lt;/name&gt;
        &lt;url&gt;http://download.elastic.co/lucenesnapshots/f0aa4fc&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
```

And also adds this dependency as it fails otherwise (needs a confirmation if it's intended or a bug though):

``` xml
&lt;dependency&gt;
    &lt;groupId&gt;log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j&lt;/artifactId&gt;
    &lt;version&gt;1.2.17&lt;/version&gt;
&lt;/dependency&gt;
```

Hope this helps. I changed the label for this issue and changes the title.
</comment><comment author="centic9" created="2016-04-29T07:27:10Z" id="215646642">Note that this is now elevated a bit by the [pioneer-program](https://www.elastic.co/blog/elastic-pioneer-program) as through this more people will likely try an alpha and stumble into this, at least I did...
</comment><comment author="nik9000" created="2016-04-29T10:57:14Z" id="215683965">&gt; Note that this is now elevated a bit by the pioneer-program as through this more people will likely try an alpha and stumble into this, at least I did...

Not that it really helps that much, but 5.0.0-alpha-2 won't need this because it uses Lucene 6.0.0.
</comment><comment author="clintongormley" created="2016-04-29T16:10:30Z" id="215783960">It was doc'ed in the blog post, and alpha2 is out now, so I'm closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't take recovery indexing into account on indexing stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17414</link><project id="" key="" /><description>Closes #17412
</description><key id="144616281">17414</key><summary>Don't take recovery indexing into account on indexing stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Tribe Node</label><label>non-issue</label><label>review</label></labels><created>2016-03-30T14:48:19Z</created><updated>2016-03-30T15:15:01Z</updated><resolved>2016-03-30T15:15:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-30T14:52:13Z" id="203470651">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add examples of useful dynamic templates to the docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17413</link><project id="" key="" /><description>Relates to #17241.
</description><key id="144601578">17413</key><summary>Add examples of useful dynamic templates to the docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label></labels><created>2016-03-30T13:58:05Z</created><updated>2016-03-31T07:45:46Z</updated><resolved>2016-03-31T07:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-30T17:10:52Z" id="203532952">Couple of minor changes, but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indexing stats now contain indexing ops from recovery </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17412</link><project id="" key="" /><description>this is a followup from #17406 where we now use the same code path for indexing even the document is a recovery operation from disk or remote. This influences the stats which are NOW correct but this depends on the semantics we want to have from them. I open this issue to discuss this and make it a blocker such that we won't forget
</description><key id="144601155">17412</key><summary>indexing stats now contain indexing ops from recovery </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Stats</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T13:56:14Z</created><updated>2016-10-18T08:53:20Z</updated><resolved>2016-03-30T15:15:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-30T14:06:49Z" id="203447729">From a user's point of view, the fact that the recovery process makes use of indexing is an implementation detail.  This number reflects "active" indexing requests, and the more interesting number is the rate of change, rather than the absolute number.  So i'd make the stats work in the same way are before #17406
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic templates ignore analyzers on keyword fields silently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17411</link><project id="" key="" /><description>The following:

```
PUT t
{
  "mappings": {
    "t": {
      "dynamic_templates": [
        {
          "strings": {
            "match_mapping_type": "string",
            "mapping": {
              "analyzer": "english",
              "type": "keyword"
            }
          }
        }
      ]
    }
  }
}

PUT t/t/1
{
  "foo": "bar"
}
```

creates a `keyword` field without an analyzer, which is good.  However, I would have expected it to throw an exception about the presence of the analyzer setting.
</description><key id="144598125">17411</key><summary>Dynamic templates ignore analyzers on keyword fields silently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2016-03-30T13:45:06Z</created><updated>2017-01-12T10:48:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-30T14:54:06Z" id="203472200">This is an old bug apparently: dynamic templates do not check for unknown parameters, they simply ignore them. While it could be easy to fix this, this would break lots of existing templates. For instance if you have a template that matches everything and adds `store: true`, then it would fail on all objects since objects do not support the `store` property.
</comment><comment author="clintongormley" created="2016-10-18T07:57:51Z" id="254434560">Should we fix this or leave it be?
</comment><comment author="jpountz" created="2016-10-18T08:01:32Z" id="254435313">I'd be willing to fix it. But the backward compatibility is not going to be easy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove duplicate getters from DiscoveryNode and DiscoveryNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17410</link><project id="" key="" /><description>Up until now we maintained two versions of getters here and there through the codebase: the standard java getters and the ones without the get prefix. This PR removes this duplication from DiscoveryNode and DiscoveryNodes classes. Only the standard java getters are kept, there is no need for duplicating these, also given that the rest of the codebase ends up calling either one or the other although they do exactly the same.

I stumbled upon this while working on #16963 and it has been bugging me for a while. I am sure we have the same problem in many other places but this PR is already big enough. This is breaking for java api (and plugins) as we return DiscoveryNode objects from nodes info, nodes stats and tasks list api.
</description><key id="144597423">17410</key><summary>Remove duplicate getters from DiscoveryNode and DiscoveryNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T13:42:23Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-31T09:45:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-30T13:43:22Z" id="203439673">Sorry about the big PR, I hope it doesn't get on the way of ongoing refactorings... it is just renames though, shouldn't be too hard to review.
</comment><comment author="nik9000" created="2016-03-30T14:07:56Z" id="203448059">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Smoke Tester: Use `node.portsfile` to find out ports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17409</link><project id="" key="" /><description>Instead of hoping for `localhost:9200` to be available, we could use the ports file to find out where to connect in the smoke tester.
</description><key id="144584024">17409</key><summary>Smoke Tester: Use `node.portsfile` to find out ports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>low hanging fruit</label><label>test</label></labels><created>2016-03-30T13:11:51Z</created><updated>2016-04-06T07:16:19Z</updated><resolved>2016-04-06T07:16:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Smoke Tester: Adapt to latest 5.0 changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17408</link><project id="" key="" /><description>The smoke tester required several changes,
ranging from plugin names, to parameter handling
in order to pass.
</description><key id="144580224">17408</key><summary>Smoke Tester: Adapt to latest 5.0 changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label></labels><created>2016-03-30T12:54:56Z</created><updated>2016-03-30T13:12:56Z</updated><resolved>2016-03-30T13:12:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-30T12:56:48Z" id="203419698">Makes sense to me!
</comment><comment author="s1monw" created="2016-03-30T12:58:17Z" id="203420068">left a comment for a followup, can you open it? LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect 'duplicate consecutive coordinates' error on geo_shape index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17407</link><project id="" key="" /><description>**Description of the problem including expected versus actual behavior**:
On Elasticsearch 2.2.1, when loading some geojson shapes into a field indexed with:

```
"location": {"type": "geo_shape", "precision": "1600m"}
```

Throws the following error:

```
Provided shape has duplicate consecutive coordinates at: (180.0, -90.0, NaN)
```

I believe the geojson is valid and should load fine.

**Steps to reproduce**:
1. Load the 'geometry' field of this [json blob of Antarctica](https://github.com/elastic/elasticsearch/files/195473/Antarctica.txt) into a geoshape index

This is a reprise of #14014 but with more details included. I agree with the analysis in that ticket by @danni that this is a artefact of the shape being simplified before being indexed - the 'duplicate consecutive coordinates' being referred to in the error quite simply don't appear even once in the original data.
</description><key id="144575745">17407</key><summary>Incorrect 'duplicate consecutive coordinates' error on geo_shape index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">smithsimonj</reporter><labels><label>:Geo</label><label>discuss</label></labels><created>2016-03-30T12:38:23Z</created><updated>2016-06-07T21:45:07Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="smithsimonj" created="2016-04-22T09:59:54Z" id="213364540">I also see the above problem in Elastic 2.3.1
</comment><comment author="smithsimonj" created="2016-05-13T16:16:56Z" id="219089995">Any news on this ticket?

I also see a related bug on this data:

```
{"type": "polygon", "coordinates": [[[-100.7193347, 37.9217785], [-100.7180306, 37.9207239], [-100.7180379, 37.9202649], [-100.715946, 37.9185112], [-100.7181033, 37.91686], [-100.7208637, 37.9161565], [-100.7264067, 37.9162093], [-100.7290355, 37.9182905], [-100.7289411, 37.9254825], [-100.7370413, 37.9320032], [-100.7344506, 37.9340071], [-100.7299369, 37.9304726], [-100.7249065, 37.9343839], [-100.7248924, 37.944431], [-100.7202908, 37.944422], [-100.7199423, 37.9439893], [-100.7199856, 37.9341727], [-100.7199863, 37.9340851], [-100.7199081, 37.9340847], [-100.7199087, 37.9340092], [-100.7199091, 37.9339619], [-100.7194596, 37.9340074], [-100.7186862, 37.9340038], [-100.7187256, 37.9288659], [-100.7194599, 37.9288675], [-100.7196063, 37.9288697], [-100.7196066, 37.9287782], [-100.7200695, 37.9287791], [-100.7200696, 37.9287473], [-100.7201195, 37.9287474], [-100.7201239, 37.9285088], [-100.7196, 37.9285077], [-100.7196003, 37.9284157], [-100.719624, 37.9282132], [-100.7196325, 37.927568], [-100.7198495, 37.9275697], [-100.7198458, 37.9272774], [-100.7197668, 37.9272145], [-100.7198542, 37.9271518], [-100.7198613, 37.9271555], [-100.7198696, 37.9271479], [-100.7199004, 37.9271347], [-100.7199004, 37.9271115], [-100.7199739, 37.9271115], [-100.7197711, 37.9271115], [-100.719771, 37.9270036], [-100.7197264, 37.9270037], [-100.7197264, 37.9269267], [-100.7197708, 37.9269266], [-100.7197707, 37.9267495], [-100.7198207, 37.9267495], [-100.7198197, 37.9267195], [-100.7197613, 37.9267195], [-100.7197614, 37.9265483], [-100.7197653, 37.9263681], [-100.7192492, 37.9263589], [-100.718921, 37.9263575], [-100.7189234, 37.9260172], [-100.7184264, 37.9260161], [-100.7184245, 37.9258315], [-100.7184284, 37.9253979], [-100.7191241, 37.9254001], [-100.719126, 37.9253251], [-100.7191263, 37.9251287], [-100.7191306, 37.9247127], [-100.719131, 37.9246704], [-100.7193174, 37.9235621], [-100.7193347, 37.9217785]]]}
```

Error is:

```
MapperParsingException[failed to parse [location]]; nested: InvalidShapeException[Self-intersection at or near point (-100.7199739, 37.9271115, NaN)];
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:463)
    at org.elasticsearch.index.mapper.DocumentParser.parseObjectOrField(DocumentParser.java:309)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:326)
    at org.elasticsearch.index.mapper.DocumentParser.parseObject(DocumentParser.java:252)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:122)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:309)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:529)
    at org.elasticsearch.index.shard.IndexShard.prepareCreateOnPrimary(IndexShard.java:506)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:215)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:224)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:158)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:66)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:639)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:279)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:271)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: com.spatial4j.core.exception.InvalidShapeException: Self-intersection at or near point (-100.7199739, 37.9271115, NaN)
    at com.spatial4j.core.shape.jts.JtsGeometry.validate(JtsGeometry.java:125)
    at org.elasticsearch.common.geo.builders.ShapeBuilder.jtsGeometry(ShapeBuilder.java:94)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:165)
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:446)
    ... 21 more
```

On elastic 2.3.2
</comment><comment author="nknize" created="2016-06-07T21:44:33Z" id="224424206">Thanks for the issue @smithsimonj 

The second parse exception is not a bug. If you look close at the shape there is a self intersection at `[-100.7199739, 37.9271115]`. Remove that coordinate and ES is happy.

![self-intersection](https://cloud.githubusercontent.com/assets/830187/15875554/21ec45a6-2cce-11e6-8ed8-a0583f5d45e0.png)

I'll dig deeper into the Antarctica parse exception. At quick glance there are a handful of coordinates at `180.000000000000171` longitude. This meridian gets translated to a valid -179.999999999999829.  There is a close point at `[-179.999999999999886, -90.0]`. So its possible this is suffering from round-off error but I'll look closer.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invoke `IndexingOperationListeners` also when recovering from store or remote</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17406</link><project id="" key="" /><description>Today we don't invoke `IndexingOperationListeners` when we are running
a recovery form store or replaying translog from remote. This is problematic since
the actual code path for indexing is different between normal indexing and recovery.
An important detail is left out on recovery since we implemented the `IndexingMemoryController`
as an `IndexingOperationListener` we might never flush the `IndexWriter` of a recovering shard
which can lead to `OOMs` on node startup / recovery.
</description><key id="144573253">17406</key><summary>Invoke `IndexingOperationListeners` also when recovering from store or remote</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>critical</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T12:28:34Z</created><updated>2016-03-31T09:06:50Z</updated><resolved>2016-03-30T13:33:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-30T12:29:01Z" id="203407688">@mikemccand @bleskes FYI
</comment><comment author="mikemccand" created="2016-03-30T13:11:03Z" id="203424556">LGTM, thanks @s1monw.

Very nice to share the same index/delete code in `IndexShard` in the recovery case too.
</comment><comment author="bleskes" created="2016-03-30T13:44:18Z" id="203439973">This is great. It does influence indexing stats in a way I don't think we want, but we should fix this in the stats listener not here. I'll will open a follow up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Wrong .asc file created for tar.gz distribution during deployment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17405</link><project id="" key="" /><description>When running a gradle deployment the `.asc` file is name `elasticsearch-5.0.0-alpha1.gz.asc` instead of `elasticsearch-5.0.0-alpha1.tar.gz.asc`, which very likely not only confuses the smoke tester.
</description><key id="144570573">17405</key><summary>Build: Wrong .asc file created for tar.gz distribution during deployment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>blocker</label><label>build</label><label>v5.0.0-alpha2</label></labels><created>2016-03-30T12:16:25Z</created><updated>2016-04-15T15:14:42Z</updated><resolved>2016-04-15T15:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Error "Field name cannot contain dot" trying to update a nested date field format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17404</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.1

**JVM version**: 1.8.0_66

**OS version**: Ubuntu 12.04.5 LTS

**Description of the problem including expected versus actual behavior**:
After migrating ES from 1.6.0 to 2.2.1 I want to update the field format for my date fields from dateOptionalTime to strict_date_optional_time. This works fine for 'normal' (here: my_field1) fields, but fails for nested fields (here: my_field2.my_subfield).

Sample mapping:

```
{
  "my_index": {
    "mappings": {
      "my_type": {
        "properties": {
          "my_field1": {
            "type": "date",
            "format": "strict_date_optional_time||epoch_millis"
          },
          "my_field2": {
            "properties": {
              "my_subfield": {
                "type": "date",
                "format": "epoch_millis||dateOptionalTime"
              }
            }
          }
        }
      }
    }
  }
}

```

Im am trying to update the field format for the nested date field my_subfield like this:

```
PUT /my_index/_mapping/my_type?update_all_types 
{
  "properties": {
    "my_field2.my_subfield": {
      "type": "date",
      "format": "strict_date_optional_time||epoch_millis"
    }
  }
}
```

and receive this error:

```
{
  "error": {
    "root_cause": [
      {
        "type": "mapper_parsing_exception",
        "reason": "Field name [my_field2.my_subfield] cannot contain '.'"
      }
    ],
    "type": "mapper_parsing_exception",
    "reason": "Field name [my_field2.my_subfield] cannot contain '.'"
  },
  "status": 400
}
```

I understand that from 2.0 onwards, field names may no longer contain a dot in their name - but this is no field with a dot in the name but a nested field.
</description><key id="144570401">17404</key><summary>Error "Field name cannot contain dot" trying to update a nested date field format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peteradh</reporter><labels /><created>2016-03-30T12:15:39Z</created><updated>2016-03-30T15:28:53Z</updated><resolved>2016-03-30T15:28:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="JD557" created="2016-03-30T14:59:46Z" id="203476313">I believe that right now, you have to write the full mapping, i.e.:

```
PUT /my_index/_mapping/my_type?update_all_types 
{
  "properties": {
    "my_field2": {
      "properties": {
        "my_subfield": {
          "type": "date",
          "format": "strict_date_optional_time||epoch_millis"
        }
      }
    }
  }
}
```

You might be interested in following issue https://github.com/elastic/elasticsearch/issues/15951
</comment><comment author="peteradh" created="2016-03-30T15:28:53Z" id="203488514">Thanks a lot, Jo&#227;o! This solved the issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove reference to utils for generating REST docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17403</link><project id="" key="" /><description>This removes the reference to a no longer existing utils directory that used to be there for generating docs and tests from Java source.

According to @karmi and @HonzaKral there is no more support for this type of doc/test generation.
</description><key id="144557236">17403</key><summary>Remove reference to utils for generating REST docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T11:37:39Z</created><updated>2016-03-30T19:18:41Z</updated><resolved>2016-03-30T19:18:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HonzaKral" created="2016-03-30T13:03:50Z" id="203421974">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ability to specify arbitrary node attributes with `node.` prefix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17402</link><project id="" key="" /><description>Today the basic node settings like `node.data` and `node.master` can't really be fully validated
since we allow to specify custom user attributes on the node level. We have to, in order to
support that, add a wildcard setting for `node.*` to let these setting pass validation.
Instead we should require a more contraint prefix like `node.attr.` that defines a namespace
that is reserved for user attributes.
This commit adds a new namespace for attributes in `node.attr`.

Closes #17280
</description><key id="144556148">17402</key><summary>Remove ability to specify arbitrary node attributes with `node.` prefix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T11:31:04Z</created><updated>2016-03-30T12:14:26Z</updated><resolved>2016-03-30T12:14:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-30T11:36:54Z" id="203390063">I think this page needs also an update: https://github.com/elastic/elasticsearch/blob/master/docs/reference/modules/cluster/allocation_awareness.asciidoc
</comment><comment author="dadoonet" created="2016-03-30T11:40:38Z" id="203391053">Should we also change anything for this line? https://github.com/elastic/elasticsearch/blob/master/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy#L136
</comment><comment author="s1monw" created="2016-03-30T11:45:41Z" id="203393529">&gt; Should we also change anything for this line? https://github.com/elastic/elasticsearch/blob/master/buildSrc/src/main/groovy/org/elasticsearch/gradle/test/NodeInfo.groovy#L136

no - this is a real setting?
</comment><comment author="dadoonet" created="2016-03-30T11:55:50Z" id="203396077">&gt; no - this is a real setting?

I have no idea... Just found this file while looking at your PR. But you wrote at some point that it was an "official" setting: https://github.com/elastic/elasticsearch/commit/9b3559f4732e1902f06ca9995a1f9027c61933bd

&gt; `tests.portsfile` is now an official setting and has been renamed to `node.portsfile`
</comment><comment author="dadoonet" created="2016-03-30T12:03:38Z" id="203397658">The change looks good to me. I'm fine with the namespace `node.attr` BTW.
May be someone else would like to review it?
</comment><comment author="javanna" created="2016-03-30T12:09:51Z" id="203400089">looks great, I think we need to update also `filtering.asciidoc`
</comment><comment author="s1monw" created="2016-03-30T12:12:35Z" id="203401173">@javanna @dadoonet updated the PR
</comment><comment author="javanna" created="2016-03-30T12:13:26Z" id="203401452">LGTM I am so happy this gets in!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PATH.DATA  Failed to obtain node lock, is the following location writable? </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17401</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:2.2.1

**JVM version**:1.8.0_73

**OS version**:Ubuntu 12.04.3 LTS

**Description of the problem including expected versus actual behavior**:
I want set path.data in elasticsearch.yml  like 
"
path.data: /mnt/disk1/elasticsearch-data,/mnt/disk1/elasticsearch-data,/mnt/disk3/elasticsearch-data,/mnt/disk4/elasticsearch-data,/mnt/disk5/elasticsearch-data,/mnt/disk6/elasticsearch-data,/mnt/disk7/elasticsearch-data,/mnt/disk8/elasticsearch-data,/mnt/disk9/elasticsearch-data,/mnt/disk10/elasticsearch-data,/mnt/disk11/elasticsearch-data,/mnt/disk12/elasticsearch-data
" 
then run 
./elasticsearch -d
user@24110:/data/elk/elasticsearch/bin$ Exception in thread "main" java.lang.IllegalStateException: Failed to obtain node lock, is the following location writable?: [/mnt/disk1/elasticsearch-data/elasticsearch, /mnt/disk1/elasticsearch-data/elasticsearch, /mnt/disk3/elasticsearch-data/elasticsearch, /mnt/disk4/elasticsearch-data/elasticsearch, /mnt/disk5/elasticsearch-data/elasticsearch, /mnt/disk6/elasticsearch-data/elasticsearch, /mnt/disk7/elasticsearch-data/elasticsearch, /mnt/disk8/elasticsearch-data/elasticsearch, /mnt/disk9/elasticsearch-data/elasticsearch, /mnt/disk10/elasticsearch-data/elasticsearch, /mnt/disk11/elasticsearch-data/elasticsearch, /mnt/disk12/elasticsearch-data/elasticsearch]
        at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:199)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:153)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
i don't know  path.data  is sure ?
</description><key id="144555316">17401</key><summary>PATH.DATA  Failed to obtain node lock, is the following location writable? </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">renweijun</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2016-03-30T11:27:17Z</created><updated>2017-03-25T15:27:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-30T20:38:17Z" id="203624981">I'm wondering if your data paths are being taken as though they were one single path, but I've tried to replicate this and can't.

Could you upload your full config file please?
</comment><comment author="renweijun" created="2016-03-31T07:33:49Z" id="203797651">```
cluster:
    name:elasticsearch
node:
    name: "elk24110"
    master: true
    data: true
index:
    number_of_shards: 5
    number_of_replicas: 2
path:
    data: /mnt/disk1/elasticsearch-data,/mnt/disk1/elasticsearch-data,/mnt/disk3/elasticsearch-data,/mnt/disk4/elasticsearch-data,/mnt/disk5/elasticsearch-data,/mnt/disk6/elasticsearch-data,/mnt/disk7/elasticsearch-data,/mnt/disk8/elasticsearch-data,/mnt/disk9/elasticsearch-data,/mnt/disk10/elasticsearch-data,/mnt/disk11/elasticsearch-data,/mnt/disk12/elasticsearch-data
    conf: /data/elk/elasticsearch/config
    work: /data/elk/elasticsearch/work
    logs: /data/elk/elasticsearch/logs
    plugins: /data/elk/elasticsearch/plugins
bootstrap:
    mlockall: true
network.bind_host: 192.168.241.10
network.publish_host: 192.168.241.10  
transport.tcp.port: 9300  
transport.tcp.compress: true  
http.port: 9200  
discovery.zen.minimum_master_nodes: 1  
discovery.zen.ping.timeout: 10s  
discovery.zen.ping.multicast.enabled: false  
discovery.zen.ping.unicast.hosts: ["192.168.241.10:9300","192.168.241.11:9300","192.168.241.12:9300","192.168.241.13:9300","192.168.241.14:9300"]
```
</comment><comment author="renweijun" created="2016-03-31T07:40:17Z" id="203800934">I later modified the configuration file is as follows, run successfully, do not know whether it is bug
![qq 20160331153610](https://cloud.githubusercontent.com/assets/3645697/14168744/9eb941ae-f756-11e5-9057-6d9f7fba1522.png)
</comment><comment author="liuzhenji" created="2017-03-25T15:27:27Z" id="289218698">renweijun,&#20219;&#21355;&#20891;&#65311;</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a soft limit on the mapping depth.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17400</link><project id="" key="" /><description>This commit adds the new `index.mapping.depth.limit` setting which controls the
maximum mapping depth that is allowed. It has a default value of 20.
</description><key id="144525838">17400</key><summary>Add a soft limit on the mapping depth.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T09:59:43Z</created><updated>2016-04-06T07:40:47Z</updated><resolved>2016-03-30T12:37:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-30T10:24:19Z" id="203369432">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Allow for file based deploy, sign packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17399</link><project id="" key="" /><description>This allows for a local file based deploy without needed nexus
auth information.

Also signing of packages has been added, either via gradle.properties
or using system properties as a fallback.

The property build.repository allows to configure another endpoint if no
snapshot build is done.
</description><key id="144511545">17399</key><summary>Build: Allow for file based deploy, sign packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>blocker</label><label>build</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-30T09:05:30Z</created><updated>2016-04-15T15:15:51Z</updated><resolved>2016-04-15T15:14:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-04T22:42:53Z" id="205525259">I looked into the issue about the asc file for tar.gz being named incorrectly. In order to fix it, in `distribution/tar/build.gradle`, remove the `archives buildTar` line, and add the following:

```
project.afterEvaluate {
  // gradle is completely broken for extensions that contain a dot, so we must be explicit
  project.signArchives.singleSignature.type = 'tar.gz.asc'
}
```
</comment><comment author="spinscale" created="2016-04-06T09:51:27Z" id="206268251">added the tar.gz fix as well, works. thx!
</comment><comment author="rjernst" created="2016-04-14T18:36:36Z" id="210091976">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Factor out slow logs into Search and IndexingOperationListeners</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17398</link><project id="" key="" /><description>This commit introduces SearchOperationListeneres which allow to hook
into search operation lifecycle and execute operations like slow-logs
and statistic collection in a transparent way. SearchOperationListenrs
can be registered on the IndexModule just like IndexingOperationListeners.
The main consumers (slow log) have already been moved out of IndexService
into IndexModule which reduces the dependency on IndexService as well as
IndexShard and makes slowlogging transparent.
</description><key id="144505371">17398</key><summary>Factor out slow logs into Search and IndexingOperationListeners</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>:Logging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T08:41:48Z</created><updated>2016-03-30T12:39:12Z</updated><resolved>2016-03-30T12:39:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-30T12:23:13Z" id="203405320">left two minor comments, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update bool-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17397</link><project id="" key="" /><description>I thought it is more clear this way, I've seen it on Lucene documentation, feel free to decline the pull request if the english is broken.
</description><key id="144498189">17397</key><summary>Update bool-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Serguzest</reporter><labels /><created>2016-03-30T08:11:28Z</created><updated>2016-03-30T20:18:35Z</updated><resolved>2016-03-30T20:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-30T20:18:35Z" id="203614615">Hi @Serguzest 

Thanks for the PR.  After reading the before and after several times, i think I prefer the before version :)  Thanks anyway, and feel free to contribute more in the future
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a soft limit on the number of shards that can be queried in a single search request.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17396</link><project id="" key="" /><description>This commit adds the new `action.search.shard_count.limit` setting which
configures the maximum number of shards that can be queried in a single search
request. It has a default value of 1000.
</description><key id="144497750">17396</key><summary>Add a soft limit on the number of shards that can be queried in a single search request.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T08:08:31Z</created><updated>2016-04-06T07:40:47Z</updated><resolved>2016-03-30T14:56:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-30T12:58:37Z" id="203420212">The goal was initially to not apply this limit to count requests, but it is not trivial to check and after some discussions with @clintongormley, this is not so much of an issue since the limit is easy to change and you can get counts of docs in indices using eg. `_cat/count` which has no limit (and does not need one).
</comment><comment author="jimczi" created="2016-03-30T13:13:02Z" id="203425936">@jpountz thanks for the explanation. 
LGTM
</comment><comment author="jpountz" created="2016-03-30T15:00:41Z" id="203477090">Thanks @jimferenczi !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_cat/indices: race condition/exception thrown when creating/deleting indices rapidly </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17395</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch Versions**:
2.1.x - 2.2.x

**JVM Version**:
OpenJDK 64-Bit Server VM (build 25.77-b03, mixed mode)

**Description of the problem including expected versus actual behavior**:

It looks like if you create and delete indices very rapidly you can sometimes end up getting a 404 index_not_found_exception from the `/_cat/indices` API. So basically rather than seeing the indices that still exist, you get an exception because one index was created and deleted between when the endpoint resolves indices and then call indices stats api for those indices.

**Steps to reproduce**:

Attached is a script that should hopefully reproduce the problem, but it can take some time (few minutes  depending on the ability of your ES cluster to handle responses :)

[cat-indices-issue.sh.zip](https://github.com/elastic/elasticsearch/files/194945/cat-indices-issue.sh.zip)

The script is Bash. It will run a number of background processes that simply spin creating and then deleting an index with the REST API.  The script waits until the output of `_cat/indices` produces the exception, then exits. **NOTE:** It will clean-up any background processes it creates but won't clean up any test indices that might still exist.

When the script dies, it will produce output like:

```
{"error":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"testindex3","index":"testindex3"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"testindex3","index":"testindex3"},"status":404}
...output of the script cleaning up after itself...
```
</description><key id="144478965">17395</key><summary>/_cat/indices: race condition/exception thrown when creating/deleting indices rapidly </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>:Allocation</label><label>adoptme</label><label>bug</label></labels><created>2016-03-30T06:38:44Z</created><updated>2016-05-25T08:02:15Z</updated><resolved>2016-05-25T08:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kwilczynski" created="2016-03-30T06:47:36Z" id="203278086">@joshuar thank you for logging this! (via @faeldt)

To add, sometimes the `"type": "null_pointer_exception"` (as an addition to the above; sorry I don't have full JSON output at hand) would show up when querying `/_cat/indices` or `/_cat/shards`, etc.

Since this is a machine parseable output, it would be delightful for it to suppress any error and just not return JSON (or any errors for that matter, unless you add `?e=true` or something).
</comment><comment author="clintongormley" created="2016-03-30T19:35:08Z" id="203599485">@kwilczynski if you happen to come across this NPE again, please paste the stack trace from the logs into this issue
</comment><comment author="kwilczynski" created="2016-03-31T01:34:26Z" id="203709092">@clintongormley I will try to fish something on the Elasticsearch side, as since I fixed this on my side (to retry on error, and I don't log in debug in production, I lost it for now, sadly).
</comment><comment author="kwilczynski" created="2016-03-31T02:40:33Z" id="203724362">@clintongormley I found the following NPE going through the older logs:

```
RemoteTransportException[[prd11-c-tky-master-search-catalogpf][10.184.20.209:9300][cluster:monitor/stats[n]]]; nested: NullPointerException;
:Caused by: java.lang.NullPointerException
     at org.elasticsearch.action.admin.cluster.health.ClusterIndexHealth.&lt;init&gt;(ClusterIndexHealth.java:73)
     at org.elasticsearch.action.admin.cluster.stats.TransportClusterStatsAction.nodeOperation(TransportClusterStatsAction.java:127)
     at org.elasticsearch.action.admin.cluster.stats.TransportClusterStatsAction.nodeOperation(TransportClusterStatsAction.java:58)
     at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:211)
     at org.elasticsearch.action.support.nodes.TransportNodesAction$NodeTransportHandler.messageReceived(TransportNodesAction.java:207)
     at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
     at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
     at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="clintongormley" created="2016-03-31T17:36:36Z" id="204043760">thanks @kwilczynski 
</comment><comment author="djschny" created="2016-05-09T15:46:22Z" id="217903474">Note I also am receiving similar with `_cat/indices` on ES 2.3.2. It happens if I'm running a node with multiple `path.data` entries and remove one of the drives that was on the `path.data`:

```
{
   "error": {
      "root_cause": [
         {
            "type": "null_pointer_exception",
            "reason": null
         }
      ],
      "type": "null_pointer_exception",
      "reason": null
   },
   "status": 500
}
```

Withe following stacktrace from the node:

```
[2016-05-09 11:40:24,520][WARN ][rest.suppressed          ] /_cat/indices Params: {}
java.lang.NullPointerException
    at org.elasticsearch.rest.action.cat.RestIndicesAction.buildTable(RestIndicesAction.java:331)
    at org.elasticsearch.rest.action.cat.RestIndicesAction.access$100(RestIndicesAction.java:52)
    at org.elasticsearch.rest.action.cat.RestIndicesAction$1$1$1.buildResponse(RestIndicesAction.java:97)
    at org.elasticsearch.rest.action.cat.RestIndicesAction$1$1$1.buildResponse(RestIndicesAction.java:94)
    at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
    at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:89)
    at org.elasticsearch.action.support.TransportAction$1.onResponse(TransportAction.java:85)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.onCompletion(TransportBroadcastByNodeAction.java:378)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.onNodeResponse(TransportBroadcastByNodeAction.java:347)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction$1.handleResponse(TransportBroadcastByNodeAction.java:319)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction$1.handleResponse(TransportBroadcastByNodeAction.java:311)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:819)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:803)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:793)
    at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:58)
    at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:134)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:412)
    at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$BroadcastByNodeTransportRequestHandler.messageReceived(TransportBroadcastByNodeAction.java:386)
    at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:33)
    at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:75)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:376)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor: replace all ocurrences of ESTestCase.getRandom() with LuceneTestCase.random()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17394</link><project id="" key="" /><description>Simple refactor.
</description><key id="144454409">17394</key><summary>Refactor: replace all ocurrences of ESTestCase.getRandom() with LuceneTestCase.random()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">camilojd</reporter><labels><label>:Core</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-30T03:34:53Z</created><updated>2016-03-30T17:40:04Z</updated><resolved>2016-03-30T12:58:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-30T12:04:53Z" id="203397919">Looks good to me. I'll see about merging in a few minutes.
</comment><comment author="nik9000" created="2016-03-30T13:00:19Z" id="203421097">Thanks @camilojd ! I had no idea that old TODO was in there and have been using getRandom the whole time. I'm happy to have just one way to do it now.
</comment><comment author="camilojd" created="2016-03-30T17:40:04Z" id="203545710">@nik9000 You're welcome! Glad to help :smiley:
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get UP-TO-DATE checks working in more tasks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17393</link><project id="" key="" /><description>Together these drop subsequent executions of `gradle precommit` from 100 seconds to 18 seconds. Most of that is thirdPartyAudit. thirdPartyAudit will now only run if you change dependencies, change its exclusion list, or clean.
</description><key id="144426880">17393</key><summary>Get UP-TO-DATE checks working in more tasks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-30T00:36:16Z</created><updated>2016-04-11T22:53:38Z</updated><resolved>2016-04-11T21:48:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T01:02:51Z" id="203700736">@rjernst can you have a look?
</comment><comment author="nik9000" created="2016-04-11T13:04:03Z" id="208330051">@rjernst I had a go at making this nicer. Let me know what you think when you get a chance. Thanks!
</comment><comment author="rjernst" created="2016-04-11T19:40:53Z" id="208521458">@nik9000 I left some more comments. In general, tasks should always be completely set up before execution, but they should not initialize things until after configuration, so this often requires delaying setup of auxiliary things in an afterEvaluate block of the ctor.
</comment><comment author="nik9000" created="2016-04-11T21:02:05Z" id="208559266">Ok - I've moved a bunch of stuff into afterEvaluate.
</comment><comment author="nik9000" created="2016-04-11T21:06:03Z" id="208560308">@rjernst I had a bunch of visibility trouble - closures can't see the properties of the class they were made from kind of stuff. But I got it working. It is ugly and I'm sure there is some groovy thing I'm missing, but yeah.
</comment><comment author="rjernst" created="2016-04-11T21:21:48Z" id="208568228">LGTM, just one suggestion to try to avoid needing setters
</comment><comment author="nik9000" created="2016-04-11T21:40:18Z" id="208574347">&gt; one suggestion to try to avoid needing setters

That worked. It came in as I was rebasing/squashing/pushing, so I've pushed it with the rebased changes if you'd like to see.
</comment><comment author="nik9000" created="2016-04-11T21:49:24Z" id="208576965">Or not. I was half way through committing it and my master had it on the end so. So I just pushed it to our master because I figured it was good. If you don't like it I'm happy to change anything.
</comment><comment author="rjernst" created="2016-04-11T22:53:38Z" id="208602051">Looks great, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort no longer supports unmapped_type for _score field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17392</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master

**JVM version**: 1.8.0_60

**OS version**: OSX

**Description of the problem including expected versus actual behavior**:

Issuing a query with a sort on the _score field with `unmapped_type` set will return a 400 with the message: `[_score] failed to parse field [unmapped_type]`. This used to work up until very recently, because Kibana sends `unmapped_type: boolean` by default with searches from the Discover tab. I'm wondering if this was a regression or an intentional change, because I can't find any mention of it in any GH issue or PR. `unmapped_type` does still work for regular fields, so I'm not sure if this is just _score, or any meta field.

**Steps to reproduce**:
1. POST a search with the following body `{"sort":[{"_score":{"order":"desc","unmapped_type":"boolean"}}]}`
2. Observe the 400 error

Kibana issue I created for more info about how we use unmapped_type: https://github.com/elastic/kibana/issues/6698
</description><key id="144407362">17392</key><summary>Sort no longer supports unmapped_type for _score field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bargs</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-03-29T22:28:04Z</created><updated>2016-04-06T10:29:06Z</updated><resolved>2016-04-06T10:29:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-30T15:43:27Z" id="203496153">It seems to be a side effect of the query refactoring efforts since all the request parsing logic got rewritten.

I am tempted to consider that the bug was to accept this parameter when sorting on `_score` since the `_score` is available on all shards and is not "mapped" anyway. Other sort options like script sort or geo-distance sort do not support this option either.
</comment><comment author="Bargs" created="2016-03-30T17:35:04Z" id="203543658">@jpountz are there any other meta fields that are sortable but don't support `unmapped_type`?
</comment><comment author="cbuescher" created="2016-03-31T09:00:46Z" id="203833648">Yes, @jpountz is right, this is a side effect of the refactoring. I just checked the old parser logic. While it parsed the "unmapped_type" parameter, it silentely ignored it, so I think the current behaviour of throwing an error is better. The only paramter that sort on `_score` now accepts is "order". All other metadata fields should accept the same parameters as any other field, however I'm not sure how useful they are in some cases. Sorting on `_doc` will accept all parameters as any other field but ignore them except for "order".

We can restore the previous behaviour of silently ignoring paramters like "missing", "unmapped_type" or "mode", but I think the current solution of treating it separately is better. Maybe @MaineC has something to add.
</comment><comment author="MaineC" created="2016-03-31T09:32:34Z" id="203850053">+0 for keeping the current behaviour, it also seems more in line with changes we made to other parsers where we added exceptions e.g. to ambiguous parameters.
</comment><comment author="cbuescher" created="2016-03-31T17:08:32Z" id="204025709">@Bargs it it okay for you if we leave the current behaviour? @clintongormley any opinion on whether we should mark this as breaking in the docs or any of the migration guides? As mentioned above, the setting never had any effect for "_score", we only silently ignored it so far. 
</comment><comment author="Bargs" created="2016-03-31T19:17:22Z" id="204086237">@cbuescher yeah, I think it's reasonable to leave it as is. I've committed a fix to Kibana which only required a small change. I would recommend marking it as a breaking change since it does have the potential to cause issues.
</comment><comment author="clintongormley" created="2016-04-04T18:57:24Z" id="205447138">@cbuescher this was undocumented and I wouldn't mark it as breaking.  The one question I have is whether `_doc` should accept `order`? Seems to defeat the object of using `_doc`, no?
</comment><comment author="cbuescher" created="2016-04-04T19:48:59Z" id="205466082">@clintongormley I agree there are no real use cases, we could also ignore it I guess. On the java API side, if we would want to also throw errors when "_doc" is used with "missing", "unmapped_type" etc.. rather than silently ignore it, we would need to add another builder class (like ScoreSortBuilder) for it. Not sure if thats worth the effort.
</comment><comment author="clintongormley" created="2016-04-06T10:29:06Z" id="206295700">@cbuescher i'm fine with leaving as is
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[windows] Service command still had positional start command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17391</link><project id="" key="" /><description>Which caused the service to fail to start on after installing the service from a `master` snapshot build.

/cc @elastic/microsoft @simonw 
</description><key id="144393460">17391</key><summary>[windows] Service command still had positional start command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T21:28:10Z</created><updated>2016-03-31T10:33:44Z</updated><resolved>2016-03-30T19:07:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2016-03-29T21:51:10Z" id="203126738">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't try to use system jna for naming conventions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17390</link><project id="" key="" /><description>When we test we add `-Djna.nosys=true` to the system properties but
we don't add it to system properties when running the naming conventions
test. This was causing the build to fail on a newly minted Ubuntu 15.10
machine, presumably because I made the mistake of installing maven using
the system package manager.
</description><key id="144366141">17390</key><summary>Don't try to use system jna for naming conventions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T19:40:25Z</created><updated>2016-03-29T21:54:00Z</updated><resolved>2016-03-29T21:53:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-29T19:40:40Z" id="203067702">@rjernst I'm not sure this is the right way to go about this, but at least it is consistent.
</comment><comment author="rjernst" created="2016-03-29T21:18:26Z" id="203109361">LGTM.
</comment><comment author="nik9000" created="2016-03-29T21:54:00Z" id="203127471">Thanks for the review @rjernst !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from regular aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17389</link><project id="" key="" /><description>We still use PROTOTYPE in pipeline aggregations and the SignificanceHeuristicStreams, both of which can wait for other PRs. This one is large enough.

I decided to abandon doWriteTo/innerWriteTo in favor of all the classes just overriding writeTo. We were ending up with just an intense number of these. To the point where the security you get from forcing the implementer to define the method wasn't worth the complexity.
</description><key id="144357008">17389</key><summary>Remove PROTOTYPE from regular aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>stalled</label></labels><created>2016-03-29T19:02:19Z</created><updated>2016-04-13T09:59:05Z</updated><resolved>2016-04-11T21:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-29T19:02:29Z" id="203052684">@javanna another.
</comment><comment author="nik9000" created="2016-03-30T16:41:51Z" id="203518462">@javanna this is almost the same as the score function one but I did less test work along with it so it is more "pure". Still huge, sadly.
</comment><comment author="nik9000" created="2016-03-30T16:42:26Z" id="203518747">Your comment in the score function one about keeping the read unction out of the parser is a good one. I can move the registration here too.
</comment><comment author="javanna" created="2016-04-01T18:37:26Z" id="204508155">I am totally tempted to leave this one to @colings86 . You are welcome Colin :)
</comment><comment author="nik9000" created="2016-04-02T01:47:29Z" id="204625438">@colings86 this PR was from before I figured out that I had to do them smaller and more incrementally. So it is something of a mess. It was also before I hit on the idea that the parsers should function interfaces - so parsers are still forwarding the read method to the constructor. All and all it isn't where I'd like it to end up but it is a step. It is just in the wrong order compared to what I'd proposed for queries.

I think maybe we should wait on this PR until after we've settled on the registration issue over in the query PR.
</comment><comment author="colings86" created="2016-04-04T10:06:19Z" id="205226347">I think we should leave this PR for now until we have removed the PROTOTYPE object from the query builders. By then we should have a complete plan of how best to do this and we can apply it consistently across the other parts of the codebase. I am concerned that if we try to scatter these changes around the codebase before completely one area we will end up with a lot of different ways of doing this rather than one consistent way everyone knows and can easily follow.
</comment><comment author="nik9000" created="2016-04-11T00:54:28Z" id="208104197">@colings86 what do you want to do with this? I'm going to be ready to pick this up on Monday morning or so. I can keep going forwards, making this larger and large. Or I can abandon it and we can do it in stages like we did for the query parsers. I'm fine with whatever makes the review easiest for you.
</comment><comment author="colings86" created="2016-04-11T10:14:16Z" id="208268350">I would personally prefer abandoning this PR and doing this in stages since this PR will be very large and hard to review currently. It also means that any general problems arising from the refactoring can be sorted out quickly on the first small PR rather than having to make the fixes across all the aggregations because all the aggs get affected
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ExtendedStatsAggregator should also pass sigma to emtpy aggs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17388</link><project id="" key="" /><description>Because sigma is also used at reduce time, it should be passed to empty aggs.
Otherwise it causes bugs when an empty aggregation is used to perform reduction
is it would assume a sigma of zero.

Closes #17362
</description><key id="144327943">17388</key><summary>ExtendedStatsAggregator should also pass sigma to emtpy aggs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.3.2</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-03-29T17:08:55Z</created><updated>2016-04-07T07:54:19Z</updated><resolved>2016-04-07T07:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-04-06T09:05:52Z" id="206239957">@jpountz I left a comment but apart from that it LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] update doc on bwc tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17387</link><project id="" key="" /><description /><key id="144315912">17387</key><summary>[TEST] update doc on bwc tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2016-03-29T16:23:20Z</created><updated>2016-03-29T16:39:16Z</updated><resolved>2016-03-29T16:39:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-29T16:34:28Z" id="202987451">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable fielddata on text fields by defaults.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17386</link><project id="" key="" /><description>`text` fields will have fielddata disabled by default. Fielddata can still be
enabled on an existing index by setting `fielddata=true` in the mappings.
</description><key id="144312233">17386</key><summary>Disable fielddata on text fields by defaults.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T16:10:00Z</created><updated>2016-04-06T07:41:34Z</updated><resolved>2016-03-30T12:36:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-29T20:51:46Z" id="203096786">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add scoring support to the percolator query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17385</link><project id="" key="" /><description>Percolator query documents are scored based on how well they match with the document being percolated.

Before the percolator query emitted a constant score.

Closes #13827
</description><key id="144310454">17385</key><summary>Add scoring support to the percolator query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T16:03:02Z</created><updated>2016-03-31T08:12:12Z</updated><resolved>2016-03-31T08:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T17:12:59Z" id="203004942">I left a minor comment. I think we should also improve explain to include the explanation of the percolated query?
</comment><comment author="martijnvg" created="2016-03-30T11:06:47Z" id="203380719">@jpountz I've updated the PR.
</comment><comment author="jpountz" created="2016-03-30T15:08:50Z" id="203481238">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename LinearInterpoatingScorer to LinearInterpo**l**atingScorer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17384</link><project id="" key="" /><description>Class is just plain spelled wrong.
</description><key id="144266279">17384</key><summary>Rename LinearInterpoatingScorer to LinearInterpo**l**atingScorer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>low hanging fruit</label></labels><created>2016-03-29T13:33:16Z</created><updated>2016-03-30T00:46:10Z</updated><resolved>2016-03-30T00:45:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T16:24:17Z" id="202984265">+1 No need for a pull request for such a trivial change. Let's just rename, test and push.
</comment><comment author="nik9000" created="2016-03-29T16:37:49Z" id="202988655">I thought maybe it is a good thing for a starting contributor?
</comment><comment author="jasontedor" created="2016-03-30T00:46:10Z" id="203172709">I took this while waiting for some test VMs to bootstrap.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update task management docs to reflect the latest changes in the interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17383</link><project id="" key="" /><description>Brings docs in line with new list task syntax and adds task cancellation API docs.
</description><key id="144265534">17383</key><summary>Update task management docs to reflect the latest changes in the interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>docs</label><label>review</label><label>v2.3.1</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T13:31:07Z</created><updated>2016-03-29T16:31:33Z</updated><resolved>2016-03-29T16:31:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-29T13:35:51Z" id="202896730">These look right to me. @clintongormley should probably also review?
</comment><comment author="clintongormley" created="2016-03-29T13:37:42Z" id="202897396">LGTM.  Based on these changes, I'm thinking that the REST endpoints should change to `cluster.get_tasks` and `cluster.cancel_tasks`  (plus a note on the cluster pending tasks docs to distinguish them from these tasks)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[WIP] Add docs to template support for _msearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17382</link><project id="" key="" /><description>This adds initial documentation on how to use templating in the _msearch resource. Examples are pretty basic and based on what was given in the original issue and PR

Relates to #10885
Relates to #15674

Still need to add the rest api specs and rest integration test as requested in #15674 - need to read up on how those work, as it's been quite a while since last time I touched those.
</description><key id="144259130">17382</key><summary>[WIP] Add docs to template support for _msearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>docs</label><label>v5.0.0-beta1</label><label>WIP</label></labels><created>2016-03-29T13:08:31Z</created><updated>2016-09-13T11:19:25Z</updated><resolved>2016-09-13T11:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-03-30T13:25:34Z" id="203431094">Added spec in last commit. Did some digging afterwards, seems like we actually do have REST tests already that show how to use _msearch in combination with templating, found them in modules/lang-mustache:

https://github.com/elastic/elasticsearch/blob/master/modules/lang-mustache/src/test/resources/rest-api-spec/test/lang_mustache/50_messy_test_msearch.yaml

@HonzaKral Can you take a look, whether the above in combination with the additions of this PR helps/ makes any sense?
</comment><comment author="dakrone" created="2016-07-11T17:02:01Z" id="231797333">LGTM
</comment><comment author="dakrone" created="2016-09-12T21:29:05Z" id="246500097">@MaineC this got a +1, is it still under progress or can it be merged?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename _reindex and _update_by_query's size parameter to max_docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17381</link><project id="" key="" /><description>The `size` parameter in reindex is confusing, mostly because reindex involves scroll and scrolls already have a size parameter. We should rename the parameter to `max_docs`. Also, the parameter is only documented in `reindex.asciidoc`, not in `update-by-query.asciidoc`. It needs to be documented in both places.
</description><key id="144256709">17381</key><summary>Rename _reindex and _update_by_query's size parameter to max_docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>discuss</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T12:58:01Z</created><updated>2016-03-31T16:38:14Z</updated><resolved>2016-03-31T16:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-30T15:48:59Z" id="203497850">I am not sure if `size` is that confusing? On my end I think I like it better than `max_docs`.
</comment><comment author="nik9000" created="2016-03-30T16:30:45Z" id="203514231">@clintongormley this is something you'd talked to me about.
</comment><comment author="clintongormley" created="2016-03-31T10:19:10Z" id="203867463">I'm coming around to prefering `size` over `max_docs`... it is consistent with what we do elsewhere, but...

We have `size` and `scroll_size` (doc'ed in update-by-query but not in reindex), but invalid values for `scroll_size` are reported as a problem with `size`.  I'm also not crazy about the fact that later, when we make the reindexing job distributed, there will be no way of guaranteeing an exact `size`.
</comment><comment author="nik9000" created="2016-03-31T12:52:44Z" id="203920115">&gt; but invalid values for scroll_size are reported as a problem with size.

Yeah, that is a bigger problem I think.
</comment><comment author="nik9000" created="2016-03-31T16:31:13Z" id="204010816">&gt; Yeah, that is a bigger problem I think.

I added a test for scroll_size and some documentation to reindex for it as well in the linked pull request. I believe the issue with it complaining about "size" when there is really a problem with scroll_size is symptom of the hackery I used to backport it. So the test wouldn't pass if I backported it to 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't store response in request cache if it is over a certain percentage of the cache size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17380</link><project id="" key="" /><description>For very large responses storing the response in the request cache may evict large numbers of other cached responses and take up most of the cache size. We should not store responses in the request cache which are over a certain percentage of the total request cache size. We already do something similar to this for the filter cache,
</description><key id="144247578">17380</key><summary>Don't store response in request cache if it is over a certain percentage of the cache size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Cache</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-29T12:21:12Z</created><updated>2016-03-29T16:24:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T16:24:27Z" id="202984314">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevents exception being raised when ordering by an aggregation which wasn't collected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17379</link><project id="" key="" /><description>If a terms aggregation was ordered by a metric nested in a single bucket aggregator which did not collect any documents (e.g. a filters aggregation which did not match in that term bucket) an ArrayOutOfBoundsException would be thrown when the ordering code tried to retrieve the value for the metric. This fix fixes all numeric metric aggregators so they return their default value when a bucket ordinal is requested which was not collected.

Closes #17225
</description><key id="144230866">17379</key><summary>Prevents exception being raised when ordering by an aggregation which wasn't collected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.3.1</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-29T11:11:10Z</created><updated>2016-03-29T14:05:25Z</updated><resolved>2016-03-29T12:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T12:24:35Z" id="202866838">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate sort option reverse.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17378</link><project id="" key="" /><description>Replace with "order" instead.

Relates to #17047
</description><key id="144224145">17378</key><summary>Deprecate sort option reverse.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>deprecation</label><label>v2.3.2</label><label>v2.4.0</label></labels><created>2016-03-29T10:34:22Z</created><updated>2016-04-14T09:31:32Z</updated><resolved>2016-04-14T09:00:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-29T11:07:56Z" id="202833666">@MaineC took a quick look, can you also remove the mention of "reverse" in `docs/reference/search/request/sort.asciidoc` and maybe we shouldn't use it in the `toXContent` of the builders (ScriptSortBuilder and GeoDistanceSortBuilder) any more but use "order" there instead since the internal field and the setters are already called "order".
</comment><comment author="MaineC" created="2016-03-29T11:44:32Z" id="202845058">@cbuescher Makes sense. Thanks for catching this. Changed.
</comment><comment author="cbuescher" created="2016-03-29T12:11:12Z" id="202860787">@MaineC thanks, did you push any changes re the `withAllDeprecated` comment I made above?
</comment><comment author="MaineC" created="2016-03-29T12:36:24Z" id="202874074">&gt; did you push any changes re the withAllDeprecated comment I made above

Thanks for the reminder - completely missed that 
</comment><comment author="MaineC" created="2016-03-30T09:03:02Z" id="203334007">@cbuescher Looks better now?
</comment><comment author="cbuescher" created="2016-03-30T09:07:11Z" id="203335662">Sorry, I missed mentioning that the same comment applies to the parse fields in ScriptSortParser and the SortParseElement. Can you add that change there as well?
</comment><comment author="MaineC" created="2016-03-30T11:34:17Z" id="203389585">Sorry, should have been obvious. Fixed.
</comment><comment author="cbuescher" created="2016-03-30T11:44:49Z" id="203392819">Great, LGTM
</comment><comment author="cbuescher" created="2016-04-06T10:17:38Z" id="206290178">@MaineC has this been merged already and can be closed?
</comment><comment author="MaineC" created="2016-04-14T08:59:46Z" id="209836540">@clintongormley Do we plan to do a 2.4 release (that is, is it enough to merge this to 2.x or should this rather be merged to the 2.3 branch as well)?
</comment><comment author="clintongormley" created="2016-04-14T09:14:29Z" id="209842231">@MaineC this can go to 2.3 as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move registration of custom node attributes to a module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17377</link><project id="" key="" /><description>During review of #16963, this comment came up: https://github.com/elastic/elasticsearch/pull/16963#discussion_r57683730

We should move registration of custom node attributes (used by e.g. the ec2 plugin) to a module.
</description><key id="144223610">17377</key><summary>Move registration of custom node attributes to a module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2016-03-29T10:31:42Z</created><updated>2017-03-12T18:41:45Z</updated><resolved>2017-03-12T18:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2017-03-12T18:41:33Z" id="285964747">CustomNodeAttributes was actually removed in #19348, which was 5.0.0-alpha5.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inner_hits aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17376</link><project id="" key="" /><description>Please, make it possible to aggregate inner_hits. Not all nested objects, but only those that come under the condition of query
</description><key id="144223018">17376</key><summary>Inner_hits aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">a-lustin-parc</reporter><labels /><created>2016-03-29T10:28:15Z</created><updated>2016-03-30T18:48:49Z</updated><resolved>2016-03-30T18:48:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Vineeth-Mohan" created="2016-03-29T15:11:06Z" id="202946650">@a-lustin-parc  - Is it on all the matched children documents or matched documents per hit ?
</comment><comment author="clintongormley" created="2016-03-30T18:48:49Z" id="203575672">@a-lustin-parc inner hits performs a further search request for every result.  This can already be heavy, adding aggregations would be too much. Rather, run a multi-search with an entry for each hit and calculate the aggregations in there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apply index template updates automatically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17375</link><project id="" key="" /><description>**Assume the following situation:**
1. Create index template
2. Create index based on the template
3. Modify the template

**Current behavior:**
The existing index isn't changed.

**Requested feature:**
The template modifications should be (optionally) applied to all matching indices.
This feature is limited: Some values (like existing fields, number of shards) can't be changed. Changes to these values should trigger an error if existing indices should be updated. Introduction of new fields is no problem, especially when dynamic mapping is set to strict mode.
</description><key id="144178500">17375</key><summary>Apply index template updates automatically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sewi-cpan</reporter><labels><label>discuss</label></labels><created>2016-03-29T07:16:15Z</created><updated>2016-03-31T09:20:06Z</updated><resolved>2016-03-31T09:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-30T13:04:56Z" id="203422258">I think the fact that modifying templates would have side effects on existing indices is trappy. The current behaviour is easier to understand/explain to me.
</comment><comment author="s1monw" created="2016-03-30T13:07:27Z" id="203423043">I agree with @jpountz the template should only have effect on index creation this won't work both ways. you can do the same thing with a settings update already?
</comment><comment author="sewi-cpan" created="2016-03-30T14:08:30Z" id="203448252">The documentation isn't clear on "templates only have effect on index creation".

Maintaining a template **and** keeping the existing indices at the same level adds additional maintenance and error possibilities. Keep both at the same level should be an option - and you're still free not to use it.
</comment><comment author="clintongormley" created="2016-03-31T09:20:06Z" id="203840588">Most mapping settings and many index settings (eg number of shards) can't be changed on an existing index, so making existing indices depend on templates would severely limit the usefulness of a template, as you wouldn't be able to change much.

Instead, the way things are today, you can adjust your template as you go forward to take into account eg changes of data size, changes you want to make in your mappings etc.

I have added documentation (18c5ea8599978f2aee657997c33af1f2b743071c)  to clarify that templates are only applied at index creation time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FunctionScoreQueryBuilder should support filter mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17374</link><project id="" key="" /><description>**Describe the feature**:
I just found function_score not support filter mode in api.
eg. like this

```
"function_score":{
          "filter":{},
          "functions": []
}

```

and it's describe in version 1.6 but not 2.1
https://www.elastic.co/guide/en/elasticsearch/reference/1.6/query-dsl-function-score-query.html

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "functions": [
        {
            "filter": {},
            "FUNCTION": {}, 
            "weight": number
        },
        {
            "FUNCTION": {} 
        },
        {
            "filter": {},
            "weight": number
        }
    ],
    "max_boost": number,
    "score_mode": "(multiply|max|...)",
    "boost_mode": "(multiply|replace|...)",
    "min_score" : number
}
```
</description><key id="144130952">17374</key><summary>FunctionScoreQueryBuilder should support filter mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MadeInChina</reporter><labels /><created>2016-03-29T01:51:38Z</created><updated>2016-03-29T07:09:05Z</updated><resolved>2016-03-29T04:58:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-29T04:58:26Z" id="202711272">Filter and queries have been merged in 2.0.
Look at breaking changes for 2.0
</comment><comment author="MadeInChina" created="2016-03-29T06:03:41Z" id="202726759">@dadoonet 
Thanks for response.And i read the breaking changes for 2.0.

&gt; Queries and filters have been merged&#8201;&#8212;&#8201;all filter clauses are now query clauses. Instead, query clauses can now be used in query context or in filter context:
&gt; 
&gt; Query context
&gt; A query used in query context will calculate relevance scores and will not be cacheable. Query context is used whenever filter context does not apply.
&gt; Filter context
&gt; A query used in filter context will not calculate relevance scores, and will be cacheable. Filter context is introduced by:
&gt; 
&gt; the constant_score query
&gt; the must_not and (newly added) filter parameter in the bool query
&gt; the filter and filters parameters in the function_score query
&gt; any API called filter, such as the post_filter search parameter, or in aggregations or index aliases

But i'm doing the elastic optimization stuffs
When run below code, average tooks 200~300 milliseconds

```
{
  "from" : 0,
  "size" : 10,
  "query":
{
  "function_score" : {
      "query" : {
            "bool" : {
              "must" : [ {
                "range" : {
                  "timestamp" : {
                    "from" : "1457403235000",
                    "to" : "1458008035000",
                    "include_lower" : true,
                    "include_upper" : true
                  }
                }
              }, {
                "term" : {
                  "type" : "image"
                }
              }, {
                "geo_bbox" : {
                  "geo" : {
                    "top_left" : [ -110.80651, 43.44131 ],
                    "bottom_right" : [ -110.804, 43.4403 ]
                  },
                "type": "indexed"
                }
              } ]
            }
          },
      "functions" : [ {
        "script_score" : {
          "script" : {
            "file" : "hot_score",
            "params" : {
              "nowTime" : "1458008709945",
              "gravity" : "2.0"
            }
          }
        }
      } ]
    }
  }     
}
```

BUT ONCE change query to filter after function_score,it's only tooks 37~38 milliseconds

```
{
  "from" : 0,
  "size" : 10,
  "query":
{
  "function_score" : {
      "query" : {
            "bool" : {
              "must" : [ {
                "range" : {
                  "timestamp" : {
                    "from" : "1457403235000",
                    "to" : "1458008035000",
                    "include_lower" : true,
                    "include_upper" : true
                  }
                }
              }, {
                "term" : {
                  "type" : "image"
                }
              }, {
                "geo_bbox" : {
                  "geo" : {
                    "top_left" : [ -110.80651, 43.44131 ],
                    "bottom_right" : [ -110.804, 43.4403 ]
                  },
                "type": "indexed"
                }
              } ]
            }
          },
      "functions" : [ {
        "script_score" : {
          "script" : {
            "file" : "hot_score",
            "params" : {
              "nowTime" : "1458008709945",
              "gravity" : "2.0"
            }
          }
        }
      } ]
    }
  }     
}
```

So i think the filter is better than query.
</comment><comment author="dadoonet" created="2016-03-29T06:33:27Z" id="202739457">You'd better follow up this discussion on discuss.elastic.co.

What does it give in elasticsearch 2.2 in term of response time? Did you try it?
</comment><comment author="MadeInChina" created="2016-03-29T06:38:53Z" id="202741264">@dadoonet Ok, We are using  elasticsearch  2.1.0. so not try it on 2.2.
</comment><comment author="dadoonet" created="2016-03-29T06:45:26Z" id="202742240">Ok. May be @brwe can comment.

But did you run it multiple times? Is it faster after some runs?
Asking that because of the way filter cache works.

If your timestamp range query moves on every call, that could be a potential cause may be?
</comment><comment author="MadeInChina" created="2016-03-29T06:49:14Z" id="202743145">@dadoonet  Yes, just run them both multiple times.but filter is faster than query
</comment><comment author="MadeInChina" created="2016-03-29T07:09:05Z" id="202749021">Fixed with

```
public class FunctionScoreQueryBuilder extends QueryBuilder implements BoostableQueryBuilder&lt;FunctionScoreQueryBuilder&gt; {

    private final QueryBuilder queryBuilder;

    private Boolean isFilterMode = false;

    private Float boost;

    private Float maxBoost;

    private String scoreMode;

    private String boostMode;

    private ArrayList&lt;QueryBuilder&gt; filters = new ArrayList&lt;&gt;();
    private ArrayList&lt;ScoreFunctionBuilder&gt; scoreFunctions = new ArrayList&lt;&gt;();
    private Float minScore = null;

    /**
     * Creates a function_score query that executes on documents that match query a query.
     * Query and filter will be wrapped into a filtered_query.
     *
     * @param queryBuilder the query that defines which documents the function_score query will be executed on.
     */
    public FunctionScoreQueryBuilder(QueryBuilder queryBuilder) {
        this.queryBuilder = queryBuilder;
    }


    /**
     * Creates a function_score query that executes on documents that match query a query.
     * Query and filter will be wrapped into a filtered_query.
     *
     * @param queryBuilder
     * @param isFilterMode
     */
    public FunctionScoreQueryBuilder(QueryBuilder queryBuilder, Boolean isFilterMode) {
        this.queryBuilder = queryBuilder;
        this.isFilterMode = isFilterMode;
    }

    public FunctionScoreQueryBuilder() {
        this.queryBuilder = null;
    }

    /**
     * Creates a function_score query that will execute the function scoreFunctionBuilder on all documents.
     *
     * @param scoreFunctionBuilder score function that is executed
     */
    public FunctionScoreQueryBuilder(ScoreFunctionBuilder scoreFunctionBuilder) {
        if (scoreFunctionBuilder == null) {
            throw new IllegalArgumentException("function_score: function must not be null");
        }
        queryBuilder = null;
        this.filters.add(null);
        this.scoreFunctions.add(scoreFunctionBuilder);
    }


    /**
     * Adds a score function that will will execute the function scoreFunctionBuilder on all documents matching the filter.
     *
     * @param filter               the filter that defines which documents the function_score query will be executed on.
     * @param scoreFunctionBuilder score function that is executed
     */
    public FunctionScoreQueryBuilder add(QueryBuilder filter, ScoreFunctionBuilder scoreFunctionBuilder) {
        if (scoreFunctionBuilder == null) {
            throw new IllegalArgumentException("function_score: function must not be null");
        }
        this.filters.add(filter);
        this.scoreFunctions.add(scoreFunctionBuilder);
        return this;
    }

    /**
     * Adds a score function that will will execute the function scoreFunctionBuilder on all documents.
     *
     * @param scoreFunctionBuilder score function that is executed
     */
    public FunctionScoreQueryBuilder add(ScoreFunctionBuilder scoreFunctionBuilder) {
        if (scoreFunctionBuilder == null) {
            throw new IllegalArgumentException("function_score: function must not be null");
        }
        this.filters.add(null);
        this.scoreFunctions.add(scoreFunctionBuilder);
        return this;
    }

    /**
     * Score mode defines how results of individual score functions will be aggregated.
     * Can be first, avg, max, sum, min, multiply
     */
    public FunctionScoreQueryBuilder scoreMode(String scoreMode) {
        this.scoreMode = scoreMode;
        return this;
    }

    /**
     * Score mode defines how the combined result of score functions will influence the final score together with the sub query score.
     * Can be replace, avg, max, sum, min, multiply
     */
    public FunctionScoreQueryBuilder boostMode(String boostMode) {
        this.boostMode = boostMode;
        return this;
    }

    /**
     * Score mode defines how the combined result of score functions will influence the final score together with the sub query score.
     */
    public FunctionScoreQueryBuilder boostMode(CombineFunction combineFunction) {
        this.boostMode = combineFunction.getName();
        return this;
    }

    /**
     * Tha maximum boost that will be applied by function score.
     */
    public FunctionScoreQueryBuilder maxBoost(float maxBoost) {
        this.maxBoost = maxBoost;
        return this;
    }

    /**
     * Sets the boost for this query. Documents matching this query will (in
     * addition to the normal weightings) have their score multiplied by the
     * boost provided.
     */
    @Override
    public FunctionScoreQueryBuilder boost(float boost) {
        this.boost = boost;
        return this;
    }

    @Override
    protected void doXContent(XContentBuilder builder, Params params) throws IOException {
        builder.startObject(FunctionScoreQueryParser.NAME);
        if (queryBuilder != null) {
            if (isFilterMode) {
                builder.field("filter");
            } else {
                builder.field("query");
            }
            queryBuilder.toXContent(builder, params);
        }
        builder.startArray("functions");
        for (int i = 0; i &lt; filters.size(); i++) {
            builder.startObject();
            if (filters.get(i) != null) {
                builder.field("filter");
                filters.get(i).toXContent(builder, params);
            }
            scoreFunctions.get(i).toXContent(builder, params);
            builder.endObject();
        }
        builder.endArray();

        if (scoreMode != null) {
            builder.field("score_mode", scoreMode);
        }
        if (boostMode != null) {
            builder.field("boost_mode", boostMode);
        }
        if (maxBoost != null) {
            builder.field("max_boost", maxBoost);
        }
        if (boost != null) {
            builder.field("boost", boost);
        }
        if (minScore != null) {
            builder.field("min_score", minScore);
        }

        builder.endObject();
    }

    public FunctionScoreQueryBuilder setMinScore(float minScore) {
        this.minScore = minScore;
        return this;
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document analyzer option for keyword type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17373</link><project id="" key="" /><description>Per the [PR](https://github.com/jpountz/elasticsearch/blob/14abfe9c1d95ff396880bc3cdac7346aaee0db88/core/src/test/java/org/elasticsearch/index/mapper/core/KeywordFieldMapperTests.java#L205), the analyzer option has been implemented for the keyword type.  Currently, the doc for keyword type does not mention `analyzer` being an [option](https://www.elastic.co/guide/en/elasticsearch/reference/master/keyword.html#keyword-params).  Would be nice to document the analyzer option along with the allowable fields once we have come up with the predefined allowable subset of analyzers for keyword.
</description><key id="144098812">17373</key><summary>Document analyzer option for keyword type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Mapping</label><label>docs</label></labels><created>2016-03-28T22:26:55Z</created><updated>2016-03-29T09:48:15Z</updated><resolved>2016-03-29T09:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T09:48:15Z" id="202809366">@ppf2 This is referring to code of a pull request that has not been merged: https://github.com/elastic/elasticsearch/pull/16934. Supporting an `analyzer` option is planned but will likely not be available in 5.0 yet.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose information from PrimaryShardsAllocator and ReplicaShardsAllocator in unassigned explanation API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17372</link><project id="" key="" /><description>This is a follow-up from #17305

It would be nice if we could always expose/see the following information:
- [x] primary shard is unassigned as only stale shard copies available (allocation ids don't match)
- [x] replica shard unassigned due to delayed shard allocation (node_left.delayed_timeout)
- [x] primary or replica shard unassigned as shard fetching is still going on (i.e. not all data available yet to decide where to place the shard)
</description><key id="144096686">17372</key><summary>Expose information from PrimaryShardsAllocator and ReplicaShardsAllocator in unassigned explanation API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label></labels><created>2016-03-28T22:13:48Z</created><updated>2016-05-23T17:47:16Z</updated><resolved>2016-05-23T17:47:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-05-23T17:47:16Z" id="221043615">Resolving this as all three additions have been added to the cluster allocation explain API
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Periodic node crash.  JVM crashes with SIGSEGV every few days.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17371</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: java version "1.8.0_74" Java(TM) SE Runtime Environment (build 1.8.0_74-b02) Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)

**OS version**: Ubuntu 14.04.4 LTS, Linux 4.2.0-30-generic #36~14.04.1-Ubuntu SMP Fri Feb 26 18:49:23 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:  During normal operation, one of my nodes will randomly crash every few days with a JVM segfault

**Steps to reproduce**:  None, problem is intermittent during normal operation.  Workload is constant indexing (log data), very infrequent searches.

**Provide logs (if relevant)**: See attached files and below
[kaiju-v4.txt](https://github.com/elastic/elasticsearch/files/192448/kaiju-v4.txt)
[hs_err_pid21172.txt](https://github.com/elastic/elasticsearch/files/192450/hs_err_pid21172.txt)

```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  [thread 140246293530368 also had an error]
[thread 140241095403264 also had an error]
[thread 140236838332160 also had an error]
SIGSEGV (0xb) at pc=0x00007f8e650b3cd7, pid=21172, tid=140246295635712
#
# JRE version: Java(TM) SE Runtime Environment (8.0_74-b02) (build 1.8.0_74-b02)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.74-b02 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x436cd7]  ciObjectFactory::create_new_metadata(Metadata*)+0x327
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /mnt/elastic/elasticsearch-2.2.0/hs_err_pid21172.log

[error occurred during error reporting , id 0xb]

#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
Aborted (core dumped)
```
</description><key id="144080480">17371</key><summary>Periodic node crash.  JVM crashes with SIGSEGV every few days.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">najik</reporter><labels /><created>2016-03-28T20:53:13Z</created><updated>2016-03-28T22:11:19Z</updated><resolved>2016-03-28T22:11:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-28T22:11:19Z" id="202600251">That screams JVM bug. From your logs:

```
Current thread (0x00007f8e6021f000):  JavaThread "C2 CompilerThread5" daemon [_thread_in_vm, id=21253, stack(0x00007f8da291e000,0x00007f8da2a1f000)]
```

It's crashing in the server compiler. Maybe try upgrading your JVM to 8u77? If the issue persists, you'll have to open an OpenJDK issue. Sorry.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from Suggesters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17370</link><project id="" key="" /><description>Lots more fun PROTOTYPE removal. This was quite large because suggesters kept their XContent parsing in their PROTOTYPES.
</description><key id="144075066">17370</key><summary>Remove PROTOTYPE from Suggesters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-28T20:27:57Z</created><updated>2016-03-30T19:17:26Z</updated><resolved>2016-03-29T21:55:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-28T20:28:05Z" id="202569306">@javanna another one.
</comment><comment author="nik9000" created="2016-03-29T13:51:49Z" id="202902465">@javanna pushed a new change to address your comments.
</comment><comment author="javanna" created="2016-03-29T17:01:17Z" id="202998847">LGTM besides the comments I left, nothing major though
</comment><comment author="nik9000" created="2016-03-29T22:01:15Z" id="203130715">Merged! Thanks for the review @javanna !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose whether a task is cancellable in the _tasks list API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17369</link><project id="" key="" /><description>**Describe the feature**:
Currently, the _tasks API produces a list of tasks e.g.

```
ZXY-Y-NJThmuoBcL1idCjg:117243: {
  node: "ZXY-Y-NJThmuoBcL1idCjg",
  id: 117243,
  type: "direct",
  action: "indices:data/write/bulk[s][p]",
  status: {
    phase: "primary"
  },
  description: "BulkShardRequest to [mynewindex] containing [100] requests",
  start_time_in_millis: 1459182369761,
  running_time_in_nanos: 6699254,
  parent_task_id: "ZXY-Y-NJThmuoBcL1idCjg:117242"
}
```

However, there's no good way for a client to tell which task is eligible for cancellation from the API.  It would be nice to see that, ideally in the task list response.
</description><key id="144071749">17369</key><summary>Expose whether a task is cancellable in the _tasks list API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>:Task Manager</label><label>discuss</label></labels><created>2016-03-28T20:15:04Z</created><updated>2016-04-06T02:52:43Z</updated><resolved>2016-04-06T02:52:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-28T20:30:04Z" id="202569971">@imotov I think it'd be pretty simple to add a `cancellable: true`/`cancellable: false` I think. 
</comment><comment author="imotov" created="2016-03-28T23:14:00Z" id="202620205">@nik9000 we would need to propagate it through TaskInfo. So, a task will need to somehow notify TaskInfo about its capabilities, and I would like to make this mechanism more generic if I can, so we have a way to notify that a task is throttle-able, for example. Need to think about it.
</comment><comment author="nik9000" created="2016-03-29T11:42:18Z" id="202844676">Something along the lines of `Set&lt;String&gt; capabilities()` on the Task level? TaskInfo can call it and sort it. I guess you could even use `capabilities().contains` in BaseTasksRequest.  Right now we really only have cancel and, after https://github.com/elastic/elasticsearch/pull/17262, throttle.
</comment><comment author="imotov" created="2016-03-30T17:18:32Z" id="203535976">I had a chance to thing about it more and and play a bit with implementing the generic way of indicating task capabilities. While I still think we need a generic way to do it, the cancellation process is very special case, it affects that way TaskManager works with tasks and probably deserves a dedicated property in TaskInfo. So, if nobody opposes, I am going to implement it the way @nik9000 has originally suggested. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] clarify where discovery.zen.minimum_master_node is required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17368</link><project id="" key="" /><description>https://github.com/elastic/elasticsearch/pull/17288 added a check to enforce that the `discovery.zen.minimum_master_nodes` configuration is set when nodes have the `host`, `port`, or `bind_host` set in either `transport` or general `network` configuration sections. This was documented incorrectly as "nodes that are bound to a non-loopback interface", which lead to confusion as I set `network.host: "localhost"` and the check was still failing.

This change updates the docs to detail the actual check. I think it also highlights how complex the check is and the need for a simpler solution.
</description><key id="144068918">17368</key><summary>[docs] clarify where discovery.zen.minimum_master_node is required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spalger</reporter><labels><label>docs</label></labels><created>2016-03-28T20:01:13Z</created><updated>2016-03-29T18:44:50Z</updated><resolved>2016-03-29T18:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-28T23:26:40Z" id="202624000">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dead code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17367</link><project id="" key="" /><description>This commit removes dead DeleteByQuery class from core.
</description><key id="144050984">17367</key><summary>Remove dead code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-28T18:49:40Z</created><updated>2016-03-28T18:59:44Z</updated><resolved>2016-03-28T18:59:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-28T18:54:50Z" id="202528158">LGTM
</comment><comment author="areek" created="2016-03-28T18:59:44Z" id="202529944">Thanks @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexShard#failShard should throw an AlreadyClosedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17366</link><project id="" key="" /><description>Today when failing a shard via `IndexShard#failShard`, we silently swallow cases where the engine is already closed. Instead, we should throw an `AlreadyClosedException`.
</description><key id="144035245">17366</key><summary>IndexShard#failShard should throw an AlreadyClosedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>adoptme</label><label>enhancement</label><label>v5.4.4</label></labels><created>2016-03-28T17:40:43Z</created><updated>2017-06-27T10:28:18Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-28T18:36:34Z" id="202521918">++
</comment><comment author="clintongormley" created="2016-10-18T07:59:24Z" id="254434891">Is this still an issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace is_true: took with took &gt;= 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17365</link><project id="" key="" /><description>This prevents tests from failing on machines that can finish the request
less than half a millisecond.
</description><key id="144026981">17365</key><summary>Replace is_true: took with took &gt;= 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>test</label><label>v2.3.1</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-28T17:05:39Z</created><updated>2016-03-29T13:23:39Z</updated><resolved>2016-03-28T17:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-28T17:05:59Z" id="202488265">@dakrone you pinged me about these failing on your (obviously very nice) machine.
</comment><comment author="dakrone" created="2016-03-28T17:09:22Z" id="202490788">LGTM
</comment><comment author="nik9000" created="2016-03-28T20:28:35Z" id="202569575">I will backport this back to 2.x when I get back to a place with good internet.
</comment><comment author="nik9000" created="2016-03-29T13:23:31Z" id="202891254">2.3: 11f90bffda50f0acc8dc1409f3f33005e1249234
2.x: 1c6be8c590ef0432f92f62886c5cd37a8c0ccf81
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add link to snapshot builds in README.md</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17364</link><project id="" key="" /><description>Please add links to elasticsearch snapshot builds to the README.md so people can easily download and try elasticsearch.  This would make it consistent with Kibana which lists snapshot builds in it's README.md (the README.md of each branch has the links to the builds of that branch).

I think people can get to a link now, if they knew where it was.  
</description><key id="144024685">17364</key><summary>Add link to snapshot builds in README.md</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LeeDr</reporter><labels><label>build</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-28T16:54:58Z</created><updated>2017-03-21T15:00:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Switch Highlighting to ObjectParser </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17363</link><project id="" key="" /><description>Creates a new method on ObjectParser called `declareNamedObjects` which targets fields of type `List&lt;T&gt;` but has special handling for constructing the field. It can be used to parse things like highlight's fields element or aggregations. I added it to ObjectParser rather than implementing it just to HighlightBuilder place because it felt like we could use it for aggregations one day. And because it felt right isolating XContent handling stuff in ObjectParser where possible.

This is the kind of thing it parses:

``` js
{
  "highlight": {
    "fields": {        &lt;------ this one
      "title": {},
      "body": {},
      "category": {}
    }
  }
}
```

or it can parse fields when it is in "ordered" mode:

``` js
{
  "highlight": {
    "fields": [        &lt;------ this one
      {"title": {}},
      {"body": {}},
      {"category": {}}
    ]
  }
}
```

Finally I got the line width tests passing on the files that I did major work on. See my comments on the second commit for more about that if you like.
</description><key id="143982753">17363</key><summary>Switch Highlighting to ObjectParser </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-28T13:52:54Z</created><updated>2016-03-31T14:34:53Z</updated><resolved>2016-03-31T14:34:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-31T01:02:28Z" id="203700693">@s1monw can you take a look at this? I think you are the most right person.
</comment><comment author="s1monw" created="2016-03-31T13:55:07Z" id="203951644">I did a quick review and it looks great. I think we should maybe since we already spend so much time on the JavaDoc  an example how to use it would have been even more helpful...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Different extended stats with apparently same query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17362</link><project id="" key="" /><description>I tested this both from Kibana and using the HTTP API of elasticsearch to check that is a problem of elasticsearch instead of Kibana:

This request:

```
curl -XPOST "127.0.0.1:9200/my_index_*/_search?pretty" -d '{
    "aggs" : {
        "grades_stats" : { "extended_stats" : { "field" : "my_field" } }
    }
}'
```

Returns this

```
"aggregations" : {
    "grades_stats" : {
      "count" : 24526,
      "min" : 0.0,
      "max" : 5545.0,
      "avg" : 108.78504444263231,
      "sum" : 2668062.0,
      "sum_of_squares" : 1.461725356E9,
      "variance" : 47764.825603616635,
      "std_deviation" : 218.5516543145273,
      "std_deviation_bounds" : {
        "upper" : 108.78504444263231,
        "lower" : 108.78504444263231
      }
    }
  }
```

With a wrong **std_deviation_bounds**. Only 24526 documents from index my_index_docs_with_my_field have the field my_field so it should does not matter that I just use my_index_\* instead of my_index_docs_with_my_field. In fact, the next request, specifying the index my_index_docs_with_my_field returns the same count of documents, 24526 and the same variance and std_deviation but with different std_deviation_bounds. These std_deviation_bounds make sense because are equal to avg+/-2*std_deviation while the first request seems wrong.

The second request:

```
curl -XPOST "127.0.0.1:9200/my_index_docs_with_my_field/_search?pretty" -d '{
    "aggs" : {
        "grades_stats" : { "extended_stats" : { "field" : "my_field" } }
    }
}'
```

returns

```
"aggregations" : {
    "grades_stats" : {
      "count" : 24526,
      "min" : 0.0,
      "max" : 5545.0,
      "avg" : 108.78504444263231,
      "sum" : 2668062.0,
      "sum_of_squares" : 1.461725356E9,
      "variance" : 47764.825603616635,
      "std_deviation" : 218.5516543145273,
      "std_deviation_bounds" : {
        "upper" : 545.8883530716869,
        "lower" : -328.3182641864223
      }
    }
  }
```

Is this behaviour normal? If so, why the first std_deviation_bounds are not equal to avg+/-2*std_deviation?
</description><key id="143941154">17362</key><summary>Different extended stats with apparently same query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">carlosvega</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2016-03-28T09:55:41Z</created><updated>2016-04-07T07:35:19Z</updated><resolved>2016-04-07T07:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T17:08:28Z" id="203002457">This is a bug indeed. When some requested indices do not have mappings for the requested field, a parameter that is used to compute this bounds (sigma) is not propagated correctly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed to send ping to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17361</link><project id="" key="" /><description>&lt;!--
My previous Elastic version was 1.7.0, but because of some problem occurred I have removed that and reinstalled 2.2.1, now I have a problem that can't send ping. 
--&gt;

**Elasticsearch version 2.2.1**:

**1.8**:

**OS X 10.10 (14A389a)**:

`Ahmads-MacBook-Pro:elasticsearch-2.2.1` mobasherfasihy$ bin/elasticsearch
[2016-03-28 13:51:26,794][INFO ][node                     ] [Sasquatch] version[2.2.1], pid[94989], build[d045fc2/2016-03-09T09:38:54Z]
[2016-03-28 13:51:26,795][INFO ][node                     ] [Sasquatch] initializing ...
[2016-03-28 13:51:27,255][INFO ][plugins                  ] [Sasquatch] modules [lang-expression, lang-groovy], plugins [], sites []
[2016-03-28 13:51:27,271][INFO ][env                      ] [Sasquatch] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [35.2gb], net total_space [369.5gb], spins? [unknown], types [hfs]
[2016-03-28 13:51:27,271][INFO ][env                      ] [Sasquatch] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-03-28 13:51:28,489][INFO ][node                     ] [Sasquatch] initialized
[2016-03-28 13:51:28,489][INFO ][node                     ] [Sasquatch] starting ...
[2016-03-28 13:51:28,561][INFO ][transport                ] [Sasquatch] publish_address {127.0.0.1:9312}, bound_addresses {[fe80::1]:9308}, {[::1]:9308}, {127.0.0.1:9312}
[2016-03-28 13:51:28,567][INFO ][discovery                ] [Sasquatch] elasticsearch/_MPsbDKrQw2c6TSz_YcuCA
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_4#}{127.0.0.1}{127.0.0.1:9303}]
ReceiveTimeoutTransportException[[][127.0.0.1:9303][internal:discovery/zen/unicast] request_id [2] timed out after [3753ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_6#}{::1}{[::1]:9300}]
ReceiveTimeoutTransportException[[][[::1]:9300][internal:discovery/zen/unicast] request_id [7] timed out after [3764ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_5#}{127.0.0.1}{127.0.0.1:9304}]
ReceiveTimeoutTransportException[[][127.0.0.1:9304][internal:discovery/zen/unicast] request_id [10] timed out after [3763ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_2#}{127.0.0.1}{127.0.0.1:9301}]
ReceiveTimeoutTransportException[[][127.0.0.1:9301][internal:discovery/zen/unicast] request_id [3] timed out after [3753ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_9#}{::1}{[::1]:9303}]
ReceiveTimeoutTransportException[[][[::1]:9303][internal:discovery/zen/unicast] request_id [5] timed out after [3764ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_10#}{::1}{[::1]:9304}]
ReceiveTimeoutTransportException[[][[::1]:9304][internal:discovery/zen/unicast] request_id [1] timed out after [3753ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_8#}{::1}{[::1]:9302}]
ReceiveTimeoutTransportException[[][[::1]:9302][internal:discovery/zen/unicast] request_id [4] timed out after [3753ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_1#}{127.0.0.1}{127.0.0.1:9300}]
ReceiveTimeoutTransportException[[][127.0.0.1:9300][internal:discovery/zen/unicast] request_id [8] timed out after [3764ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_3#}{127.0.0.1}{127.0.0.1:9302}]
ReceiveTimeoutTransportException[[][127.0.0.1:9302][internal:discovery/zen/unicast] request_id [9] timed out after [3764ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:32,390][WARN ][discovery.zen.ping.unicast] [Sasquatch] failed to send ping to [{#zen_unicast_7#}{::1}{[::1]:9301}]
ReceiveTimeoutTransportException[[][[::1]:9301][internal:discovery/zen/unicast] request_id [6] timed out after [3764ms]]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:645)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-28 13:51:33,096][INFO ][cluster.service          ] [Sasquatch] new_master {Sasquatch}{_MPsbDKrQw2c6TSz_YcuCA}{127.0.0.1}{127.0.0.1:9312}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-03-28 13:51:33,129][INFO ][http                     ] [Sasquatch] publish_address {127.0.0.1:9211}, bound_addresses {[fe80::1]:9208}, {[::1]:9208}, {127.0.0.1:9211}
[2016-03-28 13:51:33,129][INFO ][node                     ] [Sasquatch] started
`

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="143938626">17361</key><summary>failed to send ping to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mobasher-NetLinks</reporter><labels /><created>2016-03-28T09:41:47Z</created><updated>2016-03-28T11:45:14Z</updated><resolved>2016-03-28T10:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-28T10:58:14Z" id="202340583">This is normal. Elasticsearch is trying to discover other nodes on localhost. Since there are apparently no such nodes, the pings timeout. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed broken links in CONTRIBUTING.md</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17360</link><project id="" key="" /><description>Fixed broken links in CONTRIBUTING.md:
- fixed TESTING.asciidoc link
- removed Contributing to elasticsearch link (redirects back to CONTRIBUTING.md).
</description><key id="143854704">17360</key><summary>Fixed broken links in CONTRIBUTING.md</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qwerty4030</reporter><labels><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2016-03-27T22:10:38Z</created><updated>2016-03-28T23:27:39Z</updated><resolved>2016-03-28T23:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-28T23:27:25Z" id="202624129">Merged, thanks @qwerty4030!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problems with searching documents with mapper-attachments plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17359</link><project id="" key="" /><description>I have a problem with searching indexed documents with newest ES and mapper-attachment plugin. Longer discussion is located here https://discuss.elastic.co/t/problems-with-searching-from-document-contents-with-mapper-attachments-plugin/45534.

Elasticsearch version: 2.2.1 (only 1 node)
JVM version: 1.8.0u74
OS version: Windows 10 64-bit

You can reproduce the issue:
- Create index

```
PUT test_index
{
    "settings": {
        "number_of_shards": 2,
        "number_of_replicas": 0
    },
    "mappings": {
        "testfile": {
            "dynamic": "strict",
            "_source": {
                "enabled": true
            },
            "properties": {
                "fileId": {
                    "type": "integer",
                    "store": true
                },
                "contents": {
                    "type": "attachment"
                }
            }
        }
    }
}
```
- Index document with phrase "hello world"
- Check if document is indexed with match_all query

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 2,
      "successful": 2,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            "_index": "test_index",
            "_type": "testfile",
            "_id": "AVO3TKC2-BSsruK6CY8T",
            "_score": 1,
            "_source": {
               "fileId": 101,
               "contents": "aGVsbG8gd29ybGQ="
            }
         }
      ]
   }
}
```
- Try to search the document with phrase query (no results)

```
POST test_index/_search
{
  "from": 0,
  "size": 1000,
  "fields": [
    "fileId"
  ],
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "contents": {
              "query": "hello",
              "type": "phrase"
            }
          }
        }
      ]
    }
  }
}
```
- Try to search  the document with query_string (1 result)

```
POST /test_index/_search
{
  "query": {
    "query_string": {
      "query": "hello"
    }
  }
}
```

This is unexpected behaviour because all this worked fine with ES 1.7.3. It feels like something is happened with match or match_phrase queries.
</description><key id="143838563">17359</key><summary>Problems with searching documents with mapper-attachments plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaxx</reporter><labels /><created>2016-03-27T19:07:54Z</created><updated>2016-03-29T07:43:27Z</updated><resolved>2016-03-29T05:00:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GlenRSmith" created="2016-03-28T03:28:26Z" id="202217748">Possibly relevant:

After changing index to have only 1 shard

v2.2.1 throws an exception when attempting to load fielddata on a search result.

Using the query_string query that does succeed, the error is returned.

```
POST /test_index/_search
{
  "fielddata_fields": ["contents"], 
  "query": {
    "query_string": {
      "query": "hello"
    }
  }
}

{
   "error": {
      "root_cause": [
         {
            "type": "illegal_argument_exception",
            "reason": "failed to find field data builder for field contents, and type attachment"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query_fetch",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "test_index",
            "node": "NSripkQmT9qAZCuoTdzR9w",
            "reason": {
               "type": "illegal_argument_exception",
               "reason": "failed to find field data builder for field contents, and type attachment"
            }
         }
      ]
   },
   "status": 400
}
```

With v1.7.3, the fielddata is populated and returned.

```
POST /test_index/testfile/_search
{
  "fielddata_fields": ["contents"], 
  "query": {
    "query_string": {
      "query": "hello"
    }
  }
}
```

returns

```
{
   "took": 2,
   "timed_out": false,
   "_shards": {
      "total": 2,
      "successful": 2,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 0.095891505,
      "hits": [
         {
            "_index": "test_index",
            "_type": "testfile",
            "_id": "AVO7NoVgsHxrzUVsaSN9",
            "_score": 0.095891505,
            "_source": {
               "fileId": 101,
               "contents": "aGVsbG8gd29ybGQ="
            },
            "fields": {
               "contents": [
                  "hello",
                  "world"
               ]
            }
         }
      ]
   }
}
```
</comment><comment author="eskibars" created="2016-03-29T00:42:31Z" id="202646950">query_string defaults to index.query.default_field (which is _all by default).  That's why the query_string is working as expected.

You can get your match_phrase query to work by searching field contents.content, which is mapper-attachments dumps the document content to (https://www.elastic.co/guide/en/elasticsearch/plugins/2.2/mapper-attachments-usage.html)
</comment><comment author="jaxx" created="2016-03-29T05:33:57Z" id="202717817">If I'm building my queries with NEST and using expressions to get field names from properties, how do I know which field is meant for full-text search??? In previous versions it was simple - I built universal query and it worked with all fields.
</comment><comment author="dadoonet" created="2016-03-29T06:35:06Z" id="202739920">In your case it should be contents.content
</comment><comment author="jaxx" created="2016-03-29T06:59:59Z" id="202745690">I understand, but if I have a class

```
public class ESFile
{
    public int Id { get; set; }
    public string Contents { get; set; }
}
```

And I'm building my queries like 

```
builder.Build&lt;ESFile&gt;(x =&gt; x.Id).Add(x =&gt; x.Contents);
```

How can I know which field is content field? Why this functionality changed compared to earlier versions? Can I tell NEST with somekind of an attribute that Contents is attachment?
</comment><comment author="dadoonet" created="2016-03-29T07:31:54Z" id="202755941">I don't really remember why it changed. I think it was cleaner to not modify what the user entered and generate the content in another field.

So you send:

``` js
{
  "foo": "base64"
}
```

It generates behind the scene:

``` js
{
  "foo": {
     "foo": "base64",
     "content": "bar" 
  }
}
```

If you search in `foo`, it uses `foo.foo` (default behavior for multi fields). If you want to search in extracted text, you have to search in  `foo.content`.

But you have to know that it will change again in the future as mapper attachments plugin will be replaced by an ingest pipeline. See https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest-attachment.html
</comment><comment author="jaxx" created="2016-03-29T07:43:27Z" id="202763983">Thank you for your answer.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replication operation that try to perform the primary phase on a replica should be retried</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17358</link><project id="" key="" /><description>In extreme cases a local primary shard can be replaced with a replica while a replication request is in flight and the primary action is applied to the shard (via `acquirePrimaryOperationLock()`).  #17044 changed the exception used in that method to something that isn't recognized as `TransportActions.isShardNotAvailableException`, causing the operation to fail immediately instead of retrying. This commit fixes this by check the primary flag before
acquiring the lock. This is safe to do as an IndexShard will never be demoted once a primary.

Marking as non issue as this was never released.

Example failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=centos/138
</description><key id="143822217">17358</key><summary>Replication operation that try to perform the primary phase on a replica should be retried</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-03-27T15:45:12Z</created><updated>2016-03-30T18:58:00Z</updated><resolved>2016-03-29T15:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-29T10:12:07Z" id="202820106">@s1monw I pushed another commit, removing the custom exceptions and relying a primary check in TRA, before acquiring an operation lock from the relevant IndexShard instance. This is OK since a primary shard will never be demoted 
</comment><comment author="s1monw" created="2016-03-29T12:17:17Z" id="202863904">left a minor regarding a comment - LGTM otherwise thanks!
</comment><comment author="jasontedor" created="2016-03-29T14:42:14Z" id="202930732">Just a minor issue (the commented out randomization in the test), and two nits. Otherwise, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add limit to total number of fields in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17357</link><project id="" key="" /><description>This is to prevent mapping explosion when dynamic keys such as UUID are used as field names. index.mapping.total_fields.limit specifies the total number of fields an index can have. An exception will be thrown when the limit is reached. The default limit is 1000. Value 0 means no limit. This setting is runtime adjustable
</description><key id="143781757">17357</key><summary>Add limit to total number of fields in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">yanjunh</reporter><labels><label>:Mapping</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-27T06:44:56Z</created><updated>2016-04-19T10:10:06Z</updated><resolved>2016-03-29T17:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yanjunh" created="2016-03-27T06:49:11Z" id="202001829">related to #17203
#17357 
</comment><comment author="s1monw" created="2016-03-29T07:26:53Z" id="202753881">I wonder if we really need to make `0` a special value, why can't folks just use `Long.MAX_VALUE` or any high number? I also wonder if this PR can reject documents that have been accepted previously on the primary and trigger a mapping update on the replica due to some cluster state delays. This might also reject mapping merges coming from the master due to concurrent indexing? @jpountz should know more here, in any case I guess we need to make this check higher up the stack I suspect.
</comment><comment author="jpountz" created="2016-03-29T07:48:37Z" id="202765545">@simonw The code has the check under the following `if` statement: `if (reason == MergeReason.MAPPING_UPDATE)`. This means that the mapping is being merged due to a call to the update mapping API. So only the master node performs these checks, and only when there is a call to the update mapping API. In all other cases, like on data nodes or on the master node when it restores mappings from disk, the MergeReason would be `MergeReason.MAPPING_RECOVERY` so the check will be skipped. So I think we are fine?
</comment><comment author="s1monw" created="2016-03-29T07:56:14Z" id="202767070">@jpountz good can we add a comment there?
</comment><comment author="jpountz" created="2016-03-29T07:57:33Z" id="202767332">I will do it in a separate commit.
</comment><comment author="jpountz" created="2016-03-29T08:04:47Z" id="202769771">@simonw Done in c7bdfb1126d47442f7e12f996eecbb7fab315c2d.
</comment><comment author="s1monw" created="2016-03-29T08:16:05Z" id="202773631">thx @jpountz 
</comment><comment author="jpountz" created="2016-03-29T17:54:23Z" id="203025925">@yanjunh It was so close that I applied the changes I suggested and then merged. I hope you don't mind. 361adcf3870457e87d3fc94e9c799ffea2382ec8
</comment><comment author="yanjunh" created="2016-03-29T18:39:01Z" id="203042602">@jpountz Not at all. Thanks for merging this feature.
</comment><comment author="segalziv" created="2016-04-19T06:08:28Z" id="211746458">@jpountz is there a chance that this PR can get back ported to 2.3.x ? I've discussed this with @yanjunh who pointed me to a fork 2.3.1 with this fix here: https://github.com/yanjunh/elasticsearch/tree/v2.3.1-maplimit  
Thanks!
</comment><comment author="clintongormley" created="2016-04-19T09:56:59Z" id="211833597">@segalziv no, this is a breaking change so we'll keep it in 5.0 only
</comment><comment author="segalziv" created="2016-04-19T10:09:57Z" id="211841005">ok @clintongormley , thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch and kibana timeout problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17356</link><project id="" key="" /><description>I'm using ElasticSearch, everything was normal and working, but message `high desk watermark [90%].....some shards will be relocated away from this node` was showed.
But after some days kibana and elasticsearch can't connect, and connection timeout occur.

```
Ahmads-MacBook-Pro:elasticsearch-1.7.0 mobasherfasihy$ bin/elasticsearch
[2016-03-26 14:57:57,230][INFO ][node                     ] [Sage] version[1.7.0], pid[86359], build[929b973/2015-07-16T14:31:07Z]
[2016-03-26 14:57:57,231][INFO ][node                     ] [Sage] initializing ...
[2016-03-26 14:57:57,391][INFO ][com.floragunn.searchguard.SearchGuardPlugin] Class enhancements for DLS/FLS successful
[2016-03-26 14:57:57,407][INFO ][plugins                  ] [Sage] loaded [searchguard, license, marvel], sites [marvel]
[2016-03-26 14:57:57,437][INFO ][env                      ] [Sage] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [17.9gb], net total_space [369.5gb], types [hfs]
[2016-03-26 14:57:59,083][WARN ][com.floragunn.searchguard.util.SecurityUtil] AES 256 not supported, max key length for AES is 128. To enable AES 256 install 'Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files'
[2016-03-26 14:57:59,261][INFO ][node                     ] [Sage] initialized
[2016-03-26 14:57:59,261][INFO ][node                     ] [Sage] starting ...
[2016-03-26 14:57:59,333][INFO ][transport                ] [Sage] bound_address {inet[/127.0.0.1:9304]}, publish_address {inet[localhost/127.0.0.1:9304]}
[2016-03-26 14:57:59,345][INFO ][discovery                ] [Sage] elasticsearch/XR6DsFmbQk267g3UFpbQCw
[2016-03-26 14:58:03,125][INFO ][cluster.service          ] [Sage] new_master [Sage][XR6DsFmbQk267g3UFpbQCw][Ahmads-MacBook-Pro.local][inet[localhost/127.0.0.1:9304]], reason: zen-disco-join (elected_as_master)
[2016-03-26 14:58:03,149][INFO ][http                     ] [Sage] bound_address {inet[/127.0.0.1:9204]}, publish_address {inet[localhost/127.0.0.1:9204]}
[2016-03-26 14:58:03,149][INFO ][node                     ] [Sage] started
[2016-03-26 14:58:03,157][INFO ][gateway                  ] [Sage] recovered [0] indices into cluster_state
[2016-03-26 14:58:15,288][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:58:15,288][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:58:21,293][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:58:21,294][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:58:27,304][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:58:27,304][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:58:33,136][WARN ][cluster.routing.allocation.decider] [Sage] high disk watermark [90%] exceeded on [XR6DsFmbQk267g3UFpbQCw][Sage] free: 17.9gb[4.8%], shards will be relocated away from this node
[2016-03-26 14:58:33,136][INFO ][cluster.routing.allocation.decider] [Sage] high disk watermark exceeded on one or more nodes, rerouting shards
[2016-03-26 14:58:33,315][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:58:33,315][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:58:49,326][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:58:49,326][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:58:55,328][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:58:55,328][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:59:01,330][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:59:01,330][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:59:03,133][WARN ][cluster.routing.allocation.decider] [Sage] high disk watermark [90%] exceeded on [XR6DsFmbQk267g3UFpbQCw][Sage] free: 17.9gb[4.8%], shards will be relocated away from this node
[2016-03-26 14:59:07,338][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:59:07,338][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:59:23,346][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:59:23,346][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:59:29,351][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:59:29,351][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
[2016-03-26 14:59:33,136][WARN ][cluster.routing.allocation.decider] [Sage] high disk watermark [90%] exceeded on [XR6DsFmbQk267g3UFpbQCw][Sage] free: 17.9gb[4.8%], shards will be relocated away from this node
[2016-03-26 14:59:33,136][INFO ][cluster.routing.allocation.decider] [Sage] high disk watermark exceeded on one or more nodes, rerouting shards
[2016-03-26 14:59:35,355][ERROR][marvel.agent.exporter    ] [Sage] error connecting to [127.0.0.1:9200] [connect timed out]
[2016-03-26 14:59:35,355][ERROR][marvel.agent.exporter    ] [Sage] could not connect to any configured elasticsearch instances: [127.0.0.1:9200]
```

and in kibana 

```
{"name":"Kibana","hostname":"Ahmads-MacBook-Pro.local","pid":85748,"level":50,"err":{"message":"Request Timeout after 5000ms","name":"Error","stack":"Error: Request Timeout after 5000ms\n    at null.&lt;anonymous&gt; (/Users/ahmadhusseinrezae/kibana-4.1.3-darwin-x64/src/node_modules/elasticsearch/src/lib/transport.js:282:15)\n    at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)"},"msg":"","time":"2016-03-26T08:55:35.517Z","v":0}
```
</description><key id="143687412">17356</key><summary>ElasticSearch and kibana timeout problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mobasher-NetLinks</reporter><labels /><created>2016-03-26T10:40:31Z</created><updated>2016-03-30T18:18:55Z</updated><resolved>2016-03-30T18:18:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-30T18:18:55Z" id="203563459">Hi @Mobasher-NetLinks 

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>accessing fields returned from top hits aggregation </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17355</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

is it possible accessing document's field returned from top hits aggregation?
i need sorting by default score in top hits sorting, and ordering the bucket from the root aggs by some other fields returned from top hits.
for example:

```
 "aggregations": {
    "spu_group": {
        "aggregations": {
            "sku_info": {
                "top_hits": {
                    "_source": {
                        "include": "sku_id",
                        "include": "amount"
                    }, 
                    "size": 1
                }
            }
        }, 
        "terms": {
            "field": "product_id", 
            "order": {
                "sku_info.amount": "desc"
            }, 
            "size": 12
        }
    }
}
```

here i try to use "sku_info.amount" ordering the terms aggs of &#8221;product_id&#8220;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="143670979">17355</key><summary>accessing fields returned from top hits aggregation </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q11112345</reporter><labels /><created>2016-03-26T06:39:12Z</created><updated>2017-05-30T07:35:23Z</updated><resolved>2016-03-30T20:23:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-29T08:57:01Z" id="202784682">Something like this is possible, but not direct with the `top_hits` aggregation. You need to use an additional metric agg and tell the terms agg to sort by that: 

```
 "aggregations": {
    "spu_group": {
        "aggregations": {
            "sku_info": {
                "top_hits": {
                    "_source": {
                        "include": "sku_id",
                        "include": "amount"
                    }, 
                    "size": 1
                },
                "top_hit" : {
                  "max": {
                    "field": "amount"
                  }
                }
            }
        }, 
        "terms": {
            "field": "product_id", 
            "order": {
              "top_hit": "desc"
            }, 
            "size": 12
        }
    }
}
```
</comment><comment author="q11112345" created="2016-03-29T11:01:59Z" id="202832192">this is what i do now.
the spu returned are different from the max amount and the top_hits 
i may have three sku: s1 s2 s3.
top_hits return s2,
max amount return s1,
at last, the s2 shows up with order ordered by s1's amount.
</comment><comment author="martijnvg" created="2016-03-30T09:43:27Z" id="203350129">@q11112345 Yes, that makes sense. The `top_hits` agg sorts by score while the `max` agg selects the max based on the amount field. If you change the `max` agg to the highest score then I think things should work like you want it to be:

```
"top_hit" : {
   "max": {
      "script": "_score"
   }
}
```
</comment><comment author="clintongormley" created="2016-03-30T20:23:30Z" id="203616166">Looks like there's nothing more to do here.  Closing
</comment><comment author="q11112345" created="2016-03-31T03:26:57Z" id="203735044">@martijnvg  thanks very much.
i do have problem.
my problem is : 
1 pick up one doc from bucket  by sorting by one field in top-hits, for example: pick up the doc with max score in bucket.

```
"top_hits" : {
    "_source": {
            "include": "sku_id",
            "include": "amount"
        }, 
  "size": 1
}
```

2 then, sorting all buckets by another field of the doc which is picked up in "top_hits"

here i need sort buckets by doc field. and the field must be obtained from the top_hits.
i cant find a way to make this work.

there is no way to use the field returned from top_hits as i know .

may i post this problem again?
</comment><comment author="sakthibalan15" created="2017-05-30T07:34:49Z" id="304798516">I have a problem with sorting aggregations on top fields.

`aggs: { salary: {ranges: salary_ranges, score: 1}, company: {}, max_age: {ranges: age_ranges}, show_only: {order: {"_term" =&gt; "desc"}},  location: {}}`

After the search I got sorting order are: 1. Max Age, 2. Show only, 3. Company, 4. Location, 5. Salary.
But I need the sorting order are: 1. Show only, 2. Location, 3. Salary, 4. Max age, 5. Company.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_nodes/ endpoint omits data key from attributes hash for data nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17354</link><project id="" key="" /><description>While querying the /_nodes/ endpoint to categorize nodes for a test I've noticed my data node does not include the 'data' key in the attributes hash:

``` ruby
[29] pry(main)&gt; nodes_data['nodes']['r0YScyXZTX-hUhwWnseHsA']['attributes']
=&gt; {"master"=&gt;"false"}
```

The master and client, however, do have this key set.  The following shows the attributes hases for all 3 nodes in my test cluster, with data being the first:

``` ruby
[30] pry(main)&gt; nodes_data['nodes'].values.map { |n| n['attributes'] }
=&gt; [{"master"=&gt;"false"}, {"data"=&gt;"false", "master"=&gt;"false"}, {"data"=&gt;"false", "master"=&gt;"true"}]
```

Here is the complete output for the data node:

``` ruby
[34] pry(main)&gt; nodes_data['nodes']['r0YScyXZTX-hUhwWnseHsA']
=&gt; {"name"=&gt;"bbb3a55cacc1",                                  
 "transport_address"=&gt;"192.168.253.160:9301",
 "host"=&gt;"192.168.253.160",
 "ip"=&gt;"192.168.253.160",
 "version"=&gt;"2.2.1",
 "build"=&gt;"d045fc2",
 "http_address"=&gt;"es-cluster-ubuntu-1404/192.168.253.160:9201",
 "attributes"=&gt;{"master"=&gt;"false"},
 "settings"=&gt;
  {"cluster"=&gt;{"name"=&gt;"test"},
   "node"=&gt;{"data"=&gt;"true", "name"=&gt;"bbb3a55cacc1", "master"=&gt;"false"},
   "path"=&gt;{"logs"=&gt;"/usr/share/elasticsearch/logs", "plugins"=&gt;"/usr/share/elasticsearch/plugins", "home"=&gt;"/usr/share/elasticsearch"},
   "discovery"=&gt;{"zen"=&gt;{"ping"=&gt;{"multicast"=&gt;{"enabled"=&gt;"false"}, "unicast"=&gt;{"hosts"=&gt;["172.17.0.1:9300"]}}}},
   "name"=&gt;"bbb3a55cacc1",
   "client"=&gt;{"type"=&gt;"node"},
   "http"=&gt;{"port"=&gt;"9201", "cors"=&gt;{"allow-origin"=&gt;"\"/.*/\"", "enabled"=&gt;"true"}},
   "transport"=&gt;{"tcp"=&gt;{"port"=&gt;"9301"}},
   "config"=&gt;{"ignore_system_properties"=&gt;"true"},
   "network"=&gt;{"host"=&gt;"_site_", "publish_host"=&gt;"es-cluster-ubuntu-1404"}},
 "os"=&gt;{"refresh_interval_in_millis"=&gt;1000, "name"=&gt;"Linux", "arch"=&gt;"amd64", "version"=&gt;"3.13.0-24-generic", "available_processors"=&gt;1, "allocated_processors"=&gt;1},
 "process"=&gt;{"refresh_interval_in_millis"=&gt;1000, "id"=&gt;1, "mlockall"=&gt;false},
 "jvm"=&gt;
  {"pid"=&gt;1,
   "version"=&gt;"1.8.0_72-internal",
   "vm_name"=&gt;"OpenJDK 64-Bit Server VM",
   "vm_version"=&gt;"25.72-b15",
   "vm_vendor"=&gt;"Oracle Corporation",
   "start_time_in_millis"=&gt;1458929284547,
   "mem"=&gt;{"heap_init_in_bytes"=&gt;268435456, "heap_max_in_bytes"=&gt;1065025536, "non_heap_init_in_bytes"=&gt;2555904, "non_heap_max_in_bytes"=&gt;0, "direct_max_in_bytes"=&gt;1065025536},
   "gc_collectors"=&gt;["ParNew", "ConcurrentMarkSweep"],
   "memory_pools"=&gt;["Code Cache", "Metaspace", "Compressed Class Space", "Par Eden Space", "Par Survivor Space", "CMS Old Gen"],
   "using_compressed_ordinary_object_pointers"=&gt;"true"},
 "thread_pool"=&gt;
  {"force_merge"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;-1},
   "percolate"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;1000},
   "fetch_shard_started"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;2, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1},
   "listener"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;-1},
   "index"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;200},
   "refresh"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;1, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1},
   "suggest"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;1000},
   "generic"=&gt;{"type"=&gt;"cached", "keep_alive"=&gt;"30s", "queue_size"=&gt;-1},
   "warmer"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;1, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1},
   "search"=&gt;{"type"=&gt;"fixed", "min"=&gt;2, "max"=&gt;2, "queue_size"=&gt;1000},
   "flush"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;1, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1},
   "fetch_shard_store"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;2, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1},
   "management"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;5, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1},
   "get"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;1000},
   "bulk"=&gt;{"type"=&gt;"fixed", "min"=&gt;1, "max"=&gt;1, "queue_size"=&gt;50},
   "snapshot"=&gt;{"type"=&gt;"scaling", "min"=&gt;1, "max"=&gt;1, "keep_alive"=&gt;"5m", "queue_size"=&gt;-1}},
 "transport"=&gt;{"bound_address"=&gt;["172.17.0.6:9301"], "publish_address"=&gt;"192.168.253.160:9301", "profiles"=&gt;{}},
 "http"=&gt;{"bound_address"=&gt;["172.17.0.6:9201"], "publish_address"=&gt;"192.168.253.160:9201", "max_content_length_in_bytes"=&gt;104857600},
 "plugins"=&gt;[],
 "modules"=&gt;
  [{"name"=&gt;"lang-expression", "version"=&gt;"2.2.1", "description"=&gt;"Lucene expressions integration for Elasticsearch", "jvm"=&gt;true, "classname"=&gt;"org.elasticsearch.script.expression.ExpressionPlugin", "isolated"=&gt;true, "site"=&gt;false},
   {"name"=&gt;"lang-groovy", "version"=&gt;"2.2.1", "description"=&gt;"Groovy scripting integration for Elasticsearch", "jvm"=&gt;true, "classname"=&gt;"org.elasticsearch.script.groovy.GroovyPlugin", "isolated"=&gt;true, "site"=&gt;false}]}
```

**Elasticsearch version**: 2.2.1 build d045fc2
**JVM version**: 1.8.0_72
**OS version**: elasticsearch:2 Docker container

Steps to reproduce:
1. curl -XGET 'http://127.0.0.1:9200/_nodes?pretty=true'
</description><key id="143628155">17354</key><summary>/_nodes/ endpoint omits data key from attributes hash for data nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CVTJNII</reporter><labels><label>:Cluster</label></labels><created>2016-03-25T23:38:41Z</created><updated>2016-03-30T07:04:24Z</updated><resolved>2016-03-30T07:04:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-26T07:29:12Z" id="201723341">Hi @CVTJNII you mean that you would like to see `data: true` for the data node? We don't do that because it's the default, like we don't do it when master is true as well. This will improve with #16963 where we are separating node roles from attributes, so that nodes info always outputs the roles, but they are not part of attributes anymore. What do you think of that?
</comment><comment author="CVTJNII" created="2016-03-30T04:29:12Z" id="203242417">Yes, I believe #16963 will help satisfy the spirit of the request as I'm using this output to determine what the cluster type is.  However, I'd still prefer the values to be explicitly set, personally I always prefer explicit over implicit as it avoids confusion.  I also find 'true' to be an unexpected default, for booleans I assume the default will be 'false', though I suppose if that's documented and I missed it then it's okay.

I'd would also like to point out that the master is returning 'master: true' and not omitting it as a default.  As I mentioned above I prefer this behavior and think it would be beneficial if the data node could behave the same as well.  Thanks.
</comment><comment author="javanna" created="2016-03-30T07:04:09Z" id="203281106">If you rely on attributes to determine the node types, you need to take into account default values (in 2.x). All node types default to `true` (including `node.ingest`). With #16963 you can rely on the returned `roles` array instead which will always contain the roles that each node fulfils in the cluster (no matter if you set them explicitly or not). Roles won't be part of attributes anymore though, so you will need to switch to reading from `roles` from 5.0 on. I think we can consider this fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Access sort value in Spark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17353</link><project id="" key="" /><description>I am running an elasticsearch query with a "sort" on the field "_timestamp" and I want to get the "sort" value.

Running the query directly in python as something like:
`response = es.search(index="index-domains",doc_type="websites",body=query)`
I can access the "sort" value in each hit of the response.

If I try to run the same query on Spark, each value of the RDD does not have the field accessible.
`es_rdd = self.sc.newAPIHadoopRDD(inputFormatClass="org.elasticsearch.hadoop.mr.EsInputFormat",
                                keyClass="org.apache.hadoop.io.NullWritable",
                                valueClass="org.apache.hadoop.io.Text",
                                conf=es_conf)`

I tried to set:
`es_conf['es.read.metadata.sort'] = "true"
es_conf['es.read.metadata'] = "true"`

I can get more things out, but not the "sort" field.
`"_metadata":{"_index":"index-domains","_type":"websites","_id":"63DC925E46FFCBDEECD43047B6CEAF4B5F15DC4AF75E2C90E8DE12FC2834AFFE","_score":0.0}}'`

Is there a way to do so? I could not find anything on that in the docs.
</description><key id="143601245">17353</key><summary>Access sort value in Spark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">svebk</reporter><labels /><created>2016-03-25T21:20:00Z</created><updated>2016-03-25T21:23:43Z</updated><resolved>2016-03-25T21:23:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="svebk" created="2016-03-25T21:23:43Z" id="201512966">Post on hadoop repo.
https://github.com/elastic/elasticsearch-hadoop/issues/726
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ObjectParser and builders with required arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17352</link><project id="" key="" /><description>Many of our builders have required arguments: TermQueryBuilder, FieldValueFactorFunctionBuilder, etc. Normally when we use ObjectParser we have it call setter methods on pre-built objects. But that doesn't work for these objects. ObjectParser is much nicer than walking the xcontent on your own so we should figure out a way to overcome this. Ideas?
1. We could remove all the required parameters and/or create a constructor just for the ObjectParser. We already have static methods for lots of these builders anyway.
2. We could make `BuilderBuilder`s :scream:
3. We could dump everything in a map and pull it back out and build the object. Seems heavy.
   ???
</description><key id="143572239">17352</key><summary>ObjectParser and builders with required arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-25T19:08:49Z</created><updated>2016-04-27T18:16:21Z</updated><resolved>2016-04-27T18:16:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-25T22:39:54Z" id="201559046">As an addition to this, it would be nice if ObjectParser could handle empty objects correctly, for example, If I define an `ObjectParser` with three fields and then send `{}`, I expect to get an object with the 3 vars in their default variable, however, ObjectParser blows up on this.

This is related to the comment I left about ObjectParser not working in https://github.com/elastic/elasticsearch/pull/17305
</comment><comment author="nik9000" created="2016-03-29T11:43:26Z" id="202844876">&gt; ObjectParser blows up on this

If you make a failing test I'll fix it. Deal?
</comment><comment author="nik9000" created="2016-04-06T23:55:28Z" id="206625493">I'm fiddling with a solution for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPEs from ScoreFunctionBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17351</link><project id="" key="" /><description>Also lots of cleanup:
1. Line length
2. Lots more randomization on the tests
3. Work around some weirdness so we can test random score again
4. Move all the builders into one package. We don't need one per builder.
5. Switch to expectThrows.

Sorry there is so much here. I think it'd probably be easier to review each commit one at a time? I dunno.
</description><key id="143569276">17351</key><summary>Remove PROTOTYPEs from ScoreFunctionBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-25T18:58:45Z</created><updated>2016-05-02T12:15:57Z</updated><resolved>2016-04-06T17:21:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-28T13:53:45Z" id="202403588">@javanna I pushed another commit. Can you have a look at it when you get a chance?
</comment><comment author="javanna" created="2016-03-30T15:21:27Z" id="203485526">I left a few comments, looks good in general. It is fine to fix line length problems while at it, but I think this PR got a bit too big, although I do appreciate that you left all the separate commits. It's just that when I look at them, some don't have much to do with this PR and could have been another one (fixing and expanding tests, move to expectThrows etc.). Otherwise it becomes harder to review things and catch problems. But again, thanks a lot for making all those great changes!
</comment><comment author="nik9000" created="2016-03-30T15:35:34Z" id="203491485">Sorry about things getting mixed in. Some of the expectThrows changes, for example, came up because I had to alter the tests to get them to compile and I saw the old way and just went at it. But, yeah, I'll not do it next time.

The expanded test was really important here I think. Without it we can't test that I didn't break it.
</comment><comment author="nik9000" created="2016-03-30T16:40:27Z" id="203517813">@javanna pushed a new commit to change how I hacked the random score function to work. This one keep the production code the same by introducing a new score function that subclasses random score function. It is less intrusive but still funky. I think it is better though.
</comment><comment author="nik9000" created="2016-03-30T16:53:47Z" id="203524497">OK - so I removed read from the FunctionScoreParser interface. Now it is tiny! I wonder if we can remove the getNames method from it and make it a functional interface. We can pass the names in when we register the function score.
</comment><comment author="nik9000" created="2016-03-30T17:00:44Z" id="203527193">&gt; make it a functional interface

Then the fromXContent method on it can just live in the builder as a static method or on a package scoped ObjectParser instance.

That sounds good, but maybe it should wait for another PR.
</comment><comment author="nik9000" created="2016-03-31T12:26:18Z" id="203906064">@javanna I've pushed to fixed for the issues you raised for your reviewing pleasure when you get back from lunch.
</comment><comment author="nik9000" created="2016-03-31T18:12:56Z" id="204061005">@javanna Ok - rebased as discussed. Then I removed the registration back into SearchModule.
</comment><comment author="javanna" created="2016-04-01T18:01:37Z" id="204494150">I am done with my neverending review, sorry for keeping you on hold @nik9000 . I like it. Left a few minor comments, the main thing is I don't whether we want to wait till we make a decision on #17458 or get it in and then change it later based on that.
</comment><comment author="nik9000" created="2016-04-04T14:45:06Z" id="205328360">I've rebased it because it was super out of date. I also handled @javanna's comment on visibility.
</comment><comment author="javanna" created="2016-04-04T15:19:46Z" id="205345318">LGTM there are two main comments left to address about how the registration should look and making  the function parser a functional interface, but those can wait for another PR
</comment><comment author="nik9000" created="2016-04-04T15:30:41Z" id="205351514">&gt; LGTM there are two main comments left to address about how the registration should look and making the function parser a functional interface, but those can wait for another PR

Er, I just did those. Sorry. I just modified the interface of SearchModule to take the ParserField, I didn't change the looks to use ParseField. Maybe that should wait?
</comment><comment author="nik9000" created="2016-04-04T16:21:05Z" id="205374359">@javanna done with latest round of changes.
</comment><comment author="nik9000" created="2016-04-05T18:14:25Z" id="205926471">@javanna rebased again on top of your work on queries and made the changes you wanted to lookupScoreFunctionParser. Please have a look when you get a chance. This one feels like it has been hanging over my head for a while.
</comment><comment author="javanna" created="2016-04-06T08:53:42Z" id="206230096">I did another round, I had a few comments on how we look functions up still, and naming convention. maybe @cbuescher and @colings86 want to have a quick look as well so we all agree on these.
</comment><comment author="nik9000" created="2016-04-06T16:10:25Z" id="206444893">@javanna  and @cbuescher, round n+1 is ready.
</comment><comment author="nik9000" created="2016-04-06T16:42:17Z" id="206457773">@javanna and @cbuescher round n+2 ready.
</comment><comment author="javanna" created="2016-04-06T16:46:20Z" id="206459456">:clap: LGTM :metal: 
</comment><comment author="nik9000" created="2016-04-06T16:47:20Z" id="206460010">:metal: 
</comment><comment author="nik9000" created="2016-04-06T17:21:23Z" id="206473403">And we're merged!
</comment><comment author="nik9000" created="2016-04-06T17:21:30Z" id="206473428">Thanks @javanna this one was a mess.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type filters should not have a performance impact when there is a single type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17350</link><project id="" key="" /><description>Today, if you call /index/type/_search instead of /index/_search, elasticsearch
will automatically insert a type filter to only match documents of the given
type. This commit uses a new TypeQuery instead of a TermQuery for this filter,
which rewrites to a MatchAllDocsQuery when all documents of a shard match the
filtered type. This is helpful because BooleanQuery has a special rewrite rule
to remove MatchAllDocsQuery as FILTER clauses. So for instance if your query is
`+body:"quick fox" #_type:my_type`, it will be rewritten to
`+body:"quick fox" #*:*` which is then rewritten to `body:"quick fox"`.
</description><key id="143561357">17350</key><summary>Type filters should not have a performance impact when there is a single type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-25T18:27:38Z</created><updated>2016-03-29T06:55:02Z</updated><resolved>2016-03-29T06:55:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-25T18:51:10Z" id="201420229">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove legacy geo_point type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17349</link><project id="" key="" /><description>GeoPointV2 off loaded the majority of the `geo_point` functionality to lucene but supported backcompat w/ 1.x and 2.x. ES 6.0 will completely remove legacy `geo_point` type support in favor of better performing `GeoPointField` and `LatLonPoint` in Lucene 6+.
</description><key id="143558554">17349</key><summary>Remove legacy geo_point type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>breaking</label><label>v6.0.0</label></labels><created>2016-03-25T18:13:05Z</created><updated>2017-05-03T06:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-04-01T17:15:38Z" id="204475727">for the `LatLonPoint` I definitely think we should wait until Lucene 6.1.

its still in the sandbox now because it has sandiness, but it already improves a lot for 6.1. In 6.1 its "index format changes" in that it uses docvalues and encodes slightly differently into the BKD. In 6.1 it uses two-phase iterator for all operations (this is currently very important for performance, the 6.0 version would have performance regressions to users.) Also it gets an efficient sort comparator that does not need to do kagillions of haversin comparisons. But, it still has some known bugs around corner cases and other sandiness, so there is still work to do. I think we can get it cleaned up for 6.1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature][function_score] Limit individual function's score in functions array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17348</link><project id="" key="" /><description>Currently, there does not appear to be a way to place an upper or lower bound on an individual function within a function_score functions array. It would be nice to be able to place either a max or min limit on the individual function to prevent something like a field_value_factor from overshadowing other more relevant signals.

Example function_score:

"function_score": {
    "query": {},
    "boost": "boost for the whole query",
    "functions": [
        {
            "filter": {},
            "FUNCTION": {}, 
            "weight": number,
            "min_score": number                                 // New Feature
        },
        {
            "FUNCTION": {},
            "max_score": number                                // New Feature
        },
        {
            "filter": {},
            "weight": number
        }
    ],
    "max_boost": number,
    "score_mode": "(multiply|max|...)",
    "boost_mode": "(multiply|replace|...)",
    "min_score" : number
}
</description><key id="143533270">17348</key><summary>[Feature][function_score] Limit individual function's score in functions array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sean-cherbone</reporter><labels><label>:Query DSL</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-25T16:16:28Z</created><updated>2016-03-25T19:57:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T17:34:17Z" id="201380654">My initial thought was that this could be done easily with a script.  My next thought was that, actually, this could be generally useful without having to resort to scripting.  eg a gaussian decay can end up returning zero which, when multiplied by other factors....
</comment><comment author="sean-cherbone" created="2016-03-25T18:03:50Z" id="201393963">I too had considered script (and may still use it if needed) but feel that limiting low priority signal strength is a sufficiently straight forward need that functions can benefit from it.

For example, let's say I have the following factors that could indicate relevance:
- long_view_count
- short_view_count
- share_count
- tweet_count
- facebook_count
- up_vote_count
- down_vote_count
- etc...

Here are a couple of conditions that could cause problems with this scheme:
- People discover this and exploit this ranking feature to boost their irrelevant document to the top.
- A marginally relevant document that has been around for awhile and is well advertised overtakes a much more relevant but new or less advertised document.
</comment><comment author="clintongormley" created="2016-03-25T18:07:48Z" id="201397315">Well, normally you'd use a log function so that each value counts for less the higher it goes (ie the first 5 votes count a lot, but votes 100+ count for little more)
</comment><comment author="sean-cherbone" created="2016-03-25T18:22:06Z" id="201403088">Agreed, and I am using such tapering modifiers as well but here is another example that may help.

Let's say I also want to factor in cost. For most documents $10 to $100 is typical. I would expect that range to have a linear curve, representing how the average person feels about spending money on non-essentials. Now lets say that there is something that costs $1000 or $10,000, those are so far beyond the typical reach of most people that they are essentially the same but taking the log of each returns a substantial difference. Placing a max limit here would allow me to truncate these outliers as a way of saying, "they're high cost but potentially still relevant" and leaving it at that rather then (in my case) driving them way down in the relevance scale even if the cost is completely reasonable for this type of document.

I should also point out that in the case of votes, those too can be a problem, even when scaled down with log. I want to give new documents a fighting chance of being seen even though they start out with 0 votes. If some other documents that is years old has many thousands of votes, even taking the log of that will create a major boost over the new document. Here again, it may be appropriate to say that those documents having 10 to 100 votes may be proportionately more relevant but all documents having a vote count of 100 or more may be considered simply "popular" without overwhelming the other signals.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add query extract support for the blended term query and the common terms query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17347</link><project id="" key="" /><description /><key id="143530552">17347</key><summary>Add query extract support for the blended term query and the common terms query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-25T16:01:28Z</created><updated>2016-03-25T22:25:22Z</updated><resolved>2016-03-25T22:25:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-25T16:49:39Z" id="201359790">@nik9000 I've updated the PR and added more tests. Including a higher level test that also goes thought the builders.
</comment><comment author="nik9000" created="2016-03-25T16:57:58Z" id="201362798">LGTM
</comment><comment author="martijnvg" created="2016-03-25T22:25:03Z" id="201550817">Thanks Nik!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>documentation - 2.2 breaking change - ClusterHealthStatus</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17346</link><project id="" key="" /><description>a breaking change for the java client:
in 2.2 ClusterHealthStatus moved to another package
</description><key id="143489407">17346</key><summary>documentation - 2.2 breaking change - ClusterHealthStatus</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brackxm</reporter><labels><label>docs</label></labels><created>2016-03-25T12:26:42Z</created><updated>2016-03-31T11:41:31Z</updated><resolved>2016-03-31T11:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-25T14:22:26Z" id="201306631">Is it the same change as documented here? https://www.elastic.co/guide/en/elasticsearch/reference/current/float.html#_java_client
</comment><comment author="danielmitterdorfer" created="2016-03-31T11:32:30Z" id="203888652">@dadoonet No, this is related but not the same change. I've created a mini PR for this. Can you please have look at #17444?
</comment><comment author="danielmitterdorfer" created="2016-03-31T11:41:18Z" id="203892104">Fixed by #17444
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not retrieve all indices stats when checking for cache resets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17345</link><project id="" key="" /><description>Tiny optimization, I came across this when looking at a failing test.
</description><key id="143488479">17345</key><summary>Do not retrieve all indices stats when checking for cache resets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>review</label><label>test</label></labels><created>2016-03-25T12:20:15Z</created><updated>2016-03-25T12:38:48Z</updated><resolved>2016-03-25T12:38:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2016-03-25T12:29:06Z" id="201263026">not sure why we need this tiny optimization to the test... but LGTM
</comment><comment author="tlrx" created="2016-03-25T12:38:48Z" id="201265624">We don't "need" it but if we can avoid to load a whole bunch of stats when we only need 3, I think we should. Thanks for the review.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Response for create index with config {"refresh_interval": "300"} is misleading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17344</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.1

**JVM version**: 1.7.0_60

**OS version**: cent os

**Description of the problem including expected versus actual behavior**:
In es 1.x generation, create index with config {"refresh_interval": "300"} is ok, but in es 2.2.1,   create index with config {"refresh_interval": "300"}, the response is {"acknowledged":true} and index created, but no one shard is assigned, the message in log is 

`[error_info11][[error_info11][37]] ElasticsearchException[failed to create shard]; nested: ElasticsearchParseException[Failed to parse setting [index.refresh_interval] with value [300] as a time value: unit is missing or unrecognized];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:371)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:602)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:502)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:166)
    at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:600)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: ElasticsearchParseException[Failed to parse setting [index.refresh_interval] with value [300] as a time value: unit is missing or unrecognized]`
So index is created, but you can't use

**I know the true config is {"refresh_interval": "300s"}, but I think the response is error with error message and index created false rather than  response is {"acknowledged":true} and created index yout can't use**

**Steps to reproduce**:

`curl -XPOST dmslave22.et2:9200/test -d '{
    "settings": {
        "refresh_interval": "300"
    },
    "mappings": {
        "_default_":{
            "properties": {
                "field1": {
                    "type": "string"
                }
            }
        }
    }
}'`

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

 Response for create index with config {"refresh_interval": "300"} is misleading
</description><key id="143459954">17344</key><summary> Response for create index with config {"refresh_interval": "300"} is misleading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adol001</reporter><labels /><created>2016-03-25T09:01:16Z</created><updated>2016-03-25T21:21:03Z</updated><resolved>2016-03-25T16:46:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T16:46:06Z" id="201358982">In 2.x, units are required.  In fact, units are there to help you avoid the mistake that you made.  In 1.x, `300` means `300ms`, not `300s`!  I agree that the action of creating the index but then failing on shard creation is poor.  This has been fixed in 5.0 with https://github.com/elastic/elasticsearch/pull/17187
</comment><comment author="s1monw" created="2016-03-25T21:21:03Z" id="201511348">I think the actual problem here is the `{"acknowledged":true}` is the problem here. In 5.0 this request would have been rejected so it's fixed in 5.0. Unfortunately backporting to 2.x is not feasible.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 2.1.1  when query integer ,the actual result is not consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17343</link><project id="" key="" /><description>"connectMode":{"type":"string","index":"not_analyzed"},
connectMode: "12" ||  "48" || "1"      All the results will be listed    why?
connectMode: "12" ||  "48"     Results is  correct.
connectMode:  "1"    Results is  correct.

"logRank":{"type":"integer","index":"not_analyzed"},

logRank: 1  Results is  correct.
logRank: 1  || 2  || 3   All the results will be listed    why?
</description><key id="143448313">17343</key><summary>elasticsearch 2.1.1  when query integer ,the actual result is not consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dick1306</reporter><labels /><created>2016-03-25T07:50:47Z</created><updated>2016-03-25T16:38:55Z</updated><resolved>2016-03-25T16:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T16:38:55Z" id="201357315">Duplicate of https://github.com/elastic/elasticsearch/issues/17342
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 2.1.1  when query integer ,the actual result is not consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17342</link><project id="" key="" /><description>"connectMode":{"type":"string","index":"not_analyzed"},
connectMode: "12" ||  "48" || "1"      All the results will be listed    why?
connectMode: "12" ||  "48"     Results is  correct.
connectMode:  "1"    Results is  correct.

"logRank":{"type":"integer","index":"not_analyzed"},

logRank: 1  Results is  correct.
logRank: 1  || 2  || 3   All the results will be listed    why?
</description><key id="143448302">17342</key><summary>elasticsearch 2.1.1  when query integer ,the actual result is not consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dick1306</reporter><labels /><created>2016-03-25T07:50:43Z</created><updated>2016-03-25T16:38:41Z</updated><resolved>2016-03-25T16:38:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T16:38:41Z" id="201357234">@dick1306 I'm afraid I don't understand your issue at all.  But I'd suggest that the best place to start is in the forums: https://discuss.elastic.co/.  If you confirm that there is a bug in Elasticsearch, then please open a ticket with a full recreation here again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to group tasks by common parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17341</link><project id="" key="" /><description>By default, tasks are grouped by node. However, task execution in elasticsearch can be quite complex and an individual task that runs on a coordinating node can have many subtasks running on other nodes in the cluster. This commit makes it possible to list task grouped by common parents instead of by node. When this option is enabled all subtask are grouped under the coordinating node task that started all subtasks in the group. To group tasks by common parents, use the following syntax:

 GET /tasks?group_by=parents
</description><key id="143421492">17341</key><summary>Add ability to group tasks by common parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-25T04:32:55Z</created><updated>2016-03-30T23:28:32Z</updated><resolved>2016-03-30T23:28:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-28T14:07:44Z" id="202407632">Left some minor stuff but I'm mostly happy with it. Can you add a test that the XContent comes out right? Like just build the response and call toXContent with the params for grouping by task. I'm mostly concerned that the contract for ToXContent isn't very strict and if Tasks start emitting their enclosing object this will break and we won't notice.

Do you have to do anything on the REST layer to pipe the the param through? Did I miss something?
</comment><comment author="imotov" created="2016-03-28T22:54:11Z" id="202615408">&gt; Do you have to do anything on the REST layer to pipe the the param through? Did I miss something?

I don't think so. See [RestToXContentListener](https://github.com/elastic/elasticsearch/blob/c7c8bb357a1ef459a7c59db4da9cbda1c0d439db/core/src/main/java/org/elasticsearch/rest/action/support/RestToXContentListener.java#L46).

I have added a test for grouping. 
</comment><comment author="nik9000" created="2016-03-29T12:50:46Z" id="202877816">&gt; I don't think so. See RestToXContentListener.

Oh! I see it now. Funky, but it makes sense.
</comment><comment author="nik9000" created="2016-03-29T13:53:46Z" id="202903740">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add created flag to IndexingOperationListener#postIndex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17340</link><project id="" key="" /><description>fixes #17333
</description><key id="143381715">17340</key><summary>Add created flag to IndexingOperationListener#postIndex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">oillio</reporter><labels><label>:Java API</label><label>enhancement</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T22:52:15Z</created><updated>2016-03-30T20:21:26Z</updated><resolved>2016-03-30T08:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-25T21:24:16Z" id="201513404">this looks great - I will pull this in early next week
</comment><comment author="s1monw" created="2016-03-30T08:55:37Z" id="203330241">merged thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Kibana 5 error with ES 5.0-snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17339</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 5.0-snapshot

**JVM version**:

**OS version**: MacOS Yosemite

**Description of the problem including expected versus actual behavior**:

Upon loading data into Elasticsearch, the Discover tab displays an error (screenshot attached).

**Steps to reproduce**:
1. On a fresh Kibana 5/Elasticsearch 5.0-snapshot install, load the tutorial datasets from https://www.elastic.co/guide/en/kibana/current/getting-started.html.
2. Define the three index patterns
3. Open Visualize.

**Provide logs (if relevant)**:

![screen shot 2016-03-24 at 14 41 52](https://cloud.githubusercontent.com/assets/1779279/14032280/ddb42cf8-f1ce-11e5-8ae6-64cec2fbeec0.png)
</description><key id="143369063">17339</key><summary>Kibana 5 error with ES 5.0-snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels><label>feedback_needed</label></labels><created>2016-03-24T21:48:06Z</created><updated>2016-10-18T08:51:11Z</updated><resolved>2016-03-24T21:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-24T21:50:44Z" id="201040477">I think this should be opened in the [Kibana repository](https://github.com/elastic/kibana) until there is a verified bug in Elasticsearch here, no?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add created flag to IndexingOperationListener#postIndex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17338</link><project id="" key="" /><description>fixes #17333
</description><key id="143365637">17338</key><summary>Add created flag to IndexingOperationListener#postIndex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oillio</reporter><labels /><created>2016-03-24T21:32:40Z</created><updated>2016-03-24T21:53:06Z</updated><resolved>2016-03-24T21:53:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-24T21:53:06Z" id="201041933">Can you sign the CLA, and reopen this against master?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from SortBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17337</link><project id="" key="" /><description>Related to #17085
</description><key id="143355088">17337</key><summary>Remove PROTOTYPE from SortBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T20:47:11Z</created><updated>2016-03-30T18:16:18Z</updated><resolved>2016-03-26T02:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-25T11:59:32Z" id="201251637">@javanna another one!
</comment><comment author="javanna" created="2016-03-25T20:56:37Z" id="201497401">left some minors, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from ShapeBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17336</link><project id="" key="" /><description>As a side effect I think I replaced all of their error testing with `expectThrows` and I know I fixed all the line length violations in the ShapeBuilders.

Lastly I cleaned up DistanceUnit. It had serialization problems. You can read the commit history for more commentary.

Relates to #17085
</description><key id="143331548">17336</key><summary>Remove PROTOTYPE from ShapeBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T19:01:04Z</created><updated>2016-03-30T18:20:13Z</updated><resolved>2016-03-26T17:59:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T19:01:11Z" id="200972074">@javanna another one for you.
</comment><comment author="javanna" created="2016-03-25T21:13:35Z" id="201507104">left two minors, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup placeholder replacement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17335</link><project id="" key="" /><description>This change moves placeholder replacement to a pkg private class for
settings. It also adds a null check when calling replacement, as
settings objects can still contain null values, because we only prohibit
nulls on file loading. Finally, this cleans up file and stream loading a
bit to not have unnecessary exception wrapping.
</description><key id="143331101">17335</key><summary>Cleanup placeholder replacement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha4</label></labels><created>2016-03-24T18:59:15Z</created><updated>2016-06-03T20:34:42Z</updated><resolved>2016-06-03T20:34:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T21:36:31Z" id="208573276">@jasontedor I pushed a new commit.
</comment><comment author="clintongormley" created="2016-05-07T14:35:10Z" id="217641456">@jasontedor please could you review this again?
</comment><comment author="jasontedor" created="2016-06-03T20:34:27Z" id="223687616">I merged master in, fixed a test in 974c753bf6fdc00d30bab96d737447a0b9113a8f and added some additional detail to the exception message assertions in be0036542c4d1c40c0044cd22f45ea1d2534b8ec. Feel free to revert this last one if you don't agree. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace FieldStatsProvider with a method on MappedFieldType.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17334</link><project id="" key="" /><description>FieldStatsProvider had to perform instanceof calls to properly handle dates or
ip addresses. By moving the logic to MappedFieldType, each field type can check
whether all values are within bounds its way.

Note that this commit only keeps rewriting support for dates, which are the only
field for which the rewriting mechanism is likely to help (because of time-based
indices).
</description><key id="143316469">17334</key><summary>Replace FieldStatsProvider with a method on MappedFieldType.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T17:54:42Z</created><updated>2016-04-01T08:29:51Z</updated><resolved>2016-04-01T08:29:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-29T09:15:27Z" id="202794096">@jpountz I left some comments on the tests but I like how much this change simplifies the logic
</comment><comment author="jpountz" created="2016-03-31T14:57:24Z" id="203975301">@colings86 I pushed more commits to improve the testing. Would you mind having another look?
</comment><comment author="colings86" created="2016-04-01T07:47:44Z" id="204297242">@jpountz Thanks for making the changes to the tests. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide created flag to IndexingOperationListener#postCreate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17333</link><project id="" key="" /><description>I am working on a plugin that provides an IndexingOperationListener.  It would be very helpful if there was a way to tell if an indexing operation created or updated a document during the postCreate call.

**Describe the feature**:  The easiest way I see to provide this information would be to add a "created" field to the Engine.Index class.  This would be set during the index operation, just after the version is updated on this object ([here](https://github.com/elastic/elasticsearch/blob/e50eeeaffb33d7eaa0f7c2ebc71f1208460f4fcd/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L368-L376)).

I would be happy to submit a PR for this, if it is an acceptable solution.
</description><key id="143301970">17333</key><summary>Provide created flag to IndexingOperationListener#postCreate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">oillio</reporter><labels><label>:CRUD</label><label>:Java API</label></labels><created>2016-03-24T16:58:34Z</created><updated>2016-03-30T08:56:32Z</updated><resolved>2016-03-30T08:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-24T20:03:06Z" id="200997237">I think this is useful but I think we should rather change the `default void postIndex(Engine.Index index) {}` to be `default void postIndex(Engine.Index index, boolean created) {}` in `IndexingOperationListener` since it might be different on the primary and the replica? But please go ahead and submit a PR
</comment><comment author="oillio" created="2016-03-24T23:12:53Z" id="201068569">PR submitted.  While looking through the code on master, it looks like the application of IndexingOperationListener has been changed significantly compared to 2.x.

From what I can tell, there is no longer any way for a plugin to register an IndexingOperationListener on a shard.
</comment><comment author="s1monw" created="2016-03-25T21:26:37Z" id="201515810">&gt; From what I can tell, there is no longer any way for a plugin to register an IndexingOperationListener on a shard.

yeah that is true, I trashed in a refactoring and planned to add it back later... I guess I dropped the ball... 
do you wanna open another PR and add it back similar to `IndexModule#addIndexEventListener(IndexEventListener listener)` there are also tests for this in `IndexModuleTests`. Your other PR looked awesome, I would be looking forward to you adding that missing API back too?
</comment><comment author="s1monw" created="2016-03-30T08:56:32Z" id="203331043">FYI - nI was working on a similar functionality for search and added a register method on master for the `IndexingOperationListener` in https://github.com/elastic/elasticsearch/pull/17398
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw an exception if Writeable.Reader reads null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17332</link><project id="" key="" /><description>If a Writeable.Reader returns null it is always a bug, probably one that
will cause corruption in the StreamInput it was trying to read from. This
commit adds a check that attempts to catch these errors quickly including
the name of the reader.
</description><key id="143299612">17332</key><summary>Throw an exception if Writeable.Reader reads null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T16:51:44Z</created><updated>2016-03-25T16:20:26Z</updated><resolved>2016-03-24T17:03:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T16:52:14Z" id="200921019">@javanna this one tries to catch nasty mistakes earlier.
</comment><comment author="javanna" created="2016-03-24T16:58:26Z" id="200925986">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Are rescorers an extension point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17331</link><project id="" key="" /><description>Right now Elasticsearch only support one kind of rescorer: the query rescorer. But the code looks like we intended to support other rescorers. This feels like a place where Elasticsearch could expose an extension point and plugins could register additional rescorers the same way they can register additional queries. That doesn't work right now, but it wouldn't be too hard. If we did that we'd want to make an "example rescorer plugin" and all that.

By the same token, if we don't want to make rescorer an extension point we can clean up the class hierarchy and the parsing, including the public API.
</description><key id="143298127">17331</key><summary>Are rescorers an extension point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-03-24T16:45:21Z</created><updated>2016-09-08T09:08:24Z</updated><resolved>2016-09-08T09:08:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T16:45:42Z" id="200918706">@brwe, I get the sense that you might have opinions on this.
</comment><comment author="javanna" created="2016-03-24T16:46:41Z" id="200918988">I have the feeling that this was supposed to be extensible but we have broken it while refactoring things, maybe as part of the search refactoring. In fact the serialization bits are good, the parsing part needs some adapting though.
</comment><comment author="cbuescher" created="2016-03-24T18:51:11Z" id="200967536">Looking at the 2.x branch, the RescoreParseElement was never able to parse more than the QueryRescorer (apropriately named `"query"`), so this seems like it was never supported, but we could certainly think about adding it (similar to suggesters). But I'm not sure it would be a widely used feature, since you can already do a lot with query rescoring. Might be mistaken on that part though.
</comment><comment author="s1monw" created="2016-03-24T20:08:02Z" id="200999313">I think having the ability to implement a different rescorer is good and we should keep it. I also think this doesn't necessarily need to extend the search request parsing ie. when somebody want to pass something to the rescorer can they use `ext`? or maybe we just don't support it until somebody wants to do it and we add it laster. I think  we should maybe allow to register a different `Rescorer` but make the parse element not extendable?
</comment><comment author="javanna" created="2016-09-08T09:08:24Z" id="245538614">Given that this extension point was never really there, and there was no much activity on this issue either, I would go for leaving things as they are (no extension point) and eventually add what we need to add later when needed. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove PROTOTYPE from RescorerBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17330</link><project id="" key="" /><description>This changes the serialization order for QueryRescorerBuilder's but
that is ok because 5.0.0 doesn't need to be wire compatible with anything.
</description><key id="143290328">17330</key><summary>Remove PROTOTYPE from RescorerBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T16:15:46Z</created><updated>2016-04-05T10:57:56Z</updated><resolved>2016-03-24T17:05:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T16:15:52Z" id="200906590">@javanna first one!
</comment><comment author="javanna" created="2016-03-24T16:57:21Z" id="200925198">left a minor, LGTM otherwise. Maybe we should mark breaking given that it breaks bw on the serialization layer? Not a problem as it is targeted to 5.0 though.
</comment><comment author="nik9000" created="2016-03-24T16:58:56Z" id="200926358">Marked as breaking because it breaks the wire protocol but doesn't break the public API. Not the java or the REST or even the plugin API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include pings from client nodes in master election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17329</link><project id="" key="" /><description>We currently have a `discovery.zen.master_election.filter_client` setting that control whether their ping responses are ignored for master election (which is the current default). With the push to treat client nodes as normal nodes (and promote the transport/rest clients for client work), this should be changed. This commit remove this setting and it's companion `discovery.zen.master_election.filter_data` setting (currently defaulting to  false) in favor of singe `discovery.zen.master_election.ignore_non_master_pings` setting with more intuitive name (defaulting to false).

Resolves #17325
</description><key id="143290045">17329</key><summary>Include pings from client nodes in master election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>:Settings</label><label>breaking-java</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T16:14:28Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-24T16:56:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-24T16:22:02Z" id="200908830">left some nitpicks, LGTM otherwise, thanks @bleskes 
</comment><comment author="jasontedor" created="2016-03-24T16:24:25Z" id="200909830">Left a comment about the name of the setting; feel free to ignore or include :wink: at your discretion and otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster stats: fix memory available that is always set to 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17328</link><project id="" key="" /><description>As discussed in #17278 I am fixing the memory available value in the cluster stats api, for the 2.x series, although we removed this field on master. The value is always set to 0 since 2.0 beta1.
</description><key id="143289826">17328</key><summary>Cluster stats: fix memory available that is always set to 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>bug</label><label>v2.3.1</label><label>v2.4.0</label></labels><created>2016-03-24T16:13:25Z</created><updated>2016-03-29T18:12:08Z</updated><resolved>2016-03-29T18:12:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-24T16:24:15Z" id="200909744">There is still one flaw though, besides the field not being useful. Given the rest response one doesn't even understand what the value holds:

```
"nodes" : {
  os" : {
      "available_processors" : 12,
      "allocated_processors" : 12,
      "mem" : {
        "total_in_bytes" : 123456
      },
      "names" : [ {
        "name" : "Linux",
        "count" : 1
      } ]
    }
}
```

The output says `mem.total_in_bytes`, while the java api calls it `availableMemory`. This seems inconsistent.... I am again leaning towards removing this setting in 2.x as we may have to break bw comp to fix this inconsistency anyway....
</comment><comment author="javanna" created="2016-03-25T09:17:17Z" id="201214627">@tlrx @pickypg me again :) what do you think?
</comment><comment author="pickypg" created="2016-03-25T15:29:18Z" id="201333236">&gt; The output says mem.total_in_bytes, while the java api calls it availableMemory. This seems inconsistent.... I am again leaning towards removing this setting in 2.x as we may have to break bw comp to fix this inconsistency anyway...

That does change things. Technically you could just add each field to the other's output and be totally BWC, but I agree that's unnecessary given that no one would be using it the "new" field and it's going away in 5.0. I'm +1 on removing in 2.4. I still have the same resistance about removing an "expected" field in a patch release, but I don't honestly feel strong about it now that there's yet another discrepancy.

Patch LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve test to not rely on thread slowness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17327</link><project id="" key="" /><description>We have to swap the second latch before we count it down otherwise
threads might be faster than the test. This has happend on a recent
CI failure: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=ubuntu/121/console

This commit also adds a synchronized on the close method since it's
canceling and modifying a member varialbe that is assigned under lock.
</description><key id="143278751">17327</key><summary>Improve test to not rely on thread slowness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T15:33:24Z</created><updated>2016-03-24T18:29:36Z</updated><resolved>2016-03-24T15:51:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-03-24T15:46:30Z" id="200895383">+1, sneaky concurrency horrors ... I left a couple comments on maybe avoiding a lambda here.
</comment><comment author="s1monw" created="2016-03-24T15:49:26Z" id="200896661">@mikemccand pushed updates - less fancy same thing
</comment><comment author="mikemccand" created="2016-03-24T15:51:25Z" id="200897681">+1, thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MergeScheduler settings are not dynamically set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17326</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.8

**OS version**: Mac OSX 10.10.5

**Description of the problem including expected versus actual behavior**: According to https://github.com/elastic/elasticsearch/blob/2.2/docs/reference/index-modules/merge.asciidoc, the merge scheduler supports "dynamic" setting of "index.merge.scheduler.max_thread_count". But when I try to set it dynamically, ES emits a warning which says the setting is ignored.

**Steps to reproduce**:
1. start ES 2.2.0.
2. issue this update -

curl -XPUT 'localhost:9200/_cluster/settings' -d '{
    "transient" : {
        "index.merge.scheduler.max_thread_count": 1
    }
}'
1. ES ignores the setting and emits this warning -

[2016-03-24 10:16:00,069][WARN ][action.admin.cluster.settings] [Dougboy] ignoring transient setting [index.merge.scheduler.max_thread_count], not dynamically updateable

**Provide logs (if relevant)**:

present in steps to reproduce.

**Describe the feature**:
The docs explicitly mentions that the setting is "dynamic", which to me means its can be set through the rest API. If the setting is not available through the REST API, then the docs need to clarify it.
</description><key id="143277525">17326</key><summary>MergeScheduler settings are not dynamically set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkelkarbv</reporter><labels><label>non-issue</label></labels><created>2016-03-24T15:29:08Z</created><updated>2016-03-24T15:39:07Z</updated><resolved>2016-03-24T15:38:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-24T15:38:55Z" id="200891148">this is a per-index setting you have to use the index settings update API 

``` json
curl -XPUT 'http://localhost:9200/your_index_name/_settings' -d '{
    "index" : {
        "merge.scheduler.max_thread_count": 1
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename discovery.zen.master_election.filter_client setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17325</link><project id="" key="" /><description>After the removal of the `node.client` setting with #16963, we have gotten rid of the client node terminology in a lot of places, but we still have masterElectionFilterClientNodes in ZenDiscovery, and the corresponding `discovery.zen.master_election.filter_client` which should be accordingly renamed, if it still makes sense to keep.
</description><key id="143258912">17325</key><summary>Rename discovery.zen.master_election.filter_client setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>blocker</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T14:17:38Z</created><updated>2016-03-25T09:15:34Z</updated><resolved>2016-03-24T16:56:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Settings loader cleanup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17324</link><project id="" key="" /><description>This pull request is a simple cleanup of code related to loading
settings from JSON, YAML, and properties. The cleanup is just removing
some duplicate code, fixing line-length violations, and converting the
tests to use expectThrows where applicable. The only meaningful change
is commit 7323c373391f54cbeaf56ac27bd7b702c2c97d31 to refactor the
PropertiesSettingsLoader, the rest of the changes are
straightforward. The changes are broken into small commits and are
self-explanatory.
</description><key id="143257723">17324</key><summary>Settings loader cleanup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T14:14:39Z</created><updated>2016-03-24T14:44:12Z</updated><resolved>2016-03-24T14:44:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T14:22:08Z" id="200860403">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for several span queries in ExtractQueryTermsService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17323</link><project id="" key="" /><description /><key id="143255981">17323</key><summary>Add support for several span queries in ExtractQueryTermsService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T14:06:37Z</created><updated>2016-03-31T07:55:49Z</updated><resolved>2016-03-31T07:55:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T17:48:29Z" id="203024086">Left some minor comments.
</comment><comment author="martijnvg" created="2016-03-30T12:55:06Z" id="203419244">@jpountz I've updated the PR.
</comment><comment author="jpountz" created="2016-03-30T15:13:24Z" id="203482818">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>term vs termQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17322</link><project id="" key="" /><description>Shouldn't this read "termQuery" ?
</description><key id="143248878">17322</key><summary>term vs termQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Java API</label><label>docs</label><label>v2.2.2</label><label>v2.3.1</label><label>v2.4.0</label></labels><created>2016-03-24T13:42:11Z</created><updated>2016-06-24T09:30:52Z</updated><resolved>2016-06-24T09:28:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-07T14:35:33Z" id="217641475">@dadoonet could you look at this please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Fix vagrant test on debian</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17321</link><project id="" key="" /><description>Debian asks during installation, if the configuration file should be updated.
This is asked via a prompt and thus hangs.

This adds an option to always update to the newer config file, so automated
installation keeps working.
</description><key id="143243155">17321</key><summary>Tests: Fix vagrant test on debian</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label></labels><created>2016-03-24T13:22:39Z</created><updated>2016-03-24T13:42:20Z</updated><resolved>2016-03-24T13:37:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T13:25:34Z" id="200832135">LGTM. For reference, this is an instance of the failure:
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+packaging-tests/47/console
</comment><comment author="jasontedor" created="2016-03-24T13:42:20Z" id="200838902">This is because of the addition of debug logging added in 7ecfa6e2adc472cd897aeb2c245f3a095fc65d78 to try to get to the bottom of #17294. When we get to the bottom of #17294 we can remove that logging and remove this too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Two ways to define root logger level - should we choose one?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17320</link><project id="" key="" /><description>Currently the logger level of the root logger can be defined in two ways:
1. set `logger.level:DEBUG`
2. set `logger._root: DEBUG`

The first works from command line `-Des.logger.level=DEBUG`. It does **not** work with dynamic update settings like 

```
PUT /_cluster/settings
{
    "transient" : {
        "logger.level" : "INFO"
    }
}
```

The latter works  with dynamic update settings like  

```
PUT /_cluster/settings
{
    "transient" : {
        "logger._root" : "INFO"
    }
}
```

and in test annotation for regular tests `@TestLogging("_root:DEBUG")`. It does not work when passing in cmd line when we start a node with  `-Des.logger._root=DEBUG`. Consequently test annotations for bwc tests also have no effect for old version nods because here we pass the `es.logger._root` as cmd line arg.
As a funny side effect, when we use logging annotation `@TestLogging("level:DEBUG")` then the old version nodes will log in DEBUG and the new version nodes not and the other way round if we set `@TestLogging("_root:DEBUG")`

I am unsure what to do with this. If we want to allow both options we have to fix that somewhere here before we call `settingsBuilder.replacePropertyPlaceholders()` https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java#L109 I think.

But it would be better to have only one way to set the logger level which would then be a bigger operation.
</description><key id="143233222">17320</key><summary>Two ways to define root logger level - should we choose one?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label></labels><created>2016-03-24T12:37:19Z</created><updated>2016-05-25T13:22:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-03-24T12:37:53Z" id="200813317">Checked only version 2.3
</comment><comment author="jpountz" created="2016-03-30T13:06:09Z" id="203422589">+1 to having only one way
</comment><comment author="brwe" created="2016-05-25T13:11:58Z" id="221571307">Ok, I vote for `_root` but do not have a strong opinion. Let me know if you agree so we can proceed.
</comment><comment author="clintongormley" created="2016-05-25T13:22:17Z" id="221574099">Given that the way to set the logger level for (eg) cluster logs is:

```
PUT _cluster/settings
{
  "transient": {
    "logger.cluster": "INFO"
  }
}
```

not `logger.cluster.level`, I agree with using `logger._root` 
</comment><comment author="clintongormley" created="2016-05-25T13:22:35Z" id="221574179">And making `logger.level` an exception
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.2.1 strange document count in _cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17319</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.1
**JVM version**: Oracle Java 8 (1.8.0_74)
**OS version**: Ubuntu Server 14.04.4 LTS
**Description of the problem including expected versus actual behavior**: `/_cat/indices` returns twice number of docs (`docs.count` column) than exists in the index.

```
curl -XGET 'http://127.0.0.1:9200/_cat/indices?v'
health status index                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   index-1doc              2   1          2            0     39.9kb         19.9kb
green  open   index-1doc-1s0r         1   0          2            0     19.8kb         19.8kb
green  open   index-10000doc-7s3r     7   3      20000            0     76.3mb           19mb
```

**Steps to reproduce**:
1. Feed a new index with N documents using bulk insert
2. Query `/_cat/indices` and check `docs.count` column
</description><key id="143228633">17319</key><summary>ES 2.2.1 strange document count in _cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hgfischer</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-03-24T12:08:51Z</created><updated>2016-04-07T09:15:32Z</updated><resolved>2016-04-07T09:15:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T12:21:39Z" id="200808549">If I had to guess that is the total number of documents across all shards. IIRC _cat/indices has done this for a long time, presumably because it is an index metadata level thing. I agree this is confusing though.
</comment><comment author="jasontedor" created="2016-03-24T13:08:47Z" id="200823722">&gt; If I had to guess that is the total number of documents across all shards.

I don't think that that is what it does, we only sum up the documents over the primary shards; see [RestIndicesAction](https://github.com/elastic/elasticsearch/blob/bb364cc793feb57f46dea33fcf4112277da1055a/core/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java#L322).

This does not replicate for me. On a fresh two-node cluster (to ensure that replicas are allocated):

``` bash
$ curl -XPOST localhost:9200/i/t/1?pretty=1 -d '{ "f":"v" }'
{
  "_index" : "i",
  "_type" : "t",
  "_id" : "1",
  "_version" : 1,
  "_shards" : {
    "total" : 2,
    "successful" : 1,
    "failed" : 0
  },
  "created" : true
}
$ curl -XGET localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
green  open   i       5   1          1            0      6.7kb          3.3kb
$ for i in `seq 1 8192`; do curl -sS -XPOST localhost:9200/i2/t2/$i -d '{ "f":"v" }' &gt; /dev/null; done
$ curl -XGET localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
green  open   i2      5   1       8192            0    394.3kb        190.4kb
green  open   i       5   1          1            0      6.7kb          3.3kb
```

If you are able to provide a reproduction script that reproduces the issue on a _fresh_ install of Elasticsearch, could you provide it here?
</comment><comment author="hgfischer" created="2016-03-24T14:07:26Z" id="200852816">It's not the index shard/replication settings for sure, because from all 3 indexes I showed previously, they all have the problem and they have different shard/replication settings, but the docs.count is always twice the total:

```
green  open   index-1doc              2   1          2            0     39.9kb         19.9kb
green  open   index-1doc-1s0r         1   0          2            0     19.8kb         19.8kb
green  open   index-10000doc-7s3r     7   3      20000            0     76.3mb           19mb
```

**Index** `index-1doc` = 1 document, 2 shards, 1 replica = docs.count = 2
**Index** `index-1doc-1s0r` = 1 document, 1 shard, 0 replicas = docs.count = 2
**Index** `index-10000doc-7s3r` = 10000 documents, 7 shards, 3 replicas = docs.count = 20000

The 10-node cluster I'm testing on it's a fresh one. This is the global settings I changed in each node:

```
cluster.name: ****************
node.name: ****************
index.number_of_shards: 2
index.number_of_replicas: 1
path.data: ****************
path.logs: ****************
bootstrap.mlockall: true
network.host: ****************
discovery.zen.ping.multicast.enabled: false
discovery.zen.ping.unicast.hosts: [ **************** 10 IPs **************** ]
discovery.zen.minimum_master_nodes: 3
gateway.expected_nodes: 10
gateway.expected_master_nodes: 3
gateway.expected_data_nodes: 10
gateway.recover_after_time: 30m
gateway.recover_after_nodes: 5
gateway.recover_after_master_nodes: 2
gateway.recover_after_data_nodes: 5
node.max_local_storage_nodes: 1
action.destructive_requires_name: true
threadpool.bulk.queue_size: 1000
index.merge.scheduler.max_thread_count: 1
index.translog.flush_threshold_size: 1gb
index.search.slowlog.threshold.query.warn  : 10s
index.search.slowlog.threshold.query.info  : 5s
index.search.slowlog.threshold.query.debug : 2s
index.search.slowlog.threshold.query.trace : 500ms
index.search.slowlog.threshold.fetch.warn  : 1s
index.search.slowlog.threshold.fetch.info  : 800ms
index.search.slowlog.threshold.fetch.debug : 500ms
index.search.slowlog.threshold.fetch.trace : 200ms
```

I tested in another 2 3-node cluster with similar settings and in another 1-node server, also with similar settings and they all presented the same count.

I inserted the document(s) using bulk request, even with a single document.
</comment><comment author="jasontedor" created="2016-03-24T14:09:13Z" id="200853293">&gt; I inserted the document(s) using bulk request, even with a single document.

This is what we need to see, because I suspect that this is where the issue is.
</comment><comment author="hgfischer" created="2016-03-29T08:46:49Z" id="202781323">@jasontedor Why is this issue closed?!
</comment><comment author="javanna" created="2016-03-29T08:49:56Z" id="202782421">Reopening, I think it was closed by mistake, sorry @hgfischer 
</comment><comment author="clintongormley" created="2016-03-29T08:58:28Z" id="202785232">@hgfischer we're still waiting for the info that @jasontedor asked for, as we are unable to replicate this issue with the info provided thus far.
</comment><comment author="hgfischer" created="2016-03-29T09:09:17Z" id="202791685">&gt; This is what we need to see, because I suspect that this is where the issue is

Since you suspect where the issue is, do I still need to build something to reproduce this?
</comment><comment author="jasontedor" created="2016-03-29T11:25:01Z" id="202838892">&gt; Why is this issue closed?!

Because it does not replicate with the information provided. We are happy to reopen when there is a verified bug.

&gt; Since you suspect where the issue is, do I still need to build something to reproduce this?

Yes, and I'm sorry it was not clear, but the issue does not replicate for me so we need to see what you are doing.
</comment><comment author="jasontedor" created="2016-03-29T18:57:15Z" id="203050407">To be clear, I also attempted to replicate via bulk requests and the issue does not replicate. Again, starting from a fresh two-node cluster.

``` bash
$ cat &gt; request
{ "index": { "_index": "i", "_type": "t" } }
{ "f": "v" }
$ for i in `seq 1 8192`; do echo '{ "index": { "_index": "i2", "_type": "t2" } }' &gt;&gt; request; echo '{ "f": "v" }' &gt;&gt; request; done
$ curl -sS -XPOST locahost:9200/_bulk --data-binary "@request" &gt; /dev/null
$ curl -XGET localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
green  open   i2      5   1       8192            0    267.4kb        132.2kb
green  open   i       5   1          1            0      6.9kb          3.4kb
```

Note that I did not pre-assign document IDs because I suspect that whatever is going on involves requests _without_ document IDs being sent to Elasticsearch twice.
</comment><comment author="hgfischer" created="2016-03-29T19:11:28Z" id="203056874">I'm preparing a script to replicate the problem. 

BTW I'm setting document IDs with UUIDv4.
</comment><comment author="jasontedor" created="2016-03-29T19:14:15Z" id="203058337">&gt; I'm preparing a script to replicate the problem.

Thank you! It will receive my full attention as soon as it is in hand.

&gt; BTW I'm setting document IDs with UUIDv4.

Well, there goes that theory; we'll get to the bottom of it either way. :smile: 
</comment><comment author="hgfischer" created="2016-03-29T21:14:31Z" id="203108350">Here it goes: https://github.com/hgfischer/es-17319
</comment><comment author="jasontedor" created="2016-03-29T21:17:52Z" id="203109224">@hgfischer Thank you. I will take a very close look at this later tonight.
</comment><comment author="jasontedor" created="2016-03-30T01:40:50Z" id="203185410">@hgfischer Thanks for the very thorough and careful reproduction, it should be considered a model for future reproductions. What you're experiencing can be boiled down the following reproduction, starting from a fresh single-node cluster:

``` bash
$ curl -sS -XPUT localhost:9200/i -d '
&gt; {
&gt;   "mappings": {
&gt;     "t": {
&gt;       "properties": {
&gt;         "f": {
&gt;           "type": "nested"
&gt;         }
&gt;       }
&gt;     }
&gt;   }
&gt; }' &gt; /dev/null
$ curl -sS -XPOST localhost:9200/i/t/1 -d '
&gt; {
&gt;   "f": { "v": 1 }
&gt; }' &gt; /dev/null
$ curl -XGET localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
yellow open   i       5   1          2            0      3.5kb          3.5kb
```

What you're observing here is due to [your use](https://github.com/hgfischer/es-17319/blob/f8bf9fae2f40fa368819d14a0ecbfa1b8a6752d5/template.json#L86) of the [nested type](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/nested.html). From your mapping:

``` json
    "mappings": {
      "customer": {
        .
        .
        .
        "additionalData": {
          "type": "nested"
```

When a document with a field mapped as a nested type is indexed into Elasticsearch, Elasticsearch creates a hidden document for each value of the field that is mapped as a nested type. To be clear about what I mean here, with the same mapping above:

``` bash
$ curl -sS -XDELETE localhost:9200/i/t/1 &gt; /dev/null
$ curl -sS -XPOST localhost:9200/i/t/1 -d '
&gt; {
&gt;   "f": [ { "v": 1 }, { "v": 2 } ]
&gt; }' &gt; /dev/null
$ curl -XGET localhost:9200/_cat/indices
health status index pri rep docs.count docs.deleted store.size pri.store.size
yellow open   i       5   1          3            0       650b           650b
```

Note that there are three documents here: the actual document, and the two hidden documents, one for each of the values of the nested field `f`.

These hidden documents are returned in the counts because Elasticsearch retrieves the store count directly from Lucene (which of course counts these hidden documents as actual documents).

This is operating as intended.
</comment><comment author="hgfischer" created="2016-03-30T07:49:06Z" id="203300144">@jasontedor Thanks for the detailed explanation! Would you please consider adding this info on the `_cat/indices` `docs.count` documentation, please? 

What about adding a new column to `_cat/indices` with the _root_ `docs.count` and rename `docs.count` to `lucene.docs.count` maybe?

Thanks
</comment><comment author="nik9000" created="2016-03-30T12:37:22Z" id="203411427">&gt; What about adding a new column to _cat/indices with the root docs.count and rename docs.count to lucene.docs.count maybe?

That probably isn't going to happen. Those APIs don't need to execute queries to do their thing, instead relying on Lucene APIs that get to read meta. I suspect root count would need a query.

&gt; Would you please consider adding this info on the _cat/indices docs.count documentation, please?

Reopening this issue to do just that. Since you've been so good to us I have to offer you first dibs on it - the documentation is in `docs/reference/cat/indices.asciidoc` if you want to edit it. If not, one of us will do it.
</comment><comment author="jasontedor" created="2016-03-30T14:40:03Z" id="203463887">&gt; Would you please consider adding this info on the `_cat/indices` `docs.count` documentation, please? 

Sure, unless you want to take @nik9000's invitation to submit a PR yourself, I am happy to take it. Let us know either way?

&gt; What about adding a new column to `_cat/indices` with the _root_ `docs.count` and rename `docs.count` to `lucene.docs.count` maybe?

I'm hesitant to change this, I think that for the cat indices API, this is doing the right thing: counting the number of documents that are in the index. That is, this API is working at the physical index level and should return the physical count.

Note that you can get the number of root documents (non-hidden) via the cat count API:

```
$ curl -XGET localhost:9200/_cat/count/i?v
epoch      timestamp count
1459348721 10:38:41  1
$ curl -XGET localhost:9200/_cat/indices?v
health status index pri rep docs.count docs.deleted store.size pri.store.size
yellow open   i       5   1          3            0      3.7kb          3.7kb
```

This is on the same data as above, with my example of a document have two values for the field mapped as a nested type. This does exactly as @nik9000 [suggested](https://github.com/elastic/elasticsearch/issues/17319#issuecomment-203411427): it executes a query to get the count.
</comment><comment author="hgfischer" created="2016-03-30T16:18:22Z" id="203510312">Yes, I'll do the PR. I would like to see a CONTRIBUTORS file (like http://golang.org/CONTRIBUTORS) in the project too, can I add it? :)

Regarding the changes on _cat/indices, ok then. I guess the documentation is enough.
</comment><comment author="jasontedor" created="2016-03-30T16:26:53Z" id="203513125">&gt; Yes, I'll do the PR.

Awesome. :heart:

&gt; I would like to see a CONTRIBUTORS file (like http://golang.org/CONTRIBUTORS) in the project too, can I add it? :)

We have a [contributing guidelines](https://github.com/elastic/elasticsearch/blob/97eaf6c04b49ec295f9b371ebcd58f2761204e80/CONTRIBUTING.md) in the main GitHub repo. I think that we should add a pull request template to draw attention to it though.

&gt; Regarding the changes on _cat/indices, ok then. I guess the documentation is enough.

Cool, thanks again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES v2.2.1 cloud-aws errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17318</link><project id="" key="" /><description>Elasticsearch version:
v2.2.1

JVM version:
java version "1.7.0_91"
OpenJDK Runtime Environment (IcedTea 2.6.3) (7u91-2.6.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.91-b01, mixed mode)

OS version:
Ubuntu 14.04

Trying to setup a cluster in AWS using ec2 discovery with the cloud-aws plugin.

Receiving the following errors

```
[2016-03-24 09:28:17,234][WARN ][com.amazonaws.jmx.SdkMBeanRegistrySupport] 
java.security.AccessControlException: access denied ("javax.management.MBeanServerPermission" "findMBeanServer")
at java.security.AccessControlContext.checkPermission(AccessControlContext.java:474)
at java.security.AccessController.checkPermission(AccessController.java:685)
at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
at javax.management.MBeanServerFactory.checkPermission(MBeanServerFactory.java:413)
at javax.management.MBeanServerFactory.findMBeanServer(MBeanServerFactory.java:361)
at com.amazonaws.jmx.MBeans.getMBeanServer(MBeans.java:111)
at com.amazonaws.jmx.MBeans.registerMBean(MBeans.java:50)
at com.amazonaws.jmx.SdkMBeanRegistrySupport.registerMetricAdminMBean(SdkMBeanRegistrySupport.java:27)
at com.amazonaws.metrics.AwsSdkMetrics.registerMetricAdminMBean(AwsSdkMetrics.java:355)
at com.amazonaws.metrics.AwsSdkMetrics.(AwsSdkMetrics.java:316)
at com.amazonaws.AmazonWebServiceClient.requestMetricCollector(AmazonWebServiceClient.java:563)
at com.amazonaws.AmazonWebServiceClient.isRMCEnabledAtClientOrSdkLevel(AmazonWebServiceClient.java:504)
at com.amazonaws.AmazonWebServiceClient.isRequestMetricsEnabled(AmazonWebServiceClient.java:496)
at com.amazonaws.AmazonWebServiceClient.createExecutionContext(AmazonWebServiceClient.java:457)
at com.amazonaws.services.ec2.AmazonEC2Client.describeInstances(AmazonEC2Client.java:5924)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.fetchDynamicNodes(AwsEc2UnicastHostsProvider.java:118)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:230)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:215)
at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.buildDynamicNodes(AwsEc2UnicastHostsProvider.java:104)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:335)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:899)
at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:335)
at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1260)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
```

i added the following to my java policy and recieved more errors

```
permission javax.management.MBeanServerPermission "createMBeanServer";
permission javax.management.MBeanServerPermission "findMBeanServer";
```

```
[2016-03-24 10:54:28,201][WARN ][com.amazonaws.jmx.SdkMBeanRegistrySupport] 
java.security.AccessControlException: access denied ("javax.management.MBeanPermission" "com.amazonaws.metrics.MetricAdmin#-[com.amazonaws.management:type=AwsSdkMetrics]" "registerMBean")
at java.security.AccessControlContext.checkPermission(AccessControlContext.java:474)
at java.security.AccessController.checkPermission(AccessController.java:685)
at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanPermission(DefaultMBeanServerInterceptor.java:1830)
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:321)
at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
at com.amazonaws.jmx.MBeans.registerMBean(MBeans.java:52)
at com.amazonaws.jmx.SdkMBeanRegistrySupport.registerMetricAdminMBean(SdkMBeanRegistrySupport.java:27)
at com.amazonaws.metrics.AwsSdkMetrics.registerMetricAdminMBean(AwsSdkMetrics.java:355)
at com.amazonaws.metrics.AwsSdkMetrics.(AwsSdkMetrics.java:316)
at com.amazonaws.AmazonWebServiceClient.requestMetricCollector(AmazonWebServiceClient.java:563)
at com.amazonaws.AmazonWebServiceClient.isRMCEnabledAtClientOrSdkLevel(AmazonWebServiceClient.java:504)
at com.amazonaws.AmazonWebServiceClient.isRequestMetricsEnabled(AmazonWebServiceClient.java:496)
at com.amazonaws.AmazonWebServiceClient.createExecutionContext(AmazonWebServiceClient.java:457)
at com.amazonaws.services.ec2.AmazonEC2Client.describeInstances(AmazonEC2Client.java:5924)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.fetchDynamicNodes(AwsEc2UnicastHostsProvider.java:118)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:230)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider$DiscoNodesCache.refresh(AwsEc2UnicastHostsProvider.java:215)
at org.elasticsearch.common.util.SingleObjectCache.getOrRefresh(SingleObjectCache.java:55)
at org.elasticsearch.discovery.ec2.AwsEc2UnicastHostsProvider.buildDynamicNodes(AwsEc2UnicastHostsProvider.java:104)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:335)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:899)
at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:335)
at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1260)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
```

Originally posted here https://discuss.elastic.co/t/es-cloud-aws-error-2-2-1/45331
</description><key id="143219555">17318</key><summary>ES v2.2.1 cloud-aws errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DMNSteve</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>regression</label></labels><created>2016-03-24T11:14:48Z</created><updated>2016-04-12T16:12:24Z</updated><resolved>2016-04-12T14:54:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-04-01T12:33:40Z" id="204380753">@ywelsch do we need to add the following lines according to the thread on discuss?

```
permission javax.management.MBeanServerPermission "createMBeanServer";
permission javax.management.MBeanServerPermission "findMBeanServer";
permission javax.management.MBeanPermission "com.amazonaws.metrics.*", "*";
permission javax.management.MBeanTrustPermission "register";
```
</comment><comment author="ywelsch" created="2016-04-12T14:54:00Z" id="208947261">The `AccessControlException` warnings here are caused by the MBeans Metrics module of the Amazon Java SDK which is part of the EC2 discovery plugin. These warnings don't affect the functionality of the plugin, though. There is currently no way to remove these warnings except by adding the permissions above. I don't think we should do that as adding security holes just for getting rid of a log message would be a bad thing to do. We should instead make a PR to fix this in the AWS SDK.

As this is not a bug, I'll close the issue.

On a related note, I added a test in #17677 that checks the permissions needed by the EC2 discovery plugin more thoroughly.
</comment><comment author="rmuir" created="2016-04-12T16:12:24Z" id="208985828">This comes up from time to time, despite the fact we silence these warnings out of box.

https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/config/logging.yml#L13-L15

I guess folks remove these lines from the config and then are surprised about the WARN...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move the percolator from core to a module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17317</link><project id="" key="" /><description>I think it is good to isolate the percolator from core. Many of its components (PercolatorFieldMapper, PercolatorQueryBuilder) allow for this. 

Two additional tasks need to be done to make this possible:
- Make IndexWarmers pluggable. The percolator has its own index warmer that loads the percolator queries.
- Allow for modules and plugins to add metrics to the node stats api. The percolator reports on the number of percolator queries loaded into memory.
</description><key id="143198160">17317</key><summary>Move the percolator from core to a module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label></labels><created>2016-03-24T09:35:53Z</created><updated>2016-05-24T10:01:02Z</updated><resolved>2016-05-24T10:01:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T12:23:40Z" id="200809490">I like this idea! The nicest thing about the proposal is that you can run the tests for the percolator in isolation when all you've done is change percolator code. Much faster testing cycle. I did that with reindex and was very happy that I made it a module.
</comment><comment author="javanna" created="2016-03-24T12:46:43Z" id="200815598">Question: besides that it is technically possible and it makes testing easier, what are the advantages of having percolator as a module? We generally use modules for things that we want to keep isolated because of dependencies, security policies etc. does the percolator have anything like this? or maybe there are other good reasons to use modules that I may have missed?
</comment><comment author="nik9000" created="2016-03-24T12:53:32Z" id="200819721">&gt; what are the advantages of having percolator as a module?

Technically you could ship Elasticsearch without percolation if you wanted but I don't see that as super compelling. Maybe it lets us cut the lucene-memory dependency from core? That isn't a big deal either, I don't think.

Beyond the testing speed improvement (which is mighty!) the most compelling argument for moving it out of core is that we can point to core and say "this is very extensible. we've moved core features into modules to demonstrate that you can too!"

So, yeah, I don't see a reason to work really hard on this but if it isn't particularly hard I think it is worth it.
</comment><comment author="martijnvg" created="2016-03-24T12:53:59Z" id="200819801">@javanna I think the general goal is to make core slimmer? With that in mind moving the percolator to a module make sense.
</comment><comment author="martijnvg" created="2016-03-24T12:56:57Z" id="200820399">@nik9000 The 'fast vector' highligher also relies on the `lucene-memory` memory dependency, so we still would need to have this dependency in core. (until we modularise highlighters too :) )

and I think the testing benefit you mentioned is important.
</comment><comment author="nik9000" created="2016-03-24T12:59:05Z" id="200820807">&gt; and I think the testing benefit you mentioned is important.

It takes about a minute for me to test reindex if I don't modify core. If I do I really should rerun the core tests and that takes 20 minutes. And testing reindex starts Elasticsearch twice - once with just reindex and once with groovy and reindex so it can play with them together.
</comment><comment author="martijnvg" created="2016-05-24T10:01:02Z" id="221222962">Closed via #18511
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for TermsQuery in ExtractQueryTermsService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17316</link><project id="" key="" /><description>Upon indexing a percolator query with containing a terms query then also extract its terms.
</description><key id="143195059">17316</key><summary>Add support for TermsQuery in ExtractQueryTermsService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T09:21:12Z</created><updated>2016-03-24T09:27:37Z</updated><resolved>2016-03-24T09:27:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-24T09:24:12Z" id="200751137">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Let PercolatorQuery's explain use the two phase iterator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17315</link><project id="" key="" /><description>So that we don't evaluate percolator queries that don't match.

PR for #17314
</description><key id="143192261">17315</key><summary>Let PercolatorQuery's explain use the two phase iterator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T09:03:57Z</created><updated>2016-03-29T14:27:39Z</updated><resolved>2016-03-29T14:27:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-29T13:33:33Z" id="202895714">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PercolatorQuery's explain should use two-phase iteration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17314</link><project id="" key="" /><description>Currently it does this:

``` java
int result = scorer.iterator().advance(docId);
```

So if it is called on a document that does not match, it could potentially try to match many documents until it finds the next matching document. 
</description><key id="143179096">17314</key><summary>PercolatorQuery's explain should use two-phase iteration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Percolator</label></labels><created>2016-03-24T07:59:35Z</created><updated>2016-03-29T14:27:40Z</updated><resolved>2016-03-29T14:27:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make `parseMultiField` part of `parseField`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17313</link><project id="" key="" /><description>All our fields are supposed to support multi fields, so we could put the logic in
`TypeParsers.parseField` instead of duplicating the logic in every type parser.
</description><key id="143177621">17313</key><summary>Make `parseMultiField` part of `parseField`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T07:54:45Z</created><updated>2016-03-24T09:16:03Z</updated><resolved>2016-03-24T09:16:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jimczi" created="2016-03-24T08:24:07Z" id="200730827">Is there a reason why you didn't add the CompletionFieldMapper?, it's calling the parseMultiField method too.
Apart from that LGTM.
</comment><comment author="jpountz" created="2016-03-24T08:42:23Z" id="200736814">I only removed the call to `parseMultiField` from parsers that already call `parseField`, which is not the case of CompletionFieldMapper.
</comment><comment author="jimczi" created="2016-03-24T08:45:22Z" id="200738144">got it thanks. LTGM
</comment><comment author="jpountz" created="2016-03-24T08:46:00Z" id="200738279">Thanks Jim!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow configuring Windows service name, description and user</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17312</link><project id="" key="" /><description>This adds support for more environment variables to configure Windows Elasticsearch service:
- `SERVICE_DISPLAY_NAME` and `SERVICE_DESCRIPTION` to customize how the service is listed in the services list
- `SERVICE_USERNAME` and `SERVICE_PASSWORD` to configure the user to run the service (which will only be applied when both are set)

They are all optional and the current defaults will be used if they are not set.
</description><key id="143163850">17312</key><summary>Allow configuring Windows service name, description and user</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">detouched</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-03-24T06:08:12Z</created><updated>2016-04-07T21:40:39Z</updated><resolved>2016-04-07T21:22:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-25T01:25:47Z" id="201095929">This still needs a CLA to be signed but can you take a look @Mpdreamz?
</comment><comment author="detouched" created="2016-03-25T03:18:54Z" id="201120489">@jasontedor I should be listed in a corporate CLA signed by Atlassian.
</comment><comment author="Mpdreamz" created="2016-03-25T07:46:42Z" id="201186023">LGTM but I can't take it for a quick spin until Monday, @gmarz can you test this one today?
</comment><comment author="gmarz" created="2016-03-25T15:57:21Z" id="201342516">Hey @detouched I just tried testing this and I'm hitting a few issues.

**1)** The service name doesn't properly handle spaces.

```
set SERVICE_DISPLAY_NAME=My Elasticsearch Service
```

results in the service name being installed as `My`

If I set it using quotes

```
set SERVICE_DISPLAY_NAME="My Elasticsearch Service"
```

The following error occurs during installation: `Elasticsearch was unexpected at this time.`

**2)** The service description when set doesn't appear to be taking.  It's installed with an empty description.

**3)** The username and password also do not seem to be taking.  It still uses the local system account.

The default values work, however.
</comment><comment author="detouched" created="2016-03-25T18:00:58Z" id="201392189">Hello @gmarz, thank you for your feedback!

**1.** Indeed, with the syntax you used spaces are handled incorrectly. I used `set "KEY=Value with spaces"` instead but I agree that it's not a common way to set environment variables. So I've just updated the PR with a fix for that. Now syntax _without_ quotes should work (I believe it's how values with spaces are usually set):

```
set SERVICE_DISPLAY_NAME=My Elasticsearch Service
set SERVICE_DESCRIPTION=Elasticsearch Service Description
```

**2.** Unfortunately I can't reproduce that &#8212; it works for me, similar to the display name. Maybe it was also due to the fix?
**3.** The username should be fully qualified (i.e. with the domain), and such user should exist in the system. So, for instance, in my case, any of following will work (assuming `SERVICE_PASSWORD` is set):

```
set SERVICE_USERNAME=.\dpenkin
```

or

```
set SERVICE_USERNAME=DESKTOP-ETHEERQ\dpenkin
```
</comment><comment author="gmarz" created="2016-03-25T19:13:24Z" id="201436466">LGTM.  I've tested your fixes and all looks good. :+1: 

&gt; The username should be fully qualified (i.e. with the domain), and such user should exist in the system. 

Yup, that was my issue.

The empty description seems to be fixed now as well.  It was probably related to the missing quotes.  

Thanks for the quick turnaround!
</comment><comment author="jasontedor" created="2016-03-29T11:17:02Z" id="202836232">&gt; I should be listed in a corporate CLA signed by Atlassian.

Okay. We will look into why the CLA check is failing.
</comment><comment author="jasontedor" created="2016-03-30T01:55:18Z" id="203189720">@detouched I think that we sorted out the CLA check issue, but I think that you need to leave a comment here to trigger the check to execute again. Do you mind giving that a try?
</comment><comment author="detouched" created="2016-03-30T02:58:35Z" id="203217695">Sure =) Let's try it!
</comment><comment author="detouched" created="2016-03-30T02:59:11Z" id="203218034">Awesome, it worked! Thank you @jasontedor!
</comment><comment author="jasontedor" created="2016-03-30T03:04:16Z" id="203221592">Thanks @detouched! I'll step aside and let @Mpdreamz or @gmarz finish it from here. :smile:
</comment><comment author="detouched" created="2016-04-04T07:34:30Z" id="205175650">I rebased my changes to current `master` to resolve the conflict.
</comment><comment author="dakrone" created="2016-04-06T16:40:11Z" id="206456985">Ping @Mpdreamz or @gmarz want to merge this?
</comment><comment author="gmarz" created="2016-04-06T16:56:30Z" id="206463483">@detouched would you mind rebasing and squashing to a single commit?  It'll make back-porting to 2.x a bit easier.
</comment><comment author="detouched" created="2016-04-07T00:43:23Z" id="206636885">@gmarz Sure &#8212; rebased and squashed.
</comment><comment author="gmarz" created="2016-04-07T21:40:39Z" id="207098796">Merged to master and cherry-picked to 2.x.  Thanks @detouched!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Client node count not shown in stats api - ES 2.0.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17311</link><project id="" key="" /><description>I have 3 nodes in my cluster and two are master cum data nodes and one node is client node. When I issues stats api(_cluster/stats), I still see the count of client as 0 even though its is up and working.

Below is the response of stats API

nodes: {
count: {
total: 3,
master_only: 0,
data_only: 0,
master_data: 2,
client: 0
},
versions: [
"2.0.1"
],

jvm: {
max_uptime_in_millis: 1181282,
versions: [
{
version: "1.8.0_73",
vm_name: "Java HotSpot(TM) 64-Bit Server VM",
vm_version: "25.73-b02",
vm_vendor: "Oracle Corporation",
count: 3
}
],

OS is 
name: "Linux",
arch: "amd64",
version: "2.6.18-308.4.1.0.1.el5",
available_processors: 32
:
</description><key id="143163122">17311</key><summary>Client node count not shown in stats api - ES 2.0.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SKumarMN</reporter><labels><label>:Cluster</label></labels><created>2016-03-24T06:02:46Z</created><updated>2016-03-24T10:02:36Z</updated><resolved>2016-03-24T10:02:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-24T06:53:33Z" id="200698349">HI @SKumarMN how did you set up the nodes to be client nodes? Did you set `node.client` to `true` or `node.master` and `node.data` to `false`? I believe this may be related to #16565 .
</comment><comment author="SKumarMN" created="2016-03-24T08:57:06Z" id="200742284">Yes as per this doc [link
](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html) , I gave node.master and node.data to false
</comment><comment author="javanna" created="2016-03-24T10:02:28Z" id="200768010">That's what I thought, I am closing as duplicate of #16565, but thanks for reporting. This is being addressed as part of #16963 so that there is a single way to create a client node and that will be always taken into account in all the relevant apis, e.g. cluster stats
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add guard against null-valued settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17310</link><project id="" key="" /><description>This commit adds a guard against null-valued settings that are loaded
from yaml or json settings files, and also adds a test that ensures
the same remains true for settings loaded from properties files.

Relates #17292 
</description><key id="143125421">17310</key><summary>Add guard against null-valued settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-24T02:04:19Z</created><updated>2016-03-24T18:09:14Z</updated><resolved>2016-03-24T11:53:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-24T02:07:58Z" id="200615246">This gives more user-friendly error messages like:

```
Exception in thread "main" SettingsException[Failed to load settings from [elasticsearch.yml]]; nested: ElasticsearchParseException[null-valued setting found for key [cluster.name] found at line number [1], column number [14]];
Likely root cause: ElasticsearchParseException[null-valued setting found for key [cluster.name] found at line number [1], column number [14]]
```

instead of

```
Exception in thread "main" java.lang.NullPointerException: value can not be null for [cluster.name]
```

Note that prior to #17292, the error message here was even worse:

```
Exception in thread "main" java.lang.NullPointerException: Argument 'value' must not be null.
```
</comment><comment author="rjernst" created="2016-03-24T05:29:16Z" id="200676166">LGTM. I would just call the boolean "allowNullValues" though. 
</comment><comment author="bleskes" created="2016-03-24T08:33:28Z" id="200733018">nice. Happy we found a better name for the exception than the dreaded NPE
</comment><comment author="jasontedor" created="2016-03-24T13:14:56Z" id="200825873">&gt; LGTM. I would just call the boolean "allowNullValues" though.

I agree that that is better and I pushed 4d27328a8337ef1214aa7637c0247103b3835d40.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document allowed and disallowed settings for the tribe node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17309</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.1

As part of the [PR](https://github.com/elastic/elasticsearch/pull/16893#issuecomment-193349345) we changed the way (or fixed) how settings are passed through from the ES node to the tribe clients in the tribe node configuration.  There is a good suggestion in the PR to document the settings that are passed through to the tribe node clients, settings that are no longer allowed in the tribe client configurations, and settings that can be overridden in the tribe client's configuration.  But it doesn't look like this has been documented yet.  The following is what I have gathered - not sure if it's 100% correct. 

Passthroughs from ES node:
- node.name
- network.host
- network.bind_host
- network.publish_host
- transport.host
- transport.bind_host
- transport.publish_host
- path.home
- path.conf
- path.plugins
- path.logs
- path.scripts
- shield.\* (for those using Shield)

Settings no longer allowed for the tribe clients (eg. t1, t2, etc..):
- path.*

Settings that are allowed for tribe clients (can override):
- network.host
- network.bind_host
- network.publish_host
- transport.host
- transport.bind_host
- transport.publish_host
- cluster.name
- discovery.zen.ping.unicast.hosts

Will be helpful to document this (or similar) for tribe node users.
</description><key id="143120557">17309</key><summary>Document allowed and disallowed settings for the tribe node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Tribe Node</label><label>docs</label></labels><created>2016-03-24T01:26:26Z</created><updated>2016-03-30T18:07:12Z</updated><resolved>2016-03-30T18:07:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-24T17:48:41Z" id="200946762">@rjernst could you confirm this list of settings please?
</comment><comment author="rjernst" created="2016-03-24T18:00:41Z" id="200951122">Technically any node settings can be set on the tribe client, it is only the ones we disallow that can't (ie path.*). The passthrough settings look correct (although note I would not mention path.home as this is not something a user ever sets). Also, node.name is not passed through, it is used to generate the name of the tribe client node.
</comment><comment author="clintongormley" created="2016-03-30T18:07:01Z" id="203558354">thanks @ppf2 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Settings: Disallow null values when loading settings from files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17308</link><project id="" key="" /><description>The yaml spec extends json, and by that allows for nulls, which in yaml just means "there was no value". This is very trappy for users, who may have
simply forgot to add a value, for example if their yml file is generated
from a template and the value was an empty string.

This change adds a check when loading settings from a file that
disallows null values. We still "allow" null values in settings when
placed explicitly, which is things are set back to their defaults.
However, since loading from a file is the first source of settings,
there are no settings to reset back to their defaults.

see #17292

NOTE: This change also cleans up property placeholders to be a pkg private utility, and handles the case where setting values are null when applying placeholders.
</description><key id="143118390">17308</key><summary>Settings: Disallow null values when loading settings from files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2016-03-24T01:06:30Z</created><updated>2016-03-24T19:00:07Z</updated><resolved>2016-03-24T18:59:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-24T05:31:27Z" id="200677191">Thanks @jasontedor, I like that yours has the line number. I will wait for you to check in and then rework this PR as I think the cleanup to settings preparer being pkg private here are still good. 
</comment><comment author="rjernst" created="2016-03-24T18:59:56Z" id="200971509">I opened a new PR with the other changes from this in it. #17335
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix TaskId#isSet to return true when id is set and not other way around</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17307</link><project id="" key="" /><description>During refactoring the name was changed, but the logic wasn't. This commit fixes the logic to match the name.
</description><key id="143101213">17307</key><summary>Fix TaskId#isSet to return true when id is set and not other way around</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T22:58:09Z</created><updated>2016-03-24T13:55:45Z</updated><resolved>2016-03-24T00:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-23T22:59:51Z" id="200575458">LGTM
</comment><comment author="nik9000" created="2016-03-23T23:12:31Z" id="200578138">Lol. I saw this a few times and forgot each time. Sorry!
On Mar 23, 2016 6:59 PM, "Lee Hinman" notifications@github.com wrote:

&gt; LGTM
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17307#issuecomment-200575458
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update store.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17306</link><project id="" key="" /><description>It looks like the real off-heap memory store was removed in 1.2

The memory store is really based on lucene's RAMDirectory, which is on-heap.
</description><key id="143093255">17306</key><summary>Update store.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsouza</reporter><labels><label>docs</label><label>v1.7.5</label></labels><created>2016-03-23T22:09:19Z</created><updated>2016-04-06T16:41:20Z</updated><resolved>2016-04-06T16:41:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-23T22:34:55Z" id="200570384">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add API to explain why a shard is or isn't assigned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17305</link><project id="" key="" /><description>This adds a new `/_cluster/allocation/explain` API that explains why a
shard can or cannot be allocated to nodes in the cluster. Additionally,
it will show where the master _desires_ to put the shard, according to
the `ShardsAllocator`.

It looks like this:

``` json
GET /_cluster/allocation/explain?pretty
{
  "index": "only-foo",
  "shard": 0,
  "primary": false
}
```

Though, you can optionally send an empty body, which means "explain the
allocation for the first unassigned shard you find".

The output when a shard is unassigned looks like this:

``` json
{
  "shard" : {
    "index" : "only-foo",
    "index_uuid" : "KnW0-zELRs6PK84l0r38ZA",
    "id" : 0,
    "primary" : false
  },
  "assigned" : false,
  "unassigned_info" : {
    "reason" : "INDEX_CREATED",
    "at" : "2016-03-22T20:04:23.620Z"
  },
  "nodes" : {
    "V-Spi0AyRZ6ZvKbaI3691w" : {
      "node_name" : "Susan Storm",
      "node_attributes" : {
        "bar" : "baz"
      },
      "final_decision" : "NO",
      "weight" : 0.06666675,
      "decisions" : [ {
        "decider" : "filter",
        "decision" : "NO",
        "explanation" : "node does not match index include filters [foo:\"bar\"]"
      } ]
    },
    "Qc6VL8c5RWaw1qXZ0Rg57g" : {
      "node_name" : "Slipstream",
      "node_attributes" : {
        "bar" : "baz",
        "foo" : "bar"
      },
      "final_decision" : "NO",
      "weight" : -1.3833332,
      "decisions" : [ {
        "decider" : "same_shard",
        "decision" : "NO",
        "explanation" : "the shard cannot be allocated on the same node id [Qc6VL8c5RWaw1qXZ0Rg57g] on which it already exists"
      } ]
    },
    "PzdyMZGXQdGhqTJHF_hGgA" : {
      "node_name" : "The Symbiote",
      "node_attributes" : { },
      "final_decision" : "NO",
      "weight" : 2.3166666,
      "decisions" : [ {
        "decider" : "filter",
        "decision" : "NO",
        "explanation" : "node does not match index include filters [foo:\"bar\"]"
      } ]
    }
  }
}
```

And when the shard _is_ assigned, the output looks like:

``` json
{
  "shard" : {
    "index" : "only-foo",
    "index_uuid" : "KnW0-zELRs6PK84l0r38ZA",
    "id" : 0,
    "primary" : true
  },
  "assigned" : true,
  "assigned_node_id" : "Qc6VL8c5RWaw1qXZ0Rg57g",
  "nodes" : {
    "V-Spi0AyRZ6ZvKbaI3691w" : {
      "node_name" : "Susan Storm",
      "node_attributes" : {
        "bar" : "baz"
      },
      "final_decision" : "NO",
      "weight" : 1.4499999,
      "decisions" : [ {
        "decider" : "filter",
        "decision" : "NO",
        "explanation" : "node does not match index include filters [foo:\"bar\"]"
      } ]
    },
    "Qc6VL8c5RWaw1qXZ0Rg57g" : {
      "node_name" : "Slipstream",
      "node_attributes" : {
        "bar" : "baz",
        "foo" : "bar"
      },
      "final_decision" : "CURRENTLY_ASSIGNED",
      "weight" : 0.0,
      "decisions" : [ {
        "decider" : "same_shard",
        "decision" : "NO",
        "explanation" : "the shard cannot be allocated on the same node id [Qc6VL8c5RWaw1qXZ0Rg57g] on which it already exists"
      } ]
    },
    "PzdyMZGXQdGhqTJHF_hGgA" : {
      "node_name" : "The Symbiote",
      "node_attributes" : { },
      "final_decision" : "NO",
      "weight" : 3.6999998,
      "decisions" : [ {
        "decider" : "filter",
        "decision" : "NO",
        "explanation" : "node does not match index include filters [foo:\"bar\"]"
      } ]
    }
  }
}
```

Only "NO" decisions are returned by default, but all decisions can be
shown by specifying the `?include_yes_decisions=true` parameter in the
request.

Resolves #14593
</description><key id="143091666">17305</key><summary>Add API to explain why a shard is or isn't assigned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>feature</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T22:01:22Z</created><updated>2016-03-28T22:07:10Z</updated><resolved>2016-03-28T22:07:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-23T22:08:01Z" id="200563588">&gt; ```
&gt;    "explanation" : "the shard cannot be allocated on the same node id [Qc6VL8c5RWaw1qXZ0Rg57g] on which it already exists"
&gt; ```

Maybe reword this somehow - "there is already a copy of this shard assigned to this node" or something like that.

You example of when the shard is assigned still contains a NO decision which confused me. Can you explain?
</comment><comment author="dakrone" created="2016-03-23T22:12:44Z" id="200564555">&gt; Maybe reword this somehow - "there is already a copy of this shard assigned to this node" or something like that.

I'd rather leave the wording as-is for now until we determine the right thing to do in #11490, since the criteria for "same node" is likely to be changed/configurable.

&gt; You example of when the shard is assigned still contains a NO decision which confused me. Can you explain?

Technically, the shard cannot be assigned to the node it already exists on, because it already exists there (from the allocator point of view). I thought about suppressing "NO" decisions, but decided against it because if `cluster.routing.allocation.same_shard.host` is configured, it's possible the decision may still be "YES". I opted to change the `final_decision` to `CURRENTLY_ASSIGNED` to indicate this.
</comment><comment author="nik9000" created="2016-03-23T22:19:13Z" id="200566178">&gt; Technically, the shard cannot be assigned to the node it already exists on, because it already exists there (from the allocator point of view). I thought about suppressing "NO" decisions, but decided against it because if cluster.routing.allocation.same_shard.host is configured, it's possible the decision may still be "YES". I opted to change the final_decision to CURRENTLY_ASSIGNED to indicate this.

OK - I'm glad it is genuinely confusing not just confusing me. I'll think on it some more and review the rest later.
</comment><comment author="nik9000" created="2016-03-24T15:24:34Z" id="200885101">Left some pretty innocuous questions/notes. If you want to address them and merge on your own time then LGTM.
</comment><comment author="ywelsch" created="2016-03-25T09:46:40Z" id="201223057">Good work on this @dakrone (I just quickly glanced at the code, no formal review). As follow-up I would like to see an integration with `PrimaryShardAllocator` and `ReplicaShardAllocator`. This would provide insight into the following highly important scenarios (and possibly more):
- primary shard is unassigned as only stale shard copies available (allocation ids don't match)
- replica shard unassigned due to delayed shard allocation (node_left.delayed_timeout)
- primary or replica shard unassigned as shard fetching is still going on (i.e. not all data available yet to decide where to place the shard)
- replica unassigned as primary is not yet assigned
</comment><comment author="dakrone" created="2016-03-28T20:46:23Z" id="202574646">&gt; Good work on this @dakrone

Thanks!

&gt; As follow-up I would like to see an integration with PrimaryShardAllocator and ReplicaShardAllocator. This would provide insight into the following highly important scenarios (and possibly more):
&gt;  primary shard is unassigned as only stale shard copies available (allocation ids don't match)
&gt;  replica shard unassigned due to delayed shard allocation (node_left.delayed_timeout)
&gt;  primary or replica shard unassigned as shard fetching is still going on (i.e. not all data available yet to decide where to place the shard)

Definitely! I will open up a followup to work on integrating these things!

&gt; replica unassigned as primary is not yet assigned

This is handled by an AllocationDecider and thus is already captured by this work :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove suggest threadpool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17304</link><project id="" key="" /><description>In #17198, we removed suggest transport action, which
used the `suggest` threadpool to execute requests. Now
`suggest` threadpool is unused and suggest requests are
executed on the `search` threadpool.
</description><key id="143090843">17304</key><summary>Remove suggest threadpool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T21:57:54Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-23T22:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-23T21:59:43Z" id="200560744">Seems like the right thing to do to me. I know @jasontedor 's looked at the thread pool construction code recently and can probably do a better review than I can.
</comment><comment author="s1monw" created="2016-03-23T22:00:16Z" id="200560883">LGTM
</comment><comment author="jasontedor" created="2016-03-23T22:03:39Z" id="200562379">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove HighlighterParseElement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17303</link><project id="" key="" /><description>The old HighlightParseElement is still used in tests and some places in InnerHits. This removes it
and replaces the tests that checked that the original parse element and the rafactored highlighter code produce the same output with new tests that compare builder input to the SearchContextHighlight that is created.
</description><key id="143087371">17303</key><summary>Remove HighlighterParseElement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T21:43:26Z</created><updated>2016-03-30T15:01:03Z</updated><resolved>2016-03-30T15:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-27T20:36:48Z" id="202141906">So I was working on my PROTOTYPE stomp and kind of wandered - I have code that converts HighlightBuilder to ObjectParser. And then I saw HighlighterParseElement. Anyway, I'm pretty familiar with this parsing after the fun ObjectParser stuff so I'll review this tomorrow if no one gets to it.
</comment><comment author="nik9000" created="2016-03-28T14:27:21Z" id="202416330">Left a genuine question and some requests for additional comments for future me when he reads this code in six months.  Otherwise LGTM.
</comment><comment author="cbuescher" created="2016-03-29T14:19:29Z" id="202914729">@nik9000 thanks for your suggestions, I hope I answered your question and will merge this if there aren't any follow up questions.
</comment><comment author="nik9000" created="2016-03-29T14:21:43Z" id="202915722">LGTM
</comment><comment author="cbuescher" created="2016-03-30T14:45:55Z" id="203467612">@nik9000 thanks, I squashed and rebased on master. Will merge this after last local test run.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure in Build: org.elasticsearch.search.profile.QueryProfilerIT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17302</link><project id="" key="" /><description>Unable to reproduce locally.

Build (https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=centos/111/console)

Suite: org.elasticsearch.search.profile.QueryProfilerIT
  2&gt; REPRODUCE WITH: gradle :core:integTest -Dtests.seed=E6B1C23627D79398 -Dtests.class=org.elasticsearch.search.profile.QueryProfilerIT -Dtests.method="testBoosting" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=ga -Dtests.timezone=Africa/Addis_Ababa
FAILURE 50.9s J0 | QueryProfilerIT.testBoosting &lt;&lt;&lt; FAILURES!
  2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/centos/core/build/testrun/integTest/J0/temp/org.elasticsearch.search.profile.QueryProfilerIT_E6B1C23627D79398-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene60): {field1=PostingsFormat(name=Asserting), _field_names=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), field2=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting)}, docValues:{_type=DocValuesFormat(name=Lucene54), field2=DocValuesFormat(name=Lucene54), _version=DocValuesFormat(name=Lucene54)}, maxPointsInLeafNode=1793, maxMBSortInHeap=6.374764498623509, sim=ClassicSimilarity, locale=ga, timezone=Africa/Addis_Ababa
  2&gt; NOTE: Linux 2.6.32-573.12.1.el6.x86_64 amd64/Oracle Corporation 1.8.0_65 (64-bit)/cpus=4,threads=1,free=229665336,total=527433728

&gt; Throwable #1: java.lang.AssertionError: timed out waiting for green state
&gt;   2&gt; NOTE: All tests run in this JVM: [InternalEngineMergeIT, RepositoriesIT, TruncatedRecoveryIT, TopHitsIT, BlobStoreFormatIT, MetaDataIT, AliasResolveRoutingIT, RecoveryBackwardsCompatibilityIT, NettyPipeliningDisabledIT, HighlighterSearchIT, BulkProcessorIT, CorruptedFileIT, SimpleValidateQueryIT, GeoBoundsIT, ClusterStatsIT, NettyTransportPublishAddressIT, SimpleThreadPoolIT, IndexWithShadowReplicasIT, ContextCompletionSuggestSearchIT, ClusterInfoServiceIT, IngestProcessorNotInstalledOnAllNodesIT, ScriptFieldIT, ClusterSearchShardsIT, TransportSearchFailuresIT, ListenerActionIT, DateMathIndexExpressionsIntegrationIT, NestedIT, BasicAnalysisBackwardCompatibilityIT, MultiFieldsIntegrationIT, RecoveryWhileUnderLoadIT, GetIndexBackwardsCompatibilityIT, CircuitBreakerNoopIT, StressSearchServiceReaperIT, UpdateByNativeScriptIT, OpenCloseIndexIT, MatchedQueriesIT, UpdateNumberOfReplicasIT, UpgradeIT, AliasedIndexDocumentActionsIT, SimpleGetFieldMappingsIT, QueryProfilerIT]
&gt;    at __randomizedtesting.SeedInfo.seed([E6B1C23627D79398:AAAABA5277BD299C]:0)
&gt;    at org.elasticsearch.test.ESIntegTestCase.ensureGreen(ESIntegTestCase.java:880)
&gt;    at org.elasticsearch.test.ESIntegTestCase.ensureGreen(ESIntegTestCase.java:865)
&gt;    at org.elasticsearch.search.profile.QueryProfilerIT.testBoosting(QueryProfilerIT.java:389)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt; Completed [239/276] on J0 in 1098.11s, 11 tests, 1 failure &lt;&lt;&lt; FAILURES!
</description><key id="143073871">17302</key><summary>Test Failure in Build: org.elasticsearch.search.profile.QueryProfilerIT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Search</label><label>bug</label><label>test</label></labels><created>2016-03-23T20:37:35Z</created><updated>2016-05-03T14:13:49Z</updated><resolved>2016-05-03T14:13:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-24T17:15:14Z" id="200932998">@polyfractal could you take a look at this please?
</comment><comment author="polyfractal" created="2016-03-24T19:44:13Z" id="200987639">Will take a look on my flight tonight.  Appears to be unrelated to the actual profile test though, since the cluster never made it to green for the test to start.
</comment><comment author="polyfractal" created="2016-04-04T17:47:01Z" id="205412708">I forgot to update this thread: I couldn't reproduce this failure either. :/
</comment><comment author="clintongormley" created="2016-05-03T14:13:49Z" id="216541536">No further failures. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: MultiMatchQueryIT.testDefaults() fails reproducibly on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17301</link><project id="" key="" /><description>This failed on CI today at: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+periodic/181

I was able to reproduce on master (d7874d9) with

REPRODUCE WITH: gradle :core:integTest -Dtests.seed=7722FDB11A68170E -Dtests.class=org.elasticsearch.search.query.MultiMatchQueryIT -Dtests.method="testDefaults" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=th-TH -Dtests.timezone=America/Glace_Bay

Also fails with:
gradle :core:integTest -Dtests.seed=1DB8E9A029573C5C -Dtests.class=org.elasticsearch.search.query.MultiMatchQueryIT -Dtests.method="testDefaults" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=th-TH -Dtests.timezone=America/Glace_Bay
</description><key id="143073474">17301</key><summary>CI: MultiMatchQueryIT.testDefaults() fails reproducibly on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T20:35:56Z</created><updated>2016-03-30T21:22:23Z</updated><resolved>2016-03-30T21:22:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-23T21:15:52Z" id="200546557">Did some git bisect, looks like this started with e91a141.
</comment><comment author="s1monw" created="2016-03-23T21:43:12Z" id="200554889">I think this is misleading - that PR changed some stuff in the base-class that intorduces some more random calls. I think that's why
</comment><comment author="s1monw" created="2016-03-23T21:43:33Z" id="200554975">I am pretty sure this is caused by the new similarity in lucene 6
</comment><comment author="cbuescher" created="2016-03-30T21:22:22Z" id="203640866">Looks like eed885e fixed this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding exclude_template to ignore templates that match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17300</link><project id="" key="" /><description>Generally, you want your template to match all indices. However, when you supply a global template, you
do not always want that to be the case. For example, administrative indexes, like .kibana should generally not be impacted by your templates.

This adds generic support for exclude templates, but the general intent is the default behavior that it adds
surrounding global templates (`*`). If no exclude template is supplied with a global template, then it will
be ignored for any index that is prefixed by a '.' (e.g., `.kibana`).

This behavior can be overridden by supplying either an empty "exclude_template" (literally "") or specifying
any other non-null value.

Closes #17247
</description><key id="143072617">17300</key><summary>Adding exclude_template to ignore templates that match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Index Templates</label><label>enhancement</label><label>review</label></labels><created>2016-03-23T20:33:29Z</created><updated>2017-04-18T13:15:42Z</updated><resolved>2017-04-17T21:27:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2016-03-24T02:53:40Z" id="200630258">@pickypg I'm confusing the parameter name.
I think `skip_index_name` or `exclude_index_name` or `ignore_index_name` make more sense. 
What do you think?
</comment><comment author="pickypg" created="2016-03-24T05:36:55Z" id="200679319">@johtani I kind of like `ignore_indices`. What do you think?

Honestly, I've always found the `template` parameter confusing for this reason, but I wanted to name this new parameter consistently. I'm happy to wait and see if anyone else has any opinions too.
</comment><comment author="rjernst" created="2016-03-24T05:55:58Z" id="200686853">While I understand the problem, I think this would solve it from the wrong side. Instead of complicating templates, I think the solution is to allow index creation without applying templates? Then no special behavior is needed in how templates are triggered, and when these special indexes are created it would be clear they won't be affected by whatever random templates exist. 
</comment><comment author="pickypg" created="2016-03-24T14:24:01Z" id="200860939">@rjernst Are you referring to `index.hidden` over in elastic/elasticsearch#16904?

If so, that creates a chicken-or-egg problem because someone has to define that `index.hidden` setting upfront and chances are pretty decent that a global template is going to match _before_ that setting gets applied for any dynamically created index. It's fine for indices that get created upfront, but anything created that depends on template support (e.g., time-based `.` indices) is suddenly much more complicated.
</comment><comment author="clintongormley" created="2016-03-24T18:11:38Z" id="200954297">&gt; @rjernst Are you referring to index.hidden over in elastic/elasticsearch#16904?

@pickypg No, I'd add an `ignore_templates` parameter to the create-index API.  If true, no templates should be consulted when creating the index.
</comment><comment author="clintongormley" created="2016-03-25T15:07:11Z" id="201325329">Further comments added in the issue: https://github.com/elastic/elasticsearch/issues/17247#issuecomment-201325198
</comment><comment author="clintongormley" created="2016-04-15T14:18:12Z" id="210479682">@pickypg could you adjust this PR as per https://github.com/elastic/elasticsearch/issues/17247#issuecomment-210382049 please?
</comment><comment author="pickypg" created="2016-04-15T16:10:46Z" id="210523916">@clintongormley Rebased on master and renamed to `exclude_template`. Ready for a full a review now.
</comment><comment author="clintongormley" created="2016-05-07T14:36:13Z" id="217641505">@jpountz could you review this please?
</comment><comment author="clintongormley" created="2016-06-17T15:15:09Z" id="226797088">Also see https://github.com/elastic/elasticsearch/issues/18922 for a potential rename
</comment><comment author="pickypg" created="2016-06-17T15:21:10Z" id="226798794">Happy to rename it to `exclude_index_pattern`.
</comment><comment author="dakrone" created="2016-09-12T21:28:10Z" id="246499820">@jpountz I think this needs a re-review? (and probably needs to be rebased @pickypg )
</comment><comment author="rjernst" created="2016-09-12T21:44:53Z" id="246504444">I'm confused on why this is still being done on the template? The suggestion to use a flag on index creation seemed to have support.
</comment><comment author="clintongormley" created="2016-09-14T17:40:19Z" id="247095100">@rjernst see https://github.com/elastic/elasticsearch/issues/17247#issuecomment-201325198
</comment><comment author="pickypg" created="2017-04-17T21:27:14Z" id="294598242">This is now defunct with the desire to avoid template inheritance. Closing.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test Failure in Build: testScrollResponseBatchingBehavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17299</link><project id="" key="" /><description>Unable to reproduce locally.

Build Failure (https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-os-compatibility/os=ubuntu/111/)

Suite: org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests
  2&gt; REPRODUCE WITH: gradle :modules:reindex:test -Dtests.seed=8701CF7C189CEEC2 -Dtests.class=org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests -Dtests.method="testScrollResponseBatchingBehavior" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=es-SV -Dtests.timezone=Pacific/Enderbury
FAILURE 0.01s J0 | AsyncBulkByScrollActionTests.testScrollResponseBatchingBehavior &lt;&lt;&lt; FAILURES!

&gt; Throwable #1: java.lang.AssertionError: expected:&lt;{xssqpcyr=cauigsh}&gt; but was:&lt;null&gt;
&gt;    at __randomizedtesting.SeedInfo.seed([8701CF7C189CEEC2:3149B2EA2E7B5676]:0)
&gt;    at org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests.testScrollResponseBatchingBehavior(AsyncBulkByScrollActionTests.java:181)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   2&gt; NOTE: leaving temporary files on disk at: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-os-compatibility/os/ubuntu/modules/reindex/build/testrun/test/J0/temp/org.elasticsearch.index.reindex.AsyncBulkByScrollActionTests_8701CF7C189CEEC2-001
&gt;   2&gt; NOTE: test params are: codec=Asserting(Lucene60), sim=RandomSimilarity(queryNorm=false,coord=crazy): {}, locale=es-SV, timezone=Pacific/Enderbury
&gt;   2&gt; NOTE: Linux 3.13.0-83-generic amd64/Oracle Corporation 1.8.0_72-internal (64-bit)/cpus=8,threads=1,free=457043872,total=514850816
&gt;   2&gt; NOTE: All tests run in this JVM: [UpdateByQueryWithScriptTests, ReindexScriptTests, AsyncBulkByScrollActionTests]
&gt; Completed [6/17] on J0 in 0.66s, 23 tests, 1 failure &lt;&lt;&lt; FAILURES!
</description><key id="143069523">17299</key><summary>Test Failure in Build: testScrollResponseBatchingBehavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T20:21:50Z</created><updated>2016-04-05T13:56:59Z</updated><resolved>2016-03-24T13:56:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T13:27:07Z" id="200833008">Hey! This reproduces for me!
</comment><comment author="nik9000" created="2016-03-24T13:58:47Z" id="200849022">This was a threading issue I think. I could reproduce it by putting the reproduction line in a bash while loop. It'd fail about 1/3 times for me. I pushed a fix and it hasn't failed since.
</comment><comment author="s1monw" created="2016-03-24T14:11:05Z" id="200853789">@nik9000 thx I looked at it but didn't get what's wrong :)
</comment><comment author="jdconrad" created="2016-03-24T16:00:53Z" id="200901285">@nik9000 Thanks for fixing this.  I admit I only ran the test a few times as there were a lot of other build failures to look into, but a bash loop seems like a great way to do this next time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex shouldn't attempt to refresh on noops</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17298</link><project id="" key="" /><description>If the user asks for a refresh but their reindex or update-by-query
operation touched no indexes we should just skip the resfresh call
entirely. Without this commit we refresh _all_ indexes which is totally
wrong.

Closes #17296
</description><key id="143068399">17298</key><summary>Reindex shouldn't attempt to refresh on noops</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>non-issue</label><label>review</label><label>v2.3.0</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T20:16:34Z</created><updated>2016-03-24T17:57:00Z</updated><resolved>2016-03-23T22:13:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-23T20:17:25Z" id="200525958">@dakrone can you have a look?
</comment><comment author="dakrone" created="2016-03-23T20:42:30Z" id="200536671">LGTM
</comment><comment author="nik9000" created="2016-03-23T23:52:19Z" id="200586548">Merged to master. I'll backport in the morning.
</comment><comment author="nik9000" created="2016-03-24T12:44:13Z" id="200814603">2.3: 95de6169df593fd3503a0232bbd1c928975e2011
2.x: 222d14f595d9e0d348248bdbe1e2de7820f80385
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait for yellow indices when running upgrade test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17297</link><project id="" key="" /><description>This commit makes the Vagrant upgrade test wait for yellow indices
before attempting to get documents from the upgraded Elasticsearch node.

Relates #17294 
</description><key id="143064483">17297</key><summary>Wait for yellow indices when running upgrade test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>review</label><label>test</label></labels><created>2016-03-23T19:58:48Z</created><updated>2016-03-23T20:02:36Z</updated><resolved>2016-03-23T20:02:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-23T19:59:44Z" id="200519405">yeah lets do that, I think we have to wait for state being recovered in health to fix that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex and update-by-query "refresh" url parameter can cause a refresh for all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17296</link><project id="" key="" /><description>**Elasticsearch version**: 2.3, 2.x, and master branches (probably, only checked 2.3 but I'm pretty sure)

**JVM version**: 

```
manyair:elasticsearch-2.3.0 manybubbles$ java -version
java version "1.8.0_51"
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)
```

**OS version**:

```
manyair:elasticsearch-2.3.0 manybubbles$ uname -mrs
Darwin 14.5.0 x86_64
```

10.10.5 (14F1605)

**Description of the problem including expected versus actual behavior**:

``` bash
# Build an index with some docs
curl -XDELETE localhost:9200/test
for i in $(seq 1 1000); do
  curl -XPOST localhost:9200/test/test -d'{"tags": ["bannanas"]}'
  echo
done
curl -XPOST localhost:9200/test/_refresh

# This should be fast
curl -XPOST 'localhost:9200/test/_update_by_query?pretty&amp;refresh' -d'{
  "query": {
    "bool": {
      "must": [ {"match": {"tags": "bannanas"}} ],
      "must_not": [ {"match": {"tags": "chocolate"}} ]
    }
  },
  "script": {
    "inline": "ctx._source.tags += \"chocolate\""
  }
}'

# But repeat this and it tries to refresh all indexes.
curl -XPOST 'localhost:9200/test/_update_by_query?pretty&amp;refresh' -d'{
  "query": {
    "bool": {
      "must": [ {"match": {"tags": "bannanas"}} ],
      "must_not": [ {"match": {"tags": "chocolate"}} ]
    }
  },
  "script": {
    "inline": "ctx._source.tags += \"chocolate\""
  }
}'
```

That last command should instead refresh no indices. It is kind of hard to tell if it has refreshed all indices or none other than the it takes longer. On my laptop with the test data I have loaded it takes 60 seconds to refresh all indices (that seems like a long time but that is another issue). If you leave the `refresh` off of the url then update-by-query completes in 2 milliseconds.
</description><key id="143064329">17296</key><summary>Reindex and update-by-query "refresh" url parameter can cause a refresh for all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>bug</label><label>v2.3.0</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T19:58:00Z</created><updated>2016-03-23T22:13:27Z</updated><resolved>2016-03-23T22:13:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Indexing a json displaying floats in float or rounded to integer format fails </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17295</link><project id="" key="" /><description>Hi,

**Elasticsearch version**: 2.2.1 (from `docker pull elasticsearch`)

**JVM version**: OpenJDK 1.8.0_72-internal (from `docker pull elasticsearch`

**OS version**: Debian 8.3 (from `docker pull elasticsearch`

**Description of the problem including expected versus actual behavior**:

Indexing some data were float64 data could be written as integer (by luck) makes elasticsearch refuse to index them.
In a file were the first type indexed was a float type and then another in the collection could be written as integer will make the indexing fail.

I think it should accept an integer in a floating point type, even if doesn't have the floating point format.
On the other hand, I understand that it couldn't accept a floating point number in a integer type because it creates data loss.

**Steps to reproduce**:
1. `curl -X PUT localhost:9200/speed/record/1 -d '{ "speed_rec": [ { "speed": 61.23 }, { "speed": 61 } ] }'` will fail:
   `{"error":{"root_cause":[{"type":"mapper_parsing_exception","reason":"failed to parse"}],"type":"mapper_parsing_exception","reason":"failed to parse","caused_by":{"type":"illegal_argument_exception","reason":"mapper [speed_rec.speed] of different type, current_type [double], merged_type [long]"}},"status":400}`
2. `curl -X PUT localhost:9200/speed/record/1 -d '{ "speed_rec": [ { "speed": 61.23 }, { "speed": 61.0 } ] }'` will succeed `{"_index":"speed","_type":"record","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}`
</description><key id="143058874">17295</key><summary>Indexing a json displaying floats in float or rounded to integer format fails </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dolanor</reporter><labels /><created>2016-03-23T19:34:18Z</created><updated>2016-03-24T17:13:54Z</updated><resolved>2016-03-24T17:13:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-24T17:13:54Z" id="200932627">Duplicate of https://github.com/elastic/elasticsearch/issues/15377
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Vagrant upgrade test failing after relocating indexes to new folder structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17294</link><project id="" key="" /><description>We have a simple upgrade test that is failing frequently right now in master. The test is rather simple. Startup Elasticsearch 2.0.0, index some documents, stop Elasticsearch, upgrade to master, start Elasticsearch back up, and then check that those indexed documents are there. In these failures, Elasticsearch successfully starts back up, relocates the indices to the new folder structure, goes yellow, yet gives mysterious shard not found exceptions after get requests are issued against those documents. Here are the logs from a failed run:

```
[2016-03-22 16:23:40,311][INFO ][node                     ] [Manbot] version[2.0.0], pid[32374], build[de54438/2015-10-22T08:09:48Z]
[2016-03-22 16:23:40,311][INFO ][node                     ] [Manbot] initializing ...
[2016-03-22 16:23:40,366][INFO ][plugins                  ] [Manbot] loaded [], sites []
[2016-03-22 16:23:40,441][INFO ][env                      ] [Manbot] using [1] data paths, mounts [[/ (/dev/mapper/fedora-root)]], net usable_space [15.7gb], net total_space [17.4gb], spins? [possibly], types [xfs]
[2016-03-22 16:23:42,137][INFO ][node                     ] [Manbot] initialized
[2016-03-22 16:23:42,137][INFO ][node                     ] [Manbot] starting ...
[2016-03-22 16:23:42,178][INFO ][transport                ] [Manbot] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2016-03-22 16:23:42,194][INFO ][discovery                ] [Manbot] elasticsearch/_QbU4s4ISjWDgtZHjdbqJg
[2016-03-22 16:23:45,321][INFO ][cluster.service          ] [Manbot] new_master {Manbot}{_QbU4s4ISjWDgtZHjdbqJg}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-03-22 16:23:45,387][INFO ][http                     ] [Manbot] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}
[2016-03-22 16:23:45,387][INFO ][node                     ] [Manbot] started
[2016-03-22 16:23:45,393][INFO ][gateway                  ] [Manbot] recovered [0] indices into cluster_state
[2016-03-22 16:23:46,369][INFO ][cluster.metadata         ] [Manbot] [library] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [book]
[2016-03-22 16:23:46,693][INFO ][cluster.metadata         ] [Manbot] [library] update_mapping [book]
[2016-03-22 16:23:46,791][INFO ][cluster.metadata         ] [Manbot] [library2] creating index, cause [auto(index api)], templates [], shards [5]/[1], mappings [book]
[2016-03-22 16:23:46,926][INFO ][cluster.metadata         ] [Manbot] [library2] update_mapping [book]
[2016-03-22 16:23:47,107][INFO ][node                     ] [Manbot] stopping ...
[2016-03-22 16:23:47,276][INFO ][node                     ] [Manbot] stopped
[2016-03-22 16:23:47,276][INFO ][node                     ] [Manbot] closing ...
[2016-03-22 16:23:47,279][INFO ][node                     ] [Manbot] closed
[2016-03-22 16:23:48,660][WARN ][bootstrap                ] max file descriptors [65535] for elasticsearch process likely too low, increase to at least [65536]
[2016-03-22 16:23:48,677][INFO ][node                     ] [Mary "Skeeter" MacPherran] version[5.0.0-SNAPSHOT], pid[900], build[8004c51/2016-03-22T15:58:45.132Z]
[2016-03-22 16:23:48,677][INFO ][node                     ] [Mary "Skeeter" MacPherran] initializing ...
[2016-03-22 16:23:49,149][INFO ][plugins                  ] [Mary "Skeeter" MacPherran] modules [lang-mustache, lang-painless, ingest-grok, reindex, lang-expression, lang-groovy], plugins []
[2016-03-22 16:23:49,173][INFO ][env                      ] [Mary "Skeeter" MacPherran] using [1] data paths, mounts [[/ (/dev/mapper/fedora-root)]], net usable_space [15.7gb], net total_space [17.4gb], spins? [possibly], types [xfs]
[2016-03-22 16:23:49,173][INFO ][env                      ] [Mary "Skeeter" MacPherran] heap size [1015.6mb], compressed ordinary object pointers [true]
[2016-03-22 16:23:51,515][INFO ][common.util              ] [library/y0Jj3USfQpOkx0zpC88ObQ] upgrading [/var/lib/elasticsearch/elasticsearch/nodes/0/indices/library] to new naming convention
[2016-03-22 16:23:51,516][INFO ][common.util              ] [library/y0Jj3USfQpOkx0zpC88ObQ] moved from [/var/lib/elasticsearch/elasticsearch/nodes/0/indices/library] to [/var/lib/elasticsearch/elasticsearch/nodes/0/indices/y0Jj3USfQpOkx0zpC88ObQ]
[2016-03-22 16:23:51,519][INFO ][common.util              ] [library2/6Vuo4MIiQvGenbpxkLfi2Q] upgrading [/var/lib/elasticsearch/elasticsearch/nodes/0/indices/library2] to new naming convention
[2016-03-22 16:23:51,519][INFO ][common.util              ] [library2/6Vuo4MIiQvGenbpxkLfi2Q] moved from [/var/lib/elasticsearch/elasticsearch/nodes/0/indices/library2] to [/var/lib/elasticsearch/elasticsearch/nodes/0/indices/6Vuo4MIiQvGenbpxkLfi2Q]
[2016-03-22 16:23:51,605][INFO ][node                     ] [Mary "Skeeter" MacPherran] initialized
[2016-03-22 16:23:51,609][INFO ][node                     ] [Mary "Skeeter" MacPherran] starting ...
[2016-03-22 16:23:51,683][INFO ][transport                ] [Mary "Skeeter" MacPherran] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.0.0.1:9300}
[2016-03-22 16:23:54,818][INFO ][cluster.service          ] [Mary "Skeeter" MacPherran] new_master {Mary "Skeeter" MacPherran}{D0lJ0rzOTmGhTEmHl1p8sQ}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-03-22 16:23:54,900][INFO ][http                     ] [Mary "Skeeter" MacPherran] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200}, {127.0.0.1:9200}
[2016-03-22 16:23:54,901][INFO ][node                     ] [Mary "Skeeter" MacPherran] started
[2016-03-22 16:23:55,060][INFO ][gateway                  ] [Mary "Skeeter" MacPherran] recovered [2] indices into cluster_state
[2016-03-22 16:23:55,372][WARN ][rest.suppressed          ] /library/book/1 Params: {pretty=, index=library, id=1, type=book}
NoShardAvailableActionException[No shard available for [get [library][book][1]: routing [null]]]
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:205)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:184)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:93)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:57)
    at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:150)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:174)
    at org.elasticsearch.action.ingest.IngestActionFilter.apply(IngestActionFilter.java:80)
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:172)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:145)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:87)
    at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:64)
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:402)
    at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:494)
    at org.elasticsearch.rest.action.get.RestGetAction.handleRequest(RestGetAction.java:79)
    at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:51)
    at org.elasticsearch.rest.RestController.executeHandler(RestController.java:214)
    at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:174)
    at org.elasticsearch.http.HttpServer.dispatchRequest(HttpServer.java:101)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:487)
    at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:65)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:85)
    at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:83)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-22 16:23:55,561][INFO ][node                     ] [Mary "Skeeter" MacPherran] stopping ...
[2016-03-22 16:23:55,595][WARN ][cluster.action.shard     ] [Mary "Skeeter" MacPherran] [library][2] unexpected failure while sending request [internal:cluster/shard/started] to [{Mary "Skeeter" MacPherran}{D0lJ0rzOTmGhTEmHl1p8sQ}{127.0.0.1}{127.0.0.1:9300}] for shard [target shard [[[library/y0Jj3USfQpOkx0zpC88ObQ]][2], node[D0lJ0rzOTmGhTEmHl1p8sQ], [P], s[INITIALIZING], a[id=Mjw1dR_XTDO2NZ4Oqm9nGA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-22T16:23:54.974Z]]], source shard [[[library/y0Jj3USfQpOkx0zpC88ObQ]][2], node[D0lJ0rzOTmGhTEmHl1p8sQ], [P], s[INITIALIZING], a[id=Mjw1dR_XTDO2NZ4Oqm9nGA], unassigned_info[[reason=CLUSTER_RECOVERED], at[2016-03-22T16:23:54.974Z]]], message [after recovery from store]]
SendRequestTransportException[[Mary "Skeeter" MacPherran][127.0.0.1:9300][internal:cluster/shard/started]]; nested: TransportException[TransportService is closed stopped can't send request];
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:331)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:290)
    at org.elasticsearch.cluster.action.shard.ShardStateAction.sendShardAction(ShardStateAction.java:101)
    at org.elasticsearch.cluster.action.shard.ShardStateAction.shardStarted(ShardStateAction.java:333)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.lambda$applyInitializingShard$2(IndicesClusterStateService.java:627)
    at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:408)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: TransportException[TransportService is closed stopped can't send request]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:311)
    ... 8 more
[2016-03-22 16:23:55,620][INFO ][node                     ] [Mary "Skeeter" MacPherran] stopped
[2016-03-22 16:23:55,620][INFO ][node                     ] [Mary "Skeeter" MacPherran] closing ...
[2016-03-22 16:23:55,631][INFO ][node                     ] [Mary "Skeeter" MacPherran] closed
```
</description><key id="143058502">17294</key><summary>Vagrant upgrade test failing after relocating indexes to new folder structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>blocker</label><label>bug</label></labels><created>2016-03-23T19:33:04Z</created><updated>2016-04-05T13:56:00Z</updated><resolved>2016-03-25T02:01:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-25T02:01:41Z" id="201107439">These logs still seemingly indicate an odd issue here, but it's not reliably reproducible so I'm closing this until it surfaces again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Useful error message for null property placeholder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17293</link><project id="" key="" /><description>This commit adds the key to the error message when encountering a
missing property placeholder.

Closes #17292 
</description><key id="143026667">17293</key><summary>Useful error message for null property placeholder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T17:27:15Z</created><updated>2016-03-23T22:45:14Z</updated><resolved>2016-03-23T22:45:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-23T17:32:24Z" id="200455987">LGTM, but any chance we can get rid of the NullPointerException part of the message? feels awkward for settings which are configured by people out of Java land (and it's really a missing value..)
</comment><comment author="rjernst" created="2016-03-23T18:04:58Z" id="200469224">I don't think we should complicate this because of an internal bug. We should never accept null as a value in settings, so we should never get to here. So instead I think we should add checks in Settings.put for null values and fail there?
</comment><comment author="jasontedor" created="2016-03-23T19:00:22Z" id="200496937">It's not a complication, it's just improving a guard that is already there. And that guard should be there. This is a public method on a public class and it should protect against illegal values.
</comment><comment author="rjernst" created="2016-03-23T19:03:11Z" id="200497808">But you are changing the api to need extra information just for debugging. Of course the guard should be there, but it is a guard, not something a user should ever see, and so not something we need pretty error messages for.
</comment><comment author="rjernst" created="2016-03-23T19:05:46Z" id="200498655">Also, this code won't actually tell us where the brokenness is (where we are inserting a null value). Adding null value checks on all the put methods would.
</comment><comment author="jasontedor" created="2016-03-23T20:14:19Z" id="200525072">&gt; Of course the guard should be there, but it is a guard, not something a user should ever see, and so not something we need pretty error messages for.

You agree the guard should be there, but disagree that a useful error message should be produced if it trips?
</comment><comment author="rjernst" created="2016-03-23T20:17:16Z" id="200525917">I think the guard was fine before. Mucking with an api (regardless of whether it is public, which it shouldn't be, this method is used in exactly one place) should not be done to change a failure message in a precondition.
</comment><comment author="jasontedor" created="2016-03-23T20:37:55Z" id="200535363">Please correct me if I'm wrong, but I'm interpreting that as a yes, you agree that the guard should be there, but disagree that a useful error message should be produced if it trips.

&gt; Mucking with an api

This is not mucking, this is a _very_ straightforward change. I posit that this is how the guard should have been written in the first place.

&gt; this method is used in exactly one place

All the more reason that a straightforward API change to improve a guard message is not a big deal.

&gt; should not be done to change a failure message in a precondition.

Those words, "should not", are very strong. What basis do you have for making that claim?
</comment><comment author="bleskes" created="2016-03-23T22:41:05Z" id="200571646">&gt; We should never accept null as a value in settings, so we should never get to here. 

I think it's relevant to point out that the settings object does accept null values - it is the signal to go back to default and "remove" settings.

looking at the code again, having in mind that on the settings level null is a valid value, I wonder if this is the right solution - if we accept null as values, then the `PropertyPlaceholder` should actually treat them as it does empty strings - namely, things that do not have ${} in them.

Since the problem here is that null is _invalid_ for the node settings, I tend to agree that the validation better go higher up to where we read node level settings (and fix the null pointer exception on this level).

re the "api change" - I think we're blowing a single method parameter addition (for a very valid reason - a better error message, which is worth more than the check sometime - see recent correct setting suggestion when configuring the wrong one) out of proportion.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17292</link><project id="" key="" /><description>When testing master I get a not so descriptive error message. I'm guessing there are some bad settings, but it would be nice to get an error message decribing which key has a value that cannot be null.

```
Exception in thread "main" java.lang.NullPointerException: Argument 'value' must not be null.
    at java.util.Objects.requireNonNull(Objects.java:228)
    at org.elasticsearch.common.property.PropertyPlaceholder.replacePlaceholders(PropertyPlaceholder.java:80)
    at org.elasticsearch.common.settings.Settings$Builder.replacePropertyPlaceholders(Settings.java:1224)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.initializeSettings(InternalSettingsPreparer.java:133)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:107)
    at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:199)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:237)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:108)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:103)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
```
</description><key id="143025522">17292</key><summary>Improve error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">beiske</reporter><labels><label>:Settings</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T17:23:49Z</created><updated>2016-03-23T22:45:14Z</updated><resolved>2016-03-23T22:45:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="beiske" created="2016-03-23T19:27:04Z" id="200506453">The root cause was this setting in elasticsearch.yml:

```
cloud:
    aws:
        region:  #  eu-west-1,  us-east-1 etc
```

It is not a valid setting, but some hint at the key would be helpful for most I think.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactored inner hits parsing and intoduced InnerHitBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17291</link><project id="" key="" /><description>Both top level and inline inner hits are now covered by InnerHitBuilder.
Although there are differences between top level and inline inner hits,
they now make use of the same builder logic.

The parsing of top level inner hits slightly changed to be more readable.
Before the nested path or parent/child type had to be specified as encapsuting
json object, now these settings are simple fields. Before this was required
to allow streaming parsing of inner hits without missing contextual information.

Once some issues are fixed with inline inner hits (around multi level hierachy of inner hits),
top level inner hits will be deprecated and removed in the next major version.

Also I think some `SearchParseElement` implementations can be removed, but I haven't done that yet.
</description><key id="143016233">17291</key><summary>Refactored inner hits parsing and intoduced InnerHitBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search Refactoring</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T16:48:47Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-30T13:26:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-24T16:51:24Z" id="200920610">@martijnvg this looks great, even though I didn't compare it completely to the way it was before the refactoring, I have the feeling this change simplifies a lot. Left a few minor comments, only thing that was causing some trouble with the tests on my side was the `size` shadowing issue in `SearchSourceBuilder#readFrom()`, but will run the whole tests again now with a local fix. Other than that, maybe it would be good to also have a basic test for `InnerHitsBuilder`, although that one doesn't have much logic of its own.
Its also a quiet a big PR, maybe somebody else involved in the search refactoring should take a second look after this round. I'll ask around.
</comment><comment author="martijnvg" created="2016-03-24T23:08:38Z" id="201067165">@cbuescher Thanks for looking at this. I updated the PR based on your comment and fixed the `size` shadowing issue.
</comment><comment author="dimfeld" created="2016-03-25T01:59:22Z" id="201107105">With top-level `inner_hits` going away, is there a recommended way to replace the functionality? My usual case here involves using a `has_child` in the query to look for parents with a particular value in a child, and then a top-level `inner_hits` to return the top few child objects of the same type, ranked by some value using a `function_score`. 

I'm thinking that something like this will work; is there a better way?

```
{  query:  {  bool: {
   must: // main query with "real" has_child filter and whatever else goes here
   should: [
      { has_child: { type: childType, query: {function_score query}, inner_hits: {} } 
    ]
} } } 
```
</comment><comment author="martijnvg" created="2016-03-25T08:24:05Z" id="201196909">@dimfeld Yes, that should work too. Also I'm thinking of also allowing the `query` option inside inline inner hits (which would then overwrite the actual inner query), so that you can do something like this:

```
{  query:  {  bool: {
   must: [
      { has_child: { type: childType, query: {real query}, inner_hits: {query: {function_score query}} } 
    ]
} } } 
```

This would be better, because it would be more efficient as only one `has_child` query would need to be specified. 

The goal with replacing inner hits is that same things can be done with inline inner hits as top level inner hits.
</comment><comment author="dimfeld" created="2016-03-25T14:22:35Z" id="201306749">Sounds good. Thanks @martijnvg!
</comment><comment author="cbuescher" created="2016-03-29T12:39:09Z" id="202874815">@martijnvg thanks, changes look great and all works for me now, but maybe @colings86, @javanna or @s1monw want to take a second look at this before it gets in.
</comment><comment author="colings86" created="2016-03-30T10:07:33Z" id="203362541">@martijnvg I left a few minor comments but otherwise it LGTM
</comment><comment author="cbuescher" created="2016-03-30T10:43:37Z" id="203374507">@martijnvg I did another quick round, left a few minor suggestions and one question for my own understanding.
</comment><comment author="martijnvg" created="2016-03-30T11:07:20Z" id="203380828">@cbuescher @colings86 I've updated the PR.
</comment><comment author="cbuescher" created="2016-03-30T13:02:02Z" id="203421543">Thanks, looked at the last changes, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoDistanceQueryBuilder point(double lat, double lon) error in parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17290</link><project id="" key="" /><description>EDIT: make a mistake in my code... 

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.8

**OS version**: Ubuntu 14.04

**Description of the problem including expected versus actual behavior**:
I using ElasticSearch Java Api to execute a **geo_distance** query and there is problem in the function _point(..)_ from GeoDistanceQueryBuilder class.

The problem : the _point(..)_ function use the _lon_ argument to get the latitude data and the _lat_ argument to get the longitude data.

In my case, i work on Geoname data.
For example, i know that  Cherbourg-Octeville(city in France) have the coordinates:
longitude :  -1.625
latitude : 49.638889

When i execute this code : 

```
 SearchResponse resp = this.client.prepareSearch(this.index_name)
                .setTypes(this.type_name)
                .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
                .setQuery(QueryBuilders
                        .geoDistanceQuery("location")

                        .point( -1.625, 49.638889)
                        .distance(1000, DistanceUnit.METERS))
                .setSize(20)
                .execute()
                .actionGet();

        if (resp.getHits().totalHits() &gt; 0) {
            for (SearchHit hit : resp.getHits()) {
                System.out.println(hit.getSource().toString());
            }
        } 
```

**I get no results!**

**But if i inverse lat and lon, i get:**

{country=FR, alias=[CER, Cherbourg, Cherburgo-Octeville, Coriallum, Gorad Shehrbur-Aktehvil', Kiaeresburh, Ki&#230;resburh, Sermpour-Oktvil, Sherbur-Oktevil', Sherbur-Oktvil', Tchidbouo, cerpork-aktvil, sairbura, se bao-ao ke te wei er, sheruburu=okutovu~iru, shrbwrg-aktwwyl, srbwr, syeleubuleuogteubil, &#931;&#949;&#961;&#956;&#960;&#959;&#973;&#961;-&#927;&#954;&#964;&#946;&#943;&#955;, &#1043;&#1086;&#1088;&#1072;&#1076; &#1064;&#1101;&#1088;&#1073;&#1091;&#1088;-&#1040;&#1082;&#1090;&#1101;&#1074;&#1110;&#1083;&#1100;, &#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1074;&#1080;&#1083;&#1100;, &#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1077;&#1074;&#1080;&#1083;&#1100;, &#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1077;&#1074;&#1110;&#1083;&#1100;, &#1513;&#1512;&#1489;&#1493;&#1512;, &#1588;&#1585;&#1576;&#1608;&#1585;&#1711;-&#1575;&#1705;&#1578;&#1608;&#1608;&#1740;&#1604;, &#2358;&#2376;&#2352;&#2381;&#2348;&#2370;&#2352;, &#2970;&#3014;&#2992;&#3021;&#2986;&#3019;&#2992;&#3021;&#2965;&#3021;-&#2950;&#2965;&#3021;&#2975;&#3021;&#2997;&#3007;&#2994;&#3021;, &#12471;&#12455;&#12523;&#12502;&#12540;&#12523;&#65309;&#12458;&#12463;&#12488;&#12532;&#12451;&#12523;, &#29791;&#22561;-&#22885;&#20811;&#29305;&#32500;&#23572;, &#49520;&#47476;&#48512;&#47476;&#50725;&#53944;&#48716;], location={lon=-1.61636, lat=49.63984}, id=3025466, label=Cherbourg-Octeville, population=26655}
{country=FR, alias=[La Polle, la Pole], location={lon=-1.63664, lat=49.63801}, id=3007228, label=La Polle, population=0}
{country=FR, alias=[], location={lon=-1.62162, lat=49.64642}, id=3231119, label=Port de Chantereyne, population=0}
{country=FR, alias=[Cherbourg, Cherburgo-Octeville, Chervourgo, Coriallum, Gorad Shehrbur-Aktehvil', Kiaeresburh, Ki&#230;resburh, Sherbur-Oktevil', Sherbur-Oktvil', Tchidbouo, cerpork-aktvil, sairbura, se bao-ao ke te wei er, sheruburu, shrbwrg-aktwwyl, syeleubuleuogteubil, &#935;&#949;&#961;&#946;&#959;&#973;&#961;&#947;&#959;, &#1043;&#1086;&#1088;&#1072;&#1076; &#1064;&#1101;&#1088;&#1073;&#1091;&#1088;-&#1040;&#1082;&#1090;&#1101;&#1074;&#1110;&#1083;&#1100;, &#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1074;&#1110;&#1083;&#1100;, &#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1077;&#1074;&#1080;&#1083;&#1100;, &#1588;&#1585;&#1576;&#1608;&#1585;&#1711;-&#1575;&#1705;&#1578;&#1608;&#1608;&#1740;&#1604;, &#2358;&#2376;&#2352;&#2381;&#2348;&#2370;&#2352;, &#2970;&#3014;&#2992;&#3021;&#2986;&#3019;&#2992;&#3021;&#2965;&#3021;-&#2950;&#2965;&#3021;&#2975;&#3021;&#2997;&#3007;&#2994;&#3021;, &#12471;&#12455;&#12523;&#12502;&#12540;&#12523;, &#29791;&#22561;-&#22885;&#20811;&#29305;&#32500;&#23572;, &#49520;&#47476;&#48512;&#47476;&#50725;&#53944;&#48716;], **location={lon=-1.6147, lat=49.6386}**, id=6614508, label=Cherbourg-Octeville, population=39003}
{country=FR, alias=[Divette, La Divette Riviere, La Divette Rivi&#232;re], location={lon=-1.61792, lat=49.63312}, id=3021299, label=Divette, population=0}

**And it's also prove that the data are correct in the database.**

**I tried with curl, and it worked too:**
curl -XGET host -d '

```
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "geo_distance": {
          "distance": "1000m",
          "location": {
            "lat":49.6388,
            "lon":-1.625
          }
        }
      }
    }
  }
}
```

'

**and got :**

```
{
  "took": 12,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 5,
    "max_score": 1,
    "hits": [
      {
        "_index": "geoname",
        "_type": "geo",
        "_id": "AVOkIJPQ62xTQlYSuF1u",
        "_score": 1,
        "_source": {
          "id": "3025466",
          "label": "Cherbourg-Octeville",
          "alias": [
            "CER",
            "Cherbourg",
            "Cherburgo-Octeville",
            "Coriallum",
            "Gorad Shehrbur-Aktehvil'",
            "Kiaeresburh",
            "Ki&#230;resburh",
            "Sermpour-Oktvil",
            "Sherbur-Oktevil'",
            "Sherbur-Oktvil'",
            "Tchidbouo",
            "cerpork-aktvil",
            "sairbura",
            "se bao-ao ke te wei er",
            "sheruburu=okutovu~iru",
            "shrbwrg-aktwwyl",
            "srbwr",
            "syeleubuleuogteubil",
            "&#931;&#949;&#961;&#956;&#960;&#959;&#973;&#961;-&#927;&#954;&#964;&#946;&#943;&#955;",
            "&#1043;&#1086;&#1088;&#1072;&#1076; &#1064;&#1101;&#1088;&#1073;&#1091;&#1088;-&#1040;&#1082;&#1090;&#1101;&#1074;&#1110;&#1083;&#1100;",
            "&#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1074;&#1080;&#1083;&#1100;",
            "&#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1077;&#1074;&#1080;&#1083;&#1100;",
            "&#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1077;&#1074;&#1110;&#1083;&#1100;",
            "&#1513;&#1512;&#1489;&#1493;&#1512;",
            "&#1588;&#1585;&#1576;&#1608;&#1585;&#1711;-&#1575;&#1705;&#1578;&#1608;&#1608;&#1740;&#1604;",
            "&#2358;&#2376;&#2352;&#2381;&#2348;&#2370;&#2352;",
            "&#2970;&#3014;&#2992;&#3021;&#2986;&#3019;&#2992;&#3021;&#2965;&#3021;-&#2950;&#2965;&#3021;&#2975;&#3021;&#2997;&#3007;&#2994;&#3021;",
            "&#12471;&#12455;&#12523;&#12502;&#12540;&#12523;&#65309;&#12458;&#12463;&#12488;&#12532;&#12451;&#12523;",
            "&#29791;&#22561;-&#22885;&#20811;&#29305;&#32500;&#23572;",
            "&#49520;&#47476;&#48512;&#47476;&#50725;&#53944;&#48716;"
          ],
          "location": {
            "lon": -1.61636,
            "lat": 49.63984
          },
          "population": 26655,
          "country": "FR"
        }
      },
      {
        "_index": "geoname",
        "_type": "geo",
        "_id": "AVOkIJPF62xTQlYSuCig",
        "_score": 1,
        "_source": {
          "id": "3007228",
          "label": "La Polle",
          "alias": [
            "La Polle",
            "la Pole"
          ],
          "location": {
            "lon": -1.63664,
            "lat": 49.63801
          },
          "population": 0,
          "country": "FR"
        }
      },
      {
        "_index": "geoname",
        "_type": "geo",
        "_id": "AVOkIMbm62xTQlYSuIP0",
        "_score": 1,
        "_source": {
          "id": "3231119",
          "label": "Port de Chantereyne",
          "alias": [
            ""
          ],
          "location": {
            "lon": -1.62162,
            "lat": 49.64642
          },
          "population": 0,
          "country": "FR"
        }
      },
      {
        "_index": "geoname",
        "_type": "geo",
        "_id": "AVOkIMb962xTQlYSuO27",
        "_score": 1,
        "_source": {
          "id": "6614508",
          "label": "Cherbourg-Octeville",
          "alias": [
            "Cherbourg",
            "Cherburgo-Octeville",
            "Chervourgo",
            "Coriallum",
            "Gorad Shehrbur-Aktehvil'",
            "Kiaeresburh",
            "Ki&#230;resburh",
            "Sherbur-Oktevil'",
            "Sherbur-Oktvil'",
            "Tchidbouo",
            "cerpork-aktvil",
            "sairbura",
            "se bao-ao ke te wei er",
            "sheruburu",
            "shrbwrg-aktwwyl",
            "syeleubuleuogteubil",
            "&#935;&#949;&#961;&#946;&#959;&#973;&#961;&#947;&#959;",
            "&#1043;&#1086;&#1088;&#1072;&#1076; &#1064;&#1101;&#1088;&#1073;&#1091;&#1088;-&#1040;&#1082;&#1090;&#1101;&#1074;&#1110;&#1083;&#1100;",
            "&#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1074;&#1110;&#1083;&#1100;",
            "&#1064;&#1077;&#1088;&#1073;&#1091;&#1088;-&#1054;&#1082;&#1090;&#1077;&#1074;&#1080;&#1083;&#1100;",
            "&#1588;&#1585;&#1576;&#1608;&#1585;&#1711;-&#1575;&#1705;&#1578;&#1608;&#1608;&#1740;&#1604;",
            "&#2358;&#2376;&#2352;&#2381;&#2348;&#2370;&#2352;",
            "&#2970;&#3014;&#2992;&#3021;&#2986;&#3019;&#2992;&#3021;&#2965;&#3021;-&#2950;&#2965;&#3021;&#2975;&#3021;&#2997;&#3007;&#2994;&#3021;",
            "&#12471;&#12455;&#12523;&#12502;&#12540;&#12523;",
            "&#29791;&#22561;-&#22885;&#20811;&#29305;&#32500;&#23572;",
            "&#49520;&#47476;&#48512;&#47476;&#50725;&#53944;&#48716;"
          ],
          **"location": {
            "lon": -1.6147,
            "lat": 49.6386
          },**
          "population": 39003,
          "country": "FR"
        }
      },
      {
        "_index": "geoname",
        "_type": "geo",
        "_id": "AVOkIJPO62xTQlYSuFEe",
        "_score": 1,
        "_source": {
          "id": "3021299",
          "label": "Divette",
          "alias": [
            "Divette",
            "La Divette Riviere",
            "La Divette Rivi&#232;re"
          ],
          "location": {
            "lon": -1.61792,
            "lat": 49.63312
          },
          "population": 0,
          "country": "FR"
        }
      }
    ]
  }
}
```

I hope my explication was clear, it's my first post on GitHub :)

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="143011353">17290</key><summary>GeoDistanceQueryBuilder point(double lat, double lon) error in parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Jacobe2169</reporter><labels /><created>2016-03-23T16:32:45Z</created><updated>2016-03-23T16:36:42Z</updated><resolved>2016-03-23T16:36:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cli: Reject positional argument for bin/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17289</link><project id="" key="" /><description>This exits with a usage error when bin/elasticsearch recieves any
positional args.

closes #17287 

I labeled this as a non-issue since it is not yet released.
</description><key id="143003495">17289</key><summary>Cli: Reject positional argument for bin/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T16:05:16Z</created><updated>2016-03-23T18:53:33Z</updated><resolved>2016-03-23T18:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-23T16:08:19Z" id="200414455">Thanks @rjernst. LGTM.
</comment><comment author="dadoonet" created="2016-03-23T16:13:46Z" id="200417771">Just wondering if we can have a test which mixes `-E` args with positional ones like `-v foo` or `foo -v`? Would it make sense?
</comment><comment author="rjernst" created="2016-03-23T18:53:33Z" id="200493450">@dadoonet I added another test for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce `discovery.zen.minimum_master_nodes` is set when bound to a public ip</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17288</link><project id="" key="" /><description>discovery.zen.minimum_master_nodes is the single most important setting to set on a production cluster. We have no way of supplying a good default so it must be set by the user. Binding a node to a public IP (as opposed to the default local host) is a good enough indication that a node will be part of a production cluster cluster and thus it's a good tradeoff to enforce the settings. Note that nothing prevent users from setting it to 1 in a single node cluster.
</description><key id="143002336">17288</key><summary>Enforce `discovery.zen.minimum_master_nodes` is set when bound to a public ip</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T16:00:31Z</created><updated>2016-03-25T11:58:24Z</updated><resolved>2016-03-25T11:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-23T16:46:22Z" id="200434377">There's a comment at the top of `BootstrapCheck.java` about `discovery.zen.minimum_master_nodes`; can you remove it now?
</comment><comment author="jasontedor" created="2016-03-25T00:38:01Z" id="201085728">@bleskes Thank you for picking this one up. I left a few more comments about the unrelated formatting changes and the test; feel free to process and push at your discretion.

It would be nice to someday make the check much stronger than whether or not the setting is set at all (i.e., enforce that it's a quorum). I think that we should think about this over the next few months as we work on related problems in this area.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>new jopt-simple parser drops non switch arguments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17287</link><project id="" key="" /><description>&gt; $ elasticsearch blahblah

silently ignores blahblah

which can get hairy if someone forgets an -E e.g

&gt; $ elasticsearch -E es.cluster.name="x" es.node.name="y"

the latter setting is ignored.
</description><key id="143000638">17287</key><summary>new jopt-simple parser drops non switch arguments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2016-03-23T15:55:21Z</created><updated>2016-03-23T18:53:32Z</updated><resolved>2016-03-23T18:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-23T16:03:05Z" id="200412354">I'll have a fix up shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve testing for building SortFields in SortBuilders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17286</link><project id="" key="" /><description>This is a follow up issue as discussed in #17205. We have added randomized roundtrip testing for SortBuilders (toXContent -&gt; fromXContent) and serialization tests in AbstractSortBuilderTests, but currently we do no very thourough testing of the SortField that is emitted when calling the sort builders `build()` method. Most of this is covered by other integration tests, but we should investigate if we can improve the assertions made on the SortFields, while keeping the tests simple.
One of the challanges here is that most sort builders create SortFields of SortField.Type.CUSTOM, providing their own FieldComparatorSource, which in turn wraps most of the builders options and is difficult to access from tests.
</description><key id="142995497">17286</key><summary>Improve testing for building SortFields in SortBuilders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>adoptme</label><label>enhancement</label><label>test</label></labels><created>2016-03-23T15:38:43Z</created><updated>2016-03-23T15:38:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch should reject dynamic templates with unknown `match_mapping_type`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17285</link><project id="" key="" /><description>When looking at the logstash template, I noticed that it has definitions for
dynamic temilates with `match_mapping_type` equal to `byte` for instance.
However elasticsearch never tries to find templates that match the byte type
(only long or double as far as numbers are concerned). This commit changes
template parsing in order to ignore bad values of `match_mapping_type` (given
how the logstash template is popular, this would break many upgrades
otherwise). Then I hope to fail the parsing on bad values in 6.0.

Relates to #16945
</description><key id="142987601">17285</key><summary>Elasticsearch should reject dynamic templates with unknown `match_mapping_type`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha5</label></labels><created>2016-03-23T15:11:10Z</created><updated>2016-07-19T15:54:48Z</updated><resolved>2016-07-19T15:54:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T15:27:17Z" id="208400995">LGTM. For the future around this, it would be nice if this weren't a public enum. It seems like this enum is really used in place of a map.
</comment><comment author="jpountz" created="2016-04-11T16:12:59Z" id="208430085">&gt; For the future around this, it would be nice if this weren't a public enum.

Agreed. we will want to collapse the mapper sub packages at some point (waiting to be done with points integration before doing so).
</comment><comment author="dakrone" created="2016-07-11T17:00:40Z" id="231796949">@jpountz this has a +1, should it be merged?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop using PROTOTYPE in NamedWriteableRegistry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17284</link><project id="" key="" /><description>readFrom is confusing because it requires an instance of the type that it
is reading but it doesn't modify it. But we also have (deprecated) methods
named readFrom that _do_ modify the instance. The "right" way to implement
the non-modifying readFrom is to delegate to a constructor that takes a
StreamInput so that the read object can be immutable. Now that we have
`@FunctionalInterface`s it is fairly easy to register things by referring
directly to the constructor.

This change modifying NamedWriteableRegistry so that it does that. It keeps
supporting `registerPrototype` which registers objects to be read by
readFrom but deprecates it and delegates it to a new `register` method
that allows passing a simple functional interface. It also cuts Task.Status
subclasses over to using that method.

The start of #17085
</description><key id="142974961">17284</key><summary>Stop using PROTOTYPE in NamedWriteableRegistry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T14:30:14Z</created><updated>2016-03-24T18:20:25Z</updated><resolved>2016-03-24T15:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-23T14:31:58Z" id="200368839">Can someone who's been working on the great refactorings have a look at this? Like @abeyad or @cbuescher or @MaineC ? If I do more work in this area it is going to stray in that direction.
</comment><comment author="cbuescher" created="2016-03-23T14:38:04Z" id="200372790">Also adding @javanna and @colings86 since this also touches the NamedWritable infra.
</comment><comment author="javanna" created="2016-03-23T14:53:33Z" id="200379863">left a few questions. I like it though. Shout out if you need help moving forward with this refactoring, would love to help.
</comment><comment author="colings86" created="2016-03-23T14:58:02Z" id="200382235">I'm not entirely sure what this change buys us since it separates the reading and writing sides of the serialisation into different classes requiring yet another class to be added for any feature we implement and still doesn't enforce a constructor which takes a StreamInput since I can just implement Reader and read everything off the stream before calling a different constructor. Or am I missing something here?
</comment><comment author="colings86" created="2016-03-23T15:10:05Z" id="200386951">ok, actually I missed that you can just provide `ReaderImpl::new` but am I right in saying that there is no way to enforce that convention here?

I'm not against this change at all I just want to make sure I understand it
</comment><comment author="nik9000" created="2016-03-23T15:15:43Z" id="200389352">&gt; I'm not entirely sure what this change buys us

It lets you remove `readFrom` and `PROTOTYPE` from lots of places. This is hard to do in lots of places - lots of the new builders don't lend themselves well to a constructor doing the reading because of the order of the data in the stream.

&gt; yet another class to be added for any feature we implement

Its a `@FunctionalInterface` so you can implement it with a reference to a constructor. For enums or other classes that don't want a constructor you can implement it with reference to a static method. I think this nets you slightly less code because you don't need `readFrom`. You get to swap a `PROTOTYPE` object for a `public static final String NAME` which many classes already have.

&gt; I can just implement Reader and read everything off the stream before calling a different constructor

As much as I'd love to require the constructor I don't see a good way to do it. I could use the naming conventions check but that'd be a bit painful and it'd have to skip its job on enums. I've come around to your argument on #17085 - we don't need a check. Just having it be normal to implement this with a constructor reference is probably fine. I figure if someone wants to do something weird they can defend it in code review. I mean, for backwards compatibility we delegate it to `readFrom` right there in NamedWriteableRegistry but I expect for the most part we'll want a constructor for consistency.
</comment><comment author="nik9000" created="2016-03-23T15:17:35Z" id="200389952">&gt; no way to enforce that convention here?

No way at all. And I don't think we could do so without some post compile build step like naming conventions or logger usage.
</comment><comment author="colings86" created="2016-03-23T15:24:21Z" id="200392267">Ok thanks @nik9000 it makes sense now. I see how it moves things forward a bit now.
</comment><comment author="nik9000" created="2016-03-23T15:50:29Z" id="200405099">Pushed some more changes in response to comments.
</comment><comment author="nik9000" created="2016-03-24T12:07:33Z" id="200805619">@javanna, would you mind reviewing again? I pushed some changes.
</comment><comment author="javanna" created="2016-03-24T13:20:45Z" id="200829395">left two minors, LGTM otherwise
</comment><comment author="nik9000" created="2016-03-24T15:31:07Z" id="200887630">&gt; left two minors, LGTM otherwise

Fixed and merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't highlight across has_child</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17283</link><project id="" key="" /><description>&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.1.1

**JVM version**: 1.8.0_65 x86_64

**OS version**: Fedora Core 22

**Description of the problem including expected versus actual behavior**:
Using a top-level `inner_hits` to highlight a `nested` object in a child document produces hits with no highlighting.

This was alluded to on #14999 (martijnvg, 27 Jan) but seemingly isn't the focus of that issue or any other open one.

**Steps to reproduce**:

```
POST temporary
{
}

POST temporary/bunch/_mapping
{
  "_parent" : {
      "type" : "tree"
   },
  "properties": {
    "banana": {
      "type": "nested"
    }
  }
}

POST temporary/tree/1
{
}

POST temporary/bunch/1?parent=1
{
  "banana": [
    {
      "description": "a tasty banana"
    }
  ]
}

GET temporary/tree/_search
{
  "query": {
    "has_child": {
      "type": "bunch",
      "query": {
        "nested": {
          "path": "banana",
          "query": {
            "match": {
              "banana.description": "tasty"
            }
          }
        }
      }
    }
  },
  "inner_hits": {
    "children": {
      "type": {
        "bunch": {
          "inner_hits": {
            "some_highlight_name": {
              "path": {
                "banana": {
                  "highlight": {
                    "fields": {
                      "banana.description": {}
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

Expected: `inner_hits` to contain a highlighted `description` property like `"a &lt;em&gt;tasty&lt;/em&gt; banana"`

Actual: `hits` object is present but no `highlights` object.
</description><key id="142973745">17283</key><summary>Can't highlight across has_child</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chrisboyle</reporter><labels><label>:Inner Hits</label><label>discuss</label></labels><created>2016-03-23T14:25:15Z</created><updated>2017-04-28T07:58:15Z</updated><resolved>2016-03-29T15:15:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-24T16:33:11Z" id="200914664">@martijnvg can you have a look at this?  Between https://github.com/elastic/elasticsearch/issues/16664 and https://github.com/elastic/elasticsearch/issues/16653 I'm not sure if the syntax is right or not.
</comment><comment author="martijnvg" created="2016-03-24T22:38:16Z" id="201058805">@chrisboyle The reason that there is no highlight object in the response is because no query has been specified inside each inner hit. Unlike inner hits defined in the query, top level inner hits don't know about the inner query that has been used in the main query and therefor you need to redefine this. Like this:

```
curl -XGET "http://localhost:9200/temporary/tree/_search" -d'
{
  "query": {
    "has_child": {
      "type": "bunch",
      "query": {
        "nested": {
          "path": "banana",
          "query": {
            "match": {
              "banana.description": "tasty"
            }
          }
        }
      }
    }
  },
  "inner_hits": {
    "children": {
      "type": {
        "bunch": {
          "query": {
            "nested": {
              "path": "banana",
              "query": {
                "match": {
                  "banana.description": "tasty"
                }
              }
            }
          },
          "inner_hits": {
            "some_highlight_name": {
              "path": {
                "banana": {
                  "query": {
                    "match": {
                      "banana.description": "tasty"
                    }
                  },
                  "highlight": {
                    "fields": {
                      "banana.description": {}
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}'
```

This is confusing. Inner hits inlined with the query don't need this because these inner hit definition know about the context there have been specified in. At the moment multiple level of inner query inner hits don't work, but this will be fixed. Once this has been fixed top level inner hits will be deprecated (in 5.0) and eventually removed. 
</comment><comment author="chrisboyle" created="2016-03-29T15:15:22Z" id="202948103">@martijnvg Thanks, this solves our problem.

Note that it still works if you remove the query at inner_hits/children/type/bunch/query (as opposed to inner_hits/.../inner_hits/.../query).
</comment><comment author="abhishek5678" created="2017-04-28T06:00:11Z" id="297914448">Hello @martijnvg This is not work for me.when i am running this query it's shows like that:
 "type": "parsing_exception",
            "reason": "Unknown key for a START_OBJECT in [inner_hits]

Please help me to resolve this problem and reply me as soon as possible</comment><comment author="martijnvg" created="2017-04-28T07:58:15Z" id="297933907">@abhishek5678 Please ask these questions on the forum instead of an issue that has been closed for more than a year. I'm happy to help you there.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated reverse option from sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17282</link><project id="" key="" /><description>This removes the "reverse" option for sorting as discussed in #17047 - instead define sort order explicitly like shown here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_sort_order

Relates to #17047

Pending: PR to deprecate the option removed above in 2.x
</description><key id="142968588">17282</key><summary>Remove deprecated reverse option from sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T14:06:55Z</created><updated>2016-03-30T09:02:19Z</updated><resolved>2016-03-30T09:02:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-23T14:12:58Z" id="200360798">I found one mention of the 'reverse' parameter related to sort in `./reference/search/request/sort.asciidoc` which should be removed. Maybe also add a not to the migrate.asciidoc, even though the option will be deprecated in 2.x. Other than that, LGTM
</comment><comment author="s1monw" created="2016-03-23T14:39:32Z" id="200373816">can we add a test that makes sure we fail is somebody uses the option?
</comment><comment author="MaineC" created="2016-03-23T14:47:22Z" id="200376234">Thanks for the speedy comments.
- @cbuescher wrt. sort in docs: Will remove the mention, same for adding to migrate.asciidoc
- @s1monw wrt. to adding said test: Makes sense, will do
</comment><comment author="MaineC" created="2016-03-24T15:04:05Z" id="200876071">Updated. Not sure about the change to GeoDistanceSortBuilder - added an explicit check for the token "reverse", without it parsing fell through to [parsing the reference geo point for some field from a geohash string](https://github.com/elastic/elasticsearch/blob/a3fc4c0370671b6d193a6e0887108179f1fc5432/core/src/main/java/org/elasticsearch/search/sort/GeoDistanceSortBuilder.java#L471).

Would like to dig a bit more to confirm this behaviour outside the unit test with a real curl reproduction.
</comment><comment author="MaineC" created="2016-03-29T09:25:22Z" id="202798624">This is what we get when a user includes the reverse sort option after removing its support in GeoDistanceSortBuilder:

```
{
   "error": {
      "root_cause": [
         {
            "type": "parse_exception",
            "reason": "illegal latitude value [269.97802734375] for [GeoDistanceSort]"
         }
      ],    
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "products",
            "node": "MeeEqVKMQF2UvjooWHAGlw",
            "reason": {
               "type": "parse_exception",
               "reason": "illegal latitude value [269.97802734375] for [GeoDistanceSort]"
            }
         }
      ],
      "caused_by": {
         "type": "parse_exception",
         "reason": "illegal latitude value [269.97802734375] for [GeoDistanceSort]"
      }
   },
   "status": 400
}
```

When explicitly forbidding the query option like in the change above, we get something that's more readable IMHO while at the same time forbidding the user to have geo fields named "reverse":

```
{
   "error": {
      "root_cause": [
         {
            "type": "parsing_exception",
            "reason": "Sort option [reverse] no longer supported.",
            "line": 6,
            "col": 20
         }
      ],
      "type": "parsing_exception",
      "reason": "Sort option [reverse] no longer supported.",
      "line": 6,
      "col": 20
   },
   "status": 400
}
```

My personal preference would be to go with the latter option, @cbuescher any opinion?
</comment><comment author="cbuescher" created="2016-03-29T09:52:10Z" id="202810810">@MaineC I see the problem, but I'm on the fence here with treating "reverse" as a special case that cannot be used as a field name for the geo point option. That would require special documentation (all fieldnames allowed except...) and somehow doesn't feel right to me. What if instead to mitigate the problem:
- dissallow setting the field name for the geo points twice and throw an error with the fieldname already set and the different one we are trying to set (that would give errors sth. like `fieldname already set to "my_point" but trying to reset to "reverse") 
- dissallow token value other than VALUE_STRING in the case where we are parsing to geo hash. That would catch the cases where we have `"reverse" : true` in the query.
- Add the fieldname to the error message above (I think that gets triggered in build()), so users get a hint that they are using the wrong field when accidentally specifying `"reverse" : "true"|"false"` 
</comment><comment author="MaineC" created="2016-03-29T11:20:20Z" id="202836900">@cbuescher good points, I like your proposals. Changed the code accordingly. Only gotcha:

&gt; Add the fieldname to the error message above (I think that gets triggered in build()), so users get a 
&gt; hint that they are using the wrong field when accidentally specifying "reverse" : "true"|"false"

While the string "false" evaluates to an invalid coordinate, the string "true" seems to evaluate to a valid one. See here: http://www.movable-type.co.uk/scripts/geohash.html
</comment><comment author="cbuescher" created="2016-03-29T12:09:36Z" id="202859773">Okay, I'd say we have to live with the small gotcha that `"true"` as geohash is valid (nice, Kasachstan ;-)), better than making exceptions on allowed fieldnames only for catching (probably largely unused) deprecated options. Left one last minor nit, otherwise LGTM.
</comment><comment author="MaineC" created="2016-03-29T12:39:57Z" id="202875062">@cbuescher NIT fixed. Merging as soon as tests are done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move dynamic template matching logic to the MatchType enum.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17281</link><project id="" key="" /><description /><key id="142962942">17281</key><summary>Move dynamic template matching logic to the MatchType enum.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T13:45:54Z</created><updated>2016-03-23T13:54:25Z</updated><resolved>2016-03-23T13:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-23T13:49:08Z" id="200351380">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ability to specify arbitrary node attributes with `node.` prefix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17280</link><project id="" key="" /><description>Today the basic node settings like `node.data` and `node.master` can't really be fully validated since we allow to specify custom user attributes on the node level. We have to, in order to support that, add a wildcard setting for `node.*` to let these setting pass validation. Instead we should require a more contraint prefix like `node.attribute.` that defines a namespace that is reserved for user attributes. 
This can be breaking since validation will kick in and provide an error message accordingly.
</description><key id="142954963">17280</key><summary>Remove ability to specify arbitrary node attributes with `node.` prefix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>adoptme</label><label>blocker</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T13:09:23Z</created><updated>2016-03-30T20:42:53Z</updated><resolved>2016-03-30T12:14:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-23T13:14:35Z" id="200339794">+1
</comment><comment author="uboness" created="2016-03-23T15:09:02Z" id="200386407">++ not critical but wondering if `node.metadata` would be a more appropriate namespace for it?
</comment><comment author="s1monw" created="2016-03-24T14:25:13Z" id="200861258">&gt; not critical but wondering if node.metadata would be a more appropriate namespace for it?

I am not sure since we call them `attributes`?
</comment><comment author="clintongormley" created="2016-03-24T18:13:47Z" id="200954984">I prefer `attributes` as it is more concrete than metadata.  That said, personally I'd go for `node.attr.` instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After Volume Increase, large number of &#8220;java.nio.file.FileSystemException&#8221; and &#8220;TranslogException&#8221; Logs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17279</link><project id="" key="" /><description>Here is the URL to my issues at ES discussion: https://discuss.elastic.co/t/indices-created-in-new-node-after-disk-extend/43794

Since we had a disk volume deplete it&#8217;s space the ES went down until we extended it, since then ES has been stopping every day and a large amount of the below logs have been created:

Caused by: java.nio.file.FileSystemException: /var/lib/elasticsearch/elasticsearch/nodes/0/indices
/devicename-2016.02.21/4/index: Too many open files

TranslogException[failed to create new translog file]; nested: FileAlreadyExistsException[/var/lib/elasticsearch/elasticsearch/nodes/0/indices/devicename-2016.03.21/1/translog/translog-17.tlog]
I also noted that since the increase it&#8217;s switch to /node1 from /node0.

I&#8217;m aware of a bug in V2 but we&#8217;re running 2.1.1 right now.
</description><key id="142943145">17279</key><summary>After Volume Increase, large number of &#8220;java.nio.file.FileSystemException&#8221; and &#8220;TranslogException&#8221; Logs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">benlavender</reporter><labels><label>:Translog</label><label>feedback_needed</label></labels><created>2016-03-23T12:04:55Z</created><updated>2016-05-21T11:56:44Z</updated><resolved>2016-05-21T11:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-23T12:24:46Z" id="200327100">I think this is fixed by https://github.com/elastic/elasticsearch/pull/15788 any chance you can upgrade to `2.1.2` also can you provide the full stacktrace?
</comment><comment author="benlavender" created="2016-03-30T16:14:26Z" id="203508522">I upgraded to 2.3.0 via the .DEB package, I still see the below type errors when starting ES:

Caused by: [syslog-2016.03.24][[syslog-2016.03.24][1]] EngineCreationFailureException[failed to create engine]; nested: NoSuchFileException[/var/lib/elasticsearch/elasticsearch/nodes/0/indices/syslog-2016.03.24/1/translog/translog-2.ckp];

Caused by: java.nio.file.FileSystemException: /var/lib/elasticsearch/elasticsearch/nodes/0/indices/syslog-2016.03.29/4/translog/translog-21.tlog: Too many open files

The instance will keep logging this until it peaks it's CPU %
</comment><comment author="clintongormley" created="2016-03-31T10:21:07Z" id="203867840">&gt; Too many open files

You need to increase your file handle limit
</comment><comment author="benlavender" created="2016-04-07T08:42:53Z" id="206762115">Please can someone provide the method of doing this? Thanks
</comment><comment author="clintongormley" created="2016-04-07T10:21:09Z" id="206799824">@benlavender https://www.elastic.co/guide/en/elasticsearch/reference/2.3/setup-configuration.html#file-descriptors
</comment><comment author="s1monw" created="2016-05-21T11:56:43Z" id="220773819">closing no feedback
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove memory section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17278</link><project id="" key="" /><description>This seems to be a leftover of #12049, where some sigar specific stats were removed. The available memory metric was left behind but never set. This section can be removed as it is not useful.

Closes #16756
</description><key id="142935950">17278</key><summary>Remove memory section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>breaking</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T11:36:22Z</created><updated>2016-04-05T10:57:33Z</updated><resolved>2016-03-24T14:59:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-23T11:37:10Z" id="200308078">@tlrx can you have a look please?
</comment><comment author="tlrx" created="2016-03-23T12:25:23Z" id="200327225">I'm afraid it's not a left over but I broke it in #12049. We do have the information in `NodeStats` so I think we should sum it in `ClusterStats` rather than simply remove it, no?
</comment><comment author="javanna" created="2016-03-23T13:24:27Z" id="200342044">yes @tlrx that is an option, I just thought that this info is not useful in cluster stats given that it's broken since 2.0 beta 1 and nobody ever complained about it... I wonder what the purpose is of showing how much memory is available in total throughout all the nodes. You confirm we should make this work instead of removing it?
</comment><comment author="javanna" created="2016-03-23T13:33:47Z" id="200344773">one more thing: there are still cases where `getMem()` returns null I believe, then the total is completely useless as it would only take into account only the output from nodes that provided their value.
</comment><comment author="tlrx" created="2016-03-23T13:41:52Z" id="200348444">&gt; one more thing: there are still cases where getMem() returns null I believe,

Do you know which cases? I think we have `OsProbeTests` that checks the value returned by `getMem()` and I don't remember it failed on the platform we test on.

I like this info because it gives an estimation of the "size" of the cluster but other than that I'm not sure it is very useful. I'm throwing the ball to @pickypg : did you ever use this field?
</comment><comment author="javanna" created="2016-03-23T13:53:28Z" id="200353510">&gt; Do you know which cases? I think we have OsProbeTests that checks the value returned by getMem() and I don't remember it failed on the platform we test on.

Happened on my machine (mac) while manually testing. (I had made the change you are suggesting at first, before going for removing the field completely).
</comment><comment author="pickypg" created="2016-03-23T18:15:40Z" id="200473898">&gt; did you ever use this field?

@tlrx I don't see any value in having the summed amount of memory across the cluster. It's kind of neat to "see it" but I agree that it does not add value to anyone. I can't really make any useful decision based on seeing "300 GB of memory" for a cluster. It also doesn't tell you if the nodes are heterogenous, which is dangerous.

As long as it's in node stats, then that should be what matters. I don't necessarily think that we should introduce this change for existing version though since it's a non-BWC change (even if the value was useless)?
</comment><comment author="javanna" created="2016-03-23T19:07:53Z" id="200499257">&gt; I don't necessarily think that we should introduce this change for existing version though since it's a non-BWC change (even if the value was useless)?

that's debatable, the value is always set to `0` for any 2.x version. Worse than useless to me... I am all for removing it, just in case I didn't make myself clear yet :) I would be curious to hear of people actually using it...
</comment><comment author="pickypg" created="2016-03-23T19:20:35Z" id="200502895">I don't think anyone is using it properly, but if they're doing some kind of mass collection of stats, then this could be swooped up. I wouldn't want to break it over a useless value in a .z release :)
</comment><comment author="javanna" created="2016-03-23T19:24:35Z" id="200504670">That means we need to fix it in 2.x, I can't leave it as-is... but we agreed that this value is useless so we kinda fix it for nothing... I will need to sleep over this :)
</comment><comment author="javanna" created="2016-03-24T14:34:12Z" id="200864226">ok I updated the title, description and labels of this PR. This one is now targeted at 5.0 only and will remove the fields from the response, which is a breaking change. Another PR will follow-up to fix the bug in 2.x.
</comment><comment author="tlrx" created="2016-03-24T14:35:47Z" id="200865069">LGTM
</comment><comment author="pickypg" created="2016-03-24T14:42:35Z" id="200867415">LGTM. Sorry for the extra hassle!
</comment><comment author="javanna" created="2016-03-24T14:44:53Z" id="200868033">no worries! I also updated the migrate guide, which I had forgotten about at first ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version: Set version to 5.0.0-alpha1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17277</link><project id="" key="" /><description>Change version, required a minor fix in the RPM building.
In case of a alpha/beta version, the release will contain alpha/beta
as the RPM version cannot contains dashes/tildes.

Also minor fix in VersionTests to make sure the test passes, didnt cater for alpha releases.
</description><key id="142933395">17277</key><summary>Version: Set version to 5.0.0-alpha1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T11:24:13Z</created><updated>2016-03-24T17:50:02Z</updated><resolved>2016-03-24T07:37:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-23T12:25:51Z" id="200327311">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce isolated mode for all plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17276</link><project id="" key="" /><description>This removes the support of the plugin property: `isolated`. Each plugin will always have its own classloader. If the option is present in the property file it is simply ignored. 
</description><key id="142915775">17276</key><summary>Enforce isolated mode for all plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T10:03:38Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-24T08:18:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-23T11:04:12Z" id="200297666">It looks good to me.
</comment><comment author="s1monw" created="2016-03-23T14:50:32Z" id="200377562">LGTM - trash it... PR of the day! 
</comment><comment author="dakrone" created="2016-03-23T15:09:38Z" id="200386757">@jimferenczi can you mark this as breaking and put a note in the 5.0 migration guide also?
</comment><comment author="rjernst" created="2016-03-23T15:16:17Z" id="200389504">LGTM too!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Released Logstash and Beats templates to improve BWC testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17275</link><project id="" key="" /><description>These templates are a good user of the template API and we should make sure we support them at least across major version. It won't be perfect in terms of testing but it can be a simple unit/integ test that just makes sure the dynamic mapping works
</description><key id="142911432">17275</key><summary>Use Released Logstash and Beats templates to improve BWC testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Index Templates</label><label>adoptme</label><label>enhancement</label><label>test</label><label>v6.0.0-alpha1</label></labels><created>2016-03-23T09:43:32Z</created><updated>2016-10-18T15:49:10Z</updated><resolved>2016-10-18T15:49:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tsg" created="2016-03-23T09:54:49Z" id="200274163">For your convenience, here  are the links to the Beats templates:

Topbeat: https://github.com/elastic/beats/blob/master/topbeat/etc/topbeat.template.json
Packetbeat: https://github.com/elastic/beats/blob/master/packetbeat/etc/packetbeat.template.json
Filebeat: https://github.com/elastic/beats/blob/master/filebeat/etc/filebeat.template.json
Winlogbeat: https://github.com/elastic/beats/blob/master/winlogbeat/etc/winlogbeat.template.json

I can also provide sample documents for each if it helps.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check that S3 setting `buffer_size` is always lower than `chunk_size`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17274</link><project id="" key="" /><description>We can be better at checking `buffer_size` and `chunk_size` for S3 repositories.
For example, we know that:
- `buffer_size` should be more than `5mb`
- `chunk_size` should be no more than `5tb`
- `buffer_size` should be lower than `chunk_size`

Otherwise for the later, setting `buffer_size` is useless. For the record:

`chunk_size` is a Snapshot setting whatever the implementation is.
`buffer_size` is an S3 implementation setting.

Let say that you are snapshotting a 500mb file. If you set `chunk_size` to `200mb`, then Snapshot service will call S3 repository to snapshot 3 files with the following sizes:
- `200mb`
- `200mb`
- `100mb`

If you set `buffer_size` to `100mb` (AWS maximum size recommendation), the first file of `200mb` will be uploaded on S3 using the multipart feature in 2 chunks and the workflow is basically the following:
- create the multipart request and get back an `id` from AWS S3 platform
- upload part1: `100mb`
- upload part2: `100mb`
- "commit" the full upload using the `id`.

This PR also changes default values for `buffer_size` which is now `100mb` instead of `5mb` and for `chunk_size` which is now `1gb` instead of `100mb`.

BTW this PR adds new Setting methods which checks min and max values.

Closes #17244.
</description><key id="142911360">17274</key><summary>Check that S3 setting `buffer_size` is always lower than `chunk_size`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>:Settings</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T09:43:10Z</created><updated>2016-03-24T17:05:45Z</updated><resolved>2016-03-23T17:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-23T09:44:14Z" id="200271340">@imotov I'm assigning this to you. If you can review it please? Thanks!
</comment><comment author="imotov" created="2016-03-23T15:43:07Z" id="200400990">Left a minor comment. Otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade string fields to text/keyword also if `ignore_above` is set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17273</link><project id="" key="" /><description>Since this parameter is used in the logstash default template, it would be nice
to handle it.
</description><key id="142906132">17273</key><summary>Upgrade string fields to text/keyword also if `ignore_above` is set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T09:18:37Z</created><updated>2016-03-23T09:32:14Z</updated><resolved>2016-03-23T09:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-23T09:26:29Z" id="200266287">left 2 comments LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update template-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17272</link><project id="" key="" /><description /><key id="142905578">17272</key><summary>Update template-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pengqiuyuan</reporter><labels><label>docs</label></labels><created>2016-03-23T09:15:51Z</created><updated>2016-03-24T15:50:02Z</updated><resolved>2016-03-24T15:49:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-23T10:22:02Z" id="200284942">Thanks @pengqiuyuan looks good to me, may I ask you to sign our [CLA](https://www.elastic.co/contributor-agreement/) so we can get your fix in?
</comment><comment author="pengqiuyuan" created="2016-03-23T10:33:31Z" id="200287811">Yes, I've just registered the CLA &#128591;
</comment><comment author="clintongormley" created="2016-03-24T15:50:02Z" id="200896891">thanks @pengqiuyuan - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Other bucket now shows if enabled on empty buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17271</link><project id="" key="" /><description>Previous to this commit empty buckets (with a doc count of zero) would not show the 'other' bucket in the filters aggregation. Now the buildEmptyBucket() method in FiltersAggregator checks to see if the other bucket is enabled when building an empty aggregation and adds it if it is enabled.

Closes #16546
</description><key id="142905480">17271</key><summary>Other bucket now shows if enabled on empty buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label></labels><created>2016-03-23T09:15:10Z</created><updated>2016-03-24T15:46:50Z</updated><resolved>2016-03-23T09:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-23T09:18:34Z" id="200263627">Closing in favour of #17264
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unit testable IndicesClusterStateService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17270</link><project id="" key="" /><description>This PR makes IndicesClusterState unit-testable. Testability of ICSS is achieved by introducing interfaces for IndicesService, IndexService and IndexShard. These interfaces extract all relevant methods used by ICSS (which do not deal directly with store). This gives the possibility to easily mock all the store behavior away in the tests (and cuts down on dependencies). It also helps to better understand what the actual interface is between the different components.
</description><key id="142899222">17270</key><summary>Unit testable IndicesClusterStateService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Cluster</label><label>non-issue</label><label>v5.0.0-alpha4</label></labels><created>2016-03-23T08:47:16Z</created><updated>2016-06-10T10:49:12Z</updated><resolved>2016-06-10T10:47:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-06-06T08:32:06Z" id="223898269">@bleskes @s1monw I've updated the PR according to our discussions.
</comment><comment author="s1monw" created="2016-06-08T11:59:48Z" id="224568225">@ywelsch this looks AWESOME! I really like it. I think we should get it in ASAP! I left some bikeshedding while I looked at the structure, I wasn't sure if you wanna go over it again before we do a more detailed review! IMO the tests by itself justify pushing this soon :)
</comment><comment author="s1monw" created="2016-06-09T09:01:40Z" id="224838832">I did an deeper review, thanks again for this @ywelsch 
</comment><comment author="ywelsch" created="2016-06-09T16:18:54Z" id="224947142">@s1monw @bleskes I've updated the PR with your suggested changes. Please have another look.
</comment><comment author="s1monw" created="2016-06-09T16:28:36Z" id="224949918">left one comment LGTM otherwise
</comment><comment author="bleskes" created="2016-06-10T06:42:10Z" id="225105506">LGTM2 .  Awesome work @ywelsch
</comment><comment author="ywelsch" created="2016-06-10T10:49:12Z" id="225152147">Thanks for the reviews, @bleskes and @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Installation of Ingest Attachment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17269</link><project id="" key="" /><description>I am new to the EKL tools and have installed Elasticsearch 2.2, but could not find ways to install the plugin ingest attachment processor as a substitute of mapper attachment.

Would anyone know any ways to do that?
</description><key id="142892993">17269</key><summary>Installation of Ingest Attachment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kan1207</reporter><labels /><created>2016-03-23T08:15:59Z</created><updated>2016-03-23T09:13:01Z</updated><resolved>2016-03-23T08:46:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-03-23T08:46:17Z" id="200251573">ingest is a feature of 5.0 only and not part of the 2.x releases... we intend to release a first alpha soon, so you can test it either then, or - if you cant wait - you could just build an elasticsearch distribution from the source itself.
</comment><comment author="kan1207" created="2016-03-23T09:12:47Z" id="200261976">So a possible solution will be go back to an older version of ES and use mapper-attachment plugin?
</comment><comment author="kan1207" created="2016-03-23T09:13:01Z" id="200262081">Thanks for the advice
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>removed automatic index creation from delete api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17268</link><project id="" key="" /><description>The delete Api raises "index_not_found_exception" if index is not created before. The documentation mentions "The delete operation automatically creates an index if it has not been created before" which is not the actual behaviour.
</description><key id="142883230">17268</key><summary>removed automatic index creation from delete api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raianand</reporter><labels /><created>2016-03-23T07:15:28Z</created><updated>2016-03-23T08:05:29Z</updated><resolved>2016-03-23T08:05:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-23T07:46:59Z" id="200233000">I just tried and it does for me. Can you please share the commands you used to reach your conclusion?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need help in scaling up my elasticsearch-logstash-graylog setup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17267</link><project id="" key="" /><description>Hi All,

My task is to have a centralized log analysis tool that can accommodate 500 GB of log files; and can search for anything within seconds from it. So, had a basic setup of Graylog with elasticsearch and logstash. 
To start, I tried reading one log file using logstash and stored it in elasticsearch. And am able to visualize them in the Graylog web interface.

Now i need to scale up the setup so that I can index 500 GB of data in elasticsearch. 
Is it possible to read these many files through logstash and index to elasticsearch? 
Will mongodb helpful in this scenario?
How should I scale up my architecture in terms of: elasticsearch nodes, RAM, CPU Power, ES-heap size and many more... so that I can meet the requirements of the task.

Currently have 4 GB RAM in VM
CentOS 7
Elasticsearch: v 1.7.5
Logstash: v 2.2.2
Graylog: v 1.3

Kindly help me with my questions. I am very new to this environment.
Thanks.
</description><key id="142876808">17267</key><summary>Need help in scaling up my elasticsearch-logstash-graylog setup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mshar039</reporter><labels /><created>2016-03-23T06:48:04Z</created><updated>2016-03-23T07:01:32Z</updated><resolved>2016-03-23T07:01:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-23T07:01:32Z" id="200214247">We can help on http://discuss.elastic.co.

We keep this place for confirmed issues only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[feature request]es config parameters are so mess. better clean them up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17266</link><project id="" key="" /><description>1. any parameters related to cluster level shoud have prefix cluster., like cluster.action.auto_create_index, even include cluster name
2. any parameters related to cluster level better config through API rather than config file
3. any parameters related to indices should make them cluster related and therefore, start with cluster., like cluster.indices.shards
</description><key id="142854485">17266</key><summary>[feature request]es config parameters are so mess. better clean them up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Settings</label><label>discuss</label></labels><created>2016-03-23T04:04:44Z</created><updated>2016-03-25T15:44:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Adds tombstones to cluster state for index deletions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17265</link><project id="" key="" /><description>Previously, we would determine index deletes in the cluster state by
comparing the index metadatas between the current cluster state and the
previous cluster state and decipher which ones were missing (the missing
ones are deleted indices).  This led to a situation where a node that went 
offline and rejoined the cluster could potentially cause dangling indices to
be imported which should have been deleted, because when a node rejoins,
its previous cluster state does not contain reliable state.

This commit introduces the notion of index tombstones in the cluster
state, where we are explicit about which indices have been deleted.
In the case where the previous cluster state is not useful for index metadata
comparisons, a node now determines which indices are to be deleted based
on these tombstones in the cluster state.  There is also functionality to 
purge the tombstones after exceeding a certain amount. 

Closes #16358
Closes #17435 
</description><key id="142850747">17265</key><summary>Adds tombstones to cluster state for index deletions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2016-03-23T03:46:12Z</created><updated>2016-05-02T14:02:05Z</updated><resolved>2016-04-25T20:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-23T03:48:34Z" id="200154577">@bleskes @ywelsch Its a WIP, still have some functionality to implement and tests to write, but just in case you want to give a quick glance beforehand to see if its on the right track.
</comment><comment author="bleskes" created="2016-03-23T08:51:48Z" id="200253027">Hi @abeyad . Thanks for picking it up. I think this can be done in a single tombstone class which is basically a queue of deleted Index object. new entries are always added at the end. Trimming is always done at the beginning. Every time you add an entry the class automatically captures the current time (both in millis and in nanos) and add it to an internal key class.  Internally we can assert semantics like "every index appears once". That class can also have methods to do trimming (both on time and size).

Does it make sense?
</comment><comment author="abeyad" created="2016-03-23T13:49:17Z" id="200351464">@bleskes ++ on queue of deleted objects for ease of insertion and trimming from the front.  The map made it easier to assert "every index appears once" semantics, but I can separate the internal representation from what is serialized.

&gt; Every time you add an entry the class automatically captures the current time (both in millis and in nanos) and add it to an internal key class.

I'm not clear on this - I figured we would need the current time on each entry (hence creating the `IndexTombstone` class to represent each entry).  I'm not sure exactly what you mean by the adding current time to an internal key class.
</comment><comment author="s1monw" created="2016-03-23T14:11:13Z" id="200360300">&gt; Hi @abeyad . Thanks for picking it up. I think this can be done in a single tombstone class which is basically a queue of deleted Index object. new entries are always added at the end. Trimming is always done at the beginning. Every time you add an entry the class automatically captures the current time (both in millis and in nanos) and add it to an internal key class. Internally we can assert semantics like "every index appears once". That class can also have methods to do trimming (both on time and size).

I think the current design is OK. It's really a value object and doesn't contain logic. It has the serializaiton and deserialization in there which is good. It can also implement comparable which is then taking the time into account. I also think we shouldn't mix datastructure that is on the clusterstate and representation. 

Regarding a queue, I think we should just stick with a simple list we can sort once it's modified and ensure in the Clusterstate ctor that is in-fact sorted but keep it simple.

I also think we might even go without pruning in thirst PR and do the pruning as a followup?  It can block a lot of good progress. There are a lot of open questions related to this and for how long we keep there tombstones, I think we should try to keep them for as long as possible but the question of how long is very hard to answer.
</comment><comment author="abeyad" created="2016-03-28T16:27:51Z" id="202472511">@bleskes @s1monw This PR is ready for the next round of review.  I left two specific comments that need special attention, please, as I was unsure of the proper route to take:
https://github.com/elastic/elasticsearch/pull/17265/files#r57590545
https://github.com/elastic/elasticsearch/pull/17265/files#r57592054
</comment><comment author="abeyad" created="2016-03-29T22:02:10Z" id="203130988">@bleskes this PR is ready for code review
</comment><comment author="bleskes" created="2016-03-30T13:29:23Z" id="203432912">Left a bunch comments - this is getting in shape...
</comment><comment author="abeyad" created="2016-03-31T19:15:15Z" id="204085684">@bleskes Updated the PR with all the suggestions you made, except for the couple places where I had outstanding questions about how to proceed.  
</comment><comment author="abeyad" created="2016-04-01T04:19:16Z" id="204241879">@bleskes The code has been updated reflecting our discussions from earlier this afternoon.
</comment><comment author="abeyad" created="2016-04-04T06:39:24Z" id="205158621">@bleskes PR has been updated to use `MetaData.Custom` and I've incorporated the other suggestions as well.
</comment><comment author="abeyad" created="2016-04-05T16:32:38Z" id="205885496">@bleskes I made the changes you suggested, except for the couple I still had a question on
</comment><comment author="bleskes" created="2016-04-06T10:12:10Z" id="206286641">Thx @abeyad . I think this is getting really close. Because of the tricky nature of the PR I would love a second pair of eyes from @jasontedor . Als it would be great if @dakrone can validate the IndicesService changes from the shared file system point of view.

Last - I might have missed it, but do we have an integ test that make sure that indices that were deleted while a node was offline do return once the node is back?
</comment><comment author="abeyad" created="2016-04-07T04:11:24Z" id="206689651">@bleskes I made all the suggested changes and added more tests (including the ones you mentioned).  
</comment><comment author="abeyad" created="2016-04-07T23:28:52Z" id="207136317">@bleskes All comments from this morning have been addressed in the latest commit.
</comment><comment author="bleskes" created="2016-04-08T15:38:18Z" id="207483548">LGTM. Left some minor request for extending the test. Would still love it if @jasontedor and @dakrone look at this as well
</comment><comment author="jasontedor" created="2016-04-08T15:44:24Z" id="207487397">&gt; LGTM. Left some minor request for extending the test. Would still love it if @jasontedor and @dakrone look at this as well

I will review on Monday.
</comment><comment author="abeyad" created="2016-04-14T19:54:19Z" id="210121057">@bleskes @dakrone @jasontedor I've updated this PR with the latest `IndicesService` changes from master.  It is ready for review.  In particular, one new change is that, in #17638 we removed the `closed` parameter from deletedUnassignedIndex and all the deleteIndexStore methods, because we can know if an index is closed from the `IndexSettings` that is already passed in.  The only place this does not work is if a node was offline when the delete happened, and when it comes back online, it sees tombstones in the cluster state that force it to delete the index, so we load the index metadata from disk in order to delete it&#8230; but that index metadata on disk indicates the index is `OPEN`, not `CLOSED`.  So the current checks in `canDeleteIndexContents` were not sufficient to allow deletion in this case.  Instead of passing around a new variable through all the `deleteIndexStore` method, I accomplished this through a `BiFunction` predicate:

https://github.com/elastic/elasticsearch/pull/17265/files#diff-97e5abcaadbacda4084c902214cd3654R697

I'd appreciate your thoughts and any other feedback.
</comment><comment author="dakrone" created="2016-04-21T17:33:06Z" id="213029961">@abeyad I left one more comment about my view of the purging, but other than that I think this looks good to me.
</comment><comment author="abeyad" created="2016-04-21T19:21:13Z" id="213078124">@dakrone pushed a commit that leaves the responsibility of purging to the graveyard builder.
</comment><comment author="abeyad" created="2016-04-21T20:18:17Z" id="213098505">@dakrone Good suggestions - I just pushed a new commit with those.
</comment><comment author="dakrone" created="2016-04-21T20:20:27Z" id="213099093">Okay, LGTM, not sure if anyone else wants to comment though :)
</comment><comment author="abeyad" created="2016-04-21T20:24:08Z" id="213100179">I'll leave it to @jasontedor discretion if he would like to give it a review or if he's fine with the review already done.  Boaz said it was good from his end.
</comment><comment author="jasontedor" created="2016-04-25T16:27:12Z" id="214429641">@abeyad I think that the high-level concepts have been ironed out, but I left some feedback on coding details.
</comment><comment author="jasontedor" created="2016-04-25T17:09:55Z" id="214445437">LGTM. Great work @abeyad.
</comment><comment author="abeyad" created="2016-04-25T17:12:00Z" id="214445983">@jasontedor thank you for all the valuable feedback!  
</comment><comment author="clintongormley" created="2016-05-02T14:02:05Z" id="216243851">@abeyad looks like these settings still need to be documented?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting 'other' bucket on empty aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17264</link><project id="" key="" /><description>Closes #16546 
</description><key id="142819125">17264</key><summary>Setting 'other' bucket on empty aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">pjo256</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.3.0</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-23T00:26:27Z</created><updated>2016-03-23T09:26:16Z</updated><resolved>2016-03-23T09:19:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-23T09:20:02Z" id="200264199">@pjo256 LGTM, thanks for contributing :smile: 
</comment><comment author="colings86" created="2016-03-23T09:26:16Z" id="200266203">Also backported to 2.x and 2.3 branches
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add automatic type conversion support to ConvertProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17263</link><project id="" key="" /><description>closes: https://github.com/elastic/elasticsearch/issues/17139

todos:
- [x] finalize `target_field` behavior
- [x] update docs
</description><key id="142813228">17263</key><summary>add automatic type conversion support to ConvertProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T23:47:48Z</created><updated>2016-03-30T18:56:51Z</updated><resolved>2016-03-29T14:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-03-29T00:37:30Z" id="202646222">@martijnvg updated with `target_field` and docs
</comment><comment author="martijnvg" created="2016-03-29T08:18:32Z" id="202774096">left one minor comment, LGTM otherwise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make reindex throttling dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17262</link><project id="" key="" /><description>This creates a rest end point that lets the user change the throttle of reindex. It takes care to reschedule the task if the user tries to speed up the request. This is important so that users undo throttle values that make reindex sleep forever. We also listen for cancelation and wake up if we are sleeping so that cancelled reindex requests that will sleep for a long time die quickly.

Finally, this adds a field to the task status that is "for how much longer will this request sleep?" I needed it debugging some stuff and see no reason to remove it.
</description><key id="142813107">17262</key><summary>Make reindex throttling dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T23:47:01Z</created><updated>2016-03-30T20:46:19Z</updated><resolved>2016-03-30T20:46:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-22T23:49:00Z" id="200082823">This totally needs docs. Also, I'm not sure if this is a good or bad REST location:

```
        controller.registerHandler(POST, "/_update_by_query/{taskId}/_rethrottle", this);
        controller.registerHandler(POST, "/_reindex/{taskId}/_rethrottle", this);
```

@imotov I think maybe this is a good one for you to review? Or @dakrone? I'm not sure. A tiny bit of task management stuff, mostly reindex stuff.
</comment><comment author="nik9000" created="2016-03-24T13:28:20Z" id="200833569">@dakrone can you have a look at this one?
</comment><comment author="clintongormley" created="2016-03-24T18:04:49Z" id="200952317">Hmmm wondering if this should be more generic eg some way of sending messages to tasks via the task management API?
</comment><comment author="nik9000" created="2016-03-24T19:46:28Z" id="200988677">&gt; Hmmm wondering if this should be more generic eg some way of sending messages to tasks via the task management API?

The task management bit of the implementation is basically just that. But at the API level it isn't generic.
</comment><comment author="nik9000" created="2016-03-30T13:35:27Z" id="203436075">Thanks for the review @dakrone ! I pushed another commit that addresses your points - except for the one about the default value which I left a question on inline.
</comment><comment author="dakrone" created="2016-03-30T18:18:30Z" id="203563316">I left a few more comments (minor ones), but otherwise LGTM, thanks @nik9000 
</comment><comment author="nik9000" created="2016-03-30T18:35:57Z" id="203570680">&gt; I left a few more comments (minor ones), but otherwise LGTM, thanks @nik9000

Thank you! I've pushed some more javadoc in response to your comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minor typos in the comments.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17261</link><project id="" key="" /><description /><key id="142803159">17261</key><summary>Minor typos in the comments.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tiegz</reporter><labels><label>Awaiting CLA</label><label>non-issue</label></labels><created>2016-03-22T22:46:39Z</created><updated>2016-05-23T20:24:03Z</updated><resolved>2016-05-23T20:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-23T17:56:10Z" id="200466339">Hi @tiegz can I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?
</comment><comment author="dakrone" created="2016-05-23T20:24:03Z" id="221085125">Closing as no feedback in the last two months
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle regex parsing errors in Gsub and Grok Processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17260</link><project id="" key="" /><description>Currently, both Gsub and Grok parse regex strings during
Pipeline creation. Thrown parsing exceptions were leaking out, this
commit wraps those exceptions in ElasticsearchParseExceptions.
## Gsub Example

```
{
  "pipeline": {
    "processors": [
      {
        "gsub": {
          "tag": "processor_1",
          "field": "message",
          "pattern": "[",
          "replacement": ""
        }
      }
    ]
  }
}
```

expected error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "parse_exception",
            "reason": "[pattern] Invalid regex pattern. Unclosed character class near index 0\n[\n^",
            "header": {
               "processor_type": "gsub",
               "processor_tag": "processor_1",
               "property_name": "pattern"
            }
         }
      ],
      "type": "parse_exception",
      "reason": "[pattern] Invalid regex pattern. Unclosed character class near index 0\n[\n^",
      "header": {
         "processor_type": "gsub",
         "processor_tag": "processor_1",
         "property_name": "pattern"
      }
   },
   "status": 400
}
```
## Grok Example

```
{
  "pipeline": {
    "processors": [
      {
        "grok": {
          "tag": "processor_1",
          "field": "message",
          "pattern": "["
        }
      }
    ]
  }
}
```

and expected error:

```
{
   "error": {
      "root_cause": [
         {
            "type": "parse_exception",
            "reason": "[pattern] Invalid regex pattern. premature end of char-class",
            "header": {
               "processor_type": "grok",
               "processor_tag": "processor_1",
               "property_name": "pattern"
            }
         }
      ],
      "type": "parse_exception",
      "reason": "[pattern] Invalid regex pattern. premature end of char-class",
      "header": {
         "processor_type": "grok",
         "processor_tag": "processor_1",
         "property_name": "pattern"
      }
   },
   "status": 400
}
```
</description><key id="142787532">17260</key><summary>Handle regex parsing errors in Gsub and Grok Processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T21:34:19Z</created><updated>2016-03-29T15:12:36Z</updated><resolved>2016-03-29T15:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-29T15:11:03Z" id="202946638">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Adding documentation for the Painless scripting module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17259</link><project id="" key="" /><description>This is a lightly-edited version of the initial docs @jdconrad wrote. I also tweaked the scripting module doc to highlight Painless. 
</description><key id="142779214">17259</key><summary>Docs: Adding documentation for the Painless scripting module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">debadair</reporter><labels><label>:Plugin Lang Painless</label><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T21:02:27Z</created><updated>2016-03-23T20:53:55Z</updated><resolved>2016-03-23T20:53:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2016-03-22T21:20:38Z" id="200035187">LGTM.  Found one typo :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Move yaml test requiring yaml, add skip:yaml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17258</link><project id="" key="" /><description>Clients don't ship with yaml (de)serializer by default so this test must be optionally skipped
</description><key id="142762605">17258</key><summary>[TEST] Move yaml test requiring yaml, add skip:yaml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T20:00:32Z</created><updated>2016-03-23T13:51:01Z</updated><resolved>2016-03-23T13:51:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-23T10:16:40Z" id="200282722">Looks good, but we also need to add that we support this feature as part of core REST tests. It is just a matter of adding the name of the new feature to the `SUPPORTED` list in `Features` class to make sure that we run the test. I can take care of this as a followup as well.
</comment><comment author="HonzaKral" created="2016-03-23T12:11:17Z" id="200324221">Thanks @javanna, I added the feature to the class
</comment><comment author="javanna" created="2016-03-23T13:45:38Z" id="200349787">thank you @HonzaKral LGTM and tests are green
</comment><comment author="HonzaKral" created="2016-03-23T13:51:00Z" id="200352539">Merged as b139f4e0bf7566932746126beb06423a5b539b93
Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort on single field as a list produces parse error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17257</link><project id="" key="" /><description>**Elasticsearch version**: `5.0.0` (3ed4ff0)

**JVM version**: OpenJDK 64-Bit Server VM (build 25.74-b02, mixed mode)

**OS version**: Linux 4.1.20-1-lts

**Description of the problem including expected versus actual behavior**:  When sorting on a single field and passing it in a list you get: `Failed to derive xcontent` error

**Steps to reproduce**:
1. `curl -X PUT localhost:9200/i/t/42 -d '{"number": 123}`'
2. `curl localhost:9200/_search -d '{"sort": ["number"]}'`
</description><key id="142760010">17257</key><summary>Sort on single field as a list produces parse error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:Search</label><label>:Search Refactoring</label><label>bug</label></labels><created>2016-03-22T19:51:53Z</created><updated>2016-03-23T15:25:03Z</updated><resolved>2016-03-23T15:25:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-23T14:31:20Z" id="200368602">This is due to a bug in the work-around for sorting which was put in so we could parse the search request on the coordinating node but defer things that had not yet been refactored (e.g. sort) until we got to the shard. The parsing code stores each element of the sort in a list in the request on the coordinating node and then tries to read each element as JSON on the shard. Which doesn't work if the element is a string instead of JSON. Note that the workaround is only present in 5.0 so this is the only version affected by this bug

The sort refactoring is almost complete (see #17205) so I this can be closed when that PR is merged. @cbuescher is adding a test to that PR to test this problem and ensure it works in the refactored sort code.
</comment><comment author="cbuescher" created="2016-03-23T15:09:00Z" id="200386376">Added test for parsing this kind of syntax (e319985) and just tried this manually on #17205, this should be fixed once that PR is in.
</comment><comment author="HonzaKral" created="2016-03-23T15:12:56Z" id="200388229">Thanks @cbuescher!

In the mean time, if anybody finds this, I added a work around to my tests - just do `{"sort": "number"}` instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Move yaml test requiring header, add skip:headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17256</link><project id="" key="" /><description>all clients don't have this functionality so it should be marked with `skip` just as `get/50_with_headers.yaml` is
</description><key id="142749048">17256</key><summary>[TEST] Move yaml test requiring header, add skip:headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T19:10:08Z</created><updated>2016-03-22T19:54:37Z</updated><resolved>2016-03-22T19:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-22T19:44:35Z" id="199983350">LGTM
</comment><comment author="HonzaKral" created="2016-03-22T19:54:37Z" id="199989421">Merged as ca4b866
Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Link to named queries docs from bool query page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17255</link><project id="" key="" /><description>The named queries feature only makes sense with bool queries, but was not cross-referenced from the bool query documentation page.
</description><key id="142748684">17255</key><summary>Link to named queries docs from bool query page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simonw</reporter><labels><label>docs</label><label>v2.2.2</label><label>v2.3.0</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T19:08:49Z</created><updated>2016-03-23T15:54:26Z</updated><resolved>2016-03-23T15:51:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-23T15:52:26Z" id="200406586">Thanks for this @simonw ! I've merged to master and I'll backport to 2.2, 2.3, and 2.x branches.
</comment><comment author="nik9000" created="2016-03-23T15:54:26Z" id="200407656">2.2: 8fea827b29b97b2e5869e1029e94777a038b8c33
2.3: 40b371443ddc25026b40dce708bca3488e281ef3
2.x: 0047e22ec6cffcd03dfa6efccf4ecba735a9af2a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade dynamic templates that use a dynamic type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17254</link><project id="" key="" /><description>Now that string has been splitted into text and keyword, we use text as a
dynamic type when encountering string fields in a json document. However
this does not play well with existing templates that look like

```
{
  "mapping": {
    "index": "not_analyzed",
    "type": "{dynamic_type}"
  },
  "match": "*"
}
```

Since we want existing templates to keep working as much as possible in 5.0,
this commit adds a hack to dynamic templates so that elasticsearch will create
a keyword field if the `index` property is set and is either `no` or
`not_analyzed`, similarly to what was done in #16991.

While this will make upgrades easier, we still need to figure out a way to
allow users to create keyword fields when using dynamic types.
</description><key id="142743318">17254</key><summary>Upgrade dynamic templates that use a dynamic type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T18:47:44Z</created><updated>2016-03-23T09:07:54Z</updated><resolved>2016-03-23T09:07:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-22T20:46:04Z" id="200017371">That example seems to be something that would be broken in general? it would only work for string fields right? How long would we support this "hack"? I'm just weary of adding such a hack when there is nothing pushing the user to upgrade their templates.
</comment><comment author="jpountz" created="2016-03-22T22:52:08Z" id="200068128">&gt; That example seems to be something that would be broken in general? it would only work for string fields right?

Actually it works for all fields: for instance in 2.x you can configure numbers with either index=analyzed or not_analyzed and this does the same thing.

&gt; I'm just weary of adding such a hack when there is nothing pushing the user to upgrade their templates.

I think the benefit is that it gives more time for people to upgrade: instead of having to change all templates at the same time as they upgrade to 5.0, users have until 6.0 to do it. I will make sure to remove this and what was done in #16991 as soon as the master branch becomes a 6.0 snapshot.
</comment><comment author="rjernst" created="2016-03-22T23:38:59Z" id="200080688">Thanks, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] fix incorrect indent in ingest/70_bulk.yaml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17253</link><project id="" key="" /><description>Otherwise `do` and `cluster.state` are part of the same hash (json equivalent `{"do": null, "cluster.state": {}}`) which is invalid.
</description><key id="142743054">17253</key><summary>[TEST] fix incorrect indent in ingest/70_bulk.yaml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T18:46:23Z</created><updated>2016-03-22T19:54:20Z</updated><resolved>2016-03-22T19:54:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-22T19:44:07Z" id="199983212">LGTM
</comment><comment author="HonzaKral" created="2016-03-22T19:54:20Z" id="199989293">merged as f8e84f0
Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: ability to tell which "should" clauses in a bool query matched</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17251</link><project id="" key="" /><description>**Describe the feature**:

When building complex bool queries, it can be important to know which ones of the "should" clauses in the query caused a match.

Example use-case: building a simple recommendation feature, which recommends documents to users based on different criteria e.g. "matches a tag you are subscribed to" or "one of your friends has saved this document" or "similar to documents you have saved in the past" or "geolocation in the document matches your polygon-saved-search".

This feature can be constructed using a bool query, with each of the criteria provided as a separate should clause. This allows efficient retrieval and pagination. There's just one catch: if we want to tell the user "returned this document because your friend saved it and it's in your polygon-saved-search" we need to do a bunch of additional processing outside of Elasticsearch to figure out which of the clauses matched.

As far as I can tell, there is no way to get Elasticsearch to indicate which of the terms in a bool query matched the returned documents. This would be an extremely useful additional feature.

Here's one suggestion for how the syntax might work:

```
{
    "bool": {
        "must": {
            "term": {
                "user": "kimchy"
            }
        },
        "should": [
            {
                "term": {
                    "tag": "wow"
                },
                "label": "wow"
            },
            {
                "term": {
                    "tag": "elasticsearch"
                },
                "label": "elasticsearch"
            },
            {
                "range": {
                    "age": {
                        "from": 10,
                        "to": 20
                    }
                },
                "label": "agerange"
            }
        ],
        "minimum_should_match": 1,
    }
}
```

Then the hits returned could look something like this:

```
{
    "hits": [
        {
            "_index": "my-index",
            "_type": "doc",
            "_id": "9314",
            "_score": null,
            "_matched_labels": ["agerange", "wow"]
        }
    ]
}
```
</description><key id="142736736">17251</key><summary>Feature request: ability to tell which "should" clauses in a bool query matched</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">simonw</reporter><labels /><created>2016-03-22T18:20:59Z</created><updated>2016-03-23T14:57:18Z</updated><resolved>2016-03-22T18:57:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2016-03-22T18:29:00Z" id="199952228">How about [Named Queries](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search-request-named-queries-and-filters.html)?
</comment><comment author="simonw" created="2016-03-22T18:57:29Z" id="199962971">Fantastic! New feature request: link to that page from the docs for the bool query. I must have spent nearly an hour searching for a solution to this without stumbling across that page.
</comment><comment author="simonw" created="2016-03-22T19:08:59Z" id="199967879">https://github.com/elastic/elasticsearch/pull/17255 - pull request to add a link to the named queries documentation from the bool query page.
</comment><comment author="JnBrymn" created="2016-03-22T20:58:47Z" id="200022549">@mattweber (long time no see) do you know of any way to get the score of the match rather than just whether or not the query matched? 
</comment><comment author="mattweber" created="2016-03-23T14:57:18Z" id="200381812">@JnBrymn Hello!  Unfortunately, I don't.  #17116 might help, but I think you already have seen it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Use -E for system properties like Bootstrap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17250</link><project id="" key="" /><description>We should take the work done in #17088 and apply it to the PluginManager for consistency.

/cc @jasontedor 
</description><key id="142728416">17250</key><summary>PluginManager: Use -E for system properties like Bootstrap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Plugins</label><label>breaking</label><label>enhancement</label></labels><created>2016-03-22T17:53:09Z</created><updated>2016-03-22T22:04:07Z</updated><resolved>2016-03-22T22:04:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-22T20:40:56Z" id="200013382">It will be a little difficult, given how the plugin cli is structured. The environment is created in the main, before we've tried parsing any command line parameters. I'm not saying it's not possible, I'm just saying it will require some refactoring, it is not a couple line change.
</comment><comment author="jasontedor" created="2016-03-22T20:46:45Z" id="200017562">&gt; Use -E for system properties like Bootstrap

@pickypg Do you really mean `-E` for system properties? System properties should be passed as arguments to the JVM via `ES_JAVA_OPTS`, not via command-line arguments to Elasticsearch. A goal with the CLI parsing is to eventually remove the translation of the `-E` command-line arguments to system properties, and that's the primary driver for changing from `-D` to `-E` (to make it clear that `-D` arguments need to be passed as arguments to the JVM and the `-E` arguments need to be passed as arguments to Elasticsearch).
</comment><comment author="pickypg" created="2016-03-22T21:11:48Z" id="200030650">I mean it as a replacement. Specifically, I want to remove https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/plugins/InstallPluginCommand.java#L185 system property usage.

I suppose we can just come up with another way, unless we want to live with it being a system property. My whole driver is consistency since you just worked so hard to remove `-Des.*` it would be sad to see it live on here.
</comment><comment author="rjernst" created="2016-03-22T21:41:01Z" id="200041254">I don't think we should replace that system property. It is specifically meant to be a system property. It is not a setting for elasticsearch.
</comment><comment author="jasontedor" created="2016-03-22T21:43:54Z" id="200042590">@pickypg To be clear, it's okay if system properties live on, even system properties that are specific to Elasticsearch. What is not okay is the mixing of system properties with settings, and the mixing of the command-line flag used for setting system properties and settings.

That one seems like a system property to me.
</comment><comment author="pickypg" created="2016-03-22T22:04:07Z" id="200051554">Fair enough. I think that finishes this one off.

&gt; It is specifically meant to be a system property. It is not a setting for elasticsearch.

I don't totally agree with the semantics of that, but I think it's fine to disagree on it since it's not doing any harm.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make dynamic template parsing less lenient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17249</link><project id="" key="" /><description>Today unknown parameters are ignored yet carried through serialization.
</description><key id="142722495">17249</key><summary>Make dynamic template parsing less lenient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T17:35:27Z</created><updated>2016-03-22T17:54:04Z</updated><resolved>2016-03-22T17:54:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-22T17:50:16Z" id="199935242">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move top level parsing of sort element to SortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17248</link><project id="" key="" /><description>This moves the current top level parsing code for the whole "sort" element from SortParseElement into a static "fromXContent" method in SortBuilder. Also adding tests for parsing lists of SortBuilders in the different allowed syntax variations (e.g. { "sort" : "field1" }, { "sort" : { "field1" : "asc" }}, { "sort" : [ "field1", "field2" ] }.
</description><key id="142716613">17248</key><summary>Move top level parsing of sort element to SortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T17:15:17Z</created><updated>2016-03-24T16:24:07Z</updated><resolved>2016-03-23T12:04:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-22T17:16:54Z" id="199913769">@MaineC I started breaking up #17205 into parts that should be better to review. This one adds only the parsing and tests for SortBuilder#fromXContent().
</comment><comment author="MaineC" created="2016-03-23T11:25:44Z" id="200303630">Left two minor comments - up to your judgement whether to make these changes now, later or not at all.

Other than that LGTM.
</comment><comment author="cbuescher" created="2016-03-23T11:58:00Z" id="200320835">@MaineC thanks, I removed the SortBuilderParser interface and moved those two abstract methods into SortBuilder. Also adding the helper method you suggested and removing unused code that I will only need in the follow-up PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Template parameter to hide indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17247</link><project id="" key="" /><description>It comes up that you want to _not_ include certain index's in global templates, such as meant-to-be hidden indices.

Currently, the [Multiple Index syntax](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/multi-index.html) is the way that we support things like `+test*,-test3` to exclude matching indices, but templates do not support it. However, this may be unnecessarily complicated for templates because the above strategy is meant for fancy logic that can be needed for things like snapshotting and restoring.

A simple solution for this, rather than implementing Multiple Index syntax, may be to add an extra parameter:

``` json
{
  "template" : "*",
  "hide_template" : ".*"
}
```

From there, anything that matches `template`, but also matches `hide_template` (name is debatable), will _not_ have the template applied. We could even default `hide_template` to `.*` so that it excludes anything with a `.`-prefix by default.

As a note, Multiple Index syntax does provide a more consistent approach and also a more flexible one. However, it would not stop users from shooting themselves in the foot because they would have to manually exclude `.`-prefixed indices. I kind of actually want a mix of the two: multiple index without the add/remove stuff so that you can comma delimit multiple patterns for each parameter, thus allowing us to hide `.`-prefix by default.

**Workaround**

For anyone coming here because they ran into colliding templates, the workaround is pretty simple: do not apply _mapping_ changes in global templates where global means `"template" : "*"`. The downside is that it may mean that you end up repeating the template for multiple indices as a result.
</description><key id="142695564">17247</key><summary>Add Template parameter to hide indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Index Templates</label><label>enhancement</label></labels><created>2016-03-22T15:59:28Z</created><updated>2017-04-18T13:15:28Z</updated><resolved>2017-04-17T21:29:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T15:06:51Z" id="201325198">Continuing discussion from https://github.com/elastic/elasticsearch/pull/17300#issuecomment-200954297

I've just read the issue in more detail and realised that my suggestion of adding a parameter to `ignore_indices` to the create index API would not help here, as this problem exists for autocreated indices like kibana and marvel.

As far as I can tell, there are three options:
1. The option proposed in this issue ie adding an `exclude_templates` parameter which defaults to `.*`
2. Changing `*` universally to ignore indices beginning with `.`
3. Making the Kibana template fully explicit, so that nothing can be overridden

The third option would work today, but it does imply a maintenance burden on Kibana.  The second option I'm concerned about because it touches so many APIs and has the disconcerting effect of making indices disappear, eg:

```
PUT .my_index
{ "settings": .... }

GET */_settings
```

returns

```
{}
```

It'd also mean that request like `GET _cat/indices` would not return hidden indices by default, unless you write it as `GET _cat/indices/*,.*`.  I'm not sure if this change is worth it, just to deal with this case.

I'm leaning towards the first option but am open to further suggestions.
</comment><comment author="pickypg" created="2016-03-25T15:17:16Z" id="201329050">I think the first option has the ~~most~~least unintended consequences and it's the least likely to impact any existing functionality anywhere. The third option also does not quite solve it without going over-the-top explicit and it's a wack-a-mole strategy because it will always have to catch up with any new feature. For example, imagine a simplified set of templates:

``` http
PUT /_template/global
{
  "template" : "*",
  "mappings" : {
    "_default_" : {
      "_source" : { "enabled" : false }
    }
  }
}

PUT /_template/kibana
{
  "template" : ".kibana",
  "mappings" : {
    "my_type" : {
      "properties" : {
        "all_fields_mapped" : { "type" : "keyword" }
      }
    }
  }
}
```

Unless they also supplied the `_source` as being enabled explicitly, then the global template would still break things.
</comment><comment author="epixa" created="2016-04-04T15:31:07Z" id="205351814">I like the first option as well.  I think it's appropriate that the way to "fix" the problem is to configure the template that created it to begin with, and the logical default would be one that doesn't interfere with how the core products of the stack operate.

Kibana does need to move to an explicit mapping sooner rather than later, but I agree with @pickypg on this.  If moving to an explicit mapping addresses this issue for the moment, given how global templates can be used, I think that's basically just a coincidence rather than a reliable permanent solution.
</comment><comment author="epixa" created="2016-04-13T14:30:06Z" id="209474793">So what needs to happen in order to gather consensus around a solution and then to make it happen?
</comment><comment author="pickypg" created="2016-04-13T21:07:26Z" id="209647519">Just waiting on @clintongormley to decide if he's comfortable with this solution.
</comment><comment author="clintongormley" created="2016-04-14T08:13:44Z" id="209821054">We'll discuss this in FixItFriday tomorrow
</comment><comment author="colings86" created="2016-04-15T09:29:02Z" id="210382049">Discussed in FixItFriday and we agreed that the first option would be the best way forward on this. We also agreed that we preferred the option to be called `exclude_template` rather than `hide_template`
</comment><comment author="epixa" created="2016-04-15T14:09:39Z" id="210477112">Awesome!
</comment><comment author="pickypg" created="2016-04-15T20:11:17Z" id="210621956">PR in #17300
</comment><comment author="pickypg" created="2017-04-17T21:29:00Z" id="294598741">This is now defunct with the desire to avoid template inheritance. Closing.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Archive cluster level settings if unknown or broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17246</link><project id="" key="" /><description>We already archive index level settings if we find an unknown or invalid/broken
value for a setting on node startup. The same could potentially happen for persistent
cluster level settings if we remove a setting or if we add validation to a setting that
didn't exist in the past. To ensure that only valid settings are recovered into the cluster
state we archive them (prefix them with `archive.` and log a warning. Tools that check the
cluster settings can then warn users that they have broken settings in their clusterstate that
got archived.
</description><key id="142682499">17246</key><summary>Archive cluster level settings if unknown or broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T15:18:58Z</created><updated>2016-03-25T15:22:43Z</updated><resolved>2016-03-22T16:35:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-22T15:23:22Z" id="199863161">Left one really minor comment, LGTM regardless
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test GeoShapeQueryTests.testShapeFilterWithRandomGeoCollection timesout in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17245</link><project id="" key="" /><description>Reproduces on my machine in 2.x branch with the following seed:

```
mvn clean test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=4D404F5EEC9A9625 -Dtests.class=org.elasticsearch.search.geo.GeoShapeQueryTests
```

See the last failure at https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+periodic/159/console
</description><key id="142682169">17245</key><summary>Test GeoShapeQueryTests.testShapeFilterWithRandomGeoCollection timesout in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Geo</label><label>test</label></labels><created>2016-03-22T15:17:45Z</created><updated>2016-03-25T18:43:09Z</updated><resolved>2016-03-25T18:43:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Check that S3 setting `buffer_size` is always lower than `chunk_size`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17244</link><project id="" key="" /><description>`chunk_size` is a Snapshot setting whatever the implementation is.
`buffer_size` is an S3 implementation setting.

Let say that you are snapshotting a 500mb file. If you set `chunk_size` to `200mb`, then Snapshot service will call S3 repository to snapshot 3 files with the following sizes:
- `200mb`
- `200mb`
- `100mb`

If you set `buffer_size` to `100mb` (AWS maximum size recommendation), the first file of `200mb` will be uploaded on S3 using the multipart feature in 2 chunks and the workflow is basically the following:
- create the multipart request and get back an `id` from AWS S3 platform
- upload part1: `100mb`
- upload part2: `100mb`
- "commit" the full upload using the `id`.

We should definitely check that the `buffer_size` setting is always lower than `chunk_size`. Otherwise, setting this value is useless.

May be we should also change the default values for `chunk_size` and `buffer_size` in the context of S3 plugin and define respectively them to `5tb` and `100mb`?
</description><key id="142680667">17244</key><summary>Check that S3 setting `buffer_size` is always lower than `chunk_size`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label></labels><created>2016-03-22T15:12:27Z</created><updated>2016-03-23T17:54:23Z</updated><resolved>2016-03-23T17:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-22T15:12:39Z" id="199859296">@imotov @tlrx WDYT?
</comment><comment author="imotov" created="2016-03-22T19:27:26Z" id="199975462">Having specialized defaults from S3 makes perfect sense to me. `100mb` as a default for `buffer_size` looks reasonable. I am not sure about `5tb` though. I am a bit concerned that it might be too big considering S3 flakiness that we saw before and our default retry settings. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added breaking changes for the Java API to the breaking changes doc for 5.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17243</link><project id="" key="" /><description>Closes #14191
</description><key id="142670366">17243</key><summary>added breaking changes for the Java API to the breaking changes doc for 5.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Java API</label><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T14:40:31Z</created><updated>2016-03-22T15:58:40Z</updated><resolved>2016-03-22T15:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-22T14:54:27Z" id="199851721">left a small comment, thanks a lot for doing this Colin
</comment><comment author="javanna" created="2016-03-22T14:55:52Z" id="199852179">One more thing, do you mind adding the same things for validate query and explain?
See https://github.com/elastic/elasticsearch/issues/14191#issuecomment-152714057
</comment><comment author="colings86" created="2016-03-22T15:35:28Z" id="199869129">@javanna I pushed a commit to address your comments
</comment><comment author="javanna" created="2016-03-22T15:54:56Z" id="199878482">left very small comments, really nitpicks, LGTM though no need for another review
</comment><comment author="javanna" created="2016-03-22T15:56:24Z" id="199879095">Do we want to go and deprecate these methods in the last 2.x release? That would probably be nice to java api users, as their code will not compile anymore against 5.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Es goes insane when doing bulk-upsert with index from template where index-name has ":" in it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17242</link><project id="" key="" /><description>**Elasticsearch version**:2.2
**JVM version**:
**OS version**:Windows 7 64bit
**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
1. Do a scripted bulk-upsert on an index that doesn't exist that gets created from a tempalte "monthly_stats:*"
2. Do bulk-upsert (i only did 3 objects where each incremented 1 field, so request IS small)
3. Es goes insane (writes GBs of logs and takes 80% cpu)

**Provide logs (if relevant)**:
http://pastebin.com/u0qeqNWw

**Note that the problem goes away after I replace in index-name ":" to "_".**
</description><key id="142669023">17242</key><summary>Es goes insane when doing bulk-upsert with index from template where index-name has ":" in it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ddorian</reporter><labels /><created>2016-03-22T14:36:24Z</created><updated>2016-03-22T20:39:21Z</updated><resolved>2016-03-22T20:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-22T20:39:21Z" id="200012079">Unfortunately the index name leaks into the folder name, and the `:` character is invalid for filenames on Windows. From `WindowsPathParser` in the JDK:

``` java
    // Reserved characters for window path name                                                                                                                                                                                         
    private static final String reservedChars = "&lt;&gt;:\"|?*";
    private static final boolean isInvalidPathChar(char ch) {
        return ch &lt; '\u0020' || reservedChars.indexOf(ch) != -1;
    }
```

In master this is fixed by #16442 where we no longer use the index name to derive the folder name but instead use the index UUID (separating name from identity). We can't retroactively forbid `:` in index names because they are okay on non-Windows.

For now, you can workaround this by changing the `:` to another character, as you already pointed out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic mappings doesn't work for the keyword type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17241</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0-alpha1 snapshot bf98a44
**JVM version**: "1.8.0_45"
**OS version**: OS X

In topbeat we're using a dynamic mapping template like this:

```
      "dynamic_templates": [
        {
          "template1": {
            "mapping": {
              "doc_values": true,
              "ignore_above": 1024,
              "index": "not_analyzed",
              "type": "{dynamic_type}"
            },
            "match": "*"
          }
        }
```

The full template is here: https://gist.github.com/tsg/904710ed2514ffc7eed6

The template loads fine but I get an error at index time, saying "[text] fields do not support doc values". The full logs are here: https://gist.github.com/tsg/2119de476698f1228520

@jpountz explained that "now that string has been splitted into text and keyword, we made ${dynamic_type} be "text" for string fields so it is not possible to build mappings that have a dynamic type for keywords anymore." and we agreed to open this ticket to look for a solution together.

I tried to change "type" to "keyword" in the dynamic template, but that didn't help. Is there a way in the template to say "Fields are of type 'keyword' unless I explicitly define them"?
</description><key id="142666650">17241</key><summary>Dynamic mappings doesn't work for the keyword type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">tsg</reporter><labels><label>:Mapping</label><label>bug</label></labels><created>2016-03-22T14:28:05Z</created><updated>2016-04-05T13:56:43Z</updated><resolved>2016-03-31T13:11:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-31T13:11:56Z" id="203930809">Even though the `index=(not_)analyzed` trick can't be used anymore, the above template can be replaced with the following:

```
      "dynamic_templates": [
        {
          "template1": {
            "mapping": {
              "ignore_above": 1024,
              "type": "keyword"
            },
            "match_mapping_type": "string"
          }
        }
```

Since the use-case can still be addressed, I will close this issue. For the record, we did some related documentation efforts in #17413 to show how to address common problems that can be fixed with templates.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Doc fixes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17240</link><project id="" key="" /><description>Fixing some docs function outdated and some mistakes in examples.
</description><key id="142639242">17240</key><summary>Doc fixes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">SirBigoo</reporter><labels /><created>2016-03-22T12:46:37Z</created><updated>2016-03-25T14:42:11Z</updated><resolved>2016-03-25T14:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T14:42:11Z" id="201318009">Hi @SirBigoo 

I'm afraid you have created this PR against the wrong branch (ie master). Please could you reopen against the correct branch.  (You'll also need to sign the CLA before we can merge it in)

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resolve string dates and date math to millis before evaluating for rewrite in range query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17239</link><project id="" key="" /><description>Previously to this commit range queries containing `now` which a shard has relation WITHIN to would fail because `now` was not being resolved properly during the rewrite. This change resolves `now`before evaluating whether to rewrite to avoid this failure and to avoid date math having to be resolved multiple times on the shard.
</description><key id="142630804">17239</key><summary>Resolve string dates and date math to millis before evaluating for rewrite in range query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query DSL</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T12:07:04Z</created><updated>2016-03-22T16:16:24Z</updated><resolved>2016-03-22T16:16:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2016-03-22T13:32:38Z" id="199815896">@s1monw I pushed a comment to address your comments
</comment><comment author="colings86" created="2016-03-22T16:00:55Z" id="199880717">@s1monw @jpountz I have changed the rewrite so it rewrites WITHIN relations to an unbounded query [null TO null] which simplifies the rewrite logic in RangeQueryBuilder a lot (thanks @jpountz for that idea). Could you give it another look?
</comment><comment author="jpountz" created="2016-03-22T16:11:14Z" id="199885085">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused methods and fields in NestedInnerQueryParseSupport</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17238</link><project id="" key="" /><description>While reaftoring the SortBuilders in #17205 I came upon a few places in NestedInnerQueryParseSupport that seemed unused. In order to reduce the complexity while doing the rest of the refactoring, this factores out this simplifications to get a separate review for it.
</description><key id="142618995">17238</key><summary>Remove unused methods and fields in NestedInnerQueryParseSupport</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Nested Docs</label><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T11:13:28Z</created><updated>2016-03-25T14:39:35Z</updated><resolved>2016-03-22T11:16:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-22T11:14:24Z" id="199763630">LGTM - trash it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve error message if resource files have illegal encoding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17237</link><project id="" key="" /><description>This commit fixes string formatting issues in the error handling and
provides a better error message if malformed input is detected.
This commit also adds tests for both situations.

Relates to #17212
</description><key id="142613478">17237</key><summary>Improve error message if resource files have illegal encoding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Analysis</label><label>bug</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T10:54:44Z</created><updated>2016-03-22T12:29:47Z</updated><resolved>2016-03-22T12:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-22T10:54:54Z" id="199751760">@rmuir can you take a look
</comment><comment author="rmuir" created="2016-03-22T12:03:56Z" id="199780590">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QuorumGatewayIT fails on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17236</link><project id="" key="" /><description>The failure reproduces (e.g. with seed BD57BE5B57A9875A:5339034DFF2C888F). Stumbled upon it while running tests to get #16963 back up-to-date. Seems like we are missing the document that we index after the full cluster restart, when a single node is up. Seems quite bad at first glance.

```
java.lang.AssertionError: Count is 2 but 3 was expected.  Total shards: 5 Successful shards: 5 &amp; 0 shard failures:

    at __randomizedtesting.SeedInfo.seed([BD57BE5B57A9875A:5339034DFF2C888F]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:248)
    at org.elasticsearch.gateway.QuorumGatewayIT.testQuorumRecovery(QuorumGatewayIT.java:103)
```
</description><key id="142613400">17236</key><summary>QuorumGatewayIT fails on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">javanna</reporter><labels><label>test</label></labels><created>2016-03-22T10:54:24Z</created><updated>2016-03-22T15:45:06Z</updated><resolved>2016-03-22T15:45:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-22T15:43:59Z" id="199873186">Assigning @bleskes as this failure was related to #17233 not sure whether reverting the original change solves completely the issue yet and whether we should close it or not.
</comment><comment author="bleskes" created="2016-03-22T15:45:06Z" id="199873524">It does indeed. The change was reverted, so I'm closing this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Detach IndexShard from node services</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17235</link><project id="" key="" /><description>this is the last step to remove node level service from IndexShard.
This means that tests can now more easily create an IndexShard instance
without starting a node and removes the dependency between IndexShard and Client/ScriptService
</description><key id="142596273">17235</key><summary>Detach IndexShard from node services</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-22T09:45:33Z</created><updated>2016-03-22T10:02:39Z</updated><resolved>2016-03-22T10:02:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-22T09:59:09Z" id="199728123">Great. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aliases not created properly - Missing filter information - ES 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17234</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_74

**OS version**: 3.13.0-74-generic #118-Ubuntu SMP (Ubuntu Server 14.04 LTS)

**Elasticsearch Client**: [elasticsearch-py](https://github.com/elastic/elasticsearch-py)

**Description of the problem including expected versus actual behavior**:
I am trying to add aliases with filters to some indexes. Basically I have write and read aliases on an index, with filter and routing specified. Based on some criteria, I need to modify the aliases to point to a new index, at which time, I remove the write alias from the existing index, move it to the newly created index and at the same add the same read alias to the newly created index. I am using update_aliases api in the python client to perform all three operations atomically.

_Expected behaviour_: The write alias should be removed from the old index and start pointing to the new index, while containing the filter and routing specified.

_Observed behaviour_: The write alias is removed from the old index, added to the new index but doesnt contain the specified filter. Attaching code snippets in steps to reproduce.

**Steps to reproduce**:
- Apply alias actions:

```
actions = {}
actions['actions'] = [{'remove': {'index': u'goibibo_201612', 'alias': u'w_goibibo-serviceresponsesuccess_201612'}}, {'add': {'filter': {'term': {'primaryName': u'goibibo-serviceresponsesuccess'}}, 'index': u'goibibo-serviceresponsesuccess_201612', 'alias': u'w_goibibo-serviceresponsesuccess_201612'}}, {'add': {'filter': {'term': {'primaryName': u'goibibo-serviceresponsesuccess'}}, 'index': u'goibibo-serviceresponsesuccess_201612', 'alias': u'r_goibibo-serviceresponsesuccess_201612'}}]
es.indices.update_aliases(body=actions)
```
- Fetch alias (should contain the filter, but doesnt)

```
es.indices.get_alias(name='w_goibibo-serviceresponsesuccess_201612')

{u'goibibo-serviceresponsesuccess_201612': {u'aliases': {u'w_goibibo-serviceresponsesuccess_201612': {}}}}
```

**Provide logs (if relevant)**:
Didn't see any error logs in master node
</description><key id="142585840">17234</key><summary>Aliases not created properly - Missing filter information - ES 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akgoel-mo</reporter><labels /><created>2016-03-22T08:55:08Z</created><updated>2016-03-22T09:04:44Z</updated><resolved>2016-03-22T09:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-22T09:03:07Z" id="199707809">This is a duplicate of #16547, already fixed in 2.2.1. Thanks for reporting and for the nice recreation though!
</comment><comment author="akgoel-mo" created="2016-03-22T09:04:43Z" id="199708174">Thanks @javanna. Will update my elasticsearch version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed index level metadata election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17233</link><project id="" key="" /><description>When a master is elected, it reaches out to all master nodes for their cluster state, selecting the one with the highest version. At the moment, we do another round to select the index metadata with the highest version as well. This is not needed - the election of a cluster state is enough - we should just use whatever indices are in it.
</description><key id="142581416">17233</key><summary>Removed index level metadata election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label></labels><created>2016-03-22T08:29:18Z</created><updated>2016-03-22T15:29:27Z</updated><resolved>2016-03-22T09:31:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-22T08:39:57Z" id="199702364">LGTM
</comment><comment author="bleskes" created="2016-03-22T15:29:27Z" id="199866297">This change is problematic and was reverted.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup writing upgraded index state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17232</link><project id="" key="" /><description>In #17187, we upgrade index state after upgrading index folder structure. 
As we don't have to write the upgraded state in the old index folder structure,
we can cleanup how we write upgraded index state.
</description><key id="142495393">17232</key><summary>Cleanup writing upgraded index state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T23:01:50Z</created><updated>2016-03-22T18:58:20Z</updated><resolved>2016-03-22T18:57:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-22T10:09:15Z" id="199732475">LGTM
</comment><comment author="areek" created="2016-03-22T18:58:20Z" id="199963377">Thanks @bleskes for the review :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't wait for completion of list tasks tasks when wait_for_completion flag is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17231</link><project id="" key="" /><description>Waiting for completion of list tasks tasks can cause an infinite loop of a list tasks task waiting for its own completion or completion of its children. To reproduce run:

```
curl "localhost:9200/_tasks?wait_for_completion"
```
</description><key id="142488284">17231</key><summary>Don't wait for completion of list tasks tasks when wait_for_completion flag is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T22:23:04Z</created><updated>2016-03-24T12:02:52Z</updated><resolved>2016-03-24T03:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-21T22:26:36Z" id="199516221">Left small comment. LGTM otherwise. Sorry I didn't catch this when I made the feature!
</comment><comment author="nik9000" created="2016-03-24T03:06:43Z" id="200633988">If you backport to 2.3 then I think this'll get into 2.3. I have a reindex
issue to backport in the morning. Don't have link handy.
On Mar 23, 2016 11:04 PM, "Igor Motov" notifications@github.com wrote:

&gt; Merged #17231 https://github.com/elastic/elasticsearch/pull/17231.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17231#event-601212883
</comment><comment author="imotov" created="2016-03-24T03:08:47Z" id="200634578">@nik9000 not sure I follow. I was about to push the backport to 2.3, do you want me to do it, or I should hold off?
</comment><comment author="nik9000" created="2016-03-24T03:13:25Z" id="200635746">Do it! I think it should have the 2.3.0 tag instead of 2.3.1 though.
On Mar 23, 2016 11:08 PM, "Igor Motov" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 not sure I follow. I was about to
&gt; push the backport to 2.3, do you want me to do it, or I should hold off?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17231#issuecomment-200634578
</comment><comment author="nik9000" created="2016-03-24T12:02:52Z" id="200804827">@imotov backported to 2.3 with 2c93394405f944166966306055310454486a2403. This should make it into 2.3.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve error message if setting is not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17230</link><project id="" key="" /><description>We can do better than just throwing an error when we don't find a
setting. It's actually trivial to leverage lucenes slow LD StringDistance
to find possible candiates for a setting to detect missspellings and suggest
a possible setting.
This commit adds error messages like:
- `unknown setting [index.numbe_of_replica] did you mean [index.number_of_replicas]?`

rather than just reporting the setting as unknown
</description><key id="142487396">17230</key><summary>Improve error message if setting is not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T22:17:33Z</created><updated>2016-03-22T09:37:00Z</updated><resolved>2016-03-22T09:06:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-21T22:20:29Z" id="199514238">thanks @rmuir for putting this on the table - it's an awesome idea :)
</comment><comment author="rmuir" created="2016-03-21T22:21:20Z" id="199514415">+1, great improvement! just like the JDK.

Note: if later we want better support for transpositions, there is also LuceneLevenshteinDistance and JaroWinkler which are geared at those too. Someone can always explore those, this one should do well though (esp with 0.7)
</comment><comment author="mikemccand" created="2016-03-21T22:22:01Z" id="199514580">+1, very cool!
</comment><comment author="kimchy" created="2016-03-21T22:27:48Z" id="199516659">I wonder what is the levenshtein distance between a @rmuir and great features is :)
</comment><comment author="nik9000" created="2016-03-21T23:15:03Z" id="199530017">LGTM
</comment><comment author="bleskes" created="2016-03-22T08:16:40Z" id="199693572">everything that prevents from staring at screen only to feel dumb later is great :) 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update Warmer API documentation to indicate it is deprecated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17229</link><project id="" key="" /><description>The Warmer API has been deprecated in 2.x and will be removed in 5.0 (#15607). Please update product documentation to indicate status.
</description><key id="142472477">17229</key><summary>Update Warmer API documentation to indicate it is deprecated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">inqueue</reporter><labels><label>docs</label></labels><created>2016-03-21T21:08:55Z</created><updated>2016-03-22T09:36:41Z</updated><resolved>2016-03-22T09:36:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-22T02:33:15Z" id="199594362">This is covered in the [migration docs](https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_index_apis.html#_warmers) and redirects which say:

&gt; Warmers have been removed. There have been significant improvements to the index that make warmers not necessary anymore.

The pull request #15614 shows additional changes to the docs. Do you think that this isn't enough?
</comment><comment author="inqueue" created="2016-03-22T03:25:24Z" id="199613751">There are a lot of good changes to the future docs. Do you think we need something in 2.x documentation to indicate it is deprecated and going away entirely? 
</comment><comment author="jasontedor" created="2016-03-22T03:37:57Z" id="199620271">&gt; Do you think we need something in 2.x documentation to indicate it is deprecated and going away entirely?

Yeah, I don't see anything there and you're right that something should be added. Thanks for raising this.
</comment><comment author="jpountz" created="2016-03-22T08:37:37Z" id="199701366">OK I will add a deprecation warning.
</comment><comment author="jpountz" created="2016-03-22T09:36:41Z" id="199717786">Done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins listed by different name to install name not automation friendly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17228</link><project id="" key="" /><description>**Elasticsearch version 2.2.1**:

Installation of community plugins is achieved by using the command plugin install &lt;plugin name&gt; e.g. 

./plugin install lmenezes/elasticsearch-kopf

However, post install the plugins are typically listed by a name e.g. the above when is listed as simple "kopf"

./plugin list
Installed plugins in /opt/elastic/elasticsearch-2.2.1/plugins:
    - kopf

Any automation framework relies on being able to identify a delta between what is installed and what is requested.  The above requires a custom rule which seems both undocumented and alittle inconsistent e.g. it doesn't seem dropping the owner prefix ( lmenezes for the above) is sufficient.

This maybe out of the control of Elastic and a product of the packaging poor community plugins.  I therefore request that we simply store with the plugin a file indicating its install name e.g. in the same directory. 
</description><key id="142457408">17228</key><summary>Plugins listed by different name to install name not automation friendly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels /><created>2016-03-21T20:15:00Z</created><updated>2016-03-21T22:15:55Z</updated><resolved>2016-03-21T20:18:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-21T20:18:52Z" id="199458714">That logic has been removed in 5.0. Whatever the name of the plugin in the plugin descriptor, that is the name we use as the installation directory and what is listed.
</comment><comment author="gingerwizard" created="2016-03-21T22:15:55Z" id="199513141">@rjernst thankyou for this. It makes the automation work alot simpler.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Add xpack as official plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17227</link><project id="" key="" /><description>In order to be able to install `xpack` as a plugin easily and unless this works
for cross product packs, this adds xpack as an official plugin.
</description><key id="142422773">17227</key><summary>PluginManager: Add xpack as official plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T17:54:01Z</created><updated>2016-03-25T13:45:25Z</updated><resolved>2016-03-21T18:26:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-21T18:03:26Z" id="199404729">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PluginManager: Installing non-existing plugin returns confusing error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17226</link><project id="" key="" /><description>If a plugin does not exist, then this is returned

```
bin/elasticsearch-plugin install nonexistentplugin
-&gt; Downloading nonexistentplugin
Exception in thread "main" java.net.MalformedURLException: no protocol: nonexistentplugin
    at java.net.URL.&lt;init&gt;(URL.java:586)
    at java.net.URL.&lt;init&gt;(URL.java:483)
    at java.net.URL.&lt;init&gt;(URL.java:432)
    at org.elasticsearch.plugins.InstallPluginCommand.downloadZip(InstallPluginCommand.java:211)
    at org.elasticsearch.plugins.InstallPluginCommand.download(InstallPluginCommand.java:206)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:174)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:162)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:57)
```

Also I cant seem to find out the URL which is actually requested for debugging purposes anymore, might be intended though?
</description><key id="142415321">17226</key><summary>PluginManager: Installing non-existing plugin returns confusing error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Plugins</label><label>bug</label><label>v5.0.0-alpha4</label></labels><created>2016-03-21T17:27:29Z</created><updated>2016-06-15T16:56:43Z</updated><resolved>2016-06-15T16:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-21T17:36:28Z" id="199393846">`nonexistentplugin` _is_ the url the plugin cli is trying to resolve. The logic is simple (1) is it a known official plugin? (2) is it maven coordinates? (3) finally, try it as a url.
</comment><comment author="rjernst" created="2016-03-21T17:45:51Z" id="199398544">If we want this to be "nicer" we could print a message in this final case on the usage info about what is supported (official plugin name, maven coordinates, or url).
</comment><comment author="javanna" created="2016-03-21T17:53:28Z" id="199401395">&gt; If we want this to be "nicer" we could print a message in this final case on the usage info about what is supported (official plugin name, maven coordinates, or url).

++ that was the purpose of this issue I believe. The logic is clear to developers, but the current error is confusing to users.
</comment><comment author="nik9000" created="2016-03-21T18:11:42Z" id="199408410">&gt; The logic is clear to developers, but the current error is confusing to users.

I suspect this mostly comes up in the "I misspelled kuromoji" kind of use case.
</comment><comment author="rjernst" created="2016-03-21T18:14:11Z" id="199409222">Perhaps we should improve the usage information, and throw a UserError with USAGE here then. This way we don't have to repeat in this case, and in the general help.
</comment><comment author="nikoncode" created="2016-04-27T21:49:07Z" id="215240364">I think need to show message like follows:

&lt;pre&gt;
$elasticsearch-plugin install lipsum
Not found as core plugin.
Not found as maven artifact.
Not found as URL.
Usages:
install name
install groupId:artifactId:version
install file:///name.zip
&lt;/pre&gt;

Why github isn't supported now (5.x)?
</comment><comment author="clintongormley" created="2016-04-29T08:34:14Z" id="215658603">&gt; Why github isn't supported now (5.x)?

Site plugins are no longer supported, only Java plugins.  Site plugins should be reimplemented as Kibana plugins.
</comment><comment author="clintongormley" created="2016-05-05T07:28:53Z" id="217093218">It's easy enough to figure out whether the argument looks like a protocol or not, and it would be much friendlier to say "unknown plugin" in this case than "no protocol: foo".  Also, the current usage method doesn't mention that you can pass a URL or maven coordinates to install.
</comment><comment author="rjernst" created="2016-06-15T03:07:13Z" id="226076800">I opened #18876 to give a nicer error message. It does not yet force listing out all the acceptable patterns for installing, but that is a broader issue I think we need to address with real usage lines.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting array_index_out_of_bounds_exception while using order_by</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17225</link><project id="" key="" /><description>**Observations**
- ES version 2.2.1
- If number of documents indexed is 2 , there is no error or issue
- With following recreation , I am seeing array_index_out_of_bounds_exception exception 

```
curl -XDELETE 'http://localhost:9200/vm/'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm1" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm2" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm3" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm4" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm5" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm6" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm7" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm8" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm9" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm10" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/vm' -d '{ "name" : "vm11" , "rank" : 2 , "date" : 2000 }'
echo
curl -XPOST 'http://localhost:9200/vm/_refresh'
echo
curl -XPOST 'http://localhost:9200/vm/vm/_search?pretty' -d '{
  "size": 0,
  "aggs": {
    "keywords": {
      "terms": {
        "collect_mode": "breadth_first",
        "field": "name",
        "size": 1,
        "order": {
          "dateB&gt;rankAvg": "asc"
        }
      },
      "aggs": {
        "dateB": {
          "filter": {
            "term": {
              "date": 2001
            }
          },
          "aggs": {
            "rankAvg": {
              "min": {
                "field": "rank",
                "missing": 1000
              }
            }
          }
        }
      }
    }
  }
}'
```

Response

```
{
  "took" : 13,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 3,
    "failed" : 2,
    "failures" : [ {
      "shard" : 2,
      "index" : "vm",
      "node" : "xpPiHlvJQjSgYu-L10WJuA",
      "reason" : {
        "type" : "array_index_out_of_bounds_exception",
        "reason" : "1"
      }
    } ]
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "keywords" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 1,
      "buckets" : [ {
        "key" : "vm10",
        "doc_count" : 1,
        "dateB" : {
          "doc_count" : 0,
          "rankAvg" : {
            "value" : null
          }
        }
      } ]
    }
  }
}
```

Logs emitted

```
[2016-03-21 21:53:55,238][DEBUG][action.search.type       ] [Gertrude Yorkes] [vm][4], node[pkRBZKPTStiOaYYr4mH_-w], [P], v[2], s[STARTED], a[id=Tf6t7fTWRz-FSvU30ln-mA]: Failed to execute [org.elasticsearch.action.search.SearchRequest@a062c99] lastShard [true]
RemoteTransportException[[Gertrude Yorkes][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: ArrayIndexOutOfBoundsException[1];
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
    at org.elasticsearch.common.util.BigArrays$DoubleArrayWrapper.get(BigArrays.java:260)
    at org.elasticsearch.search.aggregations.metrics.min.MinAggregator.metric(MinAggregator.java:100)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$Aggregation$3.compare(InternalOrder.java:213)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$Aggregation$3.compare(InternalOrder.java:210)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$CompoundOrder$CompoundOrderComparator.compare(InternalOrder.java:280)
    at org.elasticsearch.search.aggregations.bucket.terms.InternalOrder$CompoundOrder$CompoundOrderComparator.compare(InternalOrder.java:266)
    at org.elasticsearch.search.aggregations.bucket.terms.support.BucketPriorityQueue.lessThan(BucketPriorityQueue.java:37)
    at org.elasticsearch.search.aggregations.bucket.terms.support.BucketPriorityQueue.lessThan(BucketPriorityQueue.java:26)
    at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:258)
    at org.apache.lucene.util.PriorityQueue.add(PriorityQueue.java:135)
    at org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueue.java:151)
    at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:176)
    at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:167)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:119)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:376)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
    at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

```
</description><key id="142402187">17225</key><summary>Getting array_index_out_of_bounds_exception while using order_by</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.3.1</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T16:32:38Z</created><updated>2016-04-07T02:44:04Z</updated><resolved>2016-03-29T12:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T13:43:49Z" id="201285154">@colings86 could you take a look at this please
</comment><comment author="Vineeth-Mohan" created="2016-04-07T02:44:04Z" id="206665953">Thanks guys. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: Garbage collection stats in _cluster/stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17224</link><project id="" key="" /><description>Hello,
I couldn't find any ticket relating to this. Could it be possible to have stats about the garbage collector at the cluster level?
Right now, such data is only available in `_nodes/stats` but missing in `_cluster/stats`.
</description><key id="142395393">17224</key><summary>Feature request: Garbage collection stats in _cluster/stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">McStork</reporter><labels><label>:Stats</label><label>discuss</label></labels><created>2016-03-21T16:06:24Z</created><updated>2016-04-29T21:00:54Z</updated><resolved>2016-04-29T21:00:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-23T21:44:55Z" id="213838225">@McStork I don't think that these metrics make sense at the cluster level, they are meaningful at the node level. Imagine if these were collected at the cluster level, but a node restarts. This means that the cluster-wide collection count and collection time would decrease and as such I don't see these metrics being useful at the cluster level.

Also, if you were to use these for some monitoring, what is the use of knowing that the collection count or collection time sharply increased over some time interval without knowing which node or nodes caused that to happen?

I think these metrics make sense and are useful at the node level, but not at the cluster level.
</comment><comment author="jasontedor" created="2016-04-29T21:00:49Z" id="215880620">No feedback, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve upgrade experience of node level index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17223</link><project id="" key="" /><description>In 5.0 we don't allow index settings to be specified on the node level ie.
in yaml files or via commandline argument. This can cause problems during
upgrade if this was used extensively. For instance if analyzers where
specified on a node level this might cause the index to be closed when
imported (see #17187). In such a case all indices relying on this
must be updated via `PUT /${index}/_settings`. Yet, this API has slightly
different semantics since it overrides existing settings. To make this less
painful this change adds a `preserve_existing` parameter on that API to ensure
we have the same semantics as if the setting was applied on the node level.

This change also adds a better error message and a change to the migration guide
to ensure upgrades are smooth if index settings are specified on the node level.

If a index setting is detected this change fails the node startup and prints a message
like this:

```
*************************************************************************************
Found index level settings on node level configuration.

Since elasticsearch 5.x index level settings can NOT be set on the nodes
configuration like the elasticsearch.yaml, in system properties or command line
arguments.In order to upgrade all indices the settings must be updated via the
/${index}/_settings API. Unless all settings are dynamic all indices must be closed
in order to apply the upgradeIndices created in the future should use index templates
to set default values.

Please ensure all required values are updated on all indices by executing:

curl -XPUT 'http://localhost:9200/_all/_settings?preserve_existing=true' -d '{
  "index.number_of_shards" : "1",
  "index.query.default_field" : "main_field",
  "index.translog.durability" : "async",
  "index.ttl.disable_purge" : "true"
}'
*************************************************************************************
```
</description><key id="142371174">17223</key><summary>Improve upgrade experience of node level index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T14:45:22Z</created><updated>2016-03-25T13:46:07Z</updated><resolved>2016-03-21T19:13:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-21T15:53:20Z" id="199352195">LGTM. Very helpful error message!
</comment><comment author="bleskes" created="2016-03-21T15:55:48Z" id="199353256">+1 on going the extra mile. Is it possible/realistic I wonder if we should backport it to 2.x, so people can do that before they upgrade, when the migration plugin tells them they're in potential trouble?
</comment><comment author="s1monw" created="2016-03-21T19:12:28Z" id="199431915">&gt;  Is it possible/realistic I wonder if we should backport it to 2.x, so people can do that before they upgrade, when the migration plugin tells them they're in potential trouble?

I can try looking into it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Update ospackage gradle plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17222</link><project id="" key="" /><description>The older version did not support signing of the packages.
</description><key id="142369136">17222</key><summary>Build: Update ospackage gradle plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label></labels><created>2016-03-21T14:38:39Z</created><updated>2016-05-10T03:05:34Z</updated><resolved>2016-03-21T15:18:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-21T14:40:06Z" id="199319338">LGTM if we the vagrant tests continue to pass.
</comment><comment author="Oscean" created="2016-05-09T06:22:15Z" id="217784625">The same reason or ...?

D:\workspace20160505\elasticsearch-2.3\elasticsearch-master&gt;gradle build
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:writeVersionProperties UP-TO-DATE
:buildSrc:processResources UP-TO-DATE
:buildSrc:classes UP-TO-DATE
:buildSrc:jar UP-TO-DATE
:buildSrc:sourcesJar UP-TO-DATE
:buildSrc:signArchives SKIPPED
:buildSrc:assemble UP-TO-DATE
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE

# :buildSrc:build UP-TO-DATE

# Elasticsearch Build Hamster says Hello!

  Gradle Version        : 2.8
  OS Info               : Windows 7 6.1 (amd64)
  JDK Version           : Oracle Corporation 1.8.0_91 [Java HotSpot(TM) 64-Bit S
erver VM 25.91-b14]
  JAVA_HOME             : E:\SoftwareInstall\Java\jdk8

FAILURE: Build failed with an exception.
- What went wrong:
  A problem occurred configuring project ':distribution'.
  
  &gt; Could not resolve all dependencies for configuration ':distribution:classpath'
  &gt; .
  &gt; Could not download gradle-ospackage-plugin.jar (com.netflix.nebula:gradle-o
  &gt; spackage-plugin:3.4.0)
  &gt; Could not get resource 'https://plugins.gradle.org/m2/com/netflix/nebula
  &gt; /gradle-ospackage-plugin/3.4.0/gradle-ospackage-plugin-3.4.0.jar'.
  &gt;      &gt; Could not GET 'https://plugins.gradle.org/m2/com/netflix/nebula/gradl
  &gt; e-ospackage-plugin/3.4.0/gradle-ospackage-plugin-3.4.0.jar'.
  &gt;         &gt; peer not authenticated
- Try:
  Run with --stacktrace option to get the stack trace. Run with --info or --debug
  option to get more log output.

BUILD FAILED

Total time: 46.137 secs

D:\workspace20160505\elasticsearch-2.3\elasticsearch-master&gt;
</comment><comment author="spinscale" created="2016-05-09T06:50:43Z" id="217788423">hey,

so `https://jcenter.bintray.com/com/netflix/nebula/gradle-ospackage-plugin/3.4.0/` looks reachable to me... network hiccup maybe or do you get that error consistently?
</comment><comment author="Oscean" created="2016-05-10T03:05:34Z" id="218047744">now works with the new URL,thanks a lot
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest Attachment - OCR support for images</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17221</link><project id="" key="" /><description>Hello,

the ingest attachment plugin uses Tika for content extraction, Tika supports OCR by default if Tesseract OCR is installed.

I took a look at the Ingest Attachment code to see how hard it would be to add Tika's OCR functionalities to it and I found that I would need to add a new parser (org.apache.tika.parser.ocr.TesseractOCRParser) and the appropriate security policies. 

I forked the code and did that to see if it would work, the way Tika calls Tesseract is by forking a new process, so Elasticsearch's default seccomp config makes it fail. After disabling seccomp it works. 

Would you guys be willing to accept a pull request to add that in? The changes in the code are small, the only problem is that to work seccomp will have to be disabled in the Ingest Node if the user wants OCR.
</description><key id="142363439">17221</key><summary>Ingest Attachment - OCR support for images</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">miguelxpn</reporter><labels><label>:Ingest</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-21T14:21:46Z</created><updated>2016-03-25T13:36:43Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T13:36:43Z" id="201279748">Hi @miguelxpn 

Thanks for taking a look at this.  I can't speak for everybody but I think it likely that we would be very much against disabling seccomp ("optional security isn't security at all").  I think the right way to approach this would be to have a separate OCR service that the ingest plugin can talk to.  That way, Elasticsearch doesn't have to allow forking and it restricts the possibility of exploits to just the OCR service.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM package: Do not log into systemd journal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17220</link><project id="" key="" /><description>When the RPM package is installed and elasticsearch is started via systemd, every log message is logged in `/var/log/elasticsearch/` and to the systemd journal, which can be accessed using the `journalctl` tool.

I dont think we should put anything into the journal, especially as most other services seem to be kinda scarce on output in there (given my vagrant vm).

Open for discussion.
</description><key id="142356325">17220</key><summary>RPM package: Do not log into systemd journal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>blocker</label><label>v5.0.0-beta1</label></labels><created>2016-03-21T13:59:08Z</created><updated>2016-09-14T14:45:49Z</updated><resolved>2016-09-13T12:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-21T14:43:32Z" id="199320710">@spinscale why not? Storing the logs in the journal allows an administrator to do something like: `journalctl _PID=1234 --since "20 min ago"` that is more difficult to do tailing logs.

Is there any reason why _shouldn't_ put logs in it on systemd systems?
</comment><comment author="spinscale" created="2016-03-21T15:45:46Z" id="199347924">As far as my knowledge about systemd journal goes, by default it cleans itself up in case it takes up too much space. If Elasticsearch is on a log blast, we fill up the disk twice (which I dont care too much about, monitoring will kick in and notify), but this might expire other important entries out of the journal logs, where we have the es logs anyway.

That said, I do like the querying capabilties :)
</comment><comment author="dakrone" created="2016-03-21T16:01:36Z" id="199355850">&gt; If Elasticsearch is on a log blast, we fill up the disk twice (which I dont care too much about, monitoring will kick in and notify), but this might expire other important entries out of the journal logs, where we have the es logs anyway.

That sounds like evidence that we should log _only_ to the journal on systemd machines, since you can easily configure the journal to write to a log file additionally.
</comment><comment author="nik9000" created="2016-03-21T18:08:15Z" id="199406962">So long as we only log one place I really don't care which place that is.
</comment><comment author="jasontedor" created="2016-03-21T18:12:47Z" id="199408749">I'm in favor of `systemd` logging, but if we are only going to log to one place I think that it should be `/var/log/elasticsearch/elasticsearch.log`; the journal can be confusing, and has caused trouble for users in the past (because they don't know to look there, and the vast body of knowledge accessible through searches will point people towards `/var/log/elasticsearch/elasticsearch.log` rather than `systemd`). See #16159 and the related issues for some recent issues that support this.

We can provide commented out journal configuration for end-users that want this instead of the simpler default.
</comment><comment author="tlrx" created="2016-03-21T20:24:37Z" id="199461798">When it comes to elasticsearch as a _system service_ I think we should try to integrate as much as possible with the OS features. I feel like systemd is adopted by more and more distributions and the journalctl is really powerful. We could enable persistent log with a max file size and compress by default.

Concerning the vast body of knowledge referring to /var/log/elasticsearch/elasticsearch.log file, I think dropping a file there with a single line comment saying to look at the journal is OK.
</comment><comment author="jasontedor" created="2016-03-22T02:48:37Z" id="199602369">@tlrx I'm all in favor in `systemd` but my argument is merely that if we are going to pick one, we should pick the simplest. As an added note, on all other systems we still have to support `/var/log/elasticsearch/elasticsearch.log`.

&gt; We could enable persistent log with a max file size and compress by default.

We can also provide a `logrotate.d` conf file for this.

&gt; Concerning the vast body of knowledge referring to /var/log/elasticsearch/elasticsearch.log file, I think dropping a file there with a single line comment saying to look at the journal is OK.

That does not alleviate my concerns that I've encountered many end users that have found the journal confusing.
</comment><comment author="spinscale" created="2016-03-22T07:46:06Z" id="199682001">I also completely forgot we are still bound to maintain `init.d` based systems, due to LTS releases from various distributions.

Proposal from my side, unless we are on systemd exclusively (lets see if that works out without another dozen of init systems jumping into existence): Remove the `console` from `logging.yml`, but also add documentation how to do systemd logging exclusively and include some of the nice command line arguments that Lee showed off above and mentioning to not care about log rotatation in that case.
</comment><comment author="clintongormley" created="2016-09-02T10:09:14Z" id="244336905">Discussed in FixitFriday, agreed with https://github.com/elastic/elasticsearch/issues/17220#issuecomment-199682001,, especially given that having logs in a standard place makes it easier for our diagnostic tools to capture them
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix daemon exists check</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17219</link><project id="" key="" /><description>Added proper exit code and a failure message.
</description><key id="142351490">17219</key><summary>Fix daemon exists check</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tuladhar</reporter><labels /><created>2016-03-21T13:44:44Z</created><updated>2016-03-22T03:12:49Z</updated><resolved>2016-03-22T03:12:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-22T03:12:49Z" id="199607627">Closing in favor of #17082.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement available for all StreamInput classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17218</link><project id="" key="" /><description>There are some implementation of StreamInput that implement the available method
and there are others that do not implement this method. This change makes the
available method abstract in the StreamInput class and implements the method where
it was not previously implemented.
</description><key id="142348306">17218</key><summary>Implement available for all StreamInput classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T13:32:27Z</created><updated>2016-03-23T13:15:24Z</updated><resolved>2016-03-23T13:15:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-21T14:33:51Z" id="199316928">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes the defaults for `keyed` in the percentiles aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17217</link><project id="" key="" /><description>During the aggregation refactoring the default value for `keyed` in the `percentiles` and `percentile_ranks` aggregation was inadvertently changed from `true` to `false`. This change reverts the defaults to the old (correct) value.

Relates to https://github.com/elastic/kibana/pull/6309
</description><key id="142323560">17217</key><summary>Fixes the defaults for `keyed` in the percentiles aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T11:42:22Z</created><updated>2016-03-21T12:30:03Z</updated><resolved>2016-03-21T12:30:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-21T12:09:46Z" id="199244773">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rest spec cat.snapshots repositories is required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17216</link><project id="" key="" /><description /><key id="142312761">17216</key><summary>rest spec cat.snapshots repositories is required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2016-03-21T10:49:43Z</created><updated>2016-03-25T13:25:28Z</updated><resolved>2016-03-25T13:25:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fast Vector Highlighter doesn't work on phrase searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17215</link><project id="" key="" /><description>When searching with **simple_query_string** or **match_phrase** on a field defined with `"term_vector":"with_positions_offsets"` the highlighter doesn't produce a fragment despite the document being found. This happens when a stop-word appears in the query.

Easily reproducible:

```
PUT /test
{
  "mappings" : {
    "doc" : {
      "properties" : {
        "text" : {
          "analyzer" : "english",
          "term_vector" : "with_positions_offsets",
          "type" : "string"
        }
}}}}

PUT /test/doc/1
{
  "text" : "All humans are mortal. Socrates is human. Thus, Socrates is mortal."
}

GET /test/doc/_search
{
  "query" : {
    "simple_query_string" : { "query" : "\"humans are mortal\"", "fields" : ["text"] }
  },
  "highlight" : { "fields" : { "text" : {} }  },
  "_source" : false
}
```

The issue is reproducible in v1.6 and v2.2.1

At some point it has been claimed the issue can be worked around by setting `"enable_position_increments": false`, unfortunately this option is no longer accepted by ElasticSearch.
</description><key id="142312087">17215</key><summary>Fast Vector Highlighter doesn't work on phrase searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jacool</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2016-03-21T10:45:53Z</created><updated>2016-11-25T15:55:00Z</updated><resolved>2016-11-25T15:55:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-25T13:18:42Z" id="201273523">Agreed, this is a bug.  All I can offer you at the moment is to use a slop value, which results in highlighting.
</comment><comment author="jacool" created="2016-03-27T18:44:29Z" id="202123098">It fails the same with slops, just replace my query above with this:
"query" : "\"humans are mortal thus\"~3"
</comment><comment author="clintongormley" created="2016-11-25T15:54:59Z" id="262985264">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update getting-started.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17214</link><project id="" key="" /><description>As i am using command prompt, it only works is i replace single quote with double quote
</description><key id="142304499">17214</key><summary>Update getting-started.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2016-03-21T10:09:52Z</created><updated>2016-06-17T15:41:37Z</updated><resolved>2016-06-17T15:41:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T16:43:29Z" id="206458437">Hi, thanks for the submission!

Could I ask you to sign the [CLA](https://www.elastic.co/contributor-agreement/) so we can merge this in?

Also, I think it might go better in a separate code block below the other one, so the "For Windows ..." part doesn't look like code.
</comment><comment author="clintongormley" created="2016-06-17T15:41:37Z" id="226804203">CLA not signed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suboptimal shard allocation with imbalanced shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17213</link><project id="" key="" /><description>**Elasticsearch version**:

```
# elasticsearch --version
Version: 2.2.0, Build: 8ff36d1/2016-01-27T13:32:39Z, JVM: 1.8.0_72-internal
```

**JVM version**:

```
# java -version
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**: Debian Jessie on kernel 4.1.3.

**Description of the problem including expected versus actual behavior**:

Elasticsearch does not try to spread shards for new indices equally if one node has less shards than others.

Elasticsearch on on of 12 machines died early in the morning (UTC) and caused several indices go red, even though each index has 1 replica. All of the red indices were today's indices. This is a separate issue, this one is about allocation. I unmounted the bad disk on unlucky node and restarted it, roughly 6 hours after the incident. Indices did not recover.

Since there wasn't much to do at this point, I've dropped red indices to let them refill from kafka. At the same time elasticsearch started recovery procedure to rebalance shards evenly. This happened for new indices:

![image](https://cloud.githubusercontent.com/assets/89186/13914778/88dac652-ef48-11e5-8384-307a9fc12c12.png)

This is how node list looked like:

![image](https://cloud.githubusercontent.com/assets/89186/13914781/8fa1d318-ef48-11e5-8f2c-27c886354492.png)

It seems like elasticsearch decided to ignore the fact that it's going to rebalance shards anyway and allocated more shards to the empties node. This caused much higher load on this node and made it a bottleneck.

Another issue is that all shards disappeared. I think the following happened:
1. After the incident elasticsearch recovered available indices to have 2 copies on healthy nodes.
2. After bad node rejoined it had the same copies removed since they were redundant.
3. Since bad node has less shards now, it started recovering inactive shards from old indices that were just removed from it.

This looks suboptimal if it's right. This could be #17019 again, since I run optimize and flush every night and it flushed sync happened after the disk failure.
</description><key id="142297987">17213</key><summary>Suboptimal shard allocation with imbalanced shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2016-03-21T09:45:19Z</created><updated>2016-03-25T13:32:55Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-03-21T10:53:47Z" id="199223920">The balancer currently has separate steps for:
1) allocating unassigned shards (This happens when a new index is created) and
2) rebalancing shards in the cluster
This means that if an empty node joins the cluster it will receive a large share of the new shards in step 1 while also receiving shards that come in due to rebalancing in step 2. This results in increased load on that node if the shards in 1) are hot (see also discussion in #12279 and solutions discussed). Can you confirm that the newly created shards are indeed hot shards (get more load due to heavy indexing etc.)?

Note that as an initial step towards integrating steps 1 and 2 better, I've recently removed these separate steps on the interface level of the balancer (which were separate methods) so that we have the flexibility to have them collaborate better on the implementation level #17028.

Our solution to the second issue is currently the use of `index.unassigned.node_left.delayed_timeout` (see https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html). How long was the bad node down? If it is down for a longer time than the timeout **AND** we have again 2 copies on healthy nodes, we indeed remove the old shard copies from the node even though we might in the future balance one of the copies to that node. Accounting for this specific scenario would add quite some complexity to the balancer. In the presence of throttling, this could also introduce new issues (e.g. wasting space on the new node when cluster is running tight on disk space).
</comment><comment author="bobrik" created="2016-03-21T11:03:38Z" id="199226605">&gt; Can you confirm that the newly created shards are indeed hot shards (get more load due to heavy indexing etc.)?

Yes, 100% ingestion happens on new indices since they are rotated daily.

`index.unassigned.node_left.delayed_timeout` is set to `20m`, node was down for a few hours. I understand that accounting for rebalancing introduces more complexity, but copying terabytes of data when you don't need to copy anything is not something you want to see.

Not sure how you could waste space with throttling. My idea is that you can remove extra copies, but avoid doing that on only one node. Since you remove extra copies anyway, how come you waste more space? It's the opposite: you free space equally on each node instead of freeing it all on one node.

Imagine cluster with 3 nodes and completely idle indices. I kill one node and let the cluster go green again. Once it's green, I start killed node back.

Expected: equal number of extra shards removed from each node, no rebalancing.
Actual: everything is removed from rejoined node and rebalancing kicks in.

Does this make sense?
</comment><comment author="bobrik" created="2016-03-21T12:33:53Z" id="199252576">To give you an idea how bad it is for indexing, all shards were moved away from the fresh node at 12:30:

![image](https://cloud.githubusercontent.com/assets/89186/13918751/13aca8dc-ef61-11e5-9cf4-39479ac7b956.png)

Y axis is messages per second consumed from each kafka partition.
</comment><comment author="ywelsch" created="2016-03-21T12:59:31Z" id="199260255">I understand your suggestion of no-copy balancing. In terms of implementation, we will have to investigate how much complexity this adds. The biggest impact on ingestion here seems to be first issue though (too many hot shards on one node). To address this in the short-term I suggest pre-allocating indices.
</comment><comment author="nik9000" created="2016-03-21T13:19:08Z" id="199271444">Another work around is to set [total_shards_per_node](https://www.elastic.co/guide/en/elasticsearch/reference/current/allocation-total-shards.html). You have to be careful with it because it is a rule rather than a suggestion, but it is useful. If the hot shards aren't all one index, well, total_shards_per_node will help some, but isn't perfect there. I have on my list to file an issue about getting it (or something like it) to work across indexes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dictionary_decompounder loading fails : "IOException while reading word_list_path: Input length = 1"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17212</link><project id="" key="" /><description>Not sure if I am missing something here (e.g. incorrect format of dictionary file? see below) ,however Loading a dictionary file for `dictionary_decompounder` as documented in our [docs](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/analysis-compound-word-tokenfilter.html#analysis-compound-word-tokenfilter) fails.

ES version

```
{
  "name": "Jean Grey",
  "cluster_name": "elasticsearch",
  "version": {
    "number": "2.2.1",
    "build_hash": "d045fc29d1932bce18b2e65ab8b297fbf6cd41a1",
    "build_timestamp": "2016-03-09T09:38:54Z",
    "build_snapshot": false,
    "lucene_version": "5.4.1"
  },
  "tagline": "You Know, for Search"
}
```

Repro steps

1) place dictionary file and FOP XML hyphenation pattern file under $ES_HOME/config
2) launch

```
PUT my_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "myAnalyzer2": {
            "type": "custom",
            "tokenizer": "standard",
            "filter": [
              "myTokenFilter2"
            ]
          }
        },
        "filter": {
          "myTokenFilter2": {
            "type": "hyphenation_decompounder",
            "hyphenation_patterns_path": "de.xml",
            "word_list_path": "germany.txt",
            "max_subword_size": 22
          }
        }
      }
    }
  },
  "mappings": {
    "type1": {
      "properties": {
        "field1": {
          "type": "string",
          "anlyzer": "myAnalyzer2"
        }
      }
    }
  }
} 
```

Response

```
{
   "error": {
      "root_cause": [
         {
            "type": "index_creation_exception",
            "reason": "failed to create index"
         }
      ],
      "type": "illegal_argument_exception",
      "reason": "IOException while reading word_list_path: Input length = 1"
   },
   "status": 400
}
```

Dictionary file at http://www.md5this.com/wordlists/dictionary_german.zip

```
Antonios-MacBook-Air:elasticsearch-2.2.1 abonuccelli$ ls -alrth config/germany.txt 
-rw-r--r--@ 1 abonuccelli  wheel    20M Mar 21 16:13 config/germany.txt
Antonios-MacBook-Air:elasticsearch-2.2.1 abonuccelli$ head config/germany.txt &amp;&amp; tail config/germany.txt &amp;&amp; wc -l config/germany.txt 
00brucellosis
00faa
00kiribati
00mag
00murree
00whitebait
01
013
016
019
?ppigeren
?ppigerer
?ppigeres
?ppiges
?ppigkeit
?ppigste
?ppigstem
?ppigsten
?ppigster
?ppigstes
 1744388 config/germany.txt
```

FOP XML hyphenation pattern file downloaded from site referenced in docs.https://sourceforge.net/projects/offo/files/offo-hyphenation/1.2/offo-hyphenation_v1.2.zip/download

Full Trace Exception (9 More...swallowed?)

```
elasticsearch.log-[2016-03-21 16:25:38,683][DEBUG][cluster.service          ] [Straw Man] cluster state update task [create-index [my_index], cause [api]] failed
elasticsearch.log:[my_index] IndexCreationException[failed to create index]; nested: IllegalArgumentException[IOException while reading word_list_path: Input length = 1];
elasticsearch.log-  at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:360)
elasticsearch.log-  at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$1.execute(MetaDataCreateIndexService.java:309)
elasticsearch.log-  at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
elasticsearch.log-  at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:458)
elasticsearch.log-  at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
elasticsearch.log-  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
elasticsearch.log-  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
elasticsearch.log-  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
elasticsearch.log-  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
elasticsearch.log-  at java.lang.Thread.run(Thread.java:745)
elasticsearch.log:Caused by: java.lang.IllegalArgumentException: IOException while reading word_list_path: Input length = 1
elasticsearch.log-  at org.elasticsearch.index.analysis.Analysis.getWordList(Analysis.java:241)
elasticsearch.log-  at org.elasticsearch.index.analysis.Analysis.getWordSet(Analysis.java:209)
elasticsearch.log-  at org.elasticsearch.index.analysis.compound.AbstractCompoundWordTokenFilterFactory.&lt;init&gt;(AbstractCompoundWordTokenFilterFactory.java:49)
elasticsearch.log-  at org.elasticsearch.index.analysis.compound.HyphenationCompoundWordTokenFilterFactory.&lt;init&gt;(HyphenationCompoundWordTokenFilterFactory.java:52)
elasticsearch.log-  at sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)
elasticsearch.log-  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
elasticsearch.log-  at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
elasticsearch.log-  at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
elasticsearch.log-  at org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:236)
elasticsearch.log-  at com.sun.proxy.$Proxy19.create(Unknown Source)
elasticsearch.log-  at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:161)
elasticsearch.log-  at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:66)
elasticsearch.log-  at sun.reflect.GeneratedConstructorAccessor28.newInstance(Unknown Source)
elasticsearch.log-  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
elasticsearch.log-  at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
elasticsearch.log-  at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
elasticsearch.log-  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
elasticsearch.log-  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
elasticsearch.log-  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
elasticsearch.log-  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
elasticsearch.log-  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
elasticsearch.log-  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:159)
elasticsearch.log-  at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
elasticsearch.log-  at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:358)
elasticsearch.log-  ... 9 more
elasticsearch.log-[2016-03-21 16:25:38,683][DEBUG][action.admin.indices.create] [Straw Man] [my_index] failed to create
elasticsearch.log:[my_index] IndexCreationException[failed to create index]; nested: IllegalArgumentException[IOException while reading word_list_path: Input length = 1];
elasticsearch.log-  at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:360)
elasticsearch.log-  at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$1.execute(MetaDataCreateIndexService.java:309)
elasticsearch.log-  at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
elasticsearch.log-  at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:458)
elasticsearch.log-  at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
elasticsearch.log-  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
elasticsearch.log-  at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
elasticsearch.log-  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
elasticsearch.log-  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
elasticsearch.log-  at java.lang.Thread.run(Thread.java:745)
elasticsearch.log:Caused by: java.lang.IllegalArgumentException: IOException while reading word_list_path: Input length = 1
elasticsearch.log-  at org.elasticsearch.index.analysis.Analysis.getWordList(Analysis.java:241)
elasticsearch.log-  at org.elasticsearch.index.analysis.Analysis.getWordSet(Analysis.java:209)
elasticsearch.log-  at org.elasticsearch.index.analysis.compound.AbstractCompoundWordTokenFilterFactory.&lt;init&gt;(AbstractCompoundWordTokenFilterFactory.java:49)
elasticsearch.log-  at org.elasticsearch.index.analysis.compound.HyphenationCompoundWordTokenFilterFactory.&lt;init&gt;(HyphenationCompoundWordTokenFilterFactory.java:52)
elasticsearch.log-  at sun.reflect.GeneratedConstructorAccessor29.newInstance(Unknown Source)
elasticsearch.log-  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
elasticsearch.log-  at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
elasticsearch.log-  at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
elasticsearch.log-  at org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:236)
elasticsearch.log-  at com.sun.proxy.$Proxy19.create(Unknown Source)
elasticsearch.log-  at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:161)
elasticsearch.log-  at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:66)
elasticsearch.log-  at sun.reflect.GeneratedConstructorAccessor28.newInstance(Unknown Source)
elasticsearch.log-  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
elasticsearch.log-  at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
elasticsearch.log-  at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
elasticsearch.log-  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
elasticsearch.log-  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
elasticsearch.log-  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
elasticsearch.log-  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
elasticsearch.log-  at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
elasticsearch.log-  at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
elasticsearch.log-  at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
elasticsearch.log-  at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
elasticsearch.log-  at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
elasticsearch.log-  at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:159)
elasticsearch.log-  at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
elasticsearch.log-  at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:358)
elasticsearch.log-  ... 9 more
```

I've run lsof on the germany.txt file checking for other process accessing it but can't see anything

```
Antonios-MacBook-Air:elasticsearch-2.2.1 abonuccelli$ while true; do sudo lsof /opt/elk/PROD/elasticsearch-2.2.1/config/de.xml ; sudo lsof /opt/elk/PROD/elasticsearch-2.2.1/config/germany.txt;date;done
Mon Mar 21 16:25:22 CST 2016
Mon Mar 21 16:25:23 CST 2016
Mon Mar 21 16:25:25 CST 2016
Mon Mar 21 16:25:26 CST 2016
Mon Mar 21 16:25:27 CST 2016
Mon Mar 21 16:25:29 CST 2016
Mon Mar 21 16:25:31 CST 2016
Mon Mar 21 16:25:33 CST 2016
Mon Mar 21 16:25:34 CST 2016
Mon Mar 21 16:25:36 CST 2016
Mon Mar 21 16:25:37 CST 2016
Mon Mar 21 16:25:39 CST 2016
Mon Mar 21 16:25:40 CST 2016
Mon Mar 21 16:25:42 CST 2016
Mon Mar 21 16:25:43 CST 2016
Mon Mar 21 16:25:45 CST 2016
```
</description><key id="142288233">17212</key><summary>dictionary_decompounder loading fails : "IOException while reading word_list_path: Input length = 1"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels><label>bug</label><label>discuss</label></labels><created>2016-03-21T08:50:21Z</created><updated>2016-03-22T09:58:24Z</updated><resolved>2016-03-21T12:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-21T12:40:26Z" id="199254783">wrong encoding.
</comment><comment author="s1monw" created="2016-03-21T15:04:12Z" id="199328646">What @rmuir meant is that the file is not UTF-8 encoded. All files that ES accepts must be UTF-8
</comment><comment author="rmuir" created="2016-03-21T15:21:41Z" id="199336746">By the way, i have the feeling an exception may be discarded here.

I feel like it should not be `java.lang.IllegalArgumentException: IOException while reading word_list_path: Input length = 1`, that is not a good error. I just happen to know what it means.

Per the charset api (https://docs.oracle.com/javase/7/docs/api/java/nio/charset/CodingErrorAction.html#REPORT), the original exception should have been a subclass of  CharacterCodingException, such as MalformedInputException. This would make these errors easier to understand if we can improve that.
</comment><comment author="s1monw" created="2016-03-22T09:58:24Z" id="199727901">@rmuir it's fine in master but in 2.x we shadow this exception. 5.x already puts the `MalformedInputException` as the cause  - I will put up a PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify expunge deletes option on completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17211</link><project id="" key="" /><description>It's confusing that we advertise `expunge deletes` as an option
without clarifying it's relationship to the merge policy. By default
segments with less than `10%` deleted docs will not be merged / selected
for merge. This means, deletes don't _go away_ in the suggester.

Relates to #7761
</description><key id="142285029">17211</key><summary>Clarify expunge deletes option on completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>docs</label><label>v1.7.5</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.2</label><label>v2.3.0</label><label>v2.4.0</label></labels><created>2016-03-21T08:33:58Z</created><updated>2016-03-29T09:51:43Z</updated><resolved>2016-03-21T09:02:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-03-21T08:55:33Z" id="199181573">LGTM, just minor comments, thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17210</link><project id="" key="" /><description /><key id="142275653">17210</key><summary>2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yipen9</reporter><labels /><created>2016-03-21T07:29:31Z</created><updated>2016-03-21T07:36:24Z</updated><resolved>2016-03-21T07:36:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-21T07:36:24Z" id="199157604">@yipen9 I presume this was done by mistake and will close it. If not please let us know what you meant
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up "no such index" error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17209</link><project id="" key="" /><description>Currently if you hit a cluster with a request for a non existent index you get a pretty verbose error;

```
[2016-03-21 03:02:52,862][INFO ][rest.suppressed          ] /doesntexist/_search Params: {index=doesntexist}
[doesntexist] IndexNotFoundException[no such index]
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:586)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:133)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:113)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.&lt;init&gt;(TransportSearchTypeAction.java:121)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:73)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:67)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:64)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:53)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:99)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:44)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:70)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
        at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:52)
        at org.elasticsearch.rest.BaseRestHandler$HeadersAndContextCopyClient.doExecute(BaseRestHandler.java:83)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351)
        at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:574)
        at org.elasticsearch.rest.action.search.RestSearchAction.handleRequest(RestSearchAction.java:84)
        at org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:54)
        at org.elasticsearch.rest.RestController.executeHandler(RestController.java:207)
        at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:166)
        at org.elasticsearch.http.HttpServer.internalDispatchRequest(HttpServer.java:128)
        at org.elasticsearch.http.HttpServer$Dispatcher.dispatchRequest(HttpServer.java:86)
        at org.elasticsearch.http.netty.NettyHttpServerTransport.dispatchRequest(NettyHttpServerTransport.java:363)
        at org.elasticsearch.http.netty.HttpRequestHandler.messageReceived(HttpRequestHandler.java:63)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.messageReceived(HttpPipeliningHandler.java:60)
        at org.jboss.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:88)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:145)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.codec.http.HttpContentDecoder.messageReceived(HttpContentDecoder.java:108)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
        at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:435)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

Can we just report back the first line here? - `[2016-03-21 03:02:52,862][INFO ][rest.suppressed          ] /doesntexist/_search Params: {index=doesntexist}
[doesntexist] IndexNotFoundException[no such index]`
</description><key id="142242194">17209</key><summary>Clean up "no such index" error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2016-03-21T03:04:23Z</created><updated>2016-03-21T03:25:18Z</updated><resolved>2016-03-21T03:23:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-21T03:23:23Z" id="199096701">Duplicates #16622, closed by #16627
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Install plugin permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17208</link><project id="" key="" /><description>This pull request refactors the handling of POSIX attributes when installing plugins. In particular, an exemption is no longer made merely for Windows, but instead any filesystem that does not provide POSIX support is properly handled.

Additionally, the unit tests are refactored to use mock filesystems (in addition to the native filesystem!) so that breaking issues are more likely to be caught regardless of which OS the unit tests are run on. The tests are also updated to make assertions regarding the POSIX attributes of the plugins directory, the bin directory, and the config directory. These assertions capture recent changes to these attributes.

Lastly, the checkstyle line-length violations in InstallPluginCommand.java are fixed and the suppression for this check for this file is removed.

Closes #17201
</description><key id="142236262">17208</key><summary>Install plugin permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Plugins</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-21T02:01:29Z</created><updated>2016-03-24T17:38:32Z</updated><resolved>2016-03-23T22:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-21T02:52:19Z" id="199090266">Seems good to me. OTOH I didn't remember windows for the permissions when I reviewed the last ones and I'm tired. So maybe don't trust me?
</comment><comment author="nik9000" created="2016-03-23T13:22:33Z" id="200341609">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[bug report]transport_address shows wired</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17207</link><project id="" key="" /><description>**Elasticsearch version**:1.7.0

**JVM version**:8u40

**OS version**:CentOS 6.6

when I execute _nodes/stats for my cluster, all nodes shows:
most nodes show: inet[/ipaddress:port]
but the only one show: inet[/ipaddress/ipaddress:port]
is this a minor bug of ES?
</description><key id="142234036">17207</key><summary>[bug report]transport_address shows wired</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-03-21T01:42:17Z</created><updated>2016-03-22T15:41:17Z</updated><resolved>2016-03-22T02:26:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-21T02:03:22Z" id="199085327">Can you please give an actual example and elaborate on what it is that you're expecting?
</comment><comment author="makeyang" created="2016-03-22T01:39:21Z" id="199575418">I can't show u the exact ip address. thing is: when execute _nodes/stats, most nodes show below:
"transport_address": "inet[/172.0.0.1:9300]",but there is one node shows:
"transport_address": "inet[172.0.0.2/172.0.0.2:9300]",
I guess the format shoud be ip:port, but why this one shows ip/ip:port
</comment><comment author="jasontedor" created="2016-03-22T02:26:44Z" id="199590695">It's not a bug. The `InetSocketTransportAddress` wraps a `InetSocketAddress` which will print `hostname/addr:port` if both `hostname` and `addr` are available but otherwise just `/addr:port`. I suspect this is due to your configuration.
</comment><comment author="makeyang" created="2016-03-22T02:35:50Z" id="199595444">@jasontedor  
can you help to be more specified?
how did es get hostname?
how did es get addr ?
</comment><comment author="jasontedor" created="2016-03-22T02:41:01Z" id="199598892">I can't because I'm certain this is highly dependent on your configuration.
</comment><comment author="makeyang" created="2016-03-22T02:44:09Z" id="199600586">will, which key in config/enviromemnt variable decide hostname and which decide addr?
</comment><comment author="jasontedor" created="2016-03-22T02:51:25Z" id="199603051">Various network settings such as `network.host`, the `publish_host` settings, and _your_ local DNS configuration _all_ play a r&#244;le here.
</comment><comment author="makeyang" created="2016-03-22T03:29:52Z" id="199615236">maybe and perhaps, which are not good.
by the way, can you just give a certain way to reproduce it? for one node to shows "transport_address": "inet[172.0.0.2/172.0.0.2:9300]"?
</comment><comment author="jasontedor" created="2016-03-22T15:35:35Z" id="199869176">&gt; maybe and perhaps, which are not good.

Why? I'm sorry, but I'm completely missing why this is such a big deal. 

&gt; for one node to shows "transport_address": "inet[172.0.0.2/172.0.0.2:9300]"?

For that node, you need to look at your network settings (`network.host`, `*.publish_host`, etc.) and all relevant DNS configuration (on both your DNS server and `/etc/hosts`). Are you using a cloud provider with a custom resolver?

I do not see an issue here, and the best place for any future discussion on this is on the [Elastic Discourse forums](https://discuss.elastic.co).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gender neutral README</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17206</link><project id="" key="" /><description>Reduce confusion in situations where a twitter user isn't a man
</description><key id="142218273">17206</key><summary>Gender neutral README</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlhunter</reporter><labels><label>docs</label><label>enhancement</label></labels><created>2016-03-20T23:00:28Z</created><updated>2016-03-24T14:26:02Z</updated><resolved>2016-03-24T14:25:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-21T08:17:41Z" id="199168397">I can only merge this if you sign the CLA - can you please do so?
</comment><comment author="tlhunter" created="2016-03-23T22:06:40Z" id="200563283">I've signed the CLA
</comment><comment author="s1monw" created="2016-03-24T14:26:02Z" id="200861477">merged thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to using refactored SortBuilder instead of using BytesReference in serialization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17205</link><project id="" key="" /><description>This is based on #17146 that needs to get in first. It moves the remaining parsing logic for the complete sort-element over to SortBuilder#fromXContent and moves further logic that used to be in SortParseElement concerned with building SortFields and populating the search context with it also to SortBuilder. 
In a last step for the refactoring the current workaround of using opaque BytesReference lists in SearchSourceBuilder and similar places it removed and the refactored SortBuilders used instead.

Closes #17257
</description><key id="142199297">17205</key><summary>Switch to using refactored SortBuilder instead of using BytesReference in serialization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-20T20:07:51Z</created><updated>2016-03-24T16:24:06Z</updated><resolved>2016-03-23T15:25:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-20T20:11:34Z" id="199008509">@MaineC this is still WIP since it relies on first adding the build() methods to the SortBuilders (see linked PR), but I went ahead to see how much is missing to really cut over to using the refactored SortBuilders in the SearchSourceBuilder and was able to get it working by moving over most of the current parsing logic into SortBuilder. Unfortunately this makes it a quiet large PR, but half of the changes come from #17146 and should go away once that is in.
</comment><comment author="cbuescher" created="2016-03-23T13:06:42Z" id="200337870">@MaineC after merging the two related PRs, this PR should only contain the changes that where necessary to make the switch from List&lt;BytesReference&gt; to List&lt;SortBuilder&gt; in SearchSourceBuilder and related similar places where we used List&lt;BytesReference&gt; for the sort. Some IT tests failures revealed problems (like storing "missing" String as BytesRef in FieldSortBuilder) that are also addressed here. I think this is ready to take a look.
</comment><comment author="MaineC" created="2016-03-23T14:28:45Z" id="200367842">Just went over the code. Really love the amount of code that goes away after this is merged. Also like the added tests, even if checks are kept simple at the moment. So here's my LGTM 

Caveats: Didn't run the test suite here. Also we might want to add a ticket that describes where tests could be improved (like e.g. more assertions) so those don't get forgotten.
</comment><comment author="cbuescher" created="2016-03-23T14:29:16Z" id="200368007">@MaineC @colings86 I added the last commit to show that we can now again parse syntax mentioned in https://github.com/elastic/elasticsearch/issues/17257
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias items are not ignored anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17204</link><project id="" key="" /><description>Get TermVectorResponses for like and unlike items in separate requests, so we don't have to validate responses afterwards.

Relates to #14944
</description><key id="142188370">17204</key><summary>Alias items are not ignored anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mstockerl</reporter><labels><label>:More Like This</label><label>bug</label><label>review</label><label>v2.3.2</label></labels><created>2016-03-20T18:18:27Z</created><updated>2016-04-25T14:21:17Z</updated><resolved>2016-04-20T09:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T16:44:37Z" id="206458978">Since this is for the term vectors API, maybe @brwe would be a good person to review?
</comment><comment author="brwe" created="2016-04-19T14:23:30Z" id="211943373">LGTM. Left a minor comment but I will merge tomorrow as is in case you cannot do it until then. Thanks for fixing this! Also, sorry for the late reply.
</comment><comment author="mstockerl" created="2016-04-19T18:25:27Z" id="212055816">Thanks for your comment @brwe. I removed the obsolete method.
</comment><comment author="brwe" created="2016-04-20T10:46:48Z" id="212377336">Hm. While backporting to 2.x I now found that while this commit fixes the issue it also causes mlt to now make two term vector requests instead of one. This might not be a big deal but still decreases performance a little. Would like to hear what others say. I am inclined to backport anyway and fix that later but unsure.
</comment><comment author="brwe" created="2016-04-20T13:14:33Z" id="212418889">Had a brief discussion with @jpountz who agrees that the decrease in performance is probably negligible. Backported to 2.x 2.3 and 2.2 (e4c20ab, 884ad25 and 4dcdd95). Thanks again @mstockerl !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add limit to total number of fields in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17203</link><project id="" key="" /><description>Closes #13547
This is to prevent mapping explosion when dynamic keys such as UUID are used as field names. index.mapping.total_fields.limit specifies the total number of fields an index can have. An exception will be thrown when the limit is reached. The default limit is 0 which means no limit. This setting is runtime adjustable.
</description><key id="142074125">17203</key><summary>Add limit to total number of fields in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">yanjunh</reporter><labels><label>:Mapping</label><label>enhancement</label><label>resiliency</label><label>review</label></labels><created>2016-03-19T15:43:03Z</created><updated>2016-03-27T05:54:30Z</updated><resolved>2016-03-27T05:54:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-22T15:12:20Z" id="199859182">Thanks @yanjunh. Would you mind adding some docs a test that enables the limit and makes sure that you get an exception when trying to add more fields? (like #15989)
</comment><comment author="yanjunh" created="2016-03-24T06:48:04Z" id="200697388">@jpountz Sure. I will take a look this weekend. thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>painful!!! postgres--&gt;logstash--&gt;elasticsearch  mapper_parsing_exception!!</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17202</link><project id="" key="" /><description>hi, I am so sorry to  bother you, but now I am painful and rage about a problem. the problem has spent more that one weeks. and so sad I cann't find any useful information, even if I sent an email to nicerobot because I saw his questions linked  https://github.com/elastic/elasticsearch/issues/6492. but no response.  anyone any comments and suggestions, I really appreciate that.

The exception information is(when I start the logstash) : 
## "@version"=&gt;"1", "@timestamp"=&gt;"2016-03-19T06:13:00.399Z", "coordinates"=&gt;[[#&lt;BigDecimal:4b06d615,'0.1194470174E3',10(12)&gt;, #&lt;BigDecimal:6f1f846d,'0.303230396E2',9(12)&gt;], [#&lt;BigDecimal:5cb7510d,'0.1194470149E3',10(12)&gt;, #&lt;BigDecimal:2d385b32,'0.303229822E2',9(12)&gt;], [#&lt;BigDecimal:50f396fe,'0.1194470161E3',10(12)&gt;, #&lt;BigDecimal:43dc4c61,'0.303229168E2',9(12)&gt;], [#&lt;BigDecimal:2585422e,'0.1194470219E3',10(12)&gt;, #&lt;BigDecimal:4a8b6c67,'0.303228748E2',9(12)&gt;], [#&lt;BigDecimal:60357ae,'0.119447031E3',9(12)&gt;, #&lt;BigDecimal:271dac4b,'0.303228477E2',9(12)&gt;], [#&lt;BigDecimal:28861eb4,'0.1194470463E3',10(12)&gt;, #&lt;BigDecimal:39ffb797,'0.303228234E2',9(12)&gt;], [#&lt;BigDecimal:255d2921,'0.1194470789E3',10(12)&gt;, #&lt;BigDecimal:6f68a635,'0.30322786E2',8(12)&gt;], [#&lt;BigDecimal:778366d2,'0.1194471988E3',10(12)&gt;, #&lt;BigDecimal:330434ba,'0.303227042E2',9(12)&gt;], [#&lt;BigDecimal:7fdc07da,'0.1194472167E3',10(12)&gt;, #&lt;BigDecimal:55af4f1a,'0.303226957E2',9(12)&gt;]]}, "id"]}&gt;&gt;], :response=&gt;{"index"=&gt;{"_index"=&gt;"dataservice-search", "_type"=&gt;"dataservice-search", "_id"=&gt;"28323", "status"=&gt;400, "error"=&gt;{"type"=&gt;"mapper_parsing_exception", "reason"=&gt;"failed to parse [geometry]", "caused_by"=&gt;{"type"=&gt;"parse_exception", "reason"=&gt;"shape must be an object consisting of type and coordinates"}}}}, :level=&gt;:warn}

Firstly: logstash is used to export data from postgres. and my cocnfiguration file is : 

input {
    jdbc {
        # Postgres jdbc connection string to our database, mydb
        jdbc_connection_string =&gt; "jdbc:postgresql://**.**.**.**:5432/handle_service"
        # The user we wish to execute our statement as
        jdbc_user =&gt; "user"
        jdbc_password =&gt; "*****_"
        # The path to our downloaded jdbc driver
        jdbc_driver_library =&gt; "/home/admin/dataservice-search/elasticsearch-2.1.1/lib/postgresql-9.3-1102-jdbc41.jar"
        # The name of the driver class for Postgresql
        jdbc_driver_class =&gt; "org.postgresql.Driver"
        parameters =&gt; { "favorite_artist" =&gt; "Beethoven" }
        schedule =&gt; "_ \* \* \* _"
        # our query
        statement =&gt; " select st_asgeojson(st_makevalid(geom)) As geometry_offset_geo ,_ from t_delete_road limit 20"
        type =&gt; "datasearch"
    }

}
output {
    stdout {codec =&gt; rubydebug }
        elasticsearch {
        index =&gt; "datasearch"
        document_type =&gt; "datasearch"
        document_id =&gt; "%{id}"
        hosts =&gt; [ "**.**.**.**:9200" ]
    }
}

second, I created the index in the elasticsearch:
curl -XPOST http://**.**.**.**:9200/datasearch/datasearch/_mapping  -d '{"datasearch":{"properties" : {"geometry_offset_geo":{"type":"geo_shape","tree":"quadtree","precision": "1m"}}}} '

third, I start the logstash. and then something was wrong.

But it's works when I use 
curl -XPOST 'http://**.**.**.**:9200/datasearch/datasearch/1' -d '{"geometry":{
"type":"Polygon",
"coordinates":[
[[-85.0018514,37.1311314],
[-85.0016645,37.1315293],
[-85.0016246,37.1317069],
[-85.0016526,37.1318183],
[-85.0017119,37.1319196],
[-85.0019371,37.1321182],
[-85.0019972,37.1322115],
[-85.0000002,37.1317672]]]
}
}'

I really wonder what happen among those steps. so I come here and ask for some help, please.
It's so important to me to solve this problem.

thank you so much!
</description><key id="142067129">17202</key><summary>painful!!! postgres--&gt;logstash--&gt;elasticsearch  mapper_parsing_exception!!</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wharstr9027</reporter><labels /><created>2016-03-19T14:38:24Z</created><updated>2016-03-20T03:23:45Z</updated><resolved>2016-03-19T14:53:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-19T14:53:08Z" id="198717264">Elastic reserves GitHub for bug reports and feature requests but provides a [Discourse forum](https://discuss.elastic.co) and [IRC channels](https://www.elastic.co/community) for general questions.

&gt; third, I start the logstash. and then something was wrong.

When you do reach out to the community for help, you should provide more details about what you _expected_ to happen, and what _actually_ happened. Maybe your expectations are wrong, maybe what actually happened is not what we intended, but either way we can not assess the situation without understanding what you think should have happened, and what actually happened.
</comment><comment author="wharstr9027" created="2016-03-20T03:23:45Z" id="198839304">ok, I see. thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Evaluate windows permissions in InstallPluginCommand</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17201</link><project id="" key="" /><description>#17176 tightened up directory permissions during plugin installations. Sadly the added permissions are no good on windows as their are POSIX based.   We should evaluate what permissions are required for windows.

See for example http://build-us-00.elastic.co/job/es_core_master_windows-2012-r2/2703/consoleText, failing with: 

```
Exception in thread "main" java.lang.UnsupportedOperationException: 'posix:permissions' not supported as initial attribute
    at sun.nio.fs.WindowsSecurityDescriptor.fromAttribute(WindowsSecurityDescriptor.java:358)
    at sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:492)
    at java.nio.file.Files.createDirectory(Files.java:674)
    at java.nio.file.TempFileHelper.create(TempFileHelper.java:136)
    at java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:173)
    at java.nio.file.Files.createTempDirectory(Files.java:950)
    at org.elasticsearch.plugins.InstallPluginCommand.unzip(InstallPluginCommand.java:252)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:174)
```

For now , permissions are disabled on windows with https://github.com/elastic/elasticsearch/commit/ee95c0a3843434b6fa43c8e448a192118368e6f4
</description><key id="142050739">17201</key><summary>Evaluate windows permissions in InstallPluginCommand</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Plugins</label><label>blocker</label></labels><created>2016-03-19T10:42:33Z</created><updated>2016-03-24T17:38:37Z</updated><resolved>2016-03-23T22:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-19T12:38:20Z" id="198695139">This code uses getFileAttributeView incorrectly: only BasicFileAttributeView is guaranteed. It needs to check for null in any other case.
</comment><comment author="rmuir" created="2016-03-19T12:44:16Z" id="198695439">Same goes with the createTempDirectory+with+attributes call: UOE needs to be handled. Then Constants.WINDOWS should be removed. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy module not included in 2.2.0 maven package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17200</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0

**OS version**: Mac OS X

**Description of the problem including expected versus actual behavior**:

It seems the latest elasticsearch published artifact does not contain the groovy module.

When depending on the elasticsearch 2.2.0 jar using a maven dep like this

``` xml
&lt;dependency&gt;
  &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
  &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
  &lt;version&gt;2.2.0&lt;/version&gt;
&lt;/dependency&gt;
```

the GroovyScriptEngineService class is not on the classpath. Notably, this means that when trying to run putIndexedScript using a node client, it fails with "groovy" not a valid script_lang.

When depending on the elasticsearch 2.1.2 jar, GroovyScriptEngineService exists and the putIndexedScript command works fine (after adding groovy-all as a dependency per https://github.com/elastic/elasticsearch/issues/15867
</description><key id="142045112">17200</key><summary>Groovy module not included in 2.2.0 maven package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anuraaga</reporter><labels /><created>2016-03-19T09:05:59Z</created><updated>2016-03-19T15:48:42Z</updated><resolved>2016-03-19T14:55:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-19T14:55:35Z" id="198717725">The `lang-groovy` module is provided as a separate artifact with coordinates `org.elasticsearch.module:lang-groovy:2.2.0`.
</comment><comment author="anuraaga" created="2016-03-19T15:37:50Z" id="198726680">Thanks for the pointer to the artifact, I could find GroovyScriptEngineService on the classpath now! But it seems with the separation of the modules something has changed in initialization - just having the dependency doesn't seem to be enough to enable groovy support, I still get script_lang not supported [groovy].

I have come up with this weird hack but is their an official way of loading the groovy plugin? Also all the docs indicate that groovy works out of the box, but since it doesn't seem to be true for node clients, it would be nice to mention the appropriate settings in the documentation to avoid issues filed like this one :)

``` java
// HACK: Define this class to expose a protected constructor that let's use
  // initialize GroovyPlugin.
  private static class PluginLoadingNode extends Node {

    private PluginLoadingNode(Settings preparedSettings) {
      super(InternalSettingsPreparer.prepareEnvironment(preparedSettings, null),
          Version.CURRENT,
          ImmutableList.&lt;Class&lt;? extends Plugin&gt;&gt;of(GroovyPlugin.class));
    }
  }
```
</comment><comment author="jasontedor" created="2016-03-19T15:48:15Z" id="198736081">&gt; Also all the docs indicate that groovy works out of the box, but since it doesn't seem to be true for node clients

Correct, when a regular Elasticsearch node starts we look for the `lang-groovy` module (among others) but otherwise do not.

&gt; I have come up with this weird hack but is their an official way of loading the groovy plugin? 

Yes. Note that this is what Elastiscearch does for its tests that depend on plugins. Also see the discussion on #15927.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add limit to total number of fields in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17199</link><project id="" key="" /><description>rework of #13547 on master branch
</description><key id="142015439">17199</key><summary>Add limit to total number of fields in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yanjunh</reporter><labels /><created>2016-03-19T01:42:14Z</created><updated>2016-03-19T14:33:58Z</updated><resolved>2016-03-19T14:33:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yanjunh" created="2016-03-19T14:33:58Z" id="198713730">Better to leave default as no limit for backward compatibility. I am going to close this pull request and raise a new one.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove suggest transport action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17198</link><project id="" key="" /><description>Currently `suggest` through `_search` &amp; `_suggest` endpoints use different transport actions. 
The `_suggest` endpoint uses its own transport action to avoid executing search &amp; aggregration 
phases. This change removes dedicated suggest transport action and adds optimization to 
shortcut to suggest phase when `_search` endpoint is used exclusively for suggestions.

This implies:

``` bash
GET /_suggest
{....}
```

is actually sugar for:

```
GET /_search
{
  "suggest" : ....
}
```

Besides reducing code and complexity of dealing with multiple execution paths for suggest, 
this change has two major benefits:
- simplifies implementing document fetching for `completion`
- allows using the search context to process suggestions
  - we can use `post_filter` / `query` for filtering suggestions in `completion`
  - `completion` can respect `_type` (through contexts)
  - simplify `phrase` suggester parameters 

**API Changes:**

This change removes `suggest` action from the client API:

``` java
client().prepareSuggest().addSuggestion("foo", ...)
```

is equivalent to:

``` java
client().prepareSearch().suggest(new SuggestBuilder().addSuggestion("foo", ...))
```

**Behaviour changes:**
now we use the `search` threadpool instead of `suggest` for suggest-only requests 
(would appreciate some feedback there) and the merged suggest stats are representative 
of all suggest-only requests either from `_suggest` or `_search` endpoints

relates to https://github.com/elastic/elasticsearch/issues/10217
simplifies https://github.com/elastic/elasticsearch/pull/13576
</description><key id="141986432">17198</key><summary>Remove suggest transport action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T21:48:21Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-23T20:39:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-21T19:13:58Z" id="199433135">+100 for the diff stats :)
</comment><comment author="s1monw" created="2016-03-21T19:14:51Z" id="199433801">@areek this only breaks the java API right?
</comment><comment author="areek" created="2016-03-21T19:17:31Z" id="199435309">@s1monw yes, and the `suggest` enum for the search stats request is a no-op as the suggest stats are merged with the search stats.
</comment><comment author="s1monw" created="2016-03-22T08:31:36Z" id="199700198">&gt; @s1monw yes, and the suggest enum for the search stats request is a no-op as the suggest stats are merged with the search stats.

I was more curious about the REST endpoints but I guess I will see in a second when I am reviewing :)
</comment><comment author="s1monw" created="2016-03-22T08:50:19Z" id="199705061">I left some minor comments - this looks fantastic! Please add the stats change to the breaking docs too.

thanks so much for this cleanup
</comment><comment author="areek" created="2016-03-23T18:49:16Z" id="200492148">Thanks @s1monw for the review! I added breaking docs for the java api and merging suggest stats into search stats. 
we don't use the `suggest` threadpool at all now (was used only by the dedicated suggest transport action). any thoughts? if we are ok with using `search` threadpool, we should remove the `suggest` threadpool all together.
</comment><comment author="s1monw" created="2016-03-23T19:07:25Z" id="200499116">+1 on removing suggest threadpool
</comment><comment author="s1monw" created="2016-03-23T19:09:55Z" id="200499793">LGTM - awesome lets remove the threadpool in a sep PR, we also need a breaking doc addition for that
</comment><comment author="areek" created="2016-03-23T20:40:29Z" id="200536034">Thanks @s1monw for the review! I will open a PR for removing suggest threadpool
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch ownership for data, logs, and configs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17197</link><project id="" key="" /><description>This pull request ensures that the data, logs, and configs directories have the correct ownership when Elasticsearch is installed from the packages.

Closes #12688
</description><key id="141959530">17197</key><summary>Elasticsearch ownership for data, logs, and configs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T19:34:36Z</created><updated>2016-03-25T16:35:25Z</updated><resolved>2016-03-25T01:21:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-22T15:16:40Z" id="199860856">LGTM, left one really minor comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>network.host _[networkInterface]_ not working in Centos 6.5</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17196</link><project id="" key="" /><description>&lt;&lt;&lt; Update:
Just realized the machine that I was having issues with was actually running 2.1.1, and the others were 2.2.0.  Issue reproduces with 2.1.1, regardless of the OS.

I could not find an issue to link to, but some commit fixed this issue between 2.1.1 and 2.2.0.


**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_45 (oracle)

**OS version**: Centos 6.5

The following configuration works fine under Ubuntu 14.04, but not under Centos 6.5:

```
network.host:
  - _eth0:ipv4_
  - _local:ipv4_

-----
publish_address {10.0.2.15:9300}, bound_addresses {127.0.0.1:9300}, {10.0.2.15:9300}
```

Same config, Centos 6.5 result: 

```
publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}
```
##### Other items that I tried (which **all** worked under Ubuntu 14.04):
- Works

```
network.host: "_eth0:ipv4_"

-----
publish_address {10.0.2.15:9300}, bound_addresses {10.0.2.15:9300}
```
- Not working:

```
network.host: ["_eth0:ipv4_", "_local_"]

-----
publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
```
- Not working:

```
network.host:
 - "_eth0:ipv4_"

-----
publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
```
- Not working:

```
network.host:
 - _eth0:ipv4_

-----
publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
```
</description><key id="141933441">17196</key><summary>network.host _[networkInterface]_ not working in Centos 6.5</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpcarey</reporter><labels /><created>2016-03-18T17:42:56Z</created><updated>2016-03-19T04:36:56Z</updated><resolved>2016-03-18T23:20:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpcarey" created="2016-03-18T22:40:47Z" id="198569449">adding that it worked fine under Centos7 as well.
</comment><comment author="jpcarey" created="2016-03-18T23:20:02Z" id="198580120">Just realized the machine that I was having issues with was actually running 2.1.1, and the others were 2.2.0.  Issue reproduces with 2.1.1, regardless of the OS.

I could not find an issue to link to, but some commit fixed this issue between 2.1.1 and 2.2.0.
</comment><comment author="dadoonet" created="2016-03-19T04:36:56Z" id="198632185">I think you were looking for https://github.com/elastic/elasticsearch/pull/13954
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cutover to elastic Vagrant boxes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17195</link><project id="" key="" /><description>This commit cuts the Vagrant tests over to the elastic Vagrant boxes.

Relates #16854
</description><key id="141931865">17195</key><summary>Cutover to elastic Vagrant boxes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>blocker</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T17:35:07Z</created><updated>2016-03-25T13:26:13Z</updated><resolved>2016-03-21T12:27:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-18T18:16:18Z" id="198479795">No changes required to the provisioning scripts?! If it works I'm happy with it.
</comment><comment author="elasticdog" created="2016-03-18T18:37:36Z" id="198488882">I'm guessing a lot of those provisioning steps won't cause problems, but are also unnecessary with these images...so they could eventually be simplified/removed.
</comment><comment author="nik9000" created="2016-03-18T19:03:29Z" id="198497204">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy script does not work with 'each' function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17194</link><project id="" key="" /><description>**Elasticsearch version**:

2.2.0

**JVM version**:

1.8.0_73

**OS version**:

Windows

**Description of the problem including expected versus actual behavior**:

Seems that script is cached and in this state stops work with 'each' function.

**Steps to reproduce**:

Simplified example. Please do not seek sense in it )
1. POST http://localhost:9200/testindex/testtype/testid/_update
   `{
   "script" : {
       "inline" : "stats.each { key = it.getKey(); value = stats[key]; ctx._source.visits = value.visits; }",
       "params" : {
           "stats" : {
               "internal" : {
                   "visits" : 10
               },
               "external" : {
                   "visits" : 20
               }
           }
       }
   },
   "upsert" : {
       "visits" : 10
   }
   }`
2. Repeat step 1 until it stops work. It may happen on the second time or on the 20th. You will get an error
   `{
   "error" : {
       "root_cause" : [{
               "type" : "remote_transport_exception",
               "reason" : "[nodename][127.0.0.1:9300][indices:data/write/update[s]]"
           }
       ],
       "type" : "illegal_argument_exception",
       "reason" : "failed to execute script",
       "caused_by" : {
           "type" : "script_exception",
           "reason" : "failed to run inline script [stats.each { key = it.getKey(); value = stats[key];  ctx._source.visits = value.visits; }] using lang [groovy]",
           "caused_by" : {
               "type" : "no_class_def_found_error",
               "reason" : "sun/reflect/MethodAccessorImpl",
               "caused_by" : {
                   "type" : "class_not_found_exception",
                   "reason" : "sun.reflect.MethodAccessorImpl"
               }
           }
       }
   },
   "status" : 400
   }
   `
3. If you change inline script (you can simply insert a space) then it starts work. Then go to step 2.

**Provide logs (if relevant)**:

From elastic log:
`[INFO ][rest.suppressed          ] /testindex/testtype/testid/_update Params: {index=testindex, id=testid, type=testtype}
RemoteTransportException[[nodename][127.0.0.1:9300][indices:data/write/update[s]]]; nested: IllegalArgumentException[failed to execute script]; nested: ScriptException[failed to run inline script [stats.each { key = it.getKey(); value = stats[key]; ctx._source.visits = value.visits; }] using lang [groovy]]; nested: NoClassDefFoundError[sun/reflect/MethodAccessorImpl]; nested: ClassNotFoundException[sun.reflect.MethodAccessorImpl];
Caused by: java.lang.IllegalArgumentException: failed to execute script
    at org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:256)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:196)
    at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:79)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:164)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:65)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:249)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$ShardTransportHandler.messageReceived(TransportInstanceSingleOperationAction.java:245)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: ScriptException[failed to run inline script [stats.each { key = it.getKey(); value = stats[key]; ctx._source.visits = value.visits; }] using lang [groovy]]; nested: NoClassDefFoundError[sun/reflect/MethodAccessorImpl]; nested: ClassNotFoundException[sun.reflect.MethodAccessorImpl];
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:318)
    at org.elasticsearch.action.update.UpdateHelper.executeScript(UpdateHelper.java:251)
    ... 12 more
Caused by: java.lang.NoClassDefFoundError: sun/reflect/MethodAccessorImpl
    at sun.misc.Unsafe.defineClass(Native Method)
    at sun.reflect.ClassDefiner.defineClass(Unknown Source)
    at sun.reflect.MethodAccessorGenerator$1.run(Unknown Source)
    at sun.reflect.MethodAccessorGenerator$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.reflect.MethodAccessorGenerator.generate(Unknown Source)
    at sun.reflect.MethodAccessorGenerator.generateMethod(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325)
    at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1019)
    at groovy.lang.Closure.call(Closure.java:426)
    at groovy.lang.Closure.call(Closure.java:442)
    at org.codehaus.groovy.runtime.DefaultGroovyMethods.callClosureForMapEntry(DefaultGroovyMethods.java:5228)
    at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:2107)
    at org.codehaus.groovy.runtime.dgm$163.doMethodInvoke(Unknown Source)
    at 2fe87ca3b157e7fc7a85946504c7142bec5e87df.run(2fe87ca3b157e7fc7a85946504c7142bec5e87df:1)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:311)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:308)
    ... 13 more
Caused by: java.lang.ClassNotFoundException: sun.reflect.MethodAccessorImpl
    at java.net.URLClassLoader.findClass(Unknown Source)
    at java.lang.ClassLoader.loadClass(Unknown Source)
    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:677)
    at groovy.lang.GroovyClassLoader$InnerLoader.loadClass(GroovyClassLoader.java:425)
    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:787)
    at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:775)
    ... 36 more
`
</description><key id="141912377">17194</key><summary>Groovy script does not work with 'each' function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sss13579</reporter><labels /><created>2016-03-18T16:17:29Z</created><updated>2016-03-18T16:40:05Z</updated><resolved>2016-03-18T16:20:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-03-18T16:20:05Z" id="198433304">It's fixed in ES 2.2.1 (see #16540)
</comment><comment author="sss13579" created="2016-03-18T16:40:05Z" id="198445005">Thank you!
Seems i should check new versions every day )
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document 5.0 mapping changes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17193</link><project id="" key="" /><description /><key id="141909930">17193</key><summary>Document 5.0 mapping changes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T16:06:43Z</created><updated>2016-03-22T15:24:12Z</updated><resolved>2016-03-22T15:24:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-18T16:44:42Z" id="198446872">I left some comments
</comment><comment author="dadoonet" created="2016-03-21T14:10:57Z" id="199301571">Just one ` char to fix. LGTM otherwise
</comment><comment author="jpountz" created="2016-03-22T15:24:09Z" id="199863566">Thanks @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin_name.enabled: false should prevent loading of the plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17192</link><project id="" key="" /><description>Currently, it's up to a plugin to recognize that it has been disabled and avoid initializing its parts.

We should enforce this at the ES level instead and simply avoid even loading the plugin entirely. Now that we require the plugin to have its name provided in the plugin descriptor, we can use that to automatically discover:

``` yaml
plugin_name.enabled: false
```

In that case, we should skip any handling of the plugin and log that it was disabled explicitly.
</description><key id="141894453">17192</key><summary>plugin_name.enabled: false should prevent loading of the plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Plugins</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-18T15:13:44Z</created><updated>2017-03-15T15:47:52Z</updated><resolved>2017-03-15T15:47:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-04-11T20:49:40Z" id="208555805">I don't think this is the job of ES. If a user doesn't want a plugin to run, they can remove it. I don't see any advantages to this functionality, only additional settings and behavior to maintain.
</comment><comment author="pickypg" created="2016-04-11T20:53:24Z" id="208556845">I think that's the right operational strategy.

However, I find that I will sometimes disable different plugins for different use cases -- especially in development -- and it would be good to know that they're not even on the classpath to get the opportunity to change things.
</comment><comment author="dadoonet" created="2016-04-11T20:53:27Z" id="208556864">I agree with Ryan here and actually I don't understand the use case. Is it for debugging reason? Like I want to test elasticsearch without a plugin but then re-enable it again?

Could you explain a bit more why you would like to have this @pickypg ?
</comment><comment author="pickypg" created="2016-04-11T20:53:46Z" id="208556963">Debugging with the intent to re-enable.
</comment><comment author="rjernst" created="2017-03-15T15:47:52Z" id="286784729">While I understand the intent, I don't think it is worth the complexity. If we were to have such a flag built in, plugins might want a way to disable such a flag (so they can maintain their own flag or even flags for multiple features of the plugin).  I'm closing this for now.  We can reopen in the future if new stronger use cases come to mind.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Call ensureOpen on Translog#newView() to prevent IllegalStateException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17191</link><project id="" key="" /><description>If a new view is pulled on a translog that was closed before the currently
committing translog was actually committed we run into odd IllegalStateExceptions
since we can't increment the ref count on the channel. This commit adds a missing
ensureOpen call to provide consistent AlreadyClosedExceptions.
</description><key id="141889704">17191</key><summary>Call ensureOpen on Translog#newView() to prevent IllegalStateException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v2.2.2</label><label>v2.3.0</label><label>v2.4.0</label></labels><created>2016-03-18T14:56:54Z</created><updated>2016-03-29T07:52:22Z</updated><resolved>2016-03-20T19:10:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-18T14:57:44Z" id="198397214">@bleskes  assigned it to you
</comment><comment author="bleskes" created="2016-03-18T16:49:10Z" id="198448745">LGTM. Was this signaled by a build failure?
</comment><comment author="s1monw" created="2016-03-20T19:10:08Z" id="198993881">&gt; LGTM. Was this signaled by a build failure?

I ran into looking at some logfiles... basically by accident :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle missing numeric values for ScriptSort</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17190</link><project id="" key="" /><description>There is an open Todo in ScriptSortParser concerned with handling missung values for the Numeric script type case. Currently it seems we are sorting missing values last when order is ascending, but first when the order is descending:

https://github.com/elastic/elasticsearch/blob/5107388fe9/core/src/main/java/org/elasticsearch/search/sort/ScriptSortParser.java#L185

After a brief talk with @jpountz it seems like we should probably also sort them last when order is descending.
</description><key id="141886161">17190</key><summary>Handle missing numeric values for ScriptSort</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-18T14:44:02Z</created><updated>2016-04-04T13:15:17Z</updated><resolved>2016-04-04T13:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-18T14:45:10Z" id="198392090">@clintongormley might also have an opinion about this?
</comment><comment author="jpountz" created="2016-03-18T20:55:33Z" id="198538816">+1 on sorting missing values last by default for all fields
</comment><comment author="clintongormley" created="2016-03-24T19:04:41Z" id="200973365">For normal sort, we support the `missing` parameter to determine whether missing values should be sorted `_first`, `_last`, or a custom value. Can we not do the same here?
</comment><comment author="jpountz" created="2016-03-25T14:20:55Z" id="201305510">I had not even noticed that script sorting does not support this parameter. I guess the assumption is that if special handling of missing values is needed then the script can do it by itself.
</comment><comment author="cbuescher" created="2016-04-01T15:42:30Z" id="204442160">I think we cannot support the `missing` parameter here, as @jpountz mentioned the script needs to handle those cases itself. I just played around with tests and read the code, and it appears that numeric scripts (at least for groovy, didn't check the others) always return 0.0d (because of `double runAsDouble()`), even if the script acesses a null field in the doc. I also thinks this makes the original question from the TODO I linked to irrelevant and I'd close this issue if nobody objects.
</comment><comment author="jpountz" created="2016-04-04T13:10:34Z" id="205290641">Fine with me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel doesn't load (Cannot read property 'cluster_name')</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17189</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.1

**JVM version**: OpenJDK 1.7.0.95

**OS version**: Debian 7, Kernel 3.10

**Description of the problem including expected versus actual behavior**:

Hi there,

I just installed Marvel for the first time on a production cluster running  Kibana 4.4.2, ES 2.2.1. Followed the installation instructions, but when I try to load Marvel, I get:

```
Marvel: Error 400 Bad Request: Cannot read property 'cluster_name' of undefined
```

Calling `/api/marvel/v1/clusters` returns the same:

```
{
  statusCode: 400,
  error: "Bad Request",
  message: "Cannot read property 'cluster_name' of undefined"
}
```

`cluster.name` is defined in `elasticsearch.yml` and shows (as `cluster_name`) with a `curl localhost:9200`.

So I'm not quite sure what's going on. The Marvel agent is definitely running, and I have the Marvel indices created (I already tried deleting them and recreating them):

```
.marvel-es-2016.03.16
.marvel-es-2016.03.17
.marvel-es-data
```

I also tried  stopping all the nodes and restarting them. Ideas?

Thanks!

PS: I also filed this [in the forum](https://discuss.elastic.co/t/marvel-refusing-to-load-cannot-read-property-cluster-name-of-undefined/44640) but got no response. I'm not sure whether this is an actual bug or some mistake on my part. Either way, any help debugging the issue will be welcome.
</description><key id="141876737">17189</key><summary>Marvel doesn't load (Cannot read property 'cluster_name')</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">calexicoz</reporter><labels /><created>2016-03-18T14:11:16Z</created><updated>2016-03-24T18:58:05Z</updated><resolved>2016-03-24T18:58:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-24T18:58:05Z" id="200970703">Looks like the issue has been discussed in the forum already, so I'll close this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatically add a sub keyword field to string dynamic mappings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17188</link><project id="" key="" /><description>If you add a string field to a document, it will have the following default
mapping:

```
{
  "type": "text",
  "fields": {
    "keyword": {
      "type": "keyword",
      "ignore_above": 256
    }
  }
}
```
</description><key id="141872014">17188</key><summary>Automatically add a sub keyword field to string dynamic mappings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T13:49:53Z</created><updated>2016-04-29T14:49:38Z</updated><resolved>2016-03-29T07:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-23T05:28:10Z" id="200188606">Is the intention here supposed to be to allow users to use aggregations on any json string "by default"? This means the data would then be indexed 3 times, once in the field, once in _all, and once in the keyword subfield. If I was correct above, then do we really need this subfield to be indexed, or could it just be there for doc values? I'm also more of a fan of "original" then for the name.
</comment><comment author="jpountz" created="2016-03-23T06:44:42Z" id="200209023">Your assumptions are correct. Indexing the sub field is useful so that if the user runs eg. a terms aggregations to build a pie chart, then we can build a filter on the same field when (s)he clicks on one piece of the chart. If we were to do it with the text field, we might also match values that have a different case for instance.

I used "keyword" because it was the suggested name on #12394 (see "Good out-of-the-box dynamic mappings for string fields").
</comment><comment author="clintongormley" created="2016-03-25T15:58:51Z" id="201342827">Personally I'm a fan of `keyword` as it says "this is the keyword variant of the main field".  That said, Logstash has used `raw` for a long time, as has our documentation.

If we were to use `keyword` as the default name, Logstash could continue to use `raw` in the Logstash template to maintain bwc, but I'm guessing that Logstash would prefer to use our defaults instead.  I'd rather make the right choice for the future, rather than using `raw` just for historical reasons, but I'm not sure that the choice between `keyword` vs `raw` is that clear cut.

Opinions welcome.
</comment><comment author="jpountz" created="2016-03-26T17:08:08Z" id="201893590">Should we merge it as-is so that this feature would be included in the alpha releases and we have feedback about this multi-field mapping by default? Then we can open a blocker issue about whether we want to change the naming for the GA.
</comment><comment author="rjernst" created="2016-03-26T17:10:39Z" id="201893885">+1 to merge
</comment><comment author="djschny" created="2016-04-29T14:45:00Z" id="215740910">Do we really want to couple the name of the mult-field to the the analyzer type? keyword analzyer really should be called `noop` anyway because that is how it behaves and we even explain it that way to people.
</comment><comment author="jpountz" created="2016-04-29T14:49:38Z" id="215742185">@djschny I believe the idea was more to couple it with the _field_ type than the _analyzer_ type. Please open a new issue if you think this should be changed to something else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recover broken IndexMetaData as closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17187</link><project id="" key="" /><description>Today if something is wrong with the IndexMetaData we detect it very
late and most of the time if that happens we already allocated the index
and get endless loops and full log files on data-nodes. This change tries
to verify IndexService creattion during initial state recovery on the master
and if the recovery fails the index is imported as `closed` and won't be allocated
at all.

@bleskes @ywelsch @jasontedor may I have your feedback on the approach
</description><key id="141849774">17187</key><summary>Recover broken IndexMetaData as closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T12:15:35Z</created><updated>2016-11-09T02:28:41Z</updated><resolved>2016-03-21T21:52:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-03-18T13:13:08Z" id="198349203">Left some comments but I really like the idea here :smile: . My main concern is how to make sure that `verifyIndexMetadata` does not make any disk writes or messes with existing caches etc. I'll have to have another closer look to feel confident that nothing bad happens there.
I also wonder whether we can apply the same approach when importing dangling indices (`LocalAllocateDangledIndices`).
</comment><comment author="s1monw" created="2016-03-18T13:24:54Z" id="198353685">&gt; I also wonder whether we can apply the same approach when importing dangling indices (LocalAllocateDangledIndices).

agreed I think we can - lets do a followup

&gt; Left some comments but I really like the idea here :smile: . My main concern is how to make sure that verifyIndexMetadata does not make any disk writes or messes with existing caches etc. I'll have to have another closer look to feel confident that nothing bad happens there.

it's hard to assert to be honest but form architecture perspective I made all the global impacting things listeners that we do not pass in on the verify method so I think we are ok?
</comment><comment author="s1monw" created="2016-03-21T09:06:03Z" id="199183218">@ywelsch  @bleskes I pushed another change that also prevents the index from being opened if we can't create an index service.
</comment><comment author="s1monw" created="2016-03-21T09:24:33Z" id="199189394">@ywelsch I looked into your concern and refactored the solution such taht we are creating private cache instances for the verification IndexService. This should prevent any modifications. All other datastructures are immutable.
</comment><comment author="bleskes" created="2016-03-21T19:21:09Z" id="199437509">I really like the change, but I'm afraid it's not enough, for example, when referring to an analyzer that used to be in the node settings (I tested it). The reason is that the mapper service doesn't instantiate anything until the merge method on it is called. This is imo something we should change (prepare everything in the constructor) but we don't have to do it in this PR. This patch works for me:

```
diff --git a/core/src/main/java/org/elasticsearch/indices/IndicesService.java b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
index ca75d30..bbdd693 100644
--- a/core/src/main/java/org/elasticsearch/indices/IndicesService.java
+++ b/core/src/main/java/org/elasticsearch/indices/IndicesService.java
@@ -19,6 +19,7 @@

 package org.elasticsearch.indices;

+import com.carrotsearch.hppc.cursors.ObjectCursor;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.CollectionUtil;
@@ -34,6 +35,7 @@ import org.elasticsearch.cluster.ClusterService;
 import org.elasticsearch.cluster.ClusterState;
 import org.elasticsearch.cluster.metadata.IndexMetaData;
 import org.elasticsearch.cluster.metadata.IndexNameExpressionResolver;
+import org.elasticsearch.cluster.metadata.MappingMetaData;
 import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.breaker.CircuitBreaker;
 import org.elasticsearch.common.bytes.BytesReference;
@@ -66,6 +68,7 @@ import org.elasticsearch.index.fielddata.FieldDataType;
 import org.elasticsearch.index.fielddata.IndexFieldDataCache;
 import org.elasticsearch.index.flush.FlushStats;
 import org.elasticsearch.index.get.GetStats;
+import org.elasticsearch.index.mapper.MapperService;
 import org.elasticsearch.index.merge.MergeStats;
 import org.elasticsearch.index.recovery.RecoveryStats;
 import org.elasticsearch.index.refresh.RefreshStats;
@@ -398,6 +401,12 @@ public class IndicesService extends AbstractLifecycleComponent&lt;IndicesService&gt; i
             closeables.add(indicesQueryCache);
             // this will also fail if some plugin fails etc. which is nice since we can verify that early
             IndexService service = createIndexService(nodeServicesProvider, metaData, indicesQueryCache, indicesFieldDataCache, Collections.emptyList());
+            for (ObjectCursor&lt;MappingMetaData&gt; typeMapping : metaData.getMappings().values()) {
+                // don't apply the default mapping, it has been applied when the mapping was created
+                service.mapperService().merge(typeMapping.value.type(), typeMapping.value.source(),
+                        MapperService.MergeReason.MAPPING_RECOVERY, true);
+            }
+
             closeables.add(() -&gt; service.close("metadata verification", false));
         } finally {
             IOUtils.close(closeables);
```
</comment><comment author="s1monw" created="2016-03-21T19:24:31Z" id="199439158">I was going to do that exact same thing since it would allow us to remove some places where we create an index for only that purpose. I can put this into the patch and add a test. Followups can clean up other places and we may move stuff into ctors.
</comment><comment author="s1monw" created="2016-03-21T19:41:11Z" id="199443776">@bleskes pushed an update with a new test
</comment><comment author="bleskes" created="2016-03-21T19:54:39Z" id="199448283">LGTM. Thanks
</comment><comment author="hadeslion" created="2016-11-08T09:14:10Z" id="259084315">this verifyIndexMetadata spend too much time if a node has many indices and each index has many type.
It cause a long time waiting before the index recovery when a node restart.
Can you add any config option to skip this verify?
</comment><comment author="bleskes" created="2016-11-08T13:35:22Z" id="259137425">@hadeslion there is no such settings as this is important. If it takes so long you will also run into problems elsewhere as these things should be parsable. Can you give some numbers about how many indices and types you have? how long it takes etc? Is this during node startup or later on?
</comment><comment author="hadeslion" created="2016-11-09T02:28:41Z" id="259319886">@bleskes I have 100 indices. Some of them have 400-800 types. these indices just take 10-20s for each. The others have 1200-1600 types each, and it took 1-2minutes.
I run these indices on a single node cluster. This happens when I restart the node. according to the logs, it is during the gateway state recovery. During this metadata verify, all api request return SERVICE_UNAVAILABLE/1/state not recovered
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>An `exists` query on an object should query a single term.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17186</link><project id="" key="" /><description>Currently if you run an `exists` query on an object, it will resolve all sub
fields and create a disjunction for all those fields. However the `_field_names`
mapper indexes paths for objects so we could query object paths directly.

I also changed the query parser to reject `exists` queries if the `_field_names`
field is disabled since it would be a big performance trap.

Closes #17131
</description><key id="141840687">17186</key><summary>An `exists` query on an object should query a single term.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T11:26:25Z</created><updated>2016-03-22T15:39:26Z</updated><resolved>2016-03-22T15:38:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-18T14:16:13Z" id="198379025">&gt; However the _field_names
&gt; mapper indexes paths for objects so we could query object paths directly.

It has always existed? I just want to make sure.
</comment><comment author="nik9000" created="2016-03-18T14:20:03Z" id="198380313">&gt; I also changed the query parser to reject exists queries if the _field_names
&gt; field is disabled since it would be a big performance trap.

Does that make this a breaking change? I don't have a problem with it but maybe it needs to be in the breaking changes list.
</comment><comment author="jpountz" created="2016-03-18T14:34:26Z" id="198388437">&gt; It has always existed?

Yes.

&gt; Does that make this a breaking change? 

I already added a note, see the bottom of the PR (even though I would qualify it more as a migration note than a breaking change since it will only affect users who explicitly disabled the `_field_names` field).
</comment><comment author="nik9000" created="2016-03-18T14:34:53Z" id="198388672">&gt; I already added a note, see the bottom of the PR (even though I would qualify it more as a migration note than a breaking change since it will only affect users who explicitly disabled the _field_names field).

Just saw it. Sorry for the noise.

LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ubuntu 14 complaining that the deb package is bad quality</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17185</link><project id="" key="" /><description>While installing the DEB package downloaded from here on Ubuntu 14.04:
https://www.elastic.co/thank-you?url=https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/2.2.1/elasticsearch-2.2.1.deb

&gt; The package is of bad quality
&gt; 
&gt; The installation of a package which violates the quality standards
&gt; isn't allowed. This could cause serious problems on your computer.
&gt; Please contact the person or organisation who provided this package
&gt; file and include the details beneath.
&gt; 
&gt; Lintian check results for /home/user/Setups/elasticsearch-2.2.1.deb:
&gt; E: elasticsearch: dir-or-file-in-var-run var/run/elasticsearch/

This has been reported previously in https://github.com/elastic/elasticsearch/issues/2515 and https://github.com/elastic/elasticsearch/issues/14532 and was previously resolved in https://github.com/elastic/elasticsearch/commit/a5f9173e14ae1f96ad786b81d8639a30693d9c9a

It looks like we need to revisit the lintian overrides
</description><key id="141840395">17185</key><summary>Ubuntu 14 complaining that the deb package is bad quality</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Packaging</label><label>adoptme</label><label>regression</label></labels><created>2016-03-18T11:24:44Z</created><updated>2016-03-18T18:13:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2016-03-18T11:33:35Z" id="198316746">I think this is due to using the Ubuntu App Center (or something?) to install the package.

`dpkg` and `apt-get` do not complain about this:

Example using dpkg:

```
root@ubuntu-2gb-sfo1-01:~# lsb_release -d
Description:    Ubuntu 14.04.4 LTS

root@ubuntu-2gb-sfo1-01:~# wget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/2.2.1/elasticsearch-2.2.1.deb

2016-03-18 07:31:28 (8.63 MB/s) - &#8216;elasticsearch-2.2.1.deb&#8217; saved [29307500/29307500]

root@ubuntu-2gb-sfo1-01:~# dpkg -i elasticsearch-2.2.1.deb
Selecting previously unselected package elasticsearch.
(Reading database ... 86823 files and directories currently installed.)
Preparing to unpack elasticsearch-2.2.1.deb ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Unpacking elasticsearch (2.2.1) ...
Setting up elasticsearch (2.2.1) ...
Processing triggers for ureadahead (0.100.0-16) ...
```
</comment><comment author="clintongormley" created="2016-03-18T11:36:01Z" id="198318102">Yes, apparently the App Center does a Lintian check.  That said, we added Lintian overrides to work around this previously, but it looks like we need to update them.
</comment><comment author="nik9000" created="2016-03-18T11:47:17Z" id="198321531">I guess it is just more stuff to add to the bats tests....
</comment><comment author="LeeDr" created="2016-03-18T14:32:55Z" id="198387629">Do we know the steps to reproduce this?  I've only used dpkg and apt-get to install.  We likely have the same issue across our other products?
</comment><comment author="nik9000" created="2016-03-18T14:41:38Z" id="198390939">&gt; Do we know the steps to reproduce this?

We can likely run the same linting the App Center does. I haven't made sure of it but I'm 99% sure we can reproduce if we want.
</comment><comment author="jakommo" created="2016-03-18T15:30:56Z" id="198409825">Looks like it's https://lintian.debian.org/tags/dir-or-file-in-var-run.html
</comment><comment author="jasontedor" created="2016-03-18T16:02:22Z" id="198427271">&gt; It looks like we need to revisit the lintian overrides

I don't think that we should override this one; I think that we should move the creation of `/var/run/elasticsearch` to the init script on service start (instead of when the package is installed), and destroy it when the service stops (instead of when the package is removed).
</comment><comment author="nik9000" created="2016-03-18T18:13:09Z" id="198478394">I'm ok with that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[feature request]smart routing detection when search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17184</link><project id="" key="" /><description>**Describe the feature**:
if mapping of a index has routing field:
 "_routing": {
          "path": "XXX",
          "required": true
        },

when search without explicit set rouing field but has term XXX, ES better detect it and add routing parameters
</description><key id="141821939">17184</key><summary>[feature request]smart routing detection when search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2016-03-18T09:57:51Z</created><updated>2016-03-24T19:12:28Z</updated><resolved>2016-03-18T11:38:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-18T11:38:38Z" id="198318595">`path` is no longer supported in `_routing`, and search by default goes to all shards so routing isn't required.  If you want to limit search to specific shards then you need to specify the routing yourself
</comment><comment author="makeyang" created="2016-03-21T01:34:09Z" id="199080079">you don't get my point at all, do u?
</comment><comment author="clintongormley" created="2016-03-24T19:12:28Z" id="200977684">No
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ClusterService interface, in favor of it's only production instance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17183</link><project id="" key="" /><description>We current have a ClusterService interface, implemented by InternalClusterService and a couple of test classes. Since the decoupling of the transport service and the cluster service, one can construct a ClusterService fairly easily, so we don't need this extra indirection.
</description><key id="141817925">17183</key><summary>Remove ClusterService interface, in favor of it's only production instance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T09:42:00Z</created><updated>2016-03-21T13:00:06Z</updated><resolved>2016-03-21T13:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-21T09:58:30Z" id="199203352">@bleskes: Left a few minor comments, otherwise LGTM.
</comment><comment author="bleskes" created="2016-03-21T12:36:28Z" id="199253080">@danielmitterdorfer thanks for the review. I indeed started by copying the "stop()" pattern from somewhere else and corrected my ways to use close(). Note that in the case of the cluster service there is no distinction.
</comment><comment author="bleskes" created="2016-03-21T12:36:41Z" id="199253121">and I pushed another commit addressing all you mentioned.
</comment><comment author="danielmitterdorfer" created="2016-03-21T12:39:00Z" id="199254064">@bleskes LGTM :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide better error message when an incompatible node connects to a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17182</link><project id="" key="" /><description>We should give a better exception message when an incompatible node connects
and we receive a messeage. This commit adds a clear excpetion based on the
protocol version received instead of throwing cryptic messages about not fully reaed
buffer etc.

Relates to #17090
</description><key id="141808287">17182</key><summary>Provide better error message when an incompatible node connects to a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>enhancement</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-18T08:54:24Z</created><updated>2016-03-18T09:17:05Z</updated><resolved>2016-03-18T09:17:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-18T08:54:55Z" id="198262524">this is how such an exception looks like now:

``````
[elasticsearch] [2016-03-18 09:50:23,769][WARN ][transport.netty          ] [Lasher] exception caught on transport layer [[id: 0xc42794b9, /0:0:0:0:0:0:0:1:61619 =&gt; /0:0:0:0:0:0:0:1:9300]], closing connection
[elasticsearch] java.lang.IllegalStateException: Received message from unsupported version: [2.0.0] minimal compatible version is: [5.0.0]
[elasticsearch]     at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:132)
[elasticsearch]     at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)```
``````
</comment><comment author="danielmitterdorfer" created="2016-03-18T09:01:25Z" id="198265208">LGTM
</comment><comment author="dadoonet" created="2016-03-18T09:03:41Z" id="198265787">OMG! I love it! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to disable FS/URL repository based snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17181</link><project id="" key="" /><description>I want to disable FS and URL snapshots in Elasticsearch 2.x. Essentially I do not want users of the cluster messing around with the filesystem. Users are allowed to configure S3 repositories only. Is there a way to do this today?

In general, it will be much more useful and powerful if plugins in 2.x are also allowed to override default modules as used to be the case in 1.x

I achieved this in 1.x by having a plugin do the following

``` java
repositoriesModule.registerRepository(FsRepository.TYPE, AbstractModule.class)
```

But 2.x seems to disallow registering same module twice and similar code throws the following error

```
[2016-03-17 14:40:43,195][ERROR][bootstrap ] Exception
ElasticsearchException[failed to invoke onModule]; nested: InvocationTargetException; nested: IllegalArgumentException[Can't register the same [repository] more than once for [url]];
at org.elasticsearch.plugins.PluginsService.processModule(PluginsService.java:232)
at org.elasticsearch.plugins.PluginsService.processModules(PluginsService.java:217)
at org.elasticsearch.node.Node.(Node.java:198)
at org.elasticsearch.node.Node.(Node.java:128)
at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:178)
at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:285)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.elasticsearch.plugins.PluginsService.processModule(PluginsService.java:229)
... 7 more
Caused by: java.lang.IllegalArgumentException: Can't register the same [repository] more than once for [url]
at org.elasticsearch.common.util.ExtensionPoint$ClassMap.registerExtension(ExtensionPoint.java:104)
at org.elasticsearch.repositories.RepositoryTypesRegistry.registerRepository(RepositoryTypesRegistry.java:39)
at org.elasticsearch.repositories.RepositoriesModule.registerRepository(RepositoriesModule.java:48)
at org.elasticsearch.plugin.cloud.aws.CloudAwsPlugin.onModule(CloudAwsPlugin.java:113)
... 12 more
```

Filing this feature request based on my question at https://discuss.elastic.co/t/how-to-disable-filesystem-url-snapshots-in-2-x/44776 
</description><key id="141786766">17181</key><summary>Option to disable FS/URL repository based snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malpani</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label></labels><created>2016-03-18T06:28:43Z</created><updated>2016-03-18T09:27:31Z</updated><resolved>2016-03-18T09:27:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="muralikpbhat" created="2016-03-18T08:31:42Z" id="198254159">You can achieve the desired behavior now by not specifying the path.repo and repositories.url.allowed_urls settings.
</comment><comment author="clintongormley" created="2016-03-18T09:27:30Z" id="198273864">Exactly what @muralikpbhat said.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>accessing matched term in script score function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17180</link><project id="" key="" /><description>i have some problem with script score.
i want to caculate the score with matched term.
in most situation,  i just need the count of matched term. 
but i want to know what term are the matched terms, and i can do some adjusting to different terms.

for query string "test A B C"
terms analyzed by index's analyzer will be: "test" "A" "B" "C"

if i just know the count of matched terms to one document, the score will be  4, 3, 2, 1 or 0.
but i want to have some extra points to term "A", 
the point will be like this:  A*10 + B \* 1 + C \* 1

someone has asked for the same problem before, but his has a little different from mine.

https://github.com/elastic/elasticsearch/issues/13806

i need the query string analyzed by indexer's analyzer.  the query string are always given by user input, i thinks its better be analyzed by es, i just have to make some policy to the terms analyzed by es.
</description><key id="141777838">17180</key><summary>accessing matched term in script score function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q11112345</reporter><labels /><created>2016-03-18T05:13:20Z</created><updated>2016-03-18T12:02:22Z</updated><resolved>2016-03-18T09:24:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-18T09:24:43Z" id="198272976">Hi @q11112345 

If you want to implement custom scoring like this, then you need to use more structured queries than just a high level match or query string query, eg use the function score query.

The right place to ask questions like these in the forum instead: https://discuss.elastic.co/

thanks
</comment><comment author="q11112345" created="2016-03-18T12:01:40Z" id="198324638">what does the "more structured queries" look like?
like the example given in  issue 13806? 
i understand the example you have replied to the author in issue 13806.
my problem is , i cant analyze the "query string" to terms myself, or i dont want to analyze the "query string" to terms myself.
i want all the string in querying to behave the same way, processed by es all the time. i just need to controll the scoring in script score.

there is always no response to my problems in discuss.elastic.co ...so i have to ask here. and i find someone getting the same problem and getting the answer here.

thanks anyway.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud-AWS: S3 Repository Across-Account fails verification.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17179</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0

**OS version**: Amazon Linux 2015.09.02

**Description of the problem including expected versus actual behavior**: Using Cloud-AWS Plugin to define a repository that is across AWS accounts for escrow backps/snapshots

**Steps to reproduce**:
1. Create two AWS accounts, A for our primary workload (ElasticSearch), and B for our escrow copy (think: security team)
2. Create a bucket in B (security-bucket), with an S3 Bucket Policy that permits account A to write to a prefix in this bucket, with a condition that the submitter (in A) set the ACL "bucket-owner-full-control'. Also permit account A to list and get objects from this prefix, but not DELETE. (NB: does permit overwrite, but S3Versioning can help here - another topic)
3. In account A, create an IAM Role for EC2 instance that permits access to B's security-bucket, as well as the normal EC2 discovery requirements.
4. Launch an EC2 instance running ElasticSearch in this Role.
5. Configure the ElasticSearch instance to have a repository of B's security-bucket

**Provide logs (if relevant)**:
`{"error":{"root_cause":[{"type":"repository_verification_exception","reason":"[s3backupsec] path [XXX][Dev][Elasticsearch] is not accessible on master node"}],"type":"repository_verification_exception","reason":"[s3backupsec] path [XXX][Dev][Elasticsearch] is not accessible on master node","caused_by":{"type":"i_o_exception","reason":"Unable to upload object XXX/Dev/Elasticsearch/tests-DJJ3cNL_ScqiNn6P-LQOAw/master.dat-temp","caused_by":{"type":"amazon_s3_exception","reason":"Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: CBD309137D130093)"}}},"status":500}`

**Describe the feature**:
I am using ES 2.2.0 , and setting up a cross account trust where the receiving bucket (in another AWS account0 requires "bucket-owner-full-control", Using AWS CLi confirms I can put objects with the canned ACl into the prefix I have reserved for this; I can list the objects back, and I can read the content back.

However, when trying to set up a backup location in elasticsearch, it fails the verification. My config is:
`{ "type": "s3", "settings": { "bucket": "XXX-logs", "region": "ap-southeast-2", "server_side_encryption": "True", "base_path": "XXX/Dev/Elasticsearch/", "buffer_size": "100mb", "canned_acl": "bucket-owner-full-control" } }`

However when submitting this I get:
`{"error":{"root_cause":[{"type":"repository_verification_exception","reason":"[s3backupsec] path [XXX][Dev][Elasticsearch] is not accessible on master node"}],"type":"repository_verification_exception","reason":"[s3backupsec] path [XXX][Dev][Elasticsearch] is not accessible on master node","caused_by":{"type":"i_o_exception","reason":"Unable to upload object XXX/Dev/Elasticsearch/tests-DJJ3cNL_ScqiNn6P-LQOAw/master.dat-temp","caused_by":{"type":"amazon_s3_exception","reason":"Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: CBD309137D130093)"}}},"status":500}`

Which from the S3 logs on the target S3 bucket I see a 403 AccessDenied. The only condition that is required by S3 in my case is the bucket-owner-full-control ACL. It appears that the check in index/snapshots/blobstore/BlobStoreIndexShardRepository.java calling org.elasticsearch.repositories.blobstore.BlobStoreRepository.testBlobPrefix() may not have the canned ACL being added, and thus fails to get set up.

This maybe somewhere around startVerification() in repositories/blobstore/BlobStoreRepository.java.

In my use case, the target bucket is to be write once (no deletes; escrow copy of the backup, so the corresponding "ensure moves are supported" is also likely to fail....
</description><key id="141768944">17179</key><summary>Cloud-AWS: S3 Repository Across-Account fails verification.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">JamesBromberger</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>discuss</label></labels><created>2016-03-18T03:55:41Z</created><updated>2017-03-09T13:11:42Z</updated><resolved>2016-12-23T11:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-18T09:21:45Z" id="198271665">@dadoonet could you take a look at this please?
</comment><comment author="clintongormley" created="2016-03-18T09:22:38Z" id="198272101">or @xuzha :)
</comment><comment author="xuzha" created="2016-03-18T17:53:19Z" id="198469899">np @clintongormley  I will verify this asap :-)
</comment><comment author="xuzha" created="2016-03-18T21:19:52Z" id="198546518">Thanks @JamesBromberger for reporting the issue.

I went through the code, looks like it do have the `canned acl`.  I'm a bit confused here, I saw you have two accounts here, looks like your Elasticsearch is using account A, and it is trying to use a bucket  created by account B?

Please correct me if I'm missing something here. Based on the errors msg, ES complains that it doesn't have the permission to write to the bucket. And I think the feature in https://github.com/elastic/elasticsearch/pull/14297 is for specifying a canned in your request when creating a resource. For example, if you set `log-delivery-write`, it would grant write permission to the Amazon S3 `LogDelivery` group. 
</comment><comment author="jlintz" created="2016-09-16T14:26:19Z" id="247614344">I'm seeing this on ES 2.4.0,  I get an access denied but the repository is still created and I'm able to snapshot to it successfully.  Using the IAM role policy listed in the docs
</comment><comment author="ppcharli" created="2016-10-24T06:21:23Z" id="255657012">I have the same problem on ES 2.4.1, I get an access denied but the repository is still created.
I'm using Minio Server
</comment><comment author="jlintz" created="2016-10-24T13:27:56Z" id="255740053">The issue for me ended up being that the IAM policy needs delete access to test access to the bucket.  I had a blanket policy in place that didn't allow delete's on S3 buckets that was causing this issue.  Once I removed that, things worked as expected.  
</comment><comment author="dadoonet" created="2016-12-23T11:16:58Z" id="268975574">No further feedback on this one.
I'm closing but feel free to reopen and/or add comments if any.

</comment><comment author="mikerev" created="2017-03-09T13:11:42Z" id="285347349">Elevating IAM permissions for delete rights resolved this issue for me.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Non-deterministic ordering of nodes in _cat APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17178</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_65

**OS version**: Ubuntu 15.04

**Description of the problem including expected versus actual behavior**: When hitting any Cat APIs that deal with the list of nodes, the ordering of the nodes is non-deterministic, so depending on which node handles the request, the ordering will be different.

The following APIs are affected:
- _cat/nodes
- _cat/allocation
- _cat/nodeattrs

I'm inclined to raise this as a bug, since the ordering used to be deterministic in 1.7.

**Steps to reproduce**:
1. Setup a cluster with at least two nodes
2. Hit _cat/nodes and note the ordering of entries
3. Hit _cat/nodes on a different node in the cluster and note the ordering is different
</description><key id="141742585">17178</key><summary>Non-deterministic ordering of nodes in _cat APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels><label>:CAT API</label></labels><created>2016-03-18T00:30:34Z</created><updated>2016-10-08T11:07:49Z</updated><resolved>2016-03-18T00:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-18T00:32:16Z" id="198144360">Duplicates #16575, duplicates #16975 
</comment><comment author="samcday" created="2016-03-18T00:37:36Z" id="198145406">@jasontedor I'm not sure this is necessarily a duplicate of those issues. This seems like a regression because in Elasticsearch 1.x the ordering had some kind of determinism. #16975 talks about some kind of user-controlled sorting. While that would be nice, I would just be content with deterministic sorting again :)
</comment><comment author="jasontedor" created="2016-03-18T00:43:11Z" id="198146245">&gt; I'm not sure this is necessarily a duplicate of those issues.

@samcday I understand where you're coming from, but I think that the outcome for this issue will be the same as the outcome for #16975. Either sorting will be implemented, or it won't but I don't think we will do an in-between measure just to get deterministic but non-sorted ordering. Does that make sense?
</comment><comment author="samcday" created="2016-03-18T00:44:33Z" id="198146445">Yeah that makes sense, but my fear is that something like a sorting option in the Cat API might be considered a low priority feature request. Whereas in this case I think the new behaviour is a regression that actually has a surprisingly negative #UX impact when trying to investigate a large cluster with 30-40+ nodes.
</comment><comment author="jasontedor" created="2016-03-18T00:45:47Z" id="198146644">&gt; Yeah that makes sense, but my fear is that something like a sorting option in the Cat API might be considered a low priority feature request.

@samcday That statement is _not_ incorrect, and I totally understand you here.

&gt; Whereas in this case I think the new behaviour is a regression that actually has a surprisingly negative #UX impact when trying to investigate a large cluster with 30-40+ nodes.

Is there a reason that you can't pipe the output through `sort`?
</comment><comment author="samcday" created="2016-03-18T00:48:07Z" id="198146998">There's no reason I can't do that, but more often than not I prefer to hit the cat APIs from a browser when doing ad-hoc investigations. This is because we use Shield, and the browser will remember my auth across multiple requests. Whereas if I use curl I have to mess with my `.netrc` or fiddle around with putting my password in each request.
</comment><comment author="jasontedor" created="2016-03-18T00:49:26Z" id="198147180">@samcday That is a 100% valid observation; thanks for putting it in my mind.
</comment><comment author="excalq" created="2016-09-27T06:12:59Z" id="249775376">This would make working with ES in Postman so much better, especially when doing Ops work with numerous log indices...
</comment><comment author="clintongormley" created="2016-10-08T11:07:49Z" id="252418762">See https://github.com/elastic/elasticsearch/pull/20658
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Always write shard state in SMILE format </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17177</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/pull/17123, we standardized writing global and index level states in `SMILE` format 
for efficient parsing and storage. In this PR, we change the format for shard level state 
</description><key id="141700570">17177</key><summary>Always write shard state in SMILE format </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Store</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T20:38:40Z</created><updated>2016-03-21T18:32:48Z</updated><resolved>2016-03-21T18:32:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-18T09:29:22Z" id="198274538">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Vagrant tests should be green</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17176</link><project id="" key="" /><description>This pull request brings the Vagrant tests to green on master.

A few notes:
- the Vagrant tests now include tests for modules
- the list of plugins installed now includes all official plugins
- the plugins file permissions tests were out of date
- the plugins folder was not being installed with the correct owner/group
- the installed plugins directories were not being created with the correct permissions

Closes #17152 
</description><key id="141687803">17176</key><summary>Vagrant tests should be green</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>:Plugins</label><label>blocker</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T19:43:19Z</created><updated>2016-03-24T19:00:32Z</updated><resolved>2016-03-18T19:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T19:51:07Z" id="198055318">w00t
</comment><comment author="nik9000" created="2016-03-17T20:01:44Z" id="198059083">LGTM. Left note about another thing you might want to do while you are being awesome.
</comment><comment author="rjernst" created="2016-03-17T22:14:43Z" id="198105502">@jasontedor I left some comments about concerns I have.
</comment><comment author="jasontedor" created="2016-03-17T22:25:34Z" id="198108382">&gt; I left some comments about concerns I have.

@rjernst Thanks for reviewing; I left a reply to your comments.
</comment><comment author="rjernst" created="2016-03-18T19:09:43Z" id="198498868">@jasontedor I'm fine with follow ups, but I think we need a unit test if we are going to have this user/group setting code. Other than that LGTM if we have follow up issues.
</comment><comment author="jasontedor" created="2016-03-21T02:18:06Z" id="199086513">&gt; I'm fine with follow ups, but I think we need a unit test if we are going to have this user/group setting code.

@rjernst I added such tests in #17208.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests seems to ignore plugin-security.policy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17175</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.1

**JVM version**: 1.8

**OS version**: Windows 7 64bits

**Description of the problem including expected versus actual behavior**:
My Maven project applies the security conventions described here https://www.elastic.co/guide/en/elasticsearch/plugins/2.2/plugin-authors.html#_java_security_permissions. However I'm getting some problems related with security manager inside my unit tests.

I've picked a test case from Elasticsearch Attachments plugin to help me to find the problem. When I run the testcase 'org.elasticsearch.mapper.attachments.TikaDocTests' from original project using Gradle it works fine (no SecurityExceptions), but if I copy it "as is" to my Maven project and I run it I get many SecurityExceptions related with Tika (all the permissions I wrote in my plugin-security.policy are the same used by the Attachments plugin).

I also noticed that no matter what I wrote in plugin-security.policy file, the tests work the same, it seems to be ignored.

**Steps to reproduce**: Not applies

**Provide logs (if relevant)**:
Caused by: java.lang.SecurityException: Unable to create temporary file or directory
    at java.nio.file.TempFileHelper.create(TempFileHelper.java:143)
    at java.nio.file.TempFileHelper.createTempFile(TempFileHelper.java:161)
    at java.nio.file.Files.createTempFile(Files.java:897)
    at org.apache.tika.io.TemporaryResources.createTempFile(TemporaryResources.java:79)
    at org.apache.tika.io.TikaInputStream.getPath(TikaInputStream.java:586)
    at org.apache.tika.io.TikaInputStream.getFile(TikaInputStream.java:615)
    at org.apache.tika.parser.pkg.ZipContainerDetector.detectZipFormat(ZipContainerDetector.java:143)
    at org.apache.tika.parser.pkg.ZipContainerDetector.detect(ZipContainerDetector.java:90)
    at org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:77)
    at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:112)
    at org.apache.tika.Tika.parseToString(Tika.java:496)
    at org.apache.tika.Tika.parseToString(Tika.java:571)

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**: Test framework
</description><key id="141643257">17175</key><summary>Tests seems to ignore plugin-security.policy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cadu-goncalves</reporter><labels /><created>2016-03-17T16:59:12Z</created><updated>2016-03-17T19:56:29Z</updated><resolved>2016-03-17T19:33:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cadu-goncalves" created="2016-03-17T19:01:43Z" id="198035308">Here is a Maven project that shows the error.
[myplugin.zip](https://github.com/elastic/elasticsearch/files/178493/myplugin.zip)
</comment><comment author="cadu-goncalves" created="2016-03-17T19:20:35Z" id="198043643">The only way to make the tests work obsously, is by disabling the Security Manager:
`$ mvn clean test -Dtests.security.manager=false`

Of course it is a poor workaround, we need to test against Security Manager.
</comment><comment author="rjernst" created="2016-03-17T19:33:34Z" id="198048196">@cadu-goncalves For unit tests to work with the added permissions, you need to have both your plugin properties and plugins security files in test resources. Notice that the tika plugin uses the plugin parent pom, which handles this.
https://github.com/elastic/elasticsearch/blob/2.x/plugins/mapper-attachments/pom.xml
</comment><comment author="cadu-goncalves" created="2016-03-17T19:34:31Z" id="198048521">Thanks @rjernst 
</comment><comment author="cadu-goncalves" created="2016-03-17T19:56:29Z" id="198057658">Adding both plugin-descriptor.properties and plugin-security.policy at root of test resources solved the problem. Thank you very much!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed `total` score mode in favour for `sum` score mode.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17174</link><project id="" key="" /><description>PR for #17083

This also removes some old 1.x `max_children` logic, that turns a value of `0` into `Integer.MAX_VALUE`.
This should have been removed via #13470 but found its way back.
</description><key id="141642315">17174</key><summary>Removed `total` score mode in favour for `sum` score mode.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T16:55:24Z</created><updated>2016-03-18T09:08:19Z</updated><resolved>2016-03-18T09:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-18T07:40:18Z" id="198241034">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add nbest options and NumberFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17173</link><project id="" key="" /><description>Add nbest_cost and nbest_examples parameter to KuromojiTokenizerFactory
Add KuromojiNumberFilterFactory
</description><key id="141631794">17173</key><summary>Add nbest options and NumberFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Plugin Analysis Kuromoji</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T16:17:19Z</created><updated>2016-03-25T14:39:10Z</updated><resolved>2016-03-22T11:12:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-22T09:24:39Z" id="199713710">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give the foreach processor access to the rest of the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17172</link><project id="" key="" /><description>PR for #17147
</description><key id="141616542">17172</key><summary>Give the foreach processor access to the rest of the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T15:28:00Z</created><updated>2016-03-22T09:32:53Z</updated><resolved>2016-03-22T09:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="talevy" created="2016-03-22T00:09:42Z" id="199550415">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch does not start / gradle build totally broken with JDK 9 build 110</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17171</link><project id="" key="" /><description>With the latest java 9 ea (110), a compiled version will not work at all.
Maybe we are doing something illegal/sheisty to retrieve the PID, maybe its a bug.

```
rmuir@beast:~/workspace/elasticsearch/distribution/zip/build/distributions/elasticsearch-5.0.0-SNAPSHOT$ bin/elasticsearch
Java HotSpot(TM) 64-Bit Server VM warning: Option UseParNewGC was deprecated in version 9.0 and will likely be removed in a future release.
Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class java.lang.management.ManagementFactory$PlatformMBeanFinder
    at java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:649)
    at java.lang.management.ManagementFactory.getRuntimeMXBean(ManagementFactory.java:355)
    at org.elasticsearch.monitor.jvm.JvmInfo.&lt;clinit&gt;(JvmInfo.java:53)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:245)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:108)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:103)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.
```

Also gradle itself eats it early and does not work at all:

```
rmuir@beast:~/workspace/elasticsearch$ gradle --stacktrace clean

FAILURE: Build failed with an exception.

* What went wrong:
sun.management.spi.PlatformMBeanProvider: Provider jdk.management.cmm.internal.PlatformMBeanProviderImpl not found

* Try:
Run with --info or --debug option to get more log output.

* Exception is:
java.util.ServiceConfigurationError: sun.management.spi.PlatformMBeanProvider: Provider jdk.management.cmm.internal.PlatformMBeanProviderImpl not found
    at java.util.ServiceLoader.fail(ServiceLoader.java:237)
    at java.util.ServiceLoader.access$300(ServiceLoader.java:183)
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:370)
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:402)
    at java.util.ServiceLoader$1.next(ServiceLoader.java:478)
    at java.lang.Iterable.forEach(Iterable.java:74)
    at java.lang.management.ManagementFactory$PlatformMBeanFinder.lambda$static$0(ManagementFactory.java:890)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.security.AccessController.doPrivileged(AccessController.java:428)
    at java.lang.management.ManagementFactory$PlatformMBeanFinder.&lt;clinit&gt;(ManagementFactory.java:886)
    at java.lang.management.ManagementFactory.getPlatformMXBean(ManagementFactory.java:649)
    at java.lang.management.ManagementFactory.getRuntimeMXBean(ManagementFactory.java:355)
    at org.gradle.launcher.daemon.configuration.CurrentProcess.inferJvmOptions(CurrentProcess.java:71)
    at org.gradle.launcher.daemon.configuration.CurrentProcess.&lt;init&gt;(CurrentProcess.java:33)
    at org.gradle.launcher.cli.BuildActionsFactory.canUseCurrentProcess(BuildActionsFactory.java:95)
    at org.gradle.launcher.cli.BuildActionsFactory.createAction(BuildActionsFactory.java:72)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.createAction(CommandLineActionFactory.java:242)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:232)
    at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210)
    at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35)
    at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206)
    at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:169)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:33)
    at org.gradle.launcher.cli.ExceptionReportingAction.execute(ExceptionReportingAction.java:22)
    at org.gradle.launcher.Main.doAction(Main.java:33)
    at org.gradle.launcher.bootstrap.EntryPoint.run(EntryPoint.java:45)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:520)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.runNoExit(ProcessBootstrap.java:54)
    at org.gradle.launcher.bootstrap.ProcessBootstrap.run(ProcessBootstrap.java:35)
    at org.gradle.launcher.GradleMain.main(GradleMain.java:23)
```
</description><key id="141612169">17171</key><summary>elasticsearch does not start / gradle build totally broken with JDK 9 build 110</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>build</label><label>jvm bug</label></labels><created>2016-03-17T15:11:46Z</created><updated>2016-03-17T19:41:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-17T15:15:44Z" id="197927499">@uschindler tells me he is digging in a bit.
</comment><comment author="uschindler" created="2016-03-17T15:25:37Z" id="197931124">Lucene fails with similar error (just hidden):
RamUsageEstimator tries to get the PlatformMXBean and fails with above error message. Because of this it say "no hotspot available".

The test of RamUsageEstimator does some assertions and fails because of this (it sees a 64 bit Oracle JVM, but no hotspot bean -&gt; boom):

```
java.lang.AssertionError: We should have been able to detect Hotspot's internal settings from the management bean.
    at __randomizedtesting.SeedInfo.seed([4E4F519BECD7B56E:F467726384B9DE51]:0)
    at org.junit.Assert.fail(Assert.java:93)
    at org.junit.Assert.assertTrue(Assert.java:43)
    at org.apache.lucene.util.TestRamUsageEstimator.testHotspotBean(TestRamUsageEstimator.java:114)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:520)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1764)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:871)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:907)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:921)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:880)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:816)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:827)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:804)
```
</comment><comment author="uschindler" created="2016-03-17T15:26:50Z" id="197931569">In addition, the Junit4 test runner canot display the PID of test JVMs:

```
  [junit4] Started J2 PID(&lt;pid acquire exception: java.util.ServiceConfigurationError: sun.management.spi.PlatformMBeanProvider: Provider jdk.management.cmm.internal.PlatformMBeanProviderImpl not found&gt;).
   [junit4] Started J0 PID(&lt;pid acquire exception: java.util.ServiceConfigurationError: sun.management.spi.PlatformMBeanProvider: Provider jdk.management.cmm.internal.PlatformMBeanProviderImpl not found&gt;).
   [junit4] Started J1 PID(&lt;pid acquire exception: java.util.ServiceConfigurationError: sun.management.spi.PlatformMBeanProvider: Provider jdk.management.cmm.internal.PlatformMBeanProviderImpl not found&gt;).
```
</comment><comment author="uschindler" created="2016-03-17T15:33:43Z" id="197934671">Here is my reproducer, I will open issue at OpenJDK:

``` java
import java.lang.management.ManagementFactory;
import com.sun.management.HotSpotDiagnosticMXBean;

public final class Test {

  public static void main(String... args) throws Throwable {
    ManagementFactory.getPlatformMXBean(HotSpotDiagnosticMXBean.class);
  } 

}
```

If you run this program -&gt; boom.
</comment><comment author="uschindler" created="2016-03-17T15:53:36Z" id="197944469">It also fails if you ask for the standard runtime bean (like gradle). You don't even need to ask for the hotspot bean. Any platform bean fails. So `ManagementFactory.getRuntimeMXBean()` fails the same way.
</comment><comment author="uschindler" created="2016-03-17T16:00:59Z" id="197948183">@s1monw 's garbage collector stats also fail (BTW: same applies for Solr).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature: Configuration paremeter for rejecting operation immediately when there is no master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17170</link><project id="" key="" /><description>**Describe the feature**:

Currently, when there is no master node, read operations over an index works as usual. 
However, index or update operations returns 503 after 60 seconds:

```
REQUEST:
curl -X PUT -H "Cache-Control: no-cache" -H "Postman-Token: d7f328a2-0f41-2189-15cf-9fc2af3dc2b3" -d '{"field" : "valueMocoloco1"}' "http://AAA.BBB.CCC.DDD:9200/index1/type1/document1"

RESPONSE:
{
  "error": {
    "root_cause": [
      {
        "type": "cluster_block_exception",
        "reason": "blocked by: [SERVICE_UNAVAILABLE/2/no master];"
      }
    ],
    "type": "cluster_block_exception",
    "reason": "blocked by: [SERVICE_UNAVAILABLE/2/no master];"
  },
  "status": 503
}
```

A similar case happens with cluster status request (cat indices, aliases or cluster health), returning after 30 seconds

```
REQUEST:
curl -X GET -H "Cache-Control: no-cache" -H "Postman-Token: 4e2c6bc1-1c55-59f3-3160-7f44d9a41375" "http://AAA.BBB.CCC.DDD:9200/_cluster/health"

RESPONSE:
{
  "error": {
    "root_cause": [
      {
        "type": "cluster_block_exception",
        "reason": "blocked by: [SERVICE_UNAVAILABLE/2/no master];"
      }
    ],
    "type": "cluster_block_exception",
    "reason": "blocked by: [SERVICE_UNAVAILABLE/2/no master];"
  },
  "status": 503
}
```

As all the elasticsearch nodes know the cluster state, the desired feature is to include a configuration value for allowing to have immediate answer from an elasticsearch node (with the appropiate error) in case there is no master node in the cluster.
</description><key id="141600455">17170</key><summary>Feature: Configuration paremeter for rejecting operation immediately when there is no master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pedro-dlfa</reporter><labels><label>:Cluster</label><label>:CRUD</label><label>discuss</label></labels><created>2016-03-17T14:34:34Z</created><updated>2016-03-18T10:30:57Z</updated><resolved>2016-03-18T10:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-17T14:39:37Z" id="197907844">why don't you just check the status periodically? Serving read requests is fine only updates are not possible in such a case?
</comment><comment author="pedro-dlfa" created="2016-03-17T14:49:37Z" id="197913170">Indeed, what I would like also is to check the status periodically, but without having to wait such long timeout.

I could check that elasticsearch allows a "timeout" parameter in the requests. E.g: 

`curl -X GET -H "Cache-Control: no-cache" -H "Postman-Token: 4e2c6bc1-1c55-59f3-3160-7f44d9a41375" "http://AAA.BBB.CCC.DDD:9200/_cluster/health?timeout=5s"`

However, it would be cleaner and simpler to have a general configuration value instead of having to include that "timeout" parameter in every different request.
</comment><comment author="s1monw" created="2016-03-17T15:33:00Z" id="197934423">we have the main action for this. you can just periodically poll `localhost:9200/` or whatever your rest endpoint is and it will respond with `503` immediately
</comment><comment author="pedro-dlfa" created="2016-03-17T15:47:51Z" id="197941639">Right now I'm checking the status of a specific index by checking that a specific alias is linked to an index, and then check the index health.

Calling `localhost:9200/` would mean an extra call, and also "understand" that the `503` would mean that something in the cluster is wrong, but also read operations works, but not the others.

But, could there be any other scenario where `localhost:9200/` returns `503` but read operations doesn't work?
</comment><comment author="bleskes" created="2016-03-17T16:31:05Z" id="197961756">if you are interested in monitoring no master specifically, you can also use `GET _cat/master` and see whether it gives a value. Also, each indexing operation takes a timeout parameter which you can reduce if the default 60s.
</comment><comment author="clintongormley" created="2016-03-17T19:38:54Z" id="198050123">I'm also curious as to why you see no master so frequently that you want to add an option to cope with it?
</comment><comment author="bleskes" created="2016-03-18T10:30:57Z" id="198298018">Another option that might help is to set the timeout on the indexing request to 0, which will make it fail immediately if there is no master (or no primary available):

```
curl -XPUT "http://localhost:9201/index/type/1?timeout=0" -d'
{
  "f": 1
}'
```

Note though that you will need to create all your indices in advance if you do this and not rely on them to be created automatically. 

I'm closing this now as I think we covered all the possible options. Let me know these don't cover what you want to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: CancellableTasksTests fails with uncaught exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17169</link><project id="" key="" /><description>Saw this several times in the last 12 hours on the 2.x and 2.3 branch:

```
Suite: org.elasticsearch.action.admin.cluster.node.tasks.CancellableTasksTests
  2&gt; mrt 17, 2016 3:53:26 PM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException
  2&gt; WARNING: Uncaught exception in thread: Thread[elasticsearch[TransportTasksActionTests][generic][T#8],5,TGRP-CancellableTasksTests]
  2&gt; EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$3@2c0ff1dc on EsThreadPoolExecutor[generic, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@22c80a2d[Shutting down, pool size = 4, active threads = 4, queued tasks = 0, completed tasks = 62]]]
  2&gt;    at __randomizedtesting.SeedInfo.seed([D7BCB71B5D341091]:0)
  2&gt;    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
  2&gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
  2&gt;    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:341)
  2&gt;    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:299)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendRemoveBanRequest(TransportCancelTasksAction.java:205)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.removeBanOnNodes(TransportCancelTasksAction.java:166)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.access$000(TransportCancelTasksAction.java:63)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$3.accept(TransportCancelTasksAction.java:130)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$3.accept(TransportCancelTasksAction.java:127)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock.finish(TransportCancelTasksAction.java:238)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock.onBanSet(TransportCancelTasksAction.java:226)
  2&gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$5.handleException(TransportCancelTasksAction.java:187)
  2&gt;    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  2&gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  2&gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; 
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=D7BCB71B5D341091 -Dtests.class=org.elasticsearch.action.admin.cluster.node.tasks.CancellableTasksTests -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en-US -Dtests.timezone=Etc/UTC
  2&gt; NOTE: test params are: codec=Asserting(Lucene54): {}, docValues:{}, sim=RandomSimilarity(queryNorm=true,coord=yes): {}, locale=nl-BE, timezone=Asia/Novosibirsk
  2&gt; NOTE: Linux 3.13.0-74-generic amd64/Oracle Corporation 1.8.0_72-internal (64-bit)/cpus=4,threads=1,free=451774008,total=530579456
  2&gt; NOTE: All tests run in this JVM: [NullValueObjectMappingTests, IndexSearcherWrapperTests, CompletionFieldMapperTests, VersionLookupTests, CommonTermsQueryParserTests, NamingConventionTests, ScriptContextRegistryTests, NettyTransportTests, TransportModuleTests, MustacheTests, AnalyzerBackwardsCompatTests, FileUtilsTests, TableTests, SettingsValidatorTests, FilterFieldDataTests, ExpectedShardSizeAllocationTests, ClusterStateHealthTests, IndexingMemoryControllerTests, DocumentFieldMapperTests, PluginManagerUnitTests, UniqueTokenFilterTests, SortedSetDVStringFieldDataTests, LuceneTests, TransportClientHeadersTests, DiskThresholdDeciderUnitTests, PrimaryNotRelocatedWhileBeingRecoveredTests, GatewayServiceTests, GeoUtilsTests, KeepTypesFilterFactoryTests, IndicesServiceTests, EsExecutorsTests, RetryTests, FileInfoTests, CustomBoostMappingTests, Base64Tests, TransportTasksActionTests, PreBuiltTokenFilterFactoryFactoryTests, IndexTypeMapperTests, IndexFieldDataServiceTests, PutWarmerRequestTests, PathMapperTests, ReplicaAllocatedAfterPrimaryTests, SourceFieldTypeTests, FilterPathGeneratorFilteringTests, MinScoreScorerTests, JsonXContentTests, UpdateRequestTests, RestApiParserTests, ClusterSerializationTests, ParentQueryTests, ShadowEngineTests, CodecTests, ScriptSortBuilderTests, XContentHelperTests, CompressSourceMappingTests, MetaDataTests, ThrottlingAllocationTests, FilterRoutingTests, StringFieldMapperPositionIncrementGapTests, BlacklistedPathPatternMatcherTests, RecoveriesCollectionTests, MetaStateServiceTests, ConcurrentRecyclerTests, TransportReplicationActionTests, SimpleDateMappingTests, XMoreLikeThisTests, NoisyChannelSpellCheckerTests, DeflateXContentTests, MemoryCircuitBreakerTests, IndexStoreBWCTests, TribeUnitTests, IndexRequestBuilderTests, InternalTestClusterTests, SnowballAnalyzerTests, PreferLocalPrimariesToRelocatingPrimariesTests, ParseDocumentTypeLevelsTests, ProfileTests, CountResponseTests, BinaryFieldTypeTests, StopTokenFilterTests, ZenFaultDetectionTests, BigArraysTests, ParseFieldTests, GeoShapeFieldTypeTests, LongHashTests, ChildrenConstantScoreQueryTests, PlainHighlighterTests, VectorHighlighterTests, LongFieldTypeTests, ClassPermissionTests, DynamicMappingTests, JacksonLocationTests, ShapeBuilderTests, RestAnalyzeActionTests, SeccompTests, BoundTransportAddressTests, SingleObjectCacheTests, CopyOnWriteHashMapTests, SetupSectionParserTests, TimeZoneRoundingTests, GeoPointFieldMapperTests, ScriptParameterParserTests, ProcessProbeTests, StringFieldTypeTests, UpdateThreadPoolSettingsTests, DiskUsageTests, SearchServiceTests, SmileFilteringGeneratorTests, TypeFieldTypeTests, ByteFieldTypeTests, NestedAggregatorTests, SloppyMathTests, NetworkAddressTests, SingleOrdinalsTests, NodeEnvironmentTests, DeadNodesAllocationTests, RoutingServiceTests, HppcMapsTests, NumericAnalyzerTests, IndicesOptionsTests, ZenDiscoveryUnitTests, ShardUtilsTests, DuelFieldDataTests, MergePolicySettingsTests, IndexQueryParserFilterDateRangeTimezoneTests, ThreadPoolSerializationTests, SimpleDynamicTemplatesTests, NetworkUtilsTests, LZFXContentTests, ESDirectoryReaderTests, SyncedFlushUnitTests, StoreTests, PathTests, CheckFileCommandTests, IndexAliasesServiceTests, CompoundAnalysisTests, ReplicaShardAllocatorTests, RefCountedTests, DistanceUnitTests, CustomPassageFormatterTests, CustomPostingsHighlighterTests, ExceptionSerializationTests, OriginalIndicesTests, ClusterHealthResponsesTests, CancellableTasksTests]
ERROR   0.00s J1 | CancellableTasksTests (suite) &lt;&lt;&lt;
   &gt; Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=1259, name=elasticsearch[TransportTasksActionTests][generic][T#8], state=RUNNABLE, group=TGRP-CancellableTasksTests]
   &gt; Caused by: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$3@2c0ff1dc on EsThreadPoolExecutor[generic, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@22c80a2d[Shutting down, pool size = 4, active threads = 4, queued tasks = 0, completed tasks = 62]]]
   &gt;    at __randomizedtesting.SeedInfo.seed([D7BCB71B5D341091]:0)
   &gt;    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.rejectedExecution(EsAbortPolicy.java:50)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
   &gt;    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.execute(EsThreadPoolExecutor.java:85)
   &gt;    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:341)
   &gt;    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:299)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.sendRemoveBanRequest(TransportCancelTasksAction.java:205)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.removeBanOnNodes(TransportCancelTasksAction.java:166)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction.access$000(TransportCancelTasksAction.java:63)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$3.accept(TransportCancelTasksAction.java:130)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$3.accept(TransportCancelTasksAction.java:127)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock.finish(TransportCancelTasksAction.java:238)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$BanLock.onBanSet(TransportCancelTasksAction.java:226)
   &gt;    at org.elasticsearch.action.admin.cluster.node.tasks.cancel.TransportCancelTasksAction$5.handleException(TransportCancelTasksAction.java:187)
   &gt;    at org.elasticsearch.transport.TransportService$2.run(TransportService.java:206)
   &gt;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
   &gt;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
   &gt;    at java.lang.Thread.run(Thread.java:745)
Completed [345/556 (1!)] on J1 in 0.36s, 2 tests, 1 error &lt;&lt;&lt; FAILURES!
```

This is from: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.3+multijob-os-compatibility/os=ubuntu/4/ 

but it also happened earlier on 2.x, e.g.

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+periodic/96/
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-os-compatibility/os=fedora/42/
https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-os-compatibility/os=ubuntu/42/

I wasn't able to reproduce the failure locally yet. 
@imotov, @nik9000 is this something you could take a look at?
</description><key id="141596292">17169</key><summary>CI: CancellableTasksTests fails with uncaught exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label><label>test</label></labels><created>2016-03-17T14:18:50Z</created><updated>2016-03-17T18:34:12Z</updated><resolved>2016-03-17T18:34:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-03-17T14:29:41Z" id="197902928">I think we need to backport 8652cd8 to 2.x, 2.3 as well as get #16965 to get rid of these.
</comment><comment author="nik9000" created="2016-03-17T14:56:02Z" id="197917541">I knew I saw a thing about this. I can have a look at it in a few hours if no one else gets to it.
</comment><comment author="nik9000" created="2016-03-17T18:26:52Z" id="198018699">Ok - if you backport 8652cd8aa3eb03ac10c53be8be7ad75f2a3e1175 you have to backport 17e28d05b75f092ab7f2ee2a20230ab5dc4bfc88. That test starts failing once you backport the first change and it looks like the logic in 17e28d05b75f092ab7f2ee2a20230ab5dc4bfc88 holds in 2.3/2.x so I'll just backport both.

@s1monw if you have any objections to this then reply and I'll revert them.
</comment><comment author="nik9000" created="2016-03-17T18:29:07Z" id="198020306">2.3: d32c7fb8a89e92d3ce63b266222cedaa9832818c and a0df1479a6882c85f3619bf196fb326f26774a1d
2.x: 114001db60bb01741c7de70dd8cb147af4183207 and 8c053bd92e89166cb0e2a5efe09fbb62b5fc2132
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Display reindex API and fix build doc issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17168</link><project id="" key="" /><description /><key id="141591481">17168</key><summary>Docs: Display reindex API and fix build doc issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>docs</label></labels><created>2016-03-17T14:00:57Z</created><updated>2016-03-18T09:50:01Z</updated><resolved>2016-03-18T09:45:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-17T18:33:21Z" id="198022700">LGTM but @clintongormley is the expert.
</comment><comment author="clintongormley" created="2016-03-17T19:47:49Z" id="198054177">haven't built it, but LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>parent_id query should take the child type into account too</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17167</link><project id="" key="" /><description>If this query doesn't take the child type into account then it can match other
child document types pointing to the same parent type and that have the same id too.

This is a non issue, because the `parent_id` query hasn't been released yet.
</description><key id="141591371">17167</key><summary>parent_id query should take the child type into account too</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T14:00:39Z</created><updated>2016-03-17T19:41:59Z</updated><resolved>2016-03-17T16:01:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-17T14:58:45Z" id="197919894">LGTM
</comment><comment author="martijnvg" created="2016-03-17T16:01:46Z" id="197948424">Thanks @jpountz!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't run snapshot build of elasticsearch 5.0 with plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17166</link><project id="" key="" /><description>**Elasticsearch version**: 
5.0.0-SNAPSHOT (build from master using `gradle assemble`

**JVM version**: 
java version "1.8.0_45"
Java(TM) SE Runtime Environment (build 1.8.0_45-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)

**OS version**:
OSX El Capitan Version 10.11.3 (15D21)

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:

**Scenario 1**:
1. Build a snapshot build from master using `gradle assemble`
2. Extract tar.gz
3. Run bin/elasticsearch
   4.Start up of the node fails due to the following exception 

[2016-03-17 11:34:31,066][INFO ][node                     ] [Captain Savage] version[5.0.0-SNAPSHOT], pid[4048], build[da24bfe/2016-03-17T11:23:11.636Z]
[2016-03-17 11:34:31,068][INFO ][node                     ] [Captain Savage] initializing ...
Exception in thread "main" java.lang.IllegalArgumentException: Plugin [ingest-grok] is incompatible with Elasticsearch [5.0.0]. Was designed for version [3.0.0-SNAPSHOT]
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:100)
    at org.elasticsearch.plugins.PluginsService.getModuleBundles(PluginsService.java:304)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:117)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:187)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:168)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:188)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:262)
    at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:108)
    at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:103)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:74)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:67)
Refer to the log for complete error details.

**Scenario 2**:
1. Build a snapshot build from master using `gradle assemble`
2. Extract tar.gz
3. Try to install one of the built plugins using `bin/elasticsearch-plugin install file:///Path/to/built/S3-plugin.zip
   4.Installing the plugin fails due to the following exception 

Plugins directory [/Users/me/elasticsearch-5.0.0-SNAPSHOT/plugins] does not exist. Creating...
-&gt; Downloading file:///Users/me/repository-s3-5.0.0-SNAPSHOT.zip
Exception in thread "main" java.lang.IllegalArgumentException: Plugin [repository-s3] is incompatible with Elasticsearch [5.0.0]. Was designed for version [3.0.0-SNAPSHOT]
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:100)
    at org.elasticsearch.plugins.InstallPluginCommand.verify(InstallPluginCommand.java:287)
    at org.elasticsearch.plugins.InstallPluginCommand.install(InstallPluginCommand.java:345)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:169)
    at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:155)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)
    at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:88)
    at org.elasticsearch.cli.Command.main(Command.java:53)
    at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:45)

It looks like the versioning process in the release/branching might not be updating the versions in the `plugin-descriptor.properties` for both plugins and modules
</description><key id="141559061">17166</key><summary>Can't run snapshot build of elasticsearch 5.0 with plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Plugins</label><label>feedback_needed</label></labels><created>2016-03-17T11:42:24Z</created><updated>2016-03-17T13:40:26Z</updated><resolved>2016-03-17T13:06:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-17T12:46:10Z" id="197861830">@colings86 I think that you just need to do a `gradle clean`. Can you verify if that fixes the issue for you?
</comment><comment author="colings86" created="2016-03-17T13:06:06Z" id="197870962">@jasontedor yep you are right, a `gradle clean` fixed it. Sorry for the noise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>version_conflict_engine_exception with bulk update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17165</link><project id="" key="" /><description>**Elasticsearch version**: 

```
"version" : {
    "number" : "2.1.1",
    "build_hash" : "40e2c53a6b6c2972b3d13846e450e66f4375bd71",
    "build_timestamp" : "2015-12-15T13:05:55Z",
    "build_snapshot" : false,
    "lucene_version" : "5.3.1"
  }
```

**JVM version**:  

```
"jvm":{"pid":15324,"version":"1.7.0_07","vm_name":"Java HotSpot(TM) Client VM","vm_version":"23.3-b01","vm_vendor":"Oracle Corporation","start_time_in_millis":1458163388025,"mem":{"heap_init_in_bytes":268435456,"heap_max_in_bytes":1037959168,"non_heap_init_in_bytes":12746752,"non_heap_max_in_bytes":100663296,"direct_max_in_bytes":1037959168}
```

**OS version**: 

```
"os":{"refresh_interval_in_millis":1000,"name":"Windows Server 2008 R2","arch":"x86","version":"6.1","available_processors":4,"allocated_processors":4},"process":{"refresh_interval_in_millis":1000,"id":15324,"mlockall":false},
```

**Description of the problem including expected versus actual behavior**:
I'm doing the document update with two bulk requests. The first request contains three updates and the second bulk request contains just one.
For the first bulk request the response is completely success but response for the second one said about version conflict.
The first request contains three updates of the document:

```
16:27:34.325 {ElasticSearch} 
HTTP Path: /_bulk 
HTTP POST Request: {
....
{"update": {"_index": "session-2016.03.14", "_type": "session", "_id": "3"}}
{"doc":{"states":{"state1":{"info": "some state info"}}}}

{"update": {"_index": "session-2016.03.14", "_type": "session", "_id": "3"}}
{"doc":{"states":{"state2":{"info": "some state info"}}}}


{"update": {"_index": "session-2016.03.14", "_type": "session", "_id": "3"}}
{"doc":{"states":{"state3":{"info": "some state info"}}}}
....
```

Then the second one which contains just one update:

```
16:27:34.334 {ElasticSearch} 
HTTP Path: /_bulk 
HTTP POST Request: 
{"update": {"_index": "session-2016.03.14", "_type": "session", "_id": "3"}}
{"doc":{"states":{"state4":{"info": "some state info"}}}}
```

And then the response for first request where all statuses are 200:

```
16:27:34.391 {ElasticSearch} Response from ElasticSearch localhost:9200: 
("took"=63,"errors"="false","items"=(

"JSON_ARRAY_ELEM"=("update"=("_index"="session-2016.03.14","_type"="session","_id"="3","_version"=6,"_shards"=("total"=2,"successful"=1,"failed"=0),"status"=200)),

"JSON_ARRAY_ELEM"=("update"=("_index"="session-2016.03.14","_type"="session","_id"="3","_version"=7,"_shards"=("total"=2,"successful"=1,"failed"=0),"status"=200)),

"JSON_ARRAY_ELEM"=("update"=("_index"="session-2016.03.14","_type"="session","_id"="3","_version"=8,"_shards"=("total"=2,"successful"=1,"failed"=0),"status"=200))))
```

And response for the second request with status 409:

```
16:27:34.391 {ElasticSearch} Response from ElasticSearch localhost:9200: 
("took"=25,"errors"="true","items"=(
...
"JSON_ARRAY_ELEM"=("update"=("_index"="session-2016.03.14","_type"="session","_id"="3","status"=409,"error"=("type"="version_conflict_engine_exception","reason"="[session][3]: version conflict, current [6], provided [5]","index"="session-2016.03.14","shard"="1"))),
....
```

**Steps to reproduce**:
 There is no some especial steps for reproduce, and I've observed it just once.

Additional info:

```
"gc_collectors":["Copy","MarkSweepCompact"],"memory_pools":["Code Cache","Eden Space","Survivor Space","Tenured Gen","Perm Gen"]},"thread_pool":{"generic":{"type":"cached","keep_alive":"30s","queue_size":-1},"index":{"type":"fixed","min":4,"max":4,"queue_size":200},"fetch_shard_store":{"type":"scaling","min":1,"max":8,"keep_alive":"5m","queue_size":-1},"get":{"type":"fixed","min":4,"max":4,"queue_size":1000},"snapshot":{"type":"scaling","min":1,"max":2,"keep_alive":"5m","queue_size":-1},"force_merge":{"type":"fixed","min":1,"max":1,"queue_size":-1},"suggest":{"type":"fixed","min":4,"max":4,"queue_size":1000},"bulk":{"type":"fixed","min":4,"max":4,"queue_size":50},"warmer":{"type":"scaling","min":1,"max":2,"keep_alive":"5m","queue_size":-1},"flush":{"type":"scaling","min":1,"max":2,"keep_alive":"5m","queue_size":-1},"search":{"type":"fixed","min":7,"max":7,"queue_size":1000},"fetch_shard_started":{"type":"scaling","min":1,"max":8,"keep_alive":"5m","queue_size":-1},"listener":{"type":"fixed","min":2,"max":2,"queue_size":-1},"percolate":{"type":"fixed","min":4,"max":4,"queue_size":1000},"refresh":{"type":"scaling","min":1,"max":2,"keep_alive":"5m","queue_size":-1},"management":{"type":"scaling","min":1,"max":5,"keep_alive":"5m","queue_size":-1}},..."max_content_length_in_bytes":104857600},"plugins":[]}}}
```
</description><key id="141557630">17165</key><summary>version_conflict_engine_exception with bulk update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">atm028</reporter><labels /><created>2016-03-17T11:36:15Z</created><updated>2016-03-18T13:10:48Z</updated><resolved>2016-03-17T13:38:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T13:38:01Z" id="197882590">@atm028 Your second update request happened at the same time as another request, so between fetching the document, updating it, and reindexing it, another request made an update.  

See the `retry_on_conflict` parameter in the docs: https://www.elastic.co/guide/en/elasticsearch/reference/2.2/docs-update.html#_parameters_3
</comment><comment author="atm028" created="2016-03-18T07:53:27Z" id="198243018">@clintongormley But single client and single Elasticsearch node has been used and client sent both requests in range of single connection(http 1.1 with keep-alived connection). Where the another process comes from? Or it means that each request handling in own thread? Even from the same connection.
</comment><comment author="clintongormley" created="2016-03-18T09:33:52Z" id="198275760">If you send a request and wait for the response before sending the next request, then they will be executed serially.  But I think you've sent more requests than you realise, eg looking at the error message:

```
version conflict, current [6], provided [5]
```

...you've made more than one update to that document
</comment><comment author="atm028" created="2016-03-18T09:51:12Z" id="198284474">That's true, the second update request has been sent before the first one has been done. But if the requests has been sent in single connection then updates to the document should be enrolled sequentially. And then two responses will be send to the client. Of course if the handling of them works in single thread, since it single connection. At least in code the same thread context used for dispatching request. Doesn't it? 
</comment><comment author="clintongormley" created="2016-03-18T11:37:22Z" id="198318370">No.  Requests are handled asynchronously.
</comment><comment author="atm028" created="2016-03-18T13:10:48Z" id="198348336">@clintongormley ok, thank you, now the reason is clear
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: QueryProfilerIT.testProfileMatchesRegular failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17164</link><project id="" key="" /><description>This fails reproducibly on 2.3 branch:

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.3+multijob-os-compatibility/os=amazon/4/
Revision: a1f927d293c08ea1620fb87ad4f58602b9069dc7

mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=C45E4C78F8107B36 -Dtests.class=org.elasticsearch.search.profile.QueryProfilerIT -Dtests.method="testProfileMatchesRegular" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=en-AU -Dtests.timezone=America/Tortola
</description><key id="141542243">17164</key><summary>CI: QueryProfilerIT.testProfileMatchesRegular failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>jenkins</label><label>test</label><label>v2.3.0</label></labels><created>2016-03-17T10:33:12Z</created><updated>2016-03-17T17:55:51Z</updated><resolved>2016-03-17T17:55:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-17T10:36:24Z" id="197811889">@polyfractal would this be something you can look into?
</comment><comment author="polyfractal" created="2016-03-17T14:24:11Z" id="197900840">@cbuescher On it, thanks for the ping.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17163</link><project id="" key="" /><description>I found that, despite fields with parentheses are valid on Kibana, timelion do not allow it even if they are escaped or enclosed into quotation marks. I opened an issue in the Timelion github but, I thought, what about alias of fields?, sometimes there are long fields with details about the metric (like if it's on ms or seconds) and could be quite useful to have field alias on elasticsearch or kibana. Like the index alias.
</description><key id="141538149">17163</key><summary>Field aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">carlosvega</reporter><labels /><created>2016-03-17T10:17:57Z</created><updated>2016-03-17T13:50:17Z</updated><resolved>2016-03-17T13:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T13:32:12Z" id="197880343">Hi @carlosvega 

This seems relevant to Kibana only.  I'd suggest opening this issue in the Kibana issues list instead.
</comment><comment author="carlosvega" created="2016-03-17T13:35:04Z" id="197881472">No, it is relevant to Elasticsearch the same way the index aliases are. This could be useful in general for the REST API of elastic as well. 
</comment><comment author="clintongormley" created="2016-03-17T13:44:27Z" id="197885104">&gt; sometimes there are long fields with details about the metric (like if it's on ms or seconds)

that is kibana only, nothing to do with elasticsearch.  

if you're talking about adding an alias from eg `foo(bar)` to `foo_bar`, I don't really see the point of cluttering your mappings (and the cluster state) with this.  Instead, you could use an ingest pipeline to rename the field, or the `copy_to` mapping parameter to do something similar.
</comment><comment author="carlosvega" created="2016-03-17T13:50:17Z" id="197886932">In fact, it is a Timelion issue (already done that issue), but I thought it would be an interesting suggest for elasticsearch. 

I know I could add a new field with the same value to all my documents but that could take too long. I will do that now, but it could be a nice feature to avoid this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable the indices request cache by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17162</link><project id="" key="" /><description>Now we have #16870 we can enable the request cache by default. The caching can still be disabled on a per request basis and can still be disabled in the settings, only the default value has changed. For now this is done regardless of whether the shard is active or inactive. The reasons for not looking at whether the shard is active or inactive are:
1. result caching is cheap. 
2. the cache is small and easy to GC 
3. Users press refresh frequently which, without caching, would result in the same query being rerun on an active shard.  This way we reduce the load for very little cost

Closes #17134
</description><key id="141522016">17162</key><summary>Enable the indices request cache by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Cache</label><label>enhancement</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T09:07:33Z</created><updated>2016-03-17T13:22:28Z</updated><resolved>2016-03-17T09:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-17T09:08:22Z" id="197776855">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for pluggable/custom QueryCachingPolicy </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17161</link><project id="" key="" /><description>It would be great if we can define our own QueryCachingPolicy implementation and ask Elasticsearch to use it rather than the default one. This will give us the flexibility to write our own caching policy.
So once we make this implementation and ship it as a plugin , the following configuration in the elasticsearch.yml file should chose our QueryCachingPolicy class rather than the default one.

index.query_caching_policy: MyCustomQueryCachingPolicy
</description><key id="141515535">17161</key><summary>Support for pluggable/custom QueryCachingPolicy </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vineeth-Mohan</reporter><labels><label>:Cache</label><label>:Search</label><label>discuss</label></labels><created>2016-03-17T08:40:22Z</created><updated>2016-03-18T09:30:38Z</updated><resolved>2016-03-18T09:16:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-17T13:42:36Z" id="197884184">May I ask what you are missing from the default one? Maybe we could just improve it?
</comment><comment author="Vineeth-Mohan" created="2016-03-17T13:51:12Z" id="197887171">@jpountz  - I know this is being debated on another issue. But i badly want the ability to decide if the filter needs to be cached or not cached with me itself. Depending on my client ( Free or paid ) , his usage , the volume of data he is going to search etc , we make decision for enabling caching or not on the set of keywords ( Kept in another document and looked up using terms lookup filter ) he is going to search. 

You can think this for situations like , when a client logs in , before he makes any search , we plan to cache his filter so that when he search the performance is good. When he logs out , we remove the filter too.  Similarly we have other use cases too. 

Hence kindly make expose QueryCachingPolicy via a flag or configuration or something , or provide the pluggability so that we can do it ourself.  
</comment><comment author="clintongormley" created="2016-03-17T19:34:33Z" id="198048531">&gt; we plan to cache his filter so that when he search the performance is good.

@Vineeth-Mohan and what does his filter look like?  Is it a simple term query on customer ID?
</comment><comment author="jpountz" created="2016-03-18T09:16:08Z" id="198269946">&gt; Depending on my client ( Free or paid ) [...] we make decision for enabling caching or not on the set of keywords

I don't think this would be a good idea: if free customers are hammering the cluster, then paid customers will have a better experience if caching is enabled for the free customers since their queries will use less CPU/IO. From that perspective it is better to just cache the filters that are used most often.

&gt; when a client logs in , before he makes any search , we plan to cache his filter so that when he search the performance is good

This filter is likely a term filter and term filters barely need any caching: they are already plenty fast thanks to the way inverted indices work.

The reason why I am cautious about exposing such internals of Elasticsearch is that the filter cache is not a regular cache. Regular caches trade memory for latency and throughput, they can barely make any harm if they are configured correctly. On the contrary, elasticsearch's filter cache can severely hurt latency and throughput if it is too aggressive: the inverted index encodes skip lists than can be used to efficiently skip over documents that do not match. If caching is too aggressive then many queries will have to read entire postings lists and load them into memory while the query maybe only needed to know about 10 matching docs.
</comment><comment author="Vineeth-Mohan" created="2016-03-18T09:19:36Z" id="198270787">@clintongormley  - Well the main filter is  a terms lookup on 100,000 terms. So i have a field called ids and for each user , he is interested in 100,000 ids alone and those documents alone should be shown to him. I have used a term lookup query to achieve this. This is applied to 12 billion records.
</comment><comment author="Vineeth-Mohan" created="2016-03-18T09:30:38Z" id="198274781">@jpountz - I am using a terms lookup filter of 100,000 terms against a 9 billion record indices per customer. The free client has his own set of keywords and the paid customer has his own set of keywords. And as the cache i make is one cache per the entire 100,000 terms for a client , it cant be reused for another client.  The main reason being , even if 1 keyword is different , the cache key would be different and we cant reuse the filter cache there. 

I saw the issue where you have planned to make the terms lookup query expensive , but then when i checked the code , expensive queries need 2 execution for caching. Is there some solution to make this query to get cached in the first go itself.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify how `-Djava.security.policy=someURL` must be passed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17160</link><project id="" key="" /><description>JVM specific options must be passed via `JAVA_OPTS` to the elasticsearch
process. If they are passed as a cmd arg they are simply ignored today which
can be very very confusing and should be addressed in a different issue.

@clintongormley can you take a look at this
</description><key id="141511254">17160</key><summary>Clarify how `-Djava.security.policy=someURL` must be passed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>docs</label><label>review</label><label>v2.2.2</label><label>v2.3.0</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T08:21:47Z</created><updated>2016-03-17T13:14:10Z</updated><resolved>2016-03-17T13:14:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Avoid the fetch phase when retrieving only _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17159</link><project id="" key="" /><description>Is it possible to avoid the fetch phase for a search and return only document IDs? Is _id available at the end of the query phase such that fetch would be redundant when all I want is the _id? Can this be done through the current API? 

Ultimately, I'm hoping to retrieve doc IDs from a search much faster than what I've seen so far. I've tried all documented ways in all sorts of permutations to get better performance, and I've found no satisfactory results. The best I've achieved was only a 25% speed improvement by, in parallel, querying each of 5 shards individually. An acceptable speed would be 90% faster. It would help a lot to understand whether this is reasonable and why if it is not. It's very difficult to understand why I can be given a) the first 100 results, b) the total count and c) have them sorted so quickly, but retrieving the results is very slow. 
</description><key id="141465908">17159</key><summary>Avoid the fetch phase when retrieving only _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gcampbell-epiq</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2016-03-17T03:03:21Z</created><updated>2017-07-14T10:03:18Z</updated><resolved>2016-10-25T07:41:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gcampbell-epiq" created="2016-03-17T03:17:54Z" id="197673618">Also, is there any possibility of improving performance for this (IDs only) scenario by developing a plugin? Are there any other options, documented or not that can reduce overhead?

Just to stress the importance of this, it would be crucial to our implementation and likely a deciding factor for our adoption of Elastic to replace our current massive persistence layer. 
</comment><comment author="jpountz" created="2016-03-18T11:08:54Z" id="198309519">How many ids are you retrieving per request? If few then I am surprised that the fetch phase is taking so long, if many then I'm afraid elasticsearch is not the right tool for the job: this is something that regular databases are better at.
</comment><comment author="gcampbell-epiq" created="2016-03-18T14:19:42Z" id="198380208">Returning few IDs is very fast. Returning 10k and up is slow. I'd like to understand why. Can you explain this? Also, I'd like to explore options for getting better performance. Could you provide some guidance or ideas on where to look developing performance improvements, e.g. plugin for Elastic, use Lucene directly? Why not try a query only (no fetch) search type?
</comment><comment author="nik9000" created="2016-03-18T14:40:25Z" id="198390580">&gt; I'd like to understand why.

The search phase fetches Lucene's doc ids (integers), not elasticsearch's ids (strings). The fetch phase looks up the doc ids using Lucene's stored fields mechanism. Stored fields are stored together in compressed chunks. Since _source is a stored field you have to decompress a lot of _source to get to the id field. Because it is chunked you also have to decompress stored fields for docs you didn't hit.

Aggregations are fast because they use doc values which is a non-chunked columnal structure. It is compressed, but using numeric tricks rather than a general purpose compression algorithm. If you can retool your work as an aggregation by pushing the interesting work to Elasticsearch then your thing can be orders of magnitude faster.
</comment><comment author="gcampbell-epiq" created="2016-03-18T16:14:21Z" id="198430945">That's a great explanation. Thank you so much for that and the idea. I will try it immediately. 

From looking at lucene/elastic code I had worried that the intermediate results from the query phase would not be usable. This comment appears in every(?) implementation of IndexReader in lucene.

`&lt;p&gt; For efficiency, in this API documents are often referred to via
 &lt;i&gt;document numbers&lt;/i&gt;, non-negative integers which each name a unique
 document in the index.  These document numbers are ephemeral -- they may change
 as documents are added to and deleted from an index.  Clients should thus not
 rely on a given document having the same number between sessions.`

But given this comment, I wonder if there is or could be an implementation of IndexReader that returns a usable ID. 
</comment><comment author="gcampbell-epiq" created="2016-03-18T17:25:31Z" id="198462110">This is exciting. Using the aggregation method, I was able to get back 10K IDs in 16ms. Via scroll, the same results took ~6000ms. Can you help me understand what costs or tradeoffs are made by using this method, e.g., is memory usage much greater, or performance degradation non-linear?
</comment><comment author="jpountz" created="2016-10-24T14:42:51Z" id="255760513">@jimferenczi I think I remember you did something about this?
</comment><comment author="jimczi" created="2016-10-25T07:41:11Z" id="255960630">@gcampbell-epiq in the upcoming 5.0 you can disable the stored fields retrieval. This should speed  up the search if you need `docvalue` or `fieldcache` fields only. For instance if you want to retrieve the `_uid` field you can do:

```
GET _search 
{
    "stored_fields": "_none_",
    "docvalue_fields": ["_uid"]
}
```

.. this will retrieve the `_uid` field from the fielddata (this field doesn't have docvalues) so it should be slow on the first query which needs to build the fielddata in the heap but from there the next search should be much faster than the regular one. 
</comment><comment author="cjbottaro" created="2016-12-06T16:59:30Z" id="265206245">Can someone explain how to use aggregations to return document ids only and avoid the slow fetching?</comment><comment author="nik9000" created="2016-12-06T18:17:55Z" id="265228124">&gt; Can someone explain how to use aggregations to return document ids only and avoid the slow fetching?

Do what @jimczi suggests above - disable `stored_fields` and fetch only fields with docvalues. Your best bet is to only use this with fields that have docvalues like `keyword` fields or numbers.</comment><comment author="lanpay-lulu" created="2017-07-14T10:03:18Z" id="315322486">I suggest use another field to store uid and fetching it with docvalues which should be fast enough.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split Analyzer and Writer into multiple pieces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17158</link><project id="" key="" /><description>Both the Analyzer and Writer were around 3000 lines of code making them very difficult to refactor/modify especially in an IDE.  I've split both of these into multiple more manageable pieces.  Please note there is no functional change in the code other than a few misspellings fixed in error messages.  This is not meant to refactor anything other than splitting up the files, so any requests for further refactoring will likely be pushed off for later PRs.
</description><key id="141455557">17158</key><summary>Split Analyzer and Writer into multiple pieces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Lang Painless</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T02:01:30Z</created><updated>2016-04-05T11:07:15Z</updated><resolved>2016-03-17T17:18:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-17T15:54:27Z" id="197944830">LGTM
</comment><comment author="jdconrad" created="2016-03-17T17:18:59Z" id="197983021">Checked into master via squash.  Thanks @rjernst
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>simplify handling top-level suggest results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17157</link><project id="" key="" /><description>Currently `Suggest`'s `toXContent` behaves on the name param. 
a `null` name writes suggestion entries without encapsulating them with a "suggest" 
object (needed for `_suggest` response) otherwise the suggestion entries are written in a 
"suggest" object (needed for `_search` response).

This adds a `toInnerXContent` to `Suggest` for the `_suggest` endpoint to avoid 
any/confusing usage of the name param.
</description><key id="141441750">17157</key><summary>simplify handling top-level suggest results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-17T00:36:40Z</created><updated>2016-03-17T04:36:15Z</updated><resolved>2016-03-17T04:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2016-03-17T00:42:07Z" id="197623871">@abeyad WDYT?
</comment><comment author="abeyad" created="2016-03-17T01:21:14Z" id="197635472">@areek Great changes, simplifies the code and makes each case (query vs suggest) distinct.  LGTM.
</comment><comment author="areek" created="2016-03-17T04:36:15Z" id="197697299">Thanks @abeyad for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cleanup request parsing in RestSearchAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17156</link><project id="" key="" /><description>Simplifies `_search` request parsing logic.
</description><key id="141429343">17156</key><summary>cleanup request parsing in RestSearchAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T23:15:59Z</created><updated>2016-03-16T23:43:11Z</updated><resolved>2016-03-16T23:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-16T23:25:03Z" id="197601822">Looks right to me.
On Mar 16, 2016 7:16 PM, "Areek Zillur" notifications@github.com wrote:

&gt; ## Simplifies _search request parsing logic.
&gt; 
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;   https://github.com/elastic/elasticsearch/pull/17156
&gt; Commit Summary
&gt; - cleanup request parsing in RestSearchAction
&gt; 
&gt; File Changes
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java
&gt;   https://github.com/elastic/elasticsearch/pull/17156/files#diff-0
&gt;   (15)
&gt; - _M_
&gt;   core/src/main/java/org/elasticsearch/rest/action/support/RestActions.java
&gt;   https://github.com/elastic/elasticsearch/pull/17156/files#diff-1
&gt;   (11)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elastic/elasticsearch/pull/17156.patch
&gt; - https://github.com/elastic/elasticsearch/pull/17156.diff
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17156
</comment><comment author="areek" created="2016-03-16T23:43:11Z" id="197607781">Thanks @nik9000 for the review :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: Automatically install plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17155</link><project id="" key="" /><description>So if i want to install any plugins, i have to run the `kibana plugin` command. Its a bit cumbersome as i have to install each plugin individually. Can i specify just the names of the plugins somewhere and when `kibana` starts up, it will automatically install these plugins. I remember that in Kibana 4.1.x, i could add the following to the `kibana.yml` file

```
bundled_plugin_ids:
 - plugins/dashboard/index
 - plugins/discover/index
 - plugins/doc/index
 - plugins/kibana/index
 - plugins/markdown_vis/index
 - plugins/metric_vis/index
 - plugins/settings/index
 - plugins/table_vis/index
 - plugins/vis_types/index
 - plugins/visualize/index
```

is this for already installed plugins? Or is it a way to specify auto install of plugins? Can we do something similar for Kibana 4.4.x?

How would we do this for Elastic?
</description><key id="141402005">17155</key><summary>Feature request: Automatically install plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2016-03-16T21:02:18Z</created><updated>2016-03-17T14:23:56Z</updated><resolved>2016-03-17T12:02:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2016-03-17T01:53:57Z" id="197646589">add if it supports hot install, that would be great.
</comment><comment author="clintongormley" created="2016-03-17T12:02:56Z" id="197848151">You should open this issue in the kibana issues list
</comment><comment author="abtpst" created="2016-03-17T14:22:42Z" id="197900385">thanks, but can we also do something similar for elastic? auto install the plugins?
</comment><comment author="clintongormley" created="2016-03-17T14:23:56Z" id="197900778">No we can't.  Plugins need to be installed before starting Elasticsearch.  If you have lots of plugins to install, then it would be better to do this with puppet or chef or some other automation tool.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regression Metric Aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17154</link><project id="" key="" /><description>This feature will extend #16817 by adding a single pass [Regression](https://en.wikipedia.org/wiki/Linear_regression) metric aggregator for computing a [Simple Linear Regression](https://en.wikipedia.org/wiki/Simple_linear_regression). (Single pass Multiple Linear Regression can be added as a follow on enhancement). The purpose is to compute and report the slope/intercept of the simple OLS trendline between each variable. 

Example usage (analyzing US Census data with mortality rates):

``` javascript
"my_regression" : {
    "regression" : {
        "field" : [ "poverty", "mortality", "income", "crime", "doctors"],
    }
}
```
</description><key id="141383147">17154</key><summary>Regression Metric Aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>discuss</label><label>feature</label><label>v5.4.4</label></labels><created>2016-03-16T19:51:56Z</created><updated>2017-06-27T10:28:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="k2xl" created="2017-02-09T20:39:24Z" id="278766426">Gets pushed back and back - this would be really useful especially considering prelert just joined elastic team</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SortBuilderParser.fromXContent is only called by tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17153</link><project id="" key="" /><description>I was looking into https://github.com/elastic/elasticsearch/issues/17085 and I saw that SortBuilderParser.fromXContent is only called by tests. Should it be removed?
</description><key id="141374495">17153</key><summary>SortBuilderParser.fromXContent is only called by tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2016-03-16T19:18:19Z</created><updated>2016-03-16T22:27:59Z</updated><resolved>2016-03-16T22:27:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-16T21:16:55Z" id="197552765">I feel like I have to be missing some invocation....
</comment><comment author="cbuescher" created="2016-03-16T22:27:59Z" id="197585290">@nik9000 SortBuilderParser is a new interface we introduced for the ongoing sort builder refactoring. The method will be used once we cut over to use the refactored sort builders in SearchSourceBuilder. So no, we will need it ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It's not easy being green</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17152</link><project id="" key="" /><description>The QA Vagrant tests are vital for ensuring the correctness of the Elasticsearch distributions. These tests do not currently pass and should be brought to green.

Relates #16854
</description><key id="141372867">17152</key><summary>It's not easy being green</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>blocker</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T19:10:48Z</created><updated>2016-03-18T19:28:37Z</updated><resolved>2016-03-18T19:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="elasticdog" created="2016-03-16T19:19:59Z" id="197498585">See #16854 and https://github.com/elastic/infra/issues/145 where work is progressing on this issue. We discussed this during the engineering all-hands, and we want to switch over to the VirtualBox images managed by the infra team, as we should be able to duplicate them exactly in LXC-form in order to run the tests on AWS (nested virtual machines do not work on EC2, so vanilla virtual box won't work).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc discrepancy: the scroll_id parameter does not change between scrolled pgaes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17151</link><project id="" key="" /><description>**Elasticsearch version**:
2.2.0
**JVM version**:
1.7.0
**OS version**:
fedora 21
**Description of the problem including expected versus actual behavior**:
at https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search-request-scroll.html

it states that 

&gt; &gt; "The initial search request and each subsequent scroll request returns a new _scroll_id&#8201;&#8212;&#8201;only the most recent _scroll_id should be used."

But the scroll_id parameter never changes. it's given on the first scrolling request (The one which doesn't have scroll_id in their body), and then it's never returned. 

If you keep using the same scroll_id you move forward through the scrolled results pages

That's not very API, because the same GET request with the same `scroll_id` parameter produces different results

**Steps to reproduce**:
 1.
make an initial scroll request

```
curl localhost:9200/twitter/tweet/_search?scroll=1m
```

The results come with the `_scroll_id` key

 2.

make a second request of the scrolled results

```
curl http://localhost:9200/_search/scroll?scroll_id={{scroll_id}}
```
- the results correspond to the scroll second page
- no `_scroll_id` key is present
  3.
  keep doing the same rqeuest

it keeps returning different results

**Provide logs (if relevant)**:
</description><key id="141348711">17151</key><summary>doc discrepancy: the scroll_id parameter does not change between scrolled pgaes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JoeZ99</reporter><labels /><created>2016-03-16T17:41:11Z</created><updated>2016-03-17T13:51:58Z</updated><resolved>2016-03-17T11:43:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T11:43:39Z" id="197841754">You're missing the `scroll=1m` parameter.  The scroll expiry time needs to be refreshed on every scroll request
</comment><comment author="JoeZ99" created="2016-03-17T13:36:32Z" id="197881949">well, yes, sorry I skipped that, and there is indeed a `_scroll_id` key returned in every rrequest. But the issue (doc discrepancy) remain the same. 
- At the doc, it says
  "The initial search request and each subsequent scroll request returns a new _scroll_id&#8201;&#8212;&#8201;only the most recent _scroll_id should be used."
- There is a scroll_id present in any response, **but it's always the same scroll_id**, so you get different results with exactly the same GET request.  
</comment><comment author="clintongormley" created="2016-03-17T13:45:40Z" id="197885594">&gt; There is a scroll_id present in any response, but it's always the same scroll_id, so you get different results with exactly the same GET request.

Yes? What's the problem with that?
</comment><comment author="clintongormley" created="2016-03-17T13:46:04Z" id="197885737">The scroll ID will change when a shard exhausts its result set.
</comment><comment author="JoeZ99" created="2016-03-17T13:51:58Z" id="197887386">no problem at all, just wanted to be sure. I was under the impression that a good GET request should provide exactly the same result when exactly the same resource is asked (I think I read it somewhere from RESTfull theorists). Anyway, maybe in the doc could be stated that scroll_id "may" be different and then the last one should be used.

Ok, txs again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 2.2.0: different scrolling response depending on a deprecated search_type=scan parameter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17150</link><project id="" key="" /><description>**Elasticsearch version**:
2.2.0
**JVM version**:
1.7.0

**OS version**:
Fedora 21
**Description of the problem including expected versus actual behavior**:

When making an initial scroll request, if `search_type=scan` parameter is included, the response only contains scroll data. If not, it also contains the first page of results. According to the doc, `search_type=scan` is deprecated and shouldn't have any effect on that regards.

**Steps to reproduce**:
 1
.

```
curl localhost:9200/twitter/tweet/_search?scroll=1m
```

```
{
    "_scroll_id": "cXVlcnlUaGVuRmV0Y2g7MjszOTp1NHA1TXF4dFJIT0lwbVBTSkE5TThROzQwOnU0cDVNcXh0UkhPSXBtUFNKQTlNOFE7MDs=",
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 2,
        "successful": 2,
        "failed": 0
    },
    "hits": {
        "total": 999,
        "max_score": null,
        "hits": [
            {..},{..}
         ]
     }
}
```

 2.

```
curl localhost:9200/twitter/tweet/_search?scroll=1m&amp;search_type=scan
```

```
{
    "_scroll_id": "c2NhbjsyOzQxOnU0cDVNcXh0UkhPSXBtUFNKQTlNOFE7NDI6dTRwNU1xeHRSSE9JcG1QU0pBOU04UTsxO3RvdGFsX2hpdHM6OTk5Ow==",
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 2,
        "successful": 2,
        "failed": 0
    },
    "hits": {
        "total": 999,
        "max_score": 0,
        "hits": []
    }
}
```

 3.

see how the two results are different
**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="141345458">17150</key><summary>ES 2.2.0: different scrolling response depending on a deprecated search_type=scan parameter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JoeZ99</reporter><labels /><created>2016-03-16T17:29:17Z</created><updated>2016-03-16T18:08:51Z</updated><resolved>2016-03-16T17:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-16T17:44:28Z" id="197453300">&gt; According to the doc, `search_type=scan` is deprecated and shouldn't have any effect on that regards.

Scans have [always](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/search-request-scroll.html#scroll-scan) behaved in this fashion:

&gt; The response of the initial search request will not contain any results in the `hits` array. The first results will be returned by the first scroll request.

Instead of using a scan, you should use a scroll with sort on `_doc`:

```
$ curl -XGET localhost:9200/twitter/tweet/_search?scroll=1m -d '
{
    "sort": [
        "_doc"
    ]
}'
```

The deprecation just means that the feature will be removed, not that its behavior changed.
</comment><comment author="JoeZ99" created="2016-03-16T18:00:33Z" id="197459592">oops. I really got wrong the meaning of "deprecated". Sorry and thank you.
</comment><comment author="jasontedor" created="2016-03-16T18:08:51Z" id="197462296">&gt; Sorry and thank you.

No need to be sorry at all, I'm glad that we could help clear it up. :smile: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add bwc tests for 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17149</link><project id="" key="" /><description>similar to #15514
</description><key id="141330709">17149</key><summary>add bwc tests for 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.3.0</label><label>v2.4.0</label></labels><created>2016-03-16T16:41:49Z</created><updated>2016-03-29T07:52:22Z</updated><resolved>2016-03-16T16:54:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-16T16:45:02Z" id="197417703">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor fielddata mappings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17148</link><project id="" key="" /><description>The fielddata settings in mappings have been refactored so that:
- text and string have a `fielddata` (boolean) setting that tells whether it
  is ok to load in-memory fielddata. It is true by default for now but the
  plan is to make it default to false for text fields.
- text and string have a `fielddata_frequency_filter` which contains the same
  thing as `fielddata.filter.frequency` used to (but validated at parsing time
  instead of being unchecked settings)
- regex fielddata filtering is not supported anymore and will be dropped from
  mappings automatically on upgrade.
- text, string and _parent fields have an `eager_global_ordinals` (boolean)
  setting that tells whether to load global ordinals eagerly on refresh.
- in-memory fielddata is not supported on keyword fields anymore at all.
- the `fielddata` setting is not supported on other fields that text and string
  and will be dropped when upgrading if specified.
</description><key id="141326986">17148</key><summary>Refactor fielddata mappings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>:Mapping</label><label>breaking</label><label>review</label></labels><created>2016-03-16T16:28:33Z</created><updated>2016-03-23T08:53:15Z</updated><resolved>2016-03-23T08:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-17T08:16:22Z" id="197752865">It's great this is moving along

&gt; and will be dropped when upgrading if specified.

I checked and couldn't find any clear indication in the 2.x docs that this is going happen. If people chose to disable doc_values in favor of field data their data will no longer be usable in 5.0 for things like sorting and aggs (right?) . Should we only disable field data on keyword fields for new indices? 
</comment><comment author="jpountz" created="2016-03-17T08:43:04Z" id="197765879">String and text fields will keep supporting fielddata (on by default on string fields, off by default on text fields, and this can can changed after the index has been created anyway with a mapping update call).

However numeric and boolean fields will indeed not support fielddata anymore, but this pull request is not related, it was done in #14082.

We have been recommending doc values since 1.3/1.4, then made it the default in 2.0 and the [2.x docs](https://github.com/elastic/elasticsearch/blob/2.x/docs/reference/mapping/params/doc-values.asciidoc#doc_values) are clear I think: `If you are sure that you don't need to sort or aggregate on a field, or access the field value from a script, you can disable doc values in order to save disk space`. So I think it is fine to remove fielddata support in 5.0 (except for string and text fields).
</comment><comment author="clintongormley" created="2016-03-17T13:20:42Z" id="197876405">This PR doesn't include doc changes as @jpountz is planning to redo all the mapping docs in one big review in a different PR.
</comment><comment author="bleskes" created="2016-03-17T14:12:07Z" id="197895170">&gt; If you are sure that you don't need to sort or aggregate on a field, or access the field value from a script, you can disable doc values in order to save disk space

Fair enough, I think we should make it stronger though- something like "note that in 2.x you will still be able to use field data but that option will be removed in the future".

&gt; This PR doesn't include doc changes

Indeed. I was referring to the documentation of 2.x and whether people that have chosen to not index doc values and use field data for sorting have had enough warning that this approach will not work in the next major version and require reindexing.
</comment><comment author="jpountz" created="2016-03-17T16:40:49Z" id="197966251">@bleskes Should we add such a note in the 2.x docs next to the documentation I linked to?
</comment><comment author="bleskes" created="2016-03-17T16:46:42Z" id="197969519">@jpountz I think it's better placed at the field data docs, no? 
</comment><comment author="jpountz" created="2016-03-17T16:49:55Z" id="197970537">OK I'll give it a try. I was thinking `doc_values` because users who will be in trouble are those who disabled doc values.
</comment><comment author="rjernst" created="2016-03-23T07:07:29Z" id="200217626">LGTM, this is great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow access to the rest of the document from inside ingest foreach processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17147</link><project id="" key="" /><description>It would be convenient if access to the root object was possible when using an ingest `foreach`.  The most useful would be a `_root` scope, but perhaps `_parent` might also be useful in the case of deeply nested objects?  I'd be happy with `_root` to start :)

For example:

``` js
POST _ingest/pipeline/_simulate
{
   "pipeline": {
      "description": "test",
      "processors": [
         {
            "foreach": {
               "field": "values",
               "processors": [
                  {
                    "append": {
                       "field": "_root.values_flat",
                       "value": "{{_value.key}}_{{_value.value}}"
                    }
                 }
               ]
            }
         }
      ]
   },
   "docs": [
      {
         "_index": "index",
         "_type": "type",
         "_id": "id",
         "_source": {
            "values": [
               {
                  "level": 1,
                  "key": "foo",
                  "value": "bar"
               },
               {
                  "level": 2,
                  "key": "foo",
                  "value": "baz"
               }
            ]
         }
      }
   ]
}
```

Which would yield the documents:

``` js
"docs": [
      {
         "_index": "index",
         "_type": "type",
         "_id": "id",
         "_source": {
            "values_flat": [ "foo_bar", "foo_baz" ],  // &lt;-- new
            "values": [
               {
                  "level": 1,
                  "key": "foo",
                  "value": "bar"
               },
               {
                  "level": 2,
                  "key": "foo",
                  "value": "baz"
               }
            ]
         }
      }
   ]
```

/cc @martijnvg 
</description><key id="141326734">17147</key><summary>Allow access to the rest of the document from inside ingest foreach processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Ingest</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-03-16T16:27:37Z</created><updated>2016-03-22T09:32:55Z</updated><resolved>2016-03-22T09:32:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T11:18:01Z" id="197829919">Why do we need the `_root` notation? Wouldn't just specifying `values_flat` be sufficient?
</comment><comment author="clintongormley" created="2016-03-17T11:29:37Z" id="197833661">This looks like a bug to me - the append processor doesn't work inside the foreach processor:

```
POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "description": "test",
    "processors": [
      {
        "foreach": {
          "field": "values",
          "processors": [
            {
              "append": {
                "field": "flat",
                "value": [
                  "foo"
                ]
              }
            }
          ]
        }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_type": "type",
      "_id": "id",
      "_source": {
        "values": [
          "one",
          "two"
        ]
      }
    }
  ]
}
```
</comment><comment author="martijnvg" created="2016-03-17T13:28:48Z" id="197879294">The `foreach` processor was build to only access the fields form the the current json object being processed. Only `_value` key is available in all processors under the `foreach` processor. This context variable changes whenever this processor moves the next json object in the array.

The `foreach` processor should be changed to have the rest of document in its context as well. (`_value` remains to be a special key)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add build() method to SortBuilder implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17146</link><project id="" key="" /><description>For the current refactoring of SortBuilders related to #10217,
each SortBuilder should get a build() method that produces a
SortField according to the SortBuilder parameters on the shard.

This change also slightly refactors the current parse method in
SortParseElement to extract an internal parse method that returns
a list of sort fields only needs a QueryShardContext as input
instead of a full SearchContext. This allows using this internal
parse method for testing.
</description><key id="141325849">17146</key><summary>Add build() method to SortBuilder implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T16:24:25Z</created><updated>2016-03-22T12:46:50Z</updated><resolved>2016-03-22T12:46:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-16T16:27:16Z" id="197410012">@MaineC could you take a look at this? The generic test added to AbstractSortTestCase doesn't do many assertions on the two SortFields it compares, maybe you have an idea how to add to those tests. Also, those tests will be temporary and will be obsolete once we remove the old SortParseElement.
</comment><comment author="MaineC" created="2016-03-17T09:21:28Z" id="197784635">Overall looks awesome!

Left a few minor comments and questions.

About testing: I'm not sure we can test a whole lot more without knowing which type of sort we are dealing with. One idea (that would also be valid after removing the old parsing code) would be to add tests to the individual builders that have a few deeper assertions.

With this change it should already be possible to parse more than one sort element, no? This could be an addition to the tests.
</comment><comment author="cbuescher" created="2016-03-17T18:02:21Z" id="198004198">@MaineC thanks, I adressed most of your comments, still need to look into the Median-SortMode support and maybe open an issue about sorting with missing values.

&gt; About testing: I'm not sure we can test a whole lot more without knowing which type of sort we are dealing with. One idea (that would also be valid after removing the old parsing code) would be to add tests to the individual builders that have a few deeper assertions.

I will see if we can add a "assertSortField" method to each individual test that does some more inspection, maybe that works

&gt; With this change it should already be possible to parse more than one sort element, no? This could be an addition to the tests.

No, the SortParseElement will parse a whole `"sort" : {  }` block but the builders themselves currently only represent one sort option of which we can have many. This parsing / building needs to go somewhere up into the abstract SortBuilder or somewhere else I think. But I'd say that is a follow up PR.
</comment><comment author="cbuescher" created="2016-03-18T15:02:00Z" id="198399081">@MaineC I also adressed the rest of your comments, can you take another look if this looks good now?
</comment><comment author="MaineC" created="2016-03-22T09:11:40Z" id="199710252">Looked over your changes and the comments you added - LGTM
</comment><comment author="cbuescher" created="2016-03-22T11:37:12Z" id="199773525">@MaineC thanks, I needed to move over few more lines from NestedInnerQueryParseSupport to the part in SortBuilder that is building the nested filters. I added those and merged in master once again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix column aliases in _cat/indices, _cat/nodes and _cat/shards APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17145</link><project id="" key="" /><description>Fix #17101
</description><key id="141324865">17145</key><summary>Fix column aliases in _cat/indices, _cat/nodes and _cat/shards APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">alexshadow007</reporter><labels><label>:CAT API</label><label>bug</label><label>review</label><label>v2.2.2</label><label>v2.3.0</label><label>v2.4.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T16:20:50Z</created><updated>2016-03-29T07:52:22Z</updated><resolved>2016-03-22T14:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T11:16:26Z" id="197829268">@jpountz please could you review?
</comment><comment author="javanna" created="2016-03-22T14:36:56Z" id="199842355">LGTM @alexshadow007  thanks a lot for going the extra mile and fixing the bug throughout all the affected cat apis and the docs! I am going to merge this.
</comment><comment author="javanna" created="2016-03-22T18:25:35Z" id="199951039">Merged to master and backported to 2.x, 2.2 and 2.3 branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent index level setting from being configured on a node level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17144</link><project id="" key="" /><description>Today we allow to set all kinds of index level settings on the node level which
is error prone and difficult to get right in a consistent manner.
For instance if some analyzers are setup in a yaml config file some nodes might
not have these analyzers and then index creation fails.

Nevertheless, this change allows some selected settings to be specified on a node level
for instance:
- `index.codec` which is used in a hot/cold node architecture and it's value is really per node or per index
- `index.store.fs.fs_lock` which is also dependent on the filesystem a node uses

All other index level setting must be specified on the index level. For existing clusters the index must be closed
and all settings must be updated via the API on each of the indices.

Closes #16799
</description><key id="141321187">17144</key><summary>Prevent index level setting from being configured on a node level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking</label><label>enhancement</label><label>PITA</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T16:08:28Z</created><updated>2016-03-17T14:23:01Z</updated><resolved>2016-03-17T14:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-16T18:08:41Z" id="197462241">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Schedule commands in the current thread context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17143</link><project id="" key="" /><description>Right now `ThreadPool`'s `schedule` and `scheduleWithFixedDelay` schedule a command to run without any thread context. As of https://github.com/elastic/elasticsearch/pull/17077 it will be possible to intentionally schedule a command in the current thread context. We should make that behavior the default behavior.
</description><key id="141317282">17143</key><summary>Schedule commands in the current thread context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2016-03-16T15:55:57Z</created><updated>2016-03-17T11:15:33Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>flush in case we test against version &lt;=2.0.2 before upgrading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17142</link><project id="" key="" /><description>otherwise we might run into #15832

closes #17130
</description><key id="141291522">17142</key><summary>flush in case we test against version &lt;=2.0.2 before upgrading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2016-03-16T14:29:25Z</created><updated>2016-03-16T15:45:34Z</updated><resolved>2016-03-16T15:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-16T14:33:02Z" id="197356129">LGTM
</comment><comment author="s1monw" created="2016-03-16T14:33:02Z" id="197356134">can you put a comment in there to explain WHY we are doing this?
</comment><comment author="nik9000" created="2016-03-16T14:33:20Z" id="197356261">Good call.
</comment><comment author="brwe" created="2016-03-16T14:45:33Z" id="197361845">@s1monw added a comment
</comment><comment author="s1monw" created="2016-03-16T14:53:51Z" id="197365553">LGTM can you push this to 2.2 and 2.x and maybe also to 2.1?
</comment><comment author="brwe" created="2016-03-16T15:45:34Z" id="197390445">pushed to 2.x, 2.2 and 2.1 (0dfa084 and 2e185ee)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to disable doc_count for aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17141</link><project id="" key="" /><description>**Describe the feature**:
Sometimes you don't need the count value for aggregations and I guess omitting the count might improve performance?

Also have you considered an option for checking if a field with a specific value is present in the filtered query. This might be in relating with the above suggestion.

E.g. if you wanna know if the query contains a field 'color' with the value 'red' - just as a boolean. Maybe even provide an array of values.
</description><key id="141284661">17141</key><summary>Add option to disable doc_count for aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mpskovvang</reporter><labels /><created>2016-03-16T14:03:55Z</created><updated>2016-03-17T11:13:36Z</updated><resolved>2016-03-17T11:13:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-17T11:13:35Z" id="197827900">&gt; Sometimes you don't need the count value for aggregations and I guess omitting the count might improve performance?

The code path for checking if we should collect the count will probably be more expensive than just collecting it.

&gt; Also have you considered an option for checking if a field with a specific value is present in the filtered query. This might be in relating with the above suggestion.
&gt; 
&gt; E.g. if you wanna know if the query contains a field 'color' with the value 'red' - just as a boolean. Maybe even provide an array of values.

Sorry, I have no idea what this means, but it seems unrelated to the doc_count issue so I'm going to close this.  If you want to provide more details of your suggestion please open another issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Defining a custom analyzer should a accept `token_filters` settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17140</link><project id="" key="" /><description>Our Analyze API accepts a `token_filters` parameter as the official way of adding token filters to the chain (we also support `filter` which is depracated):

```
GET _analyze
{
  "text": "An apple",
  "tokenizer": "whitespace",
  "token_filters": [
    "lowercase", "stop"
  ]
}
```

However, creating an analyzer using indexing setting only accept `filter` (no S) which is confusing. I think the following should be accepted (if not the only default): 

```
PUT index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "token_filters": [ &lt;--- broken (expects "filter")
            "lowercase",
            "stop"
          ]
        }
      }
    }
  }
}
```

It's very easy to change the code to accept both `filter`, `filters` and `token_filters` but I tend to think that  we should go strict here and say we only accept `token_filters` and automatically upgrade existing setting when we recover. This will require (some) more work as that infra is not there yet.
</description><key id="141261416">17140</key><summary>Defining a custom analyzer should a accept `token_filters` settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Analysis</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-03-16T12:42:34Z</created><updated>2016-04-20T13:05:20Z</updated><resolved>2016-04-20T13:05:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="johtani" created="2016-04-20T08:30:55Z" id="212325226">@bleskes I think this issue related to https://github.com/elastic/elasticsearch/issues/15189 
</comment><comment author="clintongormley" created="2016-04-20T13:05:19Z" id="212415385">Duplicate of #15189
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add auto detection to convert processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17139</link><project id="" key="" /><description>The `ingest` processor is able to convert string values to either a boolean, integer or float value.
This processor needs to be configured in a strict manner. For example `field_a` need to be converted to an integer. If the field doesn't exist or can't be its value can't converted to an integer this processor fails.

The idea is here the add an `auto` convert type option that tries to convert strings to either a integer/float or a boolean value based on simple rules:
- true | false -&gt; boolean 
- [-+]?[0-9] -&gt; integer
- [-+]?[0-9]_.?[0-9]_ -&gt; float

If the `auto` convert type is unable to convert than it just does nothing and leaves that field untouched.

In order to auto detect a number fields it would be helpful if the `field` option would support wildcards too. 
For all fields that match the convert processor will convert the values based on the configured convert type. If no fields match with the wildcard pattern than the `convert` processor does nothing.

I think if this gets implemented then this can supersede number detections in mappings?
</description><key id="141244347">17139</key><summary>Add auto detection to convert processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/talevy/following{/other_user}', u'events_url': u'https://api.github.com/users/talevy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/talevy/orgs', u'url': u'https://api.github.com/users/talevy', u'gists_url': u'https://api.github.com/users/talevy/gists{/gist_id}', u'html_url': u'https://github.com/talevy', u'subscriptions_url': u'https://api.github.com/users/talevy/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/388837?v=4', u'repos_url': u'https://api.github.com/users/talevy/repos', u'received_events_url': u'https://api.github.com/users/talevy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/talevy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'talevy', u'type': u'User', u'id': 388837, u'followers_url': u'https://api.github.com/users/talevy/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>feature</label></labels><created>2016-03-16T11:25:18Z</created><updated>2016-03-29T14:57:57Z</updated><resolved>2016-03-29T14:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2016-03-16T16:53:36Z" id="197421099">another potential option for this is to to create a new field when converting, only if we manage to convert it. So, if you have `field1`, and we try to convert it to numeric, we will create `field1_num` with the numeric value, but if we can't convert, we can let ingest still work, just not create the `field1_num` field.
</comment><comment author="martijnvg" created="2016-03-17T07:59:59Z" id="197747534">Maybe we can add `target_field` option? Any conversion done by this processor will be put in that field.
</comment><comment author="talevy" created="2016-03-22T23:49:21Z" id="200082893">^^^ kind of solves this. Missing `target_field` for now

I wanted to get something cleared away with `target_field` first. How should List-valued fields behave with this target field? I am ok with treating it just as another type and setting a whole new field with the new list value.
</comment><comment author="martijnvg" created="2016-03-23T17:07:43Z" id="200444940">&gt; How should List-valued fields behave with this target field? 

I think like any other field, only here multiple conversions will take place and all the these converted values are placed into the new target field which will be an array too.
</comment><comment author="talevy" created="2016-03-23T17:14:16Z" id="200447415">that's what I thought, just wanted to double check. will update PR shortly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Path hierarchy tokenizer doc could be a bit more helpful</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17138</link><project id="" key="" /><description>**Elasticsearch version**: current

**JVM version**: irrelevant

**OS version**: irrelevant

**Description of the problem including expected versus actual behavior**:

The [official documentation for path hierarchy tokenizer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html) lacks usecase descriptions, as well as examples on performing filtering or aggregations on such fields. There is no explanation on why would anyone use this tokenizer.

Expected: the doc should explain why would I be using the tokenizer, and how do I aggregate/filter/etc. for the fields that use it.

**Steps to reproduce**:
Doc issue, so no steps here.
</description><key id="141240544">17138</key><summary>Path hierarchy tokenizer doc could be a bit more helpful</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kix</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-03-16T11:12:45Z</created><updated>2016-03-17T10:05:04Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-16T12:06:38Z" id="197287266">It can certainly be improved.  In the meantime, have a look at this: https://www.elastic.co/guide/en/elasticsearch/guide/master/denormalization-concurrency.html
</comment><comment author="s1monw" created="2016-03-17T08:50:38Z" id="197768774">@kix may I ask you to reword the title and description of this issue and use less aggressive words. I'd appreciate if you could use terminology like _Path hierarchy tokenizer doc is_ `not helpful`, `needs to be improved`, `misses usecase`. We are trying hard to get docs out there that are helpful as much as we try to continuously improve communication, please help me! :)
</comment><comment author="kix" created="2016-03-17T08:56:08Z" id="197770232">@s1monw, done :)
</comment><comment author="s1monw" created="2016-03-17T09:00:20Z" id="197772866">@kix thanks man!
</comment><comment author="kix" created="2016-03-17T09:07:03Z" id="197776401">@s1monw, sorry for being a bit harsh, but sometimes when you're looking for a solution, you certainly hope that the doc will help.
</comment><comment author="s1monw" created="2016-03-17T10:05:04Z" id="197799991">@kix not a big deal description looks much better and I can tell you there is suddenly more motivation to fix that :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow create index with alias instead of index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17137</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
It is a feature request.
Creating index by alias instead of index name is really useful sometimes.
Consider this case, I have a service writing log data into ES and create a new index per month. If I use dynamic template to create index, the index name will be exactly same as the one I used to write data. Therefore, I have to use index name to write data other than an alias. It's conflict with official recommendation and i can't perform a reindex if need.
This problem will be solved if there exists an api like:
-PUT /_template/example
{
    "template": "test-*-write",
    "template_pattern": "{index}-write"
    "order": 1,
    "aliases": {
      "{index}-write": {},
      "{index}-read"; {},
    }
}
</description><key id="141239717">17137</key><summary>Allow create index with alias instead of index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xmudall</reporter><labels /><created>2016-03-16T11:09:21Z</created><updated>2016-03-21T08:13:01Z</updated><resolved>2016-03-16T12:04:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-16T12:04:37Z" id="197286306">Hi @xmudall 

&gt;  It's conflict with official recommendation and i can't perform a reindex if need.

The mostly-append logging use case is different from the single index read/write use case, where it makes sense to write to an alias.  With logging, you typically generate the index name from the timestamp of the log message, so yes, you do index directly into the index for that month, rather than into an alias.

Aliases with logging make sense for read requests, where you want to query the latest index, or the most recent three months etc. 

That said, we have an idea for a more flexible append-only work flow.  We'll be opening an issue to discuss this feature in the near future.
</comment><comment author="xmudall" created="2016-03-17T03:13:30Z" id="197672824">Hi @clintongormley 
I agree with your opinion on logging. I just used logging case as an example for easy understanding. Actually, I have a mail server provides full text searching on mails for users. I split mails by time slot (e.g. per month) to reduce resource usage especially memory (little fielddata, cache, etc.) . I use dynamic template to create index for every time slot. I hope all write/read operation is based on alias so that i can do a zero-downtime reindex if needed. That's why i have this problem.
Could you please give me any suggestions how to achieve the requirement without an alias based template? Thanks a lot.
</comment><comment author="xmudall" created="2016-03-21T08:13:01Z" id="199167186">Hi @clintongormley 
Are you there?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add default settings filters for password, key, access_key etc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17136</link><project id="" key="" /><description>This commit adds some default settings filters for settings like  `*.password` or `secret_key`. It's a simple safeguard in case a setting is registered but not marked as filtered.
</description><key id="141237466">17136</key><summary>Add default settings filters for password, key, access_key etc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Settings</label><label>enhancement</label></labels><created>2016-03-16T10:58:03Z</created><updated>2016-03-17T10:10:05Z</updated><resolved>2016-03-16T13:27:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-16T11:10:01Z" id="197264577">I don't think we should do this. We have clear APIs to hide stuff from settings APIs we don't need any wildcards that hide by default.
</comment><comment author="tlrx" created="2016-03-16T13:27:34Z" id="197328700">Then let's close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`constant_score` query should throw error on more than one filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17135</link><project id="" key="" /><description>When specifying more than one `filter` in a `constant_score` query, the last one will be the only one that will be executed, overwriting previous filters. It should rather raise a ParseException to notify the user that only one filter query is accepted.

Closes #17126
</description><key id="141228674">17135</key><summary>`constant_score` query should throw error on more than one filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T10:18:34Z</created><updated>2016-03-16T10:25:12Z</updated><resolved>2016-03-16T10:25:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-16T10:22:28Z" id="197249641">LGTM
</comment><comment author="cbuescher" created="2016-03-16T10:25:12Z" id="197250427">@s1monw thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable RequestCache by default if shard is inactive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17134</link><project id="" key="" /><description>To fully leverage #16870 we should enable the request cache by default. This has show to improve aggregations and search results dramatically on static indices. A good indicator if a shard / index is static is if it has received writes within a certain time interval which we also used to adjust indexing buffers in the past or use for issuing synced flushes. We should check`IndexShard#isActive()` and if it's not active we should allow request caching by default.

Relates to #16870 
</description><key id="141227098">17134</key><summary>Enable RequestCache by default if shard is inactive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Cache</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T10:12:46Z</created><updated>2016-03-17T09:11:22Z</updated><resolved>2016-03-17T09:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-16T10:44:32Z" id="197256658">+1 :)

&gt; On 16 Mar 2016, at 11:13, Simon Willnauer notifications@github.com wrote:
&gt; 
&gt; To fully leverage #16870 we should enable the request cache by default. This has show to improve aggregations and search results dramatically on static indices. A good indicator if a shard / index is static is if it has received writes within a certain time interval which we also used to adjust indexing buffers in the past or use for issuing synced flushes. We should checkIndexShard#isActive() and if it's not active we should allow request caching by default.
&gt; 
&gt; Relates to #16870
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly or view it on GitHub
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit request size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17133</link><project id="" key="" /><description>With this PR we add the possibility to limit the size of all in-flight requests on transport and HTTP level. The limit is guarded by a new in-flight request circuit breaker. It can be configured by `network.breaker.inflight_requests.limit` which is set to 100% of the available JVM heap per default. In practice, this means that this circuit breaker will only break if the parent circuit breaker breaks (this one is not new - but for completeness - the parent circuit breaker is configured by `indices.breaker.total.limit` with a default value of 70% of totally available JVM heap).

Closes #16011
</description><key id="141224687">17133</key><summary>Limit request size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Circuit Breakers</label><label>:Network</label><label>:REST</label><label>enhancement</label><label>resiliency</label><label>v2.4.0</label><label>v5.0.0-alpha2</label></labels><created>2016-03-16T10:04:21Z</created><updated>2016-06-21T10:33:50Z</updated><resolved>2016-04-13T08:43:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-16T10:04:32Z" id="197240408">For the review I am specifically interested in three things:
1. Should configuration properties a standard naming scheme? Currently I have adhered to the naming scheme in the respective implementation classes where the properties are defined. Note that for the circuit breaker based properties we also have the properties `transport.breaker.request.overhead` and `http.breaker.request.overhead`.
2. Are the default values ok? I have basically derived the values based on the fact that `http.max_content_length` is already 100 MB by default and it would not make sense to put a lower default limit on all in-flight requests.
3. Should we adjust the default value `indices.breaker.total.limit` for the parent circuit breaker given that we have now introduced two new child breakers?
</comment><comment author="danielmitterdorfer" created="2016-03-16T10:10:49Z" id="197244438">@dakrone: Do you have time to review this PR?
</comment><comment author="Dieken" created="2016-03-16T15:05:39Z" id="197373019">Maybe name the in-flight requests related settings to ".....requests....."  or even "......inflight_requests....".
</comment><comment author="danielmitterdorfer" created="2016-03-17T07:52:49Z" id="197744603">@Dieken: Thanks for your comments. I am currently working on another issue but I am having a closer look ASAP.
</comment><comment author="danielmitterdorfer" created="2016-03-17T13:37:44Z" id="197882474">Regarding the HTTP status code 413: Here is the relevant part from [RFC 2616, section 10](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)

&gt; 10.4.14 413 Request Entity Too Large
&gt; 
&gt; The server is refusing to process a request because the request entity is larger than  the server is willing or able to process. The server MAY close the connection to prevent the client from continuing the request.
&gt; 
&gt; If the condition is temporary, the server SHOULD include a Retry- After header field to indicate that it is temporary and after what time the client MAY try again.

So we can add an `Retry-After` header (see also [RFC 2616, section 14](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html)) with a relative delay of - say - 10 seconds to indicate that this condition may be temporary. Note that the condition is only temporary in case of multiple in-flight requests. If a single request hits the limit, it is still permanent (until somebody changes the setting of course). It may be even possible to detect these two cases but I am not sure it is worth the effort and additional code complexity.
</comment><comment author="dakrone" created="2016-03-17T21:33:20Z" id="198090330">@danielmitterdorfer left some comments on this! one question I do have is how much impact this will have on performance of HTTP and transport requests, do you have any idea how large the impact will be?
</comment><comment author="danielmitterdorfer" created="2016-03-18T07:30:55Z" id="198239730">Regarding your question on the performance impact: I'll run some benchmarks and post the results.
</comment><comment author="danielmitterdorfer" created="2016-03-21T12:10:03Z" id="199244897">I've microbenchmarked 3 scenarios with `RequestSizeLimitHandler`:
1. Handle two upstream and downstream events
2. Handle a upstream event, a pipelined upstream event, a pipelined downstream event, a downstream event
3. Handle a pipeline upstream event, a pipelined downstream event, a pipelined upstream event, a pipelined downstream event

(I did this so we have the same number of operations in all cases)

I benchmarked also the current version of `RequestSizeLimitHandler` against the original one (without pipeline support).

A rough summary (exact number depends on the scenario):

| Metric | `RequestSizeLimitHandler` without pipeline support | `RequestSizeLimitHandler` (with pipeline support) |
| --- | --- | --- |
| Allocation rate | 56 bytes / op | 79 - 104 bytes / op (scenario dependent) |
| Average Time | 82 - 113 ns / op | 94 - 119 ns / op |

Details can be found in the gist: https://gist.github.com/danielmitterdorfer/871cd62d7e16e45ae79d

It is way harder to microbenchmark the overhead of request size limiting on transport level as the new code is not that isolated as on HTTP level. However, I've run macrobenchmarks before and after the change to check for performance-related changes.

The baseline for the test was the last commit on master that I've merged (d17fd33). I've ran two trials with geonames data against a single node Elasticsearch cluster with default settings. The benchmark first bulk-indexes data via HTTP, does a force-merge, runs 20 warmup iterations of all queries and then 10 timed iterations of these queries.

| Metric | baseline (without size limiting) | size limiting enabled |
| --- | --- | --- |
| Mean Indexing Throughput [docs/s] | 13930, 12271 | 12775, 13331 |
| 99% latency match_all query [ms] | 9.79, 9.93 | 10.00, 6.29 |
| 99% latency uncached aggregation query [ms] | 115.75, 134.14 | 139.19, 119.25 |

I also ran the same benchmark against a two node cluster with default settings to also exercise the transport code:

| Metric | baseline (without size limiting) | size limiting enabled |
| --- | --- | --- |
| Mean Indexing Throughput [docs/s] | 11119, 11711 | 11725, 11715 |
| 99% latency match_all query [ms] | 9.93, 6.29 | 8.31, 8.21 |
| 99% latency uncached aggregation query [ms] | 134.14, 116.26 | 118.19, 115.67 |

So in both cases the changes hide in the run-to-run variance of the benchmarks.
</comment><comment author="danielmitterdorfer" created="2016-03-21T15:43:37Z" id="199346647">I had a chat with @bleskes about `RequestSizeLimitHandler` and I am currently looking whether we can implement this on a bit higher level (REST layer).
</comment><comment author="danielmitterdorfer" created="2016-03-22T11:29:15Z" id="199770560">I've now implemented in-flight request limiting in `RestController#dispatchRequest()` by just using a try-with-resources block. Request-response correlation is not needed anymore with this approach. Hence we only need to hold state for a single reservation (in the try-with-resources block). I've also documented the need for the HTTP in-flight request limit.
</comment><comment author="danielmitterdorfer" created="2016-03-22T14:27:29Z" id="199838779">@bleskes Your comment should now be addressed. I decided to keep `NettyHttpChannel` free of any limiter / circuit breaker logic and just use it to free the reservation when the response has been sent which makes the class simpler.
</comment><comment author="bleskes" created="2016-04-04T09:37:13Z" id="205213475">Thank you @danielmitterdorfer very much for iterating on this. I went through this and I'm concerned about the added abstractions and the amount of moving parts. Here are a couple of suggestions to simplify:
1) Remove the Limiter &amp; Releasable notions. Instead interact directly with the circuit breaker and make the channel release the bytes they need to when people respond through them. We can also make the channel be closable and only allow them to be "closed" once in order to protect against double freeing.
2) We have two in-flight requests circuit breakers, one for HTTP and once for transports. I think we can have one that holds for all requests, regardless of where they came from. (we should have a separate discussion about what the default size should be)
3) Given that fact the transport client is being replaced with a java only client, we can remove the max outgoing request size limit and simplify the code a bit more.
4) The transport service interface now has an extra limitStats method that is only used in testing. I think we can remove it and access the circuitbreaker service directly to check all was freed.

Does this all make sense?
</comment><comment author="danielmitterdorfer" created="2016-04-04T09:44:13Z" id="205215793">Thanks for your feedback. I'll incorporate the changes soon.
</comment><comment author="danielmitterdorfer" created="2016-04-04T14:21:21Z" id="205318003">@bleskes I've addressed your comments now. Can you have another look please?
</comment><comment author="bleskes" created="2016-04-05T10:53:41Z" id="205754845">Left a bunch of comments, but I think this looks great. Maybe I missed it , but do we have a test for the limit enforcing on the http layer?
</comment><comment author="danielmitterdorfer" created="2016-04-06T13:16:05Z" id="206365703">I just had unit tests on HTTP layer. I've also added an integration test. I've also adjusted the behavior on HTTP level. Instead of closing the connection silently, we return the associated status of `CircuitBreakingException` (which is now 500).
</comment><comment author="danielmitterdorfer" created="2016-04-08T11:34:55Z" id="207394005">@bleskes I add another round of commits which address all of your outstanding comments. Would be great if you could have another look.
</comment><comment author="bleskes" created="2016-04-08T14:19:18Z" id="207449465">@danielmitterdorfer I looked at the new implementation. Feels better. I added some cleanup comments... 
</comment><comment author="danielmitterdorfer" created="2016-04-11T07:30:43Z" id="208201744">@bleskes Thanks again. I have aligned the close logic and moved HTTP request limiting to HttpServer. I answered all other comments but did not change the code yet.
</comment><comment author="danielmitterdorfer" created="2016-04-11T09:27:35Z" id="208249201">@bleskes All your review comments should be addressed with the last round of commits now.
</comment><comment author="bleskes" created="2016-04-11T09:53:54Z" id="208262804">Code LGTM! Thx @danielmitterdorfer . We still have the open question of what should be a good default value for this breaker. I feel we have two options:

1) Set it to some percentage of total node memory - the complicated thing here is that different nodes have different requirements. For example, coordinating only nodes can have a lot of inflight requests (this is their job) but data nodes may have different requirements. 
2) Don't set a max and allow the parent circuit breaker to trigger where the sum of request + whatever the other breakers (field data etc.) need reaches above an accepted limit. 

@dakrone @clintongormley do you have any opinions here?
</comment><comment author="danielmitterdorfer" created="2016-04-11T12:06:22Z" id="208310548">@bleskes Thanks for your review. I have now introduced constants for all Netty headers, added Javadocs and additional helper methods in `Marker`. It should now be quite obvious what is read w.r.t. to headers. I felt that it may also make sense to add additional methods in `NettyHeaders` that help with low-level header handling (like reading the marker bytes) but I did not do this in this PR. However, this might be worth doing in another small PR (later).
</comment><comment author="bleskes" created="2016-04-11T12:54:44Z" id="208326342">sounds good @danielmitterdorfer 
</comment><comment author="dakrone" created="2016-04-11T13:23:32Z" id="208339643">&gt; We still have the open question of what should be a good default value for this breaker. I feel we have two options:
&gt; 1) Set it to some percentage of total node memory - the complicated thing here is that different nodes have different requirements. For example, coordinating only nodes can have a lot of inflight requests (this is their job) but data nodes may have different requirements.
&gt; 2) Don't set a max and allow the parent circuit breaker to trigger where the sum of request + whatever the other breakers (field data etc.) need reaches above an accepted limit.
&gt; 
&gt; @dakrone @clintongormley do you have any opinions here?

Hmm... I think setting it to 200mb sounds reasonable to me? These are nice dynamic settings, so it's easy to change at runtime and I'd rather err too low than too high.
</comment><comment author="dakrone" created="2016-04-11T13:28:09Z" id="208341361">@bleskes brought up that this is all requests, so 200mb is probably too small (and I agree), since the parent breaker limit is 70%, how about setting this to 60%?
</comment><comment author="danielmitterdorfer" created="2016-04-12T11:48:10Z" id="208861446">@dakrone: The suggestion with 60% is fine for me. If I hear no objections I change it accordingly and update the docs. /cc:@bleskes 
</comment><comment author="danielmitterdorfer" created="2016-04-12T14:48:01Z" id="208943264">We have decided to not limit the respective circuit breaker but rely on the parent circuit breaker (which is currently set to 70%). I'll update code and docs accordingly and merge the PR afterwards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>term suggest suggest_mode missing broken?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17132</link><project id="" key="" /><description>This was found during training using the core developer training dataset:

Getting all the term suggestions for framework

``` json
GET stack/_suggest
{
  "mysuggest" : {
    "text" : "framework",
    "term" : {
      "field" : "title",
      "suggest_mode" : "always",
      "sort" : "score",
      "size" : 20
    }
  }
}
```

yields:

``` json
[{
    "text": "frameworks",
    "score": 0.8888889,
    "freq": 16
}, {
    "text": "framework2",
    "score": 0.8888889,
    "freq": 1
}, {
    "text": "framwork",
    "score": 0.875,
    "freq": 5
}, {
    "text": "framewok",
    "score": 0.875,
    "freq": 2
}, {
    "text": "famework",
    "score": 0.875,
    "freq": 1
}, {
    "text": "framework's",
    "score": 0.7777778,
    "freq": 1
}, {
    "text": "freamwork",
    "score": 0.7777778,
    "freq": 1
}]
```

if we then pick any of the of these terms, except the most popular `frameworks` and feed it to the term suggester using the missing option:

``` json
GET stack/_suggest
{
  "mysuggest" : {
    "text" : "framwork",
    "term" : {
      "field" : "title",
      "suggest_mode" : "missing",
      "sort" : "score",
      "size" : 20
    }
  }
}
```

We still get back suggestions:

``` json
[{
    "text": "framework",
    "score": 0.875,
    "freq": 197
}, {
    "text": "frameworks",
    "score": 0.75,
    "freq": 9
}, {
    "text": "framewok",
    "score": 0.75,
    "freq": 1
}, {
    "text": "framework2",
    "score": 0.75,
    "freq": 1
}]
```

Only for the most `popular`  term do we get back no suggestions.

For full reference here is how `framwork` gets analyzed:

`GET stack/_analyze?field=title&amp;text=framwork`

``` json
{
  "tokens": [
    {
      "token": "framwork",
      "start_offset": 0,
      "end_offset": 8,
      "type": "&lt;ALPHANUM&gt;",
      "position": 0
    }
  ]
}
```
</description><key id="141222146">17132</key><summary>term suggest suggest_mode missing broken?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2016-03-16T09:53:33Z</created><updated>2016-03-16T10:33:37Z</updated><resolved>2016-03-16T10:33:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-16T10:33:37Z" id="197253137">This is an artefact of using multiple shards.  Try the following example with 1 shard vs 5 shards:

```
PUT t
{
  "settings": {
    "number_of_shards": 1
  }
}

POST t/t/_bulk
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "frameworks"}
{"index": {}}
{  "text": "framework2"}
{"index": {}}
{  "text": "framwork"}
{"index": {}}
{  "text": "framwork"}
{"index": {}}
{  "text": "framwork"}
{"index": {}}
{  "text": "framwork"}
{"index": {}}
{  "text": "framwork"}
{"index": {}}
{  "text": "framwok"}
{"index": {}}
{  "text": "famwork"}
{"index": {}}
{  "text": "framwork's"}
{"index": {}}
{  "text": "freamwork"}

GET t/_suggest
{
  "mysuggest" : {
    "text" : "framwork",
    "term" : {
      "field" : "text",
      "suggest_mode" : "missing",
      "sort" : "score",
      "size" : 20
    }
  }
}
```

Closing as duplicate of https://github.com/elastic/elasticsearch/issues/7472
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot run exists query on field with thousands of nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17131</link><project id="" key="" /><description>**Describe the feature**: I've got some documents with field A and inside A are thousands of nested fields. I would like to run exists query to get documents with field A presents

``` json
{
  "query": {
    "bool": {
      "must": {
        "exists": {
          "field": "A"
        }
      }
    }
  }
}
```

but then I've got an error

``` json
"type": "too_many_clauses",
"reason": "maxClauseCount is set to 1024"
```

In my case there is one nested field exists in every A, so I can query for A.B, but what can I do if there is no common fields in A and I want to see all docs with A? 
</description><key id="141216225">17131</key><summary>Cannot run exists query on field with thousands of nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">merito</reporter><labels /><created>2016-03-16T09:27:04Z</created><updated>2016-03-22T15:38:53Z</updated><resolved>2016-03-22T15:38:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-16T10:15:23Z" id="197247517">Do you mean `nested` fields or inner `object` fields?  Either way, having thousands of fields is costly, especially if those fields are sparse.  We are currently working on adding various [soft limits](https://github.com/elastic/elasticsearch/issues/11511) to things like the [number of fields](https://github.com/elastic/elasticsearch/issues/11443) you can have in an index, plus the [number of nested fields](https://github.com/elastic/elasticsearch/pull/15989) you can have in an index.

Usually your data model can be reworked to use fewer fields by specifying whatever you are using for a field name today as a value instead, eg:

```
{ "foo": "bar"}
```

could be better represented as:

```
{ "key": "foo", "value": bar" }
```

An alternative, with your current model, is to use a single field within `A` to identify documents that have an `A.*` field, eg:

```
{ "A": {
    "some_val": "foo",
    "exists": true
}}
```

Then you could just query on `A.exists`.  This will be much more efficient than querying for thousands of potential terms.
</comment><comment author="merito" created="2016-03-16T10:31:56Z" id="197252258">Yes, I mean inner objects. But still even if there is not thousands of fields I think it will be useful to have a feature to check if only the outer object exists without checking all inner objects, like in my case.
</comment><comment author="clintongormley" created="2016-03-18T11:34:03Z" id="198317071">@merito see https://github.com/elastic/elasticsearch/pull/17186
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BasicBackwardsCompatibilityIT.testIndexRollingUpgrade sometimes fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17130</link><project id="" key="" /><description>Example failure:
http://build-us-00.elastic.co/job/es_core_2x_window-2008/712

Reproduce command (I can't reproduce this locally):

```
REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.qa.backwards:2.0 -Dtests.seed=FE8754AE850E4A36 -Dtests.class=org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT -Dtests.method="testIndexRollingUpgrade" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Des.node.mode=local -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -Djava.net.preferIPv4Stack=true" -Dtests.locale=sr-BA -Dtests.timezone=Australia/Tasmania
```

Failure:

```
FAILURE 11.8s | BasicBackwardsCompatibilityIT.testIndexRollingUpgrade &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: Count is 96 but 116 was expected.  Total shards: 14 Successful shards: 14 &amp; 0 shard failures:
   &gt;    at __randomizedtesting.SeedInfo.seed([FE8754AE850E4A36:88FA0EA86CDC10E0]:0)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:244)
   &gt;    at org.elasticsearch.bwcompat.BasicBackwardsCompatibilityIT.testIndexRollingUpgrade(BasicBackwardsCompatibilityIT.java:371)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="141209587">17130</key><summary>BasicBackwardsCompatibilityIT.testIndexRollingUpgrade sometimes fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">colings86</reporter><labels><label>jenkins</label></labels><created>2016-03-16T08:52:29Z</created><updated>2016-04-01T16:28:56Z</updated><resolved>2016-04-01T16:28:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-03-16T11:53:32Z" id="197281338">I can reproduce it. Will try to figure it out.
</comment><comment author="brwe" created="2016-03-16T13:55:41Z" id="197342545">This fails because of https://github.com/elastic/elasticsearch/pull/15832.  That fix was backported to 2.0 branch but we never made a release with this fix. So now we are still testing against 2.0.2 which does not have the fix. I do not remember why we did not think of that or why it starts failing only now and did not before. Maybe something in CI changed? Anyway, I will change the test to flush before upgrading if version is &lt; 2.0.3 to make sure we don't run into it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backup/Restore threding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17129</link><project id="" key="" /><description>Hi,

Currently snapshot, restore and snapshot delete are limited to one thread only.

This means I cannot restore while a snapshot /snapshot delete is in progress.

In large environments this may be a few hours a day.

My suggestions is to create a separate thread for restores that will not be blocked by running snapshot or deletes

10x 
</description><key id="141196830">17129</key><summary>Backup/Restore threding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">idofperion</reporter><labels /><created>2016-03-16T07:33:36Z</created><updated>2016-03-16T10:05:06Z</updated><resolved>2016-03-16T10:05:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-16T10:05:06Z" id="197240735">Duplicate of https://github.com/elastic/elasticsearch/issues/11148
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Switch to maven-publish plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17128</link><project id="" key="" /><description>The build currently uses the old maven support in gradle. This commit
switches to use the newer maven-publish plugin. This will allow future
changes, for example, easily publishing to artifactory.

An additional part of this change makes publishing of build-tools part
of the normal publishing, instead of requiring a separate upload step
from within buildSrc. That also sets us up for a follow up to enable
precomit checks on the buildSrc code itself.
</description><key id="141153261">17128</key><summary>Build: Switch to maven-publish plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T02:22:07Z</created><updated>2016-03-17T18:53:50Z</updated><resolved>2016-03-17T18:53:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-16T12:19:26Z" id="197292388">Left two questions but they shouldn't block merging. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove dead code in FTL#simpleMatchToFullName</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17127</link><project id="" key="" /><description>This commit removes some dead code that resulted from removing the
ability for a field to have different names (after enforcing that fields
have the same full and index name).

Relates #15488
</description><key id="141149517">17127</key><summary>Remove dead code in FTL#simpleMatchToFullName</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-16T01:58:06Z</created><updated>2016-03-16T02:12:21Z</updated><resolved>2016-03-16T02:12:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-16T02:02:11Z" id="197112560">LGTM
</comment><comment author="jasontedor" created="2016-03-16T02:08:58Z" id="197113857">Thanks @rjernst  for the quick review. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>constant_score should throw a parse exception if more than one filter is supplied</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17126</link><project id="" key="" /><description>The `constant_score` query will accept multiple clauses in its `filter` clause, but it appears to only run the last query.  I believe the intention is for it to only accept a single clause, so multiple query objects should raise a parse exception.

E.g.

``` js
POST /test/test/
{
    "foo": "a"
}

POST /test/test/
{
    "foo": "b"
}

POST /test/test/
{
    "foo": "a b"
}

GET /test/_search
{
   "query": {
      "constant_score": {
         "filter": [
            { "term": { "foo": "x" } },
            { "term": { "foo": "a" } }
         ]
      }
   }
}
```

Which yields both documents which have `"a"`:

``` json
{
   "hits": {
      "total": 2,
      "max_score": 1,
      "hits": [
         {
            "_index": "test",
            "_type": "test",
            "_id": "AVN8hy3XgPMxNnYnA0TX",
            "_score": 1,
            "_source": {
               "foo": "a"
            }
         },
         {
            "_index": "test",
            "_type": "test",
            "_id": "AVN8h2TygPMxNnYnA0Tr",
            "_score": 1,
            "_source": {
               "foo": "a b"
            }
         }
      ]
   }
}
```

Swapping the order returns zero documents, because nothing matches `"x"`:

``` js
GET /test/_search
{
   "query": {
      "constant_score": {
         "filter": [
            { "term": { "foo": "a" } },
            { "term": { "foo": "x" } }
         ]
      }
   }
}
```

``` json
{
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```

This is especially confusing because the `bool` filter clause _does_ support multiple clauses, and by default AND's them together.
</description><key id="141125307">17126</key><summary>constant_score should throw a parse exception if more than one filter is supplied</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2016-03-15T23:14:14Z</created><updated>2016-03-16T13:05:32Z</updated><resolved>2016-03-16T10:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-16T09:58:31Z" id="197236807">I agree, unfortunately there are many places in the query parsers where specifying the same parameter twice won't throw an error but silently drops the first one. I'll add a check for this. btw, the array notation only works because those tokens get silently ignored as well.
</comment><comment author="polyfractal" created="2016-03-16T13:05:32Z" id="197316038">Thanks @cbuescher! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Properly register reindex status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17125</link><project id="" key="" /><description>Without this commit fetching the status of a reindex from a node that isn't
coordinating the reindex will fail. This commit properly registers reindex's
status so this doesn't happen. To do so it moves all task status registration
into NetworkModule and creates a method to register other statuses which the
reindex plugin calls.
</description><key id="141115274">17125</key><summary>Properly register reindex status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T22:18:06Z</created><updated>2016-03-16T13:54:59Z</updated><resolved>2016-03-16T12:21:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-15T22:18:17Z" id="197049934">@imotov this one is for you I think.
</comment><comment author="s1monw" created="2016-03-16T11:13:19Z" id="197266155">LGTM
</comment><comment author="nik9000" created="2016-03-16T13:54:59Z" id="197342356">Backported to 2.x with 35442b145cff9eb96f1c9598445bf2fc413a2a31
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transient and/or persistent threadpool index queue size configuration is not taken into account when updating them using the cluster update API.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17124</link><project id="" key="" /><description>**Elasticsearch version**: 1.7.5

**JVM version**: 1.8.0_66

**OS version**: CentOS Linux release 7.1.1503
Linux localhost 3.10.0-229.20.1.el7.x86_64

**Description of the problem including expected versus actual behavior**:

I've the following error when trying to delete a huge amount of documents :

```
....
"myIndexName": {
        "_shards": {
            "total": 1,
            "successful": 0,
            "failed": 1,
            "failures": [
                {
                    "index": "myIndexName",
                    "shard": 0,
                    "reason": "EsRejectedExecutionException[rejected execution (queue capacity 100) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1@4d215e4]"
                }
            ]
        }
    }
....
```

I know what this error means so I tried to execute the following command to resolve the issue :

```
curl -XPUT localhost:9200/_cluster/settings -d '{"transient" : {"threadpool.index.queue_size":500}, "persistent": {"threadpool.index.queue_size":500} }'
```

and I got the following response :

```
{"acknowledged":true,"persistent":{"threadpool":{"index":{"queue_size":"500"}}},"transient":{"threadpool":{"index":{"queue_size":"500"}}}}
```

But unfortunately when trying to delete again the documents, I still get the exact same error.

Then I manually updated elasticsearch.yml to add the following line :

```
...
threadpool.index.queue_size: 500
...
```

I restarted elasticsearch and the delete query was working fine.

**Steps to reproduce**:
1. manage to get a "EsRejectedExecutionException" error when indexing or deleting documents (you may do it by voluntarily set a very small threadpool.index.queue_size value in the configuration file)
2. Update this threadpool.index.queue_size using the cluster update api :

```
curl -XPUT localhost:9200/_cluster/settings -d '{"transient" : {"threadpool.index.queue_size":500}, "persistent": {"threadpool.index.queue_size":500} }'
```
1. try to index or delete the documents again

**Provide logs (if relevant)**:
No error in logs only this logs appear when executing the cluster update query :

```
[2016-03-15 15:09:52,234][DEBUG][cluster.service          ] [Living Monolith] processing [cluster_update_settings]: execute
[2016-03-15 15:09:52,240][DEBUG][cluster.service          ] [Living Monolith] cluster state updated, version [343], source [cluster_update_settings]
[2016-03-15 15:09:52,240][DEBUG][cluster.service          ] [Living Monolith] publishing cluster state version 343
[2016-03-15 15:09:52,240][DEBUG][cluster.service          ] [Living Monolith] set local cluster state to version 343
[2016-03-15 15:09:52,250][DEBUG][river.cluster            ] [Living Monolith] processing [reroute_rivers_node_changed]: execute
[2016-03-15 15:09:52,250][DEBUG][river.cluster            ] [Living Monolith] processing [reroute_rivers_node_changed]: no change in cluster_state
[2016-03-15 15:09:52,253][DEBUG][cluster.service          ] [Living Monolith] processing [cluster_update_settings]: took 19ms done applying updated cluster_state (version: 343)
[2016-03-15 15:09:52,254][DEBUG][cluster.service          ] [Living Monolith] processing [reroute_after_cluster_update_settings]: execute
[2016-03-15 15:09:52,256][DEBUG][cluster.service          ] [Living Monolith] processing [reroute_after_cluster_update_settings]: took 2ms no change in cluster_state
```
</description><key id="141114843">17124</key><summary>Transient and/or persistent threadpool index queue size configuration is not taken into account when updating them using the cluster update API.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">petitout</reporter><labels><label>feedback_needed</label></labels><created>2016-03-15T22:15:23Z</created><updated>2016-03-16T12:58:08Z</updated><resolved>2016-03-15T23:54:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-15T23:54:03Z" id="197072222">There are unit tests in place that verify that modifying these settings works as expected, and I am unable to reproduce. Against a fresh 1.7.5 node:

``` bash
19:30:20 [jason:~] $ curl -XGET localhost:9200/
{
  "status" : 200,
  "name" : "Crimson Dynamo V",
  "cluster_name" : "elasticsearch_jason",
  "version" : {
    "number" : "1.7.5",
    "build_hash" : "00f95f4ffca6de89d68b7ccaf80d148f1f70e4d4",
    "build_timestamp" : "2016-02-02T09:55:30Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
19:30:33 [jason:~] $ curl -XGET localhost:9200/_cat/thread_pool?h=index.queueSize
200
19:30:37 [jason:~] $ curl -XPUT localhost:9200/_cluster/settings?pretty=1 -d '{"transient" : {"threadpool.index.queue_size":500}, "persistent": {"threadpool.index.queue_size":500} }'
{
  "acknowledged" : true,
  "persistent" : {
    "threadpool" : {
      "index" : {
        "queue_size" : "500"
      }
    }
  },
  "transient" : {
    "threadpool" : {
      "index" : {
        "queue_size" : "500"
      }
    }
  }
}
19:30:40 [jason:~] $ curl -XGET localhost:9200/_cat/thread_pool?h=index.queueSize
500
```

Plus I see the logging statement:

```
[2016-03-15 19:30:37,222][DEBUG][threadpool               ] [Husk] creating thread_pool [index], type [fixed], size [8], queue_size [500]
```

which only happens if the thread pool is actually recreated to reflect the new queue size.

Do you possibly have other settings set here such as `threadpool.index.capacity` or `threadpool.index.queue`? It's crazy that we support all of these, but either of those would override `thread pool.index.queue_size`.

Starting from a fresh 1.7.5 node again:

``` bash
19:50:48 [jason:~] 7 $ curl -XGET localhost:9200/_cat/thread_pool?h=index.queueSize
200
19:51:08 [jason:~] $ curl -XPUT localhost:9200/_cluster/settings?pretty=1 -d '{"transient" : {"threadpool.index.capacity":500}, "persistent": {"threadpool.index.capacity":500} }'
{
  "acknowledged" : true,
  "persistent" : {
    "threadpool" : {
      "index" : {
        "capacity" : "500"
      }
    }
  },
  "transient" : {
    "threadpool" : {
      "index" : {
        "capacity" : "500"
      }
    }
  }
}
19:51:12 [jason:~] $ curl -XGET localhost:9200/_cat/thread_pool?h=index.queueSize
500
19:51:15 [jason:~] $ curl -XPUT localhost:9200/_cluster/settings?pretty=1 -d '{"transient" : {"threadpool.index.queue_size":200}, "persistent": {"threadpool.index.queue_size":200} }'
{
  "acknowledged" : true,
  "persistent" : {
    "threadpool" : {
      "index" : {
        "queue_size" : "200"
      }
    }
  },
  "transient" : {
    "threadpool" : {
      "index" : {
        "queue_size" : "200"
      }
    }
  }
}
19:51:37 [jason:~] $ curl -XGET localhost:9200/_cat/thread_pool?h=index.queueSize
500
```

showing that the `queue_size` setting has no effect after `capacity` is set. I note further that my logs match yours in this case:

```
[2016-03-15 19:51:37,497][DEBUG][cluster.service          ] [Aries] processing [cluster_update_settings]: execute
[2016-03-15 19:51:37,497][DEBUG][cluster.service          ] [Aries] cluster state updated, version [4], source [cluster_update_settings]
[2016-03-15 19:51:37,497][DEBUG][cluster.service          ] [Aries] publishing cluster state version 4
[2016-03-15 19:51:37,497][DEBUG][cluster.service          ] [Aries] set local cluster state to version 4
[2016-03-15 19:51:37,498][DEBUG][river.cluster            ] [Aries] processing [reroute_rivers_node_changed]: execute
[2016-03-15 19:51:37,498][DEBUG][river.cluster            ] [Aries] processing [reroute_rivers_node_changed]: no change in cluster_state
[2016-03-15 19:51:37,499][DEBUG][cluster.service          ] [Aries] processing [cluster_update_settings]: took 2ms done applying updated cluster_state (version: 4)
[2016-03-15 19:51:37,499][DEBUG][cluster.service          ] [Aries] processing [reroute_after_cluster_update_settings]: execute
[2016-03-15 19:51:37,499][DEBUG][cluster.service          ] [Aries] processing [reroute_after_cluster_update_settings]: took 0s no change in cluster_state
```

I will point to examples like this every time that there is an attempt at arguing that we should support more than one way to do something.
</comment><comment author="petitout" created="2016-03-16T01:12:35Z" id="197093892">With all due respect, you didn't prove that there was no issue.

I also have the same behavior than you : elasticsearch seems to acknowledge the setup and is returning the updated configuration using the GET query (curl -XPUT localhost:9200/_cluster/settings?pretty=1)

but that doesn't mean that behind the hood it actually takes the configuration into account.

please retest with the steps I've provided (it fails EVERY time) :

1 - recreate the context to get the "EsRejectedExecutionException" with a voluntarily too small value for the queue_size
2- update the queue_size with the cluster API
3- try again the request that previously got the "EsRejectedExecutionException"

If this is the way you manage issues, next time I will not waste my time opening one. 
</comment><comment author="jasontedor" created="2016-03-16T01:36:20Z" id="197102722">&gt; With all due respect, you didn't prove that there was no issue.

@petitout With the information that you provided, I am not able to reproduce the issue that you report, and after spending an hour studying the relevant code, I provided a plausible explanation for the behavior that you're seeing, including most importantly the fact that the cluster state is updated but without logging that the thread pool is recreated. I also added the [feedback_needed](https://github.com/elastic/elasticsearch/labels/feedback_needed) label to indicate that there's a possibility that new information could alter the assessment provided here so far.

&gt; but that doesn't mean that behind the hood it actually takes the configuration into account.

The [_only_](https://github.com/elastic/elasticsearch/blob/v1.7.5/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L379) way that the new queue size is updated in the the APIs (`_cat/thread_pool` and `_cluster/settings`) is if the thread pool is actually recreated with a new work queue. In the relevant code here, for a fixed thread pool, either a new executor holder is [created](https://github.com/elastic/elasticsearch/blob/v1.7.5/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L365) with updated info but _without_ changing the queue size, or a new executor holder is [created](https://github.com/elastic/elasticsearch/blob/v1.7.5/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L380) with updated info, a new queue size, _and_ a new executor. Since you're saying that the API is outputting a _new_ queue size, it is in fact the case that the queue is correctly updated.

&gt; please retest with the steps I've provided (it fails EVERY time) :

Please provide a complete shell script that reproduces what you claim, against a _fresh_ cluster. Again, I tried and it did not reproduce for me.

&gt; If this is the way you manage issues, next time I will not waste my time opening one.

I am genuinely trying to help you and I'm very sorry that you feel otherwise.
</comment><comment author="petitout" created="2016-03-16T04:06:02Z" id="197142866">@jasontedor, I started with a fresh installation on another virtual machine and I couldn't reproduce the problem.......
I'm truly sorry for the inconvenience
</comment><comment author="jasontedor" created="2016-03-16T12:58:08Z" id="197310325">&gt; I'm truly sorry for the inconvenience

@petitout No worries!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Standardize state format type for global and index level metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17123</link><project id="" key="" /><description>Followup from https://github.com/elastic/elasticsearch/pull/16442#discussion_r55978382

Currently, global and index level state format type can be configured through `gateway.format`.
This commit removes the ability to configure format type for these states. 
Now we always store these states in `SMILE` format and ensure we always write them 
to disk in the most compact way.
</description><key id="141113747">17123</key><summary>Standardize state format type for global and index level metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Store</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T22:09:07Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-16T22:49:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-16T09:46:39Z" id="197230586">@areek please could you add a note to the breaking changes docs.
</comment><comment author="bleskes" created="2016-03-16T13:14:15Z" id="197321685">change looks good to me. left some minor questions on the test
</comment><comment author="areek" created="2016-03-16T17:32:01Z" id="197445191">@clintongormley [updated](https://github.com/elastic/elasticsearch/pull/17123/commits/0c579b8eed2beda348b509f209b8614f978ddac6) breaking changes docs.

@bleskes thanks for the review, I updated the tests to use `Builder.toXContent` directly for serialization and nuked tests that were irrelevant for 5.0
</comment><comment author="bleskes" created="2016-03-16T18:12:32Z" id="197463944">LGTM. Thx @areek 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Be much much much careful about context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17122</link><project id="" key="" /><description>Preserving the context allows plugins that use the context to actually
work. This preserves the context in important spots and adds much better
testing that context is preserved.
</description><key id="141079589">17122</key><summary>Be much much much careful about context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>bug</label><label>review</label><label>v2.3.0</label></labels><created>2016-03-15T19:45:49Z</created><updated>2016-03-16T09:47:29Z</updated><resolved>2016-03-15T22:23:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-15T19:46:02Z" id="196991982">@jaymode this is for you I think. It is only a 2.x thing.
</comment><comment author="jaymode" created="2016-03-15T20:32:54Z" id="197008655">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unofficial support for JAVA_OPTS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17121</link><project id="" key="" /><description>Today, we permit JVM arguments to be passed via both `JAVA_OPTS` and `ES_JAVA_OPTS`. The JVM [officially supports](https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/envvars002.html) `JAVA_TOOL_OPTIONS` for this purpose, but we [officially do not support that](https://github.com/elastic/elasticsearch/pull/13813) because it is broken on certain platforms. The environment variable `JAVA_OPTS` is supported in some projects (most notably Tomcat) but it is not officially supported and does not have widespread support (e.g., `mvn` does not support it). Notably, we [document `ES_JAVA_OPTS`](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html) but do not document `JAVA_OPTS`. There is no need to support both. Since we won't support `JAVA_TOOLS_OPTIONS`, and we do not document `JAVA_OPTS` but document `ES_JAVA_OPTS`, we should just remove support for `JAVA_OPTS` and keep `ES_JAVA_OPTS` only.
</description><key id="141078231">17121</key><summary>Remove unofficial support for JAVA_OPTS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>breaking</label></labels><created>2016-03-15T19:40:24Z</created><updated>2016-04-13T03:07:40Z</updated><resolved>2016-04-13T03:07:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-15T22:35:35Z" id="197054662">Went on another JVM code sleuthing adventure, and found that oddly enough there [is an environment variable `_JAVA_OPTIONS`](http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/tip/src/share/vm/runtime/arguments.cpp#l3265) that is also parsed by the JVM on startup. Moreover, because of the [order](http://hg.openjdk.java.net/jdk8/jdk8/hotspot/file/tip/src/share/vm/runtime/arguments.cpp#l2449) in which the options are parsed, any flags in this environment variable _trump_ both command line flags and options passed in the `JAVA_TOOL_OPTIONS` environment variable. The ordering is `_JAVA_OPTIONS`, then command-line arguments, then `JAVA_TOOL_OPTIONS`. Yet, this environment variable appears to be completely undocumented!
</comment><comment author="jasontedor" created="2016-03-15T22:41:28Z" id="197056014">This demonstrates the above observation regarding the order of precedence:

```
18:39:14 [jason:~] $ _JAVA_OPTIONS="-Xmx4g" JAVA_TOOL_OPTIONS="-Xmx2g" java -Xmx3g -XX:+PrintFlagsFinal -version | grep MaxHeapSize
Picked up JAVA_TOOL_OPTIONS: -Xmx2g
Picked up _JAVA_OPTIONS: -Xmx4g
    uintx MaxHeapSize                              := 4294967296                          {product}
java version "1.8.0_74"
Java(TM) SE Runtime Environment (build 1.8.0_74-b02)
Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)
18:40:08 [jason:~] $ JAVA_TOOL_OPTIONS="-Xmx2g" java -Xmx3g -XX:+PrintFlagsFinal -version | grep MaxHeapSize
Picked up JAVA_TOOL_OPTIONS: -Xmx2g
    uintx MaxHeapSize                              := 3221225472                          {product}
java version "1.8.0_74"
Java(TM) SE Runtime Environment (build 1.8.0_74-b02)
Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)
18:40:12 [jason:~] $ JAVA_TOOL_OPTIONS="-Xmx2g" java -XX:+PrintFlagsFinal -version | grep MaxHeapSize
Picked up JAVA_TOOL_OPTIONS: -Xmx2g
    uintx MaxHeapSize                              := 2147483648                          {product}
java version "1.8.0_74"
Java(TM) SE Runtime Environment (build 1.8.0_74-b02)
Java HotSpot(TM) 64-Bit Server VM (build 25.74-b02, mixed mode)
```
</comment><comment author="clintongormley" created="2016-03-16T09:51:22Z" id="197233815">Actually, we do document JAVA_OPTS in two places:
- https://github.com/elastic/elasticsearch/blob/master/docs/reference/setup/configuration.asciidoc
- https://github.com/elastic/elasticsearch/blob/master/docs/plugins/plugin-script.asciidoc

I'm good with standardising on `ES_JAVA_OPTS` though.  Bonus points for warning if `JAVA_OPTS` is set and `ES_JAVA_OPTS` is not.
</comment><comment author="s1monw" created="2016-03-17T08:29:29Z" id="197760002">just linking https://github.com/elastic/elasticsearch/pull/17160 and this issue since whichever is committed first must adopt
</comment><comment author="clintongormley" created="2016-03-17T13:14:43Z" id="197874535">#17160 was closed with https://github.com/elastic/elasticsearch/commit/bd059b8cc3c8e28a5a9866621f96e3c36f9a8ee7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>revert new completion suggester in 2x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17120</link><project id="" key="" /><description>The new completion suggester is a breaking change that we should not introduce in a minor version. Instead it will be introduced in version 5.0
</description><key id="141077466">17120</key><summary>revert new completion suggester in 2x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>blocker</label><label>non-issue</label><label>review</label><label>v2.3.0</label></labels><created>2016-03-15T19:36:55Z</created><updated>2016-03-16T01:56:02Z</updated><resolved>2016-03-16T01:56:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>_analyze never responds given an unknown tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17119</link><project id="" key="" /><description>I came across this while trying to see the English analyzer, but I forgot to change `tokenizer` to `analyzer`.

```
GET /_analyze
{
  "tokenizer" : "english",
  "text" : "whatever"
}
```

The only error that comes out of Elasticsearch (tested on 2.2.0 and 2.1.1) is an obscure one:

```
[2016-03-15 14:52:11,050][ERROR][transport                ] [Jessica Drew] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@4ffecb71]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Worse still, it never actually closes the connection so the request hangs, awaiting a response (probably indicating a secondary bug in this code path).
</description><key id="141067930">17119</key><summary>_analyze never responds given an unknown tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Analysis</label><label>bug</label></labels><created>2016-03-15T18:58:12Z</created><updated>2016-03-15T19:19:32Z</updated><resolved>2016-03-15T19:11:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-15T19:11:34Z" id="196978965">Duplicates #15148, closed by #15447
</comment><comment author="clintongormley" created="2016-03-15T19:12:19Z" id="196979184">Too slow, @jasontedor !
</comment><comment author="pickypg" created="2016-03-15T19:14:15Z" id="196979871">Ugh, I searched for _analyze!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add tests for building SuggestionSearchContext from suggestion builders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17118</link><project id="" key="" /><description>When we complete the current refactoring of the suggestion builders (https://github.com/elastic/elasticsearch/pull/17096), we should add tests that check the output of the suggestion builders `build()` method, similar to what we did when refactoring the query builders. While we were refactoring the suggestion builders, we temporarily added tests that check that the results of parsing a builder in its xContent representation and calling the `build()` methods resulted in the same SuggestionSearchContext. After the deletion of SuggestParseElement we cannot use this kind of similarity check any more, so we need dedicated unit tests for that. 
</description><key id="141057682">17118</key><summary>Add tests for building SuggestionSearchContext from suggestion builders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>test</label></labels><created>2016-03-15T18:17:48Z</created><updated>2017-07-07T14:22:11Z</updated><resolved>2017-07-07T14:22:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeff303" created="2016-05-23T19:38:13Z" id="221073028">I'm interested in taking a crack at this one.  But I'm finding myself writing quite a bit of code to mock and stub out the necessary parts of the `AnalysisService`, `SimilarityService`, `MapperRegistry`, etc. that are needed in order to call `SuggestBuilder.build`.  It makes me wonder whether I'm going about this in the right way.  Does anyone know of any existing test case that provide instances that would be suitable here, or a good starting point?
</comment><comment author="nik9000" created="2016-05-23T19:43:33Z" id="221074431">&gt; Does anyone know of any existing test case that provide instances that would be suitable here, or a good starting point?

`AbstractQueryTestCase` provides a lot of it, maybe all? Maybe good to figure out a way to reuse the code rather than copy it.
</comment><comment author="cbuescher" created="2016-05-25T10:12:17Z" id="221530930">@jeff303 great that you want to look into this. What makes this kind of tests a bit tricky is that they should not require only minimal setup and mocking necessary for creating the SuggestionContext and then doing some meaningfull checks on it without having to really execute them. At least that was the idea, otherwise we are better off testing the output of the suggestion builder in integration tests like the existing `SuggestSearchTest`.
</comment><comment author="jeff303" created="2016-05-25T15:14:35Z" id="221608597">Yeah, I was able to get it working pretty easily using @nik9000  suggestion, since `AbstractQueryTestCase` does provide a lot of the necessary setup.  However, it is definitely heavier weight since it involves creating the cluster, index, etc.  I am happy to investigate what the minimum set of objects that needs to be mocked is, especially now that I have something to start with.  I mostly just wanted to make sure I wasn't going down the wrong path by missing something suitable that already exists in the other test classes.
</comment><comment author="jeff303" created="2016-05-26T21:44:18Z" id="222003897">OK, I have what I believe to be the starting point now.  I didn't end up needing any mocking.  Also, I wasn't sure if it was important to assert the `QueryShardContext` objects themselves.  If so, I could implement a deep equals (or else compare by reference in this particular case, but that seems to be of little value).  I couldn't find a cleaner way to handle the `MapperService` other than as a class level field, since there's a bit of a chicken and egg issue with the `MapperService` needing a `Supplier&lt;QueryShardContext&gt;`, which needs a reference to the `MapperService`.

If anyone could take a quick look and let me know if I'm on the right track, I could fix/expand as needed before creating the pull request.

https://github.com/jeff303/elasticsearch/blob/fix/add-suggestion-search-context-tests/core/src/test/java/org/elasticsearch/search/suggest/SuggestionSearchContextTests.java
</comment><comment author="cbuescher" created="2016-05-30T12:32:04Z" id="222482603">@jeff303 nice, I took a quick look and I think you are on the right track. However I think we should add tests for the output of the `build()` method to the already existing Phrase/Completion/TermSuggestionBuilderTests, possibly with a common setup in AbstractSuggestionBuilderTestCase. Those tests already provide ways of generating the suggestion builders with randomized parameters. The tests would call the `public SuggestionContext build(QueryShardContext context)` method on a random builder obtained by `AbstractSuggestionBuilderTestCase#randomTestBuilder()` (using a QueryShardContext that provides enough mocked services as needed) and then check that the builder pameters were correctly translated / transferred to the resulting SuggestionContext. 

To better see what we had before, you can take a look at https://github.com/elastic/elasticsearch/commit/4b803d75cf3d06faa0bb81472ca09564406c70c0#diff-8877c8d8032280d9ca494007ca04b25aL239 and some of the assertions e.g. in https://github.com/elastic/elasticsearch/commit/4b803d75cf3d06faa0bb81472ca09564406c70c0#diff-7925b951f7645dcb8a552ff8440accd3L164

Those tests basically checked that when we moved the parsing logic from its previous location in the code to the `build()` methods we have now, we didn't break anything. However since we removed the original parsing logic, we want to check things like `assertEquals(phraseSuggestionBuilder.gramSize(), phraseSuggestionContext.gramSize())`, for as much parameters as this is feasible without too heavy mocking. Let me know if you have questions in case you want to continue working on this.
</comment><comment author="jeff303" created="2016-06-20T22:04:52Z" id="227284117">@cbuescher , I'd appreciate it if you had a look and let me know if this is more on track (see `PhraseSuggestionBuilderTests` specifically).  I'm not sure if I'm collecting the info in a great way from the random suggestion builders (and putting them in the `MapperService`), or if I could avoid adding the extra getters in `DirectCandidateGeneratorBuilder` and `PhraseSuggestionBuilder`.

https://github.com/elastic/elasticsearch/compare/master...jeff303:fix/add-suggestion-search-context-tests
</comment><comment author="cbuescher" created="2016-06-21T15:34:39Z" id="227478947">Hi @jeff303, first of all thanks for working on this. I was about to leave a few comments on the code on your branch, but I can't seem to do this unless it's an official pull request, I'll add just a few thoughts here:
- adding the extra getters in `DirectCandidateGeneratorBuilder` and `PhraseSuggestionBuilder` should be okay, the paramters don't allow modification which is fine
- I briefly looked at the additional setup code and was wondering if all of this is really necessary for the tests that you are adding (not sure, might be). If in doubt, I would rather leave out testing all possible ways of e.g. setting analyzers in the suggester if that simplifies setup
- I'd try to avoid having to reset the mapper service (resetMapperService()) in AbstractSuggestionBuilderTestCase. You probably added this because the preFilter/postFilter analyzer names and fieldnames are randomly generated later with the random SuggestionBuilder. Maybe its possible to generate these analyzer and fieldnames upfront (e.g. a static array in PhraseSuggestionBuilderTests that the random setup draws from, or something similar)
- I'd investigate to pull up all checks for common SuggestionContext parameters (like size, analyzer, etc..) into the base test
- Whats the reason for having a dedicated SuggestionSearchContextTests? Currently it looks like lots of setup for just a few basic checks to me, those could probably be part of the general randomized tests, unless I'm missing something?

All in all I think this already looks like its going into a good direction, feel free to open a PR if you want to discuss details. Even if that PR turns out to need to be rewritten it provides room for discussion and detailed comments.
</comment><comment author="jeff303" created="2016-06-21T17:38:11Z" id="227514854">OK, thanks for the feedback, @cbuescher .  The `SuggestionSearchContextTests` class was just how I started out before switching over to the existing suggestion builder tests.  I will remove that, and incorporate the other points and open an official PR.
</comment><comment author="jeff303" created="2016-10-14T17:37:16Z" id="253869526">Unfortunately, I don't have the time to finish this right now, having just started a new full time job (among other things).  I went ahead and pushed my latest changes (same branch as linked above), and I feel things are maybe 80% there.
</comment><comment author="cbuescher" created="2016-10-17T08:14:52Z" id="254141671">Thanks  @jeff303 for the update and all the effort so far. Since this is stalled a bit, would it be okay for somebody else to pick it up if she is interested? I'm just asking because this issue is still labeled "adoptme" which is meant to be a label that should encourage new contributers to take a look at open issues. If you have an idea if or when you'll be able to come back to this, we can also remove that label and assign it to you.
</comment><comment author="jeff303" created="2016-10-17T21:08:54Z" id="254334729">@cbuescher , that sounds perfectly fine to me (for someone else to pick it up).  I initially grabbed it because of the label, but since I can't quite get it through the finish line, we should probably just leave it and update the assignee appropriately.
</comment><comment author="cbuescher" created="2017-07-07T14:22:04Z" id="313696149">Fixed with #25549, #25558, #25571 and #25575</comment></comments><attachments /><subtasks /><customfields /></item><item><title>503 Timeout errors on PUT request to update mapping during cluster rebalancing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17117</link><project id="" key="" /><description>Could be related to #8193 and #9323

Elasticsearch version: 1.7.3.3

I'm noticing PUT requests to update mapping are failing with a 503 error on the cluster when re-balancing occurs and re-locating shards are present on the cluster. Is this because the 30 second timeout is being reached? This is ha-penning even if the cluster is not restarted and occasional re-balancing of shards occurs.

Eg: 
PUT https://localhost:9200/Indexname/documenthighlight/_mapping
{
}

error=RemoteTransportException[[c383nha-master-9200][inet[/10.204.160.10:9300]][indices:admin/mapping/put]]; nested: ProcessClusterEventTimeoutException[failed to process cluster event (put-mapping [documenthighlight]) within 30s]; 
status=503
</description><key id="141040384">17117</key><summary>503 Timeout errors on PUT request to update mapping during cluster rebalancing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vineet85</reporter><labels /><created>2016-03-15T17:12:48Z</created><updated>2016-03-21T14:39:54Z</updated><resolved>2016-03-15T19:02:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-15T19:02:15Z" id="196974434">Hi @vineet85 

It sounds like you have a very large mapping, possibly with some less-than-responsive nodes.

Time to upgrade! (and reconsider why your mappings are so large)
</comment><comment author="vineet85" created="2016-03-21T14:39:54Z" id="199319257">Thanks, think of it the mapping update we are making is significantly large. We need to re-think our mapping.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to combine several query scores with multiply or other</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17116</link><project id="" key="" /><description>Currently we can use a bool query to combine the result of different queriers as sum or dismax which does max. But sometimes people might want to combine the results of several queries in different ways, for example: http://stackoverflow.com/questions/31755642/how-can-i-multiply-the-score-of-two-queries-together-in-elasticsearch

`function_score` could be changed to enable multiply, min, and average. 
Currently `function_score` can only make use of the score of one query and then combine that with some functions. But that could be changed by replacing the filter for each function with a query and then give access to the individual scores of these queries inside the functions

Something like this (not even trying to name stuff...):  

```
POST _search
{
  "query": {
    "function_score": {
      "query": {
        // here be a query or not, resulting score can be used later via _score
      },
      "functions": [
        {
          "query": {
            // another fancy query, maybe even another function_score?, resulting score can be used via _xyz_score
          }, 
          "boost_mode": "multiply", // need this here now too because we need to know how to combine the function with the _xyz_score
          "script_score": {
            "script": "_xyz_score * _score"
          }
        },
        {
          "query": {
            // even more query!, resulting score can be used via _xyz_score
          }, 
          "boost_mode": "sum", // need this here now too because we need to know how to combine the function with the _xyz_score
          "script_score": {
            "script": "_xyz_score * doc['b'].value"
          }
        },
        ...
      ],
      "score_mode": "multiply"
    }
  }
}
```

I think there was an issue about that already somewhere but I cannot find it.
This might relate to https://github.com/elastic/elasticsearch/issues/10049 because people could then make arbitrary complicated combinations using scripts and nesting function score queries. It would be crude though.
</description><key id="141038000">17116</key><summary>Add option to combine several query scores with multiply or other</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>discuss</label><label>feature</label></labels><created>2016-03-15T17:04:23Z</created><updated>2017-04-03T16:18:03Z</updated><resolved>2017-03-31T13:38:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2016-03-18T11:16:10Z" id="198312467">Just discussed this in fixit friday and now we think it should be differently structured, more like in #10049:

Each function produces a variable which can be named with some parameter (var_name?).

We add an additional option `scrore_mode: script` that has the results of the functions as variables. The final score is then the result of the script.

In addition, we need a different function `query_function` which returns the result of a query. We thought that the above approach (make the filters we have now together with functions score) would be confusing and convolute stuff too much.

Something like:

```
POST _search
{
  "query": {
    "function_score": {
      "query": {
        // same as before, score will be accessible via _score
      },
      "functions": [
        {
          "query_function": {
            "query": {
              // here be any query, can also be function_score
            },
            "var_name": "score_a"
          }
        },
        {
          "random_score": {
            "var_name": "score_a",
            ...
          }
        },
        ...
      ],
      "score_mode": "script",
      "combine_script": "score_a * score_b + _score"
    }
  }
}
```
</comment><comment author="babadofar" created="2016-03-27T01:51:37Z" id="201971045">Cool! 
</comment><comment author="synhershko" created="2016-04-23T23:52:14Z" id="213853949">I'm the OP of #10049 and #17820 - both seem to be satisfied by the proposed solution, so looking forward to this implementation.
</comment><comment author="brwe" created="2016-05-26T13:34:16Z" id="221871852">I guess best would be to split this in two: 1. implement query function and 2. implement custom combine. I'll start working on this unless anyone else calls dibs. 
</comment><comment author="brwe" created="2016-05-30T19:43:46Z" id="222546021">@JnBrymn-EB and I discussed a little  about the combine script parts and we thought that we should probably change the above syntax. The variable name per function could be on the same level as the filter, weight and function instead of being a parameter inside the function definition because each function score can be assigned to a variable just like every function can have a weight or a filter. Also, the script should probably follow the same script syntax we have elsewhere. The query would then look like this:

```
POST _search
{
  "query": {
    "function_score": {
      "query": {
        // same as before, score will be accessible via _score
      },
      "functions": [
        {
          "query_function": {
            "query": {
              // here be any query, can also be function_score
            }
          },
          "var_name": "score_a",
          "filter": {
               // some filter
          }
        },
        {
          "random_score": {
            ...
          },
          "var_name": "score_a",
          "weight": 3.33
        },
        ...
      ],
      "score_mode": "script",
      "combine_script": {
          "lang": "groovy",
          "inline": "score_a * score_b + _score"
      }
    }
  }
}

```
</comment><comment author="clintongormley" created="2016-06-01T16:13:28Z" id="223044222">I'd suggest changing `query_function` to `query_score`, and `combine_script` to `score_script`.  otherwise looks great!
</comment><comment author="JnBrymn-EB" created="2016-06-06T02:03:15Z" id="223854049">I'm building the combine part as we speak. Should we go with `var_name` as stated above or should we use `_name` as I've seen in other places?
</comment><comment author="brwe" created="2016-06-28T12:39:20Z" id="229036772">We settled for `var_name`. 

In addition, another question came up: A function might be associated with a filter that does not match. What value do we assign to the variable in this case? I have the feeling we need a default value here. Something like:

```
...
"functions": [
        {
          "script_variable": {
             "name": "score_a",
             "default": 123
          },
          "filter": {
               // some filter
          },
          "field_value_factor": {...}
        }
....
```
</comment><comment author="JnBrymn-EB" created="2016-06-28T20:20:24Z" id="229170394">Could we add a `missing` field here just with the [field_value_factor](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-field-value-factor) and make it default to 0 for the sake of a `score_script`? We'd have to be careful not to affect existing functionality like `score_mode=avg` which just assumes that the value doesn't exist. -- It might be a bit misleading.

Maybe another take would be adding a `default_vals` key to the `combine_script` that would enumerate the value of each clause that might be missing.
</comment><comment author="clintongormley" created="2016-07-01T12:33:23Z" id="229935282">I'd go with `missing`, and in fact we should probably apply this to all functions (this has come up before).  I'm wondering if the change to `score_mode:avg` is a problem?
</comment><comment author="brwe" created="2016-07-01T13:43:57Z" id="229950089">Just to be clear: I meant to add a default if the `"filter"` doesn't not match. In case the field is missing it would still be up to the function to decide what to do. 
</comment><comment author="brwe" created="2016-07-01T16:38:17Z" id="229992494">I'll explain in more detail what I mean.
We have two cases: 
1. the field is missing in the document
2. the filter associated with the function does not match

In the first case, we have three functions that have to deal with it: `field_value_factor` (takes a `missing` parameter and if the value is missing uses that instead of an actual value), `decay_function` (assumes the value is perfectly at the `origin`, which has greatly annoyed many users and might change, see https://github.com/elastic/elasticsearch/issues/18892) and `script_score` where everyone has to adjust the script to deal with it.

In the second case currently `function_score` acts for this document as if the function would not exist at all. 

I was only talking about 2., filter not matching. 

We could add a `score_missing` or `default` parameter that would do the following: If the filter for a function does not match then we always return this value.

This would have also the advantage that it would allow everyone to control not only input to individual functions in case field is missing (with the `missing` parameter) but also to control the output like so:

```
"function_score": {
      "functions": [
        {
          "filter": {
            "exists": {
              "field": "age"
            }
          },
          "field_value_factor": {
            "field": "age",
            "modifier": "ln"
          },
          "score_missing": 5 
        }
      ]
    }
```

Also, it would allow people to control what  `score_mode: avg` means in case a filter is not matching, which is awkward right now.

For example in this case:

```
"function_score": {
      "score_mode": "avg", 
      "functions": [
        {
          "filter": {
            "term": {
              "skill": "codes_java"
            }
          },
          "weight": 5, 
          "score_missing": 0
        },
        {
          "filter": {
            "term": {
              "skill": "speaks_human"
            }
          },
          "weight": 2, 
          "score_missing": 0
        }
      ]
    }
```

in case the term `codes_java` is not in field `skills`, the score would be computed as `(0+2)/(5+2)` instead of just `2/2` which is the default right now and might not be desirable.

For the `script_combine` we should then enforce that this parameter exists if a function is associated with a filter.

I would not call it `missing` because I at least might mix that up with the `missing` in case the field does not exist in the doc. 
</comment><comment author="clintongormley" created="2016-07-04T14:14:29Z" id="230300086">This makes sense to me.  What about calling it `no_match_score` or `default_score`?  I think I prefer the former because it is more explicit.
</comment><comment author="mckinnovations" created="2016-07-18T11:26:11Z" id="233304006">Any timeline for this feature ? when is it going to be released?
</comment><comment author="JnBrymn-EB" created="2016-08-01T05:55:50Z" id="236495488">@mckinnovations https://github.com/elastic/elasticsearch/pull/19710
</comment><comment author="serj-p" created="2017-03-02T12:06:59Z" id="283636464">Have `query_score ` or `query_function` keywords been added?
I am trying to compute max scores for docs from two queries, one calculating `field_value_factor` for `updated` field from a child doc and other is a `field_value_factor` for parent's `updated` value.  So doc score I need is `max(child.updated, doc.updated)`. I see no way to tell elasticsearch  to return such max `updated` currently.</comment><comment author="erebus1" created="2017-03-02T20:12:10Z" id="283766160">I think you can use dis_max query of 2 function_score queries
https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-dis-max-query.html</comment><comment author="erebus1" created="2017-03-02T20:13:31Z" id="283766530">But the question about timeline for query_score is really important.</comment><comment author="erebus1" created="2017-03-30T16:39:03Z" id="290468652">Guys, do you plan to implement this feature?</comment><comment author="JnBrymn-EB" created="2017-03-30T21:41:20Z" id="290553261">This is becoming more important for upcoming work at Eventbrite.</comment><comment author="clintongormley" created="2017-03-31T13:38:12Z" id="290714153">We've been rethinking this approach.  Apparently, according to research, the best way to combine scores is to add them together (which the bool query does, now that coordination and query norm are gone).

So we're looking at better ways of exposing primitives for incorporating non-textual scores into the overall score.

Closing in favour of https://github.com/elastic/elasticsearch/issues/23850</comment><comment author="JnBrymn-EB" created="2017-03-31T15:13:37Z" id="290740081">"coordination and query norm are gone" - do you have any documentation on that @clintongormley ?</comment><comment author="clintongormley" created="2017-04-03T14:48:06Z" id="291165969">@JnBrymn-EB they've been removed in Lucene 7 https://issues.apache.org/jira/browse/LUCENE-7347

query coordination was a hack to make TF/IDF work better in the face of poor TF saturation, and query norm (i believe) was essentially a failed experiment to try to make the scores from different queries comparable.

with those removed, the bool query now just does a simple sum, and boosting clauses is a much simpler calculation than before.</comment><comment author="JnBrymn-EB" created="2017-04-03T16:18:03Z" id="291193611">Fascinating! I'll have to soak this in.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>jq as response filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17115</link><project id="" key="" /><description>Just came across https://github.com/eiiches/jackson-jq, which is an implementation of jq (https://stedolan.github.io/jq/) using Jackson. I don't know about the quality of the implementation itself (looks like it has the "spec" tests embedded), but if it is good, it might be a good option for us to expose in a similar manner to filter path.
</description><key id="141002242">17115</key><summary>jq as response filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:REST</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-15T15:12:17Z</created><updated>2017-05-12T02:37:00Z</updated><resolved>2017-05-12T02:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2016-03-15T15:56:37Z" id="196894118">looked at it a bit more, it seem like this requires full tree model representation of the output before it gets filtered, which is different compared to the "stream" filtering we do in filter path, so it will have memory implications.
</comment><comment author="rashidkpc" created="2016-03-15T16:02:21Z" id="196896818">This would be amazing. Certainly has abuse implications, eg, using _search with a huge size to do aggregations on a bunch of _source fields, but would allow for some fantastic work arounds for less agreeable response structures. 

I'd love to have it, we might just need a sane limit on how big of an object could be loaded in the jq pipe.
</comment><comment author="clintongormley" created="2016-03-18T11:16:21Z" id="198312534">We talked about this in Fix It Friday and some concerns were raised.

Elasticsearch is designed to return small representations of your data. Using something like jq may well lead to abuse of this by hiding the size/complexity of the response behind the jq layer, which can have a performance and memory impact.

We already have response filtering which is efficient and simple.  jq has a syntax which means potentially dealing with syntax errors (which probably means a different response from the one the user expects)

This processing can obviously be done client side.  Is the resistance to doing this on the client because of network bandwidth?  If so, perhaps the answer is to enable HTTP compression by default instead (something which we are going to investigate https://github.com/elastic/elasticsearch/issues/7309)
</comment><comment author="kimchy" created="2016-03-21T14:15:30Z" id="199304001">I agree, this can be dangerous..., specifically with the amount of potential processing/memory it will require. Maybe we can have some examples of where this can be handy to begin with captured in this issue.
</comment><comment author="TheFrozenFire" created="2017-05-04T22:01:57Z" id="299321700">I've been looking for exactly this sort of feature. My organization does a lot of data reporting right out of ES. Right now, I'm having to maintain external tooling which queries ES and then runs the response through jq to produce the report format I need (such as converting to CSV).

I would envision this being implemented as an additional script type like mustache, groovy, and painless. One could specify a jq filter script inline, indexed, or from a file. The jq filter could be specified as a response filter, which would be executed by the coordinating node to process the response body accordingly.

I think that such functionality would come with the explicit instruction that when used, the response body becomes unpredictable and possibly non-JSON. As I don't use ES extensively, I admittedly don't know what the further-reaching implications of this would be. But, I don't see it as much different than having a load balancer in front of the coordinating nodes, and that balancer possibly returning unexpected output to HTTP requests.</comment><comment author="nik9000" created="2017-05-05T15:20:16Z" id="299493917">I don't think we're likely to work on this in the near future. Maybe never, given the whole tree processing thing. I love jq and it'd be super fun to have an implementation of it available but I don't think it is worth it and I know it isn't worth it at this time.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove "sandbox" option from script settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17114</link><project id="" key="" /><description>Today you can set, eg, `script.inline: sandbox`  which means "enable inline scripting for all languages which are considered to be sandboxed".

Given that this is the default, and the user would typically want to disable scripting, I think that the "sandbox" option just muddies the water, and the script settings should only accept `true` or `false`.

Closes #10548
</description><key id="141001717">17114</key><summary>Remove "sandbox" option from script settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Scripting</label><label>adoptme</label><label>blocker</label><label>breaking</label><label>low hanging fruit</label><label>v5.0.0-alpha3</label></labels><created>2016-03-15T15:10:45Z</created><updated>2016-05-13T15:50:54Z</updated><resolved>2016-05-13T15:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-15T15:11:13Z" id="196868043">/cc @rashidkpc 
</comment><comment author="rashidkpc" created="2016-03-15T16:45:56Z" id="196916980">Really need this to add support for other scripting languages to Kibana. We really just need to know what languages exist, and if inline scripting is enabled on them. `sandbox` basically means "maybe", so we really need the true/false.
</comment><comment author="clintongormley" created="2016-05-07T15:06:27Z" id="217643125">@dakrone would you mind working on this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ingest node to node types documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17113</link><project id="" key="" /><description>**Elasticsearch version**: 5.0.0

We list master-eligible, data, client, and tribe, but missing our new ingest node at https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-node.html
</description><key id="140999994">17113</key><summary>Add ingest node to node types documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>docs</label></labels><created>2016-03-15T15:06:10Z</created><updated>2016-03-15T18:03:40Z</updated><resolved>2016-03-15T18:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Forbid test sources to use System.out.println and Throwable.printStackTrace</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17112</link><project id="" key="" /><description>Test sources have been exempt from the bundled forbiddenApi signatures `jdk-system-out` that check among others usages of `System.out.println` and `Throwable.printStackTrace`. With the removal of benchmarks in #15356, there is no need to relax this check for tests anymore.

This PR:
- enables `jdk-system-out` checks on all sources
- removes the few remaining usages of println and printStackTrace
- removes two benchmark classes (probably missed in #15356)
</description><key id="140994271">17112</key><summary>Forbid test sources to use System.out.println and Throwable.printStackTrace</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T14:48:48Z</created><updated>2016-03-15T15:36:35Z</updated><resolved>2016-03-15T15:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-15T15:04:14Z" id="196863328">left one minor comment - LGTM otherwise, no need to re-review
</comment><comment author="jasontedor" created="2016-03-15T15:17:56Z" id="196871576">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix a potential parsing problem in GeoDistanceSortParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17111</link><project id="" key="" /><description>A test revealed a potential problem in the current GeoDistanceSortParser. 
For an input like `{ [...], "coerce" = true, "ignore_malformed" = false }` the parser will fail to parse the `ignore_malformed` boolean flag and will fall through to the last else-branch where the boolean flag will be parsed as geo-hash and `ignore_malformed` treated as field name. 

Adding fix and modifying a test so that it would fail with the old parser code.
</description><key id="140994243">17111</key><summary>Fix a potential parsing problem in GeoDistanceSortParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T14:48:41Z</created><updated>2016-03-16T09:17:09Z</updated><resolved>2016-03-16T09:17:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-15T14:50:24Z" id="196853232">@nknize @MaineC I just stumbled upon this part in the GeoDistanceSortParser when extending testing around it during some refactoring. Can you take a quick look if this change makes sense?
</comment><comment author="MaineC" created="2016-03-16T07:56:39Z" id="197201328">Makes sense to me. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fetch phase should not init a LeafSearchLookup for each document with groovy script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17110</link><project id="" key="" /><description>In 2.2 when someone uses a script field then a new `LeafSearchLookup` is created for each document that is fetched via the call stack below. This is unfortunate if the documents are fetched with scan and scroll because this renders the caches in `LeafDocLookup` and `LeafFieldsLookup` and  `LeafIndexLookup` useless. 

```
        at org.elasticsearch.search.lookup.IndexLookup.getLeafIndexLookup(IndexLookup.java:69)
    at org.elasticsearch.search.lookup.SearchLookup.getLeafSearchLookup(SearchLookup.java:57)
    at org.elasticsearch.script.groovy.GroovyScriptEngineService$4.getLeafSearchScript(GroovyScriptEngineService.java:236)
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:77)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:187)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:478)

```
</description><key id="140979589">17110</key><summary>Fetch phase should not init a LeafSearchLookup for each document with groovy script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2016-03-15T13:55:44Z</created><updated>2017-01-12T10:41:57Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2017-01-12T10:41:57Z" id="272131105">@jpountz is this fixed?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>we can use term aggregation with  moving average aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17109</link><project id="" key="" /><description>we can use term aggregation with  moving average aggregation

in elasticsearch 2.2.0

please help it is possible   

ajay.rathod39@gmail.com
</description><key id="140973079">17109</key><summary>we can use term aggregation with  moving average aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kingaj</reporter><labels /><created>2016-03-15T13:32:40Z</created><updated>2016-03-18T05:50:59Z</updated><resolved>2016-03-15T17:49:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-15T17:49:44Z" id="196945456">Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment><comment author="kingaj" created="2016-03-18T05:50:59Z" id="198218701">yes i know but so many time i open  https://discuss.elastic.co/ but it can show any thing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Try to renew sync ID if `flush=true` on forceMerge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17108</link><project id="" key="" /><description>Today we do a force flush which wipes the sync ID if there is one which
can cause the lost of all benefits of the sync ID ie. fast recovery.
This commit adds a check to renew the sync ID if possible. The flush call
is now also not forced since the IW will show pending changes if the forceMerge added new segments.
if we keep using force we will wipe the sync ID even if no renew was actually needed.

Closes #17019
</description><key id="140952602">17108</key><summary>Try to renew sync ID if `flush=true` on forceMerge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>review</label><label>v2.2.2</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T12:00:44Z</created><updated>2016-03-15T13:16:25Z</updated><resolved>2016-03-15T13:16:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-15T12:00:57Z" id="196789416">@mikemccand @bleskes can you take a look
</comment><comment author="bleskes" created="2016-03-15T12:31:17Z" id="196798701">LGTM. Thx @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>transport client dns refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17107</link><project id="" key="" /><description>Allow specifying a hostname for the transport client, and automatically refresh the ip addresses.
This is needed when using hosted elasticsearch (found) where the ip addresses can change.

Some references:
https://discuss.elastic.co/t/nonodeavailableexception-with-java-transport-client/37702
#16412
</description><key id="140940863">17107</key><summary>transport client dns refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brackxm</reporter><labels /><created>2016-03-15T11:12:41Z</created><updated>2016-03-15T11:57:46Z</updated><resolved>2016-03-15T11:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-15T11:57:46Z" id="196788054">Duplicates #16412 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndicesStore checks for `allocated elsewhere` for every shard not alocated on the local node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17106</link><project id="" key="" /><description>On each cluster-state update we check on the local node if we can delete some shards content.
For this we linearly walk all shards and check if they are allocated and started on another node
and if we can delete them locally. if we can delete them locally we go and ask other nodes if we can
delete them and then if the shared IS active elsewhere issue a state update task to delete it. Yet,
there is a bug in IndicesService#canDeleteShardContent which returns `true` even if that shards
datapath doesn't exist on the node which causes tons of unnecessary node to node communication and
as many state update task to be issued. This can have large impact on the cluster state processing
speed. 

**NOTE:** This only happens for shards that have at least one shard allocated on the node ie. if an `IndexService` exists.
</description><key id="140939593">17106</key><summary>IndicesStore checks for `allocated elsewhere` for every shard not alocated on the local node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Allocation</label><label>bug</label><label>critical</label><label>review</label><label>v2.2.2</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T11:05:37Z</created><updated>2016-03-15T11:59:45Z</updated><resolved>2016-03-15T11:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-15T11:11:10Z" id="196768161">@ywelsch @jasontedor or @bleskes wanna take a look ?
</comment><comment author="bleskes" created="2016-03-15T11:27:44Z" id="196775087">Great catch. LGTM. Can you please adapt the PR description the note that for this to happen you need one shard to be allocated on the node (and then all other shards will be checked).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forked memory index to add support for doc values.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17105</link><project id="" key="" /><description>This forks changes from the MemoryIndex to add doc values support [1] to the MemoryIndex in 2.x This PR is just this branch and fixes issues like #16832 where geo queries that depend on doc values don't fail or silently return no hits.

Once master upgrades to next Lucene 6 snapshot then #16832 is fixed as well.

The back porting of the MemoryIndex is something I don't like. But it does fix a big shortcoming that happens in version 2.2 and higher. (Classifying data based on geo queries is a common use for the percolator.) I'm not sure whether we should push this change.

Closes #16832 

1: https://issues.apache.org/jira/browse/LUCENE-7091
</description><key id="140922076">17105</key><summary>Forked memory index to add support for doc values.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>review</label></labels><created>2016-03-15T09:40:16Z</created><updated>2016-03-17T11:04:54Z</updated><resolved>2016-03-16T13:30:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-16T11:29:38Z" id="197275196">LGTM
</comment><comment author="rmuir" created="2016-03-16T12:17:24Z" id="197291407">I think its not good to fork unreleased lucene code like this. Its basically the worst software engineering decision you can make and there are all kinds of possibilities where we are "left" hanging on to the copy of it.

You can say "that will never happen" but we know it does, and thats why there is still so much of this in the codebase.

We should not do this. Its been shown time and time again that ES engineering is incapable of handling this kind of thing properly. Wait for the release for the new feature...
</comment><comment author="martijnvg" created="2016-03-16T13:30:15Z" id="197329921">Closing this issue for ES &gt;= 2.2 use the work around: https://github.com/elastic/elasticsearch/issues/16832#issuecomment-197329289

and from 5.0 things work as they did before.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also map floating-point numbers as floats when numeric detection is on.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17104</link><project id="" key="" /><description>I overlooked it in #15319 since numeric detection triggers a totally different
path in the code of dynamic mappings.
</description><key id="140921823">17104</key><summary>Also map floating-point numbers as floats when numeric detection is on.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T09:38:39Z</created><updated>2016-03-23T07:21:23Z</updated><resolved>2016-03-23T07:21:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-15T09:39:13Z" id="196742696">I put the `non-issue` label since this affects non-released code.
</comment><comment author="rjernst" created="2016-03-23T05:54:48Z" id="200193876">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reenable CreateIndexIT#testCreateAndDeleteIndexConcurrently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17103</link><project id="" key="" /><description>Since #16442 is merged we should be able to reenable this test as a followup
of #15853 - all issues blocking it have been resolved I guess.

@bleskes @ywelsch WDYT
</description><key id="140907021">17103</key><summary>Reenable CreateIndexIT#testCreateAndDeleteIndexConcurrently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label><label>test</label></labels><created>2016-03-15T08:32:55Z</created><updated>2016-03-15T08:46:22Z</updated><resolved>2016-03-15T08:46:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-15T08:36:28Z" id="196719289">LGTM. Had it on my todo list to do the same :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade azure SDK to 0.9.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17102</link><project id="" key="" /><description>We are ATM using azure SDK 0.9.0.

Azure latest release is now 0.9.3 (released in February 2016).

&lt;img width="1024" alt="the central repository search engine google chrome aujourd hui at 08 41 12" src="https://cloud.githubusercontent.com/assets/274222/13662836/a806ba3a-e69d-11e5-8655-4a838db2ef47.png"&gt;

Artifacts are on [maven central](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.microsoft.azure%22%20AND%20%28a%3Aazure-serviceruntime%20OR%20a%3Aazure-servicebus%20OR%20a%3Aazure-svc-*%29)

Change log:
## 2016.2.18 Version 0.9.3
- Fix enum bugs in azure-svc-mgmt-websites
## 2016.1.26 Version 0.9.2
- Fix HTTP Proxy for Apache HTTP Client in Service Clients
- Key Vault: Fix KeyVaultKey to not attempt to load RSA Private Key
## 2016.1.8 Version 0.9.1
- Support HTTP Proxy
- Fix token expiration issue #557
- Service Bus: Add missing attributes: partitionKey, viaPartitionKey
- Traffic Manager: Update API version, add MinChildEndpoints for NestedEndpoints
- Media: Add support for Widevine (DRM) dynamic encryption

Closes #17042.
</description><key id="140904436">17102</key><summary>Upgrade azure SDK to 0.9.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>review</label><label>upgrade</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-15T08:20:43Z</created><updated>2016-03-15T08:48:00Z</updated><resolved>2016-03-15T08:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-15T08:22:31Z" id="196712960">LGTM
</comment><comment author="s1monw" created="2016-03-15T08:22:55Z" id="196713045">@dadoonet do we also wanna upgrade `2.x`?
</comment><comment author="dadoonet" created="2016-03-15T08:23:59Z" id="196713365">Was wondering the same. I agree it makes sense to upgrade for 2.3.0
</comment><comment author="dadoonet" created="2016-03-15T08:48:00Z" id="196723014">Also applied in `2.x` branch with 728eec2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>queryCacheMemory is a shorthand for 2 different values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17101</link><project id="" key="" /><description>In `_cat/indices` API on 2.2.0:

```
query_cache.memory_size              | fcm,queryCacheMemory               | used query cache                                                                                                 
pri.query_cache.memory_size          |                                    | used query cache                                                                                                 
query_cache.evictions                | fce,queryCacheEvictions            | query cache evictions                                                                                            
pri.query_cache.evictions            |                                    | query cache evictions                                                                                            
request_cache.memory_size            | qcm,queryCacheMemory               | used request cache                                                                                               
pri.request_cache.memory_size        |                                    | used request cache
```
</description><key id="140811260">17101</key><summary>queryCacheMemory is a shorthand for 2 different values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>:CAT API</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-03-14T22:09:53Z</created><updated>2016-03-22T14:37:21Z</updated><resolved>2016-03-22T14:37:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove the ability to register custom suggesters and custom highlighters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17100</link><project id="" key="" /><description>We discussed briefly whether to remove the ability to register custom suggesters and custom highlighters via `SearchModule#registerSuggester` and `SearchModule#registerHighlighter`.  This issue is to open the floor for discussion and capture the reasoning behind whatever decision is made.

This discussion came about through the search refactoring effort and attempting to clean up unused parts of the code base.  We will need feedback from the user community as well to gauge to what extent the feature is used.
</description><key id="140797318">17100</key><summary>Remove the ability to register custom suggesters and custom highlighters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Suggesters</label><label>breaking</label><label>discuss</label></labels><created>2016-03-14T21:04:29Z</created><updated>2017-01-10T12:06:42Z</updated><resolved>2017-01-10T12:06:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-14T21:12:53Z" id="196523809">This is a project that uses custom suggesters: https://github.com/meltwater/es-prefix-suggester
</comment><comment author="abeyad" created="2016-03-14T21:13:18Z" id="196523962">@s1monw @clintongormley ^^ FYI
</comment><comment author="nik9000" created="2016-03-14T21:56:14Z" id="196538065">&gt; SearchModule#registerHighlighter

And [this](https://github.com/wikimedia/search-highlighter) is a custom highlighter. No one has gotten it working with 2.x yet, but it sees fairly heavy use.
</comment><comment author="sikoried" created="2016-03-29T18:08:35Z" id="203031268">I think having custom highlighters is a great feature.  Along with custom Tokenizers and Analyzers, I'm using this functionality to support a proprietary data format which needs to undergo a fairly sophisticated highlighting process before it can be rendered to the client.

Basically, the trio of customized Tokenizer, Analyzer and Highlighter allows to fully support custom data fields.
</comment><comment author="abeyad" created="2016-03-29T18:16:39Z" id="203034309">@sikoried Thank you for your feedback!
</comment><comment author="javanna" created="2016-03-30T07:08:30Z" id="203283042">I am not familiar with custom suggester implementations, but I am sure there are other custom highlighter implementations out there. I think we should definitely keep that extension point.
</comment><comment author="irothschild" created="2016-12-29T20:03:02Z" id="269685405">Has support for custom highlighters been removed in ES 5? 

On the breaking changes page https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_search_changes.html it says

```
Removed support for multiple highlighter names, the only supported ones are: 
plain, fvh and postings.
```

Maybe (hopefully) it's just poorly worded.</comment><comment author="nik9000" created="2016-12-29T21:04:44Z" id="269693328">Custom highlighters are still supported. That change is about removing the
alternative names for the built in highlighters.

On Thu, Dec 29, 2016, 3:03 PM Ivo Rothschild &lt;notifications@github.com&gt;
wrote:

&gt; Has support for custom highlighters been removed in ES 5?
&gt;
&gt; Here
&gt; https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking_50_search_changes.html
&gt; it says
&gt;
&gt; Removed support for multiple highlighter names, the only supported ones are: plain, fvh and postings.
&gt;
&gt; &#8212;
&gt; You are receiving this because you commented.
&gt;
&gt; Reply to this email directly, view it on GitHub
&gt; &lt;https://github.com/elastic/elasticsearch/issues/17100#issuecomment-269685405&gt;,
&gt; or mute the thread
&gt; &lt;https://github.com/notifications/unsubscribe-auth/AANLogBv0wnzrGLbAqPPzkdqOtluBaNZks5rNBH5gaJpZM4HweMw&gt;
&gt; .
&gt;
</comment><comment author="clintongormley" created="2017-01-10T12:06:42Z" id="271559224">Looks like we should keep custom highlighters and suggesters.  Closing</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Newline missing from console output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17099</link><project id="" key="" /><description>After executing a curl command, given output missing a newline character.  This keeps command line prompt on the same line as output.  Both aesthetically annoying and affects command line history being cut off and broken.

&lt;img width="1234" alt="screen shot 2016-03-14 at 2 12 23 pm" src="https://cloud.githubusercontent.com/assets/9092131/13758436/198e37d8-e9ef-11e5-99c0-0dca30e4f8e9.png"&gt;

Command line prompt is default config.
System: OSX El Capitan
</description><key id="140785561">17099</key><summary>Newline missing from console output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lolzoloz9</reporter><labels /><created>2016-03-14T20:16:24Z</created><updated>2016-03-14T20:18:03Z</updated><resolved>2016-03-14T20:18:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-14T20:18:03Z" id="196503240">Newline is added when you add ?pretty
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regexp Query Cannot Accept Escaped '+'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17098</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_72-internal

**OS version**: debian-jessie

**Description of the problem including expected versus actual behavior**:

I have a lot of data in E.164 format in a field called data.cid_info.e164. These are internationally standardized phone numbers. I want to find all phone numbers that start with +44 (UK prefix). As this is a string field, a regexp query seems sufficient. So I issue a search as follows:

``` json
{
  "query": {
    "regexp": {
      "data.cid_info.e164": "^+44.*"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 10
}
```

I expect 343 hits for today (will explain how I know this in a second) but I get 0 instead.

Oh, but you say, + is not an allowed character and must be escaped. Sure, so let's try that using the docs. It states that to escape a special character, precede it with a `\`.

``` json
{
  "query": {
    "regexp": {
      "data.cid_info.e164": "^\+44.*"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 10
}
```

Sense immediately throws an exception, but let's ignore sense's exception for a second and carry on.

``` json
{
   "error": {
      "root_cause": [
         {
            "type": "query_parsing_exception",
            "reason": "Failed to parse",
            "index": "logstash-2016.03.14"
         }
      ],
      "type": "search_phase_execution_exception",
      "reason": "all shards failed",
      "phase": "query",
      "grouped": true,
      "failed_shards": [
         {
            "shard": 0,
            "index": "logstash-2016.03.14",
            "node": "YOEgLqVGSbGgIgbXJJqSNg",
            "reason": {
               "type": "query_parsing_exception",
               "reason": "Failed to parse",
               "index": "logstash-2016.03.14",
               "caused_by": {
                  "type": "json_parse_exception",
                  "reason": "Unrecognized character escape '+' (code 43)\n at [Source: [B@1dfa7073; line: 4, column: 33]"
               }
            }
         }
      ]
   },
   "status": 400
}
```

Now, because I know my data, and I know it conforms to the E.164 format for the field, I know I can safely suggest that the + is optional, and it will return me the right counts (good for troubleshooting this bug, not a good general practice).

``` json
{
  "query": {
    "regexp": {
      "data.cid_info.e164": "^+?44.*"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 0
}
```

Resulting in:

```
{
  "_shards": {
    "failed": 0,
    "successful": 2,
    "total": 2
  },
  "hits": {
    "hits": [],
    "max_score": 0.0,
    "total": 343
  },
  "timed_out": false,
  "took": 2
}
```

Hey, it's the right number!

I believe this is due to the [JSON spec](http://www.json.org/) not allowing escaped characters that aren't control characters.

&gt; char
&gt; any-Unicode-character-
&gt;     except-"-or--or-
&gt;     control-character
&gt; \"
&gt; \
&gt; \/
&gt; \b
&gt; \f
&gt; \n
&gt; \r
&gt; \t
&gt; \u four-hex-digits

So I'm not sure that there is a way to do a web request with an escaped character in the JSON body.

Note that escaping the \ doesn't work either, as that returns 0 results `"^\\+44.*"` for reference. 

**Steps to reproduce**:
1. Issue a regex query with an escaped `+` sign (data which has a literal plus in it), see examples above
2. Note that no results were returned
</description><key id="140771672">17098</key><summary>Regexp Query Cannot Accept Escaped '+'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tebriel</reporter><labels /><created>2016-03-14T19:14:59Z</created><updated>2016-03-15T18:14:19Z</updated><resolved>2016-03-15T17:20:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-14T19:23:13Z" id="196484927">&gt; Note that escaping the \ doesn't work either, as that returns 0 results "^\+44.*" for reference.

Could you try `^[+]44.*`? That'll at least be a work around if it works.

A couple of other thoughts:
1. A prefix query should also work.
2. I'd normalize the `+` out of the phone number on the way in.
3. You will get better performance if you extract the area code to its own field. A prefix or a regex query has to walk through lots of terms but if you extract the country code to a field it'll be a single term lookup. Depending on your use case you might also want to extract stuff like npa for NANPA numbers as well.
</comment><comment author="tebriel" created="2016-03-14T19:39:31Z" id="196490006">@nik9000:

### Character Class

``` json
{
  "query": {
    "regexp": {
      "data.cid_info.e164": "^[+]44.*"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 0
}
```

0 hits still

### Prefix Query

``` json
{
  "query": {
    "prefix": {
      "data.cid_info.e164": "+44"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 10
}
```

Oddly also 0 hits.

```
{
  "_shards": {
    "failed": 0,
    "successful": 2,
    "total": 2
  },
  "hits": {
    "hits": [],
    "max_score": null,
    "total": 0
  },
  "timed_out": false,
  "took": 1
}
```

### Reworking of how data is stored

Yeah, totally get that. I could go back and reindex the 5b docs, but that's not really feasible right now. I may add a logstash pattern to split up and extract things like NPA/NXX from the e164 data.

Thanks for your help.
</comment><comment author="tebriel" created="2016-03-14T19:53:49Z" id="196495937">### Example Query + Docs

I thought this might be helpful too. Here's a query that gets back a few documents and the resulting docs.

I also looked and the e164 string field is analyzed, and has no non-standard analyzers or tokenizers on it.

```
{
  "query": {
    "regexp": {
      "data.cid_info.e164": "^+?44.*"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 5
}
```

```
{
  "_shards": {
    "failed": 0,
    "successful": 2,
    "total": 2
  },
  "hits": {
    "hits": [
      {
        "_id": "AVNzLldsnx4nT5Yl6NJi",
        "_index": "logstash-2016.03.14",
        "_score": 1.0,
        "_type": "api",
        "fields": {
          "data.cid_info.e164": [
            "+441202629580"
          ]
        }
      },
      {
        "_id": "AVNzNVhNYwYqX587IL4a",
        "_index": "logstash-2016.03.14",
        "_score": 1.0,
        "_type": "api",
        "fields": {
          "data.cid_info.e164": [
            "+441908410653"
          ]
        }
      },
      {
        "_id": "AVNzQAZTYwYqX587IR9q",
        "_index": "logstash-2016.03.14",
        "_score": 1.0,
        "_type": "api",
        "fields": {
          "data.cid_info.e164": [
            "+441202629580"
          ]
        }
      },
      {
        "_id": "AVNzTiLGnx4nT5Yl6tYt",
        "_index": "logstash-2016.03.14",
        "_score": 1.0,
        "_type": "api",
        "fields": {
          "data.cid_info.e164": [
            "+441908410653"
          ]
        }
      },
      {
        "_id": "AVNzVuqOYwYqX587Icxb",
        "_index": "logstash-2016.03.14",
        "_score": 1.0,
        "_type": "api",
        "fields": {
          "data.cid_info.e164": [
            "+441908410653"
          ]
        }
      }
    ],
    "max_score": 1.0,
    "total": 357
  },
  "timed_out": false,
  "took": 3
}
```
</comment><comment author="jimczi" created="2016-03-15T15:16:00Z" id="196870843">&gt; I also looked and the e164 string field is analyzed, and has no non-standard analyzers or tokenizers on it

If your field is analyzed then there is a big chance that the tokenizer removes the '+' automatically. I tried your example on a non-analyzed string field and it works as expected (for regexp and prefix queries). The fact that the prefix query does not work on your deployment is also a good indicator that something is wrong in your mapping. Are you using the default/standard analyzer ? For a phone number I would use a non-analyzed string field, why would you need an analyzer for this field? 
</comment><comment author="tebriel" created="2016-03-15T15:22:50Z" id="196874693">@jimferenczi If the tokenizer removed the + automatically, then wouldn't `"^44.*"`return results? As for why it's analyzed, it shouldn't be, you're right, but reindexing all the docs right now isn't feasible. It's added to the list of tech debt for this project. It's an auto-created field, so it took on whatever analyzer and tokenizer defaults there were.
</comment><comment author="jimczi" created="2016-03-15T15:34:48Z" id="196881595">@tebriel sorry I forgot to explain how I transformed your query into a "valid" Lucene pattern. As explained here https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html#_standard_operators Lucene&#8217;s patterns are always anchored. Thus you do not need to use "^". 
</comment><comment author="tebriel" created="2016-03-15T15:38:49Z" id="196884889">@jimferenczi Oh I see. Thanks, but it's still necessary for a regexp query, right? I mean, my issues aside (I have a workaround) the bigger issue here is I don't think you can issue a valid regexp query using what is documented for Elasticsearch.
</comment><comment author="jimczi" created="2016-03-15T15:50:34Z" id="196890981">Sorry I don't understand what you mean by "but it's still necessary for a regexp query". 
The character "^" is not reserved so if you use it in a regexp query it will be considered as a character to match in the string.
"44.*" =&gt; starts with 44 followed by any character sequence.
".*44" =&gt; starts with any character sequence and ends with 44.
"^44.*" =&gt; starts with the character "^" followed by 44 and ends with any character sequence (match the exact string "^44787" and no match for "44787")
</comment><comment author="tebriel" created="2016-03-15T15:56:09Z" id="196893852">@jimferenczi Oh yes, duh. Sorry, I read the docs wrong. You do not need the anchor, you are correct.

But the docs (hopefully I'm reading right this time) state that you must escape `. ? + * | { } [ ] ( ) " \` as they are "reserved characters" and json does not allow you to escape `+`. So that's still a problem, regardless of how my document is indexed, yes?
</comment><comment author="jimczi" created="2016-03-15T17:20:28Z" id="196932694">No it's not. You can "escape" the character '+' but it has nothing to do with json. I tried the regexp query "+44.*" on a non analyzed string and it works as expected. I am going to close the issue as solved. Feel free to reopen if I missed anyhting.
</comment><comment author="tebriel" created="2016-03-15T17:24:40Z" id="196934608">@jimferenczi Am I hold it it wrong? Can you issue a successful query escaping the `+`?

Issuing this query returns an exception for me:

```
{
  "query": {
    "regexp": {
      "data.cid_info.e164": "\+44.*"
    }
  },
  "fields": ["data.cid_info.e164"],
  "size": 10
}
```
</comment><comment author="jimczi" created="2016-03-15T17:58:00Z" id="196949140">You can try with double escaping like this "\\+44.*" or surrounded with double quotes: "\"+44\".*" which has the same meaning.
</comment><comment author="tebriel" created="2016-03-15T18:14:19Z" id="196956251">@jimferenczi Okay, thanks for all your help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bogus analyzer fails with a NPE in the _analyzer API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17097</link><project id="" key="" /><description>**Elasticsearch version**: 2.2

**JVM version**: 1.8u66

**OS version**: Win 8.1 

**Description of the problem including expected versus actual behavior**:

When specifying a bogus analyzer (maybe a typo), ES throws an internal NPE in the logs and the request blocks. Instead the exception should be thrown to the user.

**Steps to reproduce**:
1. 

```
curl -XPOST http://localhost:9200/_analyze -d '{
  "analyzer" : "bogus",
  "text" : "this is a test"
}'
```

**Provide logs (if relevant)**:

```
[2016-03-14 18:17:44,393][ERROR][transport                ] [Caleb Alexander] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@a566c0]
java.lang.NullPointerException
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
    at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
    at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="140723076">17097</key><summary>Bogus analyzer fails with a NPE in the _analyzer API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Analysis</label><label>:REST</label><label>bug</label></labels><created>2016-03-14T16:21:49Z</created><updated>2016-03-15T14:44:15Z</updated><resolved>2016-03-14T16:36:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-14T16:34:04Z" id="196400705">@costin Is it the same as #15148 closed by #15447 in 2.3.0?
</comment><comment author="jasontedor" created="2016-03-14T16:36:51Z" id="196401611">Duplicates #15148, closed by #15447
</comment><comment author="costin" created="2016-03-14T17:20:02Z" id="196420378">looks about right - thanks for jumping on this guys. I just assumed 2.2 is recent enough to not check for duplicates :) Lazy, I know.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring of Suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17096</link><project id="" key="" /><description>This PR refactors all suggestion builders to be able to be parsed on the coordinating node and serialised as Objects to the shards. Specifically, all SuggestionBuilder implementations implement `NamedWritable` for serialization, a `fromXContent()` method that handles parsing from xContent and a `build()` method that is called on the shard to create the `SuggestionContext`. 

Relates to #10217
</description><key id="140705917">17096</key><summary>Refactoring of Suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-14T15:22:28Z</created><updated>2016-03-16T16:01:28Z</updated><resolved>2016-03-16T16:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-14T17:39:49Z" id="196428546">@s1monw if you have time, could you take a final look at this?
</comment><comment author="s1monw" created="2016-03-16T14:10:23Z" id="197348805">I think we are good here - I mean I can review it again but it will really only be bikeshedding. These changes have been reviewed before they went into the branch so I think we are more than good with merging it. Here is my formal LGTM to merge
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate template settings (add missing byte &amp; time units)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17095</link><project id="" key="" /><description>Fixes the issue described here
Closes https://github.com/elastic/elasticsearch/issues/17093
</description><key id="140705731">17095</key><summary>Migrate template settings (add missing byte &amp; time units)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mfussenegger</reporter><labels><label>:Core</label><label>bug</label><label>v2.3.0</label></labels><created>2016-03-14T15:21:37Z</created><updated>2016-03-15T17:47:56Z</updated><resolved>2016-03-15T11:00:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2016-03-14T17:33:40Z" id="196425758">Thanks @mfussenegger this change looks great, and it's nice you were able to factor out the common code.  I just left some minor feedback.
</comment><comment author="mfussenegger" created="2016-03-14T21:10:09Z" id="196523089">@mikemccand thanks for the review. I've added a fixup commit
</comment><comment author="mikemccand" created="2016-03-15T09:42:17Z" id="196743375">Thanks @mfussenegger looks great, I'll push to 2.x!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force merge should be cancellable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17094</link><project id="" key="" /><description>We have the infrastructure to make long running stuff cancellable. Force merge seems like something that you might want to cancel.

Cancel doesn't have to immediately cancel (can't/shouldn't kill threads in Java), just make a reasonably good effort to cancel the task.
</description><key id="140692406">17094</key><summary>Force merge should be cancellable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2016-03-14T14:39:19Z</created><updated>2017-06-07T06:50:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-14T14:45:46Z" id="196344817">how would that work? you can't just go and stop the merge once it's kicked off - I think having an API to do that is misleading and will be disappointing to users
</comment><comment author="nik9000" created="2016-03-14T14:55:36Z" id="196349922">&gt; how would that work? you can't just go and stop the merge once it's kicked off - I think having an API to do that is misleading and will be disappointing to users

That is why I made this a discussion! I haven't read much of that code but it feels like an "obvious" use for cancellable tasks. If it is hard to impossible at least we'll have this issue we can point people to when they ask for it.
</comment><comment author="pickypg" created="2016-03-14T15:06:34Z" id="196358068">Perhaps it can short circuit? Since force merge is actually a series of merges in the background, maybe it can just stop in the middle _if_ possible? Then perhaps we can augment the API to state where it is so that it can succeed / fail?
</comment><comment author="kuipertan" created="2016-07-07T10:37:02Z" id="231043198">I need this api.

Once start it ,  it takes long time to finish the merging work.
</comment><comment author="geekpete" created="2017-06-07T06:50:03Z" id="306703280">So it could finish the current segment merges but not start any new segment merges?
When segments are merged, the new ones are written out and when confirmed as completed, the old ones are then removed. This is why the disk usage grows then shrinks again. So a way to cancel the current/in-progress merges and reinstate the existing segments that were about to be thrown away seems like what you'd want it to do?

And managing it with the tasks api would be nice.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>byte / time values without units in templates cause Guice errors on ES 2.X upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17093</link><project id="" key="" /><description>It seems the setting upgrade code is only run on index settings but not on templates.

If a template is created pre 2.0 which contains byte or time values without units and an index is created in &gt;= 2.0 it will create an index with invalid values causing contructors fail due to settings parse failure.

Steps to reproduce: Create template with 1.7

```
http://localhost:9200
POST /_template/t1
{
    "template": "te*",
    "settings": {
        "index": {
            "translog": {
                "interval": "8000"
            }
        }
    }
}
```

Stop 1.7 and start 2.1

Create new index:

```
http://localhost:9200
POST /test/default/1
{
    "x": "y"
}
```

This results in errors like this:

```
[test][[test][4]] ElasticsearchException[failed to create shard]; nested: ElasticsearchParseException[Failed to parse setting [index.translog.interval] with value [8000] as a time value: unit is missing or unrecognized];
    at org.elasticsearch.index.IndexService.createShard(IndexService.java:372)
```

The [migration checker](https://github.com/elastic/elasticsearch-migration) also doesn't detect this.
</description><key id="140664051">17093</key><summary>byte / time values without units in templates cause Guice errors on ES 2.X upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mfussenegger</reporter><labels /><created>2016-03-14T12:46:30Z</created><updated>2016-03-15T11:01:11Z</updated><resolved>2016-03-15T11:01:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mfussenegger" created="2016-03-14T14:51:48Z" id="196348426">I made a PR against our fork and can also make a PR against this repo if you'd like me to. Just let me know (and also tell me against which branch I should make that PR)
</comment><comment author="s1monw" created="2016-03-14T14:56:10Z" id="196350217">&gt; I made a PR against our fork and can also make a PR against this repo if you'd like me to. Just let me know (and also tell me against which branch I should make that PR)

please do! and this should be against 2.x
</comment><comment author="mikemccand" created="2016-03-15T11:01:10Z" id="196765974">Fixed with https://github.com/elastic/elasticsearch/pull/17095
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IllegalArgumentException[mapper [cells] of different type, current_type [string], merged_type [long]]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17092</link><project id="" key="" /><description>at org.elasticsearch.index.mapper.DocumentParser.innerParseDocument(DocumentParser.java:163)
    at org.elasticsearch.index.mapper.DocumentParser.parseDocument(DocumentParser.java:79)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:304)
    at org.elasticsearch.index.shard.IndexShard.prepareIndex(IndexShard.java:547)
    at org.elasticsearch.index.shard.IndexShard.prepareIndexOnPrimary(IndexShard.java:529)
    at org.elasticsearch.action.index.TransportIndexAction.prepareIndexOperationOnPrimary(TransportIndexAction.java:211)
    at org.elasticsearch.action.index.TransportIndexAction.executeIndexRequestOnPrimary(TransportIndexAction.java:223)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:157)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:65)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryPhase.doRun(TransportReplicationAction.java:595)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:263)
    at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryOperationTransportHandler.messageReceived(TransportReplicationAction.java:260)
    at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

JSON:
{"key":"46c0a372-cd9f-48c1-9afe-165e5ce5af4d","cells":[["2015-04-03 16\:32\:18:","",1428050312047000],["2015-04-03 16\:32\:18:business","wk",1428050312047000],["2015-04-03 16\:32\:18:caller","smp",1428050312047000],["2015-04-03 16\:32\:18:contextid","63CE89409EE6448555832BC08496B204-n2.work1",1428050312047000],["2015-04-03 16\:32\:18:customenv","{\"ip\":\"49.83.17.189\"}",1428050312047000],["2015-04-03 16\:32\:18:data_info","{\"transaction\":{\"merchant\":{\"mid\":\"888002189990019\"},\"orderhead\":{\"orderId\":\"150403A00005196\",\"orderType\":\"0005\",\"orderCount\":1,\"orderAmount\":200.00,\"transAmount\":200.00,\"stat\":\"00\",\"currency\":\"156\"},\"orderdetail\":{\"pay\":{\"payId\":\"150403P00005132\",\"payKid\":\"02\",\"payTid\":\"A04\",\"payOrgid\":\"B0000001\",\"payAmount\":200.00,\"payDisAmount\":0.00,\"payFee\":0.00,\"currency\":\"156\",\"medium\":\"8188310700233874\",\"stat\":\"P0\"},\"rec\":{\"recId\":\"150403E0002821\",\"accountNo\":\"8867200950010700\",\"accountType\":\"01\",\"recAmount\":200.00,\"recDisAmount\":0.00,\"recFee\":0.00,\"currency\":\"156\",\"stat\":\"C0\"}}}}",1428050312047000],["2015-04-03 16\:32\:18:eventtype","0002",1428050312047000],["2015-04-03 16\:32\:18:exchange","asynchronous",1428050312047000],["2015-04-03 16\:32\:18:format","json",1428050312047000],["2015-04-03 16\:32\:18:method","com.sand.risk.service.smp.transanalysis.TransAnalysis",1428050312047000],["2015-04-03 16\:32\:18:stats","1",1428050312047000],["2015-04-03 16\:32\:18:terminal_info","data-not-found",1428050312047000],["2015-04-03 16\:32\:18:terminal_type","web",1428050312047000],["2015-04-03 16\:32\:18:user","{\"id\":\"61546\",\"name\":\"13851126055\",\"phone\":\"13851126055\",\"email\":\"wbww888@163.com\"}",1428050312047000],["2015-04-03 16\:32\:18:utime","2015-04-03 16:37:47",1428050312047000],["2015-04-03 16\:32\:18:version","2.0.0",1428050312047000]]}

My json format is correct, why is there this exception ?
</description><key id="140618152">17092</key><summary>IllegalArgumentException[mapper [cells] of different type, current_type [string], merged_type [long]]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">soenter</reporter><labels /><created>2016-03-14T09:22:44Z</created><updated>2016-11-11T04:33:15Z</updated><resolved>2016-03-14T09:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-14T09:31:45Z" id="196224361">Sounds like `cells` has incorrect mapping. Was an array of `String` but you are now passing also a long value `1428050312047000`...

But please join us on discuss.elastic.co if you have questions like this.
This space is only for confirmed issues.
</comment><comment author="7leafcom" created="2016-11-11T04:33:07Z" id="259879067">I ran into this issue with dynamic mapping on version 2.4.1. The data is pretty much the same, but it works on one index and not on the other. Very strange. I'm trying to narrow it down.

http://stackoverflow.com/questions/40541125/inconsistent-mapperparsingexception-on-elasticsearch-2-4-1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check that _value is used in aggregations script before setting value to specialValue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17091</link><project id="" key="" /><description>FIx #14262
</description><key id="140614860">17091</key><summary>Check that _value is used in aggregations script before setting value to specialValue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">alexshadow007</reporter><labels><label>:Expressions</label><label>:Scripting</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-14T09:07:17Z</created><updated>2016-03-15T17:28:12Z</updated><resolved>2016-03-14T20:22:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-14T12:05:06Z" id="196279924">@jdconrad could you review this please?
</comment><comment author="jdconrad" created="2016-03-14T16:41:54Z" id="196404025">@alexshadow007 Thanks for looking into this, and I'm fine with this fix, but do you know why setNextVar is being called at all with _value if it's not part of a script?  If it's possible it may be a better fix in that area of the code to not call setNextVar when it's not necessary.  This seems like a bit of a workaround for some unexpected behavior to me.
</comment><comment author="rjernst" created="2016-03-14T19:07:01Z" id="196477242">I think I see now how this can happen. We init specialValue to null in ExpressionScriptEngineService. If the script does not use _value, then that null is passed through, but setNextVar is always called. An alternative might be to always init specialValue. However, I'm fine with the change as is, but would add a comment to the test to make clear this second case is being tested at the same time (eg something like `// make sure agg that does not use _value works too`).
</comment><comment author="jdconrad" created="2016-03-14T20:23:03Z" id="196506699">@alexshadow007 This has been pushed to master.  Thanks for the contribution.
</comment><comment author="jdconrad" created="2016-03-15T17:28:12Z" id="196936448">Also back ported to 2.3 now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch 2.2 in docker,exception caught on transport layer , Message not fully read (request) for requestId</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17090</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0 

**JVM version**:1.8.0_45

**OS version**:Linux gentoo  x86_64

**Description of the problem including expected versus actual behavior**:
setup a single elasticsearch node (not cluster) in the Docker, get the warning logs : exception caught on transport layer [[id: 0x55b18076, /172.17.42.1:57474 =&gt; /172.17.0.127:9300]], closing connection
java.lang.IllegalStateException: Message not fully read (request) for requestId [1191646], action [cluster/nodes/info], readerIndex [39] vs expected [57]; resetting
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:120)

172.17.0.127  is the ip address of the current docker
b9ab5f0193cc elasticsearch # ifconfig
eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 172.17.0.127  netmask 255.255.0.0  broadcast 0.0.0.0
        ether 02:42:ac:11:00:7f  txqueuelen 0  (Ethernet)
        RX packets 1604  bytes 115647 (112.9 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 1582  bytes 108924 (106.3 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
172.17.42.1 is the ip address of the docker 0
docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
        inet 172.17.42.1  netmask 255.255.0.0  broadcast 0.0.0.0
        ether 12:2e:2a:40:37:c6  txqueuelen 0  (Ethernet)
        RX packets 72955032  bytes 5384429681 (5.0 GiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 73722264  bytes 13089516894 (12.1 GiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
**Steps to reproduce**:
1. setup docker with command:
   docker run -it -d -e LANG=en_US.utf8 -p 9300:9300 -p 9200:9200 --name elastic xx:xx /bin/bash
   the image is the clean gentoo image only with jdk
2. config the elasticsearch.yml : 
   network.host: 0.0.0.0
   script.inline: on
   script.indexed: on
3. start up elasticsearch with command: 
   ./bin/elasticsearch -Des.insecure.allow.root=true

**Provide logs (if relevant)**:
b9ab5f0193cc elasticsearch # ./bin/elasticsearch -Des.insecure.allow.root=true
[2016-03-14 15:04:30,491][WARN ][bootstrap                ] running as ROOT user. this is a bad idea!
[2016-03-14 15:04:30,660][INFO ][node                     ] [White Rabbit] version[2.2.0], pid[478], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-03-14 15:04:30,660][INFO ][node                     ] [White Rabbit] initializing ...
[2016-03-14 15:04:31,016][INFO ][plugins                  ] [White Rabbit] modules [lang-expression, lang-groovy], plugins [head, analysis-smartcn], sites [head]
[2016-03-14 15:04:31,027][INFO ][env                      ] [White Rabbit] using [1] data paths, mounts [[/opt/elasticsearch-2.2.0/data (/dev/sda2)]], net usable_space [24gb], net total_space [39.2gb], spins? [possibly], types [ext4]
[2016-03-14 15:04:31,027][INFO ][env                      ] [White Rabbit] heap size [989.8mb], compressed ordinary object pointers [true]
[2016-03-14 15:04:32,027][INFO ][node                     ] [White Rabbit] initialized
[2016-03-14 15:04:32,027][INFO ][node                     ] [White Rabbit] starting ...
[2016-03-14 15:04:32,070][INFO ][transport                ] [White Rabbit] publish_address {172.17.0.127:9300}, bound_addresses {0.0.0.0:9300}
[2016-03-14 15:04:32,077][INFO ][discovery                ] [White Rabbit] elasticsearch/sJFE90EHQPGp9xsFH39CyQ
[2016-03-14 15:04:34,985][WARN ][transport.netty          ] [White Rabbit] exception caught on transport layer [[id: 0x55b18076, /172.17.42.1:57474 =&gt; /172.17.0.127:9300]], closing connection
java.lang.IllegalStateException: Message not fully read (request) for requestId [1191646], action [cluster/nodes/info], readerIndex [39] vs expected [57]; resetting
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:120)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2016-03-14 15:04:35,119][INFO ][cluster.service          ] [White Rabbit] new_master {White Rabbit}{sJFE90EHQPGp9xsFH39CyQ}{172.17.0.127}{172.17.0.127:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-03-14 15:04:35,130][INFO ][http                     ] [White Rabbit] publish_address {172.17.0.127:9200}, bound_addresses {0.0.0.0:9200}
[2016-03-14 15:04:35,130][INFO ][node                     ] [White Rabbit] started
[2016-03-14 15:04:35,208][INFO ][gateway                  ] [White Rabbit] recovered [0] indices into cluster_state
[2016-03-14 15:04:36,118][WARN ][transport.netty          ] [White Rabbit] exception caught on transport layer [[id: 0x5b154af7, /172.17.42.1:57486 =&gt; /172.17.0.127:9300]], closing connection
java.lang.IllegalStateException: Message not fully read (request) for requestId [1191272], action [cluster/nodes/info], readerIndex [39] vs expected [57]; resetting
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:120)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:75)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;
</description><key id="140596403">17090</key><summary>elasticsearch 2.2 in docker,exception caught on transport layer , Message not fully read (request) for requestId</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ukouryou</reporter><labels><label>:Java API</label><label>discuss</label></labels><created>2016-03-14T07:26:06Z</created><updated>2016-03-18T03:22:49Z</updated><resolved>2016-03-18T03:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-14T12:02:20Z" id="196278908">Hi @ukouryou 

It looks like you have mixed versions of Elasticsearch running.  Perhaps a 1.x client trying to talk to your 2.x cluser?
</comment><comment author="ukouryou" created="2016-03-15T02:24:29Z" id="196619775">@clintongormley thanks for your replay, I get the log when the elasticsearch server start up,I didn't talk to server with client
</comment><comment author="clintongormley" created="2016-03-15T17:33:43Z" id="196938985">@ukouryou it looks like you DO have a client or node hanging around somewhere.  That message indicates that some other node/client tried to connect to this node and sent a malformed request.  Likely it is malformed because it has a different major version.
</comment><comment author="ukouryou" created="2016-03-16T02:46:43Z" id="197124437">@clintongormley I have scaned the whole intranet, no other node exist.
I found that if I removed   network.host: 0.0.0.0 in the elasticsearch.yml,it works ok

&gt; 1be54772214b elasticsearch # ./bin/elasticsearch -Des.insecure.allow.root=true 
&gt; [2016-03-16 10:34:51,492][WARN ][bootstrap                ] running as ROOT user. this is a bad idea!
&gt; [2016-03-16 10:34:51,650][INFO ][node                     ] [Amphibian] version[2.2.0], pid[407], build[8ff36d1/2016-01-27T13:32:39Z]
&gt; [2016-03-16 10:34:51,650][INFO ][node                     ] [Amphibian] initializing ...
&gt; [2016-03-16 10:34:52,001][INFO ][plugins                  ] [Amphibian] modules [lang-expression, lang-groovy], plugins [head, analysis-smartcn], sites [head]
&gt; [2016-03-16 10:34:52,012][INFO ][env                      ] [Amphibian] using [1] data paths, mounts [[/opt/elasticsearch-2.2.0/data (/dev/sda2)]], net usable_space [23.7gb], net total_space [39.2gb], spins? [possibly], types [ext4]
&gt; [2016-03-16 10:34:52,012][INFO ][env                      ] [Amphibian] heap size [989.8mb], compressed ordinary object pointers [true]
&gt; [2016-03-16 10:34:52,980][INFO ][node                     ] [Amphibian] initialized
&gt; [2016-03-16 10:34:52,980][INFO ][node                     ] [Amphibian] starting ...
&gt; [2016-03-16 10:34:53,027][INFO ][transport                ] [Amphibian] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}
&gt; [2016-03-16 10:34:53,032][INFO ][discovery                ] [Amphibian] elasticsearch/ndsNzXynSQCfhbty1ePJag
&gt; [2016-03-16 10:34:56,047][INFO ][cluster.service          ] [Amphibian] new_master {Amphibian}{ndsNzXynSQCfhbty1ePJag}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
&gt; [2016-03-16 10:34:56,057][INFO ][http                     ] [Amphibian] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}
&gt; [2016-03-16 10:34:56,057][INFO ][node                     ] [Amphibian] started
&gt; [2016-03-16 10:34:56,134][INFO ][gateway                  ] [Amphibian] recovered [0] indices into cluster_state

but I want to access it outside of the docker, any other suggestions?
</comment><comment author="clintongormley" created="2016-03-16T10:02:42Z" id="197239204">@bleskes any idea what might be going on here?
</comment><comment author="bleskes" created="2016-03-16T11:29:04Z" id="197274968">I too strongly suspect there is an old 1.x client sending info requests to the cluster (we used those in 1.x to check that a node is alive).

Can you double check the source ip, 172.17.42.1 ?

Note that we you don't bound to 0.0.0.0 the ES node is not accepting any request from the outside and thus can't be reached by that client.
</comment><comment author="ukouryou" created="2016-03-17T03:05:02Z" id="197670955">@bleskes I have a  physical server ,run the ifconfig command , the output as follows

&gt;  ifconfig
&gt; docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
&gt;         inet 172.17.42.1  netmask 255.255.0.0  broadcast 0.0.0.0
&gt;         ether 0a:ea:37:dc:d9:29  txqueuelen 0  (Ethernet)
&gt;         RX packets 75912870  bytes 5869903197 (5.4 GiB)
&gt;         RX errors 0  dropped 0  overruns 0  frame 0
&gt;         TX packets 76830962  bytes 13987928119 (13.0 GiB)
&gt;         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
&gt; 
&gt; eno1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
&gt;         inet 192.168.5.201  netmask 255.255.255.0  broadcast 192.168.5.255
&gt;         ether 34:17:eb:c2:3d:cc  txqueuelen 1000  (Ethernet)
&gt;         RX packets 16581382  bytes 9437015715 (8.7 GiB)
&gt;         RX errors 0  dropped 3874435  overruns 0  frame 0
&gt;         TX packets 9692374  bytes 5086146362 (4.7 GiB)
&gt;         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
&gt;         device interrupt 20  memory 0xf7100000-f7120000  
&gt; 
&gt; lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
&gt;         inet 127.0.0.1  netmask 255.0.0.0
&gt;         loop  txqueuelen 0  (Local Loopback)
&gt;         RX packets 982  bytes 314975 (307.5 KiB)
&gt;         RX errors 0  dropped 0  overruns 0  frame 0
&gt;         TX packets 982  bytes 314975 (307.5 KiB)
&gt;         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

I have installed docker on this physical server, and also a docker container name **elasticsearch** on this server, I got the issue in the container  **elasticserach** ,  172.17.42.1 is ther ip address of the docker0, did I make it clear?

I also found that It works ok with these configurations outside of the docker container 

&gt; andy@andy:~/work/elasticsearch-2.2.0$ ./bin/elasticsearch -Dnetwork.host=0.0.0.0
&gt; [2016-03-17 10:52:12,591][INFO ][node                     ] [Maestro] version[2.2.0], pid[6771], build[8ff36d1/2016-01-27T13:32:39Z]
&gt; [2016-03-17 10:52:12,592][INFO ][node                     ] [Maestro] initializing ...
&gt; [2016-03-17 10:52:12,903][INFO ][plugins                  ] [Maestro] modules [lang-expression, lang-groovy], plugins [head, analysis-smartcn], sites [head]
&gt; [2016-03-17 10:52:12,914][INFO ][env                      ] [Maestro] using [1] data paths, mounts [[/ (/dev/sda2)]], net usable_space [833.6gb], net total_space [908.5gb], spins? [possibly], types [ext4]
&gt; [2016-03-17 10:52:12,915][INFO ][env                      ] [Maestro] heap size [989.8mb], compressed ordinary object pointers [true]
&gt; [2016-03-17 10:52:13,931][INFO ][node                     ] [Maestro] initialized
&gt; [2016-03-17 10:52:13,931][INFO ][node                     ] [Maestro] starting ...
&gt; [2016-03-17 10:52:13,974][INFO ][transport                ] [Maestro] publish_address {172.17.0.1:9300}, bound_addresses {[::]:9300}
&gt; [2016-03-17 10:52:13,980][INFO ][discovery                ] [Maestro] elasticsearch/-3ILE2hWR3WkR3icKpYqVA
&gt; [2016-03-17 10:52:17,026][INFO ][cluster.service          ] [Maestro] new_master {Maestro}{-3ILE2hWR3WkR3icKpYqVA}{172.17.0.1}{172.17.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
&gt; [2016-03-17 10:52:17,036][INFO ][http                     ] [Maestro] publish_address {172.17.0.1:9200}, bound_addresses {[::]:9200}
&gt; [2016-03-17 10:52:17,036][INFO ][node                     ] [Maestro] started
&gt; [2016-03-17 10:52:17,112][INFO ][gateway                  ] [Maestro] recovered [0] indices into cluster_state

Is this a clue?
</comment><comment author="bleskes" created="2016-03-17T07:57:49Z" id="197746366">I'm not a docker user so I might be wrong here, but  doesn't the -p 9300:9300 parameter mean that that port will accept connection from anywhere on our network (not only docker containers in the same network)? The symptoms you describe match and old client trying to connect. I can't tell where it comes from, nor do I see anything that will point in another direction. Sorry, but my I advice is to continue digging in that direction. Let us know if you have any new information.
</comment><comment author="ukouryou" created="2016-03-18T03:13:19Z" id="198181677">@bleskes @clintongormley you are right, I got the old client in an unused docker container,it works fine after stopped that container,thanks for your help 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es2.1.0 keep throw failed to execute ttl exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17089</link><project id="" key="" /><description>Elasticsearch version: 2.1.0
JVM version:8u40
OS version: linux 2.6

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.create mapping below:
  "op_logs_2016-02-29": {
    "mappings": {
      "_default_": {
        "_ttl": {
          "default": 2592000000,
          "enabled": true
        },
        "_all": {
          "enabled": false
        },
        "properties": {
          "domainType": {
            "index": "not_analyzed",
            "type": "string"
          },
          "venderId": {
            "index": "not_analyzed",
            "type": "string"
          },
          "invokeClassName": {
            "index": "not_analyzed",
            "type": "string"
          },
          "uploadTime": {
            "format": "yyyy-MM-dd HH:mm:ss",
            "type": "date"
          },
          "operatorName": {
            "index": "not_analyzed",
            "type": "string"
          },
          "uuid": {
            "index": "not_analyzed",
            "type": "string"
          },
          "createTime": {
            "format": "yyyy-MM-dd HH:mm:ss",
            "type": "date"
          },
          "clientSystemIP": {
            "index": "not_analyzed",
            "type": "string"
          },
          "operateDesc": {
            "analyzer": "ik",
            "type": "string"
          },
          "outputParamDesc": {
            "analyzer": "ik",
            "type": "string"
          },
          "clientSystemName": {
            "index": "not_analyzed",
            "type": "string"
          },
          "invokeMethodName": {
            "index": "not_analyzed",
            "type": "string"
          },
          "inputParamDesc": {
            "analyzer": "ik",
            "type": "string"
          }
        }
      }
}
}
 2.put some data

then log keep throw below:
[2016-03-14 13:36:56,626][WARN ][indices.ttl ] [172.20.130.67] failed to execute ttl purge:java.lang.NullPointerException
</description><key id="140591032">17089</key><summary>es2.1.0 keep throw failed to execute ttl exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels><label>:Mapping</label><label>feedback_needed</label></labels><created>2016-03-14T06:49:29Z</created><updated>2016-05-11T16:28:30Z</updated><resolved>2016-05-11T16:28:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-14T11:48:24Z" id="196276027">Hi @makeyang 

I've tried your recreation but it doesn't work for me.  Everything works as it should.  Could you provide more information, eg the stack trace that accompanies the NPE?  Also note that the TTL field is deprecated.
</comment><comment author="clintongormley" created="2016-05-11T16:28:30Z" id="218513785">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bootstrap does not set system properties</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17088</link><project id="" key="" /><description>Today, certain bootstrap properties are set and read via system
properties. This action-at-distance way of managing these properties is
rather confusing, and completely unnecessary. But another problem exists
with setting these as system properties. Namely, these system properties
are interpreted as Elasticsearch settings, not all of which are
registered. This leads to Elasticsearch failing to startup if any of
these special properties are set. Instead, these properties should be
kept as local as possible, and passed around as method parameters where
needed. This eliminates the action-at-distance way of handling these
properties, and eliminates the need to register these non-setting
properties. This commit does exactly that.

Additionally, today we use the "-D" command line flag to set the
properties, but this is confusing because "-D" is a special flag to the
JVM for setting system properties. This creates confusion because some
"-D" properties should be passed via arguments to the JVM (so via
ES_JAVA_OPTS), and some should be passed as arguments to
Elasticsearch. This commit changes the "-D" flag for Elasticsearch
settings to "-E".

Closes #16579, supersedes #16791
</description><key id="140545843">17088</key><summary>Bootstrap does not set system properties</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Core</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-14T00:09:53Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-15T23:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-14T04:49:32Z" id="196137752">@jasontedor I left some comments. My largest concern is again about path.home being exposed.
</comment><comment author="jasontedor" created="2016-03-15T21:49:39Z" id="197039731">&gt; I left some comments. My largest concern is again about path.home being exposed.

@rjernst Thanks for the review. I've responded to all of your comments, and most importantly have removed exposing `path.home`. I think that this is ready for another review round.
</comment><comment author="rjernst" created="2016-03-15T23:23:17Z" id="197066489">Thanks for all the investigation into eg JAVA_OPTS. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not pass double-dash arguments on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17087</link><project id="" key="" /><description>This commit addresses an issue in the init scripts which are passing
invalid command line arguments to the startup script.

Closes #17084
</description><key id="140521856">17087</key><summary>Do not pass double-dash arguments on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-13T19:51:44Z</created><updated>2016-03-13T22:58:52Z</updated><resolved>2016-03-13T22:58:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-13T21:05:22Z" id="196054623">Lgtm
On Mar 13, 2016 3:51 PM, "Jason Tedor" notifications@github.com wrote:

&gt; This commit addresses an issue in the init scripts which are passing
&gt; 
&gt; ## invalid command line arguments to the startup script.
&gt; 
&gt; You can view, comment on, or merge this pull request online at:
&gt; 
&gt;   https://github.com/elastic/elasticsearch/pull/17087
&gt; Commit Summary
&gt; - Do not pass double-dash arguments on startup
&gt; 
&gt; File Changes
&gt; - _M_ distribution/deb/src/main/packaging/init.d/elasticsearch
&gt;   https://github.com/elastic/elasticsearch/pull/17087/files#diff-0 (2)
&gt; - _M_ distribution/rpm/src/main/packaging/init.d/elasticsearch
&gt;   https://github.com/elastic/elasticsearch/pull/17087/files#diff-1 (2)
&gt; 
&gt; Patch Links:
&gt; - https://github.com/elastic/elasticsearch/pull/17087.patch
&gt; - https://github.com/elastic/elasticsearch/pull/17087.diff
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17087.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkItemResponse.Failure should have a StreamInput constructor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17086</link><project id="" key="" /><description>We usually implement the Writeable interface by delegating to a constructor that takes StreamInput. We should do that here.
</description><key id="140506471">17086</key><summary>BulkItemResponse.Failure should have a StreamInput constructor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-13T17:06:05Z</created><updated>2016-03-31T13:35:41Z</updated><resolved>2016-03-31T13:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Should we continue to use PROTOTYPE objects with Writeable?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17085</link><project id="" key="" /><description>Should we use something else? Is declaring a constructor that takes a StreamInput and referencing that constructor where we need it good enough? Better?
</description><key id="140506244">17085</key><summary>Should we continue to use PROTOTYPE objects with Writeable?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label></labels><created>2016-03-13T17:03:59Z</created><updated>2016-04-21T15:59:11Z</updated><resolved>2016-04-21T15:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-14T10:35:52Z" id="196247700">I am not sure we really started doing this or somebody just did it without thinking about if it's necessary. I think it initially came up during the clusterstate diffs where is was necessary in some places but not meant as best practice or a pattern. Especially with Java 8 where we can just pass `SomeWriteable::new` as an `Writeable.IOFunction` is a much better pattern where it's needed
</comment><comment author="nik9000" created="2016-03-14T13:53:05Z" id="196318551">Now that we can make the constructor references I wonder if we can drop `readFrom` from `Writeable` and declare that by conventions `Writeable`s always have a constructor that takes a stream input. We could enforce the convention with tooling like we do the test class hierarchy if we like.
</comment><comment author="s1monw" created="2016-03-14T14:00:29Z" id="196321070">you can do that however, I don't like the fact that the compiler doesn't require you to do so...
</comment><comment author="s1monw" created="2016-03-14T14:33:40Z" id="196335923">@nik9000 maybe we can make it an abstract class?
</comment><comment author="nik9000" created="2016-03-14T14:37:39Z" id="196339248">&gt; @nik9000 maybe we can make it an abstract class?

That doesn't really force the implementer to actually use the StreamInput constructor. It is more of a "reminder", I guess. And it could mess with class hierarchy. I don't really like it.

I think it wouldn't be too hard to make NamingConvetionsCheck check that implementers declare the constructor. It isn't the compiler, but it is a build time thing. I like that better than the abstract class I think.
</comment><comment author="colings86" created="2016-03-15T17:27:13Z" id="196935887">For my part I wouldn't like having this as a build time only check as it makes development annoying since I can have a new fix/feature/enhancement compiling and ready to go and then find when I run the build that there are problems. I personally think we should limit the build time checks as much as possible and prefer compile time errors instead.
</comment><comment author="nik9000" created="2016-03-21T13:15:39Z" id="199269155">I'm going to have a go at this. It is large and I'll do it in stages. I suspect I'll be able to do it without any extra checks simply because all Writeables want to be read and they'll need a StreamInput constructor for that to work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian package does not start in master branch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17084</link><project id="" key="" /><description>I just tried to build the debian package and start it on ubuntu 12 LTS. This is the error I got, when running the start-stop-daemon command manually, otherwise the error is concealed.

``` bash
start-stop-daemon -d /usr/share/elasticsearch --start --user elasticsearch -c elasticsearch --pidfile /var/run/elasticsearch/elasticsearch.pid --exec /usr/share/elasticsearch/bin/elasticsearch -- -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/var/lib/elasticsearch --default.path.conf=/etc/elasticsearch
root@vagrant-ubuntu-precise-64:~# Starts elasticsearch

Option           Description
------           -----------
-D               Configures an Elasticsearch setting
-V, --version    Prints elasticsearch version
                   information and exits
-d, --daemonize  Starts Elasticsearch in the background
-h, --help       show help
-p, --pidfile    Creates a pid file in the specified
                   path on start
-s, --silent     show minimal output
-v, --verbose    show verbose output
ERROR: default.path.home is not a recognized option
```
</description><key id="140501471">17084</key><summary>Debian package does not start in master branch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>blocker</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-13T16:19:19Z</created><updated>2016-03-14T00:18:31Z</updated><resolved>2016-03-14T00:18:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2016-03-13T16:19:52Z" id="195989393">looks similar for the RPM, but did not test it
</comment><comment author="jasontedor" created="2016-03-13T19:52:45Z" id="196036537">This is a consequence of #17024 and I opened #17087 to address. Note that even with the fix for this particular issue, starting Elasticsearch as a service will still fail because of #16579.
</comment><comment author="jasontedor" created="2016-03-14T00:18:31Z" id="196086830">Closed by #17087.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace score_mode `total` with `sum`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17083</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/pull/16045 the `sum` score mode was replaced with `total` to line up with the naming used in Lucene.  However, we use `sum` in many other similar places in Elasticsearch, eg sorting, function score query, nested scoring, etc.

To be consistent, we should use `sum` instead of  `total`. I also prefer it as it is more obvious how the total is calculated.
</description><key id="140480623">17083</key><summary>Replace score_mode `total` with `sum`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Parent/Child</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2016-03-13T12:19:22Z</created><updated>2016-03-18T09:08:20Z</updated><resolved>2016-03-18T09:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-13T13:18:45Z" id="195956186">can we just stick with total, it seems more widely used?
</comment><comment author="clintongormley" created="2016-03-13T13:42:05Z" id="195959210">On the contrary, we use `sum` everywhere in Elasticsearch, not `total`.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix exit code</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17082</link><project id="" key="" /><description>Exit with proper exit code (1) and an error message if elasticsearch executable binary does not exists or has insufficient permissions to execute.
</description><key id="140348731">17082</key><summary>Fix exit code</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">tuladhar</reporter><labels><label>:Packaging</label><label>bug</label><label>v5.0.0-alpha2</label></labels><created>2016-03-12T05:26:11Z</created><updated>2016-04-22T18:00:20Z</updated><resolved>2016-04-22T18:00:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-12T13:42:27Z" id="195743919">@tuladhar Thanks for picking this up. The change is good, I left a nitty comment though. I would also like that there is a test for this added to the bats tests. Let me know if you need guidance through this process but you can start by looking in [`qa/vagrant/src/test/resources/packaging/scripts`](https://github.com/elastic/elasticsearch/tree/93f1a5616881c47f8004dc552b8520beee977d14/qa/vagrant/src/test/resources/packaging/scripts).
</comment><comment author="tuladhar" created="2016-03-21T14:22:17Z" id="199309221">@jasontedor Requested changes in PR #17219 

Sorry, I haven't worked on bats tests before :(

Here's my try:

```
@test "[INIT.D] ensure elasticsearch startup script exists" {
    assert_file_exist "/usr/share/elasticsearch/bin/elasticsearch"
}
```

How about adding above test in 70_sysv_initd.bats?

Thanks
</comment><comment author="jasontedor" created="2016-03-22T03:16:09Z" id="199609092">&gt; Requested changes in PR #17219

No need to open another PR, you can just push new commits to this one. I'm closing #17219, but you can push the changes that you made there to this pull request?

&gt; Sorry, I haven't worked on bats tests before :(

Neither had I until late last week. There's always a first time, and I'd be happy to work through this one with you. :smile: 

&gt; How about adding above test in 70_sysv_initd.bats?

That's definitely the right place, but your proposal isn't testing the change here. The change here is to exit with an error status if `/usr/share/elasticsearch/bin/elasticsearch` either does not exist or is not executable. So a test of this should set these two situations up and ensure that the init scripts then exit with the expected status code.
</comment><comment author="tuladhar" created="2016-03-23T07:54:32Z" id="200234359">Thanks @jasontedor for the pointer. I have pushed changes directly to this branch :+1: 

&gt; Neither had I until late last week. There's always a first time, and I'd be happy to work through this one with you. :smile:

Glad to know that. I've checked out [bats github page](https://github.com/sstephenson/bats) and it's fairly straightforward to use. I've pushed changes to `70_sysv_initd.bats` adding elasticsearch startup existence/executable test using `test` command. Please, review.
</comment><comment author="dakrone" created="2016-04-06T16:46:39Z" id="206459613">@tuladhar I think to test this a good way to do it would be to `chmod -x` the init file, try starting ES up, verify it doesn't start and exits with a code of 1, then `chmod +x` the file back. What do you think @jasontedor?
</comment><comment author="jasontedor" created="2016-04-06T16:47:58Z" id="206460397">@dakrone Yes, that's exactly what I had in mind. We already do something similar in the plugins bats tests.
</comment><comment author="tuladhar" created="2016-04-10T05:21:21Z" id="207921413">@dakrone Sounds good :+1: 

I looked into plugins bats tests as @jasontedor mentioned:
- https://github.com/elastic/elasticsearch/blob/0f00c14afc8428a2a72c0b766d2171029dc8f6e1/qa/vagrant/src/test/resources/packaging/scripts/plugin_test_cases.bash#L168-L178

Please review the changes 79a2b03
</comment><comment author="dakrone" created="2016-04-22T17:28:19Z" id="213520665">Hi @tuladhar I ran these tests today, there is a typo but otherwise they look good, if you want to correct the typo I'll merge this, thanks!
</comment><comment author="tuladhar" created="2016-04-22T17:43:24Z" id="213525371">Thanks @dakrone 

Fixed the typo.
</comment><comment author="dakrone" created="2016-04-22T18:00:14Z" id="213530823">Pushed this as a squashed commit: 9da88a99daca0cc490a758956ec92a5fbe69abcf thanks @tuladhar!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take filterNodeIds into consideration while sending task requests to nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17081</link><project id="" key="" /><description>Port of 83d2caaa05424cc5e1bc2885ac46e73cec1c6b9d to master.
</description><key id="140319533">17081</key><summary>Take filterNodeIds into consideration while sending task requests to nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T23:44:04Z</created><updated>2016-03-26T17:18:13Z</updated><resolved>2016-03-26T17:18:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-24T17:48:27Z" id="200946631">The actual fix looks great but I can't understand the test case. I think my head isn't in the right place. Can you add more comments to it? I'll look again in a few hours. Maybe I'll understand it then.
</comment><comment author="nik9000" created="2016-03-25T12:23:59Z" id="201261119">OK. I understand now. LGTM. Probably worth fixing the comment but otherwise please merge when you get a chance!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nuke SuggestParseElement and cosmetic cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17080</link><project id="" key="" /><description>This PR removes `SuggestParseElement`, implying we can only parse suggestions through `SuggestBuilder#fromXContent`, removing duplicated parsing logic. Now we don't support parsing suggest elements from extra source (`SearchSourceBuilder#ext` bytes) anymore.
</description><key id="140298240">17080</key><summary>Nuke SuggestParseElement and cosmetic cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-03-11T21:36:22Z</created><updated>2016-03-11T22:42:08Z</updated><resolved>2016-03-11T22:42:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-11T21:56:01Z" id="195569335">LGTM!  Great to see so much of the unused code cleaned up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow range aggregation to use lt, lte, gt, and gte</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17079</link><project id="" key="" /><description>The `range` aggregation is awesome, but it currently limits you to defining buckets based on non-obvious inclusive/exclusive terms:

``` json
{
  "to" : "2015-01-01"
},
{
  "from" : "2015-01-01",
  "to" : "2016-01-01"
},
{
  "from" : "2016-01-01"
}
```

It would be great if we changed this to be more along the lines of what we do with the range query:

``` json
{
  "lt" : "2015-01-01"
},
{
  "gte" : "2015-01-01",
  "lt" : "2016-01-01"
},
{
  "gte" : "2016-01-01"
}
```

At the very least, it's more explicit about what's happening, but it also has the added benefit of matching the behavior of the query _and_ it gives a smidgen more flexibility.
</description><key id="140286333">17079</key><summary>Allow range aggregation to use lt, lte, gt, and gte</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Aggregations</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-11T20:40:21Z</created><updated>2016-03-16T14:04:06Z</updated><resolved>2016-03-15T17:06:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-11T21:01:59Z" id="195549233">Duplicates #5249 and #15198, which were closed as won't fix.
</comment><comment author="pickypg" created="2016-03-12T04:33:50Z" id="195658738">@jasontedor Good catch on those. I did search, but I apparently used the wrong words. :)

I do wonder if perhaps we should deprecate the `range` aggregation and suggest what @jpountz suggested: use the `filters` aggregation in its place? I don't like suggesting that when there's a `range` agg, but if it's the replacement -- then so be it. It could be deprecated in 2.3 and a breaking change in 5.0.
</comment><comment author="jasontedor" created="2016-03-12T18:20:53Z" id="195782999">&gt; Good catch on those. I did search, but I apparently used the wrong words. :)

@pickypg I had a hard time finding it via search too, I only persisted because I knew I had seen it before. :smile: 
</comment><comment author="pickypg" created="2016-03-14T19:34:42Z" id="196488404">@clintongormley What do you think about deprecating `range` agg in 2.3, then remove it in 5.0 as described above?
</comment><comment author="clintongormley" created="2016-03-15T17:06:44Z" id="196925917">@pickypg i think the `range` agg is very useful. I don't see any reason to remove it.  @jpountz's suggestion of using the `filters` agg instead is only for those people who need behaviour other than the default.
</comment><comment author="jpountz" created="2016-03-15T17:10:42Z" id="196928155">Another point is that the range agg has an optimization in order to quickly find the appropriate bucket, while the filters aggregation needs to evaluate all filters against incoming documents. If you have many buckets, I believe you could see a difference.
</comment><comment author="pickypg" created="2016-03-16T14:04:06Z" id="197345780">@jpountz: If it's optimized, then it makes me want to have `gt`, `gte`, `lt`, and `lte` even more than. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use a seed node to form multi-node cluster in integ tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17078</link><project id="" key="" /><description>Today we use hardcoded ports to form a cluster in the mulit-node case.
The hardcoded URIs are passed to the unicast host list which is error prone and
might cause problems if those ports are exhausted etc. This commit moves to a
less error prone way of forming the cluster where all nodes are started with port `0`
and all but the first node wait for the first node to write it's ports file to form a
cluster. This seed node is enough to form a cluster.
</description><key id="140258819">17078</key><summary>Use a seed node to form multi-node cluster in integ tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T18:39:11Z</created><updated>2016-04-05T11:06:16Z</updated><resolved>2016-03-13T09:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-11T18:43:09Z" id="195493278">LGTM
</comment><comment author="rjernst" created="2016-03-11T20:35:05Z" id="195539714">LGTM, thank you for fixing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support scheduled commands in current context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17077</link><project id="" key="" /><description>Adds support for scheduling commands to run at a later time on another
thread pool in the current thread's context:

``` java
Runnable someCommand = () -&gt; {System.err.println("Demo");};
someCommand = threadPool.getThreadContext().preserveContext(someCommand);
threadPool.schedule(timeValueMinutes(1), Names.GENERAL, someCommand);
```

This happens automatically for calls to `threadPool.execute` but `schedule`
and `scheduleWithFixedDelay` don't do that, presumably because scheduled
tasks are usually context-less. Rather than preserve the current context
on all scheduled tasks this just makes it possible to preserve it using
the syntax above.

To make this all go it moves the Runnables that wrap the commands from
EsThreadPoolExecutor into ThreadContext.

This, or something like it, is required to support reindex throttling.
</description><key id="140254279">17077</key><summary>Support scheduled commands in current context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T18:15:44Z</created><updated>2016-03-17T11:31:10Z</updated><resolved>2016-03-16T16:30:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-11T18:17:56Z" id="195486047">I'm open to hearing that this is the wrong way to do it. I tried initially just interacting directly with the ThreadContext in reindex and it worked fine but the wrapping that EsThreadPoolExecutor did had such nice error handling that I wanted to steal it for us in reindexing. And copy and pasting code makes me sad, so we, yeah, that is how I got here.
</comment><comment author="s1monw" created="2016-03-16T11:26:08Z" id="197273710">I like the way this is done. I think when we do issue commands they should by default inherit the context. Folks can still opt out by stashing the context in their runnable, no? Aside of this the cleanups are awesome
</comment><comment author="nik9000" created="2016-03-16T11:36:14Z" id="197276633">&gt; I think when we do issue commands they should by default inherit the context.

I can flip that, sure. I think it is a bigger change but I can see if I can get it working.
</comment><comment author="s1monw" created="2016-03-16T11:39:47Z" id="197277532">&gt; I can flip that, sure. I think it is a bigger change but I can see if I can get it working.

maybe we get this in as is and we can fix in a followup?
</comment><comment author="nik9000" created="2016-03-16T11:46:15Z" id="197279806">&gt; maybe we get this in as is and we can fix in a followup?

I'm happy to do that as well.
</comment><comment author="s1monw" created="2016-03-16T11:48:40Z" id="197280260">LGTM then
</comment><comment author="nik9000" created="2016-03-16T15:56:21Z" id="197394788">Broke out the default behavior change into its own issue #17143.
</comment><comment author="nik9000" created="2016-03-16T16:31:18Z" id="197411805">Merged as 80f638b56a1c1551b391ce67ae381ca47ee8a641
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>per_field_analyzer in term vectors api should work even if field does not store term vectors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17076</link><project id="" key="" /><description>Tested with elasticsearch version 2.2

The purpose of `per_field_analyzer` is to get a term vector as created by an analyzer different from the one the field is indexed with. I would expect to get a term vector for a field even if it does not have term vectors stored. Instead I get an NPE:

```

POST test/doc/1
{
  "text": "I am a happy hippo"
}


GET  test/doc/1/_termvectors
{
  "per_field_analyzer": {
    "text": "keyword"
  }
}
```

If I set `"term_vector": "yes"`, in the mapping first all works as expected:

```
PUT test 
{
  "mappings": {
    "doc": {
      "properties": {
        "text": {
          "term_vector": "yes",
          "type": "string"
        }
      }
    }
  }
}

POST test/doc/1
{
  "text": "I am a happy hippo"
}

GET  test/doc/1/_termvectors
{
  "per_field_analyzer": {
    "text": "keyword"
  }
}
```

results in (as expected): 

```
{
  "_index": "test",
  "_type": "doc",
  "_id": "1",
  "_version": 1,
  "found": true,
  "took": 1,
  "term_vectors": {
    "text": {
      "field_statistics": {
        "sum_doc_freq": 5,
        "doc_count": 1,
        "sum_ttf": 5
      },
      "terms": {
        "I am a happy hippo": {
          "term_freq": 1,
          "tokens": [
            {
              "position": 0,
              "start_offset": 0,
              "end_offset": 18
            }
          ]
        }
      }
    }
  }
}
```
</description><key id="140237708">17076</key><summary>per_field_analyzer in term vectors api should work even if field does not store term vectors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Term Vectors</label><label>bug</label></labels><created>2016-03-11T17:04:41Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-6.0.0-f0aa4fc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17075</link><project id="" key="" /><description /><key id="140231771">17075</key><summary>Upgrade to lucene-6.0.0-f0aa4fc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T16:42:53Z</created><updated>2016-03-14T11:48:36Z</updated><resolved>2016-03-14T07:08:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-11T17:14:48Z" id="195460891">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ScriptSortBuilder implement Writable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17074</link><project id="" key="" /><description>This adds methods and tests to ScriptSortBuilder that makes it implement NamedWritable and adds the fromXContent method needed to read itseld from xContent.
</description><key id="140221253">17074</key><summary>Make ScriptSortBuilder implement Writable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T15:59:37Z</created><updated>2016-03-15T17:48:34Z</updated><resolved>2016-03-15T12:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-11T16:00:19Z" id="195427974">This also relates to #15178, mind to take a look @MaineC ?
</comment><comment author="MaineC" created="2016-03-14T12:53:02Z" id="196297968">Left a few minor comments but overall looks good.
</comment><comment author="cbuescher" created="2016-03-14T14:26:10Z" id="196331225">@MaineC thanks, I added a commit that switches sort mode to an enum and addressed your review comments. Let me know if you think there is anything to add.
</comment><comment author="MaineC" created="2016-03-15T09:55:58Z" id="196746766">LGTM - nothing to add from my side (apart from rebasing)
</comment><comment author="cbuescher" created="2016-03-15T11:10:35Z" id="196768040">@MaineC thanks, after adding the SortMode enum in ScriptSortBuilder I also switched GeoDistanceSortBuilder and FieldSortBuilder to using it. Maybe you can take a last look at those changes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shorten GPG keypath for deb packaging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17073</link><project id="" key="" /><description>With this commit we apply the same GPG path shortening logic for
the RPM and the DEB packaging modules.

Relates #17053
</description><key id="140201370">17073</key><summary>Shorten GPG keypath for deb packaging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Packaging</label><label>build</label><label>v2.3.0</label></labels><created>2016-03-11T14:46:47Z</created><updated>2016-03-11T16:00:02Z</updated><resolved>2016-03-11T16:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-11T14:46:58Z" id="195396268">@nik9000 Could you please check?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add infrastructure to run REST tests on a multi-version cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17072</link><project id="" key="" /><description>This change adds the infrastructure to run the rest tests on a multi-node
cluster that users 2 different minor versions of elasticsearch. It doesn't implement
any dedicated BWC tests but rather leverages the existing REST tests.

Since we don't have a real version to test against, the tests uses the current version
until the first minor / RC is released to ensure the infrastructure works.

Given the amount of problems this change already found I think it's worth having this run with our test suite by default. The structure of this infra will likely change over time but for now it's a step into the right direction. We will likely want to split it up into `integTests` and `integBwcTests` etc. so each plugin can have it's own bwc tests but that's left for future refactoring.
</description><key id="140193445">17072</key><summary>Add infrastructure to run REST tests on a multi-version cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label><label>test</label></labels><created>2016-03-11T14:13:04Z</created><updated>2016-03-15T08:17:44Z</updated><resolved>2016-03-15T08:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-11T14:13:46Z" id="195382023">this PR also fixes a bunch of real bugs mainly related to Ingest where it's serialization wasn't tested at all and didn't fail until we ran REST tests  against multiple nodes. I am happy to move the fixes out into a different PR if that's a problem
</comment><comment author="martijnvg" created="2016-03-11T15:57:31Z" id="195427029">LGTM
</comment><comment author="s1monw" created="2016-03-13T10:00:53Z" id="195926206">thx @martijnvg - @rjernst can you take a look at the gradle parts, they are all kinda WIP but I didn't want to mix in bigger refactoring like separating the `integTest` tasks from bwc yet.
</comment><comment author="rjernst" created="2016-03-14T05:12:22Z" id="196144809">@s1monw I left some comments. I like the idea, but I'm concerned about depending on maven local, and would like something that can work in jenkins. I think we could do this by having a jenkins build use the -Dbuild.snapshot=false, and then the bwc tests go against the snapshot version which would be pulled through a normal dependency? It would still have false positives, which I really am not happy about, but it would at least catch the issues you found here and allow this to run regularly in jenkins without manual work needed.
</comment><comment author="s1monw" created="2016-03-14T09:30:28Z" id="196223779">&gt; @s1monw I left some comments. I like the idea, but I'm concerned about depending on maven local, and would like something that can work in jenkins. I think we could do this by having a jenkins build use the -Dbuild.snapshot=false, and then the bwc tests go against the snapshot version which would be pulled through a normal dependency? It would still have false positives, which I really am not happy about, but it would at least catch the issues you found here and allow this to run regularly in jenkins without manual work needed.

nothing here depends on `mavenLocal()` I left it in since it's very useful for development and it's disabled by default. I think we should commit this as it is and don't do any funky `-Dbuild.snapshot=false` builds that keep on failing. We should bump the version in this test once we have a version released and available in public repos. Until then it just runs against itself to ensure we don't break the infra and other bugs sneak in like the serialization stuff in ingest.
</comment><comment author="rjernst" created="2016-03-14T15:09:56Z" id="196360111">@s1monw I misunderstood how you planned to run these. I had thought this would only trip the problems you found when versions were different, which means running a snapshot against itself would not catch it. Therefore I had thought you meant to run this with another version by have it locally installed. What I was suggesting was a way to have this other version, but from within jenkins without requiring maven local. But if that was not the intent, then this LGTM.

Btw, the problem I have with mavenLocal is exactly when dealing with snapshots. It works just fine for released versions, since if a dependency was present in .m2, it would be the same as if it was present in the gradle cache. However, IIRC, mavenLocal never updates .m2, only uses it. So when dealing with snapshots, you can get odd behavior since it won't pickup new snapshots, but just use whatever is installed in .m2. For your intended use here that would not affect anything, but if, for example, during development a dependency was changed to use a snapshot version, and then repos.mavenLocal=true used, you can get odd results since you expect it to be updated with new snapshots but it will not.
</comment><comment author="s1monw" created="2016-03-14T20:19:48Z" id="196504216">@rjernst  I pushed a new commit moving the repo under `build/cluster/shared` I think it's ready
</comment><comment author="rjernst" created="2016-03-14T20:47:22Z" id="196515103">Thanks for the changes @s1monw. Just one small question, but LGTM either way.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Context and headers are not copied when registering percolator queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17071</link><project id="" key="" /><description>This comes from a user report on [discuss](https://discuss.elastic.co/t/shield-authorization-exception-percolation-and-indexed-shapes/43928).

When registering a percolator query (by indexing a document with type `.percolator`) that uses a query with indexed values, the headers and context are not carried over to the request used to get the indexed values. One example of this is a geo shape query that uses an indexed shape. Looking at the `GeoShapeQueryParser`, the code expects that it can get the context and headers from the `SearchContext`, but there is no `SearchContext` when the query is being parsed by the `PercolatorQueriesRegistry`.

I've confirmed the behavior on `2.x` and confirmed that the behavior is fixed in master by #15776, but this change is not something we should try to backport.
</description><key id="140183891">17071</key><summary>Context and headers are not copied when registering percolator queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Percolator</label><label>bug</label><label>discuss</label></labels><created>2016-03-11T13:32:55Z</created><updated>2016-03-20T14:22:58Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2016-03-11T14:09:09Z" id="195380031">The bigger problem here is when the the document in the shape index changes. There is no mechanism that updates the percolator query. I think this is problematic. It is even more problematic in master as there upon indexing the percolator extracts query metadata from the query and stores that with query, so that at percolate time the percolator can be smart about what queries need to be evaluated. So in master the query would really need to be reindexed.

The percolator should not accept queries that fetch data via a get request. That not only applies for the `qeo_shape` query, but also for `terms` and `template` queries. These queries should only be allowed in the percolator if the data (terms, shapes and scripts) are defined inline.
</comment><comment author="clintongormley" created="2016-03-14T11:12:21Z" id="196262571">@s1monw was working on a change to inline these items when indexing the percolator query, so eg a reference to an indexed geoshape would be expanded into the full geoshape at query index time.  Not sure where he got to with it.
</comment><comment author="martijnvg" created="2016-03-14T15:28:05Z" id="196368261">@clintongormley I think what @s1monw meant with, is that instead of caching Lucene queries we cache our own query builders, which at query time are then rewritten to Lucene queries. In case of the `geo_shape` query this would mean that we fetch the shape at query time instead of index time. This would fix the problem in ES &lt; 5.0, however from 5.0 we also during indexing store the query terms for certain percolator queries to speed up execution and in order for this to happen the percolator needs to be rewritten to a Lucene query at index time. This isn't necessarily a problem for the `geo_shape` query since the query terms extract logic hasn't been implemented (yet), but it is a problem for the `terms` query.

I think there are two options here:
- Never do the extract query terms optimisation if a percolator query needs to fetch additional data via a get request.
- The percolator should disallow queries that have been set to execute a get call to fetch terms, shapes etc.

Maybe we should go with option 1, it kind of nice to support these kind of queries in the percolator. For this I do need to update the percolator refactor PR (#16349), since I kind of assumed option 2.
</comment><comment author="clintongormley" created="2016-03-15T14:39:37Z" id="196848732">@martijnvg Having percolator queries that do lookups of data like terms or geoshapes is misleading, as it leads the user to assume that changing the indexed terms/geoshape will automatically update the percolator, which isn't the case.  Plus percolator instantiation can fail if the index holding the remote data isn't available.

I'd go for one of two options:
- Disallow lookup clauses in percolator queries
- Do the lookup at index time and replace the index/type/id with the inlined data

The only thing against the second option is that we don't change the `_source` anywhere else (other than the update API and now the ingest API), so it might be surprising.  Probably OK though.
</comment><comment author="s1monw" created="2016-03-15T15:16:00Z" id="196870850">in order to do the lookup at index time we have to either modify the source or store the rewritten query in a secondary field that we use to parse the query after we indexed it. Today we are using the soruce which is the original query. I think we can easily add a stored field that holds the rewritten one and that way we can also easily upgrade the once indexed before 5.x?
</comment><comment author="martijnvg" created="2016-03-15T15:49:10Z" id="196890405">@s1monw How would that work with the terms we extract in the `PercolatorFieldMapper` and use that to not evaluate percolator queries that would never match at search time? 

Also if the remote document changes, then how would the percolator query be updated?
</comment><comment author="s1monw" created="2016-03-15T16:05:21Z" id="196898659">@martijnvg here is some pseudo-code for `PercolatorFieldMapper`

``` Java

@Override
    public Mapper parse(ParseContext context) throws IOException {
        QueryBuilder queryBuilder = PercolatorQueriesRegistry.parseQueryBuilder(queryShardContext);
        queryBuilder = QueryBuilder.rewrite(queryBuilder, queryShardContext); // fetch the shape etc.
        byte[] queryAsBytes =  queryBuilder.toXContent();
        context.doc.addField("query_field", queryAsBytes)
        Query luceneQuery = queryBuilder.toQuery()
        ExtractQueryTermsService.extractQueryTerms(luceneQuery, context);
        return null;
    }
```

Ideally we would apply this to the source, parse, rewrite, regenerate source and only do it once!
</comment><comment author="s1monw" created="2016-03-15T16:05:45Z" id="196898904">With ingest we could maybe do that?
</comment><comment author="martijnvg" created="2016-03-15T16:19:03Z" id="196904501">@s1monw This is an Interesting approach! But I still don't understand how we would deal with updates in the remote document. (for example the document that holds the shape for `geo_shape` query)

&gt; Ideally we would apply this to the source, parse, rewrite, regenerate source and only do it once!

You mean on the coordination node? If we did this there then it would mean we would need to load IndexService on the fly. (in cases a node doesn't have any shard for that index)

&gt; With ingest we could maybe do that?

If we do this then instead of ingest I prefer to have a dedicated put query api. The reason that we then don't have to parse the entire source to a map of maps. Instead we can stream parse the query bit out of it and add back as a top level field in the source.
</comment><comment author="s1monw" created="2016-03-16T10:30:42Z" id="197252003">&gt; This is an Interesting approach! But I still don't understand how we would deal with updates in the remote document. (for example the document that holds the shape for geo_shape query)

that's not supported - that's the entire point. We have to resolve it once the request comes in, rewrite the source and forward that source to the replicas. Anything else is just not supported.
</comment><comment author="s1monw" created="2016-03-16T10:31:45Z" id="197252194">&gt; If we do this then instead of ingest I prefer to have a dedicated put query api. The reason that we then don't have to parse the entire source to a map of maps. Instead we can stream parse the query bit out of it and add back as a top level field in the source.

what are you concerned about here? if ingest is not capable of doing this why would be release it at all?
</comment><comment author="martijnvg" created="2016-03-16T12:32:22Z" id="197298382">&gt; that's not supported - that's the entire point. We have to resolve it once the request comes in, rewrite the source and forward that source to the replicas. Anything else is just not supported.

Ok, I thought that this behaviour was expected, which I know we can't do. That is why my reaction was to just not support these queries... (this what happens now in #16349, see [second commit](https://github.com/elastic/elasticsearch/pull/16349/commits/34b0b65c91a32369f3cbc696f084534c5cdb4f86))

&gt; what are you concerned about here? if ingest is not capable of doing this why would be release it at all?

I think ingest is capable of doing this, it is just that this whole map of maps conversion is't needed in the context of indexing a percolator query (but from a general ingest use case it does). I think we should make ingest a bit smarter, if we know a pipeline only adds top level fields then ingest doesn't need to do this map of maps conversion and simply appends its data to the binary source. There likely more optimisations that we can do, but just thinking out loud here.

But it does mean that ingest would need to create shard contexts on demand for indices that are not available on the local node.

I'll start with making the suggested change to the PercolatorFieldMapper in #16349, so that we already store queries in this format. As a followup issue we can then see how we can move this to ingest.
</comment><comment author="s1monw" created="2016-03-16T13:37:24Z" id="197333998">&gt; But it does mean that ingest would need to create shard contexts on demand for indices that are not available on the local node

no, all you need is `QueryParseContext` and `QueryRewriteContext` you don't need the index locally.
</comment><comment author="martijnvg" created="2016-03-16T13:39:19Z" id="197334641">&gt; no, all you need is QueryParseContext and QueryRewriteContext you don't need the index locally.

ah wait, so you meant that only the rewrite should happen in ingest? I somehow thought that you also meant that the query extraction should happen in ingest.
</comment><comment author="s1monw" created="2016-03-19T20:30:55Z" id="198779867">&gt; ah wait, so you meant that only the rewrite should happen in ingest? I somehow thought that you also meant that the query extraction should happen in ingest.

yeah I was thinking about a dedicated ingest processor that also does the rewrite?
</comment><comment author="martijnvg" created="2016-03-20T14:22:58Z" id="198941114">&gt; yeah I was thinking about a dedicated ingest processor that also does the rewrite?

+1 for an ingest processor that does a query builder rewrite. 

In order to get the 'do query rewrite once' optimisation, creating a pipeline is then required, but that is something that can be documented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shorten GPG keypath by symlinking to tmp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17070</link><project id="" key="" /><description>With this commit we symlink to the tmp directory in order to avoid
a too long socket path for GPG agent when signing an RPM package.

Closes #17053
</description><key id="140173893">17070</key><summary>Shorten GPG keypath by symlinking to tmp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Packaging</label><label>build</label><label>v2.3.0</label></labels><created>2016-03-11T12:35:51Z</created><updated>2016-03-11T13:25:35Z</updated><resolved>2016-03-11T13:08:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-11T12:39:13Z" id="195350324">@nik9000 Could you have a look at this? We shorten the socket path now by symlinking to `${java.io.tmpdir}` but we only do this for tests. I have added extra logic to avoid this workaround when doing a release (assuming that the path length is shorter in these cases anyway). I don't like that we're forced to do this but it seems that this is necessary with newer gpg versions. If you have any other suggestions, I am happy to discuss.
</comment><comment author="nik9000" created="2016-03-11T13:01:47Z" id="195355490">I'm so sorry about this mess! The fix LGTM. I wish we didn't have to do it but I certainly don't have any better ideas.
</comment><comment author="danielmitterdorfer" created="2016-03-11T13:07:10Z" id="195356470">@nik9000: Thanks for the quick review! I also wish this would not be necessary but I tried to keep the workaround as limited as possible.
</comment><comment author="dliappis" created="2016-03-11T13:25:34Z" id="195361426">:+1: @danielmitterdorfer for the quick fix! let's watch the next builds.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Filter Plugin Issue with Logstash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17069</link><project id="" key="" /><description>Hello,
My main goal is to get a tag (TRANSFER ID) from a previous log event, and i would like to include this tag in my current log event.  So I decided to use the  "Elasticsearch Filter" plugin but I'm having trouble with it:

```
Failed to query elasticsearch for previous event { &lt;
........
.......
SOME LOGS EVENTS
.......
.......
 &gt;}&gt;&gt;, :error=&gt;#&lt;Elasticsearch::Transport::Transport::Errors::MovedPermanently: [301] &lt;HTML&gt;
&lt;HEAD&gt;&lt;TITLE&gt;Redirection&lt;/TITLE&gt;&lt;/HEAD&gt;
&lt;BODY&gt;&lt;H1&gt;Redirect&lt;/H1&gt;&lt;/BODY&gt;
&gt;, :level=&gt;:warn} 
```

Here my code:

```
input {
    file{
        path =&gt; ["/home/christian/Documents/ELK/logs-interop/oma-interop-app.log-20160207"]
        start_position =&gt; "beginning"
        sincedb_path =&gt; "/dev/null"
    }
  stdin{}
}
filter {
    grok {
        patterns_dir =&gt; "./patterns"
        match =&gt; ["message", "%{DEFAULT}"]
    }
   elasticsearch {
        hosts =&gt; ["localhost"]
        query =&gt; "session_id:2" 
        fields =&gt; ["TRANSFER ID", "test_id"]
   }
}
output {
    stdout {
        codec =&gt; rubydebug
    } 
    elasticsearch {
        hosts =&gt; "127.0.0.1"
        index =&gt; "logstash-interop-app"
    }
} 
```

May you help me please.

Sincerely,
Chris
</description><key id="140171000">17069</key><summary>Elasticsearch Filter Plugin Issue with Logstash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Christian1626</reporter><labels /><created>2016-03-11T12:17:24Z</created><updated>2016-03-11T12:25:15Z</updated><resolved>2016-03-11T12:25:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-11T12:25:15Z" id="195347573">Please join us on discuss.elastic.co. We can definitely help you there.
Note that there is a forum dedicated to logstash there.

Not an elasticsearch issue/problem IMO.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> Query using a wildcard is not working on fields which have an icu_collation filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17068</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 

```
$ bin/elasticsearch --version
Version: 2.1.1, Build: 40e2c53/2015-12-15T13:05:55Z, JVM: 1.7.0_80
```

**JVM version**:

```
$ java -version
java version "1.7.0_80"
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)

```

**OS version**: Ubuntu 15.10

**Description of the problem including expected versus actual behavior**:
Query using a wildcard is not working on fields which have an icu_collation filter.

**Steps to reproduce**:
1 - Create index with a custom analyzer

```
$ curl -XPUT 'http://localhost:9200/icu_wildcard/?pretty=1' -d '{
    "settings" : {
        "index" : {
            "number_of_shards" : 1,
            "number_of_replicas" : 1
        },
        "analysis" : {
          "analyzer":{
            "unicode_l1": {
              "char_filter": [
                "icu_normalizer_nfc"
              ],
              "tokenizer": "icu_tokenizer",
              "filter": [
                "icu_collation_primary_filter"
              ]
            }
          },
          "char_filter": {
            "icu_normalizer_nfc": {
              "type": "icu_normalizer",
              "name": "nfc"
            }
          },
          "filter": {
            "icu_collation_primary_filter": {
              "type": "icu_collation",
              "strength": "primary"
            }
          }
        }
      }
}'
```

2 - Put mapping, with two fields, one with standard analyzer and our custom analyzer

```
$ curl -XPUT 'http://localhost:9200/icu_wildcard/_mapping/test?pretty=1' -d '
{
    "dynamic": false,
    "_source": {
      "enabled": false
    },
    "properties": {
      "first_name_unicode": {
        "type": "string",
        "analyzer": "unicode_l1"
      },
      "first_name_standard": {
        "type": "string",
        "analyzer": "standard"
      }
    }
  }
}'
```

3 - Add document

```
$ curl -XPUT 'http://localhost:9200/icu_wildcard/test/1?pretty=1' -d '{
    "first_name_unicode" : "ABCDEFGHIJKLabcdefghijkl",
    "first_name_standard" : "ABCDEFGHIJKLabcdefghijkl"
}'
$ curl -XPOST 'http://localhost:9200/icu_wildcard/_flush?pretty=1'
```

4 - Search on text over `first_name_standard` field

```
$ curl -XGET 'http://localhost:9200/icu_wildcard/test/_search?q=first_name_standard:abcdef*&amp;analyze_wildcard=true&amp;pretty=1'
```

It returns:

```
{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "icu_wildcard",
      "_type" : "test",
      "_id" : "1",
      "_score" : 1.0
    } ]
  }
}
```

5 - Search on text over `first_name_unicode` field

```
$ curl -XGET 'http://localhost:9200/icu_wildcard/test/_search?q=first_name_unicode:abcdef*&amp;analyze_wildcard=true&amp;pretty=1'
```

It returns:

```
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```

As far I know, it should return the same document, but for some reason I don't know, this kind of search isn't working

Hope this helps
</description><key id="140170980">17068</key><summary> Query using a wildcard is not working on fields which have an icu_collation filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ribugent</reporter><labels /><created>2016-03-11T12:17:15Z</created><updated>2016-03-12T13:11:06Z</updated><resolved>2016-03-12T13:11:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-12T13:11:06Z" id="195739315">This can never work, because of how unicode collation works.

Avoid wildcards!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting the nested objects in sorted order.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17067</link><project id="" key="" /><description>Hello,
I first want to declare that I have been introduced to  the world of elasticsearch just a week back, so there could be some obvious points I might be missing while addressing my problems.

However, there is one particular problem, I just can't seem to find a solution to, after exhaustively trying to find and answer, so I would like some help here.

I have created a dummy index to explain my problem.

The mapping is : 

```
{"mappings": {
"person" : {
  "_source": {
    "enabled": false
  }, 
  "properties": {
    "name" :  {
      "type" : "string"
    },
    "age" :{
      "type"  :"long"
    },
    "city" : {
      "type" : "string"
    },
    "attributes" : {
      "type": "nested",
      "include_in_all": true,
      "properties": {
        "attributeName": {
          "type" : "string"
        },
        "id"  :{
          "type" : "string",
          "store" :  true
        },
        "rating" : {
          "type" : "long"
        }
      }
    }
  }
}}}
```

'attributes' is nested, and we are storing the 'attributes.id'

I populated this with following entries:

```
{"name" : "abc1",
"age" : 21,
"city" : "Delhi",
"attributes" : [
  {
    "id" : 1,
    "rating"  : 5,
    "attributeName"  : "at1"
  },
  {"id" : 2,
    "rating" : 7,
    "attributeName"  : "at2"
  }
]}


 {"name" : "abc2",
 "age" : 24,
 "city" : "Mumbai",
 "attributes" : [
  {
    "id" : 4,
    "rating"  : 4,
    "attributeName"  : "at4"
  },
  {"id" : 3,
    "rating" : 8,
    "attributeName"  : "at3"
  }
]}



 {"name" : "abc3",
 "age" : 26,
 "city" : "Delhi",
 "attributes" : [
  {
    "id" : 5,
    "rating"  : 9,
    "attributeName"  : "at5"
  },
  {"id" : 6,
    "rating" : 6,
    "attributeName"  : "at6"
  }
]}
```

Now my problem required me to make a query that applies some filter on city, and some filter on attributes.rating, and gives me back just the attribute ids. There doesn't seem to be an exact solution for this, but I have been able to get it working by using inner_hits and adding id to the inner_hits' fields. Like this:

```
{"query" :{
          "bool": {
            "must": [
               {
                "match" : {
                  "city" : "Delhi"
                }
              },
              {
                "nested": {
                  "path": "attributes",
                  "query": {
                    "bool": {
                      "must": [
                       {
                          "range": {
                            "attributes.rating": {
                              "gt": 5
                            }
                          }
                        }
                      ]
                    }
                  },
                  "inner_hits" : {
                      "fields"  : ["attributes.id"]
                  }
                }
              }
            ]
          }
        }}
```

This gives me the following response:

```
{"took": 9,
"timed_out": false,
"_shards": {
"total": 5,
"successful": 5,
"failed": 0
},
"hits": {
"total": 2,
"max_score": 1.724915,
"hits": [
  {
    "_index": "test",
    "_type": "person",
    "_id": "1",
    "_score": 1.724915,
    "inner_hits": {
      "attributes": {
        "hits": {
          "total": 1,
          "max_score": 1,
          "hits": [
            {
              "_index": "test",
              "_type": "person",
              "_id": "1",
              "_nested": {
                "field": "attributes",
                "offset": 1
              },
              "_score": 1,
              "fields": {
                "attributes.id": [
                  "2"
                ]
              }
            }
          ]
        }
      }
    }
  },
  {
    "_index": "test",
    "_type": "person",
    "_id": "3",
    "_score": 1.724915,
    "inner_hits": {
      "attributes": {
        "hits": {
          "total": 2,
          "max_score": 1,
          "hits": [
            {
              "_index": "test",
              "_type": "person",
              "_id": "3",
              "_nested": {
                "field": "attributes",
                "offset": 1
              },
              "_score": 1,
              "fields": {
                "attributes.id": [
                  "6"
                ]
              }
            },
            {
              "_index": "test",
              "_type": "person",
              "_id": "3",
              "_nested": {
                "field": "attributes",
                "offset": 0
              },
              "_score": 1,
              "fields": {
                "attributes.id": [
                  "5"
                ]
              }
            }
          ]
        }
      }
    }
  }
]}}
```

The second part of my problem requires me to return the attribute id in sorted order.
So from above query how can I get attribute.id, sorted by rating in ascending order. This means that i should get Ids in sequence 6, 2, 5. However, since in my response I get the parent, not the nested doc, and since 5 and 6 are in one parent, and 2 in another, does my problem even have a solution?
</description><key id="140144476">17067</key><summary>Getting the nested objects in sorted order.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">himaniJoshi</reporter><labels /><created>2016-03-11T10:07:10Z</created><updated>2016-03-11T11:39:48Z</updated><resolved>2016-03-11T11:39:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-11T11:39:48Z" id="195333445">GitHub is reserved for feature requests and bug reports; general questions are best addressed on the [Elastic Discourse forums](https://discuss.elastic.co).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document breaking change in ClusterHealthResponse in 2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17066</link><project id="" key="" /><description>With this PR we add documentation of a breaking change related to `ClusterHealthResponse` as [reported on the forums](https://discuss.elastic.co/t/java-client-2-1-1-2-2-0-unexpected-compilation-errors-not-backward-compatible/43782).
</description><key id="140113284">17066</key><summary>Document breaking change in ClusterHealthResponse in 2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Java API</label><label>docs</label><label>v2.2.2</label></labels><created>2016-03-11T07:48:13Z</created><updated>2016-03-11T08:57:18Z</updated><resolved>2016-03-11T08:49:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-11T08:43:21Z" id="195259557">Left a small comment. LGTM otherwise.
</comment><comment author="danielmitterdorfer" created="2016-03-11T08:45:53Z" id="195260829">Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix dynamic mapper when its parent already has an update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17065</link><project id="" key="" /><description>The change to move dynamic mapping handling to the end of document
parsing has an edge case which can cause dynamic mappings to fail
document parsing. If field a.b is added as an as part of the root update,
followed by a.c.d, then we need to expand the mappers on the stack,
since a is hidden inside the root update which exists on the stack.

This change adds a test for this case, as well as tries to better
document how the logic works for building up the stack before adding a
dynamic mapper.
</description><key id="140064207">17065</key><summary>Fix dynamic mapper when its parent already has an update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T02:03:46Z</created><updated>2016-03-11T08:29:40Z</updated><resolved>2016-03-11T08:29:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-11T07:45:31Z" id="195237831">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add raw recovery progress to cat recovery API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17064</link><project id="" key="" /><description>This commit adds fields bytes_recovered and files_recovered to the cat
recovery API. These fields, respectively, indicate the total number of
bytes and files recovered. Additionally, for consistency, some translog
recovery fields have been renamed.

Closes #17022
</description><key id="140059870">17064</key><summary>Add raw recovery progress to cat recovery API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:CAT API</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-11T01:32:47Z</created><updated>2016-03-14T11:08:31Z</updated><resolved>2016-03-11T13:27:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2016-03-11T08:53:40Z" id="195263480">Left minor comment, but LGTM
</comment><comment author="jasontedor" created="2016-03-11T13:25:27Z" id="195361404">&gt; Left minor comment, but LGTM

Thanks @tlrx, I think what you suggest is better, it makes it easier to read in the output. I pushed e4ab131d72a8ae54b00dbe8eadc5105e3f19bed4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Features request: Passing Request list into RequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17063</link><project id="" key="" /><description>It's good to have a feature where we pass a list of BulkRequest. Currently we need to iterate to get all the documents need to be deleted and another set of iteration is need to build a bulk query.  Rather it will be efficient if we pass the list of delete requests. 

&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:

**JVM version**:

**OS version**:

**Description of the problem including expected versus actual behavior**:

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:
</description><key id="140053013">17063</key><summary>Features request: Passing Request list into RequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pradeepg-telsiz</reporter><labels><label>:Bulk</label><label>:Java API</label></labels><created>2016-03-11T00:50:47Z</created><updated>2016-03-14T11:09:11Z</updated><resolved>2016-03-14T11:09:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-14T11:09:11Z" id="196261847">`BulkProcessor` already provides a simplified API that allows you to add individual requests as follows:

``` java
bulkProcessor.add(new IndexRequest("twitter", "tweet", "1").source(/* your doc here */));
bulkProcessor.add(new DeleteRequest("twitter", "tweet", "2"));
```

For more details, please see the [docs on `BulkProcessor`](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-docs-bulk-processor.html).

If you instead want to delete multiple documents that match a query you can use the [delete-by-query plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugins-delete-by-query.html).

I am closing the ticket for now. In case this doesn't cover your needs just reopen it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create Script instance with NativeScriptFactory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17062</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: 2.2.0

**JVM version**: 1.7

&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**: Using native scripting on your own application is not really easy, you have to create a plugin than register your scripts (classes which implements NativeScriptFactory) and compile into a jar; then finally put this jar into elasticsearch home path.

Only after doing this you can do : 
 `new org.elasticsearch.script.Script(MY_SCRIPT_NAME, ScriptService.ScriptType.INLINE, "native", params);`, 
i.e when you want to increment a field, update an ArrayList field or for any other search features.... etc..

It would be very useful if the Script instance could be created with a NativeScriptFactory class,
something like this:
`new Script(MyNativeScriptFactory.class, params);`
with MyNativeScriptFactory.class : 

```
public class MyNativeScriptFactory implements NativeScriptFactory {
    @Override
    public ExecutableScript newScript(final Map&lt;String, Object&gt; params) {
        return new AbstractExecutableScript() {
            @Override
            public Object run() {
                //some stuff here
            }
        };
    }
}
```

I know that you can just use groovy script, but you have to enable it.
</description><key id="140045916">17062</key><summary>Create Script instance with NativeScriptFactory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">genthalili</reporter><labels /><created>2016-03-11T00:08:26Z</created><updated>2016-03-14T11:56:59Z</updated><resolved>2016-03-14T10:46:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-14T10:46:23Z" id="196251071">Hi @genthalili 

We have spent a lot of time locking down Elasticsearch to prevent security exploits.  This would be a huge gaping security hole and we definitely won't be allowing it.

In 5.0 we're releasing a new scripting language called Painless (renamed from Plan A https://github.com/elastic/elasticsearch/pull/15136) which will be enabled by default.  It is fast, safe, and secure.
</comment><comment author="genthalili" created="2016-03-14T11:56:59Z" id="196277798">@clintongormley thanks for the info, Painless is just the solution I am looking for, and that wears its name well!  
Meanwhile the release of 5.0, I'll create/install my plugin which register my scripts...
I have also noticed that when you create a Node Client (without setting `node.master` as `true`) it still installs the plugin but does not register any script, this is great for the security (unless it's a simple bug) so only the Node Master can register scripts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Current link returns 404.  Updated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17061</link><project id="" key="" /><description /><key id="140022057">17061</key><summary>Current link returns 404.  Updated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wickchucked</reporter><labels><label>docs</label><label>v2.2.2</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T21:59:36Z</created><updated>2016-03-10T23:52:02Z</updated><resolved>2016-03-10T23:52:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wickchucked" created="2016-03-10T22:05:13Z" id="195072790">I've signed the CLA now.  Not sure if this is an auto/manual thing.   
</comment><comment author="dakrone" created="2016-03-10T23:51:33Z" id="195106339">Thanks! I'll merge this in!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport reindex to 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17060</link><project id="" key="" /><description>Reindex is two related APIs:
- `_reindex` copies documents from one index to another.
- `_update_by_query` updates documents in one index.
</description><key id="140018248">17060</key><summary>Backport reindex to 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>feature</label><label>release highlight</label><label>v2.3.0</label></labels><created>2016-03-10T21:46:08Z</created><updated>2016-03-15T13:17:55Z</updated><resolved>2016-03-14T13:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-10T21:48:39Z" id="195065363">Unlike @imotov's lovely backport of task management (#16959) I didn't break out the commits that adapted this from master to 2.x. Sorry! I'll try to call out the interesting bits to review.
</comment><comment author="s1monw" created="2016-03-13T13:05:15Z" id="195953485">@nik9000 I did a very high level pass on this and I think it looks great, we reviewed most of the code already in master and it has been backing, I added a comment regarding failing any reindex job while there is any node &lt;= 2.3 in the cluster which I think should happen as early as possible but also within the actions. It's just too risky to assume this can work on anything but a fully upgraded 2.3 cluster. I think we should even fail in the rest layer as well...
</comment><comment author="nik9000" created="2016-03-13T13:09:30Z" id="195954666">Thanks for doing it!  I'll have a more thorough read through soon. I agree
with stopping the whole request in a mixed cluster. It just isn't worth the
trouble.
On Mar 13, 2016 9:05 AM, "Simon Willnauer" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 I did a very high level pass on
&gt; this and I think it looks great, we reviewed most of the code already in
&gt; master and it has been backing, I added a comment regarding failing any
&gt; reindex job while there is any node &lt;= 2.3 in the cluster which I think
&gt; should happen as early as possible but also within the actions. It's just
&gt; too risky to assume this can work on anything but a fully upgraded 2.3
&gt; cluster. I think we should even fail in the rest layer as well...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/17060#issuecomment-195953485
&gt; .
</comment><comment author="nik9000" created="2016-03-13T17:56:29Z" id="196012794">@s1monw: OK! I fixed two issues you raised, I don't want to fix two others because I like my way better, I created an issue to discuss a pattern that I used and neither of us really like (#17085), and one of your questions found an issue that is in master as well so I'd like to fix it there and backport separately.
</comment><comment author="nik9000" created="2016-03-14T12:59:17Z" id="196299757">@s1monw I believe this is ready for another round.
</comment><comment author="s1monw" created="2016-03-14T13:03:14Z" id="196300551">LGTM
</comment><comment author="nik9000" created="2016-03-14T13:49:00Z" id="196317472">Hurray! Thanks for all the review @s1monw and @bleskes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix attachments plugins with docx</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17059</link><project id="" key="" /><description>This adds getClassLoader permission for ingest-attachment and mapper-attachments plugins.

I will backport the mapper attachment fix to 2.3.

closes #16864
</description><key id="140014925">17059</key><summary>Fix attachments plugins with docx</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugin Ingest Attachment</label><label>:Plugin Mapper Attachment</label><label>enhancement</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T21:32:05Z</created><updated>2016-03-14T11:00:47Z</updated><resolved>2016-03-11T01:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-10T21:46:55Z" id="195064413">I did not run it but looking at the changes it sounds good to me.
</comment><comment author="rmuir" created="2016-03-11T01:08:51Z" id="195124927">looks good to me
</comment><comment author="rjernst" created="2016-03-12T00:12:17Z" id="195611783">2.x: a9887e7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disable HeapDumpOnOutOfMemoryError</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17058</link><project id="" key="" /><description>When HeapDumpOnOOME is enabled and a heap dump occurs, it can result in the node remaining hung for significant time while it attempts to dump out the heap.  We'd like to see an environment variable that could be set to disable the automatic heap dump on OOME if desired.  The default behavior should still be to dump heap on OOME, but the ability to optionally disable it could be useful in some situations.
</description><key id="140003587">17058</key><summary>disable HeapDumpOnOutOfMemoryError</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>enhancement</label></labels><created>2016-03-10T20:45:48Z</created><updated>2016-03-10T22:13:50Z</updated><resolved>2016-03-10T22:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-10T22:13:23Z" id="195074802">This can already be done today with `ES_JAVA_OPTS`:

```
$ ES_JAVA_OPTS="-XX:-HeapDumpOnOutOfMemoryError" ./bin/elasticsearch
```

Let's verify:

```
$ ./bin/elasticsearch
```

Then from another shell:

``` bash
$ curl -sS -XGET localhost:9200/_nodes | jq '.. | .pid? | select(. != null)'
39947
$ jcmd 39947 VM.flags | tr ' ' '\n' | grep HeapDumpOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
```

showing that `HeapDumpOnOutOfMemoryError` is enabled. Now with `ES_JAVA_OPTS`:

```
$ ES_JAVA_OPTS="-XX:-HeapDumpOnOutOfMemoryError" ./bin/elasticsearch
```

and from another shell:

``` bash
$ curl -sS -XGET localhost:9200/_nodes | jq '.. | .pid? | select(. != null)'
40671
$ jcmd 40671 VM.flags | tr ' ' '\n' | grep HeapDumpOnOutOfMemoryError
-XX:-HeapDumpOnOutOfMemoryError
```

showing that `HeapDumpOnOutOfMemoryError` is disabled.

Using `ES_JAVA_OPTS` just causes the flag to be on the command line twice, but the `ES_JAVA_OPTS` flags come last so they win.

I must say this though: if the heap is dumping so frequently that the heap dumping taking a long time is an issue, this is just a _symptom_ of a problem, not the true problem. The true problem is that the heap is dumping all the time, and that is what should be solved, not that the heaps take a long time to dump. And if you want to solve the heap dumping all the time, it helps having heap dumps to look at to find out why! So, using this flag is taking away information that can be used to solve the true problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move reindex refresh tests to unit test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17057</link><project id="" key="" /><description>The refresh tests were failing rarely due to refreshes happening
automatically on indexes with -1 refresh intervals. This commit moves
the refresh test into a unit test where we can check if it was attempted
so we never get false failures from background refreshes.

It also stopped refresh from being run if the reindex request was canceled.
</description><key id="139999714">17057</key><summary>Move reindex refresh tests to unit test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T20:27:15Z</created><updated>2016-03-14T20:25:55Z</updated><resolved>2016-03-10T23:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-10T20:39:48Z" id="195036949">LGTM
</comment><comment author="s1monw" created="2016-03-10T20:44:24Z" id="195038327">LGTM
</comment><comment author="nik9000" created="2016-03-10T20:46:02Z" id="195039138">Thanks for the reviews @dakrone and @s1monw !
</comment><comment author="nik9000" created="2016-03-14T20:25:55Z" id="196507617">Backported to 2.x: 2e7d479e8c5e219a9e6001cc24f94d005c003180
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to install kafka plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17056</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_60

**OS version**: Mac OS 10.11.3

**Description of the problem including expected versus actual behavior**:

```
Giuseppes-Mac:libexec gvalente$ pwd
/usr/local/Cellar/elasticsearch/2.2.0_1/libexec
Giuseppes-Mac:libexec gvalente$ bin/plugin install --verbose kafka
-&gt; Installing kafka...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/kafka/2.2.0/kafka-2.2.0.zip ...
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/kafka/2.2.0/kafka-2.2.0.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/kafka/2.2.0/kafka-2.2.0.zip]; 
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```

**Steps to reproduce**:
1. `cd /usr/local/Cellar/elasticsearch/2.2.0_1/libexec`
2. `bin/plugin install --verbose kafka`
</description><key id="139988799">17056</key><summary>Unable to install kafka plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">7AC</reporter><labels /><created>2016-03-10T19:41:36Z</created><updated>2016-03-10T20:04:59Z</updated><resolved>2016-03-10T19:54:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-10T19:54:47Z" id="195020786">There is no plugin named "kafka" that is maintained as part of the Elasticsearch repository which what the steps that you executed above are attempting to install.
</comment><comment author="7AC" created="2016-03-10T19:57:04Z" id="195021784">Thank you, I just assumed that was the way to install log stash plugins as well:

https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html
</comment><comment author="jasontedor" created="2016-03-10T20:04:59Z" id="195025137">No, you have to install logstash plugins using [logstash](https://www.elastic.co/guide/en/logstash/current/working-with-plugins.html). For example:

``` bash
$ ./bin/logstash-plugin install logstash-input-kafka
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix redundant stack in comments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17055</link><project id="" key="" /><description /><key id="139980131">17055</key><summary>fix redundant stack in comments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thefourtheye</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T19:01:43Z</created><updated>2016-03-10T19:50:22Z</updated><resolved>2016-03-10T19:06:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-10T19:06:50Z" id="195003749">Thanks @thefourtheye ! Much less stack now. I think "stack stack" would be a fun term.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Document the modifiers for `field_value_factor`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17054</link><project id="" key="" /><description>Resolves #13511
</description><key id="139974845">17054</key><summary>[DOCS] Document the modifiers for `field_value_factor`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>docs</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T18:35:55Z</created><updated>2016-03-10T18:45:24Z</updated><resolved>2016-03-10T18:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-10T18:40:34Z" id="194995566">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm signing fails with gpg-agent 2.1.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17053</link><project id="" key="" /><description>**Elasticsearch version**: 2.x branch

**OS version**: Fedora 23

**Description of the problem including expected versus actual behavior**:

Currently our [Fedora-based build jobs for 2.x](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+2.x+multijob-os-compatibility/os=fedora/) fail with the following message:

```
[INFO] spawn rpm --define _gpg_name 16E55242 --define _gpg_path /var/lib/jenkins/workspace/elastic+elasticsearch+2.x+multijob-os-compatibility/os/fedora/distribution/src/test/resources/dummyGpg --addsign elasticsearch-2.3.0-SNAPSHOT20160309223154.noarch.rpm
[INFO] elasticsearch-2.3.0-SNAPSHOT20160309223154.noarch.rpm:
[INFO] gpg: WARNING: unsafe permissions on homedir '/var/lib/jenkins/workspace/elastic+elasticsearch+2.x+multijob-os-compatibility/os/fedora/distribution/src/test/resources/dummyGpg'
[INFO] gpg: starting migration from earlier GnuPG versions
[INFO] gpg: can't connect to the agent: Invalid value passed to IPC
[INFO] gpg: error: GnuPG agent unusable. Please check that a GnuPG agent can be started.
[INFO] gpg: migration aborted
[INFO] gpg: can't connect to the agent: Invalid value passed to IPC
[INFO] gpg: skipped "16E55242": No secret key
[INFO] gpg: signing failed: No secret key
Build was aborted
Finished: ABORTED
```

The root cause that gpg-agent (installed version 2.1.9) creates a Unix socket with the following path name which is too long (at most 108 bytes are allowed):

```
[jenkins@slave-2f48afe8 dummyGpg]$ gpg-agent --homedir . --daemon -vvv
gpg-agent[28440]: socket name '/var/lib/jenkins/workspace/elastic+elasticsearch+2.x+multijob-os-compatibility/os/fedora/distribution/src/test/resources/dummyGpg/S.gpg-agent' is too long
```

**Steps to reproduce**:
1. Run `mvn verify` on Fedora 23
</description><key id="139920042">17053</key><summary>rpm signing fails with gpg-agent 2.1.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danielmitterdorfer</reporter><labels><label>:Packaging</label><label>bug</label><label>build</label><label>v2.3.0</label></labels><created>2016-03-10T15:21:34Z</created><updated>2016-03-14T07:15:51Z</updated><resolved>2016-03-12T13:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-03-10T15:26:35Z" id="194904328">A potential workaround for this issue is to add a symlink to a shorter path (e.g. some subdirectory in the tmp folder) and use that instead of the original path. The [GnuPG team is already discussing on shortening the Unix socket path](http://comments.gmane.org/gmane.comp.encryption.gpg.devel/21073) in a future version.
</comment><comment author="nik9000" created="2016-03-10T15:43:04Z" id="194912679">Is there some argument we can give that'll stop trying to use the agent and just use the nasty expect hacks that every other OS uses?
</comment><comment author="danielmitterdorfer" created="2016-03-10T16:06:59Z" id="194926991">I am not really familiar how the process works but the maven-rpm-plugin provides the [parameters](http://www.mojohaus.org/rpm-maven-plugin/rpm-mojo.html) `keyPassphrase`, `keyname` and `keypath` [which we already set](https://github.com/elastic/elasticsearch/blob/2.x/distribution/rpm/pom.xml#L109-L113). Also, the maven-rpm-plugin [creates an expect script](https://github.com/mojohaus/rpm-maven-plugin/blob/0efd27d3beb64c291b960731fe1bdbbb5dbd938e/src/main/java/org/codehaus/mojo/rpm/RPMSigner.java#L137).
</comment><comment author="danielmitterdorfer" created="2016-03-11T08:27:41Z" id="195255029">@nik9000: Another "solution" to this problem is to deactivate signing on this specific platform with the `skipSign` property that you've introduced recently. I am not sure whether we really want to test dummy-signing on this platform. Especially considering that the symlink mentioned above is also just a workaround IMHO. Wdyt?
</comment><comment author="danielmitterdorfer" created="2016-03-11T10:00:57Z" id="195303351">I've just learned that we cannot deactivate signing on a specific platform in the current build infrastructure, leaving us with the symlink workaround for the Maven build. :(
</comment><comment author="dliappis" created="2016-03-11T10:09:12Z" id="195305743">re: https://github.com/elastic/elasticsearch/issues/17053#issuecomment-195303351 there could be some refactoring in the jjb jobs but it would essentially break the logic of one job definition for all platforms which is one important part of the simplication/devops style management of the new CI system.

However, cc'ing @elasticdog directly here in case he has some other idea.
</comment><comment author="dliappis" created="2016-03-11T12:03:11Z" id="195341179">I looked into bypassing what exactly rpm invokes when signing with gpg and found a rather quite informative answer in the fedora forums.

In [that answer](https://ask.fedoraproject.org/en/question/56107/can-gpg-agent-be-used-when-signing-rpm-packages/?answer=57111#post-id-57111) the actual gpg sign command is explicitly defined, so as suggested by @nik9000 then I looked at a way to skip the gpg-agent. Unfortunately with gpg2 this is not possible, as `man gpg2` says:

```
      --use-agent

       --no-use-agent
              This is dummy option. gpg2 always requires the agent.
```

Still looking at other hacks we could put in the definition of `__gpg_sign_cmd`
</comment><comment author="jasontedor" created="2016-03-12T13:20:37Z" id="195741083">I think this should be closed since #17070 and #17073 were integrated? I think that this was not closed automatically because they were not merged into master, but instead into 2.x. Please reopen if I'm closing this in error.
</comment><comment author="danielmitterdorfer" created="2016-03-14T07:15:50Z" id="196176465">@jasontedor You were right; this was just an oversight. Thanks for closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running exec command from an ES 2.2 plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17052</link><project id="" key="" /><description>Hi all,
I'm trying to execute this line of code from an ES 2.2 plugin. 

Process p = Runtime.getRuntime().exec("java -jar test.jar");

but I keep getting this error: 

java.io.IOException: Cannot run program "java": error=13, Permission denied
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1047)
        at java.lang.Runtime.exec(Runtime.java:617)
        at java.lang.Runtime.exec(Runtime.java:450)
        at java.lang.Runtime.exec(Runtime.java:347)
...
Caused by: java.io.IOException: error=13, Permission denied
        at java.lang.UNIXProcess.forkAndExec(Native Method)
        at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:187)
        at java.lang.ProcessImpl.start(ProcessImpl.java:130)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1028)

I added this line 
 permission java.security.AllPermission;
to the java.policy file but it does not work. 

I'm running ES 2.2 on the following JVM:

java version "1.7.0_80"
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)

I also tried the following code: 

AccessController.doPrivileged() 

as suggested here (https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugin-authors.html) but still nothing.

Thanks in advance for your help.

Regards
</description><key id="139903057">17052</key><summary>Running exec command from an ES 2.2 plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">steccami</reporter><labels /><created>2016-03-10T14:29:38Z</created><updated>2016-03-10T16:09:58Z</updated><resolved>2016-03-10T15:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-10T15:00:22Z" id="194889077">This is caused by seccomp/seatbelt. Forking is not allowed from Elasticsearch for security reasons. This is another layer on top of java security provided by the OS and we don't have a way to shut it off. Well, the version you are working with might. There was a -D parameter you could use to disable it iirc but we've removed it and I don't remember if it is in 2.2.

If you really really really really must run an external process you are going to need to communicate with some process outside of Elasticsearch that hasn't dropped its forking permissions.
</comment><comment author="steccami" created="2016-03-10T16:09:58Z" id="194928448">Ok, tnx a lot! 
BTW I've already implemented a workaround based on REST APIs ;-)

Cheers
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>extend ip-type mapper with ability to use long value as ip address</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17051</link><project id="" key="" /><description>I want to index the long value to the ip type. The data has stored in the relational database or hdfs. In order to save storage space, I save the ip address use the long value.  But now, i want to use the elasticsearch to search it. I must convert the long to ip type before index it, and it is inconvenient.
</description><key id="139900086">17051</key><summary>extend ip-type mapper with ability to use long value as ip address</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lizhenmxcz</reporter><labels><label>:Mapping</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-10T14:18:46Z</created><updated>2016-04-17T11:31:35Z</updated><resolved>2016-04-17T11:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-14T14:43:54Z" id="196344050">I think we should stick to the usual representations of ip addresses, so -1 on my end.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard balancing model for throttled shards does not satisfy allocation deciders</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17050</link><project id="" key="" /><description>The shard balancer allocates / relocates shards to nodes based on allocation deciders as well as a weight function. Allocation deciders represent hard constraints. If the deciders say NO then a shard cannot be allocated to that node. The weight function is a soft constraint and can be seen as more of a preference (This is the true "balancing" part of the balancer). The shard balancer allocates / relocates shards to the node with the lowest weight that satisfies the allocation deciders. This is not necessarily the node with the lowest overall weight.

Allocation deciders do not only provide a YES / NO answer but also have a concept of throttling. If an allocation decider returns THROTTLE as result, it means that the shard can be allocated to the node after other shards are finished being allocated / relocated. The shard balancer has special handling for throttling when calculating the weights for nodes. The reason for this is that it wants to optimize the balance for the full allocation of all shards (i.e. it does not optimize the balance for the shards that are allocated in this round but the best balance after all (throttled) shards have been allocated). In order to do that the shard balancer creates a separate predictive model of nodes with their allocated shards to simulate the balancing of shards in the context of throttling.

The issue lies in the way this model is constructed. The main difference in allocating / relocating shards on this predictive model versus the actual allocation is the following. If the node with lowest weight is found where allocation deciders say YES, then the shard is allocated on the predictive model as well as the actual allocation. If, however, the node with lowest weight is found and allocation deciders say THROTTLE, then the shard is allocated only on the predictive model but not on the actual allocation. This means that with each throttled allocation the model diverges more and more from the actual allocation. Allocation deciders calculate their decisions however only based on the actual allocation. The predictive model is never checked if it satisfies any of the constraints. This means that the model does not represent any real future allocation anymore as it does not satisfy the allocation deciders.

Example demonstrating the issue:
https://github.com/ywelsch/elasticsearch/commit/c320581c9c2b7c1773949d1e52cdfe9f2392038e shows a test run where the predictive model contains multiple replicas of the same shard on the same node which does not reflect any realistic situation allowed by the allocation deciders.
</description><key id="139894096">17050</key><summary>Shard balancing model for throttled shards does not satisfy allocation deciders</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2016-03-10T13:59:47Z</created><updated>2016-03-10T15:41:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-10T15:09:45Z" id="194894003">Neat! I thought this was going to go into shard weighting functions but really this is orthogonal/prerequisite. Neat!
</comment><comment author="dakrone" created="2016-03-10T15:41:48Z" id="194911535">@ywelsch do you think it's worth removing the predictive model entirely? Only using the actual allocation and relying on the next cluster state assigning "real" shards with the model?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow additional settings for the node in ESSingleNodeTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17049</link><project id="" key="" /><description>This change adds a method that extending classes can override to provide additional settings
for the node used in a single node test case.
</description><key id="139870589">17049</key><summary>Allow additional settings for the node in ESSingleNodeTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T12:17:39Z</created><updated>2016-03-10T12:42:49Z</updated><resolved>2016-03-10T12:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2016-03-10T12:24:48Z" id="194818316">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resolve index names to Index instances early</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17048</link><project id="" key="" /><description>Today index names are often resolved lazily, only when they are really
needed. This can be problematic especially when it gets to mapping updates
etc. when a node sends a mapping update to the master but while the request
is in-flight the index changes for whatever reason we would still apply the update
since we use the name of the index to identify the index in the clusterstate.
The problem is that index names can be reused which happens in practice and sometimes
even in a automated way rendering this problem as realistic.
In this change we resolve the index including it's UUID as early as possible in places
where changes to the clusterstate are possible. For instance mapping updates on a node use a
concrete index rather than it's name and the master will fail the mapping update iff
the index can't be found by it's &lt;name, uuid&gt; tuple.
</description><key id="139861595">17048</key><summary>Resolve index names to Index instances early</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T11:32:45Z</created><updated>2016-03-14T10:40:35Z</updated><resolved>2016-03-14T10:40:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2016-03-11T14:19:32Z" id="195383597">Left minor comments. No second round of reviews needed on my part. Thanks @s1monw 
</comment><comment author="s1monw" created="2016-03-14T10:40:35Z" id="196248923">thanks @ywelsch for the review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate "reverse" option for sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17047</link><project id="" key="" /><description>There are three ways of specifying sort order in our REST API:

Implicitly which uses the sort element specific default, like e.g.:

```
"sort": [
   {
      "_score": {    }}]
```

Semi-implicit by requesting to reverse sort order:

```
"sort": [
   {
      "_score": {  "reverse": "true"  }}]
```

Explicitly by setting sort order:

```
"sort": [
   {
      "_score": {  "order": "asc"  }}]
```

After reviewing #17035 and going over the docs here: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html which states that "Each sort can be reversed as well." I was confused about what "reverse" actually does. If used in combination with order, what happens depends on the order (no pun intended) of the order and reverse parameters in the search request. Simple reproduction below tested on **_2.2.0**_:

```
PUT /twitter/tweet/1
{
    "message" : "trying out Elasticsearch"
}
PUT /twitter/tweet/2
{
    "message" : "trying trying out Elasticsearch"
}
```

Then issue the following searches for illustration (score based searches, default ordering is descending):

```
GET /twitter/tweet/_search
{
    "query": {"match": {
       "message": "trying"
    }},
    "sort": [
       {
          "_score": {
              "order": "desc",
              "reverse": "false"}}]}
```

... results in descending order.

```
GET /twitter/tweet/_search
{
    "query": {"match": {
       "message": "trying"
    }},
    "sort": [
       {
          "_score": {
              "order": "desc",
              "reverse": "true"}}]}
```

... results in ascending order.

All good so far. Now lets switch the order of the parameters:

```
GET /twitter/tweet/_search
{
    "query": {"match": {
       "message": "trying"
    }},
    "sort": [
       {
          "_score": {
              "reverse": "true",
              "order": "desc"}}]}
```

... results in descending order.

```
GET /twitter/tweet/_search
{
    "query": {"match": {
       "message": "trying"
    }},
    "sort": [
       {
          "_score": {
              "reverse": "false",
              "order": "desc"}}]}
```

... results in descending order.

I'd like to propose to deprecate "reverse" for 2.x and remove it in the next major release in favour of using the explicit "order" parameter only.

@clintongormley maybe I've mis-understood something, any clarifications welcome.
</description><key id="139860237">17047</key><summary>Deprecate "reverse" option for sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>v2.4.0</label></labels><created>2016-03-10T11:25:14Z</created><updated>2016-04-28T07:54:21Z</updated><resolved>2016-04-28T07:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-10T11:29:21Z" id="194800479">+1 on deprecating "reverse" parameter and removing it on master.
</comment><comment author="clintongormley" created="2016-03-10T14:12:12Z" id="194862736">+1 on deprecating and removing.  it's the first i've heard of its existence and i doubt it was ever documented
</comment><comment author="MaineC" created="2016-03-10T14:19:54Z" id="194868360">Awesome - thanks for the green light @clintongormley - wrt. to documentation: The only hint I found doing a quick google search is an example on the page on sorting in the ES guide linked above.
</comment><comment author="MaineC" created="2016-04-28T07:54:21Z" id="215339682">Both, deprecation and removal done. Closing the related issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add basic auth support for proxies in the plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17046</link><project id="" key="" /><description>The plugin manager script supports proxies with the `-Dhttp.proxyHost` and `-Dhttp.proxyPort` settings. Unfortunately this does not support authenticated proxies out of the box, but it looks fairly easy for us to add this by setting the default authenticator in eg the plugin cli main when we see `-Dhttp.proxyUser` and `-Dhttp.proxyPassword`.

From https://github.com/elastic/elasticsearch/issues/16633#issuecomment-183489149
</description><key id="139857497">17046</key><summary>Add basic auth support for proxies in the plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label></labels><created>2016-03-10T11:13:50Z</created><updated>2016-03-11T20:34:36Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sarwarbhuiyan" created="2016-03-11T07:33:44Z" id="195233520">Does this work too?

URL url = new URL(&#8220;location address&#8221;);
URLConnection uc = url.openConnection();
String userpass = username + ":" + password;
String basicAuth = "Basic " + new String(new Base64().encode(userpass.getBytes()));
uc.setRequestProperty ("Authorization", basicAuth);
InputStream in = uc.getInputStream();

Just considering whether that might be better than calling Authenticator.setDefault. Thoughts?
</comment><comment author="rjernst" created="2016-03-11T16:56:19Z" id="195451968">@sarwarbhuiyan That works, except that we still have to read the sysprops. I guess I think it is cleaner to have this behave as if java already knew about these sysprops, and do their reading/handling outside of the code which is opening the url (so it has no knowledge of proxies at all).
</comment><comment author="rjernst" created="2016-03-11T16:57:46Z" id="195452819">Note too that we have 2 places in the install plugin command that get a URL, the plugin zip itself, but also the sha1 hash for official and maven plugins. 
</comment><comment author="sarwarbhuiyan" created="2016-03-11T20:19:54Z" id="195530646">Ok, I'm with you. setting the static authentication doesn't require
touching the existing code or any new code that may forget to set it.
What's the best place to initialise this?
On Sat, 12 Mar 2016 at 01:59, Ryan Ernst notifications@github.com wrote:

&gt; Note too that we have 2 places in the install plugin command that get a
&gt; URL, the plugin zip itself, but also the sha1 hash for official and maven
&gt; plugins.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/17046#issuecomment-195452819
&gt; .
</comment><comment author="rjernst" created="2016-03-11T20:34:36Z" id="195539564">IMO, inside the main method for plugins cli.  But I am cautious about adding it until we can actually test it, which mean we need a fixture that can mock the proxy. I worked on a branch with @rmuir (https://github.com/rjernst/elasticsearch/tree/separate_plugin_cli) that has an https server for integ tests. We could do something similar in another after that is in (or maybe even in the same tests, since we can have a parallel fixture that acts as the proxy (but doesn't need to be a real proxy, we only need to know it hit the endpoint and used the user/pass auth we specified).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: return only matching terms from Search/Query API  </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17045</link><project id="" key="" /><description># **Describe the feature:**

 Queries should have the option to just return a collection of matching terms ( not the documents containing the matching terms ). This would combine the power and flexibility of queries with the output of suggesters. 
# **Example**

Let's assume the following data is indexed:

```
{title: 'Krylov subspace methods'},
{title: 'SUPG methods for finite differences'},
```

if i query the following - 

```
{
    "fuzzy" : {
        "title" : "krylo"
    }
}
```

The response object contains "krylov", not the _entire field or document_ containing "krylov".

This allows users to construct more powerful queries to find _terms_, in lieu of using comparatively limited suggesters. 

EDIT: to clarify I'm aware that something similar is achievable with the use of highlights, but that seems wasteful, less predictable and a bit hacky.
</description><key id="139854270">17045</key><summary>Feature request: return only matching terms from Search/Query API  </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">basiclaser</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2016-03-10T10:58:02Z</created><updated>2016-03-15T19:40:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-14T09:57:06Z" id="196232666">Hi @basiclaser 

What you are suggesting looks easy, but when you are working with the full query DSL it soon becomes complex.  A fuzzy query might return thousands of terms. Which ones do you choose to return to the user?  How do you apply the logic of the bool query to what is returned.

Why do you say that the suggesters are limited here?  The phrase suggester seems like just the thing you need.  It handles fuzziness, but more importantly it finds the best **related** terms.  

Perhaps what you're looking for instead is a form of query expansion, eg: do a query, get the top 5 matching docs, retrieve the most interesting terms from those docs then rerun the query with those terms?
</comment><comment author="basiclaser" created="2016-03-15T19:40:27Z" id="196989543">Hi clinton, thanks for your advice and ponderings. 
I can clarify that I definitely do want whichever terms match my query. I appreciate that this _could_ be achieved by extracting the relevant terms from the document but that feels a bit like climbing through a window to open a door. I'd rather just open the door. 

In my particular use-case there is no fuzziness, though I feel that this feature should be an available feature for whatever type of query. 

Elasticsearch is after all an analytics engine too, right? If somebody is looking to use ES as a means of analysing and returning terms, should that not simply be available? To not provide it, I think, limits the way people think about Elasticsearch.
Would you happen to know if this functionality is available in Lucene? If there is some technical limitation preventing this feature I'd love to hear about it too. 

Thanks again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Port Primary Terms to master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17044</link><project id="" key="" /><description>Primary terms is a way to make sure that operations replicated from stale primary are rejected by shards following a newly elected primary.

Original PRs adding this to the seq# feature branch #14062 , #14651

Relates to #17038
</description><key id="139846642">17044</key><summary>Port Primary Terms to master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-03-10T10:24:37Z</created><updated>2016-03-25T11:16:48Z</updated><resolved>2016-03-25T11:16:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-11T07:49:10Z" id="195238783">@jasontedor pushed an update to those exceptions... sadly testing is harder (but will be possible with new testing infra I'm working on...)
</comment><comment author="ywelsch" created="2016-03-11T09:10:37Z" id="195277750">Thanks @bleskes for porting this to master. Primary terms will be really helpful in many kind of resiliency-related scenarios (solved previously with ugly hacks, e.g. `routedBasedOnClusterVersion` (#16274)).
I left some comments and more general thoughts about primary terms. I haven't looked at src/test/*\* yet but can do this in a second iteration. 
</comment><comment author="bleskes" created="2016-03-12T10:58:04Z" id="195715862">@ywelsch I pushed a new implementation. I'm starting to warm up to making shard routing immutable on the index shard level (different PR, and not now :))
</comment><comment author="bleskes" created="2016-03-13T08:00:06Z" id="195911210">Pushed another commit fixing a side effect of changing the exception types from IllegalShardStateException to IllegalArgumentException (to help with #17038 ) - the replica wrapper shouldn't fail shards locally anymore (technical engine level errors are still dealt with by the engine), but leave it to the primary.  It was originally added in #5847 but with current machinery it's not needed. 

It's also useful to note that adding assertions of the terms invariants to IndexShard forced some clean ups in IndicesClusterService (as @ywelsch already suspected). 

I will update the PR description with these and whatever else we find once the review cycles are done. 
</comment><comment author="ywelsch" created="2016-03-14T11:45:40Z" id="196275477">Left comments/questions here and there. Updating primary terms in AllocationService makes it so much easier to understand how they change (just a single if statement, yay) :smile:.
</comment><comment author="bleskes" created="2016-03-14T20:06:51Z" id="196499839">@ywelsch thanks for the review. I pushed an update
</comment><comment author="ywelsch" created="2016-03-24T22:07:46Z" id="201045289">Left really minor comments. I think we're good with the changes in IndicesClusterStateService. Thanks again for porting this to master.
</comment><comment author="bleskes" created="2016-03-25T10:19:33Z" id="201230425">Thx @ywelsch @jasontedor . I pushed an update and responded to comments
</comment><comment author="ywelsch" created="2016-03-25T10:29:09Z" id="201232330">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can not run gradle integration tests from a plugin directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17043</link><project id="" key="" /><description>With #16989, we can not run anymore `gradle check` from a plugin directory:

``` sh
cd plugins/ingest-attachment
gradle check
```

gives:

```
...
:plugins:ingest-attachment:integTest#copyPlugins
:plugins:ingest-attachment:integTest#installIngestAttachmentPlugin
Plugins directory [/Users/dpilato/Documents/Elasticsearch/dev/es-gradle/elasticsearch/plugins/ingest-attachment/build/cluster/integTest node0/elasticsearch-5.0.0-SNAPSHOT/plugins] does not exist. Creating...
-&gt; Downloading file:/Users/dpilato/Documents/Elasticsearch/dev/es-gradle/elasticsearch/plugins/ingest-attachment/build/cluster/integTest node0/plugins tmp/ingest-attachment-5.0.0-SNAPSHOT.zip
Exception in thread "main" java.lang.IllegalArgumentException: illegal version format - snapshots are only supported until version 2.x
        at org.elasticsearch.Version.fromString(Version.java:152)
        at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:98)
        at org.elasticsearch.plugins.InstallPluginCommand.verify(InstallPluginCommand.java:255)
        at org.elasticsearch.plugins.InstallPluginCommand.install(InstallPluginCommand.java:313)
        at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:142)
        at org.elasticsearch.common.cli.CliTool.execute(CliTool.java:143)
        at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:74)
:plugins:ingest-attachment:integTest#installIngestAttachmentPlugin FAILED
```

The work-around for now is to run:

``` sh
gradle :plugins:ingest-attachment:check
```

Is this something we can fix?
</description><key id="139832938">17043</key><summary>Can not run gradle integration tests from a plugin directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label></labels><created>2016-03-10T09:30:43Z</created><updated>2016-03-10T10:29:17Z</updated><resolved>2016-03-10T10:29:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-10T09:59:19Z" id="194765246">works for me, did you try to turn it off and on again?

`````` BASH
panthor:elasticsearch simon$ cd plugins/ingest-attachment
panthor:ingest-attachment simon$ gradle check
:buildSrc:compileJava UP-TO-DATE
:buildSrc:compileGroovy UP-TO-DATE
:buildSrc:writeVersionProperties
:buildSrc:processResources
:buildSrc:classes
:buildSrc:jar
:buildSrc:sourcesJar UP-TO-DATE
:buildSrc:signArchives SKIPPED
:buildSrc:assemble
:buildSrc:compileTestJava UP-TO-DATE
:buildSrc:compileTestGroovy UP-TO-DATE
:buildSrc:processTestResources UP-TO-DATE
:buildSrc:testClasses UP-TO-DATE
:buildSrc:test UP-TO-DATE
:buildSrc:check UP-TO-DATE
:buildSrc:build
=======================================
Elasticsearch Build Hamster says Hello!
=======================================
  Gradle Version        : 2.10
  OS Info               : Mac OS X 10.11.1 (x86_64)
  JDK Version           : Oracle Corporation 1.8.0_66 [Java HotSpot(TM) 64-Bit Server VM 25.66-b17]
:core:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:core:processResources UP-TO-DATE
:core:classes
:core:jar
:plugins:ingest-attachment:compileJava
:plugins:ingest-attachment:processResources UP-TO-DATE
:plugins:ingest-attachment:classes
:plugins:ingest-attachment:checkstyleMain
:plugins:ingest-attachment:copyPluginPropertiesTemplate
:plugins:ingest-attachment:pluginProperties
:test:framework:compileJava
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:test:framework:processResources UP-TO-DATE
:test:framework:classes
:test:framework:jar
:plugins:ingest-attachment:compileTestJava
:plugins:ingest-attachment:processTestResources
:plugins:ingest-attachment:testClasses
:plugins:ingest-attachment:checkstyleTest
:plugins:ingest-attachment:forbiddenApisMain
:plugins:ingest-attachment:forbiddenApisTest
:plugins:ingest-attachment:forbiddenApis
:plugins:ingest-attachment:checkstyle
:plugins:ingest-attachment:dependencyLicenses
:plugins:ingest-attachment:forbiddenPatterns
:plugins:ingest-attachment:jarHell
:plugins:ingest-attachment:licenseHeaders
:plugins:ingest-attachment:namingConventions
:plugins:ingest-attachment:thirdPartyAudit
:plugins:ingest-attachment:precommit
:plugins:ingest-attachment:test
==&gt; Test Info: seed=446089BDC7AA2D47; jvms=3; suites=4
HEARTBEAT J2 PID(40949@panthor.local): 2016-03-10T10:58:05, stalled for 11.5s at: TikaDocTests.testFiles
Slow Tests Summary:
 22.91s | org.elasticsearch.ingest.attachment.TikaDocTests
 13.59s | org.elasticsearch.ingest.attachment.AttachmentProcessorTests
  4.64s | org.elasticsearch.ingest.attachment.AttachmentProcessorFactoryTests
  1.99s | org.elasticsearch.ingest.attachment.TikaImplTests

==&gt; Test Summary: 4 suites, 18 tests
:plugins:ingest-attachment:jar
:plugins:ingest-attachment:bundlePlugin
:rest-api-spec:compileJava UP-TO-DATE
:rest-api-spec:processResources
:rest-api-spec:classes
:rest-api-spec:jar
:plugins:ingest-attachment:copyRestSpec
:distribution:integ-test-zip:buildZip
:plugins:ingest-attachment:integTest#clean
:plugins:ingest-attachment:integTest#checkPrevious SKIPPED
:plugins:ingest-attachment:integTest#stopPrevious SKIPPED
:plugins:ingest-attachment:integTest#extract
:plugins:ingest-attachment:integTest#configure
:plugins:ingest-attachment:integTest#copyPlugins
:plugins:ingest-attachment:integTest#installIngestAttachmentPlugin
:plugins:ingest-attachment:integTest#start
:plugins:ingest-attachment:integTest#wait
:plugins:ingest-attachment:integTest
==&gt; Test Info: seed=9A749098650E4602; jvm=1; suite=1
==&gt; Test Summary: 1 suite, 4 tests
:plugins:ingest-attachment:integTest#stop
:plugins:ingest-attachment:check

BUILD SUCCESSFUL

Total time: 2 mins 11.608 secs```

``````
</comment><comment author="dadoonet" created="2016-03-10T10:29:17Z" id="194780491">So, here is what I did (from elasticsearch root dir):

``` sh
gradle clean
gradle install
```

It gives: `BUILD SUCCESSFUL`

Then:

``` sh
cd plugins/ingest-attachment
gradle check
```

It now works. o_O

So I don't know what happened before that. The only difference between my first test and this one is that in the meantime I ran `gradle :plugins:ingest-attachment:check`.

Closing it. At least it will help it someone else hit this...

Thanks for your feedback @s1monw. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade azure SDK to 0.9.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17042</link><project id="" key="" /><description>We are ATM using azure SDK 0.9.0.

Azure latest release is now 0.9.3 (released in February 2016).

&lt;img width="1024" alt="the central repository search engine google chrome aujourd hui at 08 41 12" src="https://cloud.githubusercontent.com/assets/274222/13662836/a806ba3a-e69d-11e5-8655-4a838db2ef47.png"&gt;

Artifacts are on [maven central](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.microsoft.azure%22%20AND%20%28a%3Aazure-serviceruntime%20OR%20a%3Aazure-servicebus%20OR%20a%3Aazure-svc-*%29)

Change log:
## 2016.2.18 Version 0.9.3
- Fix enum bugs in azure-svc-mgmt-websites
## 2016.1.26 Version 0.9.2
- Fix HTTP Proxy for Apache HTTP Client in Service Clients
- Key Vault: Fix KeyVaultKey to not attempt to load RSA Private Key
## 2016.1.8 Version 0.9.1
- Support HTTP Proxy
- Fix token expiration issue #557
- Service Bus: Add missing attributes: partitionKey, viaPartitionKey
- Traffic Manager: Update API version, add MinChildEndpoints for NestedEndpoints
- Media: Add support for Widevine (DRM) dynamic encryption
</description><key id="139814262">17042</key><summary>Upgrade azure SDK to 0.9.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>upgrade</label></labels><created>2016-03-10T07:57:20Z</created><updated>2016-03-15T08:26:46Z</updated><resolved>2016-03-15T08:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-14T13:05:51Z" id="196301201">@dadoonet can you put a PR up for this?
</comment><comment author="dadoonet" created="2016-03-15T08:21:11Z" id="196712612">@s1monw Sure. Done at #17102
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Cluster UUID to Persisted Index Metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17041</link><project id="" key="" /><description>This PR adds the cluster UUID to the persisted index metadata for
the cluster state, and separates the representation of the index
metadata on disk from the index metadata that is passed around in
cluster state.

The cluster UUID on the index metadata is used to check whether a dangling
index on a node's disk should be imported into the cluster. If the
cluster UUID in the cluster state is the same as the cluster UUID on the
index metadata, then the dangling index is not imported (and instead,
cleanup happens on disk). If the cluster UUID is not the same as the
cluster UUID on the index metadata, we can assume the cluster
experienced a drastic restart, so we import the dangling index into the
cluster to prevent loss of data.
</description><key id="139784145">17041</key><summary>Add Cluster UUID to Persisted Index Metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abeyad</reporter><labels><label>:Cluster</label><label>enhancement</label></labels><created>2016-03-10T04:53:40Z</created><updated>2016-03-22T14:50:45Z</updated><resolved>2016-03-22T14:50:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abeyad" created="2016-03-10T04:56:17Z" id="194664444">@bleskes @dakrone I still need to add tests and do more rigorous manual testing, but the basics work based on basic testing.  Wanted preliminary feedback on the approach.
</comment><comment author="bleskes" created="2016-03-14T13:01:44Z" id="196300221">@abeyad thx! I left some suggestions
</comment><comment author="abeyad" created="2016-03-14T20:48:25Z" id="196515395">@bleskes thank you for the feedback!  I incorporated the changes and also added some tests, including serialization tests for pre cluster UUID vs post.
</comment><comment author="abeyad" created="2016-03-14T20:55:03Z" id="196517521">@bleskes An example scenario which this implementation doesn't address is that we could have the following:
1. We have 3 nodes on pre 5.0 versions: `A`, `B`, and `C`.  `A` is the master node.
2. All 3 nodes have index `idx`.
3. `C` goes offline.
4. `A` and `B` get upgraded to 5.0 (where cluster UUIDs are now taken into account on index metadata).
5. We delete `idx`, which takes effect on the cluster state for `A` and `B`.
6. `C` comes back online.  

At this point, `C` can not tell the difference between a real deleted index and one that it should import as dangling (because `A` and `B` could have genuinely been wiped out, in which case we would want to import as dangling).  However, there doesn't seem to be a workaround from this issue unless all of the nodes have the cluster UUID stored on index metadata, which is the whole point of this commit in the first place.  So I don't think its something we need to address but its just a caveat.
</comment><comment author="bleskes" created="2016-03-15T09:27:41Z" id="196739298">Thx @abeyad . I think that is fine. The "feature" is only enabled once all data folders have been updated. I will review the code as soon as I can.
</comment><comment author="ywelsch" created="2016-03-16T08:39:00Z" id="197212928">My concern with this change is that it unnecessarily blows up the cluster state by duplicating information that is already contained in another part of the state. What needs to be done imho is to separate the representation of the persisted index metadata from the index metadata that is part of the cluster state. For that I suggest introducing a new class that wraps index metadata and the cluster UUID and use that one to persist / load index metadata on the nodes.
</comment><comment author="bleskes" created="2016-03-16T13:22:22Z" id="197326631">&gt;  separate the representation of the persisted index metadata from the index metadata that is part of the cluster state

Make sense to me. We should see how much craft that adds.. Perhaps make a sub class of `MetaDataStateFormat&lt;IndexMetaData&gt;`
</comment><comment author="abeyad" created="2016-03-18T18:59:36Z" id="198496002">@bleskes @ywelsch I've updated the code to create a separate notion of persisted index metadata vs. index metadata that is passed around in the cluster state.  Your feedback is appreciated.

@areek FYI since you were just working in some of the same classes for the index folder name change.
</comment><comment author="bleskes" created="2016-03-21T19:46:10Z" id="199445562">Left a bunch of comments. Thx @abeyad 
</comment><comment author="abeyad" created="2016-03-22T02:05:32Z" id="199584703">@bleskes Thank you for reviewing the code.  I updated the PR to implement your comments, and left questions as well.
</comment><comment author="abeyad" created="2016-03-22T14:50:44Z" id="199850239">Closing this PR without merging, because upon discussion with @bleskes, the change can cause us to lose indices in the case of split brain.  For example, suppose we have a split brain with master `A` on one side and master `B` on the other side.  If we create an index on `B` (it won't exist on `A`), then recover from split brain but `A`'s cluster state is the most recently updated so it becomes the master, then the index does not exist on `A`'s cluster state so it will delete the dangling indices instead of importing them.  

So, while the change itself is correct in terms of behavior, it can cause data loss in the split brain scenario, so the PR is being closed.  Another solution has been proposed where the cluster state will contain a list of tombstoned indices.  This will make deleted indices explicit in the cluster state instead of just relying on diffs in index metadata in the cluster state.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cloud-aws plugin is not working for new AWS region (Seoul, ap-northeast-2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17040</link><project id="" key="" /><description>I'm using Elasticsearch 1.5.2 version and cloud-aws plugin 2.5.1 version.
cloud-aws plugin is not working for new AWS region (Seoul, ap-northeast-2)

https://github.com/elastic/elasticsearch-cloud-aws/tree/v2.5.1/#version-251-for-elasticsearch-15
'Region' part of above document does not contain ap-northeast-2 region (Seoul)
plz update old version of cloud-aws plugin to support new AWS regions
</description><key id="139753310">17040</key><summary>cloud-aws plugin is not working for new AWS region (Seoul, ap-northeast-2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bezalel</reporter><labels><label>:Plugin Discovery EC2</label><label>enhancement</label></labels><created>2016-03-10T01:23:31Z</created><updated>2016-03-24T06:17:50Z</updated><resolved>2016-03-22T18:46:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-22T18:46:52Z" id="199958785">Actually it has been added from elasticsearch 2.2.0 with #16167.
So it's already there.

Closing.
</comment><comment author="bezalel" created="2016-03-24T04:30:51Z" id="200659901">How about ES 1.x plugins?
I'm using ES 1.5.2 and below 'cloud-aws' plugin.
https://github.com/elastic/elasticsearch-cloud-aws/tree/v2.5.1

I hope new AWS region support should be merged to old version plugins :D
</comment><comment author="dadoonet" created="2016-03-24T06:16:31Z" id="200693049">I don't believe we will do it. We only fix bugs in old versions.
BTW I really encourage to at least upgrade to 1.7.

That being said, you could define the endpoint manually using s3.endpoint. See 
https://github.com/elastic/elasticsearch-cloud-aws/blob/es-1.7/src/main/java/org/elasticsearch/cloud/aws/InternalAwsS3Service.java#L166
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throttling support for reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17039</link><project id="" key="" /><description>The throttling is done between batches so that we can add the wait time to the scroll timeout.
</description><key id="139704407">17039</key><summary>Throttling support for reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T21:24:02Z</created><updated>2016-03-22T16:34:47Z</updated><resolved>2016-03-22T16:34:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-09T21:25:05Z" id="194512667">This throttling support doesn't include throttling a running reindex request. That seems like a wonderful feature but it doesn't need to live in this PR.
</comment><comment author="nik9000" created="2016-03-10T14:29:54Z" id="194872039">@dakrone I think this is ready for another round of review. Thanks for the review so far!
</comment><comment author="dakrone" created="2016-03-10T18:25:13Z" id="194989353">LGTM, left another minor comment. Another thing I'd love to see is an explanation in the docs about exactly how the throttling is implemented so that people monitoring their clusters can understand what the expected behavior is. For example, discussing how the initial scroll is executed, then processing documents is delayed depending on `&lt;formula&gt;`.

I think it'd be beneficial to have in the documentation, but it's up to you if you don't think it should be there.
</comment><comment author="nik9000" created="2016-03-10T19:01:31Z" id="195002095">@dakrone I've written the throttling docs a bit. Better?
</comment><comment author="dakrone" created="2016-03-10T23:49:51Z" id="195105827">Sure, I still don't care of using terms like "bursty" because they are harder to understand from a non-native English speaker's perspective (as are all colloquialisms)
</comment><comment author="nik9000" created="2016-03-11T14:09:12Z" id="195380047">Since we're done talking about the code I'm going to squash and rebase with master.
</comment><comment author="nik9000" created="2016-03-11T14:17:56Z" id="195383187">Rebase wasn't automatic but was mostly clean. Mostly just a matter of adding both sets of changes. In one case I had to add a timeout parameter to a new test to make it all compile. Small changes.
</comment><comment author="nik9000" created="2016-03-11T14:34:51Z" id="195391543">Rebasing discovered some issues with headers and context. I'll add another test and fix.
</comment><comment author="nik9000" created="2016-03-11T20:23:45Z" id="195533883">This is now blocked on https://github.com/elastic/elasticsearch/pull/17077
</comment><comment author="nik9000" created="2016-03-16T16:41:18Z" id="197416497">&gt; This is now blocked on #17077

And that is now merged. So we can have this now.
</comment><comment author="nik9000" created="2016-03-18T13:10:31Z" id="198348281">@dakrone, do you want to re-review now that #17077 is in? Maybe just look at the last two commits?
</comment><comment author="dakrone" created="2016-03-18T15:09:54Z" id="198402450">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable acked indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17038</link><project id="" key="" /><description>Recent work on the distributed systems front brings us to a point where
we are very close to being able to enable the acked indexing test. We
have chased down one additional failure that will require the primary
terms work to address, but let's get this preparatory work in.

Closes #7572
</description><key id="139690031">17038</key><summary>Enable acked indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha2</label></labels><created>2016-03-09T20:19:04Z</created><updated>2016-04-08T23:17:51Z</updated><resolved>2016-04-06T22:13:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-10T16:34:07Z" id="194940073">Left a bunch of minor comments. Thanks for picking this up!
</comment><comment author="bleskes" created="2016-03-30T10:36:06Z" id="203372360">LGTM. Thanks again @jasontedor 
</comment><comment author="jasontedor" created="2016-04-05T21:55:36Z" id="206000940">@bleskes I've addressed all comments from the latest feedback round.
</comment><comment author="bleskes" created="2016-04-06T21:49:49Z" id="206586092">LGTM again. Thanks for all the hard work @jasontedor 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bring back tests for missing elements in the diff-serialized cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17037</link><project id="" key="" /><description>We can add it back now that we improved our compression framework.

Closes #11257
</description><key id="139682025">17037</key><summary>Bring back tests for missing elements in the diff-serialized cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Cluster</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T19:50:24Z</created><updated>2016-03-16T13:39:18Z</updated><resolved>2016-03-16T13:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-03-09T19:50:31Z" id="194475343">@jpountz What do you think? I did a few local runs and it seems to be passing fine.
</comment><comment author="jpountz" created="2016-03-14T09:50:07Z" id="196230662">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typos in comments/strings of `test` module.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17036</link><project id="" key="" /><description /><key id="139655478">17036</key><summary>Fix typos in comments/strings of `test` module.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dongjoon-hyun</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T18:04:59Z</created><updated>2016-03-10T18:49:59Z</updated><resolved>2016-03-10T18:42:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-10T18:43:07Z" id="194996285">Thanks @dongjoon-hyun ! Merged to master.
</comment><comment author="dongjoon-hyun" created="2016-03-10T18:49:59Z" id="194998421">Thank YOU! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move sort `order` field up into SortBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17035</link><project id="" key="" /><description>Currently all SortBuilder implementations have a separate order field. This PR moves it up to SortBuilder and sets it to SortOrder.ASC by default, except for `_score` sorting where the default is set to SortOrder.DESC in the ctor.
</description><key id="139652936">17035</key><summary>Move sort `order` field up into SortBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T17:53:49Z</created><updated>2016-03-10T10:30:47Z</updated><resolved>2016-03-10T10:21:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-09T17:54:21Z" id="194423493">@MaineC we briefly talked about this, mind to take a look?
</comment><comment author="MaineC" created="2016-03-10T09:24:14Z" id="194749639">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting docs do not warn about the use of _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17034</link><project id="" key="" /><description>**Describe the feature**:

The highlighting docs do not help people whose query is using the _all field explain why they are getting no highlighting. Given that the docs on the _all field page do, it would seem sensible to at least link to that information from the highlighting page. Arguably the information may be better located on the highlighting page than the _all page.

https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-highlighting.html
</description><key id="139650930">17034</key><summary>Highlighting docs do not warn about the use of _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nick-giles</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2016-03-09T17:44:35Z</created><updated>2016-03-09T17:50:00Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Don't return all indices immediately if count of expressions &gt;1 and first expression is *</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17033</link><project id="" key="" /><description>Fix #17027
</description><key id="139634911">17033</key><summary>Don't return all indices immediately if count of expressions &gt;1 and first expression is *</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">alexshadow007</reporter><labels><label>:REST</label><label>regression</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T16:50:47Z</created><updated>2016-03-09T23:51:12Z</updated><resolved>2016-03-09T23:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-09T23:00:15Z" id="194556425">Looks good to me. I'll merge to master and backport to 2.x
</comment><comment author="nik9000" created="2016-03-09T23:40:55Z" id="194569048">Thanks @alexshadow007 ! I've merged to master and am backporting it to 2.x by hand. `git cherry-pick` was butchering it.
</comment><comment author="nik9000" created="2016-03-09T23:51:12Z" id="194571552">I backported this with 4c45b49c6e57e8c0e6318147cbc00c9205b95d07.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implement helpful interfaces in reindex requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17032</link><project id="" key="" /><description>Implements IndicesRequest and CompositeIndicesRequest on
UpdateByQueryRequest and ReindexRequest (respectively) so that plugins
can reason about the request. In the case of ReindexRequest this is only
best effort because scripts can edit the request.
</description><key id="139604815">17032</key><summary>Implement helpful interfaces in reindex requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T15:17:12Z</created><updated>2016-03-14T20:57:04Z</updated><resolved>2016-03-09T21:57:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2016-03-09T16:43:47Z" id="194389578">left some minor javadoc comments, other than that LGTM
</comment><comment author="nik9000" created="2016-03-14T20:57:04Z" id="196518107">Backported to 2.x with 6e6769eaaa507928d2e643287592e548aa8d8601
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport Reindex to 2.3 tracker</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17031</link><project id="" key="" /><description>I'm starting to end up with lots of PRs that need to be backported to 2.3 for reindex. I'm going to use this issue as a tracking list:

Required:
- [x] Merging reindex branch into 2.3/2.x branch (to master was #16861) #17060
- [x] Wait for tasks to finish support for Task list API (required so tests pass consistently) (master was #16914) 5b6779ed54b2c628e1fbb4c18518d96b8f6cec4e
- [x] Test for script changing destination index (master was #17023) 61415dccafc09f9b108c896628238c2fc9879ef3
- [x] Timeout reindex if any sub requests timeout (master was #16962) 014475d6fda038992455be35f25338f13ed83ec2
- [x] Search failures make reindex fail (master was #16889) 7b1d567cd21c7dc3d7f2cd7fad889826125cd627
- [x] Make reindex requests implement interfaces that plugins like (master was #17032) 6e6769eaaa507928d2e643287592e548aa8d8601
- [x] Fix a test that fails sporadically (master was #17057) 2e7d479e8c5e219a9e6001cc24f94d005c003180

Would be nice:

&lt;s&gt;\* [ ] Throttling support (master is #17039)&lt;/s&gt; Not happening. 2.3 is eminent and throttling still hasn't been merged to master.
</description><key id="139579330">17031</key><summary>Backport Reindex to 2.3 tracker</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>blocker</label><label>Meta</label><label>v2.3.0</label></labels><created>2016-03-09T13:39:18Z</created><updated>2016-05-09T08:17:20Z</updated><resolved>2016-03-17T17:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BrandesEric" created="2016-03-09T20:50:35Z" id="194501846">Quick question @nik9000.  I am anxiously awaiting "official" reindexing functionality.  I've been following your commits and the Github issues as best I can, but I confess I'm a bit confused on when this will be RTMed?  Will there be an ES 2.3?  Or will the new "5.0" branding be the next version?  Either way, will reindexing go live as part of the next significant release (that is, either 2.3 or 5.0?)
</comment><comment author="nik9000" created="2016-03-09T20:57:50Z" id="194504245">It'll go live with both 2.3 and 5.0 with 2.3 coming sooner than 5.0.
</comment><comment author="BrandesEric" created="2016-03-09T21:00:00Z" id="194504876">@nik9000 that's great, thanks!
</comment><comment author="russcam" created="2016-05-09T08:07:20Z" id="217801149">@nik9000 are there plans to backport reindex throttling to `2.x`?
</comment><comment author="clintongormley" created="2016-05-09T08:17:19Z" id="217803141">@russcam see https://github.com/elastic/elasticsearch/pull/18020
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in clear cache documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17030</link><project id="" key="" /><description /><key id="139549149">17030</key><summary>Fix typo in clear cache documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">36degrees</reporter><labels><label>docs</label></labels><created>2016-03-09T11:22:50Z</created><updated>2016-03-09T15:50:18Z</updated><resolved>2016-03-09T14:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-09T14:33:05Z" id="194320556">thanks @36degrees 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: term prefix suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17029</link><project id="" key="" /><description>**Describe the feature**: The term suggester should allow prefix suggestions in order to display suggestions for a partially written word.
## Example

Let's assume the following data is indexed:

```
{title: 'Krylov subspace methods'},
{title: 'SUPG methods for finite differences'},
```
- If a user types `su` we would like to display the suggestions `subspace` and `SUPG`.
- If a user types `subs` we would like to display the suggestion `subspace`.

Note 1: returning single terms is _not_ supported by the completion suggester &#8211; it returns the full title (or a configurable but fixed output).

Note 2: others tried to implement this with the `highlight` feature; see [here](https://discuss.elastic.co/t/need-advice-for-my-term-completion-feature/43683/3) and [here](http://stackoverflow.com/questions/23256635/elasticsearch-phrase-prefix-search-how-do-i-get-the-matched-phrase). Obviously, parsing the highlights is just a workaround. Maybe there exists another (less hacky) way to retrieve the matched terms from elasticsearch?
</description><key id="139547511">17029</key><summary>Feature request: term prefix suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrenarchy</reporter><labels><label>:Suggesters</label><label>discuss</label><label>feature</label></labels><created>2016-03-09T11:14:28Z</created><updated>2016-11-10T18:19:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="basiclaser" created="2016-03-10T09:43:01Z" id="194758186">+1 !
</comment><comment author="s1monw" created="2016-03-10T09:54:25Z" id="194762656">@mikemccand infix suggester on es anybody? :)
</comment><comment author="clintongormley" created="2016-03-10T10:25:19Z" id="194778503">As I understand it, the analyzing infix suggester requires the index to be sorted in "score" order, ie best matches first.  This could clash with our plans to sort the index by _type to reduce sparsity.
</comment><comment author="jimczi" created="2016-03-10T10:37:47Z" id="194784635">But the analyzing infix suggester could be on a dedicated index. I don't understand why we should always mix suggest and regular search in the same index. Most of the time the suggestions are pushed separately (from the query logs for instance) and having a separate index to handle the high throughput of suggest query is highly desirable (small indices with a lot of replicas for instance). 
Or we could use another type for the suggest, sort the index by types first and use a secondary sort that depends on the type ?
</comment><comment author="mikemccand" created="2016-03-10T15:02:57Z" id="194889952">Lucene's `AnalyzingInfixSuggester` currently uses its own private index ...
</comment><comment author="s1monw" created="2016-03-14T13:07:39Z" id="196301830">&gt; But the analyzing infix suggester could be on a dedicated index. I don't understand why we should always mix suggest and regular search in the same index. Most of the time the suggestions are pushed separately (from the query logs for instance) and having a separate index to handle the high throughput of suggest query is highly desirable (small indices with a lot of replicas for instance). 
&gt; Or we could use another type for the suggest, sort the index by types first and use a secondary sort that depends on the type ?

I agree so much with you - this all size fits one stuff it just not applicable to something so specialized. 
</comment><comment author="andrenarchy" created="2016-03-28T21:20:26Z" id="202584899">Thanks for your feedback! Do I understand correctly that Lucene's `AnalyzingInfixSuggester` accomplishes the task but it needs to be made available in es? If so: would an implementation require a lot of work? (I'm not very experienced with the way es works behind the scenes)

If this gets implemented, I'd vote for a solution that can be configured easily via the mapping (in contrast to the [completion suggester](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html) where the indexed documents are enriched with `suggest` objects). Is this possible?
</comment><comment author="andrenarchy" created="2016-11-10T18:19:20Z" id="259766147">Is there any update on this? I'd still love to use such a feature. &#128521;
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify shard balancer interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17028</link><project id="" key="" /><description>This PR implements a number of simplifications to shard balancing:
- **Remove ShardsAllocators class**: This class wraps a ShardAllocator and GatewayAllocator and tries to fit them to an interface with 5 methods. Of these 5 methods, 2 are only implemented by GatewayAllocator and 2 only by ShardAllocator. Since we have only a single gateway (#8954), I removed this strange abstraction and explicitly added GatewayAllocator and a (configurable) ShardAllocator to the AllocationService.
- **Merge allocateUnassigned, moveShards and rebalance methods in ShardAllocator interface into single allocate method**: AllocationService currently just calls these methods in direct succession. By merging these methods into one, a balancer implementation can better reuse information across these 3 phases (and possibly rearrange / interleave some of them)
- **Reuse shard model across 3 phases in BalancedShardsAllocator**: With the change in step 2, the BalancedShardsAllocator can now reuse the same shard model across its (now internal) allocateUnassigned, moveShards and rebalance steps.
- Smaller enhancements (remove mutable ShardRouting as key to map, remove Decision as map value as unused, less object allocations)
</description><key id="139542557">17028</key><summary>Simplify shard balancer interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Allocation</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T10:48:08Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-11T07:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-09T11:01:50Z" id="194241986">left some minors LGTM otherwise, thanks for cleaning stuff up
</comment><comment author="ywelsch" created="2016-03-10T08:50:20Z" id="194739590">@s1monw pushed db61b86 addressing comments.
</comment><comment author="s1monw" created="2016-03-10T08:55:34Z" id="194741443">LGTM thanks
</comment><comment author="clintongormley" created="2016-03-10T10:02:20Z" id="194766471">@ywelsch could you add a note to the breaking changes docs to explain what is breaking?

thanks
</comment><comment author="ywelsch" created="2016-03-10T15:16:56Z" id="194899745">@s1monw When I ran the tests again before pushing I found an assertion failing in ModelIndex (added in this PR) that checks that we do not have two replicas of the same shard on the same node. This is a more fundamental issue with how the balancer works (also before this PR) and I opened a Github issue for that #17050.

In order not to block this PR on the issue, I have opened a separate Github issue for that #17050 and added a commit that restores the way we manage shard copies in the model (4b3cf51). 
</comment><comment author="s1monw" created="2016-03-10T19:18:07Z" id="195007085">thats fine @ywelsch lets get this one in and then wen can decide how we can fix the issue. I need to read up on it an remind myself how things work in detail and why it's that way, it's been a while since I wrote this 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-index expressions not respecting excludes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17027</link><project id="" key="" /><description>**Elasticsearch version**:

2.2.0

**JVM version**:

1.8.0_73

**OS version**:

windows

**Description of the problem including expected versus actual behavior**:

Why multiple index request (https://www.elastic.co/guide/en/elasticsearch/reference/2.2/multi-index.html) behaviour is different for elastic v1.7 and elastic v2.x?

I have sevetal indexes test1, test2...
All my indexes have sata allocation, but one has ssd. So i make 

```
PUT http://localhost:9200/test10/_settings
{"index.routing.allocation.require.hdd_type" : "ssd"}
```

and then do

```
PUT http://localhost:9200/*,-test10/_settings
{"index.routing.allocation.require.hdd_type" : "sata"}
```

So in elastic v1.7 all worked right for me - all indexes except one have sata allocation,
But elastic v2.x makes ALL indexes sata allocation.

Documentation has not been changed from v1.x. But behaviour is different.
</description><key id="139527443">17027</key><summary>Multi-index expressions not respecting excludes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sss13579</reporter><labels><label>:REST</label><label>adoptme</label><label>low hanging fruit</label><label>regression</label><label>v2.3.0</label></labels><created>2016-03-09T09:46:49Z</created><updated>2016-03-09T23:28:28Z</updated><resolved>2016-03-09T23:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-09T14:01:17Z" id="194308271">Confirmed bug from 2.0.0 onwards.  Simple recreation:

```
PUT test1
PUT test2
PUT test10
```

The following expression resolves to all three indices:

```
GET *,-test10/_validate/query?explain
```

While this expression resolves to `test1` and `test2` only.

```
GET -test10/_validate/query?explain
```

Confirmed working in 1.7.4

Note: This appears to be broken only for `*`, eg the following works correctly:

```
GET test*,-test10/_validate/query?explain
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing index name to indexing slow log</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17026</link><project id="" key="" /><description>This was lost in refactoring even on the 2.x branch. The slow-log
is not per index not per shard anymore such that we don't add the
shard ID as the logger prefix. This commit adds back the index
name as part of the logging message not as a prefix on the logger
for better testabilitly.

Closes #17025
</description><key id="139512372">17026</key><summary>Add missing index name to indexing slow log</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Logging</label><label>bug</label><label>review</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T08:41:42Z</created><updated>2016-03-09T08:49:11Z</updated><resolved>2016-03-09T08:49:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-09T08:44:51Z" id="194186119">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Slowlogs no longer include index name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17025</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: 1.8.0_40

**OS version**: OSX and Ubuntu 15.04

**Description of the problem including expected versus actual behavior**:
Unless I'm completely and utterly blind (which is a distinct possibility), it seems that both indexing and query slowlogs no longer include the name of the index that the operation was running against. This makes them almost entirely useless for us (since we have several hundred indices in our cluster).

**Steps to reproduce**:
1. Generate some queries / indexing requests that triggers a slowlog.
2. Look at the slowlog
3. Scratch your head and wonder why the `type` is present, but not the `index`.

**Provide logs (if relevant)**:

An example slowlog from 2.2.0:

```
[2016-03-09 16:26:15,225][TRACE][index.indexing.slowlog.index] took[4.4ms], took_millis[4], type[test], id[AVNaDew0JKS5u4ProrcX], routing[] , source[{"name":"vlucas/phpdotenv","version":"1.0.3","type":"library","description":"Loads environment variables from `.env` to `getenv()`, `$_ENV` and `$_SERVER` automagically.","keywords":["env","dotenv","environment"],"homepage":"http://github.com/vlucas/phpdotenv","license":"BSD","authors":[{"name":"Vance Lucas","email":"vance@vancelucas.com","homepage":"http://www.vancelucas.com"}],"require":{"php":"&gt;=5.3.2"},"require-dev":{"phpunit/phpunit":"*"},"autoload":{"psr-0":{"Dotenv":"src/"}}}]
```

An example slowlog from 1.7.5:

```
[2016-03-09 16:31:42,346][WARN ][index.indexing.slowlog.index] [Prowler] [slowtest][1] took[95.7ms], took_millis[95], type[test], id[AVNaEumoVIxyp_lAGADK], routing[], source[{"name":"vlucas/phpdotenv","version":"1.0.3","type":"library","description":"Loads environment variables from `.env` to `getenv()`, `$_ENV` and `$_SERVER` automagically.","keywords":["env","dotenv","environment"],"homepage":"http://github.com/vlucas/phpdotenv","license":"BSD","authors":[{"name":"Vance Lucas","email":"vance@vancelucas.com","homepage":"http://www.vancelucas.com"}],"require":{"php":"&gt;=5.3.2"},"require-dev":{"phpunit/phpunit":"*"},"autoload":{"psr-0":{"Dotenv":"src/"}}}]
```
</description><key id="139489767">17025</key><summary>Slowlogs no longer include index name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samcday</reporter><labels><label>:Logging</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v2.3.2</label></labels><created>2016-03-09T06:33:21Z</created><updated>2016-04-18T10:40:53Z</updated><resolved>2016-04-18T10:40:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-09T08:33:48Z" id="194183183">I am looking into this - thanks for reporting
</comment><comment author="s1monw" created="2016-03-09T09:12:51Z" id="194196566">this will be in the upcoming 2.2.1 release - thanks for reporting
</comment><comment author="samcday" created="2016-03-09T11:52:21Z" id="194260900">:+1: thanks for the &#252;ber fast response! :)
</comment><comment author="samcday" created="2016-04-06T04:47:04Z" id="206116197">@s1monw we need to re-open this one, 2.2.1 still has no index name logged in _query_ slowlogs. By the looks of things your commit fixed indexing slowlogs, but I see no mention of query slowlogs.
</comment><comment author="clintongormley" created="2016-04-06T11:45:16Z" id="206330716">Need to make the same change for query slowlogs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cli: Switch to jopt-simple</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17024</link><project id="" key="" /><description>We currently use commons-cli for command line parsing, along with complex abstractions built around it. This change simplifies how command line parsing is done, switching to the jopt-simple library.

Some notes:
- bin/elasticsearch no longer supports --foo=bar. It still supports -Dfoo=bar. This is a large simplification.
- Removed the attachments runner cli. This is no longer necessary since it is now in ingest, and can be simulated there.

Closes #11564
</description><key id="139446326">17024</key><summary>Cli: Switch to jopt-simple</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2016-03-09T01:32:43Z</created><updated>2016-07-29T12:08:39Z</updated><resolved>2016-03-11T20:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-09T01:42:56Z" id="194059459">this may still need updated sha1/licensing info etc for the new jar and removal of the old one?
</comment><comment author="nik9000" created="2016-03-09T01:52:01Z" id="194062380">I like it! 
</comment><comment author="nik9000" created="2016-03-09T01:52:30Z" id="194062579">But I didn't read it closely enough to admit to "reviewing" it.
</comment><comment author="rjernst" created="2016-03-09T02:03:43Z" id="194067452">&gt; this may still need updated sha1/licensing info etc for the new jar and removal of the old one?

Yeah I had only been running unit tests. I will run a full check and fix the shas later tonight. 
</comment><comment author="rjernst" created="2016-03-09T08:17:46Z" id="194177324">I've fixed sha/license issues. I also changed the -E back to -D for bin/elasticsearch, to minimize changes to integ tests necessary. We still can and should do the change to -E to disambiguate from java's -D, but that can be done as a followup.
</comment><comment author="uboness" created="2016-03-09T10:57:17Z" id="194240875">nice cleanup... I'd still consider using `.help` files for the help, at least with common-cli I found that putting it in files gives you much more flexibility on the format/style (specially when it comes to long descriptions and the need to wrap the lines at 80)... similar to man files. That said, I didn't try to run the help with this change to compare.
</comment><comment author="clintongormley" created="2016-03-09T14:30:41Z" id="194319777">@rjernst why does it have to be `-E` or `-D` instead of `--`?  This will be a breaking change for every installation of Elasticsearch and, besides, `--` feels more readable to me.
</comment><comment author="rjernst" created="2016-03-09T16:47:43Z" id="194391349">&gt; why does it have to be -E or -D instead of --? This will be a breaking change for every installation of Elasticsearch and, besides, -- feels more readable to me.

In order to support --, we have to turn on "leniency" in the option parser, which negates the point of having an option parser to do the option parsing work for us, since we now have to do validation for unknown options ourselves. The way -- was handled in the old parser was crazy. It would take all the unknown options, and try to manually parse them after option parsing was complete, looking for the key=value form, and then adding those to the system properties it sets. We already have 6 other ways to set configuration (and this is not an exaggeration, we have 4 different system properties prefixes, the config file itself, and these two command line alternatives). I understand that it is breaking, but I don't think it is something difficult for a user to update when they see it is no longer supported. As for "readability", I think code complexity outweighs that here, but I also personally don't see any difference.

While I don't personally think it is useful, for the same reason about having 7 different ways to configure settings above, one additional possibility is to still support -- for explicit options we find useful to have. For example, we can add `--path.conf` as a supported option. It is just the arbitrary and dynamic handling of -- that must go.
</comment><comment author="rjernst" created="2016-03-09T16:53:05Z" id="194393971">&gt; I'd still consider using .help files for the help, at least with common-cli I found that putting it in files gives you much more flexibility on the format/style (specially when it comes to long descriptions and the need to wrap the lines at 80)... similar to man files

I would not use resource files for help messaging for a number of reasons:
- We have an option parser that we've registered the options with. It seems a waste to not use its ability to generation help!
- The files can easily be out of date and incorrect (which I found numerous times when moving over the help text)
- The tests for them were not useful, they just tested that the file was printed!
- I don't think man file style should be used. If we want man files for eg `bin/elasticsearch`, then we should add those to our distribution, but --help should be usage information.

That said, I do think we can still improve the help output. Right now, we don't have a usage line in the help, but I think that can be added in a followup.

&gt; That said, I didn't try to run the help with this change to compare.

Here is what the help looks like with this PR for `bin/elasticsearch`:

```
[08:54:26][~/code/elasticsearch/distribution/zip/build/distributions/elasticsearch-5.0.0-SNAPSHOT]$ bin/elasticsearch --help
Starts elasticsearch

Option           Description
------           -----------
-D               Configures an Elasticsearch setting
-V, --version    Prints elasticsearch version
                   information and exits
-d, --daemonize  Starts Elasticsearch in the background
-h, --help       show help
-p, --pidfile    Creates a pid file in the specified
                   path on start
-s, --silent     show minimal output
-v, --verbose    show verbose output
```
</comment><comment author="clintongormley" created="2016-03-09T16:55:07Z" id="194395220">@rjernst thanks for the explanation

&gt; While I don't personally think it is useful, for the same reason about having 7 different ways to configure settings above, one additional possibility is to still support -- for explicit options we find useful to have. For example, we can add --path.conf as a supported option. It is just the arbitrary and dynamic handling of -- that must go.

No, I'd keep things consistent, otherwise the user will have to look up which options use `--` every time.
</comment><comment author="jasontedor" created="2016-03-09T16:55:23Z" id="194395335">&gt; As for "readability", I think code complexity outweighs that here, but I also personally don't see any difference.

I've been through the code (both the old, and the new) by myself and with @rjernst and not having to support multiple ways to do the same thing _greatly_ simplifies the code. 

&gt; We already have 6 other ways to set configuration (and this is not an exaggeration, we have 4 different system properties prefixes, the config file itself, and these two command line alternatives).

And I would like to take it even further when I rework #16971 on top of this pull request to remove some of this flexibility as well.
</comment><comment author="jasontedor" created="2016-03-11T18:35:29Z" id="195491252">@rjernst This is a _great_ change, I left an initial review.
</comment><comment author="rjernst" created="2016-03-11T19:48:13Z" id="195519446">@jasontedor @nik9000 I pushed a new commit.
</comment><comment author="jasontedor" created="2016-03-11T19:51:36Z" id="195520336">LGTM.
</comment><comment author="clintongormley" created="2016-03-12T15:02:11Z" id="195753573">@rjernst please add a note to breaking changes
</comment><comment author="jasontedor" created="2016-03-14T00:12:49Z" id="196086533">&gt; please add a note to breaking changes

@clintongormley I added such a note in #17088 (I carried it over from #16791).
</comment><comment author="Mpdreamz" created="2016-03-23T15:25:40Z" id="200392882">Might need to open a new issue for this but 

&gt; $ elasticsearch blahblah 

silently ignores blah blah

which can get hairy if someone forgets an `-E` e.g

&gt; $ elasticsearch -E es.cluster.name="x" es.node.name="y"

the latter setting is ignored.
</comment><comment author="nik9000" created="2016-03-23T15:39:05Z" id="200399564">Yeah, I'd open an issue for that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test script changing destination index for reindex request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17023</link><project id="" key="" /><description /><key id="139410884">17023</key><summary>Test script changing destination index for reindex request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>review</label><label>test</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T22:19:51Z</created><updated>2016-03-14T19:05:41Z</updated><resolved>2016-03-10T18:07:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-10T16:23:24Z" id="194933609">LGTM
</comment><comment author="nik9000" created="2016-03-14T19:05:41Z" id="196476656">Backported to 2.x with 61415dccafc09f9b108c896628238c2fc9879ef3
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add bytes and files progress to cat recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17022</link><project id="" key="" /><description>The cat recovery API does not output fields showing the in-progress bytes and files recovered (it only shows bytes and files percent). These fields should be added to the cat recovery API. Additionally, there is some inconsistency in the naming of the fields. Today we have:

`bytes`: bytes to be recovered
`total_bytes`: total bytes on disk
`bytes_percent`: percent of bytes to be recovered that have been recovered

`files`: files to be recovered
`total_files`: total files on disk
`files_percent`: percent of files to be recovered that have been recovered

`translog`: translog ops recovered
`translog_total`: total translog ops to be recovered
`translog_percent`: percent of translog ops to be recovered that have been recovered.

This means that:

`translog_percent = translog / translog_total`

but that

`bytes_percent != bytes / bytes_total`

and

`files_percent = files / files_total`.

Thus, we should add `bytes_recovered` and `files_recovered`, rename `translog` to `translog_ops_recovered` and rename `translog_total` to `translog_ops` and then it will be

`bytes_percent = bytes_recovered / bytes`,
`files_percent = files_recovered / files`

and

`translog_ops_percent = translog_ops_recovered / translog_ops`.
</description><key id="139407183">17022</key><summary>Add bytes and files progress to cat recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:CAT API</label><label>breaking</label></labels><created>2016-03-08T22:06:01Z</created><updated>2016-03-11T13:27:55Z</updated><resolved>2016-03-11T13:27:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2016-03-08T22:08:03Z" id="193988127">Another little inconsistency: in `_cat/indices` sizes are human-readable by default and can be switched to bytes, in `_cat/recovery` it's always bytes.
</comment><comment author="clintongormley" created="2016-03-09T13:51:19Z" id="194304811">@jasontedor note that this is a breaking change, so let's do it in 5.0
</comment><comment author="jasontedor" created="2016-03-11T13:27:55Z" id="195361889">Closed by #17064 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail closing or deleting indices during a full snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17021</link><project id="" key="" /><description>Closes #16321
</description><key id="139385357">17021</key><summary>Fail closing or deleting indices during a full snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Snapshot/Restore</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T20:28:28Z</created><updated>2016-03-14T10:18:45Z</updated><resolved>2016-03-10T17:51:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2016-03-09T22:10:23Z" id="194531931">I still don't think it's right to allow index deletion even for partial snapshots :)  Other then that and the exception message that mentions partial snapshot, LGTM.
</comment><comment author="ywelsch" created="2016-03-10T17:51:07Z" id="194975334">Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/recovery API reports bytes always equal to total_bytes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17020</link><project id="" key="" /><description>**Elasticsearch version**:

```
# elasticsearch --version
Version: 2.2.0, Build: 8ff36d1/2016-01-27T13:32:39Z, JVM: 1.8.0_72-internal
```

**JVM version**:

```
# java -version
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**: Debian Jessie on kernel 4.1.3.

**Description of the problem including expected versus actual behavior**:

I was trying to understand why recover is happening much slower than I expect it to happen with `indices.recovery.max_bytes_per_sec=512mb` and `indices.recovery.concurrent_streams=8`. While doing so (unsuccessfully so far), I realized that `bytes` is always equal to `total_bytes` in `_cat/recovery` API. This doesn't seem logical, since `bytes_percent` is not 100%.

```
# curl -s http://myhost/_cat/recovery?v | fgrep -v done
index                        shard time    type       stage source_host target_host repository snapshot files files_percent bytes       bytes_percent total_files total_bytes translog translog_percent total_translog
aaa-2016.03.08               0     560663  relocation index 10.36.11.8  10.36.11.4  n/a        n/a      268   95.1%         45476478773 78.8%         268         45476478773 0        0.0%             791066
aaa-2016.03.08               5     560665  relocation index 10.36.11.7  10.36.11.4  n/a        n/a      238   91.6%         45083552298 79.3%         238         45083552298 0        0.0%             665251
```

I'd love to see any pointers on how to debug slower than expected recoveries, since CPU, IO and network don't seem to be saturated.
</description><key id="139384468">17020</key><summary>_cat/recovery API reports bytes always equal to total_bytes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2016-03-08T20:24:28Z</created><updated>2016-03-08T22:06:19Z</updated><resolved>2016-03-08T20:32:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-08T20:32:56Z" id="193956183">There is not a bug here:

`bytes` is the size to recover in bytes
`total_bytes` is the total number of bytes in the shard

The two are equal if it's a full recovery, but they can be not equal in the case of a partial recovery (from local files first).

`bytes_percent` is the percent of bytes that need to be recovered

I hope that helps? Let me know if not. :smile: 
</comment><comment author="bobrik" created="2016-03-08T20:46:00Z" id="193959698">This is a bit confusing.
- `translog_percent = translog / total_translog`
- `bytes_percent != bytes / total_bytes`
- `files_percent != files / total_files`

Any chance to make it consistent?
</comment><comment author="jasontedor" created="2016-03-08T21:49:34Z" id="193982335">I agree it's confusing. It's like this:

`bytes_percent * bytes = bytes_recovered` (which does not have a column, but that can be added)
`files_percent * files = files_recovered` (which does not have a column, but that can be added)

The translog must always be fully recovered, however.

&gt; Any chance to make it consistent?

I think that we can do something like this: add `bytes_recovered`, and `files_recovered`, rename `translog` to `translog_ops_recovered` and rename `translog_total` to `translog_ops`. Then it will be

`bytes_percent = bytes_recovered / bytes`
`files_percent = files_recovered / files`
`translog_percent = translog_ops_recovered / translog_ops`

What do you think @bobrik?
</comment><comment author="bobrik" created="2016-03-08T21:55:51Z" id="193984809">That would be great. Will you file an issue then?
</comment><comment author="jasontedor" created="2016-03-08T22:06:19Z" id="193987666">&gt; That would be great. Will you file an issue then?

@bobrik Sure, I opened #17022.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cancellation of shard relocation does not work in 2.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17019</link><project id="" key="" /><description>**Elasticsearch version**:

```
# elasticsearch --version
Version: 2.2.0, Build: 8ff36d1/2016-01-27T13:32:39Z, JVM: 1.8.0_72-internal
```

**JVM version**:

```
# java -version
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**: Debian Jessie on kernel 4.1.3.

**Description of the problem including expected versus actual behavior**:

[Docs say](https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html) that primary and replica have to have the same synch id in order to achieve immediate recovery and avoid costly relocation. I restart 1 node out of 8 and see that most of indices recover on remaining nodes, even though restarted node rejoined. Week old indices recover too. To be fair, it did not work for me on 1.7.3 either. #6069 is is closed, therefore I'm filing this issue.

**Steps to reproduce**:
1. Restart one node.
2. Wait until node rejoins.

Expected: immediate recovery for old indices, translog recovery for active indices. Nice and easy.

Actual: almost all (if not all) recover, active indices recover all data files (terabytes of them). Ingestion is suffering from backpressure, people notice delayed indexing and tell mean things about you. Sadness and disappointment.

It seems that some indices do not have `sync_id`. I tried checking sync IDs for old indices that were recovering and the field appeared.

Before:

``` json
[
  {
    "routing": {
      "state": "STARTED",
      "primary": true,
      "node": "hOM4Or2fTG-Do4ZkR9jIRQ",
      "relocating_node": null
    },
    "commit": {
      "id": "MJ19KOilFLcuYnGni1rE+A==",
      "generation": 81,
      "user_data": {
        "translog_uuid": "OUU730pTTSOGk-07aJEMJw",
        "translog_generation": "80"
      },
      "num_docs": 62732221
    },
    "shard_path": {
      "state_path": "/disk/data6/es/main/main/nodes/0",
      "data_path": "/disk/data6/es/main/main/nodes/0",
      "is_custom_data_path": false
    }
  },
  {
    "routing": {
      "state": "STARTED",
      "primary": false,
      "node": "yNaQ5IGARhGtu5FN8AvGUQ",
      "relocating_node": null
    },
    "commit": {
      "id": "4F6/8APNSb40wCqr89bs5g==",
      "generation": 82,
      "user_data": {
        "translog_uuid": "GE4r0UDHTda2aLd-PwJ9Bg",
        "translog_generation": "80"
      },
      "num_docs": 62732221
    },
    "shard_path": {
      "state_path": "/disk/data5/es/main/main/nodes/0",
      "data_path": "/disk/data5/es/main/main/nodes/0",
      "is_custom_data_path": false
    }
  }
]
```

Then I do manual synched flush:

```
# curl -X POST -s http://myhost/myindex-2016.02.29/_flush/synced | jq .
```

``` json
{
  "_shards": {
    "total": 2,
    "successful": 2,
    "failed": 0
  },
  "www-nginx-error-2016.03.01": {
    "total": 2,
    "successful": 2,
    "failed": 0
  }
}
```

After:

``` json
[
  {
    "routing": {
      "state": "STARTED",
      "primary": true,
      "node": "hOM4Or2fTG-Do4ZkR9jIRQ",
      "relocating_node": null
    },
    "commit": {
      "id": "MJ19KOilFLcuYnGni3ExRw==",
      "generation": 82,
      "user_data": {
        "translog_uuid": "OUU730pTTSOGk-07aJEMJw",
        "sync_id": "AVNXkFKDdHUamVU5aCvy",
        "translog_generation": "80"
      },
      "num_docs": 62732221
    },
    "shard_path": {
      "state_path": "/disk/data6/es/main/main/nodes/0",
      "data_path": "/disk/data6/es/main/main/nodes/0",
      "is_custom_data_path": false
    }
  },
  {
    "routing": {
      "state": "STARTED",
      "primary": false,
      "node": "yNaQ5IGARhGtu5FN8AvGUQ",
      "relocating_node": null
    },
    "commit": {
      "id": "4F6/8APNSb40wCqr8+yqgQ==",
      "generation": 83,
      "user_data": {
        "translog_uuid": "GE4r0UDHTda2aLd-PwJ9Bg",
        "sync_id": "AVNXkFKDdHUamVU5aCvy",
        "translog_generation": "80"
      },
      "num_docs": 62732221
    },
    "shard_path": {
      "state_path": "/disk/data5/es/main/main/nodes/0",
      "data_path": "/disk/data5/es/main/main/nodes/0",
      "is_custom_data_path": false
    }
  }
]
```
</description><key id="139361594">17019</key><summary>Cancellation of shard relocation does not work in 2.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>:Index APIs</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-08T18:53:41Z</created><updated>2016-03-15T13:16:25Z</updated><resolved>2016-03-15T13:16:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobrik" created="2016-03-08T19:31:30Z" id="193934457">Probably important detail: I optimized these indices to reduce segment count, maybe it removed `sync_id`.
</comment><comment author="bleskes" created="2016-03-08T19:45:38Z" id="193938688">@bobrik I can confirm that force_merge/optimize removes the sync id marker, which is a big shame. 

@s1monw @mikemccand I think the easiest fix is to activate the shard after the force merge was done. Doing anything in the forceMerge logic in the engine will probably be tricky with concurrent writes. Do you agree/am I missing some lucene magic?
</comment><comment author="bleskes" created="2016-03-08T19:51:23Z" id="193940531">or alternatively, maybe org.elasticsearch.index.engine.InternalEngine#tryRenewSyncCommit is what want use instead of flushing. It aquires a write lock, but I think it's fine for force merge... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate lat_lon and precision_step</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17018</link><project id="" key="" /><description>With GeoPoinV2 `lat_lon` and `precision_step` parameters will be removed in 5.0 (see #16910). This PR adds deprecation logging for 2.x. Removal of parameters will be done in a follow up PR.
</description><key id="139360512">17018</key><summary>Deprecate lat_lon and precision_step</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>deprecation</label><label>v2.3.0</label></labels><created>2016-03-08T18:48:53Z</created><updated>2016-03-08T21:13:48Z</updated><resolved>2016-03-08T21:13:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-08T19:05:22Z" id="193918714">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Actually bound the generic thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17017</link><project id="" key="" /><description>This commit actually bounds the size of the generic thread pool. The
generic thread pool was of type cached, a thread pool with an unbounded
number of workers and an unbounded work queue. With this commit, the
generic thread pool is now of type scaling. As such, the cached thread
pool type has been removed. By default, the generic thread pool is
constructed with a core pool size of four, a max pool size of 128 and
idle workers can be reaped after a keep-alive time of thirty seconds
expires. The work queue for this thread pool remains unbounded.
</description><key id="139359684">17017</key><summary>Actually bound the generic thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>breaking</label><label>resiliency</label><label>v5.0.0-alpha3</label></labels><created>2016-03-08T18:45:16Z</created><updated>2016-04-26T12:29:40Z</updated><resolved>2016-04-26T12:27:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-08T18:45:30Z" id="193910576">@bleskes Can you please review _very_ carefully? :smile: 
</comment><comment author="bleskes" created="2016-03-10T12:56:49Z" id="194830514">I started going through this again and looking at the cached type now, I think we can get rid of it all together and just use scaling (and other defaults). I'm not even sure we need to add setting validation - the generic thread is not different from any other scaling queue - if you mess up the setting you're in big trouble.. I will go along if you want to keep those restriction but I'm afraid it will feel artificial. 
</comment><comment author="jasontedor" created="2016-03-10T20:18:26Z" id="195030378">&gt; I started going through this again and looking at the cached type now, I think we can get rid of it all together and just use scaling (and other defaults).

@bleskes I agree and I pushed 51ecabe13de8c7de674d4837277c4fe8290d60e1 to remove the cached thread pool type.
</comment><comment author="jasontedor" created="2016-04-23T21:03:27Z" id="213832567">@ywelsch I have updated this pull request as we discussed and I think that this ready for your review.
</comment><comment author="jasontedor" created="2016-04-25T14:45:36Z" id="214367512">@ywelsch I think this is ready for another review cycle.
</comment><comment author="ywelsch" created="2016-04-26T08:04:15Z" id="214661583">Left two minor comments. Feel free to push after addressing them. Thanks @jasontedor, LGTM.
</comment><comment author="jasontedor" created="2016-04-26T12:29:13Z" id="214723341">Thanks for a helpful and thorough review @ywelsch!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a way to tune the number of most frequently used filters in query cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17016</link><project id="" key="" /><description>**Describe the feature**:

&gt; The way it works is that Elasticsearch keeps track of the 256 most recently used filters, and only caches those that appear 5 times or more in this history. 

We currently tracks 256 in the history and caches only those that appear 5 times or more in the history, so it will end up caching maybe 50 or so distinct filters.  In a multitenant environment, there can easily be more than 50 distinct filters for each customer id, so it will be nice to provide a way to tune this based on the use case.
</description><key id="139357837">17016</key><summary>Provide a way to tune the number of most frequently used filters in query cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Cache</label><label>:Search</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-08T18:37:57Z</created><updated>2017-03-21T15:04:51Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2016-03-08T18:47:29Z" id="193911493">/cc @jpountz 

I skimmed LUCENE-6077 and see the frequency is slightly different depending on query type:  freq = 2 for "expensive to build" and "cheap to cache", while freq = 5 for everything else.  What kinds of queries fall into the freq=2 category?
</comment><comment author="jpountz" created="2016-03-11T09:49:58Z" id="195299338">freq=2 is used for all queries that need to evaluate the documents that they match on the whole index regardless of whether they are intersected with a selective filter or not. So this includes for instance range, wildcard and prefix queries.

&gt; In a multitenant environment, there can easily be more than 50 distinct filters for each customer id

I agree that we might want to consider increasing the size of the history, but I don't think caching term queries on customer ids would be very helpful: by nature an inverted index is a already a cache of matching documents for every possible term and Lucene makes sure that they can be decoded efficiently. Loading term queries in the query cache only duplicates parts of the inverted index in memory.
</comment><comment author="colings86" created="2017-03-21T15:04:51Z" id="288107528">Is this issue still relevant?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate geo_point mapping params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17015</link><project id="" key="" /><description>Adds deprecation logging to geo_point validate and normalize parameters per #16910 
</description><key id="139357492">17015</key><summary>Deprecate geo_point mapping params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>deprecation</label><label>v2.3.0</label></labels><created>2016-03-08T18:36:37Z</created><updated>2017-03-11T22:20:50Z</updated><resolved>2016-03-08T20:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-08T19:04:10Z" id="193917887">LGTM
</comment><comment author="hadoopmonica" created="2016-04-28T15:47:58Z" id="215473305">I used to use geopoint field in elasticsearch 1.7 to 2.2, but when I upgrade to elasticsearch 2.3.1, geopoint type is not recognized. I posted the issue in elastic.co, please take a look.
https://discuss.elastic.co/t/geopoint-type-field-in-template-is-not-recognized-after-upgrading-to-elasticsearch-2-3-1/48686
</comment><comment author="mohitsikri" created="2017-03-11T12:42:36Z" id="285864342">On 2.3.1 , "ignore_malformed" : true allows to store out of range geo point data(say for ex : 
  "location" : {
            "lat" : 23.23061994988693,
            "lon" : 437.3666667938232 // coterminal angle
        }
, but it returns the same result back (upon _search) , when do normalization kicks in i.e. when doc is stored or searched? Help appreciated. Thanks. Also I expected a result of bounding box query but found empty.
{
    "query": {
        "bool" : {
            "must" : {
                "match_all" : {}
            },
            "filter" : {
                "geo_bounding_box" : {
                    "pin.location" : {
                        "top_left" : [77.36005783081049, 23.227859403521386],
                        "bottom_right" : [77.37276077270508, 23.23440574921077]
                    }
                }
            }
        }
    }
} </comment><comment author="nknize" created="2017-03-11T22:20:50Z" id="285904854">Two separate topics:

1. Normalization happens at index time. The query results displayed (from `_search`) are the same as what is provided in source; normalization doesn't change the incoming source, it just indexes in the normalized coordinate.
2. The `lat` values in your example bounding box are incorrect. `top_left` lat value is &lt; `bottom_right` lat value - they need to be swapped. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting a null_pointer_exception when executing a distributed multi-term vector query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17014</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.2.0
**JVM version**:
1.8.0_72
**OS version**:
Ubuntu 14.04.4 LTS
**Description of the problem including expected versus actual behavior**:
I'm running the following query on a 3-node ES cluster running ES 2.2.0 on Microsoft Azure. When dfs is set to true, I get a null_pointer_description (demonstrated below). However, if I set dfs to false, I get term vectors. I'd really like to set dfs to true because I'd like to get term statistics for the entire index.

``` javascript
{  
    "ids" : ["AVM0c1e8FqhBZ7YfiHM8"],
    "parameters": {
        "fields": ["LongDescription"],
        "term_statistics": true,
        "field_statistics": true,
        "dfs": true
    }
}
```

The response I'm getting is:

``` javascript
{
  "docs": [
    {
      "_index": "my_index",
      "_type": "my_type",
      "_id": "AVM0c1e8FqhBZ7YfiHM8",
      "error": {
        "root_cause": [
          {
            "type": "exception",
            "reason": "failed to execute term vector request"
          }
        ],
        "type": "exception",
        "reason": "failed to execute term vector request",
        "caused_by": {
          "type": "null_pointer_exception",
          "reason": null
        }
      }
    }
  ]
}
```

**Provide logs (if relevant)**:

The original question, along with relevant logs, can be found [here](https://discuss.elastic.co/t/getting-a-null-pointer-exception-when-executing-a-distributed-multi-term-vector-query/43606/3).
</description><key id="139344603">17014</key><summary>Getting a null_pointer_exception when executing a distributed multi-term vector query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">crjackso</reporter><labels /><created>2016-03-08T17:44:36Z</created><updated>2016-03-08T18:23:18Z</updated><resolved>2016-03-08T18:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T18:23:18Z" id="193901059">Hi @crjackso 

DFS support has been removed from the termvector API.  See https://github.com/elastic/elasticsearch/pull/16452
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ScoreSortBuilder implement NamedWriteable and add fromXContent()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17013</link><project id="" key="" /><description>This change makes ScoreSortBuilder implement NamedWriteable, adds equals() and hashCode() and also implements parsing ScoreSortBuilder back from xContent. This is needed for the ongoing Search refactoring.
</description><key id="139335474">17013</key><summary>Make ScoreSortBuilder implement NamedWriteable and add fromXContent()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Search Refactoring</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T17:07:25Z</created><updated>2016-03-10T18:57:08Z</updated><resolved>2016-03-09T14:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-08T17:08:06Z" id="193871442">@MaineC I took a shot at ScoreSortBuilder to get a first feeling for the sort builders, maybe you can take a look.
</comment><comment author="MaineC" created="2016-03-09T09:03:33Z" id="194192121">Only question: From briefly looking I couldn't find the code in fromXContent in the original source - was that newly added, or didn't I look close enough?

Other than that LGTM.
</comment><comment author="cbuescher" created="2016-03-09T13:02:17Z" id="194286720">@MaineC thanks, great suggestion about ExpectedException rules. Regarding the code in fromXContent the original source, I think theres not one place where the "_score" sorts get parse but many. I think one of the main problems with the current sort refactoring is that the parsing code is so spread across SortParseElement. 
In this case, where SortField for score sort gets added in addSortField() the only parameter that matters is `reverse`. That in turn is only changed from the default in addCompoundSortField() when the `order` or `reverse` json parameter is specified (and for the short cut notation `{ "_score", "asc"}`, but I think we need to handle that separately). Hope this makes sense, would like to get your feedback on this since I think this is what makes the sort refactoring tricky.
</comment><comment author="MaineC" created="2016-03-09T13:11:41Z" id="194291550">&gt; I think one of the main problems with the current sort refactoring is that the parsing code is so spread 
&gt; across SortParseElement. 

+1000 that's biggest issue that's been making things more complicated than they look like on first sight IMHO

&gt; Hope this makes sense.

It does make sense.

Side-note: I think test-wise I'm really looking forward to something that compares the outcome of the current SortParseElement to what we will have for generating Lucene sort objects...
</comment><comment author="cbuescher" created="2016-03-09T13:44:46Z" id="194302143">@MaineC thanks, pushed update including the proposed changes in the test an will merge if you don't have further comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_detection not working with not_analyzed fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17012</link><project id="" key="" /><description>**Elasticsearch version**: 2.2.0

**JVM version**: Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)

**OS version**: OSX 10.11.1

**Description of the problem including expected versus actual behavior**:

If you enable both `date_detection: true` and `index: not_analyzed` in a dynamic template, dates are not detected.

**Steps to reproduce**:
1. Create a template with a PUT to `_template/reproducebug` and the following body (`date_detection: true` and `index:not_analyzed`):

```
{
    "template": "*",
    "mappings": {
        "_default_": {
            "date_detection": true,
            "_all": {
                "enabled": false
            },
            "dynamic_templates": [
                {
                    "template1": {
                        "mapping": {
                            "index": "not_analyzed",
                            "type": "{dynamic_type}"
                        },
                        "match": "*"
                    }
                }
            ],
            "properties": {
                "msg": {
                    "type": "string",
                    "index": "analyzed"
                }
            }
        }
    }
}
```
1. Index the following document, that contains a ISO8601 date: `{"foo":"bar", "time":"2016-03-08T09:52:09.097Z"}`
2. Check that the field time is indexed as a not_analyzed string.

**Provide logs (if relevant)**:
No relevant logs.
</description><key id="139335312">17012</key><summary>date_detection not working with not_analyzed fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palmerabollo</reporter><labels /><created>2016-03-08T17:06:43Z</created><updated>2016-03-08T22:12:12Z</updated><resolved>2016-03-08T18:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T18:18:27Z" id="193898881">You're running into this: https://github.com/elastic/elasticsearch/issues/2401

If you change your template to use `"match_mapping_type": "*"` instead of `"match": "*"` it will work
</comment><comment author="palmerabollo" created="2016-03-08T22:12:11Z" id="193989333">Yes, it worked. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong boost is applied when using query time boosting in the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17011</link><project id="" key="" /><description>Field boost mapping is now applied at query time. See https://github.com/elastic/elasticsearch/pull/16900

This causes problem when boost values of the same field within two types of the same index are different.
The following mapping:

```
PUT test?update_all_types=true
{
   "mappings": {
      "test1": {
         "properties": {
            "text": {
               "type": "string",
               "boost": 1
            }
         }
      },
      "test2": {
         "properties": {
            "text": {
               "type": "string",
               "boost": 2
            }
         }
      }
   }
}
```

... will always pick the first boost value for queries containing the text field no matter what types is queried:

```
GET test/test1/_validate/query?explain
{
   "query": {
     "match": {
         "text": "foo bar"
     }
   }
}

GET test/test2/_validate/query?explain
{
   "query": {
     "match": {
         "text": "foo bar"
     }
   }
}

GET test/_validate/query?explain
{
   "query": {
     "match": {
         "text": "foo bar"
     }
   }
}
```

In order to fix this discrepancy we have two options:
- Remove the ability to define different boost values on the same field.
- Apply the boost depending on the type of the request. If no or multiple types are defined then the boost is ignored.

IMO we should go with the first option because it's the closest to what we had before with the index time boost even though we forbid a valid use case.
</description><key id="139324750">17011</key><summary>Wrong boost is applied when using query time boosting in the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimczi</reporter><labels><label>:Mapping</label><label>bug</label><label>discuss</label></labels><created>2016-03-08T16:32:50Z</created><updated>2016-03-08T19:03:04Z</updated><resolved>2016-03-08T18:03:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-08T16:54:41Z" id="193864683">boost should be on the MappedFieldType and the same across all document types.
</comment><comment author="jimczi" created="2016-03-08T18:03:57Z" id="193894563">After chatting with @rjernst I realized that update_all_types updates all the types with the new value. This means that having multiple boosts for the same field name within the same index is not allowed.
Another thing that came up from the chat is that update_all_types should not be allowed on index creation.
</comment><comment author="rjernst" created="2016-03-08T19:03:04Z" id="193917304">Just a note on `update_all_types` for index creation: IIRC, we allowed this because of the case where you are setting metadata mappers on index creation. Due to how document type mappings are parsed, we always start out with the defaults, and anything we parse is merged with the default. What we need is to not add the default metadata mappers until after parsing, when we are creating a new mapping type. It used to be difficult, but I think with recent changes this may be possible now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Call to _analyze API timeouts when analyzer does not exisit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17010</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.2.0
**JVM version**:
java version "1.7.0_65"
OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
**OS version**:
Ubuntu 14.04.1 LTS
**Description of the problem including expected versus actual behavior**:
Making a call to analyze API turns into a timeout when the analyzer doesn't exist and no index is provided in the url, if the index is included, the expected error is returned. Tried it in an old 1.3.4 ES server and it returns the expected error quickly.

**Steps to reproduce**:
Note i'm using 'standard_s_' analyzer
1. curl -XGET 'http://localhost:9200/_analyze?pretty&amp;analyzer=standards&amp;text=this-is-a-test'
2. But this works ok (returns an error): curl -XGET 'http://localhost:9200/my_index/_analyze?pretty&amp;analyzer=standards&amp;text=this-is-a-test'
   3.

**Provide logs (if relevant)**:
This shows in elasticsearch logs everytime i make a request (with no index):

```
[2016-03-08 15:37:31,027][ERROR][transport                ] [mynode] failed to handle exception for action [indices:admin/analyze[s]], handler [org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1@227981db]
java.lang.NullPointerException
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:195)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$700(TransportSingleShardAction.java:115)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$1.handleException(TransportSingleShardAction.java:174)
        at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:821)
        at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:799)
        at org.elasticsearch.transport.TransportService$4.onFailure(TransportService.java:361)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

When making the request to the index the stacktrace is the following:

```
[2016-03-08 16:07:32,495][DEBUG][action.admin.indices.analyze] [mynode] null: failed to execute [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@1c344de9]
RemoteTransportException[[mynode][my_ip:9300][indices:admin/analyze[s]]]; nested: IllegalArgumentException[failed to find analyzer [standards]];
Caused by: java.lang.IllegalArgumentException: failed to find analyzer [standards]
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:143)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2016-03-08 16:07:32,496][INFO ][rest.suppressed          ] /my_index/_analyze Params: {index=my_index, text=this-is-a-test, analyzer=standards, pretty=}
RemoteTransportException[[mynode][my_ip:9300][indices:admin/analyze[s]]]; nested: IllegalArgumentException[failed to find analyzer [standards]];
Caused by: java.lang.IllegalArgumentException: failed to find analyzer [standards]
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:143)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:63)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:282)
        at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:275)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:299)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```
</description><key id="139320441">17010</key><summary>Call to _analyze API timeouts when analyzer does not exisit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">macebalp</reporter><labels /><created>2016-03-08T16:20:35Z</created><updated>2016-03-08T16:36:18Z</updated><resolved>2016-03-08T16:36:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-08T16:36:17Z" id="193853071">Duplicates #15148, closed by #15447
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add uuid to Index's toString</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17009</link><project id="" key="" /><description>This is useful because uuid is starting to matter more and more in index
operations so it is nice that the uuid appears in the logs.
</description><key id="139319110">17009</key><summary>Add uuid to Index's toString</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T16:15:11Z</created><updated>2016-03-08T17:59:27Z</updated><resolved>2016-03-08T16:57:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-03-08T16:17:22Z" id="193844843">LGTM
</comment><comment author="nik9000" created="2016-03-08T16:57:45Z" id="193866546">Thanks @dakrone ! Merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log when cancelling allocation of a replica because a new syncid was found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17008</link><project id="" key="" /><description>Currently the message stays in the `UnassignedInfo` for the shard,
however, it would be very useful to know the exact point (time-wise)
that the cancellation happened when diagnosing an issue.

Relates to debugging #16357
</description><key id="139319078">17008</key><summary>Log when cancelling allocation of a replica because a new syncid was found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>non-issue</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T16:15:03Z</created><updated>2016-03-08T16:21:31Z</updated><resolved>2016-03-08T16:21:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-08T16:15:59Z" id="193844425">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for IPv6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17007</link><project id="" key="" /><description>Lucene now has sandbox support for IPv6 ([LUCENE-7043](https://issues.apache.org/jira/browse/LUCENE-7043)).  

&gt; InetAddressPoint is 1-dimensional by nature: of course you can have multiple values per field, thats different. Since we prefix-compress values, we can just map IPv4 addresses to IPv6 space and it works for both types. This is consistent with what InetAddress does itself anyway.

We should look at what needs to be done to support IPv6 in Elasticsearch. We should add a new `ipv6` field which handles both IPv4 and IPv6.  

The only problem I can see is that users might expect to get an IPv4 address back (eg `192.168.1.5`) and instead it'd be returned as `0:0:0:0:0:ffff:c0a8:105`. Perhaps we should have an `ipv4` field as well that handles only IPv4 addresses, or add some option to render as IPv4 when possible?
</description><key id="139290952">17007</key><summary>Support for IPv6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>discuss</label><label>feature</label></labels><created>2016-03-08T14:27:13Z</created><updated>2016-04-14T15:57:09Z</updated><resolved>2016-04-14T15:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-08T14:32:11Z" id="193804696">&gt; The only problem I can see is that users might expect to get an IPv4 address back (eg 192.168.1.5) and instead it'd be returned as 0:0:0:0:0:ffff:c0a8:105. Perhaps we should have an ipv4 field as well that handles only IPv4 addresses, or add some option to render as IPv4 when possible?

That is not true. This is only something that happens internally in a BKD tree. Users do not see it. You do not "get things back" from that API. 

Also note: if you follow the RFC correctly, then calling https://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html#getByAddress%28byte[]%29 with a 128-bit mapped IPv4 address returns back a 32-bit Inet4Address: it does this round tripping for you. That is why all Query.toString()'s and so on work intuitively with the lucene support.
</comment><comment author="jpountz" created="2016-03-08T14:34:34Z" id="193805266">While we don't have this issue with the bkd tree, I think it's something we will have to think about for doc values, eg. for computing the top ip addresses that hit a web server.
</comment><comment author="rmuir" created="2016-03-08T14:36:42Z" id="193806101">its really not at all. just encode the same way, and use InetAddress to decode for any presentation form.
</comment><comment author="rmuir" created="2016-03-08T14:38:04Z" id="193806742">Its this part that is super-important: calling https://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html#getByAddress%28byte[]%29 with a 128-bit mapped IPv4 address returns back a 32-bit Inet4Address: it does this round tripping for you

so if you want to display to the user in any way, make an InetAddress from the bytes. it does the right thing. 

Please don't make ipv4 vs ipv6 types or any other craziness when this all works exactly as the user expects using basic java apis!
</comment><comment author="jpountz" created="2016-03-08T14:57:33Z" id="193814104">I agree there should not be two types and I'm not concerned about the Java API. The question was more about what we do on the rest layer eg. is it ok for aggs to render ip addresses differently from the way they were formatted at index time. I think it is, but it did not happen with the v4-only field so I think it's also important that we raise the question and document the behaviour.
</comment><comment author="rmuir" created="2016-03-08T14:59:25Z" id="193814657">I don't think you guys get how the transition mechanism works inside the java InetAddress class. I encourage you to play with it.
</comment><comment author="clintongormley" created="2016-03-08T15:05:11Z" id="193816265">&gt; Please don't make ipv4 vs ipv6 types or any other craziness when this all works exactly as the user expects using basic java apis!

Relax, Rob.  The only reason I suggested it was because I was unaware of the information that you have provided. Thanks for the link.

Isn't it true though that if a user indexes an IPv4 address in IPv6 format, doc values will return the address in IPv4 format? (ie not what they indexed?). 
</comment><comment author="rmuir" created="2016-03-08T15:09:45Z" id="193817660">If someone supplies an ipv4 address (mapped or not), its an ipv4 address. thats what that reserved space in the ipv4 region is for. If someone supplies an ipv6 address, its an ipv6 address.

note that ipv6 addresses can be represented in many ways. i recommend emitting canonical representations. So if someone supplies `0:0:0:0:0:0:0:1` they can expect to get it back as `::1`.

Its always "what they indexed". its the same address. Its like returning `16.0` vs returning `16`. 

Please, lets not try to overcomplicate this. Its not complicated.
</comment><comment author="clintongormley" created="2016-03-08T15:11:25Z" id="193818171">&gt; Please, lets not try to overcomplicate this. Its not complicated.

I wasn't trying to overcomplicate it, I was trying to understand it.  Thank you for explaining.
</comment><comment author="rmuir" created="2016-03-08T15:14:51Z" id="193819173">as far as docvalues, i would just use SortedSetDocValues class for internet addresses. This is critical for ipv6 data because (like the BKD tree) it will provide prefix compression. 

For network addresses, "range" operations are a common operation (e.g. cidr/prefix operations), so having ordinal indirection is not useless, and can speed things up.
</comment><comment author="rmuir" created="2016-03-08T15:21:36Z" id="193822059">And for that to work, docvalues needs a little tweak before the compression will happen: we currently always 'bail' on prefix-compression for fixed-length data: https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer.java#L423-L425

that is not always the best tradeoff...
</comment><comment author="rmuir" created="2016-03-08T15:31:12Z" id="193824917">We may even want to turn that logic off completely. basically that logic was originally added to "save space" (= save on addresses) before we had prefix compression at all...

Unfortunately, we really need to solve this to avoid "my ip address field quadrupled in size!"
</comment><comment author="rmuir" created="2016-03-08T15:44:00Z" id="193829306">I opened https://issues.apache.org/jira/browse/LUCENE-7081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for BigInteger and BigDecimal</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17006</link><project id="" key="" /><description>Lucene now has sandbox support for BigInteger ([LUCENE-7043](https://issues.apache.org/jira/browse/LUCENE-7043)), and hopefully BigDecimal will follow soon.  We should look at what needs to be done to support them in Elasticsearch.

I propose adding `big_integer` and `big_decimal` types which have to be specified explicitly - they shouldn't be a type which can be detected by dynamic mapping.

Many languages don't support big int/decimal. Javascript will convert to floats or throw an exception if a number is out of range.  This can be worked around by always rendering these numbers in JSON as strings. We can possibly accept known bigints/bigdecimals as numbers but there are a few places where this could be a problem:
- indexing a known big field (do we know ahead of time to parse a floating point as a BigDecimal?)
- dynamic mapping (a floating point number could have lost precision before the field is defined as big_decimal)
- ingest pipeline (ingest doesn't know about field mappings)

The above could be worked around by [telling Jackson](https://github.com/FasterXML/jackson-databind/wiki/Deserialization-Features) to parse floats and ints as BIG\* (`USE_BIG_DECIMAL_FOR_FLOATS` and `USE_BIG_INTEGER_FOR_INTS`) but this may well generate a lot of garbage for what is an infrequent use case.

Alternatively, we could just say that Big\* should always be passed in as strings if they are to maintain their precision.
</description><key id="139287123">17006</key><summary>Support for BigInteger and BigDecimal</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>discuss</label><label>feature</label><label>Meta</label></labels><created>2016-03-08T14:10:32Z</created><updated>2017-07-04T08:10:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-08T16:28:59Z" id="193850489">One thing to note here is that our points support is for fixed-width types. 

In other words, the BigIntegerPoint in lucene is a little misleading, it does not in fact support "Immutable arbitrary-precision integers".

Instead its a signed 128-bit integer type, more like a `long long`. If you try to give it a too-big BigInteger you get an exception! But otherwise BigInteger is a natural api for the user to provide a 128-bit integer.

On the other hand, If someone wanted to add support for a 128-bit floating point type, its of course possible, but I have my doubts there if BigDecimal is even the right java api around that (BigDecimal is a very different thing than a quad-precision floating point type).

I already see some confusion (e.g. "lossless storage") referenced to the issue so I think its important to disambiguate a little. 

Maybe names like BigInteger/BigDecimal should be avoided with these, but thats part of why the thing is in sandbox, we can change that (e.g. to LongLongPoint).
</comment><comment author="clintongormley" created="2016-03-08T17:57:37Z" id="193891190">thanks for the heads up @rmuir - i was indeed unaware of that
</comment><comment author="jpountz" created="2016-04-14T15:55:52Z" id="210016114">I'd like to collect more information about use-cases before we start implementing this type. For instance I think the natural decision would be to use `SORTED_SET` doc values, but if the main use-case is to run stats aggregations, this won't work and so the fact that we have a `long long` type will probably be confusing since users won't be able to run the operations that they expect to work.
</comment><comment author="rmuir" created="2016-04-17T11:12:11Z" id="211001495">I agree: we did some digging the other day.

One cause of confusion is many databases have a `bigint` type which is really a 64-bit long! So I'm concerned about people using a too-big type when its not needed due to naming confusion.

Also we have the challenge of how such numbers would behave in e.g. scripting and other places. Personally, i've only used BigInteger for cryptography-like things. You can see from its API its really geared at that. So maybe its not something we should expose?
</comment><comment author="ravicious" created="2016-04-17T18:36:02Z" id="211077747">@jpountz:

&gt; I'd like to collect more information about use-cases before we start implementing this type. For instance I think the natural decision would be to use SORTED_SET doc values, but if the main use-case is to run stats aggregations, this won't work (&#8230;)

Sorry for my newb questions, but why wouldn't this work? Aren't stats aggregations done with floats possibly inaccurate due to floating point arithmetics?
</comment><comment author="jpountz" created="2016-04-17T21:55:14Z" id="211121303">They can be inaccurate indeed.

The point I was making above is that Lucene provides two ways to encode doc values. On the one hand, we have `SORTED_SET`, which assigns an ordinal to every value per segment. This way you can efficiently sort and run terms, cardinality or range aggregations since these operations can work directly on the ordinals. However the cost of resolving a value given an ordinal is high enough that it would make anything that needs to have access to the actual values slow, such as a stats aggregation, just because it needs access to the actual values. On the other hand, there is `BINARY`, which just encodes the raw binary values in a column stride fashion. This would be slower for sorting and terms/cardinality/range aggregations, but reading the original values would be faster than with `SORTED_SET` so we could theoretically run eg. stats aggregations or use the values in scripts.

So knowing about the use-cases will help figure out which format to use. But then if we want to leverage all 128 bits of the values, we will have to duplicate implementations for everything that needs to add or multiply values such as stats/sum/avg aggregations. This would be an important burden in terms of maintenance so we would certainly not want to go that route without making sure that there are valid/common use-cases for it first.
</comment><comment author="devgc" created="2016-07-29T14:33:39Z" id="236196635">This feature would be useful for the Digital Forensics and Indecent Response (DFIR) community. There are lots of data structures we look at that have uint64 types. When we index these, if the field is considered a long and the value is out of range, information can be lost.
</comment><comment author="rmuir" created="2016-07-29T14:53:21Z" id="236202156">I see a 64 bit unsigned integer type (versus the 64-bit signed type we have), as a separate feature actually. This can be implemented more efficiently with lucene (and made easier with java 8).

Yeah, figuring out how to make a 64-bit unsigned type work efficiently in say, the scripting API might be a challenge as it stands today. Perhaps it truly must be a Number backed by BigInteger to work the best today, which would be slower. 

But in general, typical things such as ranges and aggregations would be as fast as the 64-bit signed type we have today, and perhaps a newer scripting api (with more type information) could make scripting faster too down the road, so it is much more compelling than larger integers (e.g. 128-bit), which will always be slower.

Use cases where BigInteger is truly needed, to me that situation is less clear. I would like for us to consider the two cases (64-bit unsigned vs larger integers) as separate.
</comment><comment author="jeffknupp" created="2016-09-24T16:52:34Z" id="249375015">@rmuir it's surprising to me that you have to ask for cases where `BigDecimal` (i.e. a decimal representation with arbitrary precision) would be needed, as much data science/analytics work requires exact representations of the source data without loss of precision. If putting my data into ES means that I am necessarily going to lose precision, that's a non-starter for _many_ uses. Nothing in the JSON spec suggests this. In fact, it expressly mentions that numerics are arbitrary precision and it is up to the various libraries to represent that properly.
</comment><comment author="jasontedor" created="2016-09-24T21:07:58Z" id="249387707">&gt; Nothing in the JSON spec suggests this. In fact, it expressly mentions that numerics are arbitrary precision and it is up to the various libraries to represent that properly.

This is not correct; the [spec says](https://tools.ietf.org/html/rfc7159#section-6):

&gt; This specification allows implementations to set limits on the range and precision of numbers accepted.

You are correct that numerics in the JSON spec are arbitrary precision, but nothing in the spec suggests that implementations must support this and, in fact, implementations do not have to support this.

The spec further says:

&gt; Since software that implements IEEE 754-2008 binary64 (double precision) numbers [[IEEE754]](https://tools.ietf.org/html/rfc7159#ref-IEEE754) is generally available and widely used, good interoperability can be achieved by implementations that expect no more precision or range than these provide, in the sense that implementations will approximate JSON numbers within the expected precision.
</comment><comment author="jeffknupp" created="2016-09-26T13:51:15Z" id="249574997">@jasontedor I was referring to [ECMA-404](http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf) but regardless, my point is that the elastic documentation specifically says that `_source`, for example, contains the original JSON message verbatim and is used for search results. I think you'd have to heavily amend statements like that in the documentation to explicitly describe how JSON numbers are handled internally in ES.

You also cut your quoting of the spec short, as the entire paragraph is:

&gt; This specification allows implementations to set limits on the range
&gt;    and precision of numbers accepted.  Since software that implements
&gt;    IEEE 754-2008 binary64 (double precision) numbers [IEEE754] is
&gt;    generally available and widely used, good interoperability can be
&gt;    achieved by implementations that expect no more precision or range
&gt;    than these provide, in the sense that implementations will
&gt;    approximate JSON numbers within the expected precision.  A JSON
&gt;    number such as 1E400 or 3.141592653589793238462643383279 may indicate
&gt;    potential interoperability problems, since it suggests that the
&gt;    software that created it expects receiving software to have greater
&gt;    capabilities for numeric magnitude and precision than is widely
&gt;    available.

This is exactly what I'm referring to, as "the software that created it" (i.e. a client) has no reason to suspect, based on the documentation, that either of these values would lose precision.
</comment><comment author="jpountz" created="2016-09-26T21:32:03Z" id="249703962">@jeffknupp 

&gt; it's surprising to me that you have to ask for cases where BigDecimal would be needed

We are asking for use-cases because depending on the expectations, the feature could be implemented in very different ways.

For instance a MySQL `BIGINT` is just a 64-bits integer, which we already support with the `long` type. We do not support unsigned numbers, but if that is a common need, then this could be something we could fix and support efficiently.

If the use-case requires more than 64 bits (eg. 128), then things are more complicated. We could probably support efficient sorting, but aggregations would be tricky.

If arbitrary precision is needed, then there is not much we can do efficiently, at least at the moment.
</comment><comment author="jasontedor" created="2016-09-26T21:39:45Z" id="249705801">&gt; I was referring to [ECMA-404](http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf)

The JSON spec only spells out the representation in JSON which is used for interchange, it is completely agnostic to how such information is represented by software consuming such JSON.

&gt; I think you'd have to heavily amend statements like that in the documentation to explicitly describe how JSON numbers are handled internally in ES.

The documentation spells out the [numeric datatypes](https://www.elastic.co/guide/en/elasticsearch/reference/current/number.html#number) that are supported.
</comment><comment author="devgc" created="2016-10-07T17:56:19Z" id="252318741">Here is a good example.

Windows uses the [USN Journal](https://msdn.microsoft.com/en-us/library/windows/desktop/aa363798%28v=vs.85%29.aspx) to record changes made to the file system. These records are extremely important "logs" for people in the DFIR community.

[Version 2](https://msdn.microsoft.com/en-us/library/windows/desktop/aa365722%28v=vs.85%29.aspx) records uses 64 bit unsigned integer to store reference numbers.

[Version 3](https://msdn.microsoft.com/en-us/library/windows/desktop/hh802708%28v=vs.85%29.aspx) records uses 128-bit ordinal number for reference numbers.

&gt; For instance a MySQL BIGINT is just a 64-bits integer, which we already support with the long type. We do not support unsigned numbers, but if that is a common need, then this could be something we could fix and support efficiently.

I would say that this is important for the DFIR community.

&gt; If the use-case requires more than 64 bits (eg. 128), then things are more complicated. We could probably support efficient sorting, but aggregations would be tricky.

I would say this is equally as important. 

There are many other logs that record these references, thus by maintaining their native types we can correlate logs to determine certain types of activity.
</comment><comment author="devgc" created="2016-10-07T17:57:56Z" id="252319143">&gt; Use cases where BigInteger is truly needed, to me that situation is less clear. I would like for us to consider the two cases (64-bit unsigned vs larger integers) as separate.

Should we go ahead and create a new issue for 64-bit unsigned type as a feature?
</comment><comment author="marcurdy" created="2016-10-07T18:45:25Z" id="252330611">&gt; For instance a MySQL BIGINT is just a 64-bits integer, which we already support with the long type. We do not support unsigned numbers, but if that is a common need, then this could be something we could fix and support efficiently.

I'm also in the digital forensics world and see merit in providing a 64-bit unsigned type.  If it were 128-bit with a speed impact, it wouldn't affect the way in which I process data.  My use is less real time and more one time run bulk processing.  The biggest factor to me would be what makes the most sense from the developer side in respect to java and OS integration.
</comment><comment author="tezcane" created="2016-10-22T22:37:24Z" id="255558145">Spring Data JPA supports BigInteger and BigDecimal, so any code where you try also use elasticsearch with will fail:

```
/**  Spring Data ElasticSearch repository for the Task entity.  */
public interface TaskSearchRepository extends ElasticsearchRepository&lt;Task,BigInteger&gt; {
     //THIS COMPILES BUT FAILS ON INIT
}

/**  Spring Data JPA repository for the Task entity. */
@SuppressWarnings("unused")
public interface TaskRepository extends JpaRepository&lt;Task,BigInteger&gt; {
    //THIS IS OK
}
```

I think a hack (that may end up being almost as efficient) is to convert my BigInteger to a string for use with elasticsearch:

```
/**  Spring Data ElasticSearch repository for the Task entity.  */
public interface TaskSearchRepository extends ElasticsearchRepository&lt;Task,String&gt; {
     //HACK, convert biginteger to string when saving to elasticsearch...
}
```

So these data types should be added in my opinion.
</comment><comment author="niemyjski" created="2016-11-04T23:52:51Z" id="258574677">We also need something like this, We are unable to store C#'s [Decimal.MaxValue](https://msdn.microsoft.com/en-us/library/system.decimal.maxvalue%28v=vs.110%29.aspx) currently.
</comment><comment author="jordansissel" created="2017-04-03T03:57:15Z" id="291043751">On use cases, I see DFIR and USN mentioned. Would either of these use cases use aggregations, or just search and sorting? If you see aggregations necessary, can you state which ones and what the use case for that is?

Apologies if I am oversimplifying, but it seems like:

* For USN, searching for individual USN and also sorting is desired (for viewing the journal in the correct order).
* For DFIR, the category is too broad for me to really speculate, but I wonder if search-and-sort is enough?

If search and sort is enough, and no aggregations are needed, I wonder if there is even need for a 128 bit numeric type-- could strings be enough for these use cases, even if they may have speed differences from a (theoretical) 128bit type?</comment><comment author="jordansissel" created="2017-04-03T03:57:40Z" id="291043789">(Oops, clicked the wrong button. Reopened.)</comment><comment author="jpountz" created="2017-04-03T07:37:33Z" id="291069523">&gt; If search and sort is enough, and no aggregations are needed, I wonder if there is even need for a 128 bit numeric type-- could strings be enough for these use cases, even if they may have speed differences from a (theoretical) 128bit type?

If the workload consists of exact search and sort (no aggs), then strings are the way to go indeed. The reason why I am interested in the workload is that Lucene provides better ways to index the data if range queries are important, but this only works with fixed-size data up to 16 bytes / 128 bits.</comment><comment author="philhagen" created="2017-04-03T22:31:55Z" id="291293336">I second the DFIR-related cases and add that we are generally beholden to the data types present in our evidence - as more operating systems and applications move to use and store larger values, we must adapt.  Truncating or throwing away information because of data type issues is dangerous.

A corollary use case has been identified in a number of forensic tools that can not parse emoji or other unicode characters.  Another is in lack of IP address functionality for IPv6 addresses.  If a tool parsing our source data cannot support those types, we lose critical context and content.

Given the incredibly valuable use case Elastic and friends have in the DFIR world, staying ahead of our source data is very important.  I hope to see these data types included in the near future.</comment><comment author="jordansissel" created="2017-04-03T22:58:32Z" id="291301928">@philhagen Thanks for the information. I have some questions (similar to what I commented on above)

What data are you storing? What operations are you doing on this data?

If the operations are search-and-sort, you can achieve this *today* with no changes to Elasticsearch by telling Elasticsearch to map your large-integers as strings. You'll get search capability and sort capability from this.

For full context, it would be helpful beyond "DFIR may use large numbers" to get more specific. What are the numbers you need to store, what are the properties and semantics of these numbers, and what operations are you needing to do across a large set of these numbers?</comment><comment author="danzek" created="2017-04-03T23:14:10Z" id="291308708">**What data are you storing?** Here's a couple examples of data the DFIR community frequently stores in Elasticsearch.

 - Microsoft Windows stores [FILETIME timestamps](https://msdn.microsoft.com/en-us/library/windows/desktop/ms724284(v=vs.85).aspx) as 100-nanosecond intervals since January 1, 1601 (UTC) in a [ULARGE_INTEGER](https://msdn.microsoft.com/en-us/library/windows/desktop/aa383742(v=vs.85).aspx) (64-bit unsigned integer value).

 - The Windows [update sequence number (USN) change journal struct](https://msdn.microsoft.com/en-us/library/windows/desktop/mt684964(v=vs.85).aspx) contains a [FILE_ID_128 struct](https://msdn.microsoft.com/en-us/library/windows/desktop/hh965605(v=vs.85).aspx) that holds a 128-bit file identifier.</comment><comment author="philhagen" created="2017-04-03T23:40:33Z" id="291320587">@jordansissel sure thing - always happy to help!
The store-as-string solution would be viable, but we often do range-based searches against the numbers (data transferred between x and y bytes), the 100ns-interval timestamps @danzek describes searched after &lt;y&gt; time, etc.
I could see the USN journals being a great candidate for storing as strings, however.  I can't (personally) think of a case where we'd search for those as a range, for example - but the challenge is that often a forensicator approaches a problem differently than anyone has before, resulting in great advancements in the field.

I fully acknowledge that a good deal of this use case is hand-wavy and that this is not as helpful as a hard-core use case.  All I can think of is that when source data is typed a certain way, tools that become the best and the most used/loved are those that accommodate the new data types.</comment><comment author="jordansissel" created="2017-04-04T00:19:36Z" id="291337805">I'm gonna close my eyes and hand-wave a USN suggestion as a workaround: that today's USN date has a set number of digits that won't add a new digit for a while (I haven't done the math, but it should be on the order of years), so relative range queries on USN-as-string should be OK given they all have the same [hand waving intensifies] number of digits?

(This comment is partly to suggest a specific workaround and partly for humor)</comment><comment author="philhagen" created="2017-04-04T01:54:28Z" id="291370471">I appreciate both workarounds and humor! :)

You're correct - for that particular value, the first digits would be sufficient (and, I've taken to this exact approach with a few cases where the really minute detail is not critical, truncating the number at a certain number of digits).

The big challenge though is not what we're seeing TODAY, it's what we might encounter in the future. I know that doesn't really lend itself to hard-sell use cases and prioritization of these issues.  However, it's often the case that "we never needed to parse the &lt;x&gt; application log data until some dirtbag used that application to do &lt;redacted redacted redacted&gt;."  Then, handling that type of log or whatever source data becomes a great forensic process that everyone can use.

Not an easy task - but I hope we can shed some light on the potential importance for numerically handling the larger values.  In time it will become core.</comment><comment author="jonstewart" created="2017-04-06T21:24:33Z" id="292327228">Howdy, I come from the DFIR world, but know Lucene well so maybe can bridge the divide a bit. ES is quickly becoming a handy tool as we deal with a variety of structures, ever changing, and it's usually not too hard to convert them to json and ingest into ES for searching and sorting.

Many of these structures come from the filesystem. There we often encounter 64-bit and sometimes 128-bit integers. These can be inode-like file identifiers or file offsets. Most of the time these integers will be ordinal, so a varint encoding makes sense&#8212;the number may be bigger than 2^32 but rarely uses the full 64 bits (let alone 128 bits). Still, we are pedantic people and encounter really weird data from time to time that can make a mockery of attempts to treat 64 uints like 64 bit signed ints, etc. There may be other times the numbers could effectively be hashes/randomized, but those cases are more rare.

I am hard-pressed to think of a need for aggregations on such fields, with the exception of a "file size". However, the distribution would be heavily skewed to sizes &lt; 2^32, so "&gt; 4GB" could always be an option.

We also encounter weird timestamp formats (like the aforementioned NTFS FILETIME, that's 100ns increments since 1601; but there's Apple's Absolute Time, the number of seconds since 2001, usually stored as a double). With these timestamps, sometimes it's nice to be able to represent the timestamp as it was (an integer, a double, a string), but what is badly needed is an arbitrary precision timestamp type. This would ensure we could normalize the variety of timestamps we encounter so we could compare apples to apples, but without losing precision. In fact, when a high precision timestamp looks like it's been truncated (has had some fractional part zeroed), that is usually a mark of manipulation, which is very interesting to us.

The unfortunate news is that with an arbitrary precision timestamp, we'd need the usual array of aggregations. Other than knowing that a class of timestamps has low precision, I don't think we have much need for high-precision aggregations. We'll want to drill down by year/month/day/day of week/time of day, etc., but I doubt we need to break things down below a second.

It's only speculation on my part, but my guess is that the scientific world also has need for high-precision timestamps.

Our use case in DFIR is heavily batch-oriented, where we usually want to ingest a bulk data set as fast as possible, and then the indices are relatively static. I don't think anyone would notice if queries had to suffer on performance in exchange for improved precision: our other tools can take days to return results, so ES/Lucene's sub-second query times seem other worldly. We wouldn't notice a few milliseconds lost there.</comment><comment author="billnbell" created="2017-04-20T17:05:41Z" id="295817025">This could also solve the range issue on UUID-4. Which I would be a huge fan of.</comment><comment author="Felk" created="2017-05-02T10:13:00Z" id="298592160">This would be really appreciated because I need to be able to save arbitrary-precision timeseries values. I'm currently working around this using strings</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.2.0 spends a lot more time in merges than 1.7.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17005</link><project id="" key="" /><description>**Elasticsearch version**:

```
# elasticsearch --version
Version: 2.2.0, Build: 8ff36d1/2016-01-27T13:32:39Z, JVM: 1.8.0_72-internal
```

**JVM version**:

```
# java -version
openjdk version "1.8.0_72-internal"
OpenJDK Runtime Environment (build 1.8.0_72-internal-b15)
OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode)
```

**OS version**: Debian Jessie on kernel 4.1.3.

**Description of the problem including expected versus actual behavior**:

I'm trying to migrate existing 1.7.3 installation to 2.2.0 and it seems that 2.2.0 has a regression with merging with large indices. To validate that observation, I ran both 1.7.3 and 2.2.0 and tried to index the same data on both to compare resource usage for both.

Test machine specs: 2 x Intel e5-2630v2 (24 threads), 128GB RAM, 128GB SSD for state, OS in RAM, 12 HDD in 6 RAID0 (striping) arrays (only 2 used for testing).

**Steps to reproduce**:
1. Run both 1.7.3 and 2.2.0 in docker containers on separate RAID0 arrays:

```
# docker run -d --net host -v /disk/data1/elasticsearch/es-1.7.3:/usr/share/elasticsearch/data -e ES_HEAP_SIZE=16g --name es-1.7.3 elasticsearch:1.7.3 -Des.http.port=9201 -Des.transport.tcp.port=9301 -Des.cluster.name=es-1.7.3 -Des.discovery.zen.ping.multicast.enabled=false -Des.index.store.throttle.type=none -Des.index.merge.scheduler.max_thread_count=4

# docker run -d --net host -v /disk/data2/elasticsearch/es-2.2.0:/usr/share/elasticsearch/data -e ES_HEAP_SIZE=16g --name es-2.2.0 elasticsearch:2.2.0 -Des.http.port=9202 -Des.transport.tcp.port=9302 -Des.cluster.name=es-2.2.0 -Des.discovery.zen.ping.multicast.enabled=false -Des.index.translog.durability=async -Des.index.translog.sync_interval=10s
```
1. Set up the same mapping to enable doc values:

```
# curl -X PUT http://127.0.0.1:9201/_template/star?pretty -d '{"template":"*","settings":{"number_of_shards":10,"number_of_replicas":1,"index.query.default_field":"request_uri","index.refresh_interval":"5s"},"mappings":{"_default_":{"_all":{"enabled":false},"dynamic_templates":[{"string_fields":{"match":"*","match_mapping_type":"string","mapping":{"type":"string","index":"not_analyzed","ignore_above":256,"doc_values":true}}},{"long_fields":{"match":"*","match_mapping_type":"long","mapping":{"type":"long","doc_values":true}}},{"double_fields":{"match":"*","match_mapping_type":"double","mapping":{"type":"double","doc_values":true}}},{"date_fields":{"match":"*","match_mapping_type":"date","mapping":{"type":"date","doc_values":true}}}],"properties":{"@timestamp":{"type":"date","doc_values":true},"@version":{"type":"long","doc_values":true}}}}}'

# curl -X PUT http://127.0.0.1:9202/_template/star?pretty -d '{"template":"*","settings":{"number_of_shards":10,"number_of_replicas":1,"index.query.default_field":"request_uri","index.refresh_interval":"5s"},"mappings":{"_default_":{"_all":{"enabled":false},"dynamic_templates":[{"string_fields":{"match":"*","match_mapping_type":"string","mapping":{"type":"string","index":"not_analyzed","ignore_above":256,"doc_values":true}}},{"long_fields":{"match":"*","match_mapping_type":"long","mapping":{"type":"long","doc_values":true}}},{"double_fields":{"match":"*","match_mapping_type":"double","mapping":{"type":"double","doc_values":true}}},{"date_fields":{"match":"*","match_mapping_type":"date","mapping":{"type":"date","doc_values":true}}}],"properties":{"@timestamp":{"type":"date","doc_values":true},"@version":{"type":"long","doc_values":true}}}}}'
```
1. Take 135M / 800M (114GB) docs from existing index on live cluster and reindex them on both test instances. Source index has the same mapping as destination indices, so 10 shards, 1 replica, doc values. Scroll gives 10k packs of docs, we index them in single bulk request per pack.

```
# docker run -it --net host --name es-1.7.3-index bobrik/esreindexer -src http://myhost/myindex-2016.03.07 -dst http://127.0.0.1:9201/myindex -pack 1000 -pool 5 -query '{"query_string":{"query":"hostname:myhost"}}'

# docker run -it --net host --name es-2.2.0-index bobrik/esreindexer -src http://myhost/myindex-2016.03.07 -dst http://127.0.0.1:9202/myindex -pack 1000 -pool 5 -query '{"query_string":{"query":"hostname:myhost"}}'
```

Expected results: both 1.7.3 and 2.2.0 have roughly the same time spent on CPU and in merging.

Actual results:

**1.7.3**:

Indexing time: 2h34m6s `2016/03/08 11:05:00 .. 2016/03/08 13:39:06`

Index stats from `_cat` API right after indexing:

```
index   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time 
myindex  10  133575122            0         23.5gb            262             5.2m                6.3h              1.2h        24.5m                0s           0s 
```

Time to optimize down to 2 segments per shard:

```
ivan@36s10:~$ time curl -s 'http://127.0.0.1:9201/myindex/_optimize?max_num_segments=2&amp;pretty'
{
  "_shards" : {
    "total" : 20,
    "successful" : 10,
    "failed" : 0
  }
}

real    13m37.409s
user    0m0.023s
sys 0m0.014s
```

Index stats from `_cat` API right after optimizing:

```
index   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time 
myindex  10  133575122            0         20.5gb             20             5.9m                6.3h              1.4h        24.5m                0s           0s 
```

Time spent on CPU for the time of test, including optimizing (from cgroup): 2889721 user, 136150 system.

**2.2.0**:

Indexing time: 2h31m33s `2016/03/08 11:05:00 .. 2016/03/08 13:36:33`

Index stats from `_cat` API right after indexing:

```
index   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time 
myindex  10  133575122            0         19.2gb            255             5.9m                6.9h              2.1h        26.2m                0s           0s 
```

Time to optimize down to 2 segments per shard:

```
# time curl -s 'http://127.0.0.1:9202/myindex/_optimize?max_num_segments=2&amp;pretty'
{
  "_shards" : {
    "total" : 20,
    "successful" : 10,
    "failed" : 0
  }
}

real    10m22.218s
user    0m0.018s
sys 0m0.010s
```

Index stats from `_cat` API right after optimizing:

```
index   pri docs.count docs.deleted pri.store.size segments.count flush.total_time indexing.index_time merges.total_time refresh.time search.query_time suggest.time 
myindex  10  133575122            0         19.2gb             20               6m                6.9h              2.2h        26.2m                0s           0s 
```

Time spent on CPU for the time of test, including optimizing (from cgroup): 3022558 user, 118353 system.

**Provide logs (if relevant)**:

**1.7.3**:

```
[2016-03-08 11:02:29,631][INFO ][node                     ] [Ramrod] version[1.7.3], pid[1], build[05d4530/2015-10-15T09:14:17Z]
[2016-03-08 11:02:29,632][INFO ][node                     ] [Ramrod] initializing ...
[2016-03-08 11:02:29,692][INFO ][plugins                  ] [Ramrod] loaded [], sites []
[2016-03-08 11:02:29,723][INFO ][env                      ] [Ramrod] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/md127)]], net usable_space [6.8tb], net total_space [7.2tb], types [ext4]
[2016-03-08 11:02:31,704][INFO ][node                     ] [Ramrod] initialized
[2016-03-08 11:02:31,704][INFO ][node                     ] [Ramrod] starting ...
[2016-03-08 11:02:31,825][INFO ][transport                ] [Ramrod] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/1.2.3.4:9301]}
[2016-03-08 11:02:31,833][INFO ][discovery                ] [Ramrod] es-1.7.3/eVU3RXfXRvqDPvNoaxIpKQ
[2016-03-08 11:02:34,849][INFO ][cluster.service          ] [Ramrod] new_master [Ramrod][eVU3RXfXRvqDPvNoaxIpKQ][36s10][inet[/1.2.3.4:9301]], reason: zen-disco-join (elected_as_master)
[2016-03-08 11:02:34,889][INFO ][http                     ] [Ramrod] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/1.2.3.4:9201]}
[2016-03-08 11:02:34,889][INFO ][node                     ] [Ramrod] started
[2016-03-08 11:02:38,239][INFO ][gateway                  ] [Ramrod] recovered [0] indices into cluster_state
[2016-03-08 11:05:01,962][INFO ][cluster.metadata         ] [Ramrod] [myindex] creating index, cause [auto(bulk api)], templates [star], shards [10]/[1], mappings [_default_, access-logs]
[2016-03-08 11:05:06,643][INFO ][cluster.metadata         ] [Ramrod] [myindex] update_mapping [access-logs] (dynamic)
```

**2.2.0**:

```
[2016-03-08 11:02:39,101][INFO ][node                     ] [Shinobi Shaw] version[2.2.0], pid[1], build[8ff36d1/2016-01-27T13:32:39Z]
[2016-03-08 11:02:39,102][INFO ][node                     ] [Shinobi Shaw] initializing ...
[2016-03-08 11:02:39,502][INFO ][plugins                  ] [Shinobi Shaw] modules [lang-expression, lang-groovy], plugins [], sites []
[2016-03-08 11:02:39,522][INFO ][env                      ] [Shinobi Shaw] using [1] data paths, mounts [[/usr/share/elasticsearch/data (/dev/md126)]], net usable_space [6.8tb], net total_space [7.2tb], spins? [possibly], types [ext4]
[2016-03-08 11:02:39,522][INFO ][env                      ] [Shinobi Shaw] heap size [15.8gb], compressed ordinary object pointers [true]
[2016-03-08 11:02:41,255][INFO ][node                     ] [Shinobi Shaw] initialized
[2016-03-08 11:02:41,255][INFO ][node                     ] [Shinobi Shaw] starting ...
[2016-03-08 11:02:41,418][INFO ][transport                ] [Shinobi Shaw] publish_address {1.2.3.4:9302}, bound_addresses {[::]:9302}
[2016-03-08 11:02:41,426][INFO ][discovery                ] [Shinobi Shaw] es-2.2.0/Gg_471eJRt2lrNM9Tf3bBA
[2016-03-08 11:02:44,447][INFO ][cluster.service          ] [Shinobi Shaw] new_master {Shinobi Shaw}{Gg_471eJRt2lrNM9Tf3bBA}{1.2.3.4}{1.2.3.4:9302}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2016-03-08 11:02:44,482][INFO ][http                     ] [Shinobi Shaw] publish_address {1.2.3.4:9202}, bound_addresses {[::]:9202}
[2016-03-08 11:02:44,482][INFO ][node                     ] [Shinobi Shaw] started
[2016-03-08 11:02:47,545][INFO ][gateway                  ] [Shinobi Shaw] recovered [0] indices into cluster_state
[2016-03-08 11:05:03,030][INFO ][cluster.metadata         ] [Shinobi Shaw] [myindex] creating index, cause [auto(bulk api)], templates [star], shards [10]/[1], mappings [_default_, access-logs]
[2016-03-08 11:05:07,886][INFO ][cluster.routing.allocation] [Shinobi Shaw] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[myindex][9], [myindex][9]] ...]).
[2016-03-08 11:05:07,946][INFO ][cluster.metadata         ] [Shinobi Shaw] [myindex] update_mapping [access-logs]
```

How bad does it get on real clusters for daily indices?
- 400M docs: 19.2h (on 1.7.3, optimized) -&gt; 1.6d (on 2.2.0, optimized)
- 3B docs: 12.2d (on 1.7.3, optimized) -&gt; 23.8d (on 2.2.0, not even optimized!)

It seems that the bigger the index, the bigger the difference.
</description><key id="139286869">17005</key><summary>Elasticsearch 2.2.0 spends a lot more time in merges than 1.7.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2016-03-08T14:09:14Z</created><updated>2016-03-31T18:21:11Z</updated><resolved>2016-03-09T16:18:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T17:52:57Z" id="193889516">The only thing I can think of here is the fact that norms are now stored on disk.  @mikemccand any other changes that may have had an impact?
</comment><comment author="mikemccand" created="2016-03-08T19:47:16Z" id="193939304">It's likely auto IO throttling, new in ES 2.0?

In ES 1.7.x we default to 20 MB/sec fixed merge throttling.

But ES 2.0, auto IO throttling starts at 20 MB/sec merge but will lower that as low as 5 MB/sec if merges are keeping up OK.
</comment><comment author="clintongormley" created="2016-03-09T13:44:39Z" id="194302112">Ah thanks @mikemccand.  Yes I see the actual indexing time is lower in 2.2, even though we're also writing norms.

Wondering if we should remove the 20 MB/s throttling if we detect the user is on SSDs?
</comment><comment author="mikemccand" created="2016-03-09T14:42:53Z" id="194324498">&gt; Wondering if we should remove the 20 MB/s throttling if we detect the user is on SSDs?

Hmmm I don't think we should make such a big change for ES 1.x?

Also, Lucene didn't add its `IOUtils.spins` method until its 5.x releases I think.
</comment><comment author="clintongormley" created="2016-03-09T16:18:56Z" id="194371779">Agreed. thanks @mikemccand 
</comment><comment author="KlavsKlavsen" created="2016-03-31T08:25:46Z" id="203817131">Interesting issue.. what settings should one set in 2.2 to avoid it lowering merge speed (and hence avoid hitting this issue as well) ?
</comment><comment author="clintongormley" created="2016-03-31T18:21:11Z" id="204063717">This issue isn't a problem.  Merges are happening more slowly, meaning that they aren't under pressure, and they're leaving more resources for search etc.  If you had too much indexing load, then merges would happen more rapidly and, if necessary, throttle indexing so that they can keep up.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Consider adding RFC822 file type (email files) parser to the Tika processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17004</link><project id="" key="" /><description>As a consequence of the clean up in the mapper attachments plugin (https://github.com/elastic/elasticsearch-mapper-attachments/issues/163) only few basic file types parsers have been kept around and for a good reason, as Tika tries to parse a very wide range of types.

I believe the .eml (rfc822) file type parser to be one of the basics and, if possible, to be added back on the list of allowed types (https://github.com/elastic/elasticsearch/blob/v2.2.0/plugins/mapper-attachments/src/main/java/org/elasticsearch/mapper/attachments/TikaImpl.java#L57-L71).

Attached is a sample email file that is using base64 already to encode the content of the file itself. With ES 2.1.2 (and mapper attachments plugin 3.1.2) using the following [gist](https://gist.github.com/astefan/3e4d357ebf472b6709af) and [taking a look at the indexed content and content_type](https://gist.github.com/astefan/ee3d71d029911adfe31e) would reveal the following:

```
            "fields": {
               "indexedBody.content_type": [
                  "message",
                  "rfc822"
               ],
               "indexedBody.content": [
                  "as",
                  "awesome",
                  "elasticsearch",
                  "is",
                  "lucene",
                  "well"
               ]
            }
```

With ES 2.2.0 and mapper attachments plugin, the same setup and query shows that the file type is not properly recognized anymore, the already base64 encoded text is not decoded properly and the .eml file is considered a simple text file:

```
            "fields": {
               "indexedBody.content_type": [
                  "1252",
                  "charset",
                  "plain",
                  "text",
                  "windows"
               ],
               "indexedBody.content": [
                  "1.0",
                  "6003.1",
                  "8",
                  "_part_0_663220121.1456846121442",
                  "alternative",
                  "base64",
                  "bcc",
                  "boundary",
.....
                  "multipart",
                  "plain",
                  "rwxhc3rpy3nlyxjjacbpcybhd2vzb21liq",
                  "subject",
                  "test",
                  "test.com",
...
               ]
            }
```

[test_mail_01.eml.txt](https://github.com/elastic/elasticsearch/files/163248/test_mail_01.eml.txt)
</description><key id="139285231">17004</key><summary>Consider adding RFC822 file type (email files) parser to the Tika processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astefan</reporter><labels><label>:Plugin Ingest Attachment</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-08T14:01:19Z</created><updated>2016-03-08T17:45:37Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T17:45:37Z" id="193885839">An email can of course contain files of any type, but we can restrict the parsing of email attachments to just known types.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread limits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17003</link><project id="" key="" /><description>The generic thread pool was previously configured to be able to create
an unlimited number of threads. The thinking is that tasks that are
submitted to its work queue must execute and should not block waiting
for a worker. However, in cases of heavy load, this can lead to an
explosion in the number of threads; this can even lead to a feedback
loop that exacerbates the problem. What is more, this can even bump into
OS limits on the number of threads that can be created.

This pull request limits the number of threads in the generic thread
pool to four times the bounded number of processors.

Accordingly, this greatly reduces the maximum number of threads that
Elasticsearch requires. This pull request also reduces the maximum
number of threads required in thbootstrap check.

Finally, a note is added to the configuration docs regarding the maximum
number of threads.
</description><key id="139284521">17003</key><summary>Thread limits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T13:57:55Z</created><updated>2016-03-10T18:34:45Z</updated><resolved>2016-03-08T14:43:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-08T14:08:24Z" id="193797305">Lgtm. 

&gt; The thinking is that tasks that are
&gt; submitted to its work queue must execute and should not block waiting
&gt; for a worker. 

For completeness - the generic thread pool has an unbound queue for this reason as well...
</comment><comment author="jasontedor" created="2016-03-08T14:17:22Z" id="193799850">Closed via 95b0a6a2cf46de2f900cc08466d9697fbb2d4e5e, 930984eb4fce446c355ba35d17fdf08d054221a6 and c0572c631db574cdcaa17413cb9376dacd1f8210.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add deprecation logging for ignore_unmapped parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17002</link><project id="" key="" /><description>This adds deprecation logging for the ignore_unmapped parameter
in FieldSortBuilder and SortParseElement respectively.

Relates to #16573 and #16910
</description><key id="139270369">17002</key><summary>Add deprecation logging for ignore_unmapped parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MaineC</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-03-08T12:51:13Z</created><updated>2016-03-10T09:20:14Z</updated><resolved>2016-03-10T09:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-03-09T09:06:08Z" id="194193800">@jpountz This add deprecation logging to 2.x as discussed in #16573 
</comment><comment author="MaineC" created="2016-03-09T13:37:53Z" id="194299040">@jpountz thanks for your comments, change got a lot smaller as a result.
</comment><comment author="jpountz" created="2016-03-10T06:48:29Z" id="194698010">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use index UUID to lookup indices on IndicesService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17001</link><project id="" key="" /><description>Today we use the index name to lookup index instances on the IndicesService
which applied to search requests but also to index deletion etc. This commit
moves the interface to expect an `Index` instance which is a &lt;name, uuid&gt; tuple
and looks up the index by uuid rather than by name. This prevents accidental modification
of the wrong index if and index is recreated or searching from the _wrong_ index in such a case.
Accessing an index that has the same name but different UUID will now result in an IndexNotFoundException.
</description><key id="139258965">17001</key><summary>Use index UUID to lookup indices on IndicesService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>resiliency</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T11:55:28Z</created><updated>2016-03-09T18:44:38Z</updated><resolved>2016-03-09T18:44:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-08T13:53:00Z" id="193792995">I don't know all the code this touches but the parts I know look fine.
</comment><comment author="bleskes" created="2016-03-09T11:32:04Z" id="194252039">I like the change. Left some minor comments here and there. I think we should simplify the concurrency aspects and code paths in IndicesService but that shouldn't stop this PR. I'll try tackle it as a follow up...
</comment><comment author="s1monw" created="2016-03-09T13:21:56Z" id="194294305">@bleskes I replied to your comments and pushed an update
</comment><comment author="bleskes" created="2016-03-09T14:24:30Z" id="194317164">OK. discussed all open comments and with the last commit I'm good. LGTM.
</comment><comment author="s1monw" created="2016-03-09T14:47:46Z" id="194326853">@bleskes I had to make some changes to the PutMapping code after the last commit, can you look again?
</comment><comment author="bleskes" created="2016-03-09T15:33:04Z" id="194348393">LGTM. I presume that failing an entire batch instead of a single request was the issue... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove NodeService injection to Discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/17000</link><project id="" key="" /><description>This was only used by the multicast plugin which is now removed... 
</description><key id="139257713">17000</key><summary>Remove NodeService injection to Discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T11:50:05Z</created><updated>2016-03-08T12:01:49Z</updated><resolved>2016-03-08T11:59:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-08T11:54:50Z" id="193750623">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moves SortParser:parse(...) to only require QueryShardContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16999</link><project id="" key="" /><description>This removes the need for accessing the SearchContext when parsing Sort elements
to queries. After applying the patch only a QueryShardContext is needed.

Relates to #15178
</description><key id="139247646">16999</key><summary>Moves SortParser:parse(...) to only require QueryShardContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-08T11:03:28Z</created><updated>2016-03-10T09:21:02Z</updated><resolved>2016-03-10T09:21:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2016-03-09T09:05:22Z" id="194193212">@cbuescher we discussed this yesterday in person, could you take a look please?
</comment><comment author="MaineC" created="2016-03-09T10:40:21Z" id="194233614">@cbuescher mentioned concerns around performance. Here's the original comment wrt. impact of moving the weight calculation from @martijnvg 

&gt; I see, right now this is tricky because of the conversion to Weight. I think we should just not do that
&gt; and change the type of Nested#innerFilter from Weight to Query. Then in Nested#innerDocs(...) we 
&gt; build the weight each time it gets invoked. This should be equal to the number of segments on a shard.
&gt; 
&gt; Creating the weight is cheap compared to what nested sorting does, so I think performance wise 
&gt; this is fine. 
&gt; 
&gt; I've tried this out locally and it seems to work (tests regarding this feature pass). The patch should be
&gt;  applied on a clean master
</comment><comment author="cbuescher" created="2016-03-09T10:40:54Z" id="194233873">@MaineC I left one question concerning potentialy increasing the number of times the query weights might get calculated with this change, other than that looks good.
</comment><comment author="MaineC" created="2016-03-09T10:42:13Z" id="194234238">Sorry for posting the answer before the question appeared.
</comment><comment author="cbuescher" created="2016-03-09T10:44:42Z" id="194235861">@MaineC almost as if you can read minds ;-) Still wondering if the calculated query weight will change on susequent invocations of Nested#innerDocs or if the first calculation could be stored somewhere internally and then reused on any subsequent call. Maybe @martijnvg can comment on this?
</comment><comment author="martijnvg" created="2016-03-09T10:57:11Z" id="194240845">&gt; Still wondering if the calculated query weight will change on susequent invocations of Nested#innerDocs

No, because will be using the same top level reader context each time it is invoked.

&gt; or if the first calculation could be stored somewhere internally and then reused on any subsequent call.

We could do that - as long as we don't reuse this `Nested` class between search requests. For example we couldn't do this on `Query` because this maybe gets cached or reused in another way between search requests.

But for now I think it is fine to keep things as they are now. I don't repeatedly computing weight here will goes issues. If it does then we can always look into reusing the weight. IMO The fact that we don't rely on SearchContext here is more important than the reuse of `Weight`.
</comment><comment author="MaineC" created="2016-03-09T13:05:48Z" id="194288000">So final verdict would be merge? ;) (Forgot to mention: Love seeing changes like this being analysed thoroughly - thank you!)
</comment><comment author="cbuescher" created="2016-03-09T13:51:46Z" id="194305092">Yes, with that explanation, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing: Simplify exception handling in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16998</link><project id="" key="" /><description>In order to test for exceptions and their content one always had to use
a fair share of boiler plate code.

```
try {
  foo.methodThrowsException();
  fail("...");
} catch (Exception e) {
  assertThat(e.getMessage(), is(""));
}
```

This can be wrapped into a lambda to shorten it like this

```
Exception e = assertThrows(Exception.class, () -&gt; foo.methodThrowsException() );
assertThat(e.getMessage(), is(""));
```

The assertion checks for wrongly or none thrown exceptions and throws an AssertionError
if that is the case.

SimpleMapperTests was changed to show how it works.
</description><key id="139209198">16998</key><summary>Testing: Simplify exception handling in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label></labels><created>2016-03-08T08:15:31Z</created><updated>2016-03-08T08:46:14Z</updated><resolved>2016-03-08T08:46:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-08T08:41:07Z" id="193664543">`expectThrows` was already added to `ESTestCase` in #16304, and can actually be removed from there now, since we now have a snapshot of Lucene 6, and LTC has it as well.
</comment><comment author="rjernst" created="2016-03-08T08:45:05Z" id="193665280">Actually it was already removed in the upgrade to Lucene 6. So just use that. :)
</comment><comment author="spinscale" created="2016-03-08T08:46:14Z" id="193665485">neat, hadnt spotted it there. Thx for the pointer! Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster/Index Shard Allocation Filtering Docs should mention you can use comma-separated values for cluster.routing.allocation.exclude.*</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16997</link><project id="" key="" /><description>To remove all ambiguity, it should be mentioned that the `cluster.routing.allocation.exclude.*` settings also accept comma-separated values in addition to a single-value or a wildcard expression. 
</description><key id="139181612">16997</key><summary>Cluster/Index Shard Allocation Filtering Docs should mention you can use comma-separated values for cluster.routing.allocation.exclude.*</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshuar</reporter><labels><label>docs</label></labels><created>2016-03-08T04:56:46Z</created><updated>2016-10-18T03:25:10Z</updated><resolved>2016-10-18T03:25:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T09:08:54Z" id="193679049">The docs say:

&gt; Assign the index to a node whose {attribute} has none of the comma-separated values.

How would you suggest improving it?  Want to send a PR?
</comment><comment author="joshuar" created="2016-03-08T21:40:44Z" id="193979789">I completely missed that because I see an example first thing using a non-pluralised term `_ip` and a single IP address, hence the assumption is it is single valued.  Maybe we should start with what values are supported, then put both the examples at the end?

Can submit a PR for this if that sounds good?
</comment><comment author="clintongormley" created="2016-03-09T13:48:11Z" id="194302918">With pleasure
</comment><comment author="joshuar" created="2016-10-18T03:25:10Z" id="254396812">Merged #20958
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve performance when retrieving only IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16996</link><project id="" key="" /><description>It would be nice if there were an API that would optimize for retrieving all IDs(only) of search results. It may be nice for a lot of reasons, but here's a good example. It would allow users to scroll to the bottom of a huge list (assuming the IDs have been cached after the original call) like in this example.
http://demos.telerik.com/kendo-ui/grid/virtualization-remote-data

Also, it would hopefully not go through the scroll API and could be just one constant download streamed to the requester. 

Another thing, can there be support for a non-json response type, something that can deserialize as fast as a DB reader? 
</description><key id="139169376">16996</key><summary>Improve performance when retrieving only IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sledgebox</reporter><labels /><created>2016-03-08T03:33:16Z</created><updated>2016-03-08T13:05:41Z</updated><resolved>2016-03-08T09:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-08T09:05:53Z" id="193676529">@sledgebox returning all the results (ids or otherwise) in a single request where "all" is a large number is very costly in a distributed system, so we won't provide an API to do that.  

I don't really understand your use case, but I think you may find the new [search_after](https://www.elastic.co/guide/en/elasticsearch/reference/master/search-request-search-after.html) functionality useful (coming in 5.0)

As far as returning just the IDs, try this:

```
GET _search?_source==false&amp;filter_path=hits.hits._id
```

&gt; Another thing, can there be support for a non-json response type, something that can deserialize as fast as a DB reader?

We won't be providing another response encoding.  Perhaps you need to look for a faster JSON library, or try out the CBOR encoding instead.
</comment><comment author="sledgebox" created="2016-03-08T13:05:41Z" id="193778101">Search_after does look useful. Is it not shorthand for something that could be done with query logic? When does 5.0 release? Can you point me to a roadmap?

I don't understand why returning all results is very costly. Can you provide a link explaining? Is it more costly than scrolling all? 

Thanks, so much.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error when trying to define an absolute path for synonyms/stopwords file.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16995</link><project id="" key="" /><description>This is the error I am getting: `reason=access denied ("java.io.FilePermission" "E:\elasticsearch\synonyms\synonyms.txt" "read")`

Now, I know the message is really specific about the problem, there's only one problem... that folder and file have literally no security restrictions, I open the file permissions to everyone, and I'm still getting the same error. 

Is this an error, or what am I doing wrong here? O.o

This is the request I am sending

```
{  
    "settings":{ 
        "analysis":{  
            "analyzer":{  
                "standard":{  
                    "tokenizer":"standard",
                    "filter":[  
                        "lowercase",
                        "asciifolding",
                        "standard",
                        "synonyms"
                    ],
                    "type":"custom"
                }
            },
            "filter":{  
                "synonyms":{  
                    "synonyms_path":"E:\\elasticsearch\\synonyms\\synonyms.txt",
                    "ignore_case":true,
                    "type":"synonym"
                }
            }
        }
    }
}
```

By the way, I'm using Windows Server 2012 here
There you have the file permissions' settings right now for the file.
&lt;img width="330" alt="screen shot 2016-03-07 at 9 27 29 pm" src="https://cloud.githubusercontent.com/assets/9121130/13589125/db20de58-e4ab-11e5-8f3c-fc8149d0fa65.png"&gt;

My ES version is 2.2.0
JVM version: 1.8.0_65
OS version: Windows Server 2012

Also, here you have the log information, just in case you found it useful:

```
[2016-03-07 21:22:16,030][DEBUG][action.admin.indices.create] [node_name] [index_name] failed to create
[index_name] IndexCreationException[failed to create index]; nested: AccessControlException[access denied ("java.io.FilePermission" "E:\elasticsearch\synonyms\synonyms.txt" "read")];
at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:360)
at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$1.execute(MetaDataCreateIndexService.java:309)
at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:45)
at org.elasticsearch.cluster.service.InternalClusterService.runTasksForExecutor(InternalClusterService.java:458)
at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:762)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.security.AccessControlException: access denied ("java.io.FilePermission" "E:\elasticsearch\synonyms\synonyms.txt" "read")
at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
at java.security.AccessController.checkPermission(AccessController.java:884)
at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
at java.io.File.isDirectory(File.java:844)
at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:82)
at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
at java.net.URL.openStream(URL.java:1038)
at org.elasticsearch.common.io.FileSystemUtils.newBufferedReader(FileSystemUtils.java:161)
at org.elasticsearch.index.analysis.Analysis.getReaderFromFile(Analysis.java:285)
at org.elasticsearch.index.analysis.SynonymTokenFilterFactory.&lt;init&gt;(SynonymTokenFilterFactory.java:64)
at sun.reflect.GeneratedConstructorAccessor68.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:828)
at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:823)
at org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:236)
at com.sun.proxy.$Proxy14.create(Unknown Source)
at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:161)
at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:66)
at sun.reflect.GeneratedConstructorAccessor30.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:887)
at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:880)
at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:159)
at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:55)
at org.elasticsearch.indices.IndicesService.createIndex(IndicesService.java:358)
... 9 more
```
</description><key id="139150518">16995</key><summary>Error when trying to define an absolute path for synonyms/stopwords file.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abrahamduran</reporter><labels /><created>2016-03-08T01:46:18Z</created><updated>2016-03-08T06:17:57Z</updated><resolved>2016-03-08T03:02:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-08T03:02:01Z" id="193578443">This is not a permissions issue, it is an AccessControlException. Elasticsearch 2.0+ runs with the java security manager, and restricts which files the application is allowed to read. Config files like synonyms should be underneath the elasticsearch config directory, which is configured to have read access.
</comment><comment author="abrahamduran" created="2016-03-08T03:20:08Z" id="193582773">So, you mean I have to copy the synonyms file under the config directory of every node?
</comment><comment author="rjernst" created="2016-03-08T06:17:57Z" id="193622820">You need the synonyms file on every node in any case. But yes, putting it in the config directory is recommended. Otherwise you can add to the policy using normal java mechanisms.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.2.0 binding to IPv6 even when IPv4 explicitly defined</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16994</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.2.0
**JVM version**:
Oracle JDK 1.8.0_66
**OS version**:
CentOS 6.7
**Description of the problem including expected versus actual behavior**:
Expected behavior: setting network.host to 0.0.0.0, or _non_loopback:ipv4_ should bind to and communicate via IPv4.
Actual behavior: ES binds to the IPv4 ports, but still communicates via IPv6.
`tcp        0      0 ::ffff:10.103.0.52:58988    ::ffff:10.103.0.118:9300    ESTABLISHED 498        655379     11143/java          
tcp        0      0 ::ffff:10.103.0.52:53290    ::ffff:10.100.21.70:9300    ESTABLISHED 498        655287     11143/java          
tcp        0      0 ::ffff:10.103.0.52:17736    ::ffff:10.100.21.48:9300    ESTABLISHED 498        655216     11143/java          
tcp        0      0 ::ffff:10.103.0.52:9300     ::ffff:10.100.21.25:39772   ESTABLISHED 498        655531     11143/java`

**Steps to reproduce**:
1. Set network.host to 0.0.0.0 or _non_loopback:ipv4_ (potentially set `master = true`)
2. Start Elasticsearch
3. Run netstat -peanut | grep java, check out all the ::ffff:(ipv4 address):port entries

**Provide logs (if relevant)**:
I haven't found any relevant logs
</description><key id="139110331">16994</key><summary>2.2.0 binding to IPv6 even when IPv4 explicitly defined</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AshtonDavis</reporter><labels /><created>2016-03-07T22:11:13Z</created><updated>2016-11-16T15:59:02Z</updated><resolved>2016-03-07T23:58:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2016-03-07T23:58:38Z" id="193514042">its not a bug: this is how java works. it uses ipv4 mapped addresses and so on.

If you bind to 0.0.0.0 its the same as binding to ::1
</comment><comment author="AshtonDavis" created="2016-03-08T00:00:51Z" id="193514535">Well that sucks. :/ Thanks.
</comment><comment author="clintongormley" created="2016-03-08T08:56:03Z" id="193669281">@ntent-ashton you can use the `:ipv4` and `:ipv6` modifiers with the "special values" listed here: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-network.html#network-interface-values
</comment><comment author="Sankardunga" created="2016-11-14T17:31:36Z" id="260402680">Is there any solution for this issue?  Still I'm struggling to find out way to bind the port only to IPv4 address.

[root@b04d2 ~]# netstat -anlp | grep 9200
tcp        0      0 ::ffff:10.207.98.10:9200    :::\*                        LISTEN      32756/java

Please advise, I appreciate your help.
Thanks
Sankar
</comment><comment author="jasontedor" created="2016-11-15T21:44:58Z" id="260779198">@Sankardunga Start the JVM with `-Djava.net.preferIPv4Stack=true` (via `ES_JAVA_OPTS`). If you have additional questions, please open a topic on the [Elastic Discourse forum](https://discuss.elastic.co).
</comment><comment author="Sankardunga" created="2016-11-16T15:59:02Z" id="260984422">Ya, got it. Thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Next scroll id and number of hits in response headers for search/scroll api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16993</link><project id="" key="" /><description>**Describe the feature**:

If the next scroll id and total number of hits in the current response were included as response headers, it would make consuming the scroll api without buffering entire responses much simpler.

This is useful when using the scroll api to retrieve a large number of documents without buffering each response from elasticsearch in memory.  At the moment, if you want to do this, you have to extract the next scroll id and determine if there are any hits in the current response while parsing the json body as a stream.  This is exactly what we're doing with the Kibana csv export functionality, but there's additional complexity in the implementation as we need to make sure to properly manage backpressure while extracting three different pieces of data from the json stream.

We don't want to buffer entire pages of responses in memory in case the documents are particularly large, but we're comfortable only buffering the response headers in memory to decide whether we need to keep scrolling after each request.  Then we just need to pipe the response body through a single transform stream to extract each hit.  I suspect the csv export implementation would be half the size with considerably less moving parts if this information were available as headers.
</description><key id="139105947">16993</key><summary>Next scroll id and number of hits in response headers for search/scroll api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">epixa</reporter><labels><label>:Scroll</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v5.4.4</label></labels><created>2016-03-07T21:52:02Z</created><updated>2017-06-27T10:28:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="epixa" created="2016-03-07T22:10:36Z" id="193478461">/cc @panda01 
</comment><comment author="clintongormley" created="2016-03-08T08:51:25Z" id="193667828">sounds like a good idea.  

One gotcha to be aware of: with `search_type: scan`, the first response you get back has no hits.  Scanning is deprecated in favour of `sort: "_doc"` which DOES return hits with the first response.
</comment><comment author="epixa" created="2016-03-08T14:54:57Z" id="193813269">Thanks for the heads up. I wasn't even aware of the `search_type: scan` feature, so I guess I started doing this recently enough to avoid the old behavior.
</comment><comment author="kubum" created="2016-03-09T22:41:10Z" id="194548223">Do you have any ideas about possible conventions for this type of response headers?
</comment><comment author="epixa" created="2016-03-10T00:42:59Z" id="194590474">I don't know if there's any precedent of hypermedia api behaviors in elasticsearch, but how about this: instead of returning scroll id and hit counts in headers, what if each request returned the next scroll/search url (including scroll id parameter) in the "link" header? http://tools.ietf.org/html/rfc5988

I've seen this same convention in other rest-like https apis, like github's: https://developer.github.com/guides/traversing-with-pagination/
</comment><comment author="epixa" created="2016-03-10T00:44:36Z" id="194590799">To clarify, in order to not return the current request's hit count as a header, I think it would need to only include the `link` header if there were actually more results to query.
</comment><comment author="clintongormley" created="2016-03-10T14:28:36Z" id="194871684">I like the idea of using the `Link:` header.  It would need to be a relative URL, but that's allowed by the spec.
</comment><comment author="epixa" created="2016-03-10T14:28:45Z" id="194871740">Actually, I gave it a little more thought, and I don't know if the `link` header is the best option for this.  The scroll id itself is actually used for _two_ different api requests: retrieving the next results and clean up, so I think it would be best if the scroll id itself were returned as header rather than built into a url.
</comment><comment author="epixa" created="2016-03-10T14:29:13Z" id="194871885">Unless we returning a clean up link as well...
</comment><comment author="epixa" created="2016-03-10T14:31:10Z" id="194872346">As an alternative, this would work _very_ nicely for us:

Response contains `ES-Scroll-Id` header if there are more results to retrieve.  It does not contain that header if there are no results.  Then we'd just grab that header whenever we see it and add a search function to the queue.
</comment><comment author="lucianogreiner" created="2016-04-12T19:45:02Z" id="209073931">+1 on this feature.

We would like to have hits counting under Http Response Headers section to be able to perfom internal tracking on requester limits.

Was there any development on this issue? What would be the best place to start to create a plugin to support this feature?

Best regards
</comment><comment author="burtonator" created="2016-04-12T20:03:34Z" id="209081227">+1 ... we really need the number of documents present in the current search result.. we need it for accounting on our end.  
</comment><comment author="alexbernotas" created="2016-05-13T11:17:32Z" id="219016331">+1 this is definetly a needed solution, it is really costly to iteerate and process results into memory if you want to paginate through 100mil+ results

is there an indication on someone accepting or starting to work on this feature? or is this posisble with count API?
</comment><comment author="burtonator" created="2016-05-13T15:55:30Z" id="219084503">Thanks @clintongormley ... I opened this bug because we need this for production here at http://spinn3r.com ... 

We now have a custom plugin where we change the ES endpoints and have our own adapter to respond to HTTP results with the header.  
</comment><comment author="dimitris-athanasiou" created="2016-11-18T10:55:33Z" id="261504607">+1 This has also made our lives harder while developing at prelert. I'll be happy to give it a go once we agree on the solution.
</comment><comment author="clintongormley" created="2016-11-18T13:46:45Z" id="261535111">&gt; Response contains ES-Scroll-Id header if there are more results to retrieve. It does not contain that header if there are no results. Then we'd just grab that header whenever we see it and add a search function to the queue.

@epixa this wouldn't be enough as we still need the scroll ID to clear the scroll.

Perhaps the Link header is the right one, we can provide one link for `next`, and use an [extension relation type](http://www.rfc-editor.org/rfc/rfc5988.txt) for the clear link.

One potential problem: scroll IDs which target many shards can be too long for the max header length on requests, I'm not sure if the same limit might apply on responses?
</comment><comment author="dimitris-athanasiou" created="2016-11-18T14:12:35Z" id="261540471">&gt; One potential problem: scroll IDs which target many shards can be too long for the max header length on requests, I'm not sure if the same limit might apply on responses?

Oh, right. Yes. Putting the scroll id in the header is just not going to work. The HTTP protocol does not specify a max size but most servers implement one, usually choosing either 8KB or 16KB. Both extremely low to accommodate a fat, juicy scroll ID. Forgot about that and how badly I've been bitten.
</comment><comment author="epixa" created="2016-11-18T14:58:55Z" id="261551722">Ah, nuts!  I hadn't thought of that.  Total number of hits could still be useful, but the gain here is significantly diminished without being able to access both up front.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deb/rpm packaging and path.data with multiple values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16992</link><project id="" key="" /><description>Hello! As reported in https://github.com/elastic/cookbook-elasticsearch/issues/441, the init script for [deb](https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L146) packaging runs `mkdir` on `$DATA_DIR`. This seems to make it impossible to use the init script for a multivalued `path.data` configuration.

Should we be checking for a comma or doing something else creative with `$DATA_DIR` before calling `mkdir`? Currently, this init script can produce a single directory named `/foo,/bar`.
</description><key id="139045948">16992</key><summary>Deb/rpm packaging and path.data with multiple values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martinb3</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2016-03-07T17:53:12Z</created><updated>2016-05-17T19:57:15Z</updated><resolved>2016-05-17T19:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove friction from the mapping changes in 5.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16991</link><project id="" key="" /><description>This tries to remove friction to upgrade to 5.0 that would be caused by mapping
changes:
- old ways to specify mapping settings (eg. store: yes instead of store:true)
  will still work but a deprecation warning will be logged
- string mappings that only use the most common options will be upgraded
  automatically to text/keyword
</description><key id="139042661">16991</key><summary>Remove friction from the mapping changes in 5.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T17:38:25Z</created><updated>2016-03-11T08:24:40Z</updated><resolved>2016-03-11T08:24:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-08T07:45:54Z" id="193652154">@rjernst I added `copy_to` and `fields`.
</comment><comment author="rjernst" created="2016-03-09T21:54:19Z" id="194526555">LGTM
</comment><comment author="jpountz" created="2016-03-10T09:10:57Z" id="194746205">Thanks Ryan. FYI after more discussions with @clintongormley about this I only left this upgrade enabled for new indices as upgrading old indices was causing too many issues (because eg. text fields don't support the deprecated object notation that allows to pass a per-document boost).
</comment><comment author="clintongormley" created="2016-03-10T10:15:28Z" id="194774679">To summarise the plan:
- String mappings in old indices will not be upgraded.
- Text/Keyword mappings can be added to old and new indices.
- String mappings on new indices will be upgraded automatically to text/keyword mappings, if possible, with deprecation logging. 
- If it is not possible to automatically upgrade, an exception will be thrown.
</comment><comment author="rashidkpc" created="2016-03-10T14:20:48Z" id="194869003">Thanks Clint. So for now we'll leave our code the same, but start working on the re-indexing upgrade strategy for the `.kibana` index, so we're ready when `string` support is completely removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test for the index_options on a keyword field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16990</link><project id="" key="" /><description>This found a bug in the validation, which was checking the wrong IndexOptions.
</description><key id="139016929">16990</key><summary>Add test for the index_options on a keyword field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T16:05:19Z</created><updated>2016-03-08T08:25:37Z</updated><resolved>2016-03-08T08:25:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-07T16:18:06Z" id="193323777">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for alpha versions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16989</link><project id="" key="" /><description>Elasticsearch 5.0 will come with alpha versions which is not supported
in the current version scheme. This commit adds support for aplpha starting
with es 5.0.0 in a backwards compatible way.
</description><key id="139011351">16989</key><summary>Add support for alpha versions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T15:46:16Z</created><updated>2016-03-10T18:34:53Z</updated><resolved>2016-03-07T17:11:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-07T15:53:16Z" id="193311786">LGTM
</comment><comment author="s1monw" created="2016-03-07T16:53:10Z" id="193342410">@nik9000 I had some failures due to some old build plugin infos.. I fixed the plugin version, @rjernst can you check my gradle changes?
</comment><comment author="rjernst" created="2016-03-07T17:11:11Z" id="193350864">Looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ParseFieldMatcher should log when using deprecated settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16988</link><project id="" key="" /><description>I always thought ParseFieldMatcher would log when using a deprecated setting,
but it does not.
</description><key id="139011087">16988</key><summary>ParseFieldMatcher should log when using deprecated settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Logging</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T15:45:09Z</created><updated>2016-03-08T08:53:07Z</updated><resolved>2016-03-08T08:53:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-07T20:36:58Z" id="193438228">Looks good, one suggestion with the tests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rework norms parameters for 5.0.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16987</link><project id="" key="" /><description>Changes:
- no more option to configure eager/lazy loading of the norms (useless now
  that orms are disk-based)
- only the `string`, `text` and `keyword` fields support the `norms` setting
- the `norms` setting takes a boolean that decides whether norms should be
  stored in the index but old options are still supported to give users time
  to upgrade
- setting a `boost` no longer implicitly enables norms (for new indices only,
  this is still needed for old indices)
</description><key id="139002524">16987</key><summary>Rework norms parameters for 5.0.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T15:10:23Z</created><updated>2016-03-14T08:32:00Z</updated><resolved>2016-03-14T07:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-07T21:15:18Z" id="193453238">@jpountz I left some comments.
</comment><comment author="jpountz" created="2016-03-10T07:11:50Z" id="194709233">@rjernst I removed the bw compat for norms on the `keyword` field.
</comment><comment author="jpountz" created="2016-03-11T09:53:55Z" id="195301923">@rjernst Could you give it another look?
</comment><comment author="rjernst" created="2016-03-11T17:07:43Z" id="195457638">Thanks @jpountz. I still wish `text` field didn't have old norms bwc either...but I realize it is using the shared `parseField`...LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable unmap hack for java 9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16986</link><project id="" key="" /><description>This mechanism changed for java 9: requires a "fake" permission of `accessClassInPackage.jdk.internal.ref`

Currently unmapping will not work with java 9 unless we make changes:

```
  2&gt; REPRODUCE WITH: gradle :core:test -Dtests.seed=D11864EB2E98C3EF -Dtests.class=org.elasticsearch.common.lucene.LuceneTests -Dtests.method="testMMapHackSupported" -Des.logger.level=WARN -Dtests.security.manager=true -Dtests.locale=jmc-TZ -Dtests.timezone=Antarctica/DumontDUrville
FAILURE 0.13s J0 | LuceneTests.testMMapHackSupported &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: MMapDirectory does not support unmapping: Unmapping is not supported, because not all required permissions are given to the Lucene JAR file: java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessClassInPackage.jdk.internal.ref") [Please grant at least the following permissions: RuntimePermission("accessClassInPackage.sun.misc"), RuntimePermission("accessClassInPackage.jdk.internal.ref"), and ReflectPermission("suppressAccessChecks")]
```

See https://issues.apache.org/jira/browse/LUCENE-6989 for more information. 

I also had to upgrade my gradle to the latest to work at all with java 9, and fix a test so the source code compiles at all. I only ran this small unit test because hotspot on java 9 has been broken for a long time.
</description><key id="138996488">16986</key><summary>Enable unmap hack for java 9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T14:47:57Z</created><updated>2016-03-08T00:36:54Z</updated><resolved>2016-03-07T20:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-03-07T15:09:51Z" id="193290297">Looks good.
</comment><comment author="mikemccand" created="2016-03-07T19:38:16Z" id="193416054">+1, thanks @rmuir.  It's important we can run under Java 9 ea even if recent builds have been badly broken...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove leniency from segments info integrity checks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16985</link><project id="" key="" /><description>Closes #16973
</description><key id="138956125">16985</key><summary>Remove leniency from segments info integrity checks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T11:25:21Z</created><updated>2016-03-09T14:05:24Z</updated><resolved>2016-03-09T10:25:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2016-03-07T15:46:07Z" id="193306089">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range query returns inaccurate results when split into two separate conditions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16984</link><project id="" key="" /><description>Elasticsearch version: 2.2.0

JVM version: 1.8.0_74

OS version: Ubuntu 14.04

I'm running this query:

```
{
  "sort": {
    "timestamp": {
      "order": "desc"
    }
  },
  "size": 100,
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "nested": {
              "path": "from",
              "query": {
                "filtered": {
                  "filter": {
                    "range": {
                      "from.vin_timestamp": {
                        "gte": "2016-02-09T22:00:00.000Z"
                      }
                    }
                  }
                }
              }
            }
          },
          {
            "nested": {
              "path": "from",
              "query": {
                "filtered": {
                  "filter": {
                    "range": {
                      "from.vin_timestamp": {
                        "lte": "2016-02-11T21:59:59.999Z"
                      }
                    }
                  }
                }
              }
            }
          }
        ]
      }
    }
  }
}
```

Which produces inaccurate results (only one condition met), however, when I combine the two ranges within one bucket I get accurate results: 

```
{
  "sort": {
    "timestamp": {
      "order": "desc"
    }
  },
  "size": 100,
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "nested": {
              "path": "from",
              "query": {
                "filtered": {
                  "filter": {
                    "range": {
                      "from.vin_timestamp": {
                        "gte": "2016-02-09T22:00:00.000Z",
                        "lte": "2016-02-11T21:59:59.999Z"
                      }
                    }
                  }
                }
              }
            }
          }
        ]
      }
    }
  }
}
```

These two queries should be identical in practice however they produce different results.
</description><key id="138954310">16984</key><summary>Range query returns inaccurate results when split into two separate conditions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">orweinberger</reporter><labels /><created>2016-03-07T11:16:40Z</created><updated>2016-03-07T13:30:03Z</updated><resolved>2016-03-07T13:20:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-07T13:20:35Z" id="193245447">@orweinberger this is working as expected.  If you want all conditions to be applied to a single nested document, then those conditions must be within the same `nested` query.  Otherwise there is no point in using nested fields.
</comment><comment author="orweinberger" created="2016-03-07T13:30:03Z" id="193248674">Doh, not sure how I missed that, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use `mmapfs` by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16983</link><project id="" key="" /><description>ES today has a default directory implementation which switches based on file extension, only using `mmapfs` for files that Lucene does heavy random access on (terms dict, doc values) and `niofs` otherwise.

But unfortunately, for compound files (`.cfs`), which I suspect are often the majority of segments, especially for  large shards, it still uses `niofs`, hurting performance when we do random access on the sub-files within those.

ES used to have issues with segment explosion, which I think contributed to wanting to reduce the mmap count, but we've fixed those, and cfs is now enabled by default, so I think the performance hit of using the inferior `niofs` on compound file segments is the wrong tradeoff.

We could alternatively keep the current "switching" directory, but for `.cfs` extension use `mmapfs`.
</description><key id="138951948">16983</key><summary>Use `mmapfs` by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2016-03-07T11:03:16Z</created><updated>2016-04-08T18:25:06Z</updated><resolved>2016-04-08T18:25:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2016-03-07T15:07:37Z" id="193289235">Strong +1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

This hybrid one is horrible, sorry. It is just caused by some rare customers misusing their cluster. In addition, it goes back to the time when Lucene showed the crazy OOM when mapping failed (which is/was a Java bug). We fixed this so mmap failed now throws an IOException as you would expect.
</comment><comment author="bleskes" created="2016-03-08T09:59:07Z" id="193697426">I'm wondering if something has changed since we last evaluated and explicitly decided to not put cfs files under mmap - see discussion on https://github.com/elastic/elasticsearch/pull/6636
</comment><comment author="mikemccand" created="2016-03-08T20:03:29Z" id="193945104">I think way back then we had problems with "segment explosion" (e.g. index throttling was only just added when that issue was created, there were settings to turn off merging, etc.).

We also didn't use Lucene's defaults for CFS, I think, so a shard could have lots of small files.

Shards today should have fewer segments, should be more often CFS, and shouldn't run up against the "too many memory maps" very easily.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove old and unsupported version constants</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16982</link><project id="" key="" /><description>All version &lt;= 2.0 are not supported anymore. This commit removes all
uses of these versions.
</description><key id="138949514">16982</key><summary>Remove old and unsupported version constants</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T10:51:36Z</created><updated>2016-03-07T15:03:18Z</updated><resolved>2016-03-07T15:03:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-07T14:42:39Z" id="193276947">LGTM
</comment><comment author="colings86" created="2016-03-07T14:47:45Z" id="193278715">@s1monw Left one comment but otherwise this LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't call IR#leaves() after global field data has been constructed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16981</link><project id="" key="" /><description>This IR may already be closed and GlobalFieldData is cached and this can cause AlreadyClosedException while checking the assert.
</description><key id="138946512">16981</key><summary>Don't call IR#leaves() after global field data has been constructed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Fielddata</label><label>non-issue</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T10:37:05Z</created><updated>2016-03-07T12:54:41Z</updated><resolved>2016-03-07T12:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-07T10:48:05Z" id="193201246">LGTM
</comment><comment author="martijnvg" created="2016-03-07T12:54:41Z" id="193237561">thx @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add deprecation logging when users use the SCAN search type.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16980</link><project id="" key="" /><description>Relates to #16910
</description><key id="138933577">16980</key><summary>Add deprecation logging when users use the SCAN search type.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.3.0</label></labels><created>2016-03-07T09:41:18Z</created><updated>2016-03-07T09:59:34Z</updated><resolved>2016-03-07T09:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-07T09:43:30Z" id="193182488">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ClusterStateHealth's validationFailures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16979</link><project id="" key="" /><description>The cluster health response has a `validation_failures` entry , which is currently [wired](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/health/ClusterStateHealth.java#L93) to internal structural validation of the routing table. 

As discussed in #4531 , these entry can be removed and replaced with assertion code in the routing table it self. I have never any production validation failure - we unit test them well and errors are exposed before release.  
</description><key id="138929500">16979</key><summary>Remove ClusterStateHealth's validationFailures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2016-03-07T09:20:52Z</created><updated>2016-04-20T13:37:42Z</updated><resolved>2016-04-20T13:37:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>'us-east-1` is not a valid region when creating s3 bucket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16978</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**: master

**JVM version**: 1.8

**OS version**: OSX

**Describe the feature**: 

When we using `us-east-1`, s3 plugin will first get the endpoint by using the `region`, and then use the endpoint to create the client, this is fine. 

But when creating the bucket, s3 plugin will will the `bucket` name to build a `CreateBucketRequest` ([link](https://github.com/elastic/elasticsearch/blob/mase/plugins/repository-s3/src/main/java/org/elasticsearch/cloud/aws/blobstore/S3BlobStore.java#L96)), `us-east-1` is not valid for aws s3 sdk.

Reproduce:

```
PUT _snapshot/test-1
{
  "type": "s3",
  "settings": {
    "bucket": "test-us-east-1",
    "region": "us-east-1"
  }
}
```

```
{
   "error": {
      "root_cause": [
         {
            "type": "repository_exception",
            "reason": "[test-6] failed to create repository"
         }
      ],
      "type": "repository_exception",
      "reason": "[test-6] failed to create repository",
      "caused_by": {
         "type": "creation_exception",
         "reason": "Guice creation errors:\n\n1) Error injecting constructor, com.amazonaws.services.s3.model.AmazonS3Exception: The specified location-constraint is not valid (Service: Amazon S3; Status Code: 400; Error Code: InvalidLocationConstraint; Request ID: 85CFF34E01878232), S3 Extended Request ID: Ob5XZJsy8IH7HaZy/moMNAgvaH3ZIrHN9fxyimecIp+xtMZI8nE/sc2YVIoTuf2SuEXyoiQP1wE=\n  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)\n  while locating org.elasticsearch.repositories.s3.S3Repository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error",
         "caused_by": {
            "type": "amazon_s3_exception",
            "reason": "The specified location-constraint is not valid (Service: Amazon S3; Status Code: 400; Error Code: InvalidLocationConstraint; Request ID: 85CFF34E01878232)"
         }
      }
   },
   "status": 500
}
```

Succeed after removed the region

```
PUT _snapshot/test-1
{
  "type": "s3",
  "settings": {
    "bucket": "test-us-east-1"
  }
}

```

Same thing happened if we used `us-west` | `ap-southeast` | `eu-central`... 
I think if we could get the endpoint, we should using the valid region name and let user be able to create buckets.
</description><key id="138928323">16978</key><summary>'us-east-1` is not a valid region when creating s3 bucket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Repository S3</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2016-03-07T09:17:19Z</created><updated>2017-02-27T12:58:17Z</updated><resolved>2017-02-24T17:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="debarshiraha" created="2016-09-28T21:43:25Z" id="250309684">As per http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html: "If you are creating a bucket on the US East (N. Virginia) region (us-east-1), you do not need to specify the location constraint".

Other regions work fine. I can take this up.
</comment><comment author="dadoonet" created="2017-02-24T15:52:32Z" id="282325808">Knowing that we deprecated `region` in #22848 and removed it in #22853, I think we should close this issue as it won't be fixed.

@clintongormley WDYT?</comment><comment author="rjernst" created="2017-02-24T17:26:32Z" id="282351003">This issue wasn't even about region really, it was about auto bucket creation (failing because of specifying region).  Since auto bucket creation is now gone, we can close.</comment><comment author="dadoonet" created="2017-02-24T17:27:14Z" id="282351186">Thanks @rjernst </comment><comment author="yandooo" created="2017-02-27T12:58:17Z" id="282712314">@dadoonet and @rjernst thanks.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis : Allow string explain param in JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16977</link><project id="" key="" /><description>Move some test methods from AnalylzeActionIT to RestAnalyzeActionTest
Allow string explain param if it can parse
Fix wrong param name in rest-api-spec

Closes #16925
</description><key id="138885793">16977</key><summary>Analysis : Allow string explain param in JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:Analysis</label><label>bug</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-07T04:41:06Z</created><updated>2016-03-16T19:37:17Z</updated><resolved>2016-03-08T08:04:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-07T05:34:42Z" id="193112110">Left some minor conments. It looks good to me.
</comment><comment author="johtani" created="2016-03-07T07:59:24Z" id="193144663">@dadoonet Thanks for reviewing. Fix your comment and remove unused import from AnalyzeActionIT
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16976</link><project id="" key="" /><description /><key id="138874350">16976</key><summary>fix version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thefourtheye</reporter><labels><label>docs</label></labels><created>2016-03-07T03:13:14Z</created><updated>2016-03-07T13:41:39Z</updated><resolved>2016-03-07T09:54:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thefourtheye" created="2016-03-07T03:46:20Z" id="193079000">I just signed CLA. Is there anyway CI can be re-triggered?
</comment><comment author="clintongormley" created="2016-03-07T09:54:50Z" id="193185997">thanks @thefourtheye 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Built-in sorting in _cat APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16975</link><project id="" key="" /><description>While using `sort(1)` is great and all, more often than not I find myself using browser for `_cat` APIs. It's easy to bookmark and reuse.

Unfortunately, `/_cat/indices?v` and similar endpoints have undefined order of rows and it is not always convenient. Moreover, `?v` makes using `sort(1)` less convenient, since header is sorted too.

I suggest adding another parameter to sort on any column list. It would accept the same thing as `h`. Example request to sort on index time:

```
curl -s 'http://127.0.0.1:9200/_cat/indices?v&amp;h=index,pri,docs,indexing.index_time&amp;s=indexing.index_time'
```
</description><key id="138840480">16975</key><summary>Built-in sorting in _cat APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label></labels><created>2016-03-06T21:25:36Z</created><updated>2016-10-11T16:29:22Z</updated><resolved>2016-10-11T16:29:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-07T09:53:34Z" id="193185716">Nice idea.  Sort should be ascending by default, and accept `:desc` to invert the order.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add points format to stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16974</link><project id="" key="" /><description>Follow up from https://github.com/elastic/elasticsearch/pull/16964

We do things like breakdown RAM usage by parts of the index (postings, norms, etc). We should add points here as well.
</description><key id="138832991">16974</key><summary>add points format to stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Stats</label><label>adoptme</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha2</label></labels><created>2016-03-06T20:19:42Z</created><updated>2016-04-15T13:59:14Z</updated><resolved>2016-04-15T13:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>remove all leniency from checksum verification / segment IDs / commit IDs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16973</link><project id="" key="" /><description>Followup from https://github.com/elastic/elasticsearch/pull/16964

With lucene 5+ indexes, we always have these features in index files, so conditional version checks or best-effort checks should all be cleaned up.
</description><key id="138832912">16973</key><summary>remove all leniency from checksum verification / segment IDs / commit IDs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>adoptme</label><label>enhancement</label></labels><created>2016-03-06T20:18:01Z</created><updated>2016-03-09T10:25:42Z</updated><resolved>2016-03-09T10:25:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Build empty extended stats aggregation if no docs collected for bucket</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16972</link><project id="" key="" /><description>Closes #16812

I think, this change was missed in #9544
</description><key id="138721467">16972</key><summary>Build empty extended stats aggregation if no docs collected for bucket</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexshadow007</reporter><labels><label>:Aggregations</label><label>bug</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-05T21:01:51Z</created><updated>2016-03-07T12:33:40Z</updated><resolved>2016-03-07T09:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2016-03-07T09:44:30Z" id="193182703">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Redocument the `index.merge.scheduler.max_thread_count` setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16971</link><project id="" key="" /><description>Closes #16961
</description><key id="138691026">16971</key><summary>Redocument the `index.merge.scheduler.max_thread_count` setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>review</label></labels><created>2016-03-05T15:31:19Z</created><updated>2016-03-09T10:30:49Z</updated><resolved>2016-03-07T09:42:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-05T15:31:52Z" id="192672708">@mikemccand how does this sound?
</comment><comment author="mikemccand" created="2016-03-05T18:53:20Z" id="192706026">+1 thanks @clintongormley.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoNodeAvailableException when use BulkRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16970</link><project id="" key="" /><description>**Elasticsearch version**:2.1.0

**JVM version**: 1.7

**Description of the problem including expected versus actual behavior**:

NoNodeAvailableException when use BulkRequest

**Steps to reproduce**:
 1.
 2.
 3.

**Provide logs (if relevant)**:
[03-05 22:17:26,154] ERROR [][][Thread-7] c.y.s.common.canal.BaseCanalClient[126] - process error!
org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [{#transport#-1}{127.0.0.1}{localhost/127.0.0.1:4593}]
    at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:290) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:207) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.client.transport.support.TransportProxyClient.execute(TransportProxyClient.java:55) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.client.transport.TransportClient.doExecute(TransportClient.java:286) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:351) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59) ~[elasticsearch-2.2.0.jar:2.2.0]
    at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67) ~[elasticsearch-2.2.0.jar:2.2.0]
    at com.yuanpin.slora.es.base.BaseIndexType.handleEvents(BaseIndexType.java:431) ~[slora.biz-2.0-SNAPSHOT.jar:na]
    at com.yuanpin.slora.common.canal.CanalEventRegister.trigger(CanalEventRegister.java:81) ~[slora.common-2.0-SNAPSHOT.jar:na]
    at com.yuanpin.slora.common.canal.BaseCanalClient.process(BaseCanalClient.java:114) ~[slora.common-2.0-SNAPSHOT.jar:na]
    at com.yuanpin.slora.common.canal.BaseCanalClient$2.run(BaseCanalClient.java:68) [slora.common-2.0-SNAPSHOT.jar:na]
    at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17]
</description><key id="138685045">16970</key><summary>NoNodeAvailableException when use BulkRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidcai19840412</reporter><labels /><created>2016-03-05T14:23:50Z</created><updated>2016-03-05T15:45:14Z</updated><resolved>2016-03-05T15:45:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-05T15:45:14Z" id="192675216">It sounds like your transport client can't connect to any nodes.

Please ask questions like these in the forum instead: https://discuss.elastic.co/

The github issues list is reserved for bug reports and feature requests only.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot filter on _parent field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16969</link><project id="" key="" /><description>Hi,

in 2.2, I cannot filter on the _parent field anymore. E.g. the query:

``` js
 "query": {
    "bool": {
      "filter": [
              { "terms": { "_parent": [ [ "0#2998", "0#3029" ] ] }}
            ]
          }
        }
```

does not return anything for me, while the same with `_routing` instead of `_parent` does.
In the result I can clearly see, that the content of the `_parent` field is correct (in this case, has the same value as `_routing`). Also, in 1.x filtering on `_parent` worked.

Regards,
Tamas
</description><key id="138673051">16969</key><summary>Cannot filter on _parent field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mesztam</reporter><labels /><created>2016-03-05T12:14:09Z</created><updated>2017-06-26T12:24:18Z</updated><resolved>2016-03-05T15:41:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-05T15:41:53Z" id="192674951">Hi @mesztam 

It works for me in 2.2:

```
PUT t 
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

PUT t/parent/1
{}

PUT t/child/2?parent=1
{}

GET t/_search
{
  "query": {
    "terms": {
      "_parent": ["parent#1"]
    }
  }
}
```

However, you shouldn't rely on this working in the future. This behaviour will be removed in 5.0.  See https://github.com/elastic/elasticsearch/issues/5399#issuecomment-156814603 for more

Use a `has_parent` query instead
</comment><comment author="martijnvg" created="2016-03-05T15:43:53Z" id="192675132">@mesztam Or from 5.0 you can use the new `parent_id` query, to find child documents with a specific parent id value.
</comment><comment author="Ragazzo" created="2017-06-26T11:55:14Z" id="311038266">@martijnvg can you given an example of such query? would be very usefull, since docs are very unclear about it.</comment><comment author="martijnvg" created="2017-06-26T12:04:59Z" id="311040184">@Ragazzo The `parent_id` query is described here: https://www.elastic.co/guide/en/elasticsearch/reference/5.4/query-dsl-parent-id-query.html</comment><comment author="Ragazzo" created="2017-06-26T12:14:38Z" id="311042107">@martijnvg I saw that, but how can I query all childs by several parents ids? I mean `parent_id IN (1,2,3,4,5)`? Docs only describe 1 to 1 match.</comment><comment author="martijnvg" created="2017-06-26T12:22:22Z" id="311043697">@Ragazzo You can wrap multple `parent_id` queries in the should clause of a `bool` query:
https://www.elastic.co/guide/en/elasticsearch/reference/5.4/query-dsl-bool-query.html</comment><comment author="Ragazzo" created="2017-06-26T12:23:51Z" id="311043988">@martijnvg but I am quering by parent table, all what I want to do is to get parents with childs, based on childs parent_id. Is it possible?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add build methods to create CompletionSuggestionBuilder from query and parse contexts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16968</link><project id="" key="" /><description>This implements methods in CompletionSuggestionBuilder to construct a builder given query and parse contexts respectively. Now we ensure that the builder outputs identical output when used directly or through json parsing by storing query contexts as binary in the builder. All validation requiring `MapperService` are done once the suggestion is built on the shard level. 
</description><key id="138655135">16968</key><summary>Add build methods to create CompletionSuggestionBuilder from query and parse contexts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Search Refactoring</label><label>:Suggesters</label><label>review</label></labels><created>2016-03-05T06:49:01Z</created><updated>2016-03-09T17:33:03Z</updated><resolved>2016-03-09T17:33:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2016-03-08T21:14:41Z" id="193970593">@areek I did a round of review and left a few questions and comments. I assume there is no alternative to going back to parsing the query contexts on the shard. I wonder if there's an easier way to mock the mapper service in the tests, I found that part a bit hard to follow. Other than that I think it looks great.
</comment><comment author="areek" created="2016-03-09T16:39:02Z" id="194386742">Thanks @cbuescher for the feedback, I addressed all the comments!
</comment><comment author="cbuescher" created="2016-03-09T16:40:02Z" id="194387293">Looks great, by which I mean LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when multiple zones are used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16967</link><project id="" key="" /><description>**Elasticsearch version**: 2.1.2
**JVM version**:1.8.0_74
**OS version**:Linux 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt20-1+deb8u3 (2016-01-17) x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:
GCE Discovery is not working when multiple zones are used. I followed the steps here: https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-gce-usage-discovery-zones.html
I have tested my configuration by changing the configuration file to use 1 zone and GCE discovery works correctly and the nodes can communicate correctly. As soon as I add a second zone the nodes stop communicating to each other. 

I was expecting to configure all nodes to communicate with each other across all Google Cloud zones.  If I need a new node in a different zone I could clone my existing instance template and move it to the new zone and communication between nodes will happen automatically across all zones.

**Steps to reproduce**:
1. Install Elastic 2.1.2 +  GCE plugin for 2 Linux instances in the Google Cloud
2. Configure one of the instances to use multiple zones

**Provide logs (if relevant)**:

```
[2016-03-04 13:25:35,730][WARN ][discovery.gce ] [Firebrand] Exception caught during discovery java.lang.NullPointerException : null
[2016-03-04 13:25:35,732][TRACE][discovery.gce ] [Firebrand] Exception caught during discovery
java.lang.NullPointerException
at com.google.common.collect.Iterables$3.transform(Iterables.java:512)
at com.google.common.collect.Iterables$3.transform(Iterables.java:509)
at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
at com.google.common.collect.Iterators$5.hasNext(Iterators.java:548)
at org.elasticsearch.common.util.CollectionUtils.iterableAsArrayList(CollectionUtils.java:390)
at org.elasticsearch.cloud.gce.GceComputeServiceImpl.instances(GceComputeServiceImpl.java:97)
at org.elasticsearch.discovery.gce.GceUnicastHostsProvider.buildDynamicNodes(GceUnicastHostsProvider.java:123)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.sendPings(UnicastZenPing.java:335)
at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.ping(UnicastZenPing.java:240)
at org.elasticsearch.discovery.zen.ping.ZenPingService.ping(ZenPingService.java:106)
at org.elasticsearch.discovery.zen.ping.ZenPingService.pingAndWait(ZenPingService.java:84)
at org.elasticsearch.discovery.zen.ZenDiscovery.findMaster(ZenDiscovery.java:879)
at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:335)
at org.elasticsearch.discovery.zen.ZenDiscovery.access$5000(ZenDiscovery.java:75)
at org.elasticsearch.discovery.zen.ZenDiscovery$JoinThreadControl$1.run(ZenDiscovery.java:1236)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
[2016-03-04 13:25:35,743][DEBUG][discovery.gce ] [Firebrand] 0 node(s) added
```

My configuration yml file:

```
cluster.name: elastic
network.host: 0.0.0.0
http.port: 9201
http.cors.enabled : true
http.cors.allow-origin : /.*/
transport.tcp.port: 9301
cloud:
  gce:
      project_id: app
      zone: ["asia-east1-a", "us-central1-a"]
discovery:
      type: gce
shield.enabled: false
readonlyrest:
    enable: true
    response_if_req_forbidden: Not found
    access_control_rules:

    - name: Accept only requests with api keys
      type: allow
      api_keys: [XXX]
      methods: [GET,POST,PUT,DELETE,OPTIONS]
```

By the way: I've also configured the metadata es_port=9301

GCE discovery only works if the zone property is changed to one of the following:

```
zone: ["asia-east1-a"] 
zone: asia-east1-a
```
</description><key id="138643620">16967</key><summary>NullPointerException when multiple zones are used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ghorkov</reporter><labels><label>:Plugin Cloud GCE</label><label>bug</label></labels><created>2016-03-05T04:44:27Z</created><updated>2016-06-30T09:44:55Z</updated><resolved>2016-06-30T09:44:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-03-05T10:58:49Z" id="192622689">I looked at the source code and I think that this could happen if you have absolutely no instance running in one of the zones you mentioned. Is that your case?
If I'm right, that means we need to catch this case properly instead of sending a NPE.

Could you confirm that please?

For the record, we have a test which tries settings with 2 zones: https://github.com/elastic/elasticsearch/blob/2.1/plugins/cloud-gce/src/test/java/org/elasticsearch/discovery/gce/GceDiscoverySettingsTests.java#L72.
</comment><comment author="ghorkov" created="2016-03-05T22:08:27Z" id="192750735">Thank you dadoonet,

That is correct at the moment I don't have any instances running in the second zone. This is my current setup:

&gt; zone: asia-east1-a has node1 &amp; node2
&gt; zone: us-central1-a doesn't have any active nodes

The problem is that when a second zone is added to the configuration file the communication between the nodes is terminated. I'm using autoscaling so depending on traffic I may need a new node on a different zone or if traffic is low the nodes in a certain zone can be shutdown leaving that zone empty
</comment><comment author="dadoonet" created="2016-03-06T16:03:32Z" id="192921094">Thanks for confirming. Definitely something we need to fix.
</comment><comment author="dadoonet" created="2016-06-30T09:18:44Z" id="229606825">@ghorkov I was able to reproduce it and came with fixes for 5.x and 2.x versions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cli: Simplify test terminals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16966</link><project id="" key="" /><description>This commit simplifies and consolidates the two different
implementations of terminals used in tests. There is now a single
MockTerminal which captures output, and allows accessing as one large
string (with unix style \n as newlines), as well as configuring
input.
</description><key id="138637925">16966</key><summary>Cli: Simplify test terminals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2016-03-05T03:14:53Z</created><updated>2016-03-06T21:17:00Z</updated><resolved>2016-03-06T21:17:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-03-05T23:34:12Z" id="192760089">&gt; without unix style \n as newlines

What do you mean by this?
</comment><comment author="rjernst" created="2016-03-05T23:36:21Z" id="192760851">The mock terminal writes \n as newlines into its byte buffer. 
</comment><comment author="rjernst" created="2016-03-05T23:36:49Z" id="192760917">Oops, it should be _with_!
</comment><comment author="nik9000" created="2016-03-05T23:45:54Z" id="192761780">Left some suggestions but LGTM other than the nocommit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle RejectedExecution gracefully in TransportService during shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16965</link><project id="" key="" /><description>Today we might run into a rejected execution exception when we shutdown the node while handling a transport exception. The exception is run in a seperate thread but that thread might not be able to execute due to the shutdown. Today we barf and fill the logs with large exception. This commit catches this exception and logs it as debug logging instead.

Extends changes made in 8652cd8
</description><key id="138637028">16965</key><summary>Handle RejectedExecution gracefully in TransportService during shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-05T02:58:13Z</created><updated>2016-03-28T20:31:06Z</updated><resolved>2016-03-28T20:31:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-23T14:55:51Z" id="200381282">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>upgrade to lucene 6.0.0-snapshot-bea235f</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16964</link><project id="" key="" /><description>A minimal upgrade to lucene 6 (though with minor cleanups to not leave a COMPLETE mess). 

It uses nothing new at all, cleanups etc are intentionally kept to be relatively minimal. lucene major version upgrades are chaotic enough without that.

Tests pass, it would be good to have in jenkins to help test the upcoming lucene release.
</description><key id="138573798">16964</key><summary>upgrade to lucene 6.0.0-snapshot-bea235f</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v5.0.0-alpha1</label></labels><created>2016-03-04T20:07:57Z</created><updated>2016-03-07T10:52:53Z</updated><resolved>2016-03-07T09:40:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-06T12:33:30Z" id="192883921">LGTM
</comment><comment author="jpountz" created="2016-03-06T16:50:51Z" id="192929231">LGTM
</comment><comment author="s1monw" created="2016-03-07T10:52:53Z" id="193202562">I opened [this](https://github.com/elastic/elasticsearch/pull/16982/) as a followup
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for node.client setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16963</link><project id="" key="" /><description>As discussed in #16565, the node.client setting is an unnecessary shortcut for `node.data: false` and `node.master: false`. We have places where we treat nodes with `node.client` set to `true` differently compared to when nodes have both `node.master` set to `false` and `node.data` set to `false`. Also, with the addition of `node.ingest` (or potentially new roles in the future), it becomes confusing to figure out whether a node client will act as ingest node or not. 

This PR removes the `node.client` setting in favour of explicitly using `node.master`, `node.data` and `node.ingest` instead. Note that `node.client` is now a prohibited node setting rather than just being ignored, so that users know they have to upgrade their settings. A node will not start if it has `node.client` among its settings.

Additionally, tribe nodes and all of the tribe clients are now coordinating nodes only (with the addition of the ingest node type they mistakenly became ingest nodes as well, which they shouldn't have done).

Internally, the DiscoveryNode class supports now the notion of roles, through the inner Role enum. A node with no roles is implicitly a coordinating only node, which is the least that a node can do.

The following are the breaking changes made on the REST (and java api) layer:
- `_cat/nodes` used to return `c` for client node or `d` for data node as part of the `node.role` column. It now returns `m` for master eligible, `d` for data and/or `i` for ingest. A node with no explicit roles will be a coordinating only node and marked with `-`. A node can have multiple roles. The master column has been adapted to return only whether a node is the current master (`*`) or not (`-`).
- The cluster stats api now returns counts for each node role. The `master_data`, `master_only`, `data_only` and `client` fields have been removed from the response in favour of `master`, `data`, `ingest` and `coordinating_only`. The same node can have multiple roles, thus contributing to multiple role counts. Every node is implicitly a coordinating node, so whenever a node has no explicit roles, it will be counted as coordinating only.

In addition, the documentation has been adapted and some tests renamed in our test infra.

Node roles and node attributes have been separated internally within `DiscoveryNode`. Previously node roles would get printed out as part of node attributes in nodes info, nodes stats and tasks list apis, but now that they are stored separately they are printed out as part of a new field of type array called `roles`. One nice side effect of this is that roles will contain also the default ones, while when printed as part of attributes or settings only the ones that differ from default would get printed.

Closes #16565
</description><key id="138566012">16963</key><summary>Remove support for node.client setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Settings</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2016-03-04T19:37:35Z</created><updated>2016-03-30T19:16:28Z</updated><resolved>2016-03-29T19:55:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2016-03-22T11:38:57Z" id="199773908">I merged master in after two weeks of inactivity and make everything work again, reviews are welcome!
</comment><comment author="s1monw" created="2016-03-23T10:40:39Z" id="200291121">@javanna I left some suggestiosn and questions
</comment><comment author="s1monw" created="2016-03-24T20:15:45Z" id="201001924">@javanna what's the status here I'd still like to see [this](https://github.com/elastic/elasticsearch/pull/16963/files#r57137919) addressed
</comment><comment author="javanna" created="2016-03-24T22:07:00Z" id="201045100">@s1monw I am working on it it's the last one on my list, I haven't said I addressed all the comments yet :) will ping you when ready for another review
</comment><comment author="javanna" created="2016-03-25T22:10:39Z" id="201543372">I pushed a bunch of new commits and merged master a number of times. It's much better now, all the nocommits are gone and attributes and roles are well separated within `DiscoveryNode`. I added a new `roles` field in the output of nodes stats, nodes info and tasks list api, because we would previously print node types as part of attributes when they differ from the default. The main reason why the PR got so much bigger is the move of some Streamable to Writeable and the fact that we now require to specify attributes and roles when creating a `DiscoveryNode` instance (I didn't want to have an empty default nor a default with all the roles, better be explicit). This last one is a test problem only, as DiscoveryNodeService takes care of creating nodes in production and does the right thing. Also fixed some line length problems while touching files.

@s1monw ready for another round of review ;)
</comment><comment author="s1monw" created="2016-03-29T07:53:21Z" id="202766400">@javanna we are close, I left another round of review comments
</comment><comment author="javanna" created="2016-03-29T10:46:48Z" id="202828559">Ready for another round of review, left a couple of questions on review comments. @clintongormley maybe you want to have a look at the api changes in nodes info, nodes stats, cluster stats, cat nodes and tasks list? They are all listed in the description of the PR.
</comment><comment author="s1monw" created="2016-03-29T11:40:51Z" id="202844425">@javanna I replied to your comments and added some myself. I think we don't need another round of review - if you feel you addressed it go push... LGTM 
</comment><comment author="jaymode" created="2016-03-29T18:17:40Z" id="203034761">@javanna the fix in the TransportClientNodesService LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reindex should timeout if sub-requests timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16962</link><project id="" key="" /><description>Sadly, it isn't easy to simulate a timeout during an integration test, you
just have to cause one. Groovy's sleep should do the job.
</description><key id="138564564">16962</key><summary>Reindex should timeout if sub-requests timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Reindex API</label><label>enhancement</label><label>review</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2016-03-04T19:30:15Z</created><updated>2016-03-14T19:44:24Z</updated><resolved>2016-03-10T18:06:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2016-03-07T09:02:17Z" id="193165710">didn't look at the code at all - sorry - just a tip concerning simulating timeouts: did you take a look at the MockTransportService? I suspect it should be easy to something similar to what we do in addFailToSendNoConnectRule  and simulate a timeout exception. 
</comment><comment author="dakrone" created="2016-03-10T16:37:00Z" id="194941073">Left minor comments, LGTM
</comment><comment author="nik9000" created="2016-03-14T19:44:24Z" id="196492673">Backported to 2.x with 014475d6fda038992455be35f25338f13ed83ec2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document index.merge.scheduler.max_thread_count again?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16961</link><project id="" key="" /><description>**Elasticsearch version**: 2.x

**Description of the problem including expected versus actual behavior**:

It looks like we are still recommending `index.merge.scheduler.max_thread_count: 1` to be set for HDDs even with the auto merge throttling functionality in 2.x.  Since we have removed the "merge" module section from the docs completely starting in 2.0, it will be nice to add this particular recommendation back to the reference guide (somewhere). 

&gt; index.merge.scheduler.max_thread_count
&gt; The maximum number of threads that may be merging at once. Defaults to Math.max(1, Math.min(3, Runtime.getRuntime().availableProcessors() / 2)) which works well for a good solid-state-disk (SSD). If your index is on spinning platter drives instead, decrease this to 1.
</description><key id="138555264">16961</key><summary>Document index.merge.scheduler.max_thread_count again?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>docs</label></labels><created>2016-03-04T18:54:51Z</created><updated>2016-03-07T09:42:27Z</updated><resolved>2016-03-07T09:42:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-04T19:41:34Z" id="192434048">Makes sense to me.  @mikemccand ?
</comment><comment author="mikemccand" created="2016-03-04T20:08:01Z" id="192444864">+1

But can we perhaps put it on a new doc page "If you use spinning disks..." instead of bringing "merge settings" page back to life for just this one setting?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update groovy api docs to not mention node client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16960</link><project id="" key="" /><description>I noticed while working on the `node.client` removal, that the groovy api docs mention as the only way to create a client, the use of a node client, which we just took out of the java api docs. We should fix that, hopefully the groovy api can make use of the transport client instead.
</description><key id="138551009">16960</key><summary>Update groovy api docs to not mention node client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">javanna</reporter><labels><label>docs</label><label>v5.0.0-alpha3</label></labels><created>2016-03-04T18:38:49Z</created><updated>2016-05-03T15:37:59Z</updated><resolved>2016-05-03T15:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T14:07:40Z" id="216539134">@pickypg can this be closed?
</comment><comment author="pickypg" created="2016-05-03T15:37:59Z" id="216569518">Yep. Deleted the file entirely with 3ca02d647eafcd875802109fcbd009e80d37e5bb
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backport of task management api to 2x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16959</link><project id="" key="" /><description>The first 8 commits are merged code from the master. The last 3 commits contain the new code that is specific to 2.x branch and is not present in the current master. These 3 commits remove java 8 structures, add backward-compatibility layer, and fix the filterNodeIds bug that was discovered during backporting. The last commit fixing the filterNodeIds bug will need to be forward-ported to master as well. I will open a separate PR for it.
</description><key id="138548917">16959</key><summary>Backport of task management api to 2x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Task Manager</label><label>feature</label><label>release highlight</label><label>review</label><label>v2.3.0</label></labels><created>2016-03-04T18:29:21Z</created><updated>2016-03-14T10:28:56Z</updated><resolved>2016-03-10T19:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2016-03-10T14:51:14Z" id="194884031">I am not sure I can add anything to this in terms of review. This change is so big and has been backing on master for a long time. I think we should go with it and fix things as then come up. @imotov WDYT?
</comment><comment author="imotov" created="2016-03-10T15:11:57Z" id="194895664">@s1monw I tried to make this backport to be as reviewable as possible. The first 8 commits are literal backport of corresponding master commits. The last 4 commits with about 600 lines changed lines are the actual backport. However, most of these lines are java 8 to java 7 translation and not that interesting. But if you could review just [this one file](https://github.com/elastic/elasticsearch/pull/16959/files#diff-a8b76a2179d60499b2351dd37d5f08e1L25), I would really appreciate it, since this is pretty much the only non-mechanical change in the entire thing. In master this is an interface with default method implementation, which is not available in 1.7.
</comment><comment author="s1monw" created="2016-03-10T15:14:59Z" id="194898390">that file was easy LGTM ;)
</comment><comment author="nik9000" created="2016-03-10T16:01:42Z" id="194924648">I did a scan on the backport commits and they LGTM as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to remove indices from target cluster for previously deleted indices upon snapshot restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16958</link><project id="" key="" /><description>&lt;!--
If you are filing a feature request, please remove the above bug
report block and provide responses for all of the below items.
--&gt;

**Describe the feature**:

Provide an option for target cluster to be aware of previously deleted indices when restoring snapshots so that deleted indices from the source cluster will cause the target cluster to also remove its indices upon restoring.   This is helpful when attempting to "mirror" clusters without having to keep track of deleted indices outside of the snapshot restore process.  The following is one such use case:

```
1. Take snapshot 1 in Active cluster

2. Close all indices in Passive cluster

3. Passive cluster restores from snapshot 1

4. Remove index1 from alias1 in Active cluster

5. Delete index1 from Active cluster

6. Take snapshot 2 in Active cluster

7. Close all indices in Passive cluster

8. Passive cluster restores from snapshot 2
```

The result is that index1 is closed and remains in alias1 in the Passive cluster after step 8, which would cause problem when the Passive cluster becomes the Active cluster (DR) for if someone then tries to query using alias1 (this will result in an IndexClosedException).  The obvious workaround here will be to manually manage deleted indices by keeping track of what indices have been deleted from Active and delete same indices from the Passive cluster.

The request here is to provide a configurable option for admins to specify whether they want the snapshot restore to automatically remove previously-deleted indices from the target cluster.
</description><key id="138548620">16958</key><summary>Option to remove indices from target cluster for previously deleted indices upon snapshot restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Snapshot/Restore</label><label>discuss</label><label>enhancement</label></labels><created>2016-03-04T18:28:28Z</created><updated>2016-03-04T19:33:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-04T19:33:47Z" id="192430505">In other words, an option to delete all indices that aren't in the snapshot currently being restored.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Installing river-jdbc via command line fails, file not found on elastic's servers.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/16957</link><project id="" key="" /><description>&lt;!--
GitHub is reserved for bug reports and feature requests. The best place
to ask a general question is at the Elastic Discourse forums at
https://discuss.elastic.co. If you are in fact posting a bug report or
a feature request, please include one and only one of the below blocks
in your new issue.
--&gt;

&lt;!--
If you are filing a bug report, please remove the below feature
request block and provide responses for all of the below items.
--&gt;

**Elasticsearch version**:
2.2.0
**JVM version**:
8
**OS version**:
10.11.2 El Capitan
**Description of the problem including expected versus actual behavior**:
Installing plugin river-jdbc fails. 
**Steps to reproduce**:
1. Run command ./bin/plugin install river-jdbc --verbose

**Provide logs (if relevant)**:

```
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/river-jdbc/2.2.0/river-jdbc-2.2.0.zip ...
Failed: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/river-jdbc/2.2.0/river-jdbc-2.2.0.zip]; nested: FileNotFoundException[https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/river-jdbc/2.2.0/river-jdbc-2.2.0.zip]; 
ERROR: failed to download out of all possible locations..., use --verbose to get detailed information
```
</description><key id="138547494">16957</key><summary>Installing river-jdbc via command line fails, file not found on elastic's servers.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GeorgeL9</reporter><labels /><created>2016-03-04T18:24:00Z</created><updated>2016-03-04T18:31:35Z</updated><resolved>2016-03-04T18:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-03-04T18:31:35Z" id="192398499">Rivers were deprecated and removed from Elasticsearch 1.5.0 to Elasticsearch 2.0.0 in #10345. Can I suggest the [Logstash JDBC input plugin](https://github.com/logstash-plugins/logstash-input-jdbc) as an alternative?
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>